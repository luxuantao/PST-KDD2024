<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AGNAS: Attention-Guided Micro-and Macro-Architecture Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zihao</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Research Center for Intelligent Computing Systems</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Computer Architecture</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Research Center for Intelligent Computing Systems</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Computer Architecture</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Computer Architecture</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shun</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Research Center for Intelligent Computing Systems</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Computer Architecture</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Longxing</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Research Center for Intelligent Computing Systems</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Computer Architecture</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jilin</forename><surname>Mei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Research Center for Intelligent Computing Systems</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Computer Architecture</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yinhe</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Research Center for Intelligent Computing Systems</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Computer Architecture</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaowei</forename><surname>Li</surname></persName>
						</author>
						<title level="a" type="main">AGNAS: Attention-Guided Micro-and Macro-Architecture Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Micro-and macro-architecture search have emerged as two popular NAS paradigms recently. Existing methods leverage different search strategies for searching micro-and macro-architectures. When using architecture parameters to search for micro-structure such as normal cell and reduction cell, the architecture parameters can not fully reflect the corresponding operation importance. When searching for the macrostructure chained by pre-defined blocks, many sub-networks need to be sampled for evaluation, which is very time-consuming. To address the two issues, we propose a new search paradigm, that is, leverage the attention mechanism to guide the micro-and macro-architecture search, namely AGNAS. Specifically, we introduce an attention module and plug it behind each candidate operation or each candidate block. We utilize the attention weights to represent the importance of the relevant operations for the micro search or the importance of the relevant blocks for the macro search. Experimental results show that AGNAS can achieve 2.46% test error on CIFAR-10 in the DARTS search space, and 23.4% test error when directly searching on ImageNet in the ProxylessNAS search space. AGNAS also achieves optimal performance on NAS-Bench-201, outperforming state-of-the-art approaches. The source code can be available at https://github.com/Sunzh1996/AGNAS.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Neural Architecture Search (NAS) has attracted lots of attention in recent years, because it can automatically find optimal neural networks in the pre-defined search space for target tasks. More importantly, the searched architecture can perform better than hand-crafted neural networks in many computer vision tasks, such as image classification <ref type="bibr" target="#b53">(Zoph &amp; Le, 2016;</ref><ref type="bibr" target="#b54">Zoph et al., 2018;</ref><ref type="bibr" target="#b13">Guo et al., 2020)</ref>, object detection <ref type="bibr" target="#b7">(Chen et al., 2019;</ref><ref type="bibr" target="#b12">Ghiasi et al., 2019)</ref>, semantic segmentation <ref type="bibr" target="#b5">(Chen et al., 2018;</ref><ref type="bibr" target="#b26">Liu et al., 2019)</ref>. Early NAS methods adopted reinforcement learning (RL) <ref type="bibr" target="#b0">(Baker et al., 2016;</ref><ref type="bibr" target="#b1">Bello et al., 2017;</ref><ref type="bibr" target="#b54">Zoph et al., 2018)</ref> or evolutionary algorithms (EA) <ref type="bibr" target="#b33">(Real et al., 2017;</ref><ref type="bibr" target="#b27">Liu et al., 2018b;</ref><ref type="bibr" target="#b34">Real et al., 2019)</ref>, requiring to train many sub-networks from scratch, which takes thousands or even tens of thousands of GPU-hours. To alleviate this issue, ENAS <ref type="bibr" target="#b32">(Pham et al., 2018)</ref> proposed weight sharing among child architectures in a pre-defined search space, which dramatically reduced the search cost to a few GPU-days.</p><p>Search space plays a vital role in NAS, and can be broadly categorized into micro search space and macro search space. Specifically, as shown in Figure <ref type="figure">1</ref> (a), the micro search, which is also referred to as operation search, aims to determine the operation associated with each pair of nodes in a cell and then manually stack a series of identical cells to build a target neural network. DARTS <ref type="bibr" target="#b28">(Liu et al., 2018c)</ref> relaxed the discrete search space to be continuous, and used a bi-level optimization to alternately optimize the architecture parameters and network weights, thus achieving an efficient end-to-end training. Thereafter, recent works <ref type="bibr" target="#b42">(Xie et al., 2018;</ref><ref type="bibr" target="#b10">Dong &amp; Yang, 2019;</ref><ref type="bibr" target="#b22">Li et al., 2020;</ref><ref type="bibr" target="#b8">Chu et al., 2020;</ref><ref type="bibr" target="#b6">Chen &amp; Hsieh, 2020;</ref><ref type="bibr" target="#b24">Liang et al., 2019;</ref><ref type="bibr" target="#b52">Zhou et al., 2020)</ref> pointed out that the bi-level optimization of DARTS <ref type="bibr" target="#b28">(Liu et al., 2018c)</ref> suffers from performance collapse issues. Moreover, recent works <ref type="bibr" target="#b37">(Wang et al., 2021;</ref><ref type="bibr" target="#b43">Xie et al., 2021)</ref> also demonstrated that the operation associated with the largest magnitude of architecture parameters does not necessarily result in the highest validation accuracy after discretization. In light of the above-mentioned observations, we propose an alternative paradigm, that is, to leverage the attention mechanism to indicate the importance of candidate operations rather than using architecture parameters.</p><p>On the other hand, the macro architecture of neural network is determined by macro search. For example, borrowing from the design of MobileNet <ref type="bibr" target="#b14">(Howard et al., 2017)</ref>, the macro architecture can be chain-style and candidate blocks of each layer are choosable, as shown in Figure <ref type="figure">1 (b)</ref>. Though prior works <ref type="bibr" target="#b13">(Guo et al., 2020;</ref><ref type="bibr" target="#b9">Chu et al., 2021;</ref><ref type="bibr" target="#b48">You et al., 2020)</ref> have proposed to search for superb architectures with a heuristic algorithm based on the pre-trained supernet, such paradigm requires to sample and evaluate numerous sub-networks, which is still time-consuming. However, by using the attention mechanism, we can obtain the searched architecture as the search process converges, without the need for post-sampling.</p><p>In summary, we propose a novel paradigm that aims to search micro and macro architectures in one framework based on the attention mechanism to solve the aforementioned issues. Similar to how the human brain selectively focuses on certain parts of the input <ref type="bibr" target="#b2">(Briggs et al., 2013)</ref>, we demonstrate that the attention mechanism can be used to emphasize useful parts of the network while ignoring the trivial ones. Therefore, we propose to utilize attention weights to indicate the importance of candidate operations. As previous works have pointed out that channels with smaller attention weights can be pruned with neglected influence on network performance <ref type="bibr" target="#b29">(Luo &amp; Wu, 2020;</ref><ref type="bibr" target="#b39">Wang et al., 2019;</ref><ref type="bibr" target="#b47">Yamamoto &amp; Maeno, 2018)</ref>, we propose to accumulate the attention weights of output channels from different candidate operations to conduct the micro search, and the attention weights from different choice blocks to conduct the macro search.</p><p>Unlike prior methods <ref type="bibr" target="#b31">(Nakai et al., 2020;</ref><ref type="bibr" target="#b17">Jiang et al., 2021;</ref><ref type="bibr" target="#b18">Jing et al., 2020;</ref><ref type="bibr" target="#b38">Wang et al., 2020)</ref> that included the attention module in the search space to have more candidate operations, we exploit the attention mechanism to guide the neural architecture search. As illustrated in Figure <ref type="figure">1</ref> (c), we insert the attention module to the output feature map of candidate operations, and then use the attention weights to identify the optimal operation with a clear advantage. Similarly, the candidate blocks (e.g. MBConv blocks in the ProxylessNAS search space) for each layer can also be optimized by the attention module for the macro search, which is shown in Figure <ref type="figure">1 (d)</ref>. As soon as the training of the supernet converges, the importance of operations or blocks is then specified by the attention weights, which are optimized together with network parameters by an efficient gradient descent algorithm such as SGD. Taking the DARTS search space as an example, the operations with the highest attention weight on each edge is kept to build the searched cell for each layer.</p><p>Our contributions can be summarized as follows:</p><p>• We first propose a novel search paradigm by leveraging the attention mechanism to efficiently search for the micro and macro neural architectures in one framework.</p><p>• We use the accumulated channel attention weights to indicate the importance of candidate operations or choice blocks by plugging the attention module behind the output of each edge in a cell or the output of each layer.</p><p>• Benefited from the attention mechanism, the truly operation strength in micro search can be more outstanding and the macro architecture can be easily obtained in an end-to-end manner.</p><p>• We conduct extensive experiments on three popular search spaces, achieving convincing results that outperform previous state-of-the-arts, clearly demonstrating the effectiveness of the proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Micro Search. Micro search aims to determine the operation associated with each edge in a cell. The micro search space proposed by NASNet <ref type="bibr" target="#b54">(Zoph et al., 2018)</ref> is widely used in later works. For example, many works <ref type="bibr" target="#b25">(Liu et al., 2018a;</ref><ref type="bibr" target="#b34">Real et al., 2019;</ref><ref type="bibr" target="#b32">Pham et al., 2018)</ref> proposed their search strategies in the NASNet search space, whereas their search process is discrete and usually takes thousands or even tens of thousands of GPU-hours. Later on, DARTS <ref type="bibr" target="#b28">(Liu et al., 2018c)</ref> proposed relaxing the discrete search space to be continuous so that the search cost can be dramatically reduced by gradient decent technique. DARTS formulates a bi-level problem that simultaneously optimizes the architecture parameters and super-network weights. Unfortunately, the optimization process suffers from the generalizability <ref type="bibr" target="#b42">(Xie et al., 2018;</ref><ref type="bibr" target="#b10">Dong &amp; Yang, 2019;</ref><ref type="bibr" target="#b4">Chang et al., 2019;</ref><ref type="bibr" target="#b22">Li et al., 2020;</ref><ref type="bibr" target="#b8">Chu et al., 2020)</ref> and stability <ref type="bibr" target="#b49">(Zela et al., 2020;</ref><ref type="bibr" target="#b6">Chen &amp; Hsieh, 2020;</ref><ref type="bibr" target="#b37">Wang et al., 2021)</ref> issues. In order to mitigate the performance gap between the super-network and discrete architectures, SNAS <ref type="bibr" target="#b42">(Xie et al., 2018)</ref>, GDAS <ref type="bibr" target="#b10">(Dong &amp; Yang, 2019)</ref>, and DATA <ref type="bibr" target="#b4">(Chang et al., 2019)</ref> adopted the differentiable Gumbel-Softmax to approximate one-hot encoding. SGAS <ref type="bibr" target="#b22">(Li et al., 2020)</ref> gradually pruned redundant operations during the search process, hence the search space progressively squeezes and approximates to the final target architecture. On the other hand, RobustDARTS <ref type="bibr" target="#b49">(Zela et al., 2020)</ref> leveraged eigenvalues of architecture parameters to monitor the search process and applied stronger regularizations to eliminate the instability. SDARTS <ref type="bibr" target="#b6">(Chen &amp; Hsieh, 2020)</ref> proposed a perturbationbased regularization to smooth the loss landscape to overcome the unstable optimization issue. Moreover, recent work <ref type="bibr" target="#b37">(Wang et al., 2021)</ref> pointed out that the values of architecture parameters do not necessarily reflect operation strength and proposed a perturbation-based architecture selection strategy, which, however, requires fine-tuning after discretizing each edge, leading to substantial computation costs. Therefore, considering that the optimization collapse and the uncertainty of selecting architecture process caused by architecture parameters, we propose a novel paradigm and introduce the attention mechanism to guide the neural architecture search instead of using architecture parameters.</p><p>Macro Search. Macro search methods firstly train a chainstyle super-network consisting of a series of choice blocks, and then derive the final optimal sub-network based on the validation accuracy. Because the super-network can be used as a basic performance estimator for different architectures, many works focus on solving the problem of super-network training fairly and efficiently in a huge search space. SPOS <ref type="bibr" target="#b13">(Guo et al., 2020)</ref> trains the super-network through uniform path sampling. FairNAS <ref type="bibr" target="#b9">(Chu et al., 2021)</ref> enforces fairness constraints to alleviate the super-network bias and boost the evaluation capacity. GreedyNAS <ref type="bibr" target="#b48">(You et al., 2020)</ref> proposed to ease the training burden by encouraging to focus more on those potentially good paths instead of all paths. However, these methods require sampling many sub-networks based on the evolutionary algorithm and evaluating their performance, so the search phase is time-consuming. Instead, we introduce the attention mechanism to the super-network training, so that the search phase can be naturally integrated into the training process.</p><p>Attention Mechanism. The visual attention mechanism is inspired by the neuronal structure of the early primate visual system <ref type="bibr" target="#b16">(Itti et al., 1998)</ref>, and it began to attract lots of attention when <ref type="bibr" target="#b30">(Mnih et al., 2014)</ref> applied this mechanism to the RNN model for image classification. Afterwards, lots of works <ref type="bibr" target="#b44">(Xu et al., 2015;</ref><ref type="bibr" target="#b36">Wang et al., 2017;</ref><ref type="bibr" target="#b15">Hu et al., 2018;</ref><ref type="bibr" target="#b40">Woo et al., 2018;</ref><ref type="bibr" target="#b23">Li et al., 2019)</ref> designed different attention structures with the aim of aggregating space or channels information to improve the performance. Meanwhile, some works <ref type="bibr" target="#b29">(Luo &amp; Wu, 2020;</ref><ref type="bibr" target="#b39">Wang et al., 2019;</ref><ref type="bibr" target="#b47">Yamamoto &amp; Maeno, 2018)</ref> applied the attention mechanism to prune channels with smaller attention weights. Inspired by the fact that attention weights can indicate the channel importance, we introduce it to reflect the influence of candidate operations on the super-network with the aim of guiding the neural architecture search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminary and Limitations</head><p>In the micro search space, the cell is defined as a directed acyclic graph (DAG) with N nodes, and each edge (i, j) between every node is associated with mixed operation ō(i,j) that is parameterized as architecture parameters α (i,j) by using softmax relaxation. The differentiable architecture search can be formulated as a bi-level optimization problem as follows:</p><formula xml:id="formula_0">min α L val (w * (α), α) s.t. w * (α) = arg min w L train (w, α)<label>(1)</label></formula><p>where α (the architecture parameters) and w (the network weights) are alternately optimized on the validation and training datasets respectively.</p><p>Limitation. Whereas, the architecture parameter α may not accurately indicate how much the operation contributes to the super-network's performance as illustrated in <ref type="bibr" target="#b37">(Wang et al., 2021;</ref><ref type="bibr" target="#b43">Xie et al., 2021)</ref>. So, we argue that the architecture parameter may not be necessary and propose another alternative paradigm based on the attention mechanism.</p><p>In the macro search space, the super-network is chained by a sequence of layers that contains many candidate choice blocks. Earlier methods divide the macro search procedure into super-network training that can be expressed as:</p><formula xml:id="formula_1">w * (a) = arg min w E a∼A L train (w, a)<label>(2)</label></formula><p>After the super-network trained to convergence, then perform the sub-network searching as:</p><formula xml:id="formula_2">a * = arg max a∼A ACC val (a, w * (a))<label>(3)</label></formula><p>Limitation. Whereas, hundreds of sub-networks need to be evaluated with the evolutionary algorithm in order to discover the optimal sub-network. Instead, we do not need to sample the sub-networks because once the super-network converges, the optimal sub-network will be indicated by the attention weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Micro Search with Attention</head><p>The goal of the micro search is to determine each operation associated with each edge in a cell. For each searched cell,  the edge (i, j) is associated with all m candidate operations, that are applied to the predecessor input node i, resulting in the m output feature maps</p><formula xml:id="formula_3">F o k (k = 1, 2, ..., m). Assume that each output feature map is F o k ∈ R b×c×h×w .</formula><p>The DARTS approach integrates all m output feature maps by element-wise addition, so the feature map of the output node j is also</p><formula xml:id="formula_4">F out ∈ R b×c×h×w .</formula><p>Instead, we concatenate all m output feature maps in channel dimension before delivering to the output node j, yielding the intermediate concatenated feature map F con ∈ R b×(m * c)×h×w . To distinguish the importance of each candidate operation, we apply the attention module to the concatenated feature map F con . Specifically, as illustrated in Figure <ref type="figure" target="#fig_0">2</ref>, the concatenated feature map F con will first go through a global average pooling to get the feature map</p><formula xml:id="formula_5">F con gap ∈ R b×(m * c)×1×1 , F con gap = 1 h × w h p=1 w q=1 F con (p, q) (4)</formula><p>and then the global average pooling feature map F con gap is applied to two fully-connected layers followed by a Sigmoid layer. To reduce the complexity of the model and improve generalisation, we use a bottleneck structure with two fully-connected layers. The first FC layer is responsible for dimensionality reduction and the second FC layer for restoring the original dimensionality,</p><formula xml:id="formula_6">Atten = σ(W 2 Relu(W 1 F con gap ))<label>(5)</label></formula><p>where W 1 ∈ R c r ×c and W 2 ∈ R c× c r are the fullyconnection weights, r is the hyper-parameter coefficient of dimensionality reduction, σ(•) is the Sigmoid function whose output, Atten ∈ R b×(m * c)×1×1 , i.e., the attention weights, can be regarded as the magnitude of importance of each channel.</p><p>The attention weights here are the activation values of all channels. Therefore, the importance of the first candidate operation A 1 can be determined as the summation of the activation values of the first c channels. Similarly, the importance of each operation is expressed as the sum of the activation values of the corresponding number of channels,</p><formula xml:id="formula_7">         A 1 = c i=1 Atten(i) A 2 = 2c i=c Atten(i) . . . A m = mc i=(m−1)c Atten(i) (6)</formula><p>Next, in order to back propagate gradient and update the weights of the attention module, the learned activation values of each channel are multiplied by the original concatenated feature map F con to obtain the output feature map with each re-weighted channel</p><formula xml:id="formula_8">F con ∈ R b×(m * c)×h×w , F con = F con ⊗ Atten (7)</formula><p>Finally, the weighted feature map will be also applied the element-wise addition in the channel dimension to get the feature map F out of the output node j,</p><formula xml:id="formula_9">F out = F con [1:c] ⊕ F con [c:2c] ⊕ • • • ⊕ F con [(m−1)c:mc]<label>(8)</label></formula><p>At the end of the micro search, we forward propagate all images of the validation datasets to obtain the attention weights associated with all candidate operations on each edge in a cell. Finally, the optimal operation on each edge is selected according to the largest aggregated attention weights,</p><formula xml:id="formula_10">A * = argmax(A 1 , A 2 , • • •, A m ) (9)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Macro Search with Attention</head><p>For the macro search, we leverage the attention mechanism to search for the specific choice block at each layer of the target network. As shown in Figure <ref type="figure" target="#fig_0">2</ref>, the super-network is chained by pre-defined n choice blocks at each layer, and the attention module is plugged in the end of each layer.</p><p>The output of the previous layer will be applied to all n choice blocks of the current layer to generate n different feature maps</p><formula xml:id="formula_11">F o k (k = 1, 2, ..., n), each of which is repre- sented as F o k ∈ R b×c×h×w .</formula><p>Similar to the micro search, we firstly concatenate the n output feature maps in the channel dimension. Then the concatenated feature map F con ∈ R b×(n * c)×h×w is applied to the attention module to compute the channel attention weights by Eq.( <ref type="formula">4</ref>) and Eq.( <ref type="formula" target="#formula_6">5</ref>).</p><p>Afterwards, the channel activation weights are multiplied by the concatenated feature map F con to get the re-weighted output F con ∈ R b×(n * c)×h×w , and the element-wise addition in channel dimension is applied to the F con to obtain the output F out of this layer as in Eq.( <ref type="formula">7</ref>) and ( <ref type="formula" target="#formula_9">8</ref>).</p><p>At the end of the macro search, the importance of each block at each layer can be represented by the summation of the attention weights of the corresponding number of channels on validation datasets,</p><formula xml:id="formula_12">         A 1 = c i=1 Atten(i) A 2 = 2c i=c Atten(i) . . . A n = nc i=(n−1)c Atten(i) (10)</formula><p>where A j (j = 1, 2, ..., n) represents the importance of the j-th choice block. Finally, the optimal choice block at each layer is selected by the largest attention weights,</p><formula xml:id="formula_13">A * = argmax(A 1 , A 2 , • • •, A n ) (11)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Theoretical Analysis</head><p>The reason why attention weights can produce more meaningful and reliable results may be explained by the fact that the approach is data-dependent, so that the attention weights can vary more and better fit the data as they are different for each sample.  <ref type="formula">6</ref>) for micro search or Eq.( <ref type="formula">10</ref>) for macro search 6: return derived micro cells based on the attention weights A * by Eq.( <ref type="formula">9</ref>) or derived macro network based on the attention weights A * by Eq.( <ref type="formula">11</ref>) pooling (GAP) as the pre-processing before computing attention weights. Whereas, GAP is actually a special case of Two-Dimensional Discrete Cosine Transform (2D-DCT) as follows,</p><formula xml:id="formula_14">f 2d h,w = H−1 i=0 W −1 j=0 x 2d i,j cos( hπ H (i + 1 2 ))cos( wπ W (j + 1 2 )) f 2d 0,0 = H−1 i=0 W −1 j=0 x 2d i,j cos( 0 H (i + 1 2 ))cos( 0 W (j + 1 2 )) = H−1 i=0 W −1 j=0 x 2d i,j = GAP (x 2d )HW s.t. h ∈ {0, 1, . . . , H − 1} , w ∈ {0, 1, . . . , W − 1}<label>(12)</label></formula><p>Therefore, f 2d 0,0 represents the lowest frequency component of 2D-DCT, and it is proportional to GAP. According to F-Principle <ref type="bibr" target="#b46">(Xu et al., 2019b)</ref>, low-frequency components help to improve the generalization of networks. That is, the operation, i.e., architecture derived by attention weights can achieve better performance than that derived by architectural parameters due to the GAP pre-processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>5.1. Search Space DARTS Search Space. Following DARTS <ref type="bibr" target="#b28">(Liu et al., 2018c)</ref>, a cell is defined as a directed acyclic graph (DAG) consisting of an ordered sequence of N nodes. The input node is represented by the outputs from the previous two cells, each intermediate node aggregates information flows from all of its predecessors, and the output node is defined as a concatenation of a fixed number of its predecessors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Test Err.(%) Params(M) Search Cost Search (GPU-days) Algorithm NASNet-A <ref type="bibr" target="#b54">(Zoph et al., 2018)</ref> 2.65 3.3 1800 RL AmoebaNet-A <ref type="bibr" target="#b34">(Real et al., 2019)</ref> 3.34±0.06 3.2 3150 EA AmoebaNet-B <ref type="bibr" target="#b34">(Real et al., 2019)</ref> 2.55±0.05 2.8 3150 EA PNAS <ref type="bibr" target="#b25">(Liu et al., 2018a)</ref> 3.41±0.09 3.2 225 SMBO ENAS <ref type="bibr" target="#b32">(Pham et al., 2018)</ref> 2.89 4.6 0.5 RL DARTS (1st order) <ref type="bibr" target="#b28">(Liu et al., 2018c)</ref> 3.00±0.14 3.3 1.5 Gradient DARTS (2nd order) <ref type="bibr" target="#b28">(Liu et al., 2018c)</ref> 2.76±0.09 3.3 4 Gradient SNAS <ref type="bibr" target="#b42">(Xie et al., 2018)</ref> 2.85±0.02 2.8 1.5 Gradient GDAS <ref type="bibr" target="#b10">(Dong &amp; Yang, 2019)</ref> 2.93 3.4 0.21 Gradient BayesNAS <ref type="bibr" target="#b51">(Zhou et al., 2019)</ref> 2.81±0.04 3.4 0.2 Gradient Robust-DARTS <ref type="bibr" target="#b49">(Zela et al., 2020)</ref> 2.95±0.21 N/A 1.6 Gradient PC-DARTS <ref type="bibr" target="#b45">(Xu et al., 2019a)</ref> 2.57±0.07 3.6 0.1 Gradient DATA <ref type="bibr" target="#b4">(Chang et al., 2019)</ref> 2.59 3.4 1 Gradient SGAS(Cri.1 avg.) <ref type="bibr" target="#b22">(Li et al., 2020)</ref> 2.66±0.24 3.7 0.25 Gradient SDARTS-ADV <ref type="bibr" target="#b6">(Chen &amp; Hsieh, 2020)</ref> 2.61±0.02 The diagnostic information about accuracy, loss, and parameters is accessible on three datasets including CIFAR-10, CIFAR-100, and ImageNet-16-120.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results on CIFAR-10</head><p>We conduct the micro search by constructing a supernetwork with eight cells in the DARTS search space. Reduction cells are located at the 1/3 and 2/3 of the network, whereas the rest are normal cells. The super-network plugged by the attention module is trained on half of the CIFAR-10 ( <ref type="bibr" target="#b20">Krizhevsky et al., 2009)</ref> training datasets. In particular, we use the SGD optimizer to update network weights with initial learning rate of 0.025, momentum 0.9, and weight decay 3 × 10 −4 . We search for 50 epochs with the batch size of 64. The micro search process elapses nine hours on 1080Ti GPU with only 10 GB GPU memory.</p><p>At the end of the micro search process, we can obtain eight different cells according to the attention weights by forwarding propagation on the validation datasets. We divide the super-network into three stages based on the location of the reduction cell. And one of the normal cells is then selected at each stage to build the target network.</p><p>Finally, in the evaluating phase, the target network which consists of 20 layers with initial channel size of 36 is trained on the whole CIFAR-10 training datasets. We use an SGD optimizer with a weight decay of 3 × 10 −4 and a momentum of 0.9. The initial learning rate starts from 0.025 and follows the cosine annealing strategy to a minimum of 0. The network is trained from scratch for 600 epochs with batch size of 96.</p><p>As shown in Table <ref type="table" target="#tab_1">1</ref>, the results show that our method achieves state-of-the-art performance with the accuracy of 97.54% compared with other methods in the DARTS search space. Differently, all the previous methods stack the same searched cell to evaluate the performance. However, we build the target network using the different normal cells at different stages, and can obtain the average accuracy of 97.47%, indicating that our method can search for a stable architecture even with different seed initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods Test Err. (%) Params Search Cost Search</head><p>Top-1 Top-5 (M) (GPU-days) Algorithm MnasNet <ref type="bibr" target="#b35">(Tan et al., 2019)</ref> 26 8.2 4.2 2000 RL NASNet <ref type="bibr" target="#b54">(Zoph et al., 2018)</ref> 26.0 8.4 5.3 1800 RL AmoebaNet <ref type="bibr" target="#b34">(Real et al., 2019)</ref>.</p><p>24.3 7.6 6.4 3150 EA PNAS <ref type="bibr" target="#b25">(Liu et al., 2018a)</ref> 25.8 8.1 5.1 225 SMBO FBNet-C <ref type="bibr" target="#b41">(Wu et al., 2019)</ref> 25.1 7.9 5.5 9 Gradient ProxylessNAS(GPU) <ref type="bibr" target="#b3">(Cai et al., 2018)</ref> 24.9 7.5 7.1 8.3 Gradient SPOS <ref type="bibr" target="#b13">(Guo et al., 2020)</ref> 26.0 8.4 5.3 11 ‡ Evolution FairNAS-A <ref type="bibr" target="#b9">(Chu et al., 2021)</ref> 24.66 7.8 4.6 16 ‡ Evolution GreedyNAS-C <ref type="bibr" target="#b48">(You et al., 2020)</ref> 23.8 7.5 4.7 8 ‡ Evolution RLNAS <ref type="bibr" target="#b50">(Zhang et al., 2021)</ref> 24 Table <ref type="table">3</ref>. Search results on NAS-bench-201. We report the average performance for three independent runs of searching. "Optimal" indicates the highest accuracy for each dataset on NAS-Bench-201.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Results on ImageNet</head><p>We conduct the macro search directly on ImageNet datasets <ref type="bibr" target="#b21">(Krizhevsky et al., 2017)</ref> in the ProxylessNAS search space. Specifically, the super-network is chained by a series of blocks, each of which contains many candidate choice operations. The super-network with the added attention module is trained on 8 NVIDIA V100 GPUs on 10% of the training datasets for 50 epochs with a batch size of 64 per GPU.</p><p>After that, the optimal choice operation for each layer is determined according to the channel attention weights that are computed on the validation datasets.</p><p>The final derived network is trained from scratch on the entire ImageNet training datasets. We use the SGD optimizer with an initial learning rate of 0.5, weight decay of 4 × 10 −5 , and momentum of 0.9. The network is trained for 240 epochs with the batch size of 1024 on 8 NVIDIA V100 GPUs.</p><p>From Table <ref type="table" target="#tab_2">2</ref>, we can see that by directly searching on ImageNet within 3.3 GPU-days, AGNAS achieves Top-1 test error of 23.4% and Top-5 test error of 6.8%. The results show that the proposed method is very efficient and significantly outperforms the other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Results on NAS-Bench-201</head><p>In NAS-Bench-201 search space, we add the attention module to each edge in a cell. It is worth noting that the benchmark can only retrieve the super-network stacked by the same cell. Therefore, we average the attention weights of the edges in the same position of all cells for selecting the corresponding operations in order to obtain the final single cell. We search for 50 epochs on CIFAR-10 and then index the accuracy of the searched architecture on the three datasets CIFAR-10, CIFAR-100, and ImageNet-16-120, respectively. We keep the hyperparameters the same as DARTS, and repeat the experiment three times with different initial random seeds. The results are shown in Table <ref type="table">3</ref>. Compared to other methods, AGNAS achieves the best performance on three datasets. Furthermore, the best accuracies are all close to the global optimal, demonstrating the effectiveness of the neural architecture with the attention mechanism.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Attention Weight vs. Architecture Parameter</head><p>Here we investigate the effect of the attention weights vs. architecture parameters on the performance of the supernetwork. Specifically, we experiment AGNAS and DARTS on CIFAR-10 dataset, and randomly discretize an edge in a cell at the end of the search phase. The super-network is expected to get the highest discretization accuracy if we keep the optimal operation measured by the attention weight or alpha weights on the edge and prune the other operations. Moreover, we use the Kendall τ metric <ref type="bibr" target="#b19">(Kendall, 1938)</ref> to measure the correlation between two rankings of alpha weights or attention weights and discretization accuracy. The closer this metric is to 1, the higher the correlation between the two sequences; otherwise, the opposite is true.</p><p>With the instance of both DARTS and AGNAS discretizing the second edge in the first cell, we compare the discretization accuracy of the super-network when only one of the operations is retained on this edge. The Op 4 has the largest attention weight, which also bring the highest discretization accuracy, as demonstrated in Figure <ref type="figure" target="#fig_3">3 (a)</ref>. Nonetheless, as shown in Figure <ref type="figure" target="#fig_3">3</ref> (b), the largest alpha weight on Op 4 does not reach the highest accuracy after discretizing the edge, showing that the alpha weight cannot reflect the true strength of the operation. Furthermore, our approach achieves higher Kendall τ than DARTS as shown in Figure <ref type="figure" target="#fig_3">3 (c)</ref>, which means that attention weights better reflect operational importance than architectural parameters. More details are provided in the Appendix C. In addition, as shown in Figure <ref type="figure">5</ref>, we visualize the macro architecture directly searched in the ProxylessNAS search space on ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Limitations</head><p>Although AGNAS achieves good performance, we must acknowledge that the resource consumption is a little higher than architecture parameter-based methods (e.g., AGNAS: 9.7 GB GPU-Memory vs. DARTS: 9.4 GB GPU-Memory).</p><p>We are also exploring whether there are more appropriate mechanisms to determine the true importance of candidate operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we propose to leverage the attention mechanism to search for micro and macro architectures in one framework. We use the attention weights to measure the importance of candidate operations and choice blocks. Extensive experiments validate that the proposed AGNAS is effective and significantly outperforms state-of-the-art approaches. In the future, we will simultaneously search for micro-and macro-architecture in the AGNAS framework and apply AGNAS to other computer vision tasks such as object detection and semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>We conduct the micro search and macro search on two popular image classification datasets including CIFAR-10 and ImageNet. CIFAR-10 datasets have 50K training RGB images and 10K testing RGB images with a fixed spatial resolution of 32×32. In the search phase, we split the training datasets in half to train the super-network weights and the attention module weights, and the other half as validation datasets to select the optimal operation in micro search by forwarding propagation of all images to get the corresponding attention weights. The ILSVRC2012 ImageNet dataset contains 1.28M training and 50K validation images with 1000 object categories. In the macro search phase, we sample 10% of the training datasets to update the super-network and the attention module weights, and 2.5% training datasets as validation datasets are used to select the final choice block at each layer based on the attention weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Analysis of Collapse</head><p>DARTS introduces the architecture parameters and leverages a bi-level optimization to alternately optimize the architecture parameters and network weights. However, the optimization may prefer non-parametric operations, especially skipconnections, thus resulting in the performance collapse issue. Therefore, we investigate how the number of skip-connection for DARTS and AGNAS changes througout the search process in the NAS-Bench-201 search space.</p><p>We repeat the experiments three times with different initialized seeds, and the results are shown in Figure <ref type="figure" target="#fig_5">6</ref>. We plot the number of skip-connection per epoch together with the test accuracy in each experiment. As shown in Figure <ref type="figure" target="#fig_5">6</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Attention Weight vs. Architecture Parameter</head><p>We have discretized additional edges in different cells to illustrate that the attention weights are more consistent than architectural parameters in measuring the importance of candidate operations. As shown in Figure <ref type="figure" target="#fig_8">7</ref>-9, when AGNAS discretizes the corresponding edges in different cells, the operations with the largest attention weight all achieve the highest discretization accuracy. Whereas, DARTS with the largest alpha weight does not reach the highest accuracy after discretizing the same edge, and the ranking consistency, i.e., the Kendall τ is also inferior to AGNAS, indicating that the attention weights can better reflect the operational importance than architecture parameters.          </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure2. The main framework of the proposed AGNAS. Specifically, the goal of the micro search is to determine the optimal operation associated with each edge in a cell based on the attention module added on each edge. The macro search aims to discover the optimal choice block by the plug-in attention module on each layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Moreover, we also analyze it from the perspective of the frequency principle. Notice that we use the global average Algorithm 1 AGNAS Search Algorithm Input: Supernet N with Attention moudule M, Training data D train , Validation data D val . Output: Attention weights A, Searched model. Micro Search &amp; Macro Search 1: Build supernet N with pre-defined cells or blocks as well as the attention module M 2: while not converaged do 3: Update network weights w on training data D train by descending ∇ w L train (w) 4: end while 5: Compute attention weights A on validation data D val by Eq.(</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>The ranking of attention weights or alpha weights against discretization accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Comparison of attention weights and architecture parameters on measuring the importance of candidate operations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Figure 4. Micro cell searched on CIFAR-10 and the evaluated target network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure6. The number of skip-connection and test accuracy on CIFAR-10 as the search process in the NAS-Bench-201. We repeat each experiment three times with different initial seeds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>The ranking of attention weights or alpha weights against discretization accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Comparison of attention weights and architecture parameters on measuring the importance of candidate operations both on the 1st edge of the 2nd cell.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>The ranking of attention weights or alpha weights against discretization accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Comparison of attention weights and architecture parameters on measuring the importance of candidate operations both on the 13th edge of the 3rd cell.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>The ranking of attention weights or alpha weights against discretization accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Comparison of attention weights and architecture parameters on measuring the importance of candidate operations both on the 10th edge of the 5th cell.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Search results on CIFAR-10 and comparison with other state-of-the-art methods. Unlike other methods that only perform the micro search for identical cells, AGNAS firstly conducts the micro search to obtain the different cells and then builds the target network by stages. We report the average results for three independent runs with different initial random seeds.</figDesc><table><row><cell>3.3</cell><cell>1.3</cell><cell>Gradient</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Search results on ImageNet and comparison with other state-of-the-art methods. ‡ denotes the search cost includes the additional subnet searching with the evolutionary algorithm.</figDesc><table><row><cell>.4</cell><cell>7.4</cell><cell>5.3</cell><cell>N/A</cell><cell>Evolution</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work is supported in part by the National Key R&amp;D Program of China under Grant No. 2018AAA0102701, and in part by the National Natural Science Foundation of China under Grant No. 62176250.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02167</idno>
		<title level="m">Designing neural network architectures using reinforcement learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural optimizer search with reinforcement learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Attention enhances synaptic efficacy and the signal-to-noise ratio in neural circuits</title>
		<author>
			<persName><forename type="first">F</forename><surname>Briggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Mangun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Usrey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">499</biblScope>
			<biblScope unit="issue">7459</biblScope>
			<biblScope unit="page" from="476" to="480" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Data: architecture approximation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Searching for efficient multi-scale architectures for dense image prediction</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stabilizing differentiable architecture search via perturbation-based regularization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Backbone search for object detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><surname>Detnas</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fair darts: Eliminating unfair advantages in differentiable architecture search</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rethinking evaluation fairness of weight sharing neural architecture search</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><surname>Fairnas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Searching for a robust neural architecture in four gpu hours</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Nas-bench-201: Extending the scope of reproducible neural architecture search</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Nas-fpn: Learning scalable feature pyramid architecture for object detection</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Single path one-shot neural architecture search with uniform sampling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><surname>Mobilenets</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Efficient convolutional neural networks for mobile vision applications</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A model of saliency-based visual attention for rapid scene analysis. IEEE Transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning efficient, explainable and discriminative representations for pulmonary nodules classification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page">107825</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Nasabn: A neural architecture search framework for attention-based networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">X</forename><surname>Zugeng</surname></persName>
		</author>
		<editor>IJCNN</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A new measure of rank correlation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Kendall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1/2</biblScope>
			<biblScope unit="page" from="81" to="93" />
			<date type="published" when="1938">1938</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sgas: Sequential greedy architecture search</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">C</forename><surname>Delgadillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Selective kernel networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Darts+</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.06035</idno>
		<title level="m">Improved differentiable architecture search with early stopping</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Hierarchical representations for efficient architecture search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Darts: Differentiable architecture search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2018">2018c</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Autopruner: An end-to-end trainable filter pruning method for efficient deep model inference</title>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page">107461</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Recurrent models of visual attention</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Att-darts: Differentiable neural architecture search for attention</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nakai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Matsubara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Uehara</surname></persName>
		</author>
		<editor>IJCNN</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameters sharing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Suematsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Rethinking architecture selection in differentiable nas</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attentionnas: Spatiotemporal attention cell search for video classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A convolutional neural network pruning method based on attention mechanism</title>
		<author>
			<persName><forename type="first">X.-J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SEKE</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Convolutional block attention module</title>
		<author>
			<persName><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<author>
			<persName><surname>Cbam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fbnet: Hardwareaware efficient convnet design via differentiable neural architecture search</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Snas: stochastic neural architecture search</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Exploiting operation importance for differentiable neural architecture search</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Kung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TNNLS</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pc-darts: Partial channel connections for memory-efficient architecture search</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Z.-Q</forename><forename type="middle">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<title level="m">Frequency principle: Fourier analysis sheds light on deep neural networks. In ICML</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pruning channels with attention statistics for deep network compression</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Maeno</surname></persName>
		</author>
		<author>
			<persName><surname>Pcas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Towards fast one-shot nas with greedy supernet</title>
		<author>
			<persName><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Greedynas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Understanding and robustifying differentiable architecture search</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Marrakchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Neural architecture search with random labels</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Bayesnas: A bayesian approach for neural architecture search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Theoryinspired path-regularized differential network architecture search</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
