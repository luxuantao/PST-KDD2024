<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Genetic algorithm-optimized multi-channel convolutional neural network for stock market prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hyejung</forename><surname>Chung</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Business</orgName>
								<orgName type="institution">Ewha Womans University</orgName>
								<address>
									<addrLine>52 Ewhayeodae-gil, Seodaemun-gu</addrLine>
									<postCode>03760</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Kyung-Shik</forename><surname>Shin</surname></persName>
							<email>ksshin@ewha.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">School of Business</orgName>
								<orgName type="institution">Ewha Womans University</orgName>
								<address>
									<addrLine>52 Ewhayeodae-gil, Seodaemun-gu</addrLine>
									<postCode>03760</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Genetic algorithm-optimized multi-channel convolutional neural network for stock market prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C4271B2D2F1B7B567C07C4F3218768E3</idno>
					<idno type="DOI">10.1007/s00521-019-04236-3(</idno>
					<note type="submission">Received: 26 November 2018 / Accepted: 9 May 2019 Ó Springer-Verlag London Ltd., part of Springer Nature 2019</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T10:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Convolutional neural network</term>
					<term>Genetic algorithm</term>
					<term>Deep learning</term>
					<term>Stock market prediction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, artificial intelligence technologies have received considerable attention because of their practical applications in various fields. The key factor in this prosperity is deep learning which is inspired by the information processing in biological brains. In this study, we apply one of the representative deep learning techniques multi-channel convolutional neural networks (CNNs) to predict the fluctuation of the stock index. Furthermore, we optimize the network topology of CNN to improve the model performance. CNN has many hyper-parameters that need to be adjusted for constructing an optimal model that can learn the data patterns efficiently. In particular, we focus on the optimization of feature extraction part of CNN, because this is the most important part of the computational procedure of CNN. This study proposes a method to systematically optimize the parameters for the CNN model by using genetic algorithm (GA). To verify the effectiveness of our model, we compare the prediction result with standard artificial neural networks (ANNs) and CNN models. The experimental results show that the GA-CNN outperforms the comparative models and demonstrate the effectiveness of the hybrid approach of GA and CNN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, artificial intelligence (AI) technologies have received considerable attention because of their practical applications in various fields including computer vision and natural language processing and have become an active research area. In the history of AI, deep learning techniques such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have been considered a key factor in this prosperity. Deep learning, also called a deep neural network, is a type of artificial neural networks (ANNs) that has many hidden layers between the input and the output layers. Deep learning not only increases the complexity of the model but also assigns an appropriate role to each neuron for the purpose of analysis; therefore, it can effectively solve the target problems that could not be solved using traditional neural network models <ref type="bibr" target="#b0">[1]</ref>.</p><p>Deep Learning has recently attracted considerable attention with the huge success of the deep learning-based image classification model, AlexNet, which won the Ima-geNet large-scale visual recognition challenge (ILSVRC) in 2012 <ref type="bibr" target="#b1">[2]</ref>. AlexNet considerably improved the classification accuracy of object recognition and proved the potential of deep learning techniques. Considering the huge success of AlexNet, the deep learning models started to have deeper and more complicated architecture. In addition to the advancement of learning capability, deep learning is expanding its application range to various fields.</p><p>In this study, we apply multi-channel CNN model to predict the fluctuation of the stock index. As interest in financial investment has increased because of the continuous economic recession and low interest rates environment, there has been a steady attempt to apply AI techniques to financial analysis. Despite the fact that many studies have presented various methodologies, the stock market has been considered a very difficult area to predict because of its extreme noise and nonlinear characteristics. Recently, CNN has been applied to various time-series problems of a diverse nature, such as voice recognition and natural language processing, and many studies have demonstrated its efficiency on time-series data <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. As CNN has the advantage of extracting the local features of the data, it can be an effective prediction methodology for the time-series problems by capturing the temporal property of the given dataset.</p><p>Furthermore, in this study, we aim to optimize the network topology of CNN to improve the model performance. Although many studies have shown effectiveness of CNN models in various classification problems, there are several shortcomings in building and using the model. Despite its predictive power, neural network models require longer training time and extra resources as well as careful tuning of various hyper-parameters to reach its full potential <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>. In fact, the main drawback of CNN is its difficulty to determine a suitable model that can successfully reflect the characteristics of the problem and efficiently learn the data patterns, due to the numerous learning methods, network architectures, and other controlling parameters <ref type="bibr" target="#b7">[8]</ref>. In particular, the architectural factors including kernel size of the convolutional layers, the number of kernels, and the pooling window size are very important, because these parameters directly affect the results of the feature extraction, which is a crucial part of the computational procedure of CNN. However, most of the existing studies tend to depend on trial-and-error-based methods, and such methods are more of an art than a science. Therefore, here, we propose a method to systematically optimize the parameters for the CNN model by using genetic algorithm (GA). We employ GA in the search for CNN's optimal architecture which allows to make use of neural networks' ability to model complex nonlinear functions while automatically solving the optimization problem <ref type="bibr" target="#b6">[7]</ref>. Many studies have applied GA in conjunction with other AI and machine learning techniques such as ANN <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>, support vector machine (SVM) <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref>, and case-based reasoning (CBR) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. However, few studies have attempted to integrate GA and CNN, despite a great potential for effective applications in this area.</p><p>This paper consists of seven sections. In Sect. 1, we describe the background and the purpose of the research, and in Sect. 2, we examine the research trends of stock market forecasting and the recent research on the CNN. In Sect. 3, we introduce the main methodology used in this study, namely, CNN and GA. In Sect. 4, we propose a CNN model optimized by GA. In Sect. 5, we explain the experimental design that used in the study. Section 6 presents the experimental results and analysis. Finally, in Sect. 7, conclusions and future research directions are discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Stock market prediction</head><p>The stock market is known to be very difficult to predict and analyze because of the nature of its noisy and nonlinear environment <ref type="bibr" target="#b19">[20]</ref>. According to the efficient market hypothesis (EMH), the price of the capital market immediately reflects all the available information; therefore, forecasting and achieving excess returns are impossible <ref type="bibr" target="#b20">[21]</ref>. However, stock market forecasting is treated as an important research topic in various academic and practical fields such as mathematics, statistics, and financial engineering, and the results of empirical studies have demonstrated the possibility of stock market prediction <ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref>.</p><p>The methodology of stock market forecasting is divided into fundamental analysis and technical analysis <ref type="bibr" target="#b19">[20]</ref>. The fundamental analysis, on one hand, includes qualitative and quantitative analysis as a way to predict future stock prices by analyzing the intrinsic value of stocks. Qualitative analysis is based on factors that cannot be quantified, such as political situations, economy, and labor issues. Quantitative analysis is based on factors that can be quantified, such as economic indicators and financial statements <ref type="bibr" target="#b24">[25]</ref>. On the other hand, technical analysis aims at predicting future market trends on the basis of the historical stock market data, such as past stock prices and trading volumes, without considering the external indicators <ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref>.</p><p>Traditionally, stock market analysis adopted a variety of statistical approaches including exponential smoothing (ES), autoregressive integrated moving average (ARIMA), autoregressive conditional heteroscedasticity (ARCH) and generalized autoregressive conditional heteroscedasticity (GARCH) <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref>. However, stock prediction systems using such methods with various statistical assumptions did not achieve satisfactory results, because of the difficulty in meeting statistical assumptions such as linearity and normality postulates and independence among input variables <ref type="bibr" target="#b33">[34]</ref>.</p><p>Because financial markets are considered as chaotic and nonparametric dynamic systems <ref type="bibr" target="#b19">[20]</ref>, it is essential to develop more flexible models which can learn complex dimensionality. AI and machine learning approaches are highly capable of capturing nonlinear data patterns without the need of any background knowledge about the data sets <ref type="bibr" target="#b34">[35]</ref>. Accordingly, AI and machine learning models started to be applied to the financial time-series forecasting problems approving better ability to investigate links between market elements <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref>.</p><p>Among the various machine learning techniques, ANN and SVM are the most widely used for the stock market prediction. Saad et al. <ref type="bibr" target="#b39">[40]</ref> investigated the effectiveness of three neural network models, namely probabilistic neural network (PNN), time-delay neural network (TDNN), and RNN for low false alarm stock trend predictions. They used different predictability analysis techniques including phase diagrams, correlation dimension and Lyapunov exponent and analyzed the neural network results based on a history of the daily closing price. Consequently, the RNN outperformed other considered models. Fernandez-Rodrıguez et al. <ref type="bibr" target="#b35">[36]</ref> proposed a technical trading rule based on ANN. To investigate the profitability of this investment strategy, the proposed model was applied to the general index of the Madrid stock market. The experimental results demonstrated the superiority of the ANN-based technical trading rule for a bear market and a stable market. Some applications of SVM to stock market prediction also have been reported with excellent generalization ability. Huang et al. <ref type="bibr" target="#b21">[22]</ref> used the SVM to predict the weekly volatility of the Nikkei 225 stock index futures. They used macroeconomic indicators including interest rate and consumer price index as the input variables and compared the proposed model with statistical methods such as linear discriminant analysis (LDA) and quadratic discriminant analysis (QDA) along with the Elman backpropagation neural network (EBNN) to prove the effectiveness of the SVM. Yu et al. <ref type="bibr" target="#b40">[41]</ref> developed SVM-based stock selection system. To obtain informative and low-dimensional financial time-series, principal component analysis (PCA) was employed for the feature selection. The experimental results presented that PCA-SVM achieved higher returns of stocks than other benchmark models.</p><p>Some researchers hybridized several different AI techniques to improve the prediction accuracy <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref>. In particular, many researchers have integrated GA and other AI techniques to enhance the effectiveness of the forecasting procedure. GA has shown good performance in exploring a complex search space, guided by evolution operators such as crossover and mutation. Kim and Han <ref type="bibr" target="#b25">[26]</ref> combined the ANN model with GA for feature discretization and optimization of the connection weights between the layers in the ANN. The proposed model was applied to the prediction of Korea composite stock price index 200 <ref type="bibr">(KOSPI 200)</ref>. The hybrid ANN model with globally searched discretization and connection weights outperformed the conventional back-propagation neural network. Armano et al. <ref type="bibr" target="#b26">[27]</ref> integrated ANN and GA to develop a stock market forecasting system by experimenting on the data of the COMIT (Banca Commerciale Italiana) and the S&amp;P500 stock index. The forecasting system was composed of genetic and neural components, and each part of the model contains different information. The genetic component dealt with inputs to encode information retrieved from a technical analysis, whereas the neural part dealt with the past stock prices. The simulation results demonstrated the superiority of the proposed model and repeatedly outperformed the buy-and-hold strategy. Kim et al. <ref type="bibr" target="#b43">[44]</ref> used the GA-based multiple classifier model to predict the KOSPI. They combined the results of multiple classifiers developed by human experts, users, and machine learning techniques through GA. The experimental results of the proposed model were compared with well-known combining algorithms such as the majority vote (MV), the weighted majority vote (WMV), the Borda count (BC), the weighted Borda count (WBC), the Bayesian method, the behavior-knowledge space (BKS), and the Dempster-Shafer theory. The results showed that the GA-based multiple classifier outperformed the individual classifiers and the multiple classifiers that exploited the conventional combining method.</p><p>In recent years, with considerable achievements in various prediction tasks, deep learning techniques have been applied in the financial field. Heaton et al. <ref type="bibr" target="#b44">[45]</ref> used the deep neural network for financial prediction and classification problems. They autoencoded all the stocks of the NASDAQ biotechnology index (IBB) over the period 2012-2016 and ranked them by the similarity to their own autoencoded version. The proposed model demonstrated that the deep learning method could dramatically improve the prediction performance of the financial market. Chong et al. <ref type="bibr" target="#b28">[29]</ref> proposed the deep feature learning-based stock market prediction model. They predicted future market trends with a deep neural network and compared the effectiveness of unsupervised feature extraction techniques including autoencoder, restricted Boltzmann machine (RBM), and PCA. To evaluate the performance of each method, they used normalized mean squared error (NMSE), root-mean-squared error (RMSE), mean absolute error (MAE), and mutual information (MI). Fischer and Krauss <ref type="bibr" target="#b45">[46]</ref> used long short-term memory (LSTM) networks to forecast the volatility of the stocks listed on S&amp;P 500 from 1992 to 2015. They demonstrated that LSTM networks outperform the traditional approaches that do not make use of past information such as random forests and logistic regression classifiers. Further, they found one common pattern among the stocks and formalized a rulebased reversal strategy.</p><p>Deep learning techniques have shown outstanding performance in various fields, but application to the financial area has not been studied much thus far. The limitation of these studies is that they are restricted to only the basic deep learning techniques and do not extend to the novel models. Moreover, some of these studies did not achieve outstanding performance partly because many researchers still do not seriously consider the adequate network design of deep learning models for stock prediction. Table <ref type="table" target="#tab_0">1</ref> presents a summary of relevant studies that apply machine learning approaches to stock market forecasting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Recent research trend of CNN</head><p>CNN is a neural network model that mimics the visual processing in the human brain and is usually applied to image classification problems in the field of computer vision. The research that generated the interest in CNN was the handwriting recognition study of LeCun et al. <ref type="bibr" target="#b46">[47]</ref>. LeCun et al. <ref type="bibr" target="#b46">[47]</ref> applied LeNet-5, a modern form of CNN, to the handwritten data of mixed national institute of standards and technology (MNIST) and compared the experimental results with other machine learning techniques such as the k-nearest neighbor (k-NN) and SVM. CNN began to be used for image classification tasks in earnest when ''AlexNet'' of Krizhevsky et al. <ref type="bibr" target="#b1">[2]</ref> won the ILSVRC, which is a global visual recognition competition. Krizhevsky et al. <ref type="bibr" target="#b1">[2]</ref> proposed a CNN model similar to LeNet-5 but with more hidden layers. With the huge success of AlexNet, many researchers started to develop deeper and more complex neural network models for image recognition and classification tasks such as VGGNet <ref type="bibr" target="#b47">[48]</ref>, GoogleNet <ref type="bibr" target="#b48">[49]</ref> and ResNet <ref type="bibr" target="#b49">[50]</ref>.</p><p>Furthermore, CNN has achieved good results in timeseries problems. When convolution kernels that share the same weights are applied to local signals at different time segments, a type of translation invariance is obtained. Since the temporal location of an event has a significant effect on the outcomes of the time-series analysis, CNN can efficiently identify the periodic patterns while decreasing the unnecessary operation with the convolution kernels. Furthermore, significant information can be extracted through the pooling layers. When applying the CNN to the time-series data, usually one-dimensional (1D) kernels are used instead of the conventional two-dimensional (2D) or three-dimensional (3D) kernels to perform the convolution operations.</p><p>Kim <ref type="bibr" target="#b2">[3]</ref> performed a sentence-level sentiment analysis using CNN with pre-trained word vectors including word2vec. Each channel of CNN was composed of different word embedding methods, and the kernel size was varied to extract the features from different angles. The proposed model showed better performance than baseline models such as a model with a randomly initialized word vector and the single-channel CNN.</p><p>Zheng et al. <ref type="bibr" target="#b3">[4]</ref> conducted experiments on multivariate time-series data sets that belong to different domains by using a multi-channel CNN. Each channel of the CNN took a single dimension of the multivariate time-series as the input and learned the features individually. The proposed model combined the features that were learned from an individual channel and fed them into the fully connected layer to perform the classification. The experimental results showed that the proposed multi-channel CNN model outperformed the nearest neighbor classification (particularly 1-NN) combined with dynamic time warping (DTW) and ANN.</p><p>Although CNN has achieved satisfactory results in various fields, there are only a few studies on the optimization of the models' hyper-parameters. There are various hyper-parameters in the CNN that can affect the performance of the model, such as the kernel size, the number of kernels, and so on. However, most of the existing studies designed neural network models based on the decision of experts or experimental methods instead of systematic methods. <ref type="bibr" target="#b47">[48]</ref> used a number of small kernels, such as 3 9 3 and 1 9 1, to demonstrate the effectiveness of small kernels. They compared the experimental results with those of the CNN models with relatively larger kernel size, such as 11 9 11 and 7 9 7. They achieved outstanding results through these small kernels and insisted that the small kernels could augment the nonlinearity and increase the learning effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simonyan and Zisserman</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Hyper-parameter optimization of neural network models</head><p>Hyper-parameters of neural network models, such as number and size of hidden layers, activation thresholds, learning rate, dropout rate and so on must be defined before actual training of the model. There are many methods for hyper-parameter tuning of neural network models, such as Bayesian optimization-based methods <ref type="bibr" target="#b50">[51]</ref>, gradient search <ref type="bibr" target="#b51">[52]</ref>, grid search and random search <ref type="bibr" target="#b52">[53]</ref>. In addition to those approaches, various metaheuristic algorithms such as GA, particle swarm optimization (PSO) and tabu search have also been applied to hyper-parameter optimization problems in different applications. Jaddi et al. <ref type="bibr" target="#b53">[54]</ref> proposed the combined method GA and ANN which allows the exploration of different number of nodes for each layer, number of hidden layers, weights and biases. Tian et al. <ref type="bibr" target="#b54">[55]</ref> proposed a constraint-based GA to find optimal architectures of two hidden layers for detecting loss of a nuclear power plant's coolant accident. The GA used in this study created an initial population of architectural factors of neural networks by using a proposed constraint satisfaction algorithm; random walk. The experimental results demonstrated that the GA-optimized ANN outperformed random search, exhaustive search and a support vector regression (SVR) in terms of generalization performance. Ciancio et al. <ref type="bibr" target="#b55">[56]</ref> compared the performances of four optimization techniques; GA, tabu search, Taguchi and decision trees. They investigated the optimal architectures of ANN for manufacturing problems such as extrusion process, the rolling process and the shearing process. For each problem, the four algorithms searched for the optimal value of the number of hidden layers, the number of neurons of each hidden layer, the types of the activation functions and the type of the training algorithm.</p><p>As the result, GA outperformed other methods when applied to the problems of extrusion and shearing processes.</p><p>Although many studies have attempted to apply GA for the designing of ANN architecture, literature remains limited for the CNN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Convolutional neural network (CNN)</head><p>CNN is a subtype of ANN for processing data with a gridlike structure, such as images, which are 2D grids of pixels, and time-series data, which are 1D grids taking samples at regular time intervals. The basic mechanism of CNN is inspired by the organization of the visual cortex of animals. CNN is composed of one or more convolutional layers and fully connected layers; additionally, pooling layers can be used. The conceptual schematic representation of the basic CNN structure is depicted in Fig. <ref type="figure">1</ref>.</p><p>CNN is based on three major concepts that help to efficiently learn important patterns from the input data: local connectivity, weight sharing, and sub-sampling <ref type="bibr" target="#b56">[57]</ref>.</p><p>In traditional ANN models, neurons in the lower layer are connected to all the neurons in the upper layer, and the relationship between the neurons is derived using matrix multiplication <ref type="bibr" target="#b57">[58]</ref>. The local connectivity of CNN means that the neurons in the lower layer are connected only to the local receptive field of the higher layer; the other part is not included in the operation, thus reducing the computational complexity with a sparse interaction where the convolution kernels move over the entire input data <ref type="bibr" target="#b57">[58]</ref>. Figure <ref type="figure">2</ref> is an example of local connectivity in CNN.</p><p>Like other ANN models, CNN has weights and biases, which are updated during the learning process. However, CNN applies the same weights and biases to all the neurons in the corresponding layer. This makes it possible to detect the same features in different areas of the input data, and the model shows excellent learning performance with fewer parameters than other neural network models <ref type="bibr" target="#b0">[1]</ref>. The CNN topology uses the spatial relationships between neurons to reduce the number of parameters for the learning process, and the performance is therefore improved by using the standard back-propagation algorithms <ref type="bibr" target="#b57">[58]</ref>.</p><p>Sub-sampling refers to the process of extracting a sample after separating each layer. By reducing the number of features, we can reduce the computational complexity and avoid overfitting.</p><p>The recent developments in CNN have increased the fitting capacity considerably. As shown in Fig. <ref type="figure">1</ref>, CNN is a neural network model with multiple hidden layers consisting of two different types of layers, namely convolutional and pooling layers. The main computational layers of CNN and their operating process are as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Convolutional layer</head><p>In general, convolutional layers extract features that have local correlations when the positional relationships between the local features are determined. In the convolutional layer, the kernel (or filter) extracts features while moving on the input data at regular intervals. As shown in Fig. <ref type="figure">3</ref>, the corresponding elements in the kernel and the input data are multiplied and the results are summed. The elements constituting the kernel applied to the multiplication operation correspond to the weight matrix of the conventional ANN. The trainable kernels at all the possible offsets are convolved with input data and produce feature maps in the convolutional layer. The training process of a convolutional layer is resembling that for a conventional ANN using back-propagation. However, unlike standard ANN, CNN only needs to train the kernels of each convolutional layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Pooling layer</head><p>In the pooling layer, the samples of the most representative features are extracted from the convolutional layer. The sampling methods include max-pooling and average pooling. Sampling is conducted by extracting the maximum or the average value of each interval. The pooling procedure can remove the unnecessary detailed information, thereby ensuring that only the important information is selected for the learning procedure. Through this process, the influence of the noise data can be reduced and better robustness is gained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Fully connected layer</head><p>The fully connected layer links the neurons generated in the convolutional layer and the pooling layer to all the neurons of the higher layer and performs the prediction and classification operations. It has the same structure and performs the same role as those of the standard ANN models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Genetic algorithm (GA)</head><p>GA is a parallel and global search algorithm inspired by Darwin's theory of ''survival of the fittest'' <ref type="bibr" target="#b58">[59]</ref>. GA can effectively solve complex problems by simulating the process of natural evolution. In general, if the problem is too computationally complicated to be solved, GA can be used as a method to obtain a solution close to the optimal one. GA can be characterized by the genetic operators that imitate the biological process of reproduction. The chromosomes which are possible solutions to a computational problem are generated randomly, and the chromosomes with higher fitness get better opportunities to be reproduced <ref type="bibr" target="#b26">[27]</ref>. GA increasingly produces better solutions by gradually transforming the chromosomes that are the candidate solutions to the given problem. In this process, the solution can be expressed as a gene, and the process of creating a better solution by transformation can be understood as evolution.</p><p>The genetic representation and the definition of the fitness function are the two major requirements of a GA process. Genetic representation is a way to express the potential solutions (individuals) of the target problem. It is important because the result of the genetic representation affects all GA operations, such as crossover and mutation; thus, it should be able to reflect the nature of the target problem well. The fitness function is a type of objective function used to guide simulations toward optimal design solutions. This function measures the performance of each chromosome, and through this evaluation process, GA finds the optimal solution for the problem <ref type="bibr" target="#b59">[60]</ref>.</p><p>Six stages are considered in GA: initialization, fitness calculation, selection, crossover, mutation, and termination condition check <ref type="bibr" target="#b60">[61]</ref>, as shown in Fig. <ref type="figure" target="#fig_1">4</ref>.</p><p>In the initialization stage, a GA randomly initializes a population of candidate solutions that will perform the genetic operations. The size of the population usually depends on the characteristics of the target problem and ranges to several hundreds or thousands of possible solutions.</p><p>Once the population is initialized, the fitness of each chromosome is calculated in every generation. The predefined fitness function is used to select good solutions <ref type="bibr" target="#b59">[60]</ref>. In the GA process, the determination of the fitness function is crucial step, because it can seriously affect the performance of model. If the definition of the fitness function is inappropriate for the target problem, GA can converge on the wrong solution.</p><p>Selection allocates reproductive opportunities to each chromosome. In the process of calculating the fitness of each solution, only solutions with higher fitness values are selected for the recombination process. The main idea of selection is to choose better solutions and let superior ones pass their genes to the next generation.</p><p>Crossover is the most important phase in GA operation. The selected chromosomes generate offspring by combining genetic information of two parents. Two of the chosen chromosomes swap the corresponding parts of the string and change the gene combinations to create new candidate solutions. Figure <ref type="figure">5</ref> illustrates the process of crossover operation.</p><p>In the mutation process, randomly chosen chromosomes flip the individual bits as shown in Fig. <ref type="figure">6</ref>. The crossover process is limited in that completely new information cannot be generated. However, this limitation can be overcome by the mutation operation, because it alters each bit of a chromosome to a completely new value. The aim of this process is to maintain the diversity in the population and prevent the local optimum problem.</p><p>The newly produced chromosomes through the generational process are evaluated to check if the termination condition has been reached. The GA process is over when the population has converged. However, the whole process is iterated until at least one of the termination conditions is satisfied. The general termination conditions are as follows <ref type="bibr" target="#b53">[54]</ref><ref type="bibr" target="#b54">[55]</ref><ref type="bibr" target="#b55">[56]</ref>:</p><p>• A solution which meets the minimum criteria is derived.</p><p>• The predefined number of generations is reached.</p><p>• A successive state that no longer produces better results is reached.</p><p>4 GA-optimized multi-channel CNN model</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Financial time-series analysis using multichannel deep CNN</head><p>The term ''time-series'' refers to the series of sequential data points according to the flow of time. As the temporal changes of these data points are caused by various reasons, the prediction and utilization of time-series data are intricate. Despite these difficulties, a time-series analysis is significant because of the many opportunities of its application in a broad range of real-world scenarios. In fact, multivariate time-series can provide more information and patterns of the same underlying phenomena and help improve the performance of prediction models when compared to univariate time-series. Therefore, we use multivariate financial time-series data to predict the For the purpose of local processing of input data, the input spectrum is divided into the size of local kernels. As depicted in Fig. <ref type="figure">7</ref>, the predefined size of kernel sweeps over the input signal with shared weights and each kernel learns the local temporal information. After the convolution operation, the learned features go through a nonlinear activation function. The output values from the convolutional layer which are called as feature maps are the input of the pooling layer. The feature maps are pooled by means of max-pooling or average pooling to reduce the features dimension and improve computational efficiency. Several authors have stipulated that max-pooling shows a better performance than average pooling while allowing faster convergence by selecting superior invariant features and avoiding the overfitting phenomena <ref type="bibr" target="#b61">[62]</ref>. Hence, we adopted the max-pooling method for further improvement of the prediction performance.</p><p>Stage 2 At the end of feature extraction, we concatenate the trainable fully connected part to perform further classification. These fully connected layers which are connected to the two output classes (up/down trend) are anticipated to combine local information extracted from the lower layers. The feature maps of each channel are flattened and combined as the input of fully connected layers. Then the fully connected layers map the features from convolutional and pooling operations to target labels. In other words, the classification is conducted based on the learned features from the convolutional and pooling layers. For each feature, the weight is given through the training process and finally the total score for target classes is calculated. The input data is considered to belong to the class with the higher score.</p><p>In this study, the CNN model includes 7-channel inputs. For each channel, convolution kernels that extract the features move over the univariate time-series and learn hierarchical features. The CNN model of this study utilizes 2 convolutional layers and 1 pooling layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Optimizing the architecture of multi-channel CNN with evolutionary algorithm</head><p>There has been a dramatic improvement in the use of deep learning techniques in various areas of research. Despite the fact that deep learning can learn features and optimize weight parameters in a data-driven manner, the selection of the model architecture still remains a manual process in a highly intuitive manner. The network topology for the input and the output variables and the learning parameters are examples of the part that require expert intervention for tuning and adjustment. However, as the total number of network structures increases exponentially with the increase in the network's depth, it is impossible to evaluate all the possible candidates to discover the optimal set of components. To overcome this cumbersome process and retain the flexibility of the model, we propose a novel approach by using the GA to find the optimal topology of the CNN model automatically.</p><p>CNN is generally composed of two parts. One is the feature extraction part, which contains the convolutional and pooling layers and the other is the trainable fully connected part for the prediction or classification. In this study, we specifically focus on generating the optimal topology in the feature extraction part. The convolutional and pooling layers of the CNN detect the patterns and extract the key features of the given input as the kernels move over the original data. Thus, determining the appropriate design for these layers can result in considerable performance improvements. Optimum subset of such hyper-parameters may vary with the dynamics of the input signal, so these parameters are best determined with a systematic approach. In the case of the size of kernel, if the kernel size is very large, the detailed characteristics of the input data cannot be taken into consideration, and a very small kernel may cause confusion by learning too much information. Moreover, the number of kernels for each convolutional layer can affect the process of feature learning. Every kernel produces different feature maps and acts as a feature detector with a different point of view. As the number of kernels increases, the perspective of analyzing input data varies. However, rather than simply increasing the number of kernels, it is necessary to find an optimal value that can learn features from the input data well and reduce the computational complexity. In most studies that use CNN, the network structure is chosen based on the empirical performance rather the theoretical justifications.</p><p>In this study, we employ GA to investigate the optimal structure of a multi-channel CNN model to improve the performance of stock market prediction. Every architectural factor can affect the performance of the network; therefore, a simultaneous adjustment of all of the parameters has to be conducted for the finding of the CNN's optimal structure. The search space associated with the deep neural network designing problem is very large and complex in nature. We employed GA, because its robustness has been theoretically and empirically demonstrated as well as its ability to efficiently solve the large combinational problems. GA has several advantages over other conventional search algorithms in the context of optimization of network design. First, GA is different from other search methods since it deals with population of coded solutions, possessing a global search function and fitting a large-scale parallel process. These characteristics reduce the chance of GA converging toward a local minimum compared to other optimization techniques as it simultaneously considers many points in the search space. In contrast to the single solution-based algorithms such as tabu search and simulated annealing, the GA has excellent global search ability since it is a population-based algorithm <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b62">63]</ref>. Second, GA detects a set of solutions of a network component which have a minimum cost. These components may correspond to quite different designs that can be then compared in terms of other important but nonquantifiable objectives <ref type="bibr" target="#b9">[10]</ref>. Lastly, GA only uses a fitness function, while more conventional search methods need existence and continuity of derivatives or other auxiliary information <ref type="bibr" target="#b63">[64]</ref>.</p><p>The search process of GA consists of the following steps:</p><p>Step 1 The search process of GA begins by creating an initial set of possible solutions. The most important task in using the GA is to express the potential solution of the problem to be solved by the chromosomes and to establish the measurement standard for the performance of each chromosome. As the expression of the potential solution affects all the genetic operations, each chromosome should appropriately reflect the characteristics of the target problem.</p><p>As mentioned above, in this study, we optimize the number of kernels, the kernel size, and the pooling window size applied to the operation of the convolutional and pooling layer through GA; therefore, the size of the components of each layer is encoded in binary strings. As shown in Fig. <ref type="figure" target="#fig_3">8</ref>, candidate solutions constitute the population of GA. Each chromosome contains the potential value of number of kernels for each convolutional layer, size of kernels for each channel, and pooling window size. That is, various combinations of the candidate structures of the multi-channel CNN compose the population of the solutions.</p><p>Step 2 After the population initialization step, the performance of each chromosome is measured by a predefined fitness function. In this study, the fitness function consists of the total classification accuracy of the stock market movement (up/down trends). According to the fitness score, the chromosomes that achieve higher classification accuracy will be reproduced more often than those with lower accuracy.</p><p>Step 3 Once the fitness scores for the whole population have been calculated, the genetic operators of selection, crossover and mutation are applied in order to produce the new population of the next generation. The genetic operators provide new information into the population. Through these operations, GA tends to converge on optimal or nearoptimal solutions. Finally, the ultimate subset of hyperparameters for multi-channel CNN is obtained based on the GA search. We illustrate the simplified architecture of the proposed multi-channel CNN model in Fig. <ref type="figure" target="#fig_4">9</ref> for easier presentation.</p><p>5 Experimental design</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data and variables</head><p>In this study, 17-year data of the daily KOSPI from January 4, 2000, to December 31, 2016, are used as the experimental data, and the dataset is collected from Bloomberg. The raw data includes the low price, high price, opening price, closing price, and trading volume of 4203 trading days in total. The first 80% of the data is used as the training set, 20% of training data is chosen as validation set, and the remaining 20% is used as the holdout set. The summary statistics of the data components is presented in Table <ref type="table" target="#tab_1">2</ref>.</p><p>As input variables significantly affect the performance of the model, careful consideration should be given to the selection. In this study, seven technical indicators are selected as the input variables. Fund managers and traders capture important market signals through technical indicators, and many studies related to stock market prediction have proven the empirical effectiveness of these technical indicators. For the empirical experiment, we referred to the earlier literature <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b64">65]</ref>. The selected technical indicators and formulas are shown in Table <ref type="table">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Model development</head><p>In this study, we employ a multi-channel CNN model consisting of seven channels of input variables, two filter layers, one pooling layer, and two fully connected layers. The inputs of the proposed model are multiple 1D subsequences which are separated from the multivariate timeseries. The feature learning is conducted on individual univariate time-series and we concatenate fully connected layers at the end of feature learning to perform the classification. The output variable is the movement direction of the stock index of the following day, and the output values are represented in the range of 0 and 1. As the range of the values of raw data varies widely, the input data are linearly scaled to the value between 0 and 1, acquiring the normalized multi-dimensional time-series. This data scaling process increases the efficiency of training procedure of neural network model. The linearly scaled value of x 0 can be expressed as follows:</p><formula xml:id="formula_0">x 0 ¼ x À minðxÞ maxðxÞ À minðxÞ<label>ð1Þ</label></formula><p>where minðxÞ and maxðxÞ are, respectively, the minimum and the maximum values of the sample x.</p><p>In contrast to image classification, we exploit the 1D kernels to extract the local temporal information and the size and the number of the kernels are searched by the GA. In the fully connected layers, we use 200 and 40 hidden nodes, respectively. We implement a rectified linear unit (ReLU) as the activation function of all the layers except the output layer. ReLU is a nonlinear function that outputs x if the input value x is positive and 0 otherwise. The ReLU function was proposed to solve the gradient vanishing problem because traditional sigmoidal functions make the back-propagated errors converge to zero with the increase in the number of layers of the neural network. The ReLU function with input x can be expressed as follows:  To perform the binary classification, the output layer adopts the logistic sigmoid as the activation function. The sigmoid function can be defined as follows:</p><formula xml:id="formula_1">f ðxÞ ¼ 1 1 þ e Àx<label>ð3Þ</label></formula><p>Moreover, the adaptive moment estimation (Adam) optimizer is used for weight parameter learning, whose effectiveness is proven by applications to various deep learning models. Adam is known as an efficient weight optimizer, particularly for models with nonstationary and noisy data <ref type="bibr" target="#b65">[66]</ref>.</p><p>To perform the genetic search, we organize a population of chromosomes with the encoded bits of the candidate subset of the parameters of CNN. The encoded parameters and the corresponding chromosome values used in this study are described below. GA has many controlling parameters to adjust in addition to determining the fitness functions and the problem representation. There has been much debate regarding the optimal controlling parameters that should be specified for the experiment. For this experiment, we set the controlling parameters by referring to previous studies that applied GA to financial problems <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b59">60]</ref>. In this study, the crossover rate is set to 0.7, and the mutation rate is set to 0.25. The detailed information of the GA parameters is provided in Table <ref type="table" target="#tab_2">4</ref>.</p><p>Python version 3.6.1, which is a scientific programming language, and the deep learning framework Keras version 2.0.4 are used to build the proposed optimized CNN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Result and analysis</head><p>This study proposes an integrated deep CNN model to predict KOSPI index movement by deriving the optimal network structure using GA. Table <ref type="table" target="#tab_3">5</ref> shows the optimal CNN hyper-parameters investigated by GA.</p><p>When it comes to stock market prediction, each technical indicator has a different effective time window for the prediction, an aspect considered by only a few studies. However, the proposed model could, first, reflect the effective temporal properties of each input variable for stock market prediction through the individual GA optimization of each channel, and second, enable the finding of the best time window for each technical indicator.</p><p>To verify the effectiveness of the proposed model, we compare its performance to standard ANN and CNN models without optimization. We employed accuracy as the performance measure of the classification results. The accuracy evaluates the ability of a classifier to provide a level of accurate diagnosis. The formula of accuracy is as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accuracy</head><formula xml:id="formula_2">¼ TP þ TN TP þ TN þ FP þ FN % ð Þ<label>ð4Þ</label></formula><p>where TP = true positives, TN = true negatives, FP = false positives and FN = false negatives. Table <ref type="table" target="#tab_4">6</ref> compares the proposed model against the benchmark models (Fig. <ref type="figure" target="#fig_6">10</ref>).</p><p>The ANN model is organized with one hidden layer containing 10 hidden neurons. The number of hidden neurons is determined by repeated experiments with values between 2 and 12 and selected based on the performance of the validation set. Experimental ANN prediction accuracy is 60.67% and 58.62% for training and holdout data, respectively, which is lower than other CNN models. As presented in Fig. <ref type="figure" target="#fig_7">11</ref>, ANN model could not get high performance during training process due to ANN limitations such as its inability to reflect the temporal properties of stock market data when applied on its own. For ease of comparison, the basic CNN model has the same structure as the proposed model except the optimization structure. The comparative basic CNN model employs 1 9 5 kernels in each convolutional layer, which is commonly exploited in current time-series classification study using CNN <ref type="bibr" target="#b3">[4]</ref>. Prediction accuracy for the standard CNN model is 71.69% and 70.16% for the training and holdout set, respectively, which is significantly higher than the basic ANN model. We believe that the ability to monitor the temporal properties and flexibility of CNN can provide strong support in stock market prediction problem dealing with complicated and rich information in financial time-series data. The training and validation accuracy per epoch in standard CNN model is depicted in Fig. <ref type="figure" target="#fig_0">12</ref>. Finally, prediction accuracy for our proposed model presented 75.95% and 73.74% for the training and holdout data. Moreover, as shown in Fig. <ref type="figure">13</ref>, the integrated GA-CNN approach converged faster and got higher performance than other comparative models. These results may be caused by the fact that the globally investigated CNN architecture increases the computational efficiency of the model. Thus, we can conclude that the simultaneous  The predictive performance of proposed model shows statistically significant difference at the 1% significance level for both the ANN and standard CNN model, i.e., the proposed model performs significantly better than comparative models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>Stock market prediction has been widely studied in various fields, such as economics, mathematics, and statistics due to its practical importance. However, current statistical techniques and econometric models are limited to reflect nonlinear characteristics of financial time-series data. This study predicts KOSPI fluctuations using CNN, a popular deep learning technique that can learn complex correlation between data. However, various hyper-parameters within the CNN must be adjusted by the practitioner. Kernel size and number, and the pooling size are particularly important parameters because they directly affect the feature extraction process, which is the most important process during a CNN training. Previous studies have mostly used trial-anderror-based approaches with iterative experiments to determine network topology. Therefore, we propose a methodology to optimize CNN structure using GA, an evolutionary search algorithm, in an attempt to further enhance stock market prediction effectiveness.</p><p>Each CNN channel is composed of a sequence of technical indicators. We use a neural network model with two convolutional layers and one pooling layer. Optimized kernels are applied to the input data and optimal kernel size was simultaneously adopted for each layer. We compare the performance of the proposed model with standard CNN and ANN models and confirm superior predictive ability. The comparative ANN model is organized with one hidden layer containing 10 hidden neurons. ANN's classification accuracy at hold out data is 58.62% which the lowest performance. The standard (not optimized by GA) CNN model reached an accuracy of 70.16%, exhibiting a much better than the ANN model. Lastly, the GA-optimized   CNN model achieved a 73.74% accuracy at hold out data proving the effectiveness of proposed approach. Moreover, we conducted the McNemar statistical tests to further explore the results. The test results can be interpreted as empirical proof that the difference in accuracy performance between the benchmark models and the proposed model is significant in a statistical manner. As we can see from the experimental results, optimizing the CNN structure using GA verifies the possibility of solving the problem effectively by flexible CNN application to noisy and nonlinear stock market data.</p><p>This study suggests an optimization method for deep learning techniques that could be effectively applied to stock market prediction, but there remains a margin of improvement. Firstly, GA has many controlling parameters that can have a great influence on the model's performance. However, this study used a fixed set of parameters selected based on past research due to the limitations of computing resources. In fact, GA-CNN model iterates the learning process of CNN whenever genetic evolution happens, so it requires considerable computing power. Thus, future research should extend the range of controlling parameters of GA to improve the prediction results. Second, CNN models include various parameters that can affect its predictive performance. Although this study optimized convolutional and pooling layers, further research can target other CNN model factors, such as learning parameters and parameters in fully connected layers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 Fig. 2</head><label>12</label><figDesc>Fig.1The basic architecture of CNN</figDesc><graphic coords="7,204.13,59.24,340.32,120.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4</head><label>4</label><figDesc>Fig.4The operation process of GA</figDesc><graphic coords="8,53.87,535.46,232.56,160.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 Fig. 6</head><label>56</label><figDesc>Fig. 5 Crossover operation</figDesc><graphic coords="9,85.00,59.24,425.28,176.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 8</head><label>8</label><figDesc>Fig. 8 Example of a chromosome structure used in this study</figDesc><graphic coords="11,85.00,538.42,425.28,157.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 9</head><label>9</label><figDesc>Fig.9The basic structure of proposed model</figDesc><graphic coords="12,53.92,59.24,487.68,237.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>•</head><label></label><figDesc>Number of kernels in each convolutional layer: 1-63 • Kernel size of individual channels in the first convolutional layer: 1-31 • Kernel size of individual channels in the second convolutional layer: 1-31 • Window size of the pooling layer: 1-31</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10</head><label>10</label><figDesc>Fig. 10 Classification result for training and holdout set</figDesc><graphic coords="14,204.13,471.74,340.32,233.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11</head><label>11</label><figDesc>Fig. 11 Training and validation accuracy per epoch in ANN model</figDesc><graphic coords="15,54.71,59.24,230.88,154.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12</head><label>12</label><figDesc>Fig. 12 Training and validation accuracy per epoch in standard CNN model</figDesc><graphic coords="15,53.87,249.55,232.56,154.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 13 Table 7</head><label>137</label><figDesc>Fig. 13 Training and validation accuracy per epoch in GA-optimized CNN model</figDesc><graphic coords="15,53.87,453.26,232.56,156.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>A summary of previous studies on stock market prediction</figDesc><table><row><cell>Researchers (year)</cell><cell>Data type</cell><cell>Goal</cell><cell>Method(s)</cell><cell>Benchmark</cell><cell>Performance</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>measure</cell></row><row><cell>Saad, Prokhorov, and</cell><cell>US four stocks</cell><cell>Compare neural network</cell><cell>TDNN, RNN,</cell><cell>Fisher's linear classifier</cell><cell>False alarm</cell></row><row><cell>Wunsch (1998) [40]</cell><cell>of NYSE and</cell><cell>models (TDNN, RNN,</cell><cell>and PNN</cell><cell></cell><cell>rate</cell></row><row><cell></cell><cell>NASDAQ</cell><cell>and PNN)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fernandez-Rodrıguez,</cell><cell>Spain MADX</cell><cell>Investigate the profitability</cell><cell>ANN</cell><cell>Buy-and-hold strategy</cell><cell>Trading</cell></row><row><cell>Gonzalez-Martel, and</cell><cell></cell><cell>of technical trading rule</cell><cell></cell><cell></cell><cell>simulation</cell></row><row><cell>Sosvilla-Rivero (2000)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>[36]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Huang, Nakamori, and</cell><cell>Japan NIKKEI</cell><cell>Predict weekly volatility of</cell><cell>SVM</cell><cell>LDA, QDA, and EBNN</cell><cell>Directional</cell></row><row><cell>Wang (2005) [22]</cell><cell>225 index</cell><cell>stock index futures</cell><cell></cell><cell></cell><cell>accuracy</cell></row><row><cell>Yu, Chen, and Zhang (2014)</cell><cell>China SSEC</cell><cell>Construct stock selection</cell><cell>SVM ? PCA</cell><cell>-</cell><cell>Directional</cell></row><row><cell>[41]</cell><cell></cell><cell>system</cell><cell></cell><cell></cell><cell>accuracy</cell></row><row><cell>Kim and Han (2000) [26]</cell><cell>Korea KOSPI</cell><cell>Predict direction of stock</cell><cell>ANN ? GA</cell><cell>ANN</cell><cell>Directional</cell></row><row><cell></cell><cell>200</cell><cell>movement</cell><cell></cell><cell></cell><cell>accuracy</cell></row><row><cell>Armano, Marchesi, and</cell><cell>Italy COMIT</cell><cell>Forecast next-day price of</cell><cell>ANN ? GA</cell><cell>Buy-and-hold strategy</cell><cell>Percentage</cell></row><row><cell>Murru (2005) [27]</cell><cell>and US S&amp;P</cell><cell>stock market indexes</cell><cell></cell><cell></cell><cell>of</cell></row><row><cell></cell><cell>500</cell><cell></cell><cell></cell><cell></cell><cell>annualized</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>returns</cell></row><row><cell>Kim, Min, and Han (2006)</cell><cell>Korea KOSPI</cell><cell>Predict direction of stock</cell><cell>ANN ? human-</cell><cell>MV, WMV, Bayesian, BC,</cell><cell>Directional</cell></row><row><cell>[44]</cell><cell></cell><cell>movement</cell><cell>driven</cell><cell>WBC, BKS, and</cell><cell>accuracy</cell></row><row><cell></cell><cell></cell><cell></cell><cell>classifier ? GA</cell><cell>Dempster-Shafer theory</cell><cell></cell></row><row><cell>Heaton, Polson, and Witte</cell><cell>NASDAQ</cell><cell>Find a selection of</cell><cell>DNN and LSTM</cell><cell>-</cell><cell>MSE</cell></row><row><cell>(2017) [45]</cell><cell>biotechnology</cell><cell>investment strategy</cell><cell>network</cell><cell></cell><cell></cell></row><row><cell></cell><cell>index (IBB)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Chong, Han, and Park</cell><cell>Korea 38 stocks</cell><cell>Compare deep feature</cell><cell>DNN</cell><cell>Autoregressive model and</cell><cell>NMSE,</cell></row><row><cell>(2017) [29]</cell><cell>of KOSPI</cell><cell>learning-based stock</cell><cell></cell><cell>ANN</cell><cell>RMSE,</cell></row><row><cell></cell><cell></cell><cell>market prediction model</cell><cell></cell><cell></cell><cell>MAE, and</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MI</cell></row><row><cell>Fischer and Krauss (2017)</cell><cell>US S&amp;P 500</cell><cell>Predict direction of stock</cell><cell>LSTM network</cell><cell>Random forest and logistic</cell><cell>Directional</cell></row><row><cell>[46]</cell><cell></cell><cell>movement</cell><cell></cell><cell>regression</cell><cell>accuracy</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>The summary statistics of input data</figDesc><table><row><cell></cell><cell cols="2">Opening High</cell><cell>Low</cell><cell cols="2">Closing Volume</cell></row><row><cell>Mean</cell><cell cols="4">1431.28 1439.90 1420.12 1430.54</cell><cell>420,925,246</cell></row><row><cell cols="5">Median 1581.60 1588.73 1566.83 1579.21</cell><cell>379,664,992</cell></row><row><cell>Max</cell><cell cols="5">2225.95 2231.47 2202.92 2228.96 2,379,293,952</cell></row><row><cell>Min</cell><cell>466.57</cell><cell>472.31</cell><cell>463.54</cell><cell>468.76</cell><cell>136,328,992</cell></row><row><cell>SD</cell><cell>541.09</cell><cell>541.45</cell><cell>539.59</cell><cell>540.63</cell><cell>192,818,209</cell></row><row><cell cols="3">Table 3 The selected technical indicators and formulas</cell><cell cols="2">Technical indicator</cell><cell></cell><cell>Formula</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Momentum</cell><cell></cell><cell>C t À C tÀn</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Stochastic K %</cell><cell></cell><cell>CtÀLLtÀn HHtÀnÀLLtÀn Â 100</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Relative strength index (RSI)</cell><cell>100 À</cell><cell>1þ</cell><cell>À</cell><cell>100 t¼0 Up tÀI =n P nÀ1 Á = À</cell><cell>P nÀ1 t¼0 DwtÀI=n Á</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Moving average convergence divergence (MACD)</cell><cell>EMA 12 À EMA 26</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">LW %R</cell><cell></cell><cell>HnÀCt HnÀLn Â 100</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">A/D oscillator</cell><cell></cell><cell>Ht ÀCtÀ1 HtÀLt Â 100</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Commodity channel index (CCI)</cell><cell>MÀm dÂ0:015 Â 100</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4</head><label>4</label><figDesc>List of parameters used for the GA</figDesc><table><row><cell>Parameter</cell><cell>Value</cell></row><row><cell>Population size</cell><cell>100</cell></row><row><cell>Number of generations</cell><cell>10</cell></row><row><cell>Crossover rate</cell><cell>0.7</cell></row><row><cell>Mutation rate</cell><cell>0.25</cell></row><row><cell>Chromosome representation</cell><cell>Binary strings</cell></row><row><cell>Fitness function</cell><cell>Hit ratio</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 The</head><label>5</label><figDesc></figDesc><table><row><cell>optimized values of multi-channel CNN</cell><cell>Layer</cell><cell>Optimized parameter</cell><cell>Optimal value</cell></row><row><cell></cell><cell>Convolutional layer</cell><cell></cell><cell></cell></row><row><cell></cell><cell>First convolutional layer</cell><cell># of kernel</cell><cell>56</cell></row><row><cell></cell><cell></cell><cell>Size of kernel</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Channel 1 (momentum)</cell><cell>4</cell></row><row><cell></cell><cell></cell><cell>Channel 2 (stochastic K%)</cell><cell>22</cell></row><row><cell></cell><cell></cell><cell>Channel 3 (RSI)</cell><cell>27</cell></row><row><cell></cell><cell></cell><cell>Channel 4 (MACD)</cell><cell>6</cell></row><row><cell></cell><cell></cell><cell>Channel 5 (LW %R)</cell><cell>17</cell></row><row><cell></cell><cell></cell><cell>Channel 6 (A/D oscillator)</cell><cell>10</cell></row><row><cell></cell><cell></cell><cell>Channel 7 (CCI)</cell><cell>23</cell></row><row><cell></cell><cell>Second convolutional layer</cell><cell># of kernel</cell><cell>63</cell></row><row><cell></cell><cell></cell><cell>Size of kernel</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Channel 1 (momentum)</cell><cell>3</cell></row><row><cell></cell><cell></cell><cell>Channel 2 (stochastic K%)</cell><cell>2</cell></row><row><cell></cell><cell></cell><cell>Channel 3 (RSI)</cell><cell>12</cell></row><row><cell></cell><cell></cell><cell>Channel 4 (MACD)</cell><cell>3</cell></row><row><cell></cell><cell></cell><cell>Channel 5 (LW %R)</cell><cell>6</cell></row><row><cell></cell><cell></cell><cell>Channel 6 (A/D oscillator)</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell>Channel 7 (CCI)</cell><cell>4</cell></row><row><cell></cell><cell>Pooling layer</cell><cell>Pooling size</cell><cell>2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6</head><label>6</label><figDesc>Summary of experimental result</figDesc><table><row><cell></cell><cell>ANN</cell><cell>CNN</cell><cell>GA-CNN</cell></row><row><cell>Training</cell><cell>60.67</cell><cell>71.69</cell><cell>75.95</cell></row><row><cell>Validation</cell><cell>58.40</cell><cell>69.94</cell><cell>74.69</cell></row><row><cell>Holdout</cell><cell>58.62</cell><cell>70.16</cell><cell>73.74</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This research did not receive any specific grant from funding agencies in the public, commercial, or not-for-profit sectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compliance with ethical standards</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of interest</head><p>The authors declare that they have no conflict of interest.</p><p>Publisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th conference on neural information processing systems (NIPS)</title>
		<meeting>the 26th conference on neural information processing systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploiting multichannels deep convolutional neural networks for multivariate time series classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.1007/11704-015-4478-2</idno>
		<ptr target="https://doi.org/10.1007/11704-015-4478-2" />
	</analytic>
	<monogr>
		<title level="j">Front Comput Sci</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="96" to="112" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Load frequency controller design via BAT algorithm for nonlinear interconnected power system</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Abd-Elazim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Ali</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ijepes.2015.11.029</idno>
		<ptr target="https://doi.org/10.1016/j.ijepes.2015.11.029" />
	</analytic>
	<monogr>
		<title level="j">Int J Electr Power Energy Syst</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="166" to="177" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imperialist competitive algorithm for optimal STATCOM design in a multimachine power system</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Abd-Elazim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Ali</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ijepes.2015.09.004</idno>
		<ptr target="https://doi.org/10.1016/j.ijepes.2015.09.004" />
	</analytic>
	<monogr>
		<title level="j">Int J Electr Power Energy Syst</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="136" to="146" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Load frequency controller design of a two-area system composing of PV grid and thermal generator via firefly algorithm</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Abd-Elazim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Ali</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00521-016-2668-y</idno>
		<ptr target="https://doi.org/10.1007/s00521-016-2668-y" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput Appl</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="607" to="616" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature14539</idno>
		<ptr target="https://doi.org/10.1038/nature14539" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A novel classification model for cotton yarn quality based on trained neural network using genetic algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Amin</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.knosys.2012.10.008</idno>
		<ptr target="https://doi.org/10.1016/j.knosys.2012.10.008" />
	</analytic>
	<monogr>
		<title level="j">Knowl Based Syst</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="124" to="132" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Time series forecasting by evolving artificial neural networks with genetic algorithms, differential evolution and estimation of distribution algorithm</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Donate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Sa ´nchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>De Miguel</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00521-011-0741-0</idno>
		<idno>1007/s00521-011-0741-0</idno>
		<ptr target="https://doi.org/10" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput Appl</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="20" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A flexible ANN-GA-multivariate algorithm for assessment and optimization of machinery productivity in complex production units</title>
		<author>
			<persName><forename type="first">A</forename><surname>Azadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Mianaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Asadzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saberi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sheikhalishahi</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jmsy.2014.11.007</idno>
		<ptr target="https://doi.org/10.1016/j.jmsy.2014.11.007" />
	</analytic>
	<monogr>
		<title level="j">J Manuf Syst</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="46" to="75" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A neuro-genetic approach for modeling and optimizing a complex cogeneration process</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seijo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Echanobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Del Campo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Garcia-Sedano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schmeck</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.asoc.2016.07.026</idno>
		<ptr target="https://doi.org/10.1016/j.asoc.2016.07.026" />
	</analytic>
	<monogr>
		<title level="j">Appl Soft Comput</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="347" to="358" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Airblast prediction through a hybrid genetic algorithm-ANN model</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Armaghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hasanipanah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mahdiyar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mza</forename><surname>Majid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Amnieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Tahir</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00521-016-2598-8</idno>
		<ptr target="https://doi.org/10.1007/s00521-016-2598-8" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput Appl</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="619" to="629" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Parameter selection of support vector machines and genetic algorithm based on change area search</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00521-011-0603-9</idno>
		<ptr target="https://doi.org/10.1007/s00521-011-0603-9" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput Appl</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Enhancing SVM performance in intrusion detection using optimal feature subset selection based on genetic principal components</title>
		<author>
			<persName><forename type="first">I</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alghamdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alelaiwi</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00521-013-1370-6</idno>
		<ptr target="https://doi.org/10.1007/s00521-013-1370-6" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput Appl</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7-8</biblScope>
			<biblScope unit="page" from="1671" to="1682" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An efficient intrusion detection system based on hypergraph-Genetic algorithm for parameter optimization and feature selection in support vector machine</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Somu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kirthivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liscano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Sriram</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.knosys.2017.07.005</idno>
		<ptr target="https://doi.org/10.1016/j.knosys.2017.07.005" />
	</analytic>
	<monogr>
		<title level="j">Knowl Based Syst</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">GA-SVM based feature selection and parameter optimization in hospitalization expense modeling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huiling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wenwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.asoc.2018.11.001</idno>
		<ptr target="https://doi.org/10.1016/j.asoc.2018.11.001" />
	</analytic>
	<monogr>
		<title level="j">Appl Soft Comput</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="323" to="332" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Case-based reasoning supported by genetic algorithms for corporate bond rating</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0957-4174(98)00063-3</idno>
		<ptr target="https://doi.org/10.1016/S0957-4174(98)00063-3" />
	</analytic>
	<monogr>
		<title level="j">Expert Syst Appl</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="85" to="95" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bankruptcy prediction modeling with hybrid case-based reasoning and genetic algorithms approach</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.asoc.2008.08.002</idno>
		<idno>08.002</idno>
		<ptr target="https://doi.org/10.1016/j.asoc" />
	</analytic>
	<monogr>
		<title level="j">Appl Soft Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="599" to="607" />
			<date type="published" when="2008">2009. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Introduction to financial forecasting</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Abu-Mostafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Atiya</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF00126626</idno>
		<ptr target="https://doi.org/10.1007/BF00126626" />
	</analytic>
	<monogr>
		<title level="j">Appl Intell</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="205" to="213" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient capital markets: a review of theory and empirical work</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Fama</surname></persName>
		</author>
		<idno type="DOI">10.2307/2325486</idno>
		<ptr target="https://doi.org/10.2307/2325486" />
	</analytic>
	<monogr>
		<title level="j">J Finance</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="383" to="417" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Forecasting stock market movement direction with support vector machine</title>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nakamori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Sy</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cor.2004.03.016</idno>
		<ptr target="https://doi.org/10.1016/j.cor.2004.03.016" />
	</analytic>
	<monogr>
		<title level="j">Comput Oper Res</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2513" to="2522" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Predicting the Brazilian stock market through neural networks and adaptive exponential smoothing methods</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>De Faria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Albuquerque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jtp</forename><surname>Cavalcante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Albuquerque</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2009.04.032</idno>
		<ptr target="https://doi.org/10.1016/j.eswa.2009.04.032" />
	</analytic>
	<monogr>
		<title level="j">Expert Syst Appl</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="12506" to="12509" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Prediction of selected Indian stock using a partitioning-interpolation based ARIMA-GARCH model</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Reddy</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.aci.2014.09.002</idno>
		<idno>09.002</idno>
		<ptr target="https://doi.org/10.1016/j.aci.2014" />
	</analytic>
	<monogr>
		<title level="j">Appl Comput Inform</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="130" to="143" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Computational intelligence and financial markets: a survey and future directions</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Cavalcante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Brasileiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">L</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Nobrega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Oliveira</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2016.02.006</idno>
		<ptr target="https://doi.org/10.1016/j.eswa.2016.02.006" />
	</analytic>
	<monogr>
		<title level="j">Expert Syst Appl</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="194" to="211" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Genetic algorithms approach to feature discretization in artificial neural networks for the prediction of stock price index</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.1016/s0957-4174(00)00027-0</idno>
		<ptr target="https://doi.org/" />
	</analytic>
	<monogr>
		<title level="j">Expert Syst Appl</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="125" to="132" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A hybrid genetic-neural architecture for stock indexes forecasting</title>
		<author>
			<persName><forename type="first">G</forename><surname>Armano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marchesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Murru</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ins.2003.03.023</idno>
		<ptr target="https://doi.org/10.1016/j.ins.2003.03.023" />
	</analytic>
	<monogr>
		<title level="j">Inf Sci</title>
		<imprint>
			<biblScope unit="volume">170</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="33" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Predicting direction of stock price index movement using artificial neural networks and support vector machines: the sample of the Istanbul Stock Exchange</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Boyacioglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Baykan</surname></persName>
		</author>
		<author>
			<persName><surname>¨k</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2010.10.027</idno>
		<ptr target="https://doi.org/10.1016/j.eswa.2010.10.027" />
	</analytic>
	<monogr>
		<title level="j">Expert Syst Appl</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="5311" to="5319" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep learning networks for stock market analysis and prediction: methodology, data representations, and case studies</title>
		<author>
			<persName><forename type="first">E</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Park</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2017.04.030</idno>
		<ptr target="https://doi.org/10.1016/j.eswa.2017.04.030" />
	</analytic>
	<monogr>
		<title level="j">Expert Syst Appl</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="187" to="205" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Time series analysis: forecasting and control, revised edn</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Box</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Jenkins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976">1976</date>
			<pubPlace>Holden-Day, Oakland</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Autoregressive conditional heteroscedasticity with estimates of the variance of United Kingdom inflation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Engle</surname></persName>
		</author>
		<idno type="DOI">10.2307/1912773</idno>
		<ptr target="https://doi.org/10.2307/1912773" />
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="987" to="1007" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A note on GARCH predictable variances and stock market efficiency</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Schwaiger</surname></persName>
		</author>
		<idno type="DOI">10.1016/0378-4266(94)00081-d</idno>
		<ptr target="https://doi.org/10.1016/0378-4266(94)00081-d" />
	</analytic>
	<monogr>
		<title level="j">J Bank Finance</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="949" to="953" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Stock index forecasting based on a hybrid model</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Guo</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.omega.2011.07.008</idno>
		<ptr target="https://doi.org/10.1016/j.omega.2011.07.008" />
	</analytic>
	<monogr>
		<title level="j">Omega</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="758" to="766" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A hybrid model based on adaptive-network-based fuzzy inference system to forecast Taiwan stock market</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Ho</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2011.04.127</idno>
		<ptr target="https://doi.org/10.1016/j.eswa.2011.04.127" />
	</analytic>
	<monogr>
		<title level="j">Expert Syst Appl</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="13625" to="13631" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Surveying stock market forecasting techniques-part II: soft computing methods</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Atsalakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Valavanis</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2008.07.006</idno>
		<ptr target="https://doi.org/10.1016/j.eswa.2008.07.006" />
	</analytic>
	<monogr>
		<title level="j">Expert Syst Appl</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="5932" to="5941" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On the profitability of technical trading rules based on artificial neural networks: evidence from the Madrid stock market</title>
		<author>
			<persName><forename type="first">F</forename><surname>Fernandez-Rodrıguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gonzalez-Martel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sosvilla-Rivero</surname></persName>
		</author>
		<idno type="DOI">10.1016/s0165-1765(00)00270-6</idno>
		<ptr target="https://doi.org/10.1016/s0165-1765(00" />
	</analytic>
	<monogr>
		<title level="j">Econ Lett</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="270" to="276" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Application of support vector machines in financial time series forecasting</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<idno type="DOI">10.1016/s0305-0483(01)00026-3</idno>
		<ptr target="https://doi.org/10.1016/s0305-0483(01)00026-3" />
	</analytic>
	<monogr>
		<title level="j">Omega</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="309" to="317" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Using support vector machine with a hybrid feature selection method to the stock trend prediction</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2009.02.038</idno>
		<ptr target="https://doi.org/10.1016/j.eswa.2009.02.038" />
	</analytic>
	<monogr>
		<title level="j">Expert Syst Appl</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="10896" to="10904" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Comparison of ARIMA and artificial neural networks models for stock price prediction</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Adebiyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O</forename><surname>Adewumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Ayo</surname></persName>
		</author>
		<idno type="DOI">10.1155/2014/614342</idno>
		<ptr target="https://doi.org/10.1155/2014/614342" />
	</analytic>
	<monogr>
		<title level="j">J Appl Math</title>
		<imprint>
			<biblScope unit="volume">2014</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Comparative study of stock trend prediction using time delay, recurrent and probabilistic neural networks</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Prokhorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Wunsch</surname></persName>
		</author>
		<idno type="DOI">10.1109/72.728395</idno>
		<ptr target="https://doi.org/10.1109/72.728395" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1456" to="1470" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A SVM stock selection model within PCA</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.procs.2014.05.284</idno>
		<ptr target="https://doi.org/10.1016/j.procs.2014.05.284" />
	</analytic>
	<monogr>
		<title level="j">Procedia Comput Sci</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="406" to="412" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Stock market prediction using artificial neural networks with optimal feature transformation</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00521-004-0428-x</idno>
		<ptr target="https://doi.org/10.1007/s00521-004-0428-x" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput Appl</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="255" to="260" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Hybridizing nonlinear independent component analysis and support vector regression with particle swarm optimization for stock index forecasting</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00521-012-1198-5</idno>
		<ptr target="https://doi.org/10.1007/s00521-012-1198-5" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput Appl</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7-8</biblScope>
			<biblScope unit="page" from="2417" to="2427" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">An evolutionary approach to the combination of multiple classifiers to predict a stock price index</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2005.09.020</idno>
		<idno>09.020</idno>
		<ptr target="https://doi.org/10.1016/j.eswa" />
	</analytic>
	<monogr>
		<title level="j">Expert Syst Appl</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="241" to="247" />
			<date type="published" when="2005">2006. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep learning for finance: deep portfolios</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Heaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">G</forename><surname>Polson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Witte</surname></persName>
		</author>
		<idno type="DOI">10.1002/asmb.2230</idno>
		<ptr target="https://doi.org/10.1002/asmb.2230" />
	</analytic>
	<monogr>
		<title level="j">Appl Stoch Models Bus Ind</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="21" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep learning with long short-term memory networks for financial market predictions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Krauss</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ejor.2017.11.054</idno>
		<ptr target="https://doi.org/10.1016/j.ejor.2017.11.054" />
	</analytic>
	<monogr>
		<title level="j">Eur J Oper Res</title>
		<imprint>
			<biblScope unit="volume">270</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="654" to="669" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<idno type="DOI">10.1109/5.726791</idno>
		<ptr target="https://doi.org/10.1109/5.726791" />
	</analytic>
	<monogr>
		<title level="j">Proc IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fergus</forename><forename type="middle">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European</title>
		<meeting>the European</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A hybrid short-term load forecasting model based on variational mode decomposition and long short-term memory networks considering relevant factors with Bayesian optimization algorithm</title>
		<author>
			<persName><forename type="first">F</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.apenergy.2019.01.055</idno>
		<ptr target="https://doi.org/10.1016/j.apenergy.2019.01.055" />
	</analytic>
	<monogr>
		<title level="j">Appl Energy</title>
		<imprint>
			<biblScope unit="volume">237</biblScope>
			<biblScope unit="page" from="103" to="116" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Gradient-based optimization of hyperparameters</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.1162/089976600300015187</idno>
		<ptr target="https://doi.org/10.1162/089976600300015187" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1889" to="1900" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Random search for hyper-parameter optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Mach Learn Res</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="281" to="305" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A solution representation of genetic algorithm for neural network weights and structure</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Jaddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Abdullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Hamdan</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ipl.2015.08.001</idno>
		<ptr target="https://doi.org/10.1016/j.ipl.2015.08.001" />
	</analytic>
	<monogr>
		<title level="j">Inf Process Lett</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="25" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A constraint-based genetic algorithm for optimizing neural network architectures for detection of loss of coolant accidents of nuclear power plants</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Vinod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">V</forename><surname>Santhosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tawfik</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2018.09.014</idno>
		<ptr target="https://doi.org/10.1016/j.neucom.2018.09.014" />
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">322</biblScope>
			<biblScope unit="page" from="102" to="119" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Heuristic techniques to optimize neural network architecture in manufacturing applications</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ciancio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ambrogio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gagliardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Musmanno</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00521-015-1994-9</idno>
		<ptr target="https://doi.org/10.1007/s00521-015-1994-9" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput Appl</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2001" to="2015" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Convolutional networks for images, speech, and time series</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The handbook of brain theory and neural networks</title>
		<title level="s">Cambridge Neural Computing and Applications</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Arbib</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A survey of deep neural network architectures and their applications</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E</forename><surname>Alsaadi</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2016.12.038</idno>
		<idno>12.038</idno>
		<ptr target="https://doi.org/10.1016/j.neucom" />
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">234</biblScope>
			<biblScope unit="page" from="11" to="26" />
			<date type="published" when="2016">2017. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Adaptation in natural and artificial systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Holland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<publisher>The University of Michigan Press</publisher>
			<pubPlace>Ann Arbor</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A hybrid approach based on neural networks and genetic algorithms for detecting temporal patterns in stock markets</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Shin</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.asoc.2006.03.004</idno>
		<ptr target="https://doi.org/10.1016/j.asoc.2006.03.004" />
	</analytic>
	<monogr>
		<title level="j">Appl Soft Comput</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="569" to="576" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Genetic algorithms for pattern recognition</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>CRC Press</publisher>
			<pubPlace>Boca Raton</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A theoretical analysis of feature pooling in visual recognition</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning (ICML-10)</title>
		<meeting>the 27th international conference on machine learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="111" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Mine blast algorithm for environmental economic load dispatch with valve loading effect</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Elazim</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00521-016-2650-8</idno>
		<ptr target="https://doi.org/10.1007/s00521-016-2650-8" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput Appl</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="261" to="270" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Genetic algorithms compared to other techniques for pipe optimization</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Dandy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="DOI">10.1061/(ASCE)0733-9496(1994)120:4(423)</idno>
		<ptr target="https://doi.org/10.1061/" />
	</analytic>
	<monogr>
		<title level="j">J Water Resour Plan Manag</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="1994">1994. 1994</date>
			<publisher>ASCE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">An empirical methodology for developing stock market trading systems using artificial neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Vanstone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Finnie</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2008.08.019</idno>
		<ptr target="https://doi.org/10.1016/j.eswa.2008.08.019" />
	</analytic>
	<monogr>
		<title level="j">Expert Syst Appl</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="6668" to="6680" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Adam: a method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd international conference on learning representation (ICLR)</title>
		<meeting>the 3rd international conference on learning representation (ICLR)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
