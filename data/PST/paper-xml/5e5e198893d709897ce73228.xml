<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spatio-Temporal Attention Networks for Action Recognition and Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jun</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Lab of Software Development Environment</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
							<email>xlliu@nlsde.buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Lab of Software Development Environment</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Innovation Center</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenxuan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Lab of Software Development Environment</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mingyuan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Lab of Software Development Environment</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jingkuan</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Lab of Software Development Environment</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Lab of Software Development Environment</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Information Engineering and Computer Science</orgName>
								<orgName type="institution">University of Trento</orgName>
								<address>
									<settlement>Trento</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">China</forename><forename type="middle">X</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Beijing Advanced Innovation Center for Big Data-Based Precision Medicine</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Spatio-Temporal Attention Networks for Action Recognition and Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">89EF0ABE6782337DA162CF4BAF90F07D</idno>
					<idno type="DOI">10.1109/TMM.2020.2965434</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2020.2965434, IEEE Transactions on Multimedia</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>3D CNN</term>
					<term>spatio-temporal attention</term>
					<term>temporal attention</term>
					<term>spatial attention</term>
					<term>action recognition</term>
					<term>action detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, 3D Convolutional Neural Network (3D CNN) models have been widely studied for video sequences and achieved satisfying performance in action recognition and detection tasks. However, most of the existing 3D CNNs treat all input video frames equally, thus ignoring the spatial and temporal differences across the video frames. To address the problem, we propose a spatio-temporal attention (STA) network that is able to learn the discriminative feature representation for actions, by respectively characterizing the beneficial information at both the frame level and the channel level. By simultaneously exploiting the differences in spatial and temporal dimensions, our STA module enhances the learning capability of the 3D convolutions when handling the complex videos. The proposed STA method can be wrapped as a generic module easily plugged into the state-of-the-art 3D CNN architectures for video action detection and recognition. We extensively evaluate our method on action recognition and detection tasks over three popular datasets (UCF-101, HMDB-51 and THUMOS 2014), and the experimental results demonstrate that adding our STA network module can obtain the state-of-the-art performance on UCF-101 and HMDB-51, which has the top-1 accuracies of 98.4% and 81.4% respectively, and achieve significant improvement on THUMOS 2014 dataset compared against original models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In recent years, the explosion of the availability of video data has brought challenges to efficient video analysis and understanding. Video action recognition and detection has become one of the most widely studied tasks in computer vision <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b4">[5]</ref>, and encouraging progress has been achieved following the success of various architectures based on deep convolutional neural networks (CNNs) <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. However, common CNNs methods have been primarily developed for 2D images, and can hardly capture the temporal information contained in the video, which plays an important role in action detection and recognition.</p><p>Modelling the temporal variations in the video is a challenging problem that is made even harder if the detection is to be performed in an online setting and in real-time.</p><p>The traditional solutions such as the improved Dense Trajectories (iDT) <ref type="bibr" target="#b7">[8]</ref> highly depend on the optical flow-based hand-crafted features to extract temporal trajectories. There are also deep end-to-end solutions <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. For example, Simonyan and Zisserman <ref type="bibr" target="#b8">[9]</ref> introduced a two-stream CNN architecture that uses two 2D networks corresponding to the optical flow and the appearance, respectively. Despite the good performance, extracting the optical flow is usually computationally intensive and becomes intractable on largescale datasets <ref type="bibr" target="#b10">[11]</ref>.</p><p>To avoid the expensive computation while still capturing the spatio-temporal correlations, a number of 3D CNNs methods have been proposed in the past few years <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b16">[17]</ref>. In <ref type="bibr" target="#b12">[13]</ref>, Ji et al. were among the first to introduce the concept of the 3D CNN and applied 3D convolution to extract spatio-temporal features from videos. Tran et al. proposed a simple, yet effective 3D CNN approach (C3D) that can deal with input videos of varying temporal lengths well <ref type="bibr" target="#b0">[1]</ref>. Recently, various types of 3D CNNs with different structures were proposed, e.g., pseudo-3D CNNs <ref type="bibr" target="#b17">[18]</ref>, two-stream I3D <ref type="bibr" target="#b18">[19]</ref>, mixed 3D/2D convolutional tube <ref type="bibr" target="#b19">[20]</ref>, etc. The 3D CNNs can directly extract spatial and temporal features from raw videos, and thus their performance in the field of action recognition and detection has been significantly improved recently.</p><p>In practice 3D CNNs can largely improve the video processing speed with a satisfying performance, e.g., at least an order of magnitude faster than real-time <ref type="bibr" target="#b20">[21]</ref>. Many studies further modified and improved 3D CNNs to increase the capacity of representing finer temporal relations in a local spatio-temporal window <ref type="bibr" target="#b21">[22]</ref>. Despite the promising progress, without explicitly extracting the most informative information in spatial and temporal dimensions of the videos, the existing 3D CNN solutions still lack learning capacity of discriminative spatio-temporal feature representation for actions. For example, traditional 3D CNNs usually learn the temporal feature by equally treating the consecutive frames, while in practice, different frames might convey quite different contributions to the action recognition, e.g., the frames with motion blur may provide fewer cues for activity recognition. Similarly, along the spatial dimension, the differences between the visual information from different channels are usually undistinguished in the existing 3D CNN solutions.</p><p>It has been noted in visual cognition literature that 1520-9210 (c) 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. humans do not focus their attention on an entire scene of each frame at once <ref type="bibr" target="#b22">[23]</ref>. Instead, they focus sequentially on different parts in different frames to extract relevant information <ref type="bibr" target="#b23">[24]</ref>. Thus, to improve the power of the 3D CNNs for extracting the informative features from the video, in this paper we propose a novel spatio-temporal attention (STA) network for action recognition and detection. The proposed network can exploit the discriminative information at two levels (i.e., frame level and channel level), and thus improve the capability of the 3D CNNs with a more powerful spatio-temporal feature learning. In the temporal dimension, while the traditional convolution is localized at the local receptive field, our approach can capture the temporal attention by learning varying framewise weights with the global visual information. Besides, in the spatial dimension, we further consider the differences among the various channels in 3D CNNs for activity representation, and develop a spatial attention at the channel level, which together with frame level attention can avoid the computational bottleneck and also enhance the feature representing power. Our spatio-temporal attention network is flexible and efficient. It can be easily plugged into the state-of-the-art 3D CNN frameworks and thus improve their performance significantly.</p><p>The main contribution of this paper is that we devise a novel and efficient spatio-temporal attention mechanism in the 3D CNNs for action recognition and detection tasks. Instead of directly employing two similar temporal or spatial attention separately, our mechanism captures temporal and spatial attention simultaneously in a single module, and behaves as a kind of soft attention which learns the attention weights adaptively. The proposed module serves as a simple, yet generic module for many 3D CNNs, and in practice it only needs to be appended to the later convolutional layers without increasing too much computational cost. Figure <ref type="figure" target="#fig_0">1</ref> shows a typical example using our STA module plugged into ResNeXt-101(64f) on "Jump" action from HMDB-51 dataset. We extensively evaluate our method on action recognition and detection tasks over three popular datasets (UCF-101, HMDB-51 and THUMOS 2014), and the experimental results demonstrate that our STA module can achieve the state-of-the-art performance for action recognition task and obtain considerably improved performance compared against original models in action detection task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK A. Action Recognition and Action Detection</head><p>Action recognition <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b17">[18]</ref> and action detection <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b28">[29]</ref> have been an important research field for visual understanding <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>. In the literature, more and more researchers have attempted to improve the accuracy of the action recognition on the common action recognition datasets, including HMDB-51, UCF-101, Sports-1M. However, thus far the results achieved by the action detection methods, using traditional hand-crafted features <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, CNN features <ref type="bibr" target="#b33">[34]</ref>- <ref type="bibr" target="#b35">[36]</ref> and fusing two types of features <ref type="bibr" target="#b36">[37]</ref>, have not been satisfactory. In recent years, as the video data explosively increased, the expensive computation and insufficient annotations have become the key challenges for video action recognition and detection. Therefore, more and more researchers dedicated effort to the problems of fast recognition or detection with limited supervised information <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b37">[38]</ref> and even unsupervised methods <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, which have been proven effective to deal with the challenge in the large-scale recognition and detection tasks.</p><p>For video action recognition and detection, the temporal feature learning serves as a critical part to achieve satisfying performance. In <ref type="bibr" target="#b8">[9]</ref>, Simonyan et al. proposed a two-stream CNN architecture which incorporates a spatial network and a temporal one based on the multi-frame dense optical flow. Peng et al. developed a multi-region two-stream R-CNN model by stacking optical flow over several frames <ref type="bibr" target="#b40">[41]</ref>. Feichtenhofer et al. directly exploited the power of the CNN in videos and studied a number of ways of fusing CNNs both spatially and temporally <ref type="bibr" target="#b41">[42]</ref>. Most of the state-of-theart action recognition approaches rely on traditional local optical flow estimation and the computationally expensive two-stage prediction. In <ref type="bibr" target="#b42">[43]</ref> the motion information is implicitly captured by the CNN architecture, achieving the goal of fast computation and end-to-end training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. 3D CNNs</head><p>In the literature, Kim et al. first introduced the 3D filter field descriptor <ref type="bibr" target="#b11">[12]</ref>, and later Ji et al. further proposed the 3D CNNs descriptor <ref type="bibr" target="#b12">[13]</ref>. Following the basic idea, and based on the practical work <ref type="bibr" target="#b0">[1]</ref> and its released source code, extensive studies have been proposed and focusing mainly on the 3D CNNs <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b43">[44]</ref>. Recently, the focus has been on how to improve the performance of features learning for 3D CNNs considering the temporal dimension <ref type="bibr" target="#b21">[22]</ref>. In <ref type="bibr" target="#b6">[7]</ref> CNNs, which demonstrated superior performances over several state-of-the-art techniques at the same time being more computationally efficient and using less memory <ref type="bibr" target="#b17">[18]</ref>.</p><p>In <ref type="bibr" target="#b19">[20]</ref>, the authors employed the mixed 3D/2D convolutional tube for human action recognition. This approach integrates 2D CNNs with the 3D convolution module to generate deeper and more informative feature maps, while reducing the training complexity in each round of spatiotemporal fusion, and taking advantage of well-established 2D CNNs for visual recognition in static images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Attention</head><p>Attention mechanism has been widely used in various fields <ref type="bibr" target="#b44">[45]</ref>- <ref type="bibr" target="#b49">[50]</ref>. For example, <ref type="bibr">Bahdanau et</ref>   <ref type="bibr" target="#b53">[54]</ref>, which is a lightweight gating mechanism, to model channelwise relationships. In <ref type="bibr" target="#b54">[55]</ref> the authors devised the Convolutional Block Attention Module (CBAM), a simple yet effective attention module for feed-forward convolutional neural networks, which is a channel and spatial attention for image classification and image object detection task. Similarly, for video data, Sharma et al. proposed a soft attention based recurrent model for action recognition and demonstrated that the model tends to recognize important elements in video frames based on the activities it detects <ref type="bibr" target="#b23">[24]</ref>. <ref type="bibr" target="#b46">[47]</ref> designed a fully differentiable temporal attention filters for human activity recognition from videos. <ref type="bibr" target="#b55">[56]</ref> proposed an attentional pooling for Action Recognition. As to the 3D CNNs, Xie et al. replaced many of the 3D convolutions by low-cost 2D convolutions to seek a balance between speed and accuracy, and further improved the accuracy of their model by using feature gating based attention mechanism <ref type="bibr" target="#b14">[15]</ref>. Long et al. proposed a local feature integration framework based on attention clusters, and introduced a shifting operation to capture more diverse signals <ref type="bibr" target="#b56">[57]</ref>. <ref type="bibr" target="#b57">[58]</ref> presented the non-local operations to capturing long-range dependencies for video classification. <ref type="bibr" target="#b58">[59]</ref> devised a factorized action-scene network (FASNet) to generate content-attention representation, which can encode and fuse the most relevant motion information and scene information for action recognition. <ref type="bibr" target="#b49">[50]</ref> introduced a hierarchical self-attention network for video action localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Discussions</head><p>We have to point out that our STA is the lightweight and generic module capturing both the temporal and spatial attentions for 3D CNN models. To guarantee the learning power of the module, we devise that the squeezing and expanding operations can perform simultaneously along both the spatial and temporal dimensions. In <ref type="bibr" target="#b53">[54]</ref>, Hu et al. proposed a similar network structure named Squeezeand-Excitation (SE) block, which adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. However, our STA fundamentally differs from SE in terms of both the receptive field and the network structure. First, STA module fits the 3D CNN models by considering both the frame level and the channel level, while SE only focuses on 2D CNNs at the channel level. Besides, STA possesses a much stronger learning power than SE, owing to its paired convolution/deconvolution network structure, which can significantly improve the network capacity for better feature representation.</p><p>Very recently, in <ref type="bibr" target="#b15">[16]</ref>, Diba et al. extended the SE block to a Spatio-Temporal Channel Correlation (STC) block for the 3D networks, mainly attempting to model the correlations between channels with respect to temporal and spatial features, rather than capturing the spatial and temporal attentions in a simultaneous way. Therefore, compared to STC, our STA module couples spatial and temporal dimensions, and thus can better find the informative characteristics of the features, based on the global contextual information from multiple dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SPATIO-TEMPORAL ATTENTION NETWORKS</head><p>Attention is the behavioral and cognitive process of selectively concentrating on a discrete aspect of information, whether deemed subjective or objective, while ignoring other perceivable information. Visual attention is one of the key mechanisms of perception that enables humans to efficiently select the visual data of most potential interest <ref type="bibr" target="#b59">[60]</ref>. For video action recognition, a proper attention model can help answer the question of where and when it needs to look at the image evidence to draw a classification decision <ref type="bibr" target="#b47">[48]</ref>. The attention model for action recognition/detection can potentially help infer the action happening in videos by focusing only on the relevant places in certain frames along the temporal domain.</p><p>In this paper, we introduce the spatio-temporal attention that learns different focusing weights for different frames in the temporal dimension and different focusing weights for different channels in the spatial dimension. Before elaborating our spatio-temporal attention (STA) network model, we first introduce the basic notations.</p><p>Suppose we have an input frame sequence X = {x i,j ∈ R h×w , i = 1, . . . , l, j = 1, . . . , c}, where x i,j is the feature map at time i and channel j, l is the number of frames in each input sliding temporal window, and c denotes the number of channels. Our STA module attempts to learn the attention W = {w i,j , i = 1, . . . , l, j = 1, . . . , c} weighting the frames and channels respectively in temporal and spatial dimension. We define an attention function T (•) for the STA module, which learns the weights W from the input features X. Based on T , we can define the output sequence Y = {y i,j ∈ R h×w , i = 1, . . . , l, j = 1, . . . , c} generated by passing X through the STA module. In this paper, there will be two types of attention function, i.e., the temporal attention function T t (•) at frame level and the spatial attention function T s (•) at the channel level. Figure <ref type="figure" target="#fig_1">2</ref> shows the whole spatio-temporal attention network module.</p><p>The design of the STA module follows the following principles: (1) the module should be simple and efficient, relying on simple operations like convolution and deconvolution; (2) the module should enable the robust and nonlinear learning capacity of the spatio-temporal feature representation in 3D CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Temporal Attention Module</head><p>In practice a robust action recognition model can generate correct predictions by only focusing on a small but informative part of action video, instead of the whole one. This indicates that the attention mechanism plays quite an important role in learning discriminative feature representation for actions. The existing 3D CNN models, simultaneously utilizing both the spatial and temporal information, have achieved promising performance. However, they usually indiscriminately handle the temporal frames and thus can hardly guarantee a stable recognition performance. Next we first consider integrating the temporal attention into the 3D convolutions, so that the enhanced model can distinguish the most informative frames and thus extract the characteristics of the actions.</p><p>1) Temporal Attention: We introduce a temporal attention mechanism at the frame level. Specifically, for the input feature maps X t (here we use X t to denote the input into each layer without distinguishing the original "frame sequence" for the first layer and "feature maps" for the hidden layers), the temporal attention mechanism learns a frame-wise weights matrix W t via the transform function T t . In practice the input sliding window is usually too short for extracting the variance information among the consecutive frames, especially when a convolutional layer is applied to further reduce the length, and causes the loss of the valuable information for action recognition. To avoid this problem, we design our temporal attention function T t , which first expands the temporal dimension by a deconvolutional (Deconv) layer to preserve more temporal information, and then squeezes it by a convolutional layer (Conv2) to maintain the original length for further processing.</p><p>As we know, down-sampling will lose the information and up-sampling will increase the model capacity for capturing more information. For example, the traditional Deconv based on the bilinear interpolation can preserve the original information and increase more additional interpolated information <ref type="bibr" target="#b60">[61]</ref>. We adopt the Deconv operation to up-sample the feature maps via learning the corresponding weights. Owing to the temporal space enlarged via Deconv operation, the information in the temporal space is expanded.</p><p>Specifically, the processing flow of T t , whose output can assign the attention weights W t ∈ R l×1 to each individual frames, can be defined as a composite function:</p><formula xml:id="formula_0">T t = δ t • S t • ψ t • E t ,<label>(1)</label></formula><p>where S t and E t are the squeezing and expanding operations, with the corresponding filter parameters M St and M Et , respectively. E t corresponds to the operation expanding the l-length input to the rl-length one, and S t keeps the input length unchanged. • denotes the composition operation over multiple functions, generating the compound function. We apply the nonlinear activation functions ψ t and δ t to further amplifying the differences among the temporal sequences. In our experiments, we choose ReLU and Sigmoid as the nonlinear activation functions ψ t and δ t , respectively.</p><p>2) Dimension Squeezing: The above temporal attention network module can differentiate the temporal variation and activate the informative part in the input sequence. However, since the input feature maps X t consists of multiple channels, learning the attention weights will be computationally expensive. Besides, since the above module processes each channel independently, it is unable to exploit the global temporal attention based on the contextual information among multiple channels. To alleviate this problem, we further employ a squeezing function P t to reduce the dimensions and leave only the temporal one, namely, transforming the input X t to Z t ∈ R l×1 that only holds the temporal dimension.</p><p>The function P t not only condenses the channel information, but also largely reduces the computation. In practice, it can be easily implemented by a convolutional layer (Conv1), which has c × h × w learnable parameters A t = {a j ∈ R h×w , j = 1, . . . , c}:</p><formula xml:id="formula_1">Z t (i) = c j=1 h m=1 w n=1 a j (m, n) • x i,j (m, n).</formula><p>(2)</p><p>3) Network Module: condenses them into the input frame-wise features in the temporal dimension. It behaves like the bottleneck structure <ref type="bibr" target="#b61">[62]</ref> and significantly reduces the computational cost. After the dimension squeezing, the remaining operations in Equation (1) only work in the temporal dimension.</p><p>Specifically, E t and S t in Equation ( <ref type="formula" target="#formula_0">1</ref>) respectively accomplish the expanding and squeezing operations, which can be implemented by the matrix multiplication in Deconv layer and Conv2 layer. The Deconv becomes quite useful and indispensable to enhance the temporal variance, when we place the temporal attention module at the higher layers of the deep 3D CNNs, because the input frame windows will be largely squeezed after a number of convolutions before the layer. The Conv2 layer keeps the temporal dimension the same as the input, and thus guarantees that our temporal attention module can be inserted into the 3D CNN architectures without network modification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Spatial Attention Module</head><p>The various channels in 3D CNN models contain the correlated spatial information, and discovering the spatial attention across them also benefits the learning of discriminative features for actions. Inspired by <ref type="bibr" target="#b53">[54]</ref>, we attempt to exploit the differences between different channels in 3D CNNs, expecting the success in 2D CNN field can be transferred into 3D CNN field.</p><p>1) Spatial Attention: Specifically, for the input sequence X s , our spatial attention network module applies the function T s to discriminate the meaningful channels and acquire the corresponding score for each channel. The attention extracting process includes one channel squeezing operation and one expanding operation. Specifically, we attempt to borrow the bottleneck structure, where the squeezing function S s compresses multiple channels into a correlated one by a deconvolution (Deconv), concentrating on the spatial information for the global view. Besides, to resume the channel dimension for module integration in the 3D CNNs, an expanding function E s based on convolution (Conv2) is further applied at the channel level. Such a bottleneck structure not only purifies the spatial information, but also reduces the computational complexity. Now we have the formal definition of the spatial attention function T s :</p><formula xml:id="formula_2">T s = δ s • E s • ψ s • S s ,<label>(3)</label></formula><p>where the nonlinear functions δ s and ψ s are also necessary in a channel-wise attention mechanism for enhancing the representational power. We choose Sigmoid and ReLU as δ s and ψ s , respectively. Similar to the temporal attention module, E s and S s are the expanding and squeezing operations, with the corresponding filter parameters M Es and M Ss , respectively. By inputting a number of action video sequence data X s , the attention module T s will learn the attention weight W s for all channels.</p><p>2) Dimension Squeezing: Similar to temporal attention, we prefer to first reducing the dimensions involved in the 3D convolutions and squeeze both the temporal dimension and the feature map in each channel, remaining the channel dimension only. This operation makes the network module mainly focus on the spatial content among different channels, without the side effect of the other dimensions. Similarly, we have the dimension squeezing function P s for spatial attention, transforming the input X s into one dimensional sequence Z s ∈ R c×1 for T s to discriminate the most informative channels. This can be implemented by a convolution with c × l × h × w × c learnable parameters</p><formula xml:id="formula_3">A s = {a k ∈ R l×c×h×w , k = 1, . . . , c}: Z s (k) = l i=1 c j=1 h m=1 w n=1 a k (i, j, m, n) • x i,j (m, n). (4)</formula><p>3) Network Module: Figure <ref type="figure" target="#fig_2">3</ref>(b) shows the whole structure of the spatial module. Conv1 and Deconv layers together accomplish the compression of the feature map, temporal dimension and the channels. Namely, we squeeze all the other dimensions using P s except the channel dimension in Conv1 layer, and exploit the bottleneck structure to reduce the computational complexity using S s in Deconv layer. Conv2 layer further helps completing the channel resumption with more discriminative information at the channel level (i.e., E s ). In practice, to reduce both the memory cost of GPU and the cost of multiplication computation, we can simply merge the squeezing channel operation S s of Deconv into Conv1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Spatio-Temporal Attention</head><p>We can put together the temporal and the spatial attention modules to constitute the spatio-temporal attention (STA) module for 3D CNN models. The STA module can simultaneously pursue the corresponding frame-wise and channel-wise attention weights, from the input temporal sliding window at a specified layer of the 3D CNNs.</p><p>1) Spatio-Temporal Composition: Traditional 3D CNN models apply the 3D convolutional kernel on the local receptive field, and thus lack the capability of acquiring the contextual information in the feature map.</p><p>To address this problem, in the STA module we first mix the squeezing operations along temporal and channel dimensions. This process can fully utilize the whole feature maps to capture the temporal and spatial attention, which at the same time can largely reduce the computational cost. We define a dimension squeezing function P as the Cartesian product of the squeezing functions in the spatial and temporal attention modules:</p><formula xml:id="formula_4">P = P s × P t ,<label>(5)</label></formula><p>transforming the input X into a two dimensional matrix. Later we will show that this actually corresponds to a convolution operation.</p><p>Based on the input squeezing, now we can have the transformation T for our STA module. The transformation consists of both the linear compression/expansion operations and the nonlinear activation to increase the model capacity:</p><formula xml:id="formula_5">T = δ • (S t • E s ) • ψ • (E t • S s ) • P,<label>(6)</label></formula><p>where δ and ψ are the nonlinear activation functions, and we adopt Sigmoid and ReLU respectively. In our experiments, to reduce the number of multiplication computation and learnable parameters, we move the operation S s , squeezing channel dimension, from Deconv to Conv1 in Figure <ref type="figure" target="#fig_1">2</ref>. Finally, for the input temporal sliding window features X, our STA module learns the spatio-temporal attention weight matrix W to represent X by Y as follows:</p><formula xml:id="formula_6">y i,j = w i,j x i,j .<label>(7)</label></formula><p>2) Network Module: Corresponding to Equation ( <ref type="formula" target="#formula_5">6</ref>), there are two convolutional layers (Conv1 and Conv2), one deconvolutional layer (Deconv) and two activation functions. Figure <ref type="figure" target="#fig_1">2</ref> shows the components of our STA integrating both the frame-wise and channel-wise attention. Usually we can simply place the module into the 3D CNN architecture, and feed it with the frame-wise features. Then Conv1 accomplishes P operation, i.e., the compression and isolation of the spatial information from each channel, only leaving the temporal dimension and channel dimension with the global spatial information. Then corresponding to t • S s , the Deconv layer is appended to increase the learning capacity of the module in the inserted layer. Finally, we add another convolutional layer Conv2 to resume both the temporal length and the channel number as S t • E s does.  In Figure <ref type="figure" target="#fig_3">4</ref>, we also give the examples of our STA in two state-of-the-art 3D CNN architectures: ResNeXt-101 and ResNet-152.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we will extensively evaluate our spatiotemporal attention (STA) network model in the tasks of video recognition and detection over several widely-used video datasets. Besides, we will compare our method with a number of state-of-the-art video recognition and detection methods including the two-stream CNN and 3D CNN methods. Furthermore, we conduct some extensive experiments to analyze and discuss our STA module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>We adopt the three challenging datasets UCF-101 <ref type="bibr" target="#b71">[72]</ref>, HMDB-51 <ref type="bibr" target="#b72">[73]</ref> and THUMOS 2014 <ref type="bibr" target="#b73">[74]</ref> for video action recognition and detection. UCF-101 dataset consists of 13,320 videos from 101 realistic action categories, such as "Diving" and "Walking with a dog", partitioned into three splits for training and testing. HMDB-51 dataset contains 7,000 video clips distributed in 51 action classes, which also has three splits as UCF-101. The above two datasets are widely used to evaluate the performance of action recognition algorithms. For action detection task, as in <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b74">[75]</ref>, <ref type="bibr" target="#b75">[76]</ref>, we choose the THUMOS 2014 dataset consisting of 13,320 videos for training, 1,010 videos for  <ref type="bibr" target="#b63">[64]</ref> 95.9 73.5 STM <ref type="bibr" target="#b64">[65]</ref> 96.2 72.2 C3D <ref type="bibr" target="#b0">[1]</ref> 77.4 46.7 P3D-199 <ref type="bibr" target="#b17">[18]</ref> 89.2 62.9 RGB Only + 3D CNNs STC-ResNet-101(64f) <ref type="bibr" target="#b15">[16]</ref> 93.7 66.8 Nonlocal-ResNeXt-101(64f) <ref type="bibr" target="#b57">[58]</ref> 95.0 72.4 I3D RGB + DMC-Net (I3D) <ref type="bibr" target="#b65">[66]</ref> 96.5 77.8 ResNeXt-101(16f) <ref type="bibr" target="#b6">[7]</ref> 90 validation, and 1,574 videos for testing from 20 action categories. Unlike UCF-101 and HMDB-51, the videos in THUMOS 2014 are untrimmed. The irrelevant frames in these untrimmed videos make the detection more difficult.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>Our STA module can be easily integrated into the existing 3D CNNs frameworks. In our experiments, we will plug it into a number of popular architectures for action recognition and detection, including ResNeXt-101(16f), ResNeXt-101(64f), ResNet-18(16f) and ResNet-152(16f) with different frame lengths such as 16 frames (16f) and 64 frames (64f). Since the receptive field in their later convolutional layers is relatively wider and more semantically meaningful for these powerful deep models, we simply prefer to place our STA module at their later convolutional layers so that the model can well capture the spatial and temporal correlations. In all the experiments, our model will be finetuned based on the well-trained raw 3D CNN models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Action Recognition and Detection Performance</head><p>In our experiments, we first investigate the performance of our model in the tasks of action recognition and detection.</p><p>1) Action Recognition: We evaluate the action recognition performance on UCF-101 and HMDB-51 datasets. Table I reports the top-1 recognition accuracy. We compare our model with a number of state-of-the-art action recognition methods, including the two-stream models <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b69">[70]</ref>, <ref type="bibr" target="#b70">[71]</ref>, the 3D CNN models <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b17">[18]</ref>,   <ref type="bibr" target="#b65">[66]</ref> and other very recent models <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b64">[65]</ref> extracting the temporal features. The results on each dataset are averaged over the three splits. For our method, we simply adopt and embed our module into ResNeXt-101 architecture, which is fine-tuned based on the published well-trained models in <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>.</p><p>From Table <ref type="table" target="#tab_3">I</ref> we can easily observe that the basic 3D CNN models (C3D <ref type="bibr" target="#b0">[1]</ref> and P3D-199 <ref type="bibr" target="#b17">[18]</ref>), only relying </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>METHOD</head><p># LAYERS # PARAM # MULT STC <ref type="bibr" target="#b15">[16]</ref> 10 on the 3D convolutions without considering the spatiotemporal attentions, can hardly beat the others. Among all the methods, ResNeXt-101(64f), i.e., using 3D convolution with 64 frames per temporal sliding window, can obtain a very satisfying performance. But with the help of our STA module, STA-ResNeXt-101(64f) can faithfully obtain almost the best performance among RGB only methods, with encouraging performance gains based on the discriminating information at both frame level and channel level. The performance gain over HMDB-51 dataset is more obvious, increasing from the original 70.2% obtained by ResNeXt-101(64f) to 74.1%. Moreover, we insert our STA module into MARS <ref type="bibr" target="#b4">[5]</ref> model, which is the state-of-the-art model using optical flow for action recognition with published source codes 1 As shown in Table <ref type="table" target="#tab_3">I</ref>, MARS embedding our STA module, obtains improved performance, especially, "STA-MARS+RGB+Flow" obtains the state-of-theart performance, 98.4 and 81.4 on UCF-101 and HMDB-51 datasets respectively.</p><formula xml:id="formula_7">c 2 × (l 2 + l + 2)/r ≈ O(c 2 ) c × (l + 3) + c 2 × (l 2 + l + 2)/r ≈ O(c 2 ) STA 5 c × (h × w + 4) + 5 ≈ O(c) c × l × (h × w + 4) + 3 × l ≈ O(c)</formula><p>In Table <ref type="table" target="#tab_3">II</ref>, we further investigate the performance over the three splits of UCF-101 and HMDB-51 using the ResNeXt-101 and MARS architectures. It is obvious that equipped with our STA module, STA-ResNeXt-101 can obtain improved performance, compared with the original ResNeXt-101 network on each split, without too much additional computational cost. Furthermore, "STA-MARS+RGB+Flow" obtains the best performance for every split. Table <ref type="table" target="#tab_3">III</ref> reports the model complexity with or without STA module in ResNeXt-101 network, where the STA module only brings a slight increase (5.2%) in model size compared to the base one.</p><p>Additionally, we compare the complexity between STA and STC modules in Table <ref type="table" target="#tab_7">IV</ref>. The state-of-the-art method STC needs 10 layers but our STA only needs 5 layers to model the spatio-temporal attention mechanism for video action analysis. Since c is usually larger (e.g., ResNeXt layer4: c=1024) than l, h and w (e.g., ResNeXt-101(64f) layer4: l=8, h=7, and w=7, respectively), the parameters and multiplication complexity in our STA module are O(c), while those of STC are O(c<ref type="foot" target="#foot_1">2</ref> ). We can see that our STA module is much more computational efficient than STC. To further demonstrate the performance improvement when applying our STA module, we also adopt another popular network architecture C3D, P3D-199 and ResNet-152, and compare the performance on the split1 of UCF-101 and HMDB-51 respectively. For C3D and P3D-199, we adopted the pretrained models and code (C3D 2 is pretrained on Sports-1M; P3D-199<ref type="foot" target="#foot_2">3</ref> is pretrained on Kinetics600) and then fine-tuned our STA based on the original models. As Table <ref type="table">V</ref> shows, both STA-C3D and STA-P3D-199 achieve better performance than the base networks C3D and P3D-199. Besides, we can also find the performance improvements on HMDB-51 dataset are larger than those on UCF-101. This is mainly due to the fact that the average video length in UCF-101 dataset is longer than that in HMDB-51 dataset.</p><p>Figure <ref type="figure" target="#fig_4">5</ref> shows the accuracy performance of STA-ResNet-152 and ResNet-152. Again, we observe that our STA module embedded in the 3D CNNs can help improve the performance, as it simultaneously considers both the spatial and the temporal attention in learning good features for video action. The experimental results over different 3D CNN architectures including ResNeXt-101 and ResNet-152 also show that STA is a generic and flexible module in 2) Action Detection: Besides action recognition, we also study the performance of our STA module in the action detection task. In this experiment, we compare our model with the state-of-the-art method S-CNN <ref type="bibr" target="#b13">[14]</ref>. The evaluation is conducted in the provided well-trained proposal network and localization network. We insert our module in the localization network based on ResNet-18 architecture. To fine-tune our STA 3D CNN model, we decompress UCF-101 and THUMOS 2014 videos into frames at 25 frames-per-second (fps). For UCF-101, we select temporal sliding windows, whose Intersection-over-Union (IoU) with groundtruth instance is larger than 0.5, as our training samples, and set its IoU as the measurement of overlap in the experiment.</p><p>The mean average precision (mAP) results are shown in Table <ref type="table" target="#tab_9">VI</ref>. From the table, we can see that our STA-S-CNN consistently outperforms the original S-CNN <ref type="bibr" target="#b13">[14]</ref> with respect to different IoU settings. Especially when IoU is lower, the performance gain becomes more significant. This phenomenon indicates that our STA module, capable of distinguishing the frames in the temporal dimension, can explore the most informative ones, even when they fall in the overlap region between the predicted sliding window and the groundtruth. Subsequently, it can robustly boost the detection accuracy under extreme scenarios.</p><p>Figure <ref type="figure" target="#fig_5">6</ref> demonstrates this observation by two action detection examples on THUMOS 2014. In the two examples, both STA-S-CNN and S-CNN models attempt to detect the action "Javalin Throw" and "Basketball Dunk" respectively, and the overlaps are set between 0.1 and 0.2 during the evaluation. For action "Javalin Throw", we can see that S-CNN fails to correctly detect the action, but STA-S-CNN can find the most important frames (i.e., from frame #14480 to #14527) with strong temporal attention for the action, and thus is able to generate the correct prediction. A similar observation can be made in the example of action "Basketball Dunk" detection.</p><p>To further demonstrate the effectiveness of our STA module, we also insert our STA module into the R-C3D <ref type="bibr" target="#b29">[30]</ref> model (named STA-R-C3D). For both R-C3D and STA-R-C3D, we choose the same pretrained model and code for training (R-C3D<ref type="foot" target="#foot_3">4</ref> is pretrained on Sports-1M). As Table <ref type="table" target="#tab_9">VI</ref> shows, compared to original R-C3D model, our STA-R-C3D can get higher mAP performance, which is similar to the performance when using S-CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Model Analysis and Discussion</head><p>In this section, we will extensively study the proposed STA module, including the effect of attention mechanism, the parameter setting, and the place to plug it in.</p><p>1) Ablation Analysis: First we check the effect of the spatial attention (SA), temporal attention (TA) and spatio-temporal attention (STA) respectively. In this experiment, we adopt the base model ResNext-101(64f), and form three 3D CNN models with different attention parts, namely SA-ResNext-101(64f), TA-ResNext-101(64f) and  Besides, the two attention modules combined (i.e., our STA) can obtain further performance gains with more spatio-temporal feature learning power. Additionally, we also show the performance for SENet because its behavior similar to our SA module. However, our SA-ResNeXt-101(64f) model can obtain better performance compared against SENet-ResNeXt-101(64f) on both datasets.</p><p>2) Embedded Layer: As mentioned previously, we usually prefer to placing the STA module at the later convolutional layers of the 3D CNN models, since STA can capture temporal and spatial attentions at the high level. To validate this, Table <ref type="table" target="#tab_9">VIII</ref> shows the difference when placing the STA module at the early layers (stage conv2) and the later layers (stage conv5) of ResNext-101. On both UCF-101 and HMDB-51, STA embedded into the later layers can obtain the best performance. This is consistent with our intuition that later layers of the deep networks provide more semantic and meaningful information in temporal and spatial dimensions. This means that in practice it is more necessary to capture the attentions and then determine the beneficial frames and channels at the later layers.</p><p>3) Temporal Deconvolution &amp; Step Size: In standard 3D CNN models, the convolution operation along the temporal dimension will compress the video sequence and inevitably lose the valuable information for action recognition. This is more critical at the later layers where the temporal dimension has been largely transformed and squeezed. To alleviate this issue, a deconvolution (Deconv) operation is introduced in our STA module, which will stretch the temporal length and enhance the feature representing capacity.</p><p>We evaluate the performance of the STA-ResNeXt-101(64f) network with or without the Deconv layer.    UCF-101 and HMDB-51. It is easy to see that using the temporal deconvolution can enlarge the learning capacity of our STA module, and thus increase the recognition performance.</p><p>In Figure <ref type="figure">7</ref> we also investigate the effect of the temporal step size in the Deconv operation. On both datasets, we can see that the small step size (i.e., 1) gives the best performance. This is because the larger step size indicates an undesirable stretching transformation along the temporal dimension, which might bring more distortions among the frames for recognition. Therefore, in practice a proper small step size for deconvolution is preferred for a satisfying performance without too much computations. In our experiments setting, we choose 1 and 3 for step size and kernel size respectively to accomplish up-sampling operation.</p><p>4) Sliding Window Size: The STA module works over an input window to capture the temporal attentions. Therefore, it is important to see the connections between the window size and the performance of STA module. In Table <ref type="table" target="#tab_14">X</ref> we compare the performance of ResNeXt-101 model using different window sizes including 16 and 64 respectively. It is obvious that with a large input window, our STA module can provide much better performance on both UCF-101 and HMDB-51 datasets. This is because the long input frame sequence conveys more temporal information. Subsequently, it is easier for STA to capture the frame-wise attention, and find the most meaningful frames characterizing the action. 5) Video Length: In Figure <ref type="figure" target="#fig_7">8</ref> (a) and (b) we show the statistics of the video length (i.e., the number of frames decompressed with 30 fps) in HMDB-51 and UCF-101 datasets. It is easy to see that the video length of HMDB-51 varying mainly between 20 and 120, is very different from that of UCF-101 ranging from 60 to 320. This means that on the average the video in UCF-101 is longer than that in HMDB-51. Furthermore, we also investigate the performance using our STA model on videos of different length from both datasets. Figure <ref type="figure" target="#fig_7">8</ref> also shows the accuracy curves, where the input sliding window length of our STA model is set to 64 frames. On both datasets, we can conclude that the proposed STA model can work better on short videos than on longer ones. This is mainly because by setting a proper sliding window length, STA can find the most meaningful frames characterizing the action in the input window. This helps explain why in Table I in most cases the performance improvement on HMDB-51 is larger than that on UCF-101.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In this paper we proposed a novel spatio-temporal attention (STA) module to improve 3D CNNs for action recog-1520-9210 (c) 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. nition and detection, which attempts to exploit the discriminative information at both frame level and channel level. Comprised of a number of convolution and deconvolution operations, our STA module simultaneously distinguishes the characteristics in temporal and spatial dimensions, and further improves the capability of the 3D CNNs with more powerful feature learning. The proposed module serves as a generic module for many 3D CNNs without increasing too much computational cost. Our experiments on several state-of-the-art 3D CNNs architectures and three different datasets have demonstrated that the STA module can obtain the state-of-the-art performance in action recognition task (98.4% and 81.4% on UCF-101 and HMDB-51 datasets respectively), and achieve improved performance for action detection task.</p><p>1520-9210 (c) 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2020.2965434, IEEE Transactions on Multimedia</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Spatio-temporal attention of STA-ResNeXt-101(64f) on "Jump" action from HMDB-51 dataset. The top row shows the successive frames in the video and the corresponding temporal attention weights learnt by our STA module, where the frames with the most discriminative features for "Jump" action can be identified with the larger weights. The bottom row shows the feature maps (channels) for the specified frame and their spatial attention weights, where the more informative feature maps play more important roles in the action recognition and detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The architecture of our STA network module embedded in the 3D CNN model. It consists of two convolutional layers (Conv1 and Conv2), one deconvolutional layer (Deconv), and two active functions (ReLU and Sigmoid). Our STA module outputs W that weights the input feature maps ( denotes the element-wise multiplication between the input feature map and the weights).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. The design details of the temporal attention and spatial attention network modules. Both the temporal attention module and the spatial attention module consist of two convolutional layers (Conv1 and Conv2) and one deconvolutional layer (Deconv), which respectively weight the input feature maps at the frame level and the channel level (⊗ denotes the matrix multiplication).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. STA examples in ResNeXt-101 and ResNet-152: (a) a block of STA-ResNeXt-101 with cardinality = 32, (b) a block of STA-ResNet-152. We add our STA module at the bottom of the blocks (⊕ denotes the element-wise sum).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Accuracy performance of STA-ResNet-152 and ResNet-152 on split1 of UCF-101 and HMDB-51.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Illustrative examples showing the action detection result when the overlap threshold is lower. The overlap threshold is set between 0.1 and 0.2 during evaluation. Detection results for two action instances ("Javalin Throw" and "Basketball Dunk") from THUMOS 2014 dataset: two correct predictions are obtained by STA-S-CNN model, while neither is correct by the original S-CNN model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>51 Fig. 7 .</head><label>517</label><figDesc>Fig. 7. The effect of different temporal step sizes in Deconv layer. (a) Top-1 accuracy performance for different step sizes of STA-ResNeXt-101 on UCF-101 split1. (b) Top-1 accuracy performance for different step sizes of STA-ResNeXt-101 on HMDB-51 split1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Performance comparison between ResNeXt with STA and without STA on HMDB-51 and UCF-101 datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, Hara et al. presented an explicit analysis on various 3D CNNs, and concluded that the deep 3D CNNs together with Kinetics [19] can retrace the successful results of 2D CNNs and ImageNet. Since the 3D CNNs heavily rely on expensive computation and memory cost, Qiu et al. introduced a surrogate architecture named Pseudo-3D 1520-9210 (c) 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2020.2965434, IEEE Transactions on Multimedia</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I TOP</head><label>I</label><figDesc>-1 ACCURACY PERFORMANCE ON UCF-101 AND HMDB-51 COMPARED WITH STATE-OF-THE-ART METHODS.</figDesc><table><row><cell>TYPE</cell><cell>METHOD</cell><cell>UCF-101</cell><cell>HMDB-51</cell></row><row><cell></cell><cell>Girdhar et al. [56]</cell><cell>-</cell><cell>52.2</cell></row><row><cell></cell><cell>ST Multiplier Net [63]</cell><cell>94.2</cell><cell>68.9</cell></row><row><cell>RGB Only + Others</cell><cell>Meng et al. [48]</cell><cell>87.1</cell><cell>53.1</cell></row><row><cell></cell><cell>TSM+Offline</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IV COMPLEXITY</head><label>IV</label><figDesc>COMPARISON BETWEEN STC AND STA, INCLUDING THE TOTAL NUMBER OF LAYERS (# LAYERS), PARAMETERS (# PARAM) AND MULTIPLICATIONS(# MULT).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VI MAP</head><label>VI</label><figDesc>PERFORMANCE USING OUR STA MODULE OVER 3D CNNS INCLUDING S-CNN AND R-C3D ON THUMOS 2014.</figDesc><table><row><cell>METHODS</cell><cell>0.1</cell><cell>0.2</cell><cell>IOU 0.3</cell><cell>0.4</cell><cell>0.5</cell></row><row><cell>S-CNN [14]</cell><cell>45.7</cell><cell>42.9</cell><cell>37.8</cell><cell>27.5</cell><cell>18.4</cell></row><row><cell>STA-S-CNN</cell><cell>46.8</cell><cell>43.9</cell><cell>38.2</cell><cell>28.2</cell><cell>18.7</cell></row><row><cell>R-C3D [30]</cell><cell>55.2</cell><cell>55.1</cell><cell>52.8</cell><cell>45.9</cell><cell>34.8</cell></row><row><cell>STA-R-C3D</cell><cell>56.6</cell><cell>56.4</cell><cell></cell><cell>47.5</cell><cell>36.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>STA-ResNext-101(64f). We evaluate their performance on split1 of UCF-101 and HMDB-51 datasets, and report the best accuracy they obtained in TableVII. From the table, we can see that both our spatial attention and temporal attention can help improve the recognition accuracy, owing to the fact that the mechanisms can exploit the most discriminative information at frame level or channel level respectively.</figDesc><table><row><cell cols="2">TABLE VIII</cell><cell></cell></row><row><cell cols="3">TOP-1 ACCURACY PERFORMANCE WHEN EMBEDDING STA MODULE</cell></row><row><cell cols="3">INTO DIFFERENT LAYERS, I.E., THE EARLY LAYERS (EL) AND THE</cell></row><row><cell cols="3">LATER LAYERS(LL), OF RESNEXT-101.</cell></row><row><cell>METHOD</cell><cell>UCF-101</cell><cell>HMDB-51</cell></row><row><cell>ResNeXt-101(64f)</cell><cell>94.0</cell><cell>71.2</cell></row><row><cell>STA-ResNeXt-101(64f)-EL</cell><cell>94.5</cell><cell>73.0</cell></row><row><cell>STA-ResNeXt-101(64f)-LL</cell><cell>95.0</cell><cell>75.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Table IX lists the accuracies of the two settings on the split1 of</figDesc><table><row><cell>…</cell><cell></cell><cell></cell><cell></cell><cell>…</cell><cell>…</cell></row><row><cell>weight:</cell><cell>0.49</cell><cell>0.483</cell><cell>0.495</cell><cell>0.496</cell></row><row><cell>#14400</cell><cell></cell><cell cols="2">Predict Sliding Window : JavalinThrow</cell><cell>#14527</cell></row><row><cell></cell><cell></cell><cell>#14480</cell><cell></cell><cell>Ground Truth: JavalinThrow</cell><cell>#14665</cell></row><row><cell>…</cell><cell></cell><cell></cell><cell></cell><cell>…</cell><cell>…</cell></row><row><cell>weight:</cell><cell>0.502</cell><cell>0.504</cell><cell>0.5</cell><cell>0.51</cell></row><row><cell>#4993</cell><cell></cell><cell cols="2">Predict Sliding Window: BasketballDunk</cell><cell>#5056</cell></row><row><cell></cell><cell></cell><cell></cell><cell>#5042</cell><cell>Ground Truth: BasketballDunk</cell><cell>#5073</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE X THE</head><label>X</label><figDesc>EFFECT OF THE SLIDING WINDOW SIZE ON SPLIT1 OF UCF-101 AND HMDB-51.</figDesc><table><row><cell>METHOD</cell><cell>UCF-101</cell><cell>HMDB-51</cell></row><row><cell>ResNeXt-101(16f)</cell><cell>90.1</cell><cell>64.1</cell></row><row><cell>STA-ResNeXt-101(16f)</cell><cell>90.7</cell><cell>64.6</cell></row><row><cell>STA-ResNeXt-101(64f)</cell><cell>95.0</cell><cell>75.0</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Authorized licensed use limited to: University of Canberra. Downloaded on April 30,2020 at 21:43:04 UTC from IEEE Xplore. Restrictions apply.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://github.com/jfzhang95/pytorch-video-recognition</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://github.com/qijiezhao/pseudo-3d-pytorch</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://github.com/sunnyxiaohu/R-C3D.pytorch</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by National Natural Science Foundation of China (61690202, 61872021), Fundamental Research Funds for Central Universities (YWF-19-BJ-J-271), Beijing Municipal Science and Technology Commission (Z171100000117022), and State Key Lab of Software Development Environment (SKLSDE-2018ZX-04).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3D convolutional networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exploiting mid-level semantics for large-scale complex video classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2518" to="2530" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Video imprint segmentation for temporal action detection in untrimmed videos</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>in the Association for the Advance of Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos using action pattern trees</title>
		<author>
			<persName><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="717" to="730" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">MARS: Motion-augmented rgb stream for action recognition</title>
		<author>
			<persName><forename type="first">Nieves</forename><surname>Crasto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.11248</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3D CNNs retrace the history of 2D CNNs and imagenet</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sequential deep trajectory descriptor for action recognition with three-stream CNN</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1510" to="1520" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Endto-end learning of motion representation for video understanding</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Human action recognition using a modified convolutional neural network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Neural Networks</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3D convolutional neural networks for human action recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage CNNs</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spatio-temporal channel correlation networks for action classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yousefzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Two-stream 3-D convnet fusion for action recognition in videos with arbitrary size and length</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="634" to="644" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3D residual networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">MiCT: Mixed 3D/2D convolutional tube for human action recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Real-time action recognition with enhanced motion vector CNNs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Appearance-and-relation networks for video classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09125</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The dynamic representation of scenes</title>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">A</forename><surname>Rensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Cognition</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="17" to="42" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Action recognition using visual attention</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04119</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Temporal action localization by structured maximal sums</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Stroud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Turn tap: Temporal unit regression network for temporal action proposals</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ctap: Complementary temporal action proposal generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rethinking the faster R-CNN architecture for temporal action localization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Temporal gaussian mixture layer for videos</title>
		<author>
			<persName><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ryoo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.06316</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">R-C3D: Region convolutional 3D network for temporal activity detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">RGB-D-based human motion recognition with deep learning: A survey</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ogunbona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">171</biblScope>
			<biblScope unit="page" from="118" to="139" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>The lear submission at thumos 2014</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fast saliency based pooling of fisher encoded dense trajectories</title>
		<author>
			<persName><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Seidenari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision THUMOS Workshop</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">End-to-end learning of action detection from frame glimpses in videos</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Real-time temporal action localization in untrimmed videos by sub-action discovery</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A proposal-based solution to spatio-temporal action detection in untrimmed videos</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gleason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schwarcz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Action recognition and detection by combining motion and appearance features</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">THUMOS14 Action Recognition Challenge</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Action sets: Weakly supervised action segmentation without ordering constraints</title>
		<author>
			<persName><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Unsupervised learning of long-term motion dynamics for videos</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.01821</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unsupervised action discovery and localization in videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-region two-stream R-CNN for action detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Convolutional twostream network fusion for video action recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Hidden twostream convolutional networks for action recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00389</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">CDC: convolutional-de-convolutional networks for precise temporal action localization in untrimmed videos</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Learning latent subevents in activity videos using temporal attention filters</title>
		<author>
			<persName><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ryoo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>in the Association for the Advance of Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Interpretable spatio-temporal attention for video action recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning unsupervised video object segmentation through visual attention</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Hierarchical self-attention network for action localization in videos</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pramono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.00685</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Residual attention network for image classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06904</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01507</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">CBAM: Convolutional block attention module</title>
		<author>
			<persName><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Attentional pooling for action recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Attention clusters: Purely attention based local feature integration for video classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Content-attention representation by factorized action-scene network for action recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1537" to="1547" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Computational visual attention</title>
		<author>
			<persName><forename type="first">S</forename><surname>Frintrop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Analysis of Human Behavior</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="69" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Spatiotemporal multiplier networks for video action recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">TSM: Temporal shift module for efficient video understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">STM: Spatiotemporal and motion encoding for action recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">DMC-Net: Generating discriminative motion cues for fast compressed video action recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Action recognition with trajectorypooled deep-convolutional descriptors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">PAN: Persistent appearance network with an efficient motion cue for fast action recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Representation flow for action recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">HACS: Human action clips and segments dataset for recognition and temporal localization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">HMDB: A large video database for human motion recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">THUMOS challenge: Action recognition with a large number of classes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://crcv.ucf.edu/THUMOS14/" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">A self-adaptive proposal model for temporal action detection based on reinforcement learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.07251</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
