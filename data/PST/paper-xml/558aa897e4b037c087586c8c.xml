<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Comparametric Equations with Practical Applications in Quantigraphic Image Processing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Steve</forename><surname>Mann</surname></persName>
						</author>
						<title level="a" type="main">Comparametric Equations with Practical Applications in Quantigraphic Image Processing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4BA4ED01F6596730AFA27B77987C9B70</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Comparametric equation</term>
					<term>comparametric plot</term>
					<term>image processing</term>
					<term>lightspace</term>
					<term>personal imaging</term>
					<term>photography</term>
					<term>quantigraphic imaging</term>
					<term>wearable cybernetics</term>
					<term>Wyckoff principle</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>It is argued that, hidden within the flow of signals from typical cameras, through image processing, to display media, is a homomorphic filter. While homomorphic filtering is often desirable, there are some occasions where it is not. Thus, cancellation of this implicit homomorphic filter is proposed, through the introduction of an antihomomorphic filter. This concept gives rise to the principle of quantigraphic image processing, wherein it is argued that most cameras can be modeled as an array of idealized light meters each linearly responsive to a semi-monotonic function of the quantity of light received, integrated over a fixed spectral response profile. This quantity is neither radiometric nor photometric, but, rather, depends only on the spectral response of the sensor elements in the camera. A particular class of functional equations, called comparametric equations, is introduced as a basis for quantigraphic image processing. Comparametric equations are fundamental to the analysis and processing of multiple images differing only in exposure. The well-known "gamma correction" of an image is presented as a simple example of a comparametric equation, for which it is shown that the underlying quantigraphic function does not pass through the origin. For this reason it is argued that exposure adjustment by gamma correction is inherently flawed, and alternatives are provided. These alternatives, when applied to a plurality of images that differ only in exposure, give rise to a new kind of processing in the "amplitude domain" (as opposed to the time domain or the frequency domain). While the theoretical framework presented in this paper originated within the field of wearable cybernetics (wearable photographic apparatus) in the 1970s and early 1980s, it is applicable to the processing of images from nearly all types of modern cameras, wearable or otherwise. This paper is a much revised draft of a 1992 peer-reviewed but unpublished report by the author, entitled "Lightspace and the Wyckoff principle.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T HE theory of quantigraphic image processing, with com- parametric equations, arose out of the field of wearable cybernetics, within the context of so-called mediated reality (MR) <ref type="bibr" target="#b0">[1]</ref> and personal imaging <ref type="bibr" target="#b1">[2]</ref>. However, it has potentially much more widespread applications in image processing than just the wearable photographic personal assistant for which it was developed. Accordingly, a general formulation that does not necessarily involve a wearable photographic system will be given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. WYCKOFF PRINCIPLE AND THE RANGE OF LIGHT</head><p>The quantity of light falling on an image sensor array, or the like, is a real valued function of two real variables and . An image is typically a degraded measurement of this function, where degredations may be divided into two categories, those that act on the domain and those that act on the range . Sampling, aliasing, and blurring act on the domain, while noise (including quantization noise) and the nonlinear response function of the camera act on the range .</p><p>Registering and combining multiple pictures of the same subject matter will often result in an improved image of greater definition. There are four classes of such improvement:</p><p>1) increased spatial resolution (domain resolution); 2) increased spatial extent (domain extent); 3) increased tonal fidelity (range resolution); 4) increased dynamic range (range extent).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. What is Good for the Domain is Good for the Range</head><p>The notion of producing a better picture by combining multiple input pictures has been well-studied with regards to the domain of these pictures. Horn and Schunk, for example, provide means of determining optical flow <ref type="bibr" target="#b2">[3]</ref>, and many researchers have then used this result to spatially register multiple images in order to provide a single image of increased spatial resolution and increased spatial extent. Subpixel registration methods such as those proposed by <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b4">[5]</ref> attempt to increase domain resolution. These methods depend on slight (subpixel) shift from one image to the next. Image compositing (mosaicking) methods such as those proposed by <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b7">[8]</ref> attempt to increase domain extent. These methods depend on large shifts from one image to the next.</p><p>Methods that are aimed at increasing domain resolution and domain extent tend to also improve tonal fidelity, to a limited extent, by virtue of a signal averaging and noise reducing effect. However, we shall see in what follows, a generalization of the concept of signal averaging called quantigraphic signal averaging. This generalized signal averaging allows images of different exposure to be combined to further improve upon tonal fidelity (range resolution), beyond improvements possible by traditional signal averaging. Moreover, the proposed methodology drastically increases dynamic range (range extent). Just as spatial shifts in the domain improve the image, we will also see how exposure shifts (shifts in the range, ) can, with the proposed methodology, result in even greater improvents to the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Extending Dynamic Range and Improvement of Range Resolution by Combining Differently Exposed Pictures of the Same Subject Matter</head><p>The principles of quantigraphic image processing and the notion of using differently exposed pictures of the same subject matter to make a picture composite of extended dynamic range was inspired by the pioneering work of Charles Wyckoff who invented so-called "extended response film" <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>.</p><p>Most everyday scenes have a far greater dynamic range than can be recorded on a photographic film or electronic imaging apparatus. However, a set of pictures, that are identical except for their exposure, collectively show us much more dynamic range than any single picture from that set, and also allow the camera's response function to be estimated, to within a single constant scalar unknown <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>.</p><p>A set of functions <ref type="bibr" target="#b0">(1)</ref> where are scalar constants, is known as a Wyckoff set <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b11">[12]</ref>. A Wyckoff set of functions, describes a set of images differing only in exposure, when is the continuous spatial coordinate of the focal plane of an electronic imaging array (or piece of film), is the quantity of light falling on the array (or film), and is the unknown nonlinearity of the camera's (or combined film's and scanner's) response function. Generally, is assumed to be a pointwise function, e.g., invariant to .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Photoquantity</head><p>The quantity, , in <ref type="bibr" target="#b0">(1)</ref>, is called the photoquantigraphic quantity <ref type="bibr" target="#b12">[13]</ref>, or just the photoquantity (or photoq) for short. This quantity is neither radiometric (e.g. neither radiance nor irradiance) nor photometric (e.g. neither luminance nor illuminance). Most notably, since the camera will not necessarily have the same spectral response as the human eye, or, in particular, that of the photopic spectral luminous efficiency function as determined by the CIE and standardized in 1924, is neither brightness, lightness, luminance, nor illuminance. Instead, quantigraphic imaging measures the quantity of light integrated over the spectral response of the particular camera system <ref type="bibr" target="#b1">(2)</ref> where is the actual light falling on the image sensor and is the spectral sensitivity of an element of the sensor array. It is assumed that the spectral sensitivity does not vary across the sensor array.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Camera as an Array of Lightmeters</head><p>The quantity reads in units that are quantifiable (e.g. linearized or logarithmic), in much the same way that a photographic light meter measures in quantifiable (linear or logarithmic) units. However, just as the photographic light meter imparts to the measurement its own spectral response (e.g., a light meter using a selenium cell will impart the spectral response of selenium cells to the measurement) quantigraphic imaging accepts that there will be a particular spectral response of the camera, which will define the quantigraphic unit . Each camera will typically have its own quantigraphic unit. In this way, the camera may be regarded as an array of lightmeters, each being responsive to the quantigral <ref type="bibr" target="#b2">(3)</ref> where is the spatially varying spectral distribution of light falling on the image sensor.</p><p>Thus, varying numbers of photons of lesser or greater energy (frequency times Planck's constant) are absorbed by a given element of the sensor array, and, over the temporal quantigration time of a single frame in the video sequence (or the exposure time of a still image) result in the photoquantity given by (3).</p><p>In the case of a color camera, or other color processes, is simply a vector quantity. Color images may arise from as little as two channels, as in the old bichromatic (orange and blue) motion pictures, but more typically arise from three channels, or sometimes more as in the four color offset printing, or even the high quality Hexachrome printing process. A typical color camera might, for example, include three channels, e.g., , , , where each component is derived from a separate spectral sensitivity function. Alternatively, another space such as YIQ, YUV, or the like, may be used, in which, for example, the Y (luminance) channel has full resolution and the U and V channels have reduced (e.g., half in each linear dimension giving rise to one quarter the number of pixels) spatial resolution and reduced quantizational definition. In this paper, the theory will be developed and explained for greyscale images, where it is understood that most images are color images, for which the procedures are applied either to the separate color channels, or by way of a multichannel quantigrahic analysis. Thus in both cases (greyscale or color) the continuous spectral information is lost through conversion to a single number or to typically three numbers, , , . Although it is easiest to apply the theory of this paper to color systems having distinct spectral bands, there is no reason why it cannot also be applied to more complicated polychromatic, possibly tensor, quantigrals.</p><p>Ordinarily cameras give rise to noise, e.g., there is noise from the sensor elements and further noise within the camera (or equivalently noise due to film grain and subsequent scanning of a film, etc.). Thus a goal of quantigraphic imaging is to attempt to estimate the photoquantity , in the presence of noise. Since is destroyed, the best we can do is to estimate . Thus is the fundamental or "atomic" unit of quantigraphic image processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Accidentally Discovered Compander</head><p>Most cameras do not provide an output that varies linearly with light input. Instead, most cameras contain a dynamic range compressor, as illustrated in Fig. <ref type="figure">1</ref>. Historically, the dynamic range compressor in video cameras arose because it was found that televisions did not produce a linear response to the video signal. In particular, it was found that early cathode ray screens provided a light output approximately equal to voltage raised to the exponent of 2.5. Rather than build a circuit into every Fig. <ref type="figure">1</ref>. Typical camera and display: light from subject matter passes through lens (typically approximated with simple algebraic projective geometry, e.g. an idealized "pinhole") and is quantified in units "q" by a sensor array where noise n is also added, to produce an output which is compressed in dynamic range by a typically unknown function f . Further noise n is introduced by the camera electronics, including quantization noise if the camera is a digital camera and compression noise if the camera produces a compressed output such as a JPEG image, giving rise to an output image f (x; y). The apparatus that converts light rays into f (x; y) is labeled CAMERA. The image f is transmitted or recorded and played back into a DISPLAY system where the dynamic range is expanded again. Most cathode ray tubes exhibit a nonlinear response to voltage, and this nonlinear response is the expander. The block labeled "expander" is generally a side effect of the display, and is not usually a separate device. It is depicted as a separate device simply for clarity. Typical print media also exhibit a nonlinear response that embodies an implicit "expander." television to compensate for this nonlinearity, a partial compensation (exponent of 1/2.22) was introduced into the television camera at much lesser total cost since there were far more televisions than television cameras in those days before widespread deployment of video surveillance cameras and the like. Indeed, the original model of television is suggested by the names of some of the early players: ABC (American Broadcasting Corporation); NBC (National Broadcasting Corporation); etc.. Names like this suggest that they envisioned a national infrastructure in which there would be one or two television cameras and millions of television receivers.</p><p>Through a very fortunate and amazing coincidence, the logarithmic response of human visual perception is approximately the same as the inverse of the response of a television tube (e.g. human visual response turns out to be approximately the same as the response of the television camera) <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. For this reason, processing done on typical video signals will be on a perceptually relevant tone scale. Moreover, any quantization on such a video signal (e.g. quantization into 8 bits) will be close to ideal in the sense that each step of the quantizer will have associated with it a roughly equal perceptual change in perceptual units.</p><p>Fig. <ref type="figure">2</ref> shows plots of the compressor (and expander) used in video systems together with the corresponding logarithm , and antilogarithm , plots of the human visual system and its inverse. (The plots have been normalized so that the scales match.)</p><p>With images in print media, there is a similarly expansive effect in which the ink from the dots bleeds and spreads out on the printed paper, such that the mid tones darken in the print. For this reason printed matter has a nonlinear response curve similar in shape to that of a cathode ray tube (e.g., the nonlinearity expands the dynamic range of the printed image). Thus cameras designed to capture images for display on video screens have approximately the same kind of built-in dynamic range compression suitable for print media as well.</p><p>It is interesting to compare this naturally occurring (and somewhat accidental) development in video and print media with the deliberate introduction of companders (compressors Fig. <ref type="figure">2</ref>. The power law dynamic range compression implemented inside most cameras has approximately the same shape of curve as the logarithmic function, over the range of signals typically used in video and still photography. Similarly, the power law response of typical cathode ray tubes, as well as that of typical print media, is quite similar to the antilog function. Therefore, the act of doing conventional linear filtering operations on images obtained from typical video cameras, or from still cameras taking pictures intended for typical print media, is, in effect, homomorphic filtering with an approximately logarithmic nonlinearity. and expanders) in audio. Both the accidentally occurring compression and expansion of picture signals and the deliberate use of logarithmic (or mu-law) compression and expansion of audio signals serve to allow 8 bits to be used to often encode these signals in a satisfactory manner. (Without dynamic range compression, 12 to 16 bits would be needed to obtain satisfactory reproduction.)</p><p>Most still cameras also provide dynamic range compression built into the camera. For example, the Kodak DCS-420 and DCS-460 cameras capture internally in 12 bits (per pixel per color) and then apply dynamic range compression, and finally output the range-compressed images in 8 bits (per pixel per color).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Why Stockham was Wrong</head><p>When video signals are processed, using linear filters, there is an implicit homomorphic filtering operation on the photoquantity. As should be evident from Fig. <ref type="figure">1</ref>, operations of storage, Fig. <ref type="figure">3</ref>. The anti-homomorphic filter: Two new elements f and f have been inserted, as compared to Fig. <ref type="figure">1</ref>. These are estimates of the the inverse and forward nonlinear response function of the camera. Estimates are required because the exact nonlinear response of a camera is generally not part of the camera specifications. (Many camera vendors do not even disclose this information if asked.) Because of noise in the signal f , and also because of noise in the estimate of the camera nonlinearity f , what we have at the output of f is not q, but, rather, an estimate, q. This signal is processed using linear filtering, and then the processed result is passed through the estimated camera response function, f , which returns it to a compressed tone scale suitable for viewing on a typical television, computer, or the like, or for further processing. transmission, and image processing take place between approximately reciprocal nonlinear functions of dynamic range compression and dynamic range expansion.</p><p>Many users of image processing methodology are unaware of this fact, because there is a common misconception that cameras produce a linear output, and that displays respond linearly. In fact there is a common misconception that nonlinearities in cameras and displays arise from defects and poor quality circuits, when in actual fact these nonlinearities are fortuitously present in display media and deliberately present in most cameras.</p><p>Thus, the effect of processing signals such as in Fig. <ref type="figure">1</ref> with linear filtering is, whether one is aware of it or not, homomorphic filtering.</p><p>Stockham advocated a kind of homomorphic filtering operation in which the logarithm of the input image was taken, followed by linear filtering (e.g. linear space invariant filters), followed by taking the antilogarithm <ref type="bibr" target="#b15">[16]</ref>.</p><p>In essence, what Stockham didn't appear to realize, is that such homomorphic filtering is already manifest in simply doing ordinary linear filtering on ordinary picture signals (whether from video, film, or otherwise). In particular, the compressor gives an image (ignoring noise and ) which has the approximate effect of (e.g., roughly the same shape of curve, and roughly the same effect, e.g., to brighten the mid-tones of the image prior to processing), as shown in Fig. <ref type="figure">2</ref>. Similarly a typical video display has the effect of undoing (approximately) this compression, e.g. darkening the mid-tones of the image after processing with . Thus in some sense what Stockham did, without really realizing it, was to apply dynamic range compression to already range compressed images, then do linear filtering, then apply dynamic range expansion to images being fed to already expansive display media.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. On the Value of Doing the Exact Opposite of What Stockham Advocated</head><p>There exist certain kinds of image processing for which it is preferable to operate linearly on the photoquantity . Such operations include sharpening of an image to undo the effect of the point spread function (PSF) blur of a lens. It is interesting to note that many textbooks and papers that describe image restoration (e.g. deblurring an image) fail to take into account the inherent nonlinearity deliberately built into most cameras.</p><p>What is needed to do this deblurring and other kinds of quantigraphic image processing is an anti-homomorphic filter. The manner in which an anti-homomorphic filter is inserted into the image processing path is shown in Fig. <ref type="figure">3</ref>.</p><p>Consider an image acquired through an imperfect lens that imparts a blurring to the image. The lens blurs the actual spatiospectral (spatially varying and spectrally varying) quantity of light , which is the quantity of light falling on the sensor array just prior to being measured by the sensor array <ref type="bibr" target="#b3">(4)</ref> This blurred spatiospectral quantity of light is then photoquantified by the sensor array <ref type="bibr" target="#b4">(5)</ref> which is just the blurred photoquantity .</p><p>Thus the antihomomorphic filter of Fig. <ref type="figure">3</ref> can be used to better undo the effect of lens blur than traditional linear filtering which simply applies linear operations to the signal and therefore operates homomorphically rather than linearly on the photoquantity .</p><p>Thus we see that in many practical situations, there is an articulable basis for doing exactly the opposite of what Stockham advocated (e.g., expanding the dynamic range of the image before processing and compressing it afterward as opposed to what Stockham advocated which was to compress the dynamic range before processing and expand it afterward).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Using Differently Exposed Pictures of the Same Subject Matter to Get a Better Estimate of</head><p>Because of the effects of noise (quantization noise, sensor noise, etc.), in practical imaging situations, the Wyckoff set that describes a plurality of pictures that differ only in exposure (1) should be rewritten <ref type="bibr" target="#b5">(6)</ref> where each image has associated with it a separate realization of a quantigraphic noise process and an image noise process which includes noise introduced by the electronics of the dynamic range compressor , and other electronics in the camera that affect the signal after its dynamic range has been compressed. In the case of a digital camera, also includes quantization noise (applied after the image has undergone dynamic range compression). Furthermore, in the case of a camera that produces a data-compressed output, such as the Kodak DC260 which produces JPEG images, also includes data-compression noise (JPEG artifacts, etc., which are also applied to the signal after it has undergone dynamic range compression). Refer again to Fig. <ref type="figure">1</ref>.</p><p>If it were not for noise, we could obtain the photoquantity from any one of a plurality of differently exposed pictures of the same subject matter, e.g. as <ref type="bibr" target="#b6">(7)</ref> where the existence of an inverse for follows from the semimonotonicity assumption. Semimonotonicity follows from the fact that we expect pixel values to either increase or stay the same with increasing quantity of light falling on the image sensor. <ref type="foot" target="#foot_1">1</ref> However, because of noise, we obtain an advantage by capturing multiple pictures that differ only in exposure. The dark ("underexposed") pictures show us highlight details of the scene that would have been overcome by noise (e.g., washed out) had the picture been "properly exposed." Similarly, the light pictures show us some shadow detail that would not have appeared above the noise threshold had the picture been "properly exposed."</p><p>Each image thus provides us with an estimate of the actual photoquantity <ref type="bibr" target="#b7">(8)</ref> where is the quantigraphic noise associated with image , and is the image noise for image . This estimate of , may be written <ref type="bibr" target="#b8">(9)</ref> where is the estimate of based on considering image , and is the estimate of the exposure of image based on considering a plurality of differently exposed images. The estimated is also typically based on an estimate of the camera response function , which is also based on considering a plurality of differently exposed images. Although we could just assume a generic function , in practice, varies from camera to camera. We can, however, make certain assumptions about that are reasonable for most cameras, such as the fact that does not decrease when is increased (that is semimonotonic), and that it is usually smooth, and that</p><p>. In what follows, it will be shown how and are estimated from multiple differently exposed pictures. For the time being, let us suppose that they have been successfully estimated, so that we can calculate from each of the input images . Such calculations, for each input image , give rise to a plurality of estimates of , which in theory would be identical, were it not for noise. However, in practice, because of noise, the estimates are each corrupted in different ways. Therefore it has been suggested, that multiple differently exposed images may be combined together to provide a single estimate of which can then be turned into an image of greater dynamic range, greater tonal resolution, and lesser noise <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b11">[12]</ref>. In particular, the criteria under which collective processing of multiple differently exposed images of the same subject matter will give rise to an output image which is acceptable at every point in the output image, are summarized below.</p><p>Wyckoff signal/noise criteria: , such that 1)</p><p>; 2) .</p><p>The first criterion indicates that for every pixel in the output image, at least one of the input images provides sufficient exposure at that pixel location to overcome sensor noise, . The second criterion states that of those at least one input images, at least one of them provides an exposure that falls favorably (e.g. is neither overexposed nor underexposed) on the response curve of the camera, so as not to be overcome by camera noise . The manner in which differently exposed images of the same subject matter are combined is illustrated, by way of an example involving three input images, in Fig. <ref type="figure" target="#fig_0">4</ref>.</p><p>Moreover, it has been shown <ref type="bibr" target="#b10">[11]</ref> that the constants as well as the unknown nonlinear response function of the camera can be determined, up to a single unknown scalar constant, given nothing more than two or more pictures of the same subject matter, in which the pictures differ only in exposure. Thus the reciprocal exposures used to tonally register (tonally align) the multiple input images are estimates, , in Fig. <ref type="figure" target="#fig_0">4</ref>. These exposure estimates are generally made by applying an estimation algorithm to the input images, either while simultaneously estimating , or as a separate estimation process (since only has to be estimated once for each camera but the exposure is estimated for every picture that is taken).</p><p>Owing to the large dynamic range that some Wyckoff sets can cover, small errors in tend to have adverse effects on the overall estimate . Thus it may be preferable to estimate as a separate process (e.g. by taking hundreds of exposures with the camera under computer program control). Once is known (previously measured), then can be estimated for a particular set of images. The first exposure (CAMERA set to exposure 1), gives rise to an exposure k q, the second to k q and the third to k q. Each exposure has a different realization of the same noise process associated with it, and the three noisy pictures that the camera provides are denoted f , f , and f . These three differently exposed pictures comprise a noisy Wyckoff set. In order to combine them into a single estimate, the effect of f is undone with an estimate, f that represents our best guess of what the function f is. While many video cameras use something close to the standard f = kq function, it is preferable to attempt to estimate f for the specific camera in use. Generally this estimate is made together with an estimate of the exposures k . After re-expanding the dynamic ranges with f , the inverse of the estimated exposures 1= k are applied. In this way, the darker images are made lighter and the lighter images are made darker so that they all (theoretically) match. At this point the images will all appear as if they were taken with identical exposure, except for the fact that the pictures that were brighter to start with will be noisy in lighter areas of the image and those that had been darker to start with will be noisy in dark areas of the image. Thus rather than simply applying ordinary signal averaging, a weighted average is taken. The weights are the spatially varying certainty functions, c (x; y). These certainty functions turn out to be the derivative of the camera response function shifted up or down by an amount k . In practice, since f is an estimate, so is c , so it is denoted Ä‰ in the figure. The weighted sum is q(x; y), the estimate of the photoquantity q(x; y). To view this quantity on a video display, it is first adjusted in exposure, and may be adjusted to a different exposure level than any of the exposure levels used in taking the input images. In this figure, for illustrative purposes, it is set to the estimated exposure of the first image, k . The result is then range-compressed with f for display on an expansive medium (DISPLAY).</p><p>The final estimate for , depicted in Fig. <ref type="figure" target="#fig_0">4</ref>, is given by <ref type="bibr" target="#b9">(10)</ref> where is given by <ref type="bibr" target="#b10">(11)</ref> from which we can see that are just shifted versions of , e.g. dilated versions of . The intuitive significance of the certainty function is that it captures the slope of the response function which indicates how quickly the output (pixel value or the like) of the camera varies for given input. In the case of a noisy camera, especially a digital camera where quantization noise is involved, generally the output of a camera will be most reliable where it is most sensitive to a fixed change in input light level. This point where the camera is most responsive to changes in input is at the peak of the certainty function . The peak in tends to be near the middle of the camera's exposure range. On the other hand, where the camera exposure input is extremely large or small (e.g. the sensor is very much overexposed or very much underexposed), the change in output for a given input is much less. Thus the output is not very responsive to the input and the change in output can be easily overcome by noise. Thus tends to fall off toward zero on either side of its peak value.</p><p>The certainty functions are functions of . We may also write the uncertainty functions, which are functions of pixel value in the image (e.g., functions of greyvalue in ), as <ref type="bibr" target="#b11">(12)</ref> and its reciprocal is the certainty function in the domain of the image (e.g., the certainty function in pixel coordinates) <ref type="bibr" target="#b12">(13)</ref> where</p><p>. Note that is the same for all images (e.g. for all values of image index ), whereas was defined separately for each image. For any , the function is a shifted (dilated) version of any other certainty function, , where the shift (dilation) depends on the log exposure, (the exposure ). The final estimate of ( <ref type="formula">10</ref>) is simply a weighted sum of the estimates from obtained from each of the input images, where each input image is weighted by the certainties in that image.</p><p>1) Exposure interpolation and extrapolation: The architecture of this process is shown in Fig. <ref type="figure">5</ref>, which depicts an image acquisition section (in this illustration, of three images), followed by an analysis section (to estimate ), followed by a resynthesis section to generate an image again at the output (in this case four different possible output images are shown).</p><p>The output image can look like any of the input images, but with improved signal to noise ratio, better tonal range, better color fidelity, etc. Moreover, an output image can be an interpolated or extrapolated version in which it is lighter or darker than any of the input images. It should be noted that this process of interpolation or extrapolation provides a new way of adjusting the tonal range of an image. The process is illustrated in Fig. <ref type="figure">5</ref>. The image synthesis portion may also include various kinds of deblurring operations, as well as other kinds of image sharpening and lateral inhibition filters to reduce the dynamic range of the output image without loss of fine details, so that it can be printed on paper or presented to an electronic display in such a way as to have optimal tonal definition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. COMPARAMETRIC IMAGE PROCESSING: COMPARING DIFFERENTLY EXPOSED IMAGES OF THE SAME SUBJECT MATTER</head><p>As previously mentioned, comparison of two or more differently exposed images may be done to determine , or simply to tonally register the images without determining . Also, as previously mentioned, tonal registration is more numerically stable than estimation of , so there are some advantages to comparametric analysis and comparametric image processing in which one of the images is selected as a reference image, and others are expressed in terms of this reference image, rather than in terms of . Typically the dark images are lightened, and/or the light images are darkened so that all the images match the selected reference image. Note that in such lightening and darkening operations, full precision is retained for further comparametric processing. Thus all but the reference image will be stored as an array of floating point numbers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Misconceptions About Gamma Correction</head><p>So-called gamma correction (raising the pixel values in an image to an exponent) is often used to lighten or darken images. While gamma correction does have important uses, such as lightening or darking images to compensate for incorrect display settings, it will now be shown that when one uses gamma correction to lighten or darken an image to compensate for incorrect exposure, that whether one is aware of it or not, one is making an unrealistic assumption about the camera response function.</p><p>Proposition III.1: Tonally registering differently exposed images of the same subject matter by gamma correcting them with exponent is equivalent to assuming that the nonlinear response function of the camera is . Proof: The process of gamma correcting an image may be written <ref type="bibr" target="#b13">(14)</ref> where is the original image, and is the lightened or darkened image. Solving for , the camera response function, we obtain <ref type="bibr" target="#b14">(15)</ref> We see that the response function <ref type="bibr" target="#b14">(15)</ref> does not pass through the origin, e.g.</p><p>, not zero. Since most cameras are designed so that they produce a signal level output of zero when the light input is zero, the function does not correspond to a realistic or reasonable camera response function. Even media which does not itself fall to zero at zero exposure (like film, for example) is ordinarily scanned in such a way that the scanned output is zero for zero exposure, assuming that the (minimum density for the particular emulsion being scanned) is properly set in the scanner. Therefore it is inappropriate and incorrect to use gamma correction to lighten or darken differently exposed images of the same subject matter, when the goal of this lightening or darkening is tonal registration (making them look the "same," apart from the effects of noise which will be accentuated in the shadow detail of the images that are lightened and the highlight detail of images that are darkened).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparametric Plots and Comparametric Equations</head><p>To understand the shortcomings of gamma correction, and to understand some alternatives, the concept of comparametric equations and comparametric plots will now be introduced.</p><p>Equation ( <ref type="formula">14</ref>) is an example of what is called a comparametric equation <ref type="bibr" target="#b16">[17]</ref>.</p><p>Comparametric equations are a special case of the more general class of equations called functional equations <ref type="bibr" target="#b17">[18]</ref> and comparametric plots are a special case of the more general class of plots called parametric plots.</p><p>The notion of a parametric plot is well-understood. For example, the parametric plot , is a plot of a circle of radius . Note that it does not depend explicity on , so long as the domain of includes at least all points on the interval from 0 to , modulo .</p><p>A comparametric plot is a special kind of parametric plot in which a function is plotted against itself, and in which the Fig. <ref type="figure">5</ref>. Quantigraphic image exposure adjustment on a Wyckoff set: Multiple (in this example, 3) differently exposed (in this example by two aperture stops) images are acquired. Estimates of q from each image are obtained. These are combined by weighted sum. The weights are the estimates of the certainty function shifted along the exposure axis by an amount given by the estimated exposure for each image. From the estimated photoquantity q, one or more output images may be generated by multiplying by the desired synthetic exposure and passing the result through the estimated camera nonlinearity. In this example, four synthetic pictures are generated. These are extrapolated and interpolated versions of the input exposures. The result is a "virtual camera" <ref type="bibr" target="#b16">[17]</ref> in which a picture can be generated as if the user were free to select the original exposure settings that had been used on the camera originally taking the input images.</p><p>parameterization of the ordinate is a linearly scaled parameterization of the abscissa.</p><p>More precisely, the comparametric plot is defined as follows: Definition III.1: A plot along coordinates is called a comparametric plot <ref type="bibr" target="#b16">[17]</ref>  The plot defines what is called a comparametric equation: Definition III.2: Equations of the form are called comparametric equations <ref type="bibr" target="#b16">[17]</ref>.</p><p>A better understanding of comparametric equations may be had by referring to the following diagram: <ref type="bibr" target="#b15">(16)</ref> wherein it is evident that there are two equivalent paths to follow from to <ref type="bibr" target="#b16">(17)</ref> Equation ( <ref type="formula">17</ref>) may be rewritten <ref type="bibr" target="#b17">(18)</ref> which provides an alternative definition of comparametric equation to that given in Definition III.2.</p><p>Equation ( <ref type="formula">14</ref>) is an example of a comparametric equation, and ( <ref type="formula">15</ref>) is a solution of <ref type="bibr" target="#b13">(14)</ref>.</p><p>It is often preferable that comparametric equations be on the interval from zero to one in the range of . Equivalently stated, we desire comparametric equations to be on the interval from zero to one in the domain of and the range of . In this case, the corresponding plots and equations are said to be unicomparametric. (Actual images typically range from 0 to 255 and must thus be rescaled so that they range from 0 to 1, for unicomparametric image processing.)</p><p>Often we also impose a further constraint that , and the constraint that and . Solving a comparametric equation is equivalent to determining the unknown camera response function from a pair of images that differ only in exposure, when the comparametric equation represents the relationship between greyvalues in the two pictures, and the comparametric ratio, , represents the ratio of exposures (e.g., if one picture was given taken with twice the exposure of the other, then ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Zeta Correction of Images</head><p>An alternative to gamma correction is proposed. This alternative, called zeta correction, will also serve as another example of a comparametric equation.</p><p>For zeta correction, we simply adjust the exponential solution (15) of the comparametric equation given by traditional gamma correction so that the solution passes through the origin:</p><p>. (For simplicity, and without loss of generality, has been set to , the comparametric exposure ratio.) Thus,</p><p>. Preferably (to be unicomparametric) we would like to have , so we use the response function <ref type="bibr" target="#b18">(19)</ref> which is a solution to the corresponding comparametric equation <ref type="bibr" target="#b19">(20)</ref> The comparametric equation ( <ref type="formula">20</ref>) forms the basis for zeta correction of images for <ref type="bibr" target="#b20">(21)</ref> Implicit in zeta correction of images is the assumption of an exponential camera response function, which, although not realistic (given that the exponential function expands dynamic range, and most cameras have compressive response functions rather than expansive response functions), is preferable to gamma correction because of the implicit notion of a response function for which . With standard IEEE arithmetic, values of can range from approximately 50 to 1000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Affine Comparametric Equation and Affine Correction of Images</head><p>In this section, one of the two most useful (in the author's opinion) comparametric equations is introduced, giving rise to affine correction of images. Affine correction is an improvement over that of zeta correction (which itself was an improvement over gamma correction).</p><p>First consider the classic model <ref type="bibr" target="#b21">(22)</ref> used by photographers to characterize the response of a variety of photographic emulsions, including so-called extended response film <ref type="bibr" target="#b8">[9]</ref>. It is well known that ( <ref type="formula">22</ref>) becomes the equation of a straight line when expressed in logarithmic coordinates, if we subtract (as many scanners such as PhotoCD attempt to do by prompting the user to scan a piece of blank film from the film trailer before scanning the rest of the roll of film)</p><p>It is an interesting coincidence that the comparametric plot of this function ( <ref type="formula">22</ref>) is also a straight line.</p><p>Proposition III.3: The comparametric plot corresponding to the standard photographic response function ( <ref type="formula">22</ref>) is a straight line. The slope is , and the intercept is . Proof:</p><p>Re-arranging to eliminate gives so that <ref type="bibr" target="#b23">(24)</ref> Note that the constant does not appear in the comparametric equation. Thus we cannot determine from the comparametric equation. The physical (intuitive) interpretation is that we can only determine the nonlinear response function of a camera up to a single unknown scalar constant.</p><p>Note that ( <ref type="formula">14</ref>) looks quite similar in form to <ref type="bibr" target="#b21">(22)</ref>, and in fact is identical if we set and . However, one must recall that ( <ref type="formula">14</ref>) is a comparametric equation and ( <ref type="formula">22</ref>) is a solution to a (different) comparametric equation. Thus we must be careful not to confuse the two. The first corresponds to gamma correction of an image, while the second corresponds to the camera response function that is implicit in applying <ref type="bibr" target="#b23">(24)</ref> to lighten or darken the image. To make this distinction clear, applying <ref type="bibr" target="#b23">(24)</ref> to lighten or darken an image will be called affine correcting (e.g. correcting by modeling the comparametric function with a straight line). The special case of affine correction when the intercept is equal to zero will be called linear correction.</p><p>Preferably affine correction of an image also includes a step of clipping values greater than 1 to 1, and values less than zero to zero, in the output image <ref type="bibr" target="#b24">(25)</ref> If the intercept is zero and the slope is greater than one, the effect, neglecting noise, of <ref type="bibr" target="#b24">(25)</ref>, is to lighten the image in a natural manner that properly simulates the effect of having exposed the image with greater exposure. In this case, the effect theoretically identical to that which would have been obtained by using a greater exposure on the camera, assuming the response function of the camera follows the power law , as many cameras do in practice. Thus it has been shown that the correct way to lighten an image is to apply linear correction, not gamma correction (apart from correction of an image to match an incorrectly adjusted display device or the like, where gamma correction is still the correct operation to apply).</p><p>Here we have worked forward, starting with the solution ( <ref type="formula">22</ref>) and deriving the comparametric equation <ref type="bibr" target="#b23">(24)</ref> of which ( <ref type="formula">22</ref>) is a solution. It is much easier to generate comparametric equations from their solutions than it is to solve comparametric equations.</p><p>The above comparametric equation is both useful and simple. The simplicity is in the ease with which it is solved, and by the fact that the solution happens to be the most commonly used camera response model in photography. As we will later see, when processing images, the comparametric function can be estimated by fitting a straight line through data points describing the comparametric relation between images. However, there are two shortcomings to affine correction.</p><p>1) It is not inherently unicomparametric so it must be clipped to 1 when it exceeds 1 and clipped to zero when it falls below zero, as shown in <ref type="bibr" target="#b24">(25)</ref>. 2) Its solution, only describes the response of cameras within their normal operating regime. Since the art of quantigraphic image processing involves a great deal of image processing done on images that have been deliberately and grossly overexposed or underexposed, there is a need for a comparametric model that captures the essence of cameras at both extremes (e.g., both overexposure and underexposure) of exposure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Preferred Correction of Images</head><p>Although affine correction was an improvement over zeta correction, which itself was an improvement over gamma correction, affine correction still has the two shortcomings listed above. Therefore another form of image exposure correction is proposed, and it will be called the preferred correction. This new exposure correction is unicomparametric (bounded in normalized units between 0 and 1) and also has a parameter to control the softness of the transition into the toe and shoulder regions of the response function, rather than the hard clipping introduced by ( <ref type="formula">25</ref>).</p><p>As with affine correction, the preferred correction will be introduced first by its solution, from which the comparametric equation will be derived. The solution is <ref type="bibr" target="#b25">(26)</ref> which has only three parameters. Thus no extra unnecessary degrees of freedom (which might otherwise capture or model noise) have been added over and above the number of degrees of freedom in the previous model <ref type="bibr" target="#b21">(22)</ref>.</p><p>An intuitive understanding of ( <ref type="formula">26</ref>) can be better had by rewriting it for <ref type="bibr" target="#b26">(27)</ref> where the soft transition into the toe (region of underexposure) and shoulder (region of overexposure) is evident by the shape of this curve on a logarithmic exposure scale. This model may, at first, only seem like a slight improvement over <ref type="bibr" target="#b21">(22)</ref>, given our common intuition that most exposure information is ordinarily captured in the central portion that is linear on the logarithmic exposure plot. However, it is important that we unlearn what we have been taught in traditional photography, where incorrectly exposed images are ordinarily thrown away rather than used to enhance the other images! It must be emphasized that comparametric image processing differs from traditional image processing in the sense that in comparametric image processing (using the Wyckoff principle, as illustrated in Fig. <ref type="figure" target="#fig_0">4</ref> the images typically include some that are deliberately underexposed and overexposed. In fact this overexposure of some images and underexposure of other images is often deliberately taken to extremes. Therefore, the additional sophistication of the model ( <ref type="formula">26</ref>) is of great value in capturing the essence of a set of images where some extend to great extremes in the toe or shoulder regions of the response function.</p><p>Proposition III.4: The comparametric equation of which the proposed photographic response function ( <ref type="formula">26</ref>) is a solution, is given by <ref type="bibr" target="#b27">(28)</ref> where . This function <ref type="bibr" target="#b27">(28)</ref> gives rise to the preferred correction of images (e.g., the preferred recipe for lightening or darkening an image). Again, does not depend on , which is consistent with our knowledge that the comparametric equation captures the information of up to a single unknown scalar proportionality constant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Some Solutions to Some Comparametric Equations That Are Particularly Illustrative or Useful</head><p>Some examples of comparametric equations and their solutions are summarized in Table <ref type="table" target="#tab_0">I</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Properties of Comparametric Equations</head><p>As stated previously, the comparametric equation only provides information about the actual photoquantity up to a single unknown scalar quantity, e.g., if is a solution of comparametric equation then so is . In general we can think of this as a coordinate transformation from to , in the domain of . More generally, the comparametric plot has the same shape as the comparametric plot , for all bijective . From this fact, we can construct a property of comparametric equations in general:</p><p>Proposition III.5: A comparametric equation has solution , for any bijective function .</p><p>. Likewise, we can also consider coordinate transformations in the range of comparametric equations, and their effects on the solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition III.6: A comparametric equation , has solution</head><p>, where is a comparametric equation with solution . Properties of comparametric equations related to their coordinate transformations are presented in Table <ref type="table" target="#tab_2">II</ref>   This solution also appears in Table <ref type="table" target="#tab_0">I</ref>. We may also use this solution to seed the solution of the comparametric equation second from the bottom of Table <ref type="table" target="#tab_0">I</ref>, by using . The equation second from the bottom of Table I may then be further coordinate transformed into the equation at the bottom of Table I by using . Thus properties of comparametric equations, such as those summarized in Table <ref type="table" target="#tab_2">II</ref>, can be used to help solve comparametric equations, such as those listed in Table <ref type="table" target="#tab_0">I</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PRACTICAL IMPLEMENTATIONS</head><p>This section pertains to the practical implementation of the theory presented in previous sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Comparing Two Images That Differ Only in Exposure</head><p>Without loss of generality, consider two differently exposed pictures of the same subject matter, and , and recognize that in the absence of noise, the relationship between the two images would be <ref type="bibr" target="#b28">(29)</ref> so that <ref type="bibr" target="#b29">(30)</ref> where , , and . It is evident that ( <ref type="formula">30</ref>) is a comparametric equation.</p><p>This process <ref type="bibr" target="#b29">(30)</ref> of "registering" the second image with the first differs from the image registration procedure commonly used in much of machine vision <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b21">[22]</ref> and image resolution enhancement <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b6">[7]</ref> because it operates on the range, , (tonal range) of the image as opposed to its domain (spatial coordinates) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparagram</head><p>The comparagram is defined as the joint histogram between two images that differ only in exposure. The comparagram is a square matrix of size by , where is number of greylevels <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. It can be seen from ( <ref type="formula">30</ref>) that the general problem of solving ( <ref type="formula">30</ref>) can be done directly on the comparagram instead of the original pair of images. This comparametric approach has the added advantage of breaking the problem down into two separate simpler steps:</p><p>1) comparametric regression: finding a smooth semimonotonic function, , that passes through most of the highest bins in the comparagram. 2) solving the comparametric equation: unrolling this function, into , by regarding it an iterative map onto itself (see Fig. <ref type="figure" target="#fig_2">6</ref>.) The iterative map (logistic map) is most familiar in chaos theory <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, but here, since the map is monotonic, the result is a deterministic function. Separating this estimation process into two stages also allows us a more direct route to "registering" the image domains, if for example, we do not need to know , but only require , which is the recipe for expressing the range of in the units of . In particular, we can lighten or darken images to match one another without ever having to solve for . The first part of the above two step process allows us to determine the relationship between two pictures that differ only in exposure, so that we can directly perform operations like image exposure interpolation and extrapolation as in Fig. <ref type="figure">5</ref>, but skipping the intermediate step of computing . Not all image processing applications require determining , so there is great value in simply understanding the relationship between differently exposed pictures of the same subject matter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparametric Regression and the Comparagram</head><p>In situations where the image data is extremely noisy, and/or where a closed-form solution for is desired, a parameterized form of the comparametric function is used, in which a function corresponding to a suitably parameterized response function is selected. The method amounts to a curve fitting problem in which the parameters of are selected so that best fits one or more comparagrams constructed from two or more differently exposed images under analysis. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparametric Regression to a Straight Line</head><p>The result <ref type="bibr" target="#b23">(24)</ref> suggests that can be determined from two differently exposed images by applying linear regression to the comparagram of the images, , treating each entry as a datapoint, and weighting by the number of bin counts at each point. Often this is done by weighting with . For example, (assuming empty bins are not counted) provides the classic linear regression problem in which all nonempty bins are weighted equally and the slope and intercept of the best-fit line through nonempty bins is found. Generally is chosen somewhere between 1/4 and 2.</p><p>A simple example is presented, that of reverse-engineering the standard Kodak PhotoCD scanner issued to most major photographic processing and scanning houses. In most situations, a human operator runs the machine, and decides, by visual inspection, what "brightness" level to scan the image at (there is also an automatic exposure feature which allows the operator to preview the scanned image and decide whether or not the chosen "brightness" level needs to be overridden). By scanning the same image at different "brightness" settings, a Wyckoff set results. This allows the scanner to capture nearly the entire dynamic range of the film, which is of great utility since typical photographic negative film captures far greater dynamic range than possible with the scanner as it is ordinarily used. A photographic negative taken from a scene of extremely high contrast (a sculpture on exhibit at the List Visual Arts Center, in a completely darkened room, illuminated with a bare flash lamp from one side only) was selected because of its great dynamic range that could not be captured in any single scan. A Wyckoff set was constructed by scanning the same negative at five different "brightness" settings (Fig. <ref type="figure">7</ref>). The settings were controlled by a slider that was calibrated in arbitrary units from 99 to +99, while running Kodak's proprietary scanning software. Kodak provides no information about what these units mean. Accordingly, the goal of the experiment was to find a closed-form mathematical equation describing the effects of the "brightness" slider on the scans, and to recover the unknown nonlinearity of the scanner. In order to make the problem a little more challenging and, more importantly, to better-illustrate the principles of comparametric image Fig. <ref type="figure">7</ref>. These scans from a photographic negative differ only in the choice of "brightness" setting selected using the slider provided on the X-windows screen by the proprietary Kodak PhotoCD scanning software. The slider is calibrated in arbitrary units from 099 to +99. Five scans were done and the setting of the slider is noted above each scan. Fig. <ref type="figure">8</ref>. Pairwise comparagrams of the images in Fig. <ref type="figure">8</ref>. It is evident that the data are well fitted by a straight line, which suggests that Kodak must have used the standard nonlinear response function f(q) = + q in the design of their PhotoCD scanner.</p><p>processing, dmin procedure of scanning a blank film at the beginning of the roll was overridden.</p><p>Comparagrams , , , and were computed from the five images ( through ) of Fig. <ref type="figure">7</ref>, and are displayed as density plots (e.g. treating them as images of dimension 256 by 256 pixels, where the darkness of the image is proportional to the number of counts-darkness rather than lightness to make it easier to see the pattern) in Fig. <ref type="figure">8</ref>. Linear regression was applied to the data, and the best-fit straight line is shown passing through the data points. Because the dmin procedure was overridden, notice that the plots do not pass through the origin. The two leftmost plots had nearly identical slopes and intercepts, and likewise for the two rightmost, which indicates that the arbitrary Kodak units of "brightness" are self-consistent (e.g. which describes the relationship between a scan at a "brightness" of 40 units and one of 20 units is essentially the same as which describes the relationship between a scan at a "brightness" of 20 units and one of 0 units). Since there are three parameters in <ref type="bibr" target="#b21">(22)</ref>, , , and , which describe only two degrees of freedom (slope and intercept), may be chosen so that works out to be linearly proportional to arbitrary Kodak units. Thus, setting (where is the average slope of the two leftmost plots and the average slope of the two rightmost plots) results in the value From this we obtain . Thus, we have that</p><formula xml:id="formula_1">(31)</formula><p>where is in arbitrary Kodak units (e.g. for the leftmost image, for the next image, for the middle image, , and ). Thus (31) gives us a closed-form solution that describes the response curve associated with each of the five exposures , , . The curves may be differentiated, and, if these derivatives are evaluated at , the so-called certainty images, shown in Fig. <ref type="figure">9</ref> are obtained.</p><p>In the next example, the use of the certainty functions to construct an optimal estimate, will be demonstrated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Comparametric Regression to the Preferred Model</head><p>For this second example, the comparametric model proposed in (28) will be used.</p><p>In many practical situations, real-world images are very noisy.</p><p>Accordingly, an example of noisy images that comprise a Wyckoff set (Fig. <ref type="figure" target="#fig_3">10</ref>), in which an extremely poor scan was deliberately used to scan images from a publication <ref type="bibr" target="#b10">[11]</ref>, is now considered.</p><p>That the images in Fig. <ref type="figure" target="#fig_3">10</ref> are of very poor quality is evidenced by their comparagram [Fig. <ref type="figure">11(a)</ref>]. Using regression of (28) to the joint comparagram combined with the knowledge (from the publication from which the images were obtained <ref type="bibr" target="#b10">[11]</ref>) that , it was found that and . This data provides a closed-form solution for the response function. The two effective response functions, which are shifted versions of this one response function, where the relative shift is , are plotted in Fig. <ref type="figure">12</ref>, together with their derivatives. (Recall that the derivatives of the response functions are the certainty functions.) Since a closed-form solution has been obtained, it may be easily differentiated without the further increase in noise Fig. <ref type="figure">9</ref>. Certainty functions express the rate of change of f (q(x; y)) with Q(x; y). The certainty functions may be used to compute the certainty images, f (c ). White areas in one of the certainty images indicate that pixel values f (q) change fastest with a corresponding change in the photoquantity, Q. When using the camera as a lightmeter (e.g., a quantigraphic instrument to estimate q), it will be most sensitive where the certainty images are white. White areas of these certainty images correspond to mid-grey values (midtones) of the corresponding original images in Fig. <ref type="figure">7</ref>, while dark areas correspond to extreme pixel values (either highlights or shadows) of the original images in Fig. <ref type="figure">7</ref>. Black areas of the certainty image indicate that Q changes drastically with small changes in pixel value, and thus an estimate of Q in these areas will be overcome by image noise n . that usually accompanies differentiation. Otherwise, when determining the certainty functions from poor estimates of , the certainty functions would be even more noisy than the poor estimate of itself. The resulting certainty images, denoted by , are shown in Fig. <ref type="figure" target="#fig_5">13</ref>. Each of the images, gives rise to an actual estimate of the quantity of light arriving at the image sensor (9) These esimates were combined by way of <ref type="bibr" target="#b9">(10)</ref> resulting in the composite image appears shown in Fig. <ref type="figure" target="#fig_0">14</ref>. Note that the resulting image looks very similar to , except that it is a floating point image array, of much greater tonal range and image quality.</p><p>Furthermore, given a Wyckoff set, a composite image may be rendered at any in-between exposure from the set (exposure interpolation), as well as somewhat beyond the exposures given (exposure extrapolation). This result suggests the "Virtu-alCamera" <ref type="bibr" target="#b16">[17]</ref> which allows images to be rendered at any desired exposure, once is computed.</p><p>This capability is somewhat similar to QuickTime VR and other image-based rendering systems, except that it operates in the range of the images rather than their domain.</p><p>V. SPATIOTONAL QUANTIGRAPHIC FILTERS Ordinarily, most print and display media have limited dynamic range. Thus one might be tempted to argue against the utility of the Wyckoff principle based on this fact, e.g. one might ask "since televisions and print media cannot display more than a very limited dynamic range, why bother building a Wyckoff camera that can capture such dynamic ranges?" Why bother capturing the photoquantity with more accuracy than is needed for display? Some possible answers to this question are as follows.</p><p>1) Estimates of are still useful for machine vision, and other applications that do not involve direct viewing of a final picture. An example is the wearable face recognizer <ref type="bibr" target="#b24">[25]</ref> which determines the identity of an individual from a plurality of differently exposed pictures of that person, and then presents the identity in the form of a text label (virtual name tag) on the retina of an eye of the wearer of the eyeglass-based apparatus. Since need not be displayed, the problem of output dynamic range, etc., of the display (e.g. number of distinct intensity levels of the laser beam shining into a lens of the eye of the wearer) is of no consequence. 2) Even though the ordinary dynamic range and the range resolution (typically 8 bits) is sufficient for print media (given the deliberately introduced nonlinearities that best use the limited range resolution), when performing operations such as deblurring, noise artifacts become more evident. In general, sharpening involves high pass filtering, and thus sharpening will often tend to uncover noise artifacts that would normally exist below the perceptual threshold when viewed through ordinary display media. In particular, sharpening often uncovers noise in the shadow areas, making dark areas of the image appear noisy in the final print or display. Thus in addition to the benefits of performing sharpening quantigraphically by applying an anti-homomorphic filter as in Fig. <ref type="figure">3</ref> to undo the blur of ( <ref type="formula">5</ref>), there is also further benefit from doing the generalized anti-homomorphic filtering operation at the point in Fig. <ref type="figure" target="#fig_0">4</ref>, rather than just that depicted in in Fig. <ref type="figure">3</ref>. 3) A third benefit of capturing more information than can be displayed, is that it defers the choice of which information Fig. <ref type="figure" target="#fig_0">14</ref>. Composite image made by simultaneously estimating the unknown nonlinearity of the camera as well as the true quantity of light incident on the camera's sensor array, given two input images from Fig. <ref type="figure" target="#fig_3">10</ref>. The combined optimal estimate of q is expressed here, in the coordinates of the lighter (rightmost) image. Although nothing has been done to appreciably enhance this image (e.g. the procedure of estimating q and then just converting it back into a picture again may seem pointless) we can note that while the image appears much like the rightmost input image, that the clipping of the highlight details has been softened somewhat. Later we will see methods of actual image enhancement done by processing q prior to converting it back to an image again.</p><p>to get rid of. For example, a camera could be constructed in such a way that it had no exposure adjustments: neither automatic nor manual settings. Instead, the camera would be more like an array of lightmeters that would capture an array of light measurements. Decisions as to what subject matter is of importance could then be made at the time of viewing or the time of printing. Such a camera has been incorporated into eyeglasses <ref type="bibr" target="#b12">[13]</ref>, allowing the wearer to completely forget about the camera, with no need to worry about settings or adjustments. In this way the wearer can capture once-in-a-lifetime moments like a baby's first steps, and then worry about adjusting the exposure settings later. Exposure can then be adjusted in the peaceful quiet of the living room, long after the picture A dynamic range in excess of a million to one was captured in q, and the estimate was then quantigraphically sharpened, with filter S, resulting in a lateral inhibition effect so that the output is no longer monotonically related to the input. Notice, for example, that the sail is as dark as some shadow areas inside the fortress. Because of this filtering, a tremendous dynamic range has been captured and reduced to that of printed media.</p><p>is captured and the confusing excitement of the moment has passed. In this way exposure can be adjusted carefully in a quiet setting away from the busy and distracting action of everyday life. Since these decisions are made later, they can also be changed, as there is no need to committ to one particular exposure setting. Moreover, deferring exposure decisions may have forensic value. For ex-ample, ordinary everyday subject matter and scenes may later become crime scenes, such that previously taken pictures in those spaces may help solve a crime. A family photographing their child's first trip to a grocery store may inadvertently capture an image of a fire exit illegally chained shut in the background. A fatal fire at some time in the future might call for evidence against the owner of the shop, where deferred choices of exposure may assist in the production of a picture exposed optimally for the fire exit in the background rather than the child in the foreground. Since the wearable apparatus transmits images to the World Wide Web, various viewers can each adjust the image interactively to suit their own display and perceptual capabilities, as well as their own preferences. 4) A fourth benefit from capturing a true and accurate measurement of the photoquantity, even if all that is desired is a nice looking picture (e.g. even if what is desired is not necessarily a true or accurate depiction of reality), is that additional processing may be done to produce a picture in which the limited dynamic range of the display or print medium shows a much greater dynamic range of input signal, through the use of further image processing on the photoquantity prior to display or printing. It is this fourth benefit that will be further described, as well as illustrated through a very compelling example, in this section.</p><p>Ordinarily, humans can not directly perceive the "signal" we process numerically, but, rather, we perceive the effects of the "signal" on perceptible media such as television screens or the like. In particular, in order to display , it is typically converted into an image and displayed, for example, on a television screen. Fig. <ref type="figure" target="#fig_0">14</ref> is an attempt to display, on the printed page, a signal which contains much greater dynamic range than can be directly represented on the page. To do this, the estimate was converted into an image by evaluating</p><p>. Even though we see some slight benefit, over (one of the input images) the benefit has not been made fully visible in this print.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. An Extreme Example with Spatiotonal Processing of Photoquantities</head><p>To fully appreciate the benefits of quantigraphic image processing, let us consider a seemingly impossible scene to photograph reasonably (in a natural way without bringing in lighting equipment of any kind).</p><p>Fig. <ref type="figure" target="#fig_6">15</ref> depicts a scene in which there is a dynamic range in excess of a million to one. In this case, two pictures were captured with several orders of magnitude difference between the two exposures. Thus the quantigraphic estimate has far greater dynamic range than can be directly viewed on a television or on the printed page. Display of would fail to show the shadow details, while display of would fail to show the highlight details.</p><p>In this case, even if we use the virtual camera architecture depicted in Fig. <ref type="figure">5</ref>, there is no single value of display exposure for which a display image will capture both the inside of the abandonded fortress and the details looking outside through the open doorway.</p><p>Therefore, a strong highpass (sharpening) filter, is applied to , to sharpen the photoquantity , as well as provide lateral inhibition similar to the way in which the human eye functions. Then the filtered result, , is displayed upon the printed page [Fig . <ref type="figure" target="#fig_6">15(c)</ref>], in the projective coordinates of the second (rightmost) image, . Note the introduction of spatial coordinates , and . These compensate for projection (e.g. if the camera moves slightly between pictures), as described in <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. In particular, the parameters of a projective coordinate transformation are typically estimated together with the nonlinear camera response function and the exposure ratio between pictures <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>.</p><p>As a result of the filtering operation, notice that there is no longer a monotonic relationship between input photoquantity and output level on the printed page. Notice, for example, that the sail is as dark as some shadow areas inside the fortress. Because of this filtering, the dynamic range of the image may be reduced to that of printed media, while still revealing details of the scene. This example answers the question "why capture more dynamic range than you can display."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The author would like to acknowledge assistance or contributions to this project from Kodak, Digital Equipment Corporation, Compaq, Xybernaut, WaveRider, CITO, and NSERC.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Wyckoff principle: Multiple differently exposed images of the same subject matter are captured by a single camera. In this example there are three different exposures.The first exposure (CAMERA set to exposure 1), gives rise to an exposure k q, the second to k q and the third to k q. Each exposure has a different</figDesc><graphic coords="6,56.34,62.28,480.72,343.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>of the function . A function has a family of comparametric plots, one for each value of the constant , which is called the comparametric ratio. Proposition III.2: When a function is monotonic, the comparametric plot can be expressed as a monotonic function not involving . Thus the plot in Definition III.1 may be rewritten as a plot , not involving . In this form, the function is called the comparametric function, and expresses the range of the function as a function of the range of the function , independently of the domain, , of the function .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Comparametric procedure for finding the pointwise nonlinearity of an image sensor from two pictures differing only in their exposures. (COMPARAMETRIC PLOT) Plot of pixel values in one image against corresponding pixel values in the other. (RESPONSE CURVE) Points on the response curve, found from only the two pictures, without any knowledge about the characteristics of the image sensor. These discrete points are only for illustrative purposes. If a logarithmic exposure scale is used, (as most photographers do) then the points fall uniformly on the Q = log(q=q ) axis.</figDesc><graphic coords="12,311.64,62.28,233.04,104.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Noisy images badly scanned from a publication. These images are identical except for exposure and a good deal of quantization noise, additive noise, scanning noise, etc. (a) Darker image shows clearly the eight people standing outside the doorway, but shows little of the architectural details of the dimly lit interiour. (b) Lighter image shows the architecture of the interiour, but it is not even possible to determine how many people are standing outside, let alone recognize any of them.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 11 .Fig. 12 .Fig. 10 ,</head><label>111210</label><figDesc>Fig. 11. Comparametric regression: (a) Comparagram. Note that because the images were extremely noisy, the comparagram is spread out over a fat ridge. Note also the gaps in the comparagram owing to the poor quality of the scanning process. (b) Even the comparagram of the images prior to the deliberately poor scan of them is itself quite spread out, indicating the images were quite noisy to begin with. (c) Comparametric regression is used to solve for the parameters of the comparametric function. The resulting comparametric plot is a noise-removed version of the comparagram, e.g., provides a smoothly constrained comparametric relationship between the two differently exposed images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 13 .</head><label>13</label><figDesc>Fig.<ref type="bibr" target="#b12">13</ref>. Certainty images which will be used as weights when the weighted sum of estimates of the actual quantity of light is computed. Bright areas correspond to large degrees of certainty.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 15 .</head><label>15</label><figDesc>Fig.<ref type="bibr" target="#b14">15</ref>. Extreme example to illustrate nonmonotonic processing: (a) An underexposed picture shows details such as the horizon and the sail of a boat, as seen through an open doorway, even though the sail is backlit with extremely bright light. (b) The picture is taken from inside an abandoned fortress with no interiour lights. Light coming in from the open door is largely lost in the vastness of the dark interiour, so that a much longer exposure is needed to show any detail of the inside of the fortress. (c) Sharpened (filtered) quantigraphic estimate f (k S q( Ã‚ x + b =Ä‰ x + d )) expressed in the projective coordinates of the second image in the image sequence (right hand image). A dynamic range in excess of a million to one was captured in q, and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="3,68.46,62.28,453.36,136.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="4,55.62,62.28,482.16,109.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="11,111.96,107.16,366.48,218.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="13,71.94,62.27,446.40,124.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="13,56.88,234.78,476.51,122.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I ILLUSTRATIVE</head><label>I</label><figDesc>OR USEFUL EXAMPLES OF COMPARAMETRIC EQUATIONS AND THEIR SOLUTIONS. THE THIRD FROM THE TOP AND SECOND FROM THE BOTTOM WERE FOUND TO DESCRIBE A LARGE VARIETY OF CAMERAS AND HAVE BEEN USED IN A WIDE VARIETY OF QUANTIGRAPHIC IMAGE PROCESSING APPLICATIONS. THE SECOND ONE FROM THE BOTTOM IS THE ONE THAT IS MOST COMMONLY USED BY THE AUTHOR</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II SOME</head><label>II</label><figDesc>PROPERTIES OF COMPARAMETRIC EQUATIONS. THIS TABLE COULD BE EXTENDED OVER SEVERAL PAGES, MUCH LIKE AN EXTENSIVE TABLE LISTING PROPERTIES OF LAPLACE TRANSFORMS, OR A</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Manuscript received March 31, 1999; revised March 1, 2000. The associate editor coordinating the review of this manuscript and approving it for publication was Dr. Michael Frater.The author is with the Department of Electrical and Computer Engineering, University of Toronto, Toronto, Ont., M5S 3G4, Canada.Publisher Item Identifier S 1057-7149(00)06143-1.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>Except in rare instances where the illumination is so intense as to damage the imaging apparatus, as, for example, when the sun burns through photographic negative film and appears black in the final print or scan.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Steve Mann pictured here wearing an early embodiment of his so-called "wearable computer" (WearComp) and EyeTap video camera and reality mediator (WearCam) invention, received the Ph.D. degree from the Massachusetts Institute of Technology (MIT), Cambridge, in 1997.</p><p>He is currently a faculty member with the Department of Electrical and Computer Engineering, University of Toronto, Toronto, Ont., Canada. He has been working on his WearComp invention for more than <ref type="bibr" target="#b19">20</ref>  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An historical account of the &apos;WearComp&apos; and &apos;WearCam&apos; projects developed for &apos;personal imaging</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Symp. Wearable Computing</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">Oct. 13-14, 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Wearable computing: A first step toward personal imaging</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="25" to="32" />
			<date type="published" when="1997-02">Feb. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Determining optical flow</title>
		<author>
			<persName><forename type="first">B</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="185" to="203" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improving resolution by image registration</title>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput., Vis., Graph., Image Process</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="231" to="239" />
			<date type="published" when="1991-05">May 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Virtual bellows: Constructing high-quality images from video</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 1st Int. Conf. Image Processing</title>
		<meeting>IEEE 1st Int. Conf. Image essing<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">Nov. 13-16, 1994</date>
			<biblScope unit="page" from="363" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Compositing multiple pictures of the same scene</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 46th Annu. IS&amp;T Conf</title>
		<meeting>46th Annu. IS&amp;T Conf<address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">May 9-14, 1993</date>
			<biblScope unit="page" from="50" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Video orbits of the projective group; a simple approach to featureless estimation of parameters</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mass. Inst. Technol</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep. 338</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Video mosaics for virtual environments</title>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graph. Applicat</title>
		<imprint>
			<biblScope unit="page" from="22" to="30" />
			<date type="published" when="1996-03">Mar. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An experimental extended response film</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Wyckoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SPIE Newslett</title>
		<imprint>
			<biblScope unit="page" from="16" to="20" />
			<date type="published" when="1962-07">June-July 1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">An experimental extended response film</title>
		<imprint>
			<date type="published" when="1961-03">Mar. 1961</date>
			<publisher>Germeshausen &amp; Grier, Inc</publisher>
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep. no. B-321</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Being &apos;undigital&apos; with digital cameras: Extending dynamic range by combining differently exposed pictures</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIT Media Lab Perceptual Computing Section</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<date type="published" when="1994">1994</date>
			<pubPlace>Cambridge, MA, Tech</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">&apos;pencigraphy&apos; with AGC: Joint parameter estimation in both domain and range of functions in same orbit of the projective-Wyckoff group</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mann</surname></persName>
		</author>
		<ptr target="http://hi.eecg.toronto.edu/icip96/index.html" />
	</analytic>
	<monogr>
		<title level="j">MIT Media Lab</title>
		<imprint>
			<date type="published" when="1994-12">Dec. 1994</date>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Humanistic intelligence/humanistic computing: &apos;wearcomp&apos; as a new framework for intelligent signal processing</title>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1998-11">Nov. 1998</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2123" to="2151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A Technical Introduction to Digital Video</title>
		<author>
			<persName><forename type="first">C</forename><surname>Poynton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName><surname>Online</surname></persName>
		</author>
		<ptr target="http://wearcam.org/gamma.htm" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image processing in the context of a visual model</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Stockham</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1972-07">July 1972</date>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="828" to="842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Personal imaging and lookpainting as tools for personal documentary and investigative photojournalism</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Mobile Network</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="36" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>AczÃ©l</surname></persName>
		</author>
		<title level="m">Lectures on Functional Equations and Their Applications</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic</publisher>
			<date type="published" when="1966">1966</date>
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K P</forename><surname>Horn</surname></persName>
		</author>
		<title level="m">Robot Vision</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>McGraw-Hill</publisher>
			<date type="published" when="1926">1986, ch. 1, 2 6, 9, 10, 12</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Direct methods for recovering motion</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K P</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Weldon</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Motion and structure from motion in a piecewise planar environment</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">D</forename><surname>Faugeras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lustman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Pattern Recognit. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="485" to="508" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3-D scene representation as a collection of images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Laveau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Faugeras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision Pattern Recognition<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-06">June 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Period-harmonic-tupling jumps to chaos and fractal-scaling in a class of series</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Woon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chaos, Solitons, Fractals</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="125" to="130" />
			<date type="published" when="1995-01">Jan. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Random affine iterated function systems: Curve generation and wavelets</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Rev</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="614" to="627" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Wearable, tetherless computer-mediated reality: WearCam as a wearable face-recognizer, and other applications for the disabled</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TR 361, MIT Media Lab Perceptual Computing Section</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">S</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<date type="published" when="1997-09">Sept. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">IS&amp;T&apos;s 48th Annu</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
		<ptr target="http://wearcam.org/ist95.htm" />
		<imprint>
			<date type="published" when="1995">May 7-11, 1995</date>
			<biblScope unit="page" from="422" to="428" />
			<pubPlace>Washington, D.C.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Mann</surname></persName>
		</author>
		<title level="m">Proc. IEEE Int. Conf. Image Processing</title>
		<meeting>IEEE Int. Conf. Image essing<address><addrLine>Lausanne, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">Sept. 16-19, 1996</date>
			<biblScope unit="page" from="193" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Mann</surname></persName>
		</author>
		<title level="m">AAAI Fall Symp. Developing Assistive Technology People Disabilities</title>
		<imprint>
			<date type="published" when="1996-09-11">Nov. 9-11, 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">S</forename><surname>Mann</surname></persName>
		</author>
		<ptr target="http://wearcam.org/vmp.htm" />
		<imprint>
			<date type="published" when="1996-02">Feb. 2, 1996</date>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
