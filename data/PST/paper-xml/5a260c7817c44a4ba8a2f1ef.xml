<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SonoNet: Real-Time Detection and Localisation of Fetal Standard Scan Planes in Freehand Ultrasound</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Christian</forename><forename type="middle">F</forename><surname>Baumgartner</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Konstantinos</forename><surname>Kamnitsas</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jacqueline</forename><surname>Matthew</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tara</forename><forename type="middle">P</forename><surname>Fletcher</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sandra</forename><surname>Smith</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lisa</forename><forename type="middle">M</forename><surname>Koch</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Bernhard</forename><surname>Kainz</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Rueckert</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="laboratory">were with the Biomedical Image Analysis Group</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Division of Imaging Sciences and Biomedical Engineering</orgName>
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Division of Imaging Sciences and Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Biomedical Research Centre</orgName>
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<addrLine>Guy&apos;s and St Thomas&apos; NHS Foundation</addrLine>
									<settlement>London</settlement>
									<country>UK, UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SonoNet: Real-Time Detection and Localisation of Fetal Standard Scan Planes in Freehand Ultrasound</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0FFBDFC155B3AC3C10EFECA81AA05BFA</idno>
					<idno type="DOI">10.1109/TMI.2017.2712367</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMI.2017.2712367, IEEE Transactions on Medical Imaging</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T10:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Convolutional neural networks</term>
					<term>fetal ultrasound</term>
					<term>standard plane detection</term>
					<term>weakly supervised localisation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Identifying and interpreting fetal standard scan planes during 2D ultrasound mid-pregnancy examinations are highly complex tasks which require years of training. Apart from guiding the probe to the correct location, it can be equally difficult for a non-expert to identify relevant structures within the image. Automatic image processing can provide tools to help experienced as well as inexperienced operators with these tasks. In this paper, we propose a novel method based on convolutional neural networks which can automatically detect 13 fetal standard views in freehand 2D ultrasound data as well as provide a localisation of the fetal structures via a bounding box. An important contribution is that the network learns to localise the target anatomy using weak supervision based on imagelevel labels only. The network architecture is designed to operate in real-time while providing optimal output for the localisation task. We present results for real-time annotation, retrospective frame retrieval from saved videos, and localisation on a very large and challenging dataset consisting of images and video recordings of full clinical anomaly screenings. We found that the proposed method achieved an average F1-score of 0.798 in a realistic classification experiment modelling real-time detection, and obtained a 90.09% accuracy for retrospective frame retrieval. Moreover, an accuracy of 77.8% was achieved on the localisation task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>A BNORMAL fetal development is a leading cause of perinatal mortality in both industrialised and developing countries <ref type="bibr" target="#b27">[28]</ref>. Overall early detection rates of fetal abnormalities are still low and are hallmarked by large variations between geographical regions <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b14">[15]</ref>.</p><p>The primary modality for assessing the fetus' health is 2D ultrasound due to its low cost, wide availability, real-time capabilities and the absence of harmful radiation. However, the diagnostic accuracy is limited due to poor signal to noise ratio and image artefacts such as shadowing. Furthermore, it can be difficult to obtain a clear image of a desired view if the fetal pose is unfavourable.</p><p>Currently, most countries offer at least one routine ultrasound scan at around mid-pregnancy between 18 and 22 weeks Fig. <ref type="figure">1</ref>. Overview of proposed SonoNet: (a) 2D fetal ultrasound data can be processed in real-time through our proposed convolutional neural network to determine if the current frame contains one of 13 fetal standard views (here the 4 chamber view (4CH) is shown); (b) if a standard view was detected, its location can be determined through a backward pass through the network.</p><p>of gestation <ref type="bibr" target="#b27">[28]</ref>. Those scans typically involve imaging a number of standard scan planes on which biometric measurements are taken (e.g. head circumference on the transventricular head view) and possible abnormalities are identified (e.g. lesions in the posterior skin edge on the standard sagittal spine view). In the UK, guidelines for selecting and examining these planes are defined in the fetal abnormality screening programme (FASP) handbook <ref type="bibr" target="#b21">[22]</ref>.</p><p>Guiding the transducer to the correct scan plane through the highly variable anatomy and assessing the often hard-tointerpret ultrasound data are highly sophisticated tasks, requiring years of training <ref type="bibr" target="#b19">[20]</ref>. As a result these tasks have been shown to suffer from low reproducibility and large operator bias <ref type="bibr" target="#b6">[7]</ref>. Even identifying the relevant structures in a given standard plane image can be a very challenging task for certain views, especially for inexperienced operators or non-experts. At the same time there is also a significant shortage of skilled sonographers, with vacancy rates reported to be as high as <ref type="bibr" target="#b17">18</ref>.1% in the UK <ref type="bibr" target="#b30">[31]</ref>. This problem is particularly pronounced in parts of the developing world, where the WHO estimates that many ultrasound scans are carried out by individuals with little or no formal training <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Contributions</head><p>With this in mind, we propose a novel system based on convolutional neural networks (CNNs) for real-time automated detection of 13 fetal standard scan planes, as well as localisation of the fetal structures associated with each scan plane in the images via bounding boxes. We model all standard views which need to be saved according to the UK FASP guidelines for mid-pregnancy ultrasound examinations, plus the most commonly acquired cardiac views. The localisation is achieved in a weakly supervised fashion, i.e. with only image-level scan plane labels available during training. This is an important aspect of the proposed work as bounding box annotations are not routinely recorded and would be too time-consuming to create for large datasets. Fig. <ref type="figure">1</ref> contains an overview of the proposed method. Our approach achieves real-time performance and very high accuracy in the detection task and is the first in the literature to tackle the weaklysupervised localisation task on freehand ultrasound data. All evaluations are performed on video data of full mid-pregnancy examinations.</p><p>The proposed system can be used in a number of ways. It can be employed to provide real-time feedback about the content of a image frame to the operator. This may reduce the number of mistakes made by inexperienced sonographers and could also be applied to automated quality control of acquired images. We also demonstrate how this system can be used to retrospectively retrieve standard views from very long videos, which may open up applications for automated analysis of data acquired by operators with minimal training and make ultrasound more accessible to non-experts. The localisation of target structures in the images has the potential to aid nonexperts in the detection and diagnosis tasks. This may be particularly useful for training purposes or for applications in the developing world. Moreover, the saliency maps and bounding box predictions improve the interpretability of the method by visualising the hidden reasoning of the network. That way we hope to build trust into the method and also provide an intuitive way to understand failure cases. Lastly, automated detection and, specifically, localisation of fetal standard views are essential preprocessing steps for other automated image processing such as measurement or segmentation of fetal structures.</p><p>This work was presented in preliminary form in <ref type="bibr" target="#b1">[2]</ref>. Here, we introduce a novel method for computing category-specific saliency maps, provide a more in-depth description of the proposed methods, and perform a significantly more thorough quantitative and qualitative evaluation of the detection and localisation on a larger dataset. Furthermore, we significantly outperform our results in <ref type="bibr" target="#b1">[2]</ref> by employing a very deep network architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Related work</head><p>A number of papers have proposed methods to detect fetal anatomy in videos of fetal 2D ultrasound sweeps (e.g. <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>). In those works the authors have been aiming at detecting the presence of fetal structures such as the skull, heart or abdomen rather than specific standardised scan planes.</p><p>Yaqub et al. <ref type="bibr" target="#b33">[34]</ref> have proposed a method for the categorisation of fetal mid-pregnancy 2D ultrasound images into seven standard scan planes using guided random forests. The authors modelled an "other" class consisting of non-modelled standard views. Scan plane categorisation differs significantly from scan plane detection since in the former setting it is already known that every image is a standard plane. In standard plane detection on a real-time data stream or video data, standard views must be distinguished from a very large amount of background frames. This is a very challenging task due to the vast amount of possible appearances of the background class.</p><p>Automated fetal standard scan plane detection has been demonstrated for 1-3 standard planes in short videos of 2D fetal ultrasound sweeps <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b22">[23]</ref>. The earlier of those works rely on extracting Haar-like features from the data and training a classifier such as AdaBoost or random forests on them <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b34">[35]</ref>.</p><p>Motivated by advances in computer vision, there has recently been a shift to analyse ultrasound data using CNNs. The most closely related work to ours is that by Chen et al. <ref type="bibr" target="#b9">[10]</ref> who employed a classical CNN architecture with five convolutional and two fully-connected layers for the detection of the standard abdominal view. During test time, each frame of the input video was processed by evaluating the classifier multiple times for overlapping image patches. The drawback of this approach is that the classifier needs to be applied numerous times, which precludes the system from running in real-time. In <ref type="bibr" target="#b7">[8]</ref>, the same authors extended the above work to three scan planes and a recurrent architecture which took into account temporal information, but did not aim at real-time performance.</p><p>An important distinction between the present study and all of the above works is that the latter used data acquired in single sweeps while we use freehand data. Sweep data are acquired in a fixed protocol by moving the ultrasound probe from the cervix upwards in one continuous motion <ref type="bibr" target="#b9">[10]</ref>. However, not all standard views required to determine the fetus' health status are adequately captured using a sweep protocol. For example, imaging the femur or the lips normally requires careful manual scan plane selection. Furthermore, data obtained using the sweep protocol are typically only 2-5 seconds long and consist of fewer than 50 frames <ref type="bibr" target="#b9">[10]</ref>. In this work, we consider data acquired during real clinical abnormality screening examinations in a freehand fashion. Freehand scans are acquired without any constraints on the probe motion and the operator moves from view to view in no particular order. As a result such scans can last up to 30 minutes and the data typically consists of over 20,000 individual frames for each case. To our knowledge, automated fetal standard scan plane detection has never been performed in this challenging scenario.</p><p>A number of works have been proposed for the supervised localisation of structures in ultrasound. Zhang et al. <ref type="bibr" target="#b34">[35]</ref> developed a system for automated detection and fully supervised localisation of the gestational sac in first trimester sweep ultrasound scans. Bridge et al. <ref type="bibr" target="#b4">[5]</ref> proposed a method for the localisation of the heart in short videos using rotation invariant features and support vector machines for classification. In more recent work, the same authors have extended the method for the supervised localisation of three cardiac views taking into account the temporal structure of the data <ref type="bibr" target="#b3">[4]</ref>. The method was also able to predict the heart orientation and cardiac phase.</p><p>To our knowledge, the present work is the first to perform localisation in fetal ultrasound in a weakly supervised fashion. Although, weakly supervised localisation (WSL) is an active area of research in computer vision (e.g. <ref type="bibr" target="#b26">[27]</ref>) we are not aware of any works which attempt to perform WSL in real-time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data</head><p>Our dataset consisted of 2694 2D ultrasound examinations of volunteers with gestational ages between 18-22 weeks which have been acquired and labelled during routine screenings by a team of 45 expert sonographers according to the guidelines set out in the UK FASP handbook <ref type="bibr" target="#b21">[22]</ref>. Those guidelines only define the planes which need to be visualised, but not the sequence in which they should be acquired. The large number of sonographers involved means that the dataset contains a large number of different operator-dependent examination "styles" and is therefore a good approximation of the normal variability observed between different sonographers. In order to reflect the distribution of real data, no selection of the cases was made based on normality or abnormality. Eight different ultrasound systems of identical make and model (GE Voluson E8) were used for the acquisitions. For each scan we had access to freeze-frame images saved by the sonographers during the exam. For a majority of cases we also had access to screen capture videos of the entire fetal exam.</p><p>1) Image data: A large fraction of the freeze-frame images corresponded to standard planes and have been manually annotated during the scan allowing us to infer the correct ground-truth (GT) label. Based on those labels we split the image data into 13 standard views. In particular, those included all views required to be saved by the FASP guidelines, the four most commonly acquired cardiac views, and the facial profile view. An overview of the modelled categories is given in Table <ref type="table" target="#tab_0">I</ref> and examples of each view are shown in Fig. <ref type="figure" target="#fig_6">7</ref>.</p><p>Additionally, we modelled an "other" class using a number of views which do not need to be saved according to the FASP guidelines but are nevertheless often recorded at our partner hospital. Specifically, the "other" class was made up from the arms, hands and feet views, the bladder view, the diaphragm view, the coronal face view, the axial orbits view, and views of the cord-insert, cervix and placenta. Overall, our dataset contained 27731 images of standard views and 6856 of "other" views. The number of examples for each class ranged from 543 for the profile view to 4868 for the brain (tv.) view. Note that a number of the cases were missing some of the standard planes while others had multiple instances of the same view acquired at different times.</p><p>2) Video data: In addition to the still images, our dataset contained 2638 video recordings of entire fetal exams, which were on average over 13 minutes long and contained over 20000 frames. 2438 of those videos corresponded to cases for which image data was also available. Even though in some examinations not all standard views were manually annotated, we found that normally all standard views did appear in the video.</p><p>It was possible to find each freeze-frame image in its corresponding video if the latter existed. As will be described in more detail in Sec. II-D we used this fact to augment our training dataset in order to bridge the small domain gap between image and video data. Specifically, the corresponding frames could be found by iterating through the video frames and calculating the image distance of each frame to the freezeframe image. The matching frame was the one with the minimum distance to the freeze-frame.</p><p>As is discussed in detail in Sec. III, all evaluations were performed on the video data in order to test the method in a realistic scenario containing motion and a large number of irrelevant background frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Preprocessing</head><p>The image and video data were preprocessed in five steps which are summarised in Table <ref type="table" target="#tab_0">II</ref> and will be discussed in detail in the following.</p><p>Since, in this study, we were only interested in structural images we removed all freeze-frame images and video frames containing colour Doppler overlays from the data. We also removed video frames and images which contained split views showing multiple locations in the fetus simultaneously.</p><p>To prevent our algorithm from learning the manual annotations placed on the images by the sonographers rather than from the images themselves, we removed all the annotations using the inpainting algorithm proposed in <ref type="bibr" target="#b32">[33]</ref>.</p><p>We rescaled all image and frame data and cropped a 224x288 region containing most of the field of view but excluding the vendor logo and ultrasound control indicators. We also normalised each image by subtracting the mean intensity value and dividing by the image pixel standard deviation.</p><p>In order to tackle the challenging scan plane detection scenario in which most of the frames do not show any of the standard scan planes, a large set of background images needed to be created. The data from the "other" classes mentioned above were not enough to model this highly varied category.</p><p>Note that our video data contained very few frames showing standard views and the majority of frames were background. Thus, it was possible to create the background class by randomly sampling frames from the available video recordings. Specifically, we sampled 50 frames from all training videos and 200 frames from all testing videos. While we found that 50 frames per case sufficed to capture the full variability of the background class during training, we opted for a larger number of background frames for the test set in order to evaluate the method in a more challenging and realistic scenario. This resulted in a very large background class with 110638 training images and 105611 testing images. Note that operators usually hold the probe relatively still around standard planes, while the motion is larger when they are searching for views. Thus, in order to decrease the chance of randomly sampling actual standard planes, frames were only sampled where the probe motion, i.e. image distance to previous video frame, was above a small threshold. Note that the location in the video of some of the standard scan planes could be determined by comparing image distances to the freeze frames as described earlier (see Sec. II-A). However, this knowledge could not be used to exclude all standard views for the background class sampling because it only accounted for a very small fraction of standard views in the video. The videos typically contained a large number of unannotated standard views in the frames before and after the freeze frame, and also in entirely different positions in the video.</p><p>The images from the "other" category were also added to the background class. Overall the dataset including the background class had a substantial (and intentional) class imbalance between standard views and background views. For the test set the standard view to background ratios were between 1:138 and 1:1148, depending on the category.</p><p>In the last step, we split all of the cases into a training set containing 80% of the cases and test set containing the remaining 20%. The split was made on the case level rather than the image level to guarantee that no video frames originating from test videos were used for training. Note that not all cases contained all of the standard views and as a result the ratios between test and training images were not exactly 20% for each class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Network architecture</head><p>Our proposed network architecture, the sonography network or SonoNet, is inspired by the VGG16 model which consists of 13 convolutional layers and 3 fully-connected layers <ref type="bibr" target="#b29">[30]</ref>. However, we introduce a number of key changes to optimise it for the real-time detection and localisation tasks. The network architectures explored in this work are summarised in Fig. <ref type="figure" target="#fig_0">2</ref>.</p><p>Generally, the use of fully-connected layers restricts the model to fixed image sizes which must be decided during training. In order to obtain predictions for larger, rectangular input images during test time, typically the network is evaluated multiple times for overlapping patches of the training image size. This approach was used, for example, in some related fetal scan plane detection works <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Fully convolutional networks, in which the fully-connected layers have been replaced by convolutions, can be used to calculate the output to arbitrary images sizes much more efficiently in a single forward pass. The output of such a network is no longer a single value for each class, but rather a class score map with a size dependent on the input image size <ref type="bibr" target="#b18">[19]</ref>. In order to obtain a fixed-size vector of class scores, the class score maps can then be spatially aggregated using the sum, mean or max function to obtain a single prediction per class. Fully convolution networks have been explored in a number of works in computer vision (e.g. <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b16">[17]</ref>), and in medical image analysis, for example for mitosis detection <ref type="bibr" target="#b8">[9]</ref>.</p><p>Simonyan et al. <ref type="bibr" target="#b29">[30]</ref> proposed training a traditional model with fully-connected layers, but then converting it into a fully convolutional architecture for efficient testing. This was achieved by converting the first fully-connected layer to a convolution over the full size of the last class score map (i.e. a 7x7 convolution for the VGG16 network), and the subsequent ones to 1x1 convolutions. In the case of 224x288 test images this would produce 1x14 class score maps for each category.</p><p>In this work we use the spatial correspondence between class score maps with the input image to obtain localised category-specific saliency maps (see Sec. II-F). Consequently, it is desirable to design the network such that it produces class score maps with a higher spatial resolution. To this end, we forgo the final max-pooling step in the VGG16 architecture and replace all the fully-connected layers with two 1x1 convolution layers. Following the terminology introduced by Oquab et al. <ref type="bibr" target="#b24">[25]</ref>, we will refer to those 1x1 convolutions as adaptation layers. The output of those layers are K class score maps F k , where K is the number of modelled classes (here K = 14, i.e. 13 standard views plus background). We then aggregate them using the mean function to obtain a prediction vector which is fed into the final softmax. In this architecture the class score maps F k have a size of 14x18 for an 224x288 input image. Note that each neuron in F k corresponds to a receptive field in the original image creating the desired spatial correspondence with the input image. During training, each of the neurons learns to respond to category-specific features in its receptive field. Note that the resolution of the class score maps is not sufficient for accurate localisation. In Sec. II-F we will show how F k can be upsampled to the original image resolution using a backpropagation step to create categoryspecific saliency maps.</p><p>The design of the last two layers of the SonoNet is similar to work by Oquab et al. <ref type="bibr" target="#b24">[25]</ref>. However, in contrast to that work, we aggregate the final class score maps using the mean function rather than the max function. Using the mean function incorporates the entire image context for the classification while using the max function only considers the receptive field of the maximally activated neuron. While max pooling aggregation may be beneficial for the localisation task <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b24">[25]</ref>, we found the classification accuracy to be substantially  lower using that strategy.</p><p>Since we are interested in operating the network in realtime, we explore the effects of reducing the complexity of the network on inference times and detection accuracy. In particular, we investigate three versions of the SonoNet. The SonoNet-64 uses the same architecture for the first 13 layers as the VGG16 model, with 64 kernels in the first convolutional layer. We also evaluate the SonoNet-32 and the SonoNet-16 architectures, where the number of all kernels in the network is halved and quartered, respectively.</p><p>In contrast to the VGG16 architecture, we include batch normalisation in every convolutional layer <ref type="bibr" target="#b15">[16]</ref>. This allows for much faster training because larger learning rates can be used. Moreover, we found that for all examined networks using batch normalisation produced substantially better results.</p><p>In addition to the three versions of the SonoNet, we also compare to a simpler network architecture which is loosely inspired by the AlexNet <ref type="bibr" target="#b17">[18]</ref>, but has much fewer parameters. This is also the network which we used for our initial results presented in <ref type="bibr" target="#b1">[2]</ref>. Due to the relatively low complexity of this network compared to the SonoNet, we refer to it as SmallNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Training</head><p>We trained all networks using mini-batch gradient descent with a Nesterov momentum of 0.9, a categorical cross-entropy loss and with an initial learning rate of 0.1. We subsequently divided the learning rate by 10 every time the validation error stopped decreasing. In some cases we found that a learning rate of 0.1 was initially too aggressive to converge immediately. Therefore, we used a warm-up learning rate of 0.01 for 500 iterations <ref type="bibr" target="#b13">[14]</ref>. Since the SmallNet network did not have any batch normalisation layers it had to be trained with a lower initial learning rate of 0.001.</p><p>Note that there is a small domain gap between the annotated image data and the video data we use for our real-time detection and retrospective retrieval evaluations. Specifically, the video frames are slightly lower resolution and have been compressed. In order to overcome this, we automatically identified all frames from the training videos which corresponded to the freeze-frame images in our training data. However, as mentioned in Sec. II-A not all cases had a corresponding video, such that the frame dataset consisted of fewer instances than the image dataset. To make the most of our data while ensuring that the domain gap is bridged, we combined all of the images and the corresponding video frames for training. We used 20% of this combined training dataset for validation.</p><p>In order to reduce overfitting and make the network more robust to varying object sizes we used scale augmentation <ref type="bibr" target="#b29">[30]</ref>. That is, we extracted square patches of the input images for training by randomly sampling the size of the patch (between 174x174 and 224x224) and then scaling it up to 224x224 pixels. To further augment the dataset, we randomly flipped the patches in the left-right direction, and rotated them with a random angle between -25 • and 25 • .</p><p>The training procedure needed to account for the significant class imbalance introduced by the randomly sampled background frames. Class imbalance can be addressed either by introducing an asymmetric cost-function, by post-processing the classifier output, or by sampling techniques <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b12">[13]</ref>. We opted for the latter approach which can be neatly integrated with mini-batch gradient descent. We found that the strategy which produced the best results was randomly sampling minibatches that were made up of the same number of standard planes and background images. Specifically, we used 2 images of each of the 13 standard planes and 26 background images per batch.</p><p>The optimisation typically converged after around 2 days of training on a Nvidia GeForce GTX 1080 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Frame annotation and retrospective retrieval</head><p>After training we fed the network with cropped video frames with a size of 224x288. This resulted in K class score maps F k with a size of 14x18. Those where averaged in the mean pooling layer to obtain a single class score a k for each category k. The softmax layer then produced the class confidence c k of each frame. The final prediction was given by the output with the highest confidence.</p><p>For retrospective frame retrieval we calculated and recorded the confidence c k for each class over the entire duration of an input video. Subsequently, we retrieved the frame with the highest confidence for each class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Weakly supervised localisation</head><p>After determining the 14x18 class score maps F k and the image category in a forward pass through the network, the fetal anatomical structures corresponding to that category can then be localised in the image. A coarse localisation could already be achieved by directly relating each of the neurons in F k to its receptive field in the original image. However, it is also possible to obtain pixel-wise maps containing information about the location of class-specific target structures at the resolution of the original input images. This can be achieved by calculating how much each pixel influences the activation of the neurons in F k . Such maps can be used to obtained much more accurate localisation. Examples of F k and corresponding saliency maps are shown in Fig. <ref type="figure" target="#fig_1">3</ref>.</p><p>In the following we will show how category-specific saliency and confidence maps can be obtained through an additional backward pass through the network. Secondly, we show how to post-process the saliency maps to obtain confidence maps from which we then extract a bounding box around the detected structure.</p><p>1) Category-specific saliency maps: Generally, categoryspecific saliency maps S k can be obtained by computing how much each pixel in the input image X influences the current prediction. This is equivalent to calculating the gradient of the last activation before the softmax a k with respect to the pixels of the input image X.</p><formula xml:id="formula_0">S k = ∂a k ∂X<label>(1)</label></formula><p>The gradient can be obtained efficiently using a backward pass through the network <ref type="bibr" target="#b28">[29]</ref>. Springenberg et al. <ref type="bibr" target="#b31">[32]</ref> proposed a method for performing this back-propagation in a guided manner by allowing only error signals which contribute to an increase of the activations in the higher layers (i.e. layers closer to the network output) to back-propagate. In particular, the error is only back-propagated through each neuron's ReLU unit if the input to the neuron x, as well as the error in the next higher layer δ n are positive. That is, the back-propagated error δ n-1 of each neuron is given by</p><formula xml:id="formula_1">δ n-1 = δ n σ(x)σ(δ n ),<label>(2)</label></formula><p>where σ(•) is the unit step function. Examples of saliency maps obtained using this method are shown in Fig. <ref type="figure" target="#fig_1">3b</ref>. It can be observed that those saliency maps, while highlighting the fetal anatomy, also tend to highlight background features, which adversely affects automated localisation.</p><p>In this work, we propose a method to generate significantly less noisy, localised saliency maps by taking advantage of the spatial encoding in the class score maps F k . As can be seen in Fig. <ref type="figure" target="#fig_1">3a</ref>, the class score maps can be interpreted as a coarse confidence map of the object's location in the input frame. In particular, each neuron h n k (X) in F k has a receptive field in the original image X. In our preliminary work <ref type="bibr" target="#b1">[2]</ref>, we backpropagated the error only from a fixed percentile P of the most highly activated neurons in F k to achieve a localisation effect. However, this required heuristic selection of P . In this paper, we propose a more principled approach. Note that very high or very low values in the saliency map mean that a change in that pixel will have a large effect on the classification score. However, those values do not necessarily correspond to high activations in the class score map. For example, an infinitesimal change in the input image may not have a very large impact if the corresponding output neuron is already very highly activated. Conversely, another infinitesimal change in the input image may have a big impact on a neuron with low activation, for example by making the image look less like a competing category. To counteract this, we preselect the areas of the images which are likely to contain the object based on the class score maps and give them more influence in the saliency map computation. More specifically, we use the activations h n k (X) in F k to calculate the saliency maps as a weighted linear combination of the influence of each of the receptive fields of the neurons in F k . In this manner, regions corresponding to highly activated neurons will have more importance than neurons with low activations in the resulting saliency map. In the following, we drop the subscripts for the category k for conciseness. We calculate the saliency map S as</p><formula xml:id="formula_2">S = n h n &gt;0 (X) ∂h n (X) ∂X ,<label>(3)</label></formula><p>where h n &gt;0 are the class score map activations thresholded at zero, i.e. h n &gt;0 = h n σ(h n ). By thresholding at zero we essentially prevent negative activations from contributing to the saliency maps. Note that it is not necessary to backpropagate for each neuron h n separately. In fact, the saliency can still be calculated in a single back-propagation step, which can be seen by rewriting Eq. 3 as</p><formula xml:id="formula_3">S = n 1 2 ∂(h n &gt;0 (X)) 2 ∂X = 1 2 ∂e T F &gt;0 • F &gt;0 e ∂X ,<label>(4)</label></formula><p>where F &gt;0 is the class score map thresholded at zero, • is the element-wise matrix multiplication and e is a vector with all ones. The first equality stems from the chain-rule and the observation that h n &gt;0 h n = h n &gt;0 h n &gt;0 , and the second equality stems from rewriting the sum in matrix form.</p><p>Examples of saliency maps obtained using this method are shown in Fig. <ref type="figure" target="#fig_1">3c</ref>. It can be seen that the resulting saliency maps are significantly less noisy and the fetal structures are easier to localise compared to the images obtained using the approach presented in <ref type="bibr" target="#b31">[32]</ref>.</p><p>2) Bounding box extraction: Next, we post-process saliency maps obtained using Eq. 4 to obtain confidence maps from which we then calculate bounding boxes. In a first step, we take the absolute value of the saliency map S and blur it using a 5x5 Gaussian kernel. This produces confidence maps of the location of the structure in the image such as the ones shown in Fig. <ref type="figure" target="#fig_2">4b</ref>. Note that even though both structures are challenging to detect on those views, the confidence maps localise them very well, despite artefacts (shadows in row 1) and similar looking structures (arm in row 2).</p><p>Due to the way the gradient is calculated structures that appear dark in the images (such as cardiac vessels) will usually have negative saliencies and structures that appear bright (bones) will usually have positive saliencies in S k . We exploit this fact to introduce some domain knowledge into the localisation procedure. In particular, we only consider positive saliencies for the femur, spine and lips, and we only consider negative saliencies for all cardiac views. We use both positive and negative for the remainder of the classes.</p><p>Next, we threshold the confidence maps using the Isodata thresholding method proposed in <ref type="bibr" target="#b10">[11]</ref>. In the last step, we take the largest connected component of the resulting mask and fit the minimum rectangular bounding box around it. Two examples are shown in Fig. <ref type="figure" target="#fig_2">4c</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Real-time scan plane detection</head><p>In order to quantitatively assess the detection performance of the different architectures we evaluated the proposed networks on the video frame data corresponding to the freezeframes from the test cohort including the large amount of randomly sampled background frames. We measured the algorithm's performance using the precision (TP / (TP + FP)) and recall (TP / (TP + FN)) rates as well as the F1-score, which is defined as the harmonic mean of the precision and recall.</p><p>In Table <ref type="table" target="#tab_3">III</ref> we report the average scores for all examined networks. Importantly, the average was not weighted by the number of samples in each category. Otherwise, the average scores would be dominated by the massive background class.</p><p>In Table <ref type="table" target="#tab_4">IV</ref> we furthermore report the frame rates achieved on a Nvidia Geforce GTX 1080 GPU for the detection task alone, the localisation task alone and both of them combined. There is no consensus in literature over the minimum frame rate required to qualify as real-time, however, a commonly used figure is 25 frames per second (fps), which coincides with the frame rate our videos were recorded at.</p><p>From Tables III and IV it can be seen that SonoNet-64 and SonoNet-32 performed very similarly on the detection task with SonoNet-64 obtaining slightly better F1-scores, but failing to perform the localisation task at more than 25 fps. The SonoNet-32 obtained classification scores very close to the SonoNet-64 but at a substantially lower computational cost, achieving real-time in both the detection and localisation tasks. Further reducing the complexity of the network led to more significant deteriorations in detection accuracy as can be seen from the SonoNet-16 and the SmallNet network. Thus, we conclude that the SonoNet-32 performs the best out of the examined architectures which achieve real-time performance and we use that architecture for all further experiments and results.</p><p>In Table <ref type="table" target="#tab_5">V</ref> we show the detailed classification scores for the SonoNet-32 for all the modelled categories. The rightmost column lists the number of test images in each of the classes. Additionally, the class confusion matrix obtained with SonoNet-32 is shown in Fig. <ref type="figure" target="#fig_4">5</ref>. The results reported for this classification experiment give an indication of how the method performs in a realistic scenario. The overall ratio of standard planes to background frames is approximately 1:24 meaning that in a video on average 1 second of any of the standard views is followed by 24 seconds of background views. This is a realistic reflection of what we observe in clinical practice. Some of the most important views for taking measurements   and assessing the fetus' health (in particular the brain views, the abdominal view and the femur view) were detected with F1-scores of equal to or above 0.9, which are very high scores considering the difference in number of images for the background and foreground classes. The lowest detection accuracies were obtained for the profile view, the rightventricular outflow tract (RVOT) and the three vessel view (3VV). The two cardiac views -which are only separated from each other by a slight change in the probe angle and are very similar in appearance -were often confused with each other by the proposed network. This can also be seen in the confusion matrix in Fig. <ref type="figure" target="#fig_4">5</ref>. We also noted that for some views the method produced very high recall rates with relatively low precision. The Spine (sag.) view and the profile view were particularly affected by this. We found that for a very large fraction of those false positive images, the prediction was in fact correct, but the images had an erroneous background ground-truth label. This can be explained by the fact that the spine and profile views appear very frequently in the videos without being labelled and thus many such views were inadvertently sampled in the background class generation process. Examples cases with correct predictions but erroneous ground-truth labels for the profile and spine (sag.) classes are shown in the first three columns of Fig. <ref type="figure" target="#fig_5">6</ref>. We observed the same effect for classes which obtained higher precision scores as well. For instance, we verified that the majority of background frames classified as Brain (Cb.) are actually true detections. Examples are also shown in Fig. <ref type="figure" target="#fig_5">6</ref>. All of the images shown in the first three columns of Fig. <ref type="figure" target="#fig_5">6</ref> are similar in quality to our groundtruth data and could be used for diagnosis. Unfortunately, it is infeasible to manually verify all background images. We therefore conclude that the precision scores (and consequently F1-scores) reported in Tables III and V can be considered a lower bound of the true performance.</p><p>For a qualitative evaluation, we also annotated a number of videos from our test cohort using the SonoNet-32. Two example videos demonstrating the SonoNet-32 in a real clinical exam are available at https://www.youtube.com/ watch?v=4V8V0jF0zFc and https://www.youtube.com/watch? v=yPCvAdOYncQ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Retrospective scan plane retrieval</head><p>We also evaluated the SonoNet-32 for retrospective retrieval of standard views on 110 random videos from the test cohort. The average duration of the recordings was 13 min 33 sec containing on average 20321 frames. The retrieved frames were manually validated by two clinical experts in obstetrics with 11 years and 3 years of experience, respectively. The time-consuming manual validation required for this experiment precluded using a larger number of videos. Table VI summarises the retrieval accuracy (TP / (P + N)) for 13 standard planes. We achieved an average retrieval accuracy of 90.09%. As above, the most challenging views proved to be the cardiac views for which the retrieval accuracy was 82.12%. The average accuracy for all non-cardiac views was 95.52%. In contrast to the above experiment, the results in this section were obtained directly from full videos, and thus reflect the true performance of the method in a real scenario. The retrieved frames for two cases from the test cohort are shown in Fig. <ref type="figure" target="#fig_6">7</ref> along with the ground truth (GT) frames saved by the sonographers. In the case shown in Fig. <ref type="figure" target="#fig_6">7a</ref>, all views have been correctly retrieved. It can be seen that most of the retrieved frames either matched the GT exactly or were of equivalent quality. We observed this behaviour throughout the test cohort. However, a number of wrong retrievals occasionally occurred. In agreement with the quantitative results in Tab. VI, we noted that cardiac views were affected the most. Fig. <ref type="figure" target="#fig_6">7b</ref> shows a case for which two cardiac views have been incorrectly retrieved (marked in red).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Weakly supervised localisation</head><p>We quantitatively evaluated the weakly supervised localisation using SonoNet-32 on 50 images from each of the 13 modelled standard scan planes. The 650 images were manually annotated with bounding boxes which were used as ground truth. We employed the commonly used intersection over union (IOU) metric to measure the similarity of the automatically estimated bounding box to the ground truth <ref type="bibr" target="#b11">[12]</ref>. Table <ref type="table" target="#tab_5">VII</ref> summarises the results. As in <ref type="bibr" target="#b11">[12]</ref>, we counted a bounding box as correct if its IOU with the ground truth was equal to or greater than 0.5. Using this metric we found that on average 77.8% of the automatically retrieved bounding boxes were correct. Cardiac views were the hardest to localise with an average accuracy of 62.0%. The remaining views obtained an average localisation accuracy of 84.9%.</p><p>In Fig. <ref type="figure" target="#fig_7">8</ref> we show examples of retrieved bounding boxes for each of the classes. From these examples, it can be seen that our proposed method was able to localise standard planes which are subject to great variability in scale and appearance. Qualitatively very good results were achieved for small structures such as the lips or the femur. The reason why this was not reflected in the quantitative results in Table VII was that the IOU metric more is more sensitive to small deviations in small boxes than in large ones.</p><p>We noted that the method was relatively robust to artefacts and performed well in cases where it may be hard for nonexperts to localise the fetal anatomy. For instance, the lips view in the third column of Fig. <ref type="figure" target="#fig_7">8</ref> and the RVOT view in the second column were both correctly localised.</p><p>The last column for each structure in Fig. <ref type="figure" target="#fig_7">8</ref> shows cases with incorrect (IOU &lt; 0.5) localisation. It can be seen that the method almost never failed entirely to localise the view. Rather, the biggest source of error was inaccurate bounding boxes. In many cases the saliency maps were dominated by the most important feature for detecting this view, which caused the method to focus only on that feature at the expense of the remainder of the view. An example is the stomach in the abdominal view shown in the fourth column of Fig. <ref type="figure" target="#fig_7">8</ref>. Another example is the brain (tv.) view, for which the lower partswhere the ventricle is typically visualised -was much more important for the detection. In other cases, regions outside of the object also appeared in the saliency map, which caused the bounding box to overestimate the extent of the fetal target structures. An example is the femur view, where the other femur also appeared in the image and caused the bounding box to cover both.</p><p>An example video demonstrating the real-time localisation for a representative case can be viewed at https://www. youtube.com/watch?v=yPCvAdOYncQ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DISCUSSION AND CONCLUSION</head><p>In this paper, we presented the first real-time framework for the detection and bounding box localisation of standard views in freehand fetal ultrasound. Notably, the localisation task can be performed without the need for bounding boxes during training. Our proposed SonoNet employs a very deep convolutional neural network, based on the widely used VGG16 architecture, but optimised for real-time performance and accurate localisation from category-specific saliency maps.</p><p>We showed that the proposed network achieves excellent results for real-time annotation of 2D ultrasound frames and retrospective retrieval on a very challenging dataset.</p><p>Future work will focus on including the temporal dimension in the training and prediction framework as was done for sweep data in <ref type="bibr" target="#b7">[8]</ref> and for fetal cardiac videos in <ref type="bibr" target="#b3">[4]</ref>. We expect that especially the detection of cardiac views may benefit from motion information.</p><p>We also demonstrated the method's ability for real-time, robust localisation of the respective views in a frame. Currently, the localisation is based purely on the confidence maps shown in Fig. <ref type="figure" target="#fig_2">4</ref>. Although, this already leads to very accurate localisation, we speculate that better results may be obtained by additionally taking into account the pixel intensities of the original images. Potentially, the proposed localisation method could also be combined using a multi-instance learning framework in order to incorporate the image data into the bounding box prediction <ref type="bibr" target="#b26">[27]</ref>.</p><p>We also note that the confidence maps could potentially be used in other ways, for instance, as a data term for a graphical model for semantic segmentation <ref type="bibr" target="#b2">[3]</ref>.</p><p>The pretrained weights for all of the network architectures compared in this paper are available at https://github.com/ baumgach/SonoNet-weights. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview of proposed network architectures. Each network consists of a feature extractor, an adaptation layer, and the final classification layer. All convolutional operations are denoted by squared brackets. Specifically, we use the following notation: [kernelsize x number of kernels / stride]. The factor in front of the squared brackets indicates how many times this operation is repeated. Max-pooling is always performed with a kernel size of 2x2 and a stride of 2 and is denoted by MP. All convolutions are followed by a batch normalisation layer before the ReLu activation, except the SmallNet network, for which no batch normalisation was used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Examples of saliency maps. Column (a) shows three different input frames, (b) shows the corresponding class score maps F k obtained in the forward pass of the network, (c) shows saliency maps obtained using the method by Springenberg et al. [32] and (d) shows the saliency maps resulting from our proposed method. Some of the unwanted saliency artefacts are highlighted with arrows in (c).</figDesc><graphic coords="6,311.98,56.07,251.06,154.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Examples of saliency map post-processing for two challenging views: (a) shows two input images, (b) shows the resulting confidence maps for those images, and (c) shows the resulting bounding boxes.</figDesc><graphic coords="7,134.63,123.43,79.72,62.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Class confusion matrix for SonoNet-32.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Examples of video frames labelled as background but classified as one of three standard views. The first three columns were randomly sampled from the set of false positives and are in fact correct detections. The last column shows manually selected true failure cases.</figDesc><graphic coords="8,85.06,234.71,214.96,214.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Results of retrospective retrieval for two example subjects. The respective top rows show the ground truth (GT) saved by the sonographer. The bottom rows show the retrieved (RET) frames. For subject (a) all frames have been correctly retrieved. For subject (b) the frames marked with red have been incorrectly retrieved.</figDesc><graphic coords="10,48.96,56.06,514.04,524.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Examples of weakly supervised localisation using the SonoNet-32. The first three columns for each view show correct bounding boxes marked in green (IOU ≥ 0.5), the respective last columns shows an example of an incorrect localisation marked in red (IOU &lt; 0.5). The ground truth bounding boxes are shown in white.</figDesc><graphic coords="11,48.96,56.06,514.09,370.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I OVERVIEW</head><label>I</label><figDesc>OF THE MODELLED CATEGORIES.</figDesc><table><row><cell cols="2">Views required by FASP:</cell></row><row><cell>Brain (cb.)</cell><cell>Brain view at the level of the cerebellum</cell></row><row><cell>Brain (tv.)</cell><cell>Brain view at posterior horn of the ventricle</cell></row><row><cell>Lips</cell><cell>Coronal view of the lips and nose</cell></row><row><cell>Abdominal</cell><cell>Standard abdominal view at stomach level</cell></row><row><cell>Kidneys</cell><cell>Axial kidneys view</cell></row><row><cell>Femur</cell><cell>Standard femur view</cell></row><row><cell>Spine (sag.)</cell><cell>Sagittal spine view</cell></row><row><cell>Spine (cor.)</cell><cell>Coronal spine view</cell></row><row><cell cols="2">Cardiac views:</cell></row><row><cell>4CH</cell><cell>Four chamber view</cell></row><row><cell>3VV</cell><cell>Three vessel view</cell></row><row><cell>RVOT</cell><cell>Right ventricular outflow tract</cell></row><row><cell>LVOT</cell><cell>Left ventricular outflow tract</cell></row><row><cell>Other:</cell><cell></cell></row><row><cell>Profile</cell><cell>Median facial profile</cell></row><row><cell>Background</cell><cell>Non-modelled standard views and background frames</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III CLASSIFICATION</head><label>III</label><figDesc>SCORES FOR THE FOUR EXAMINED NETWORK ARCHITECTURES.</figDesc><table><row><cell>Network</cell><cell>Precision</cell><cell>Recall</cell><cell>F1-score</cell></row><row><cell>SonoNet-64</cell><cell>0.806</cell><cell>0.860</cell><cell>0.828</cell></row><row><cell>SonoNet-32</cell><cell>0.772</cell><cell>0.843</cell><cell>0.798</cell></row><row><cell>SonoNet-16</cell><cell>0.619</cell><cell>0.900</cell><cell>0.720</cell></row><row><cell>SmallNet</cell><cell>0.354</cell><cell>0.864</cell><cell>0.461</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV FRAME</head><label>IV</label><figDesc>RATES IN FPS FOR THE DETECTION (FORWARD PASS), LOCALISATION (BACKWARD PASS) AND THE TWO COMBINED.</figDesc><table><row><cell>Network</cell><cell>Detection</cell><cell>Localisation</cell><cell>Det. &amp; Loc.</cell></row><row><cell>SonoNet-64</cell><cell>70.4</cell><cell>21.9</cell><cell>16.7</cell></row><row><cell>SonoNet-32</cell><cell>125.4</cell><cell>35.8</cell><cell>27.9</cell></row><row><cell>SonoNet-16</cell><cell>196.7</cell><cell>55.9</cell><cell>43.5</cell></row><row><cell>SmallNet</cell><cell>574.1</cell><cell>226.0</cell><cell>162.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V DETAILED</head><label>V</label><figDesc>CLASSIFICATION SCORES FOR SONONET-32</figDesc><table><row><cell>Class</cell><cell>Precision</cell><cell>Recall</cell><cell>F1-score</cell><cell># Images</cell></row><row><cell>Brain (Cb.)</cell><cell>0.90</cell><cell>0.96</cell><cell>0.93</cell><cell>549</cell></row><row><cell>Brain (Tv.)</cell><cell>0.86</cell><cell>0.98</cell><cell>0.92</cell><cell>764</cell></row><row><cell>Profile</cell><cell>0.46</cell><cell>0.91</cell><cell>0.61</cell><cell>92</cell></row><row><cell>Lips</cell><cell>0.88</cell><cell>0.91</cell><cell>0.89</cell><cell>496</cell></row><row><cell>Abdominal</cell><cell>0.93</cell><cell>0.90</cell><cell>0.92</cell><cell>474</cell></row><row><cell>Kidneys</cell><cell>0.77</cell><cell>0.77</cell><cell>0.77</cell><cell>166</cell></row><row><cell>Femur</cell><cell>0.87</cell><cell>0.93</cell><cell>0.90</cell><cell>471</cell></row><row><cell>Spine (cor.)</cell><cell>0.72</cell><cell>0.94</cell><cell>0.81</cell><cell>81</cell></row><row><cell>Spine (sag.)</cell><cell>0.60</cell><cell>0.87</cell><cell>0.71</cell><cell>156</cell></row><row><cell>4CH</cell><cell>0.81</cell><cell>0.73</cell><cell>0.77</cell><cell>306</cell></row><row><cell>3VV</cell><cell>0.68</cell><cell>0.59</cell><cell>0.63</cell><cell>287</cell></row><row><cell>RVOT</cell><cell>0.60</cell><cell>0.58</cell><cell>0.59</cell><cell>284</cell></row><row><cell>LVOT</cell><cell>0.82</cell><cell>0.74</cell><cell>0.78</cell><cell>317</cell></row><row><cell>Background</cell><cell>1.00</cell><cell>0.99</cell><cell>0.99</cell><cell>104722</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>28 104013</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The system was furthermore comprised of an Intel Xeon CPU E5-1630 v3 at 3.70GHz and</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2133" xml:id="foot_1"><p>MHz DDR4 RAM.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by the Wellcome Trust IEH Award [102431].</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automated retrieval of standard diagnostic fetal cardiac ultrasound planes in the second trimester of pregnancy: a prospective evaluation of software</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abuhamad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Falkensammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Reichartseder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ultrasound Obst Gyn</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="36" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Real-time standard scan plane detection and localisation in fetal ultrasound using fully convolutional neural networks</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Baumgartner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kainz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc MICCAI</title>
		<meeting>MICCAI</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="203" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast approximate energy minimization via graph cuts</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Veksler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T Pattern Anal</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1222" to="1239" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automated annotation and quantitative description of ultrasound videos of the fetal heart</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Bridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ioannou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Noble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med Image Anal</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="147" to="161" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Object localisation in fetal ultrasound images using invariant features</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Bridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Noble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc ISBI</title>
		<meeting>ISBI</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="156" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">British Isles Network of Congenital Anomaly Registers</title>
	</analytic>
	<monogr>
		<title level="m">Congenital anomaly statistics 2010 -England and Wales</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Volumetric (3D) imaging reduces inter-and intraobserver variation of fetal biometry measurements</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Sahota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ultrasound Obst Gyn</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="447" to="452" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic fetal ultrasound standard plane detection using knowledge transferred recurrent neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J-Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P-A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc MICCAI</title>
		<meeting>MICCAI</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="507" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mitosis detection in breast cancer histology images via deep cascaded networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1160" to="1166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Standard plane localization in fetal ultrasound via domain transferred deep neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J Biomed Health Inform</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1627" to="1636" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Images thresholding using isodata technique with gamma distribution</title>
		<author>
			<persName><forename type="first">A</forename><surname>El-Zaart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit Image Anal</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="41" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (VOC) challenge</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int J Comput Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data</title>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T Knowl Data En</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1263" to="1284" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Disparities in the prenatal detection of critical congenital heart disease</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Block</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Frommelt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Prenatal Diag</publisher>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="859" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<title level="m">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Fully convolutional neural networks for crowd segmentation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4464</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Adv Neur In</title>
		<imprint>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<title level="m">Network in network</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Searching for structures of interest in an ultrasound video sequence</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Maraci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Napolitano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Papageorghiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Noble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc MLMI</title>
		<meeting>MLMI</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="133" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fisher vector encoding for detecting objects of interest in ultrasound videos</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Maraci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Napolitano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Papageorghiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Noble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc ISBI</title>
		<meeting>ISBI</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="651" to="654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fetal anomalie screen programme handbook</title>
	</analytic>
	<monogr>
		<title level="j">NHS Screening Programmes</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Selective search and sequential detection for standard plane localization in ultrasound</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><surname>Sh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C-T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International MICCAI Workshop on Computational and Clinical Challenges in Abdominal Imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="203" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Standard plane localization in ultrasound by radial component model and selective search</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C-T</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P-A</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ultrasound Med Biol</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2728" to="2742" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Is object localization for free? -Weakly-supervised learning with convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="685" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">From image-level to pixel-level labeling with convolutional networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1713" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Weakly supervised large scale object localization with multiple instance learning and bag splitting</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T Pattern Anal</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="405" to="416" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Practice guidelines for performance of the routine mid-trimester fetal ultrasound scan</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Salomon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Alfirevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Berghella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bilardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K-Y.</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Malinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Munoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ultrasound Obst Gyn</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="116" to="126" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<title level="m">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Society of Radiographers. Sonographer workforce survey analysis</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="28" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6806</idno>
		<title level="m">Striving for simplicity: The all convolutional net</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An image inpainting technique based on the fast marching method</title>
		<author>
			<persName><forename type="first">A</forename><surname>Telea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Graph Tools</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="34" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Guided random forests for identification of key fetal anatomy and image categorization in ultrasound scans</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yaqub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Papageorghiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Noble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc MICCAI</title>
		<meeting>MICCAI</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="687" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Intelligent scanning: Automated standard plane selection and biometric measurement of early gestational sac in routine ultrasound examination</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med Phys</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="5015" to="5027" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Training cost-sensitive neural networks with methods addressing the class imbalance problem</title>
		<author>
			<persName><forename type="first">Z-H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T Knowl Data En</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="77" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
