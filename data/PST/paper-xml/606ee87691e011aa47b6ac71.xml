<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Reordering for Cache-Efficient Near Neighbor Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Benjamin</forename><surname>Coleman</surname></persName>
							<email>&lt;ben.coleman@rice.edu&gt;</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Rice University</orgName>
								<address>
									<settlement>Houston</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Santiago</forename><surname>Segarra</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Rice University</orgName>
								<address>
									<settlement>Houston</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Amazon Web Services</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anshumali</forename><surname>Shrivastava</surname></persName>
							<email>&lt;anshumali@rice.edu&gt;</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Rice University</orgName>
								<address>
									<settlement>Houston</settlement>
									<region>TX</region>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rice University</orgName>
								<address>
									<settlement>Houston</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Anshumali Shrivastava</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Alex Smola</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Graph Reordering for Cache-Efficient Near Neighbor Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph search is one of the most successful algorithmic trends in near neighbor search. Several of the most popular and empirically successful algorithms are, at their core, a simple walk along a pruned near neighbor graph. Such algorithms consistently perform at the top of industrial speed benchmarks for applications such as embedding search. However, graph traversal applications often suffer from poor memory access patterns, and near neighbor search is no exception to this rule. Our measurements show that popular search indices such as the hierarchical navigable smallworld graph (HNSW) can have poor cache miss performance. To address this problem, we apply graph reordering algorithms to near neighbor graphs. Graph reordering is a memory layout optimization that groups commonly-accessed nodes together in memory. We present exhaustive experiments applying several reordering algorithms to a leading graph-based near neighbor method based on the HNSW index. We find that reordering improves the query time by up to 40%, and we demonstrate that the time needed to reorder the graph is negligible compared to the time required to construct the index.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Near neighbor search is a fundamental building block within many applications in machine learning systems. Informally, the task can be understood as follows. Given a dataset D = {x 1 , x 2 , ...x N }, we wish to build a data structure that can be queried with any point q to obtain the k points x i ∈ D that have the smallest distance to the query. This data structure is called a near neighbor index. Near neighbor Applications. Near neighbor search has always been the core of canonical learning algorithms such as neighborbased classification and regression. However, the problem has recently become the focus of intense research activity due to its central role in the task of prediction with embedding models. In a neural embedding model, objects are transformed into embeddings in R d , where d ranges from 100 to 1000 and N often exceeds 100 million. Prediction typically consists of finding the k nearest embeddings. For example, Amazon's deep semantic search engine works by embedding the product catalog and the search query into the same high-dimensional space. The engine recommends the products whose embeddings are nearest to the embedded search query <ref type="bibr" target="#b32">(Nigam et al., 2019)</ref>.</p><p>Since the search occurs for every query, the latency and recall of the machine learning system critically depend on the ability to perform fast near neighbor search in the high-recall regime. Similar problems exist in natural language processing <ref type="bibr" target="#b23">(Kusner et al., 2015)</ref>, information retrieval <ref type="bibr" target="#b16">(Huang et al., 2013)</ref>, computer vision <ref type="bibr" target="#b13">(Frome et al., 2013)</ref> and other application domains. Beyond machine learning, near neighbor indices are used to match audio recordings, detect plagiarism, classify ECG signals in healthcare, and perform copyright attribution with blockchain <ref type="bibr" target="#b19">(Jafari et al., 2021)</ref>. As a result, near neighbor search has become a central bottleneck for many applications.</p><p>Latency Challenges. Solutions to the near neighbor problem are incredibly diverse, ranging from hardwareaccelerated brute force <ref type="bibr" target="#b22">(Johnson et al., 2019)</ref> to spacepartitioning trees <ref type="bibr" target="#b4">(Beygelzimer et al., 2006)</ref>. Graph algorithms have emerged as a markedly effective class of methods for high-dimensional near neighbor search. For example, four of the top five search libraries on the well-established ANN-benchmarks leader board use a graph index <ref type="bibr" target="#b1">(Aumüller et al., 2020)</ref>. Because these libraries have been integrated with many large-scale production systems <ref type="bibr" target="#b21">(Johnson et al., 2017;</ref><ref type="bibr" target="#b29">Malkov &amp; Yashunin, 2018)</ref>, they have been hand-arXiv:2104.03221v1 [cs.DS] 7 Apr 2021 optimized in C++ at a large engineering cost. Systems for fast near neighbor search are the focus of a highly active research area, and optimizations have a clear practical impact.</p><p>In this paper, we investigate the possibility of using graph reordering to improve approximate near neighbor search. Graph reordering is a cache optimization that works by placing neighboring nodes in consecutive (or near-consecutive) memory locations. When a node is loaded into memory by a graph processing algorithm, modern CPU architectures will automatically load nodes from nearby memory locations as well. By reordering the nodes, we ensure that nodes which are likely to be visited by the algorithm are already pre-loaded into the CPU cache. The relabeling process may seem simple, but RAM access is responsible for up to 70% of the runtime for tasks such as PageRank, graph decomposition, and graph diameter computation <ref type="bibr" target="#b39">(Wei et al., 2016)</ref>. Since cache hits are an order of magnitude faster than memory access, reordering can speed up graph applications by 50% or more <ref type="bibr" target="#b3">(Balaji &amp; Lucia, 2018)</ref>. Our work reveals that near neighbor search can be significantly accelerated through graph reordering, a hitherto unexplored optimization for this important machine learning task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Our Contribution</head><p>We integrate six recent graph ordering algorithms into the hierarchical navigable small-world graph (HNSW), a leading graph index that provides state-of-the-art performance on common benchmarks. We perform an exhaustive comparison on the SIFT100M, GIST, and DEEP100M datasets where we benchmark the query time and cache miss rate. Our experiments show that graph reordering is a viable and effective way to improve practical machine learning deployments that involve near neighbor search. We make the following new observations:</p><p>• Graph reordering substantially improves the query time of near neighbor search, with speedups of 10-40% on large embedding datasets.</p><p>• Objective-based techniques, which optimize the node order to maximize a global cache coherence score, have the best performance. Recently-proposed lightweight graph algorithms, which only use local node features, do not improve near neighbor search.</p><p>• Although objective-based methods are among the most expensive reordering algorithms, the reordering time is often an order of magnitude smaller than the time needed to construct the search index.</p><p>The question of whether reordering can improve near neighbor search is nontrivial because reordering can actually slow down graph processing, depending on the characteristics of the input graph and the node access pattern of the application <ref type="bibr" target="#b3">(Balaji &amp; Lucia, 2018)</ref>. It should be noted that near neighbor graphs have much different structural properties than the graphs that typically benefit from reordering. For example, reordering algorithms often obtain large speedups on graphs with power-law degree distributions <ref type="bibr" target="#b12">(Faldu et al., 2019)</ref>, which near neighbor graphs do not have. It is also not clear whether the process of reordering a search index is prohibitively expensive. For commonly adopted formulations of the reordering problem, finding the optimal node labeling function is an NP-hard problem <ref type="bibr" target="#b39">(Wei et al., 2016)</ref>. Approximate algorithms and heuristics exist, but their complexity scales quadratically with the sum of node degrees. This is not an issue for graphs like social networks, which have a low average degree by virtue of their power-law distribution, but could pose serious problems when reordering near neighbor graphs, which are more densely connected.</p><p>For industrial-scale embedding search, the graph index is frequently rebuilt to reflect changes in the model. Therefore, it is critical that our performance improvements do not substantially inflate the index construction time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background and Related Work</head><p>Near Neighbor Graphs. Graph-based algorithms such as HNSW, pruned approximate near neighbor graphs (PANNG) <ref type="bibr" target="#b17">(Iwasaki, 2016)</ref>, and optimized near neighbor graphs (ONNG) <ref type="bibr" target="#b18">(Iwasaki &amp; Miyazaki, 2018)</ref> feature prominently in leader boards and production systems at web-scale companies. Apart from recent theoretical progress <ref type="bibr" target="#b34">(Prokhorenkova &amp; Shekhovtsov, 2020)</ref>, the primary focus of graph index research has been to develop heuristics (such as diversification, pruning, and hierarchical structures) that improve the properties of the graph, increase search accuracy, and reduce search time. In Section 3, we show that most search algorithms have essentially the same computational workload because these heuristics produce highly similar graph structures.</p><p>An alternative method to reduce the search time is to develop implementation techniques that can be used to build fast systems. Due to their industrial importance, search indices have been optimized aggressively. However, there has been surprisingly little work to improve the cache locality properties of graph-based near neighbor search, even though graph algorithms are known to have poor cache performance. <ref type="bibr" target="#b5">Boytsov &amp; Naidan (2013)</ref> describe memory fragmentation as an important consideration for HNSW, and the HNSW software manual <ref type="bibr" target="#b31">(Naidan et al., 2015)</ref> describes a "flattened index" to improve the memory access pattern of near neighbor search. The flat layout stores nodes and data together in a contiguous block of memory without using pointers. The efficient layout improves cache performance, but it is agnostic to the graph structure. Graph reordering is a compatible technique that uses information about the graph to further improve the layout.</p><p>Ordering with Space-Filling Curves. The closest idea related to graph reordering is likely the use of space-filling curves for search problems <ref type="bibr" target="#b6">(Chan, 2002)</ref>. A space-filling curve is a line that passes through each point in the dataset. By minimizing the length of this line, we obtain a point ordering that places nearby points in consecutive positions. This idea has mainly been used to implement cache-efficient near neighbor search via a linear scan through the (ordered) data but has also been applied to graphs. <ref type="bibr" target="#b8">Connor &amp; Kumar (2010)</ref> use the Morton ordering (a specific kind of spacefilling curve) to improve the cache performance of graph construction for 3D point clouds. However, this work does not apply to machine learning applications such as embedding search for two important reasons. First, space-filling curves suffer from the curse of dimensionality and are not an effective way to order the high-dimensional vectors commonly encountered in embedding search. Second, existing work uses space-partitioning curves and reordering to speed up the construction of the graph, while we are interested in whether reordering can improve the query time. To the best of our knowledge, this paper is the only investigation of graph reordering for fast near neighbor queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Core Components of a Graph Search Index</head><p>In this section, we introduce the key ingredients behind graph-based search indexing. As will become apparent, popular graph-based indices (HNSW, PANNG, and ONNG) share the same core components. Thus, although we focus on HNSW for our experiments, we expect our findings to generalize to most graph-based indices used in practice.</p><p>From an implementation perspective, the core components of a graph index are diversification / pruning, search initialization, and beam search. Although major search algorithms do differ in nontrivial ways, these differences do not affect the node access pattern of the application or the properties of the graph that are relevant to graph reordering.</p><p>In a near neighbor graph index, each node corresponds to an element from the dataset. Two nodes are connected if there is a small distance between the corresponding elements. To the best of our knowledge, all existing theoretical models of near neighbor graphs adopt one of the following two construction processes <ref type="bibr" target="#b24">(Laarhoven, 2017;</ref><ref type="bibr" target="#b36">Sebastian &amp; Kimia, 2002)</ref>, where we assume the existence of a metric d : V → V between every pair of nodes.</p><p>Definition 1 A graph is an α-near neighbor (α-NN) graph if there is an edge between all nodes v 1 and v 2 such that</p><formula xml:id="formula_0">d(v 1 , v 2 ) &lt; α. Definition 2 A graph is a k-nearest neighbor (k-NN) graph if each node v is connected to its k nearest nodes.</formula><p>The two graph models differ by degree distribution and connectivity. In the k-NN graph, each node has an outdegree equal to k, while nodes in the α-NN graph may be connected to a variable number of neighbors. Furthermore, k-NN graphs are directed, while α-NN graphs are undirected because the distance function d is symmetric.</p><p>It should be noted that both models may produce disconnected graphs, with nodes in sparsely populated regions of the dataset forming separate connected components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Constructing Near Neighbor Graphs</head><p>In practice, one cannot efficiently construct the k-NN or α-NN graphs because O(N 2 ) computations are required to find the nearest neighbors of each point in the dataset. Nearly all modern algorithms solve this chicken-and-egg problem via "bootstrapping," where we iteratively add nodes to a partially-constructed graph. To add a node q to the graph, we query the current version of the graph to find the (approximate) neighbors of q, whose edges are then updated to include q. The result is an approximate k-NN or α-NN graph. An alternative to search-based construction is to begin with a randomly-initialized graph and apply a procedure called NN-Descent, where we query and update each node in the graph <ref type="bibr" target="#b10">(Dong et al., 2011)</ref>. Because the technique converges to a close approximation of the true graph in a small number of iterations, one may also use it to refine a graph obtained using other methods.</p><p>In the past, high-performance algorithms such as SWG used the graph directly <ref type="bibr" target="#b28">(Malkov et al., 2014)</ref>. However, the fastest indices no longer use the raw k-NN or α-NN graphs. Instead, the final search index is obtained by removing unnecessary edges (or pruning) and by adding edges (or diversifying) in order to improve the navigability of the graph. Each graph index implements slightly different methods to accomplish this improved navigability, but they are all driven by similar principles inherited from <ref type="bibr" target="#b0">Arya &amp; Mount (1993)</ref>. Below, we describe several algorithms from the top of the ANN-benchmarks leaderboard.</p><p>HNSW. Arguably the most popular graph index, HNSW is widely used for industrial applications <ref type="bibr" target="#b29">(Malkov &amp; Yashunin, 2018)</ref>. The HNSW index consists of a layered graph where each layer contains progressively more points from the dataset. Layer l is a pruned k-NN graph of all nodes from layer l − 1, with the addition of O(e l ) new nodes not contained in previous layers. Because the layer populations increase exponentially, the upper layers contain relatively few nodes while the lowest layer is a k-NN graph of the complete data. The intuition behind HNSW is that we may examine the upper layers to quickly identify a good neighborhood for further exploration in the lowest level. HNSW directly uses the pruning heuristic from <ref type="bibr" target="#b0">Arya &amp; Mount (1993)</ref>, which is as follows. Suppose that node B is one of the k nearest neighbors of node A. If node B is closer to one of the other neighbors of A than it is to A, then prune the link between A and B.</p><p>PANNG and ONNG. PANNG prunes a k-NN graph to remove edges when alternative paths exist between two nodes <ref type="bibr" target="#b17">(Iwasaki, 2016)</ref>. The PANNG algorithm first adds edges to the k-NN graph so that the graph is fully connected and undirected. Then, the algorithm removes edges according to the following pruning heuristic. Given two nodes A and B, if B is not one of the top k r neighbors of A and there exists a different path between A and B of length ≤ p, then prune the edge. The values of k r and p are hyperparameters. When p = 2 and k r = 0, the PANNG heuristic prunes the same links as the HNSW heuristic, except under a corner case that is rare in practice<ref type="foot" target="#foot_0">1</ref> . ONNG extends PANNG with an additional diversification step to ensure that each node has in-degree at least k I , then applies the same heuristic with p = 2 <ref type="bibr" target="#b18">(Iwasaki &amp; Miyazaki, 2018)</ref>. In other words, ONNG ensures that all nodes are reachable, then removes the longest edge from all triangles in the graph.</p><p>Other Algorithms. Several other contributions, such as FANNG <ref type="bibr" target="#b15">(Harwood &amp; Drummond, 2016)</ref> and methods based on NN-Descent, have at one point been strong contenders for the fastest search algorithm. The pruning techniques employed by such methods are similar to those described above. We refer the reader to <ref type="bibr" target="#b38">Shimomura et al. (2020)</ref> for recent review and thorough discussion.</p><p>Similarities among Graph Indices. Most graph algorithms begin by adding "long distance" edges between nodes and then pruning edges when a more desirable alternative path can be found. Under common hyperparameter choices, the pruning heuristics all behave similarly. It should come as no surprise that the resulting graphs have approximately the same performance on real-world search tasks <ref type="bibr" target="#b1">(Aumüller et al., 2020)</ref>. As previously mentioned, we expect these similarities to drive the generalization of our results (here illustrated for HNSW) to other popular graph indices based on the same principles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Searching Near Neighbor Graphs</head><p>Graph-based near neighbor search consists of a walk along the edges of the graph. At each node x encountered during the walk, we record the distance d(q, x) between the query q and the current node. We explore the graph until we arrive at a node which is closer to the query than any candidate or neighbor of a candidate seen so far.</p><p>Beam Search. There are two search algorithms in widespread use: greedy search and beam search<ref type="foot" target="#foot_1">2</ref> . In greedy search, we simply choose the nearest node to the query from the neighbors of our current node. We stop the search process once none of the outgoing edges lead to a node that is closer than the current node. Beam search is conceptually similar to greedy search, but we maintain a dynamic list of M candidates to investigate. Beam search allows us to explore outbound paths from any of the best M nodes seen so far. If none of the outgoing edges of the i th candidate lead to a closer node, then we consider the neighbors of candidate (i + 1). Once none of the M nodes have edges that lead to closer neighbors, we return the best points from the list of candidates as our search results. Note that when M = 1, beam search and greedy search are equivalent.</p><p>Beam Search Initialization. Many recent innovations in graph k-NN reduce to initialization methods that produce a good starting position for beam search. For example, HNSW uses greedy search over the hierarchical portion of the graph to find a seed node in the lowest layer, which is used to initialize beam search. PANNG and ONNG traverse a space-partitioning tree to extract seed nodes. Alternative options include initialization via locality-sensitive hashing, node centrality measures, and even random selection. Recent experiments suggest that the type of initialization is immaterial to performance <ref type="bibr" target="#b25">(Lin &amp; Zhao, 2019a;</ref><ref type="bibr">b)</ref>. <ref type="bibr" target="#b34">Prokhorenkova &amp; Shekhovtsov (2020)</ref> provide theoretical support for this idea by proving that beam search over an α-NN graph solves the approximate near neighbor problem given a "sufficiently close" initialization that can be found by repeated random sampling. Laarhoven (2017) prove a related result for greedy search, which relies on a similar initialization.</p><p>Initialization is also fairly inexpensive. To illustrate this, we constructed an HNSW graph on the SIFT1M dataset with k = 8 edges per node. We used beam search with M = 500 and found that the hierarchical part of the search process was responsible for only 2.9% (±1.9%) of the query time.</p><p>We then replaced the hierarchy with a process where we randomly select 50 nodes and use the closest option as the initialization, to reproduce the experiments by <ref type="bibr" target="#b25">Lin &amp; Zhao (2019a)</ref>. We found no statistically significant difference in terms of recall or query time over 10k queries. The practical takeaway is that under typical query conditions, initialization is an important but computationally insignificant part of the search process. Differences in the initialization method do not affect the validity of our experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Graph Reordering</head><p>In this section, we introduce the graph reordering methods that we apply to near neighbor graphs with the goal of speeding up the search. Formally, graph reordering can be seen as constructing a labeling or indexing function P : V → {1, . . . , N } that assigns each node v ∈ V in a graph to a unique integer index (or label) between 1 and N following a pre-specified rule or in order to maximize some objective. Many formulations are possible, but generally we require P to map connected nodes to similar (nearby) labels. The function P is then used as the memory layout for the graph, with node v assigned to memory location P (v).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Techniques</head><p>We consider three broad categories of reordering algorithms.</p><p>The first category contains algorithms that optimize P to maximize an objective function. The optimization problem is typically NP-hard, so we are usually forced to accept an approximate solution for P . The second category assigns labels to nodes based on their degree or other local graph features. Finally, graph partitioning algorithms can be repurposed to work as reordering algorithms by assigning a contiguous memory range to each partition. Below, we review several recent algorithms from each category. These are considered in Section 5 to assess their ability to accelerate near neighbor search.</p><p>Gorder. Gorder (graph-order) is a graph reordering algorithm that seeks to maximize the overlap between the neighborhoods of nodes with consecutive labels <ref type="bibr" target="#b39">(Wei et al., 2016)</ref>. Indeed, Gorder finds P by maximizing the number of shared edges among size-w blocks of consecutive nodes. This is a good proxy for cache efficiency because a block that contains many overlapping nodes is likely to avoid a cache miss, as each node's neighbors are stored no further than w memory locations away. Formally, the Gorder labeling function P GO can be found through the following maximization</p><formula xml:id="formula_1">P GO = arg max P u,v ∈ V s.t. |P (u)−P (v)|&lt;w S s (u, v) + S n (u, v), (1)</formula><p>where S s (u, v) indicates whether u and v are directly connected and S n (u, v) counts how many common neighbors they have. In other words, Gorder maximizes the average neighborhood overlap between nodes that are within w positions of each other under the labeling function. Intuitively, two nodes will be placed together if they share a direct edge or, even if that is not the case, if they share many common neighbors. Maximizing the objective in (1) is NP-hard, but the Gorder algorithm in <ref type="bibr" target="#b39">Wei et al. (2016)</ref> provides a 1/(2w)-approximate solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reverse Cuthill Mckee.</head><p>The bandwidth of a matrix is defined as the maximum distance of a nonzero element from the main diagonal. The Reverse Cuthill Mckee (RCM) algorithm <ref type="bibr" target="#b9">(Cuthill &amp; McKee, 1969;</ref><ref type="bibr" target="#b14">George, 1971</ref>) is a reordering method originally introduced to minimize the bandwidth of a sparse symmetric matrix. If we apply this method to an adjacency matrix, then the corresponding optimization problem has an immediate interpretation in terms of graph reordering <ref type="bibr" target="#b2">(Auroux et al., 2015</ref>)</p><formula xml:id="formula_2">P RCM = arg min P max (u,v)∈E |P (u) − P (v)|, (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where E is the edge set of the graph of interest. From (2) it follows that the RCM objective is to minimize the maximum label difference between connected nodes. Similar to Gorder, the problem is difficult: the exact solution is NP-complete. The RCM algorithm is a heuristic method based on breadth-first search to find a function P with low (but not necessarily optimal) bandwidth.</p><p>MLOGA and MLINA. The objective in (2) motivated extensions that focus on an aggregated measure (as opposed to the maximum) of the discrepancies between labels of connected nodes. Both the minimum logarithmic arrangement (MLOGA) and the minimum linear arrangement (MLINA), which originally arose in the context of social network compression, are examples of these approaches <ref type="bibr" target="#b7">(Chierichetti et al., 2009)</ref>. More precisely, MLINA seeks to minimize the sum of label discrepancies</p><formula xml:id="formula_4">P MLN = arg min P (u,v)∈E |P (u) − P (v)|,<label>(3)</label></formula><p>whereas MLOGA first applies a logarithmic transformation to these discrepancies</p><formula xml:id="formula_5">P MLG = arg min P (u,v)∈E log(|P (u) − P (v)|),<label>(4)</label></formula><p>where, again, E is the edge set of the graph of interest. As expected, both problems are NP-hard. However, specialized heuristics have been developed to approximate the solution of both MLOGA <ref type="bibr" target="#b7">(Chierichetti et al., 2009;</ref><ref type="bibr" target="#b35">Safro &amp; Temkin, 2011)</ref> and MLINA <ref type="bibr" target="#b39">(Wei et al., 2016)</ref>.</p><p>Degree Sorting. Degree sorting is a lightweight reordering algorithm based on the idea that high-degree nodes are likely to share many edges. Indeed, many practical graphs obey a power-law degree distribution, where a small number of nodes form a densely connected sub-graph. For undirected graphs with a power law degree distribution, degree sorting will create a group of neighboring high-degree nodes in the first contiguous memory block. To obtain P , we simply sort the nodes in descending degree order and let P (v) be the sorted rank of node v. Since degree sorting only requires local node information, it is orders of magnitude faster than the optimization-based methods in ( <ref type="formula">1</ref>)-( <ref type="formula" target="#formula_5">4</ref>).</p><p>Hub Sorting. Hub sorting <ref type="bibr" target="#b40">(Zhang et al., 2017)</ref> is similar to degree sorting, but with the caveat that we only sort hubs  (nodes with many connections). The algorithm first splits the nodes into two groups (hubs and non-hubs) based on a degree threshold. The threshold is set as the average degree, so that hubs are defined as nodes with greater-than-average degree. To find P , the hubs are sorted by degree and the non-hubs keep their original ordering. Experiments have show that degree and hub sorting are not always beneficial: they may even slow down graph processing if the original ordering of the graph had good locality properties <ref type="bibr" target="#b3">(Balaji &amp; Lucia, 2018)</ref>. Selective hub sorting was introduced to address this issue. In selective hub sorting, we only reorder a graph if the hubs are densely connected to each other. To decide whether to sort the graph, we compute the packing factor, a computationally inexpensive diagnostic value that predicts whether hub sorting is likely to be effective <ref type="bibr" target="#b3">(Balaji &amp; Lucia, 2018)</ref>.</p><p>Hub Clustering. Hub clustering is the same as hub sorting, but without sorting the hubs after separating high and low degree nodes <ref type="bibr" target="#b3">(Balaji &amp; Lucia, 2018)</ref>. The primary advantage of hub clustering is that it is an incredibly lightweight reordering algorithm that can be accomplished in a single pass through the graph.</p><p>Degree-Based Grouping. Degree-Based Grouping (DBG) <ref type="bibr" target="#b12">(Faldu et al., 2019)</ref> is an extension of hub clustering to multiple groups. Nodes are divided into w groups based on degree ranges associated with each group. The groups are sorted in descending degree order, but the order of the nodes within each group is not changed. The authors of <ref type="bibr" target="#b12">(Faldu et al., 2019)</ref> use logarithmically spaced thresholds to evenly distribute nodes among groups for power-law graphs. Since k-NN graphs do not have power-law degree distributions, our implementation uses the w quantiles of the degree distribution instead.</p><p>Graph Partitioning. Graph partitioning is a very wellstudied area with a large number of established methods.</p><p>One can create a graph reordering algorithm from any graph partitioning method by using the graph partitioning method to assign nodes to groups and using the same labeling scheme as in DBG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Greedy Search and Graph Reordering</head><p>In this section, we show that objective-based graph ordering methods improve the complexity of greedy search under the idealized cache model of computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Experiment Setup. Our experiments measure index performance using wall-clock query time under different reordering strategies. Since cache misses are sensitive to a variety of external factors, we took extra care to design an objective comparison. For example, multicore systems often share the L3 CPU cache, allowing cache performance to be affected by unrelated programs running on a different core.</p><p>To obtain accurate and repeatable results, we designed our experiments to mimic real-world production environments while controlling as many external factors as possible. We benchmark the system after a "warm start," where assets To avoid the difficulties associated with timing very short events, we record the total time needed to perform 10k queries and report the average query time. Finally, we restrict the query program to a single core, and we ensure that the benchmark is the only program running on the server. We run all of our experiments on a server with 252 GB of RAM, 28 Intel Xeon E5-2697 CPUs, and a shared 36 MB L3 cache.</p><p>We measure cache miss rates using the Linux perf tool to record hardware CPU counters. The perf tool measures hardware counters for events such as data reads and cache hits. We use these counters to compute the cache miss rate by dividing the number of data cache misses by the number of data references. We run our most successful reordering methods (Gorder and RCM) on SIFT100M and report results for the L1, L2, L3 and TLB caches. To validate these measurements, we also provide experiments with the cachegrind tool for all reordering algorithms. Cachegrind runs the program through a virtual processor that records every instruction and cache reference while executing the program. We annotate the source code of our HNSW program and report the cache misses for the lines of code which execute node traversals and distance computations.</p><p>Implementation Details. We extended the nmslib implementation of HNSW to support graph reordering. We im-Figure <ref type="figure">3</ref>. 99 th percentile of latency (P99) for SIFT100M (lower is better). Note the log scale. We observe an improvement of 17% with RCM and 30% with Gorder.</p><p>plemented minor changes to the index to speed up graph construction and facilitate reordering. However, the memory access pattern and computational aspects remain unchanged.</p><p>To ensure that our performance numbers are realistic, we verified that our implementation uses the same memory layout, requires the same number of distance computations, and produces the same graph as nmslib-HNSW. We use the optimized, flat layout for nodes to avoid memory fragmentation and provide the most competitive baseline possible. We represent links as a fixed-size array of integers and embed the data alongside the node label and link array.</p><p>Objective-Based Reordering. Since our k-NN search benchmarks involve datasets with millions of nodes, we avoid algorithms that are known to scale poorly to large graphs. We implemented Gorder and RCM, but we did not implement MLOGA or MLINA because these algorithms have very high runtime without tangible improvements over Gorder or RCM <ref type="bibr" target="#b39">(Wei et al., 2016)</ref>.</p><p>Degree-Based Reordering. We follow the suggestion of <ref type="bibr" target="#b3">Balaji &amp; Lucia (2018)</ref> to use the in-degree as a local feature for hub clustering and degree-based methods because beam search is a so-called push application<ref type="foot" target="#foot_2">3</ref> . We use the average degree as the threshold for hub clustering and sorting, and we use 8 groups given by the 8 quantiles of the degree distribution for DBG.</p><p>At first glance, it may seem that degree-based reordering algorithms are inappropriate for near neighbor search because the ideal k-NN graph has a constant degree distribution. However, this is not true. Popular graph algorithms accept the maximum number of links as a hyperparameter, but the pruning and diversification heuristics produce graphs with nontrivial variations in node degree. Figure <ref type="figure" target="#fig_1">2</ref> shows the out-degree distribution of several real-world near neighbor graphs. While these graphs do not follow a true power-law distribution, there is enough variation that we may reasonably expect lightweight reordering to perform well.</p><p>Datasets. We use large datasets that are representative of embedding search tasks. We perform experiments on datasets from ANN-benchmarks as well as on 10M and 100M sized subsets of the SIFT1B and DEEP1B benchmark tasks. Table <ref type="table" target="#tab_2">3</ref> contains information about our datasets. In all of our experiments, we request the top 100 neighbors of a query and we report the recall of the top 100 ground truth neighbors.</p><p>Hyperparameters. For construction, HNSW requires the determination of two parameters: the maximum number of edges for each node (k c ) and the size of the beam search buffer (M c ) used to find the k c neighbors during graph construction. We set M c = 100 and construct indices for  From these options, we select the index with the best recall-latency trade-off (for recall &gt; 0.95) and use that index for our graph reordering experiments. While it is true that the best hyperparameters change based on the recall, there is typically a clear winner for the high-recall regime. To query the index, HNSW requires the beam search buffer size (M q ), which controls the trade-off between recall and query latency. We reorder the graph and issue the same set of 10K queries for each ordering. When we query the index, we vary the beam search buffer size M q from 100 to 5000.</p><p>Results. Figure <ref type="figure" target="#fig_2">4</ref> shows the effect of graph reordering on the query time vs. recall tradeoff. We report the R100@100 recall, or the recall of the top 100 ground truth neighbors among the top 100 returned search results. For a given recall value, we calculate the speedup as the ratio of average latency without/with reordering, where the averages are over 10K queries and 5 runs. Figure <ref type="figure" target="#fig_0">1</ref> presents the cost of reordering the graph using various methods and shows how reordering time scales with the number of nodes N and the maximum degree k c of each node.  <ref type="figure">3</ref>, to ensure that our altered graph layout does not increase the tail latency. We find that Gorder and RCM improve both the P99 and average latencies by at least 20% in the high-recall regime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>Our experiments suggest that graph reordering could become a standard preprocessing step to improve the query time, since it does not substantially inflate the index construction time. Reordering only affects the representation of nodes in memory and does not change the recall, search algorithm, or other properties of the graph index. Objectivebased reordering algorithms are most effective for this problem, with a typical speedup of 10% on small datasets (N &lt; 1M) and speedups of up to 40% on large datasets and in the high-recall regime (Figure <ref type="figure" target="#fig_2">4</ref>). The Gorder and RCM algorithms consistently yield speedups in all our experiments. This likely occurs because the reordering objective is a good proxy for cache coherence. It is reasonable to conclude that objective-driven methods will rarely (if ever) slow down the index. However, lightweight reordering algorithms are far less effective for near neighbor search than for applications previously considered in the literature. Although degree-based clustering does perform well on some tasks (e.g. SIFT10M), near neighbor graphs seem to have a pathological degree distribution for such methods.</p><p>Cost of Reordering. Most studies of graph reordering are focused on applications such as PageRank, where the graph is processed a small number of times to obtain an output. In such cases, the objective is to obtain an end-to-end speedup, which penalizes long graph reordering times. Such applications favor lightweight (but less effective) algorithms over more complex reordering techniques. However, the production requirements of near neighbor search are exactly the opposite: a single index may be queried millions of times over its life cycle. Near neighbor graph construction is already an expensive offline task, so the runtime disadvantage of graph reordering is less problematic for near neighbor search than for other applications.</p><p>Nonetheless, we observe that the graph reordering time is, in many cases, negligible when compared to the graph construction time. For example, our most expensive reordering algorithm (Gorder) was a factor of 10x faster than HNSW construction for most datasets (Figure <ref type="figure" target="#fig_0">1</ref>). Even though the reordering time of Gorder scales quadratically with degree, reordering is still feasible even for large graph indices on datasets such as DEEP10M with many connected neighbors (up to k = 120). This is likely a consequence of near neighbor graph construction heuristics, which decrease the average node degree with aggressive pruning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Benefits of Reordering.</head><p>Reordering is an index-agnostic method to improve the performance of graph-based near neighbor search. Because reordering depends on the node access pattern of beam search, which is common to most algorithms, reordering is applicable to a wide range of practical search tasks. It should be noted that most real-world deployments function under strict latency requirements: a max-imum search time of 20 ms is a common constraint <ref type="bibr" target="#b32">(Nigam et al., 2019)</ref>. A 20% improvement in search time allows the system to perform more sophisticated processing of the search results, or alternatively to operate at a higher recall.</p><p>Extensions. Reordering is likely to exhibit synergy with other algorithms and performance tricks used in production systems. Ideas from graph reordering may benefit partitionbased search because recent algorithms for locality-sensitive hashing (LSH) use k-NN graph cuts to form data partitions <ref type="bibr" target="#b11">(Dong et al., 2020)</ref>. For example, one could reorder partition locations in memory to speed up multi-probe methods that access multiple nearby partitions.</p><p>Near neighbor graphs are also frequently combined with sample compression methods such as product quantization or other codebooks <ref type="bibr" target="#b20">(Jegou et al., 2010)</ref>. Recent experiments with preprocessing transformations suggest that it is beneficial to perform the graph search on a subspace of reduced dimension <ref type="bibr" target="#b34">(Prokhorenkova &amp; Shekhovtsov, 2020)</ref>. This could amplify the effects of graph reordering. When the data size is small, a larger sub-graph can fit into the CPU cache, improving the benefits of memory locality for graph processing. This idea is supported by our experiments, where we observe larger speedups for SIFT and DEEP than for MNIST and GIST, which have a large per-sample storage cost. Thus, graph reordering may show even greater benefits when integrated with quantized search indices and dimensionality reduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We introduce graph reordering to the popular and important task of graph-based near neighbor search. We find that by relabeling the graph, we can obtain query-time speedups of up to 40%. The reordering process is inexpensive when compared to graph construction, and our results apply broadly to most graph-based search indices. Therefore, we expect that reordering will become a standard preprocessing step for graph-based near neighbor search.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure1. Graph reordering scales to large datasets and complex near neighbor graphs. In our experiments, the reordering algorithms required less time to run than graph construction (left). Reordering is feasible even for large graphs with many nodes (SIFT with k = 16, middle) and for densely connected graphs (k ≤ 120) on large datasets (GIST1M, right).</figDesc><graphic url="image-1.png" coords="6,55.86,67.06,158.40,118.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. In practice, pruned k-NN graphs are far from being kregular. The figure shows the empirical out-degree distribution of the HNSW base layer of three graph indices. Although we begin with k = 32 outbound edges per node, the pruning heuristic removes over half of the edges from the network.</figDesc><graphic url="image-4.png" coords="6,81.20,243.36,180.00,135.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Effect of graph reordering on the query time vs. recall tradeoff. Reordering algorithms above the black line have faster query time than the original ordering. Algorithms below the line cause a slowdown. The speedup changes with the recall because the memory access pattern of beam search changes with the buffer size.</figDesc><graphic url="image-9.png" coords="8,55.86,186.86,158.40,118.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Perf stat results for cache misses and TLB misses on SIFT100M. We report the L1, L2, L3 misses as the hardware counter for misses divided by the total number of data loads.</figDesc><table><row><cell cols="5">Algorithm L1 (%) L2 (%) L3 (%) TLB (%)</cell></row><row><cell>Original</cell><cell>19.53</cell><cell>13.9</cell><cell>6.5</cell><cell>3.85</cell></row><row><cell>RCM</cell><cell>17.37</cell><cell>7.61</cell><cell>5.1</cell><cell>2.56</cell></row><row><cell>Gorder</cell><cell>14.46</cell><cell>9.6</cell><cell>4.0</cell><cell>2.14</cell></row><row><cell cols="5">such as the index, query and, data are pre-loaded into RAM.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Cache miss rates for node traversals and distance computations on SIFT100M (lower is better). It should be noted that due to the 100x latency difference between cache and RAM, small improvements to the cache miss rate can substantially speed up an algorithm. The ranking of algorithms by cache miss rate agrees with the ranking of algorithms by speedup in Figure4.</figDesc><table><row><cell cols="6">Original Gorder RCM DegSort (in) DegSort (out) HubSort HubCluster DBG</cell></row><row><cell>L1 (%) 22.76 19.28 20.61</cell><cell>22.76</cell><cell>22.76</cell><cell>22.85</cell><cell>22.77</cell><cell>22.81</cell></row><row><cell>L3 (%) 13.56 8.32 8.91</cell><cell>13.55</cell><cell>13.56</cell><cell>13.63</cell><cell>13.51</cell><cell>13.34</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Datasets. Each dataset has N entries, d features, and requires "vector size" bytes for each entry.</figDesc><table><row><cell>Dataset</cell><cell>N</cell><cell>d</cell><cell>vector size</cell></row><row><cell>GIST</cell><cell>1 M</cell><cell>960</cell><cell>3.8 kB</cell></row><row><cell>SIFT</cell><cell cols="2">10 -100 M 128</cell><cell>128 B</cell></row><row><cell>DEEP</cell><cell cols="2">10 -100 M 96</cell><cell>384 B</cell></row><row><cell>MNIST</cell><cell>60 k</cell><cell>784</cell><cell>3.1 kB</cell></row><row><cell cols="2">k c ∈ {4, 8, 16, 32, 64, 96}.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>contains</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">This occurs when the intermediary node C on the path from A to B is not a neighbor of B. This is rare when B and C are near neighbors of A because d(B, C) ≤ d(A, B) + d(A, C).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">We use the definition of beam search from<ref type="bibr" target="#b34">Prokhorenkova &amp; Shekhovtsov (2020)</ref>. The graph literature uses a slightly different definition, where every candidate is explored (rather than the best one).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">See Section 2 of<ref type="bibr" target="#b3">Balaji &amp; Lucia (2018)</ref> for the definition of push/pull applications.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Approximate nearest neighbor queries in fixed dimensions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Mount</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SODA</title>
				<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="271" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Annbenchmarks: A benchmarking tool for approximate nearest neighbor algorithms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Aumüller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bernhardsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Faithfull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Systems</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page">101374</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reordering very large graphs for fun &amp; profit</title>
		<author>
			<persName><forename type="first">L</forename><surname>Auroux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Erra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Web AlGorithms</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">When is graph reordering an optimization? studying the effect of lightweight graph reordering across applications and input graphs</title>
		<author>
			<persName><forename type="first">V</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lucia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Symposium on Workload Characterization (IISWC)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="203" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cover trees for nearest neighbor</title>
		<author>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
				<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Engineering efficient and effective non-metric space library</title>
		<author>
			<persName><forename type="first">L</forename><surname>Boytsov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Naidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Similarity Search and Applications</title>
				<editor>
			<persName><forename type="first">N</forename><surname>Brisaboa</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">O</forename><surname>Pedreira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Zezula</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="280" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Closest-point problems simplified on the ram</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth annual ACM-SIAM symposium on Discrete algorithms</title>
				<meeting>the thirteenth annual ACM-SIAM symposium on Discrete algorithms</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="472" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On compressing social networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chierichetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lattanzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitzenmacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Panconesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="219" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast construction of k-nearest neighbor graphs for point clouds</title>
		<author>
			<persName><forename type="first">M</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="599" to="608" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Reducing the bandwidth of sparse symmetric matrices</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cuthill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mckee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1969 24th national conference</title>
				<meeting>the 1969 24th national conference</meeting>
		<imprint>
			<date type="published" when="1969">1969</date>
			<biblScope unit="page" from="157" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient k-nearest neighbor graph construction for generic similarity measures</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Moses</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on World wide web</title>
				<meeting>the 20th international conference on World wide web</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="577" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning space partitions for nearest neighbor search</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">P</forename><surname>Razenshteyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wagner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A closer look at lightweight graph reordering</title>
		<author>
			<persName><forename type="first">P</forename><surname>Faldu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Diamond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Symposium on Workload Characterization (IISWC)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Devise: a deep visualsemantic embedding model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Neural Information Processing Systems</title>
				<meeting>the 26th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Computer implementation of the finite element method</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>George</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971">1971</date>
		</imprint>
		<respStmt>
			<orgName>STANFORD UNIV CA DEPT OF COMPUTER SCIENCE</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fanng: Fast approximate nearest neighbour graphs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5713" to="5722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName><forename type="first">P.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Information &amp; Knowledge Management</title>
				<meeting>the 22nd ACM international conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2333" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pruned bi-directed k-nearest neighbor graph for proximity search</title>
		<author>
			<persName><forename type="first">M</forename><surname>Iwasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Similarity Search and Applications</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="20" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Optimization of indexing based on k-nearest neighbor graph for proximity search in high-dimensional data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Iwasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Miyazaki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.07355</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Maurya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nagarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Crushev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.08942</idno>
		<title level="m">A survey on locality sensitive hashing algorithms and their applications</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Product quantization for nearest neighbor search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="117" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08734</idno>
		<title level="m">Billion-scale similarity search with gpus</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Billion-scale similarity search with gpus</title>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">From word embeddings to document distances</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">I</forename><surname>Kolkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on International Conference on Machine Learning</title>
				<meeting>the 32nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="957" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Graph-based time-space trade-offs for approximate near neighbors</title>
		<author>
			<persName><forename type="first">T</forename><surname>Laarhoven</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.03158</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">P.-C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-L</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02077</idno>
		<title level="m">A comparative study on hierarchical navigable small world graphs</title>
				<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Graph based nearest neighbor search: Promises and failures</title>
		<author>
			<persName><forename type="first">P.-C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-L</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02077</idno>
		<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh IEEE international conference on computer vision</title>
				<meeting>the seventh IEEE international conference on computer vision</meeting>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Approximate nearest neighbor algorithm based on navigable small world graphs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Malkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ponomarenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Logvinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Krylov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Systems</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="61" to="68" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">A</forename><surname>Malkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Yashunin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Naidan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Boytsov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Malkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Novak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.05470</idno>
		<title level="m">Non-metric space library manual</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semantic product search</title>
		<author>
			<persName><forename type="first">P</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lakshman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shingavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Teo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2876" to="2885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mash: fast genome and metagenome distance estimation using minhash</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Ondov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Treangen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Melsted</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Mallonee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Phillippy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome biology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Graph-based nearest neighbor search: From practice to theory</title>
		<author>
			<persName><forename type="first">L</forename><surname>Prokhorenkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shekhovtsov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7803" to="7813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multiscale approach for the network compression-friendly ordering</title>
		<author>
			<persName><forename type="first">I</forename><surname>Safro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Temkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Discrete Algorithms</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="190" to="202" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Object recognition supported by user interaction for service robots</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Kimia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>IEEE</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="291" to="296" />
		</imprint>
	</monogr>
	<note>Metric-based shape retrieval in large databases</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Nearestneighbor methods in learning and vision</title>
		<author>
			<persName><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">377</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A survey on graph-based methods for similarity searches in metric spaces</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Shimomura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Oyamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Vieira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Kaster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Systems</title>
		<imprint>
			<biblScope unit="page">101507</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Speedup graph processing by graph ordering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 International Conference on Management of Data</title>
				<meeting>the 2016 International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1813" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Making caches work for graph analytics</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kiriansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mendis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amarasinghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Big Data (Big Data)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="293" to="302" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
