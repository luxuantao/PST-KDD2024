<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Harnessing adversarial examples with a surprisingly simple defense</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Ali</forename><surname>Borji</surname></persName>
							<email>aliborji@gmail.com</email>
						</author>
						<title level="a" type="main">Harnessing adversarial examples with a surprisingly simple defense</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>I introduce a very simple method to defend against adversarial examples. The basic idea is to raise the slope of the ReLU function at the test time. Experiments over MNIST and CIFAR-10 datasets demonstrate the effectiveness of the proposed defense against a number of strong attacks in both untargeted and targeted settings. While perhaps not as effective as the state of the art adversarial defenses, this approach can provide insights to understand and mitigate adversarial attacks. It can also be used in conjunction with other defenses.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The lockdown has not been too bad! After all, we found some time to explore what we always wanted to but did not have time for it. For me, I have been passing the time by immersing myself in adversarial ML. I was playing with the adversarial examples tutorial in PyTorch 2 and came across something interesting. So, I decided to share it with you. It is a simple defense that works well against untargeted attacks, and to some extent against targeted ones. Here is how it goes. The idea is to train a CNN with the ReLU activation function but increase its slope at the test time (Fig. <ref type="figure" target="#fig_0">1</ref>). Lets call this function Sloped ReLU or SReLU for short: SReLU(α, x) = α max(0, x), where α is the slope. SReLU becomes ReLU for α = 1. To investigate this idea, I ran the following CNNs (Fig. <ref type="figure" target="#fig_0">10</ref>) over MNIST <ref type="bibr" target="#b0">[1]</ref>:</p><p>Conv ⇒ Pool ⇒ SReLU ⇒ Conv ⇒ Pool ⇒ SReLU ⇒ FC ⇒ SReLU ⇒ FC and over CIFAR- <ref type="bibr">10 [2]</ref> (referred to as CIFAR10-CNN1):</p><p>Conv ⇒ SReLU ⇒ Pool ⇒ Conv ⇒ SReLU ⇒ Pool ⇒ FC ⇒ SReLU ⇒ FC ⇒ SReLU ⇒ FC I also tried a variation of the latter network with SReLUs only after the first two FC layers (referred to as CIFAR10-CNN2). I chose α ∈ {0.5, 1, 2, 5, 10, 100}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Train with</head><p>Test with y= max(0,x)</p><p>x y y= a max(0,x) Here, I emphasize on the FGSM attack since it is very straightforward <ref type="bibr" target="#b2">[3]</ref>. To craft an adversarial example x adv , FGSM adds a tiny portion of the gradient of the loss w.r.t the input image back to the input image (i.e. gradient ascent in loss):</p><formula xml:id="formula_0">x adv ← clip(x + × sign(∇ x J(θ, x, y)))<label>(1)</label></formula><p>The perturbed image needs to be clipped to the right range (here [0,1] in all experiments over both datasets). ∈ [0, 1] balances the attack success rate versus imperceptibility. = 0 corresponds to the model performance on the original test set (i.e. no perturbation). To gain a higher attack rate more perturbation (i.e. larger ) is needed which leads to a more noticeable change (and vice versa).</p><p>The above formulation is for the untargeted attack. For the targeted attack, instead of increasing the loss for the true class label, we can lower the loss for the desired target class label y targ (or we could do both):</p><formula xml:id="formula_1">x adv ← clip(x − × sign(∇ x J(θ, x, y targ )))<label>(2)</label></formula><p>In addition to FGSM, I also considered a number of other strong attacks including BIM (also known as IFGSM; iterative FGSM) <ref type="bibr" target="#b3">[4]</ref>, RFGSM <ref type="bibr" target="#b4">[5]</ref>, StepLL <ref type="bibr" target="#b3">[4]</ref>, PGD-40 <ref type="bibr" target="#b5">[6]</ref>, and DeepFool <ref type="bibr" target="#b6">[7]</ref>. In almost all of these attacks (except DeepFool for which I varied the number of iterations), there is a parameter that controls the magnitude of perturbation (here represented by ). In what follows I will show the results over MNIST and CIFAR-10 datasets against both untargeted and targeted attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Performance against untargeted attacks</head><p>Results are shown in Fig. <ref type="figure" target="#fig_1">2</ref> over entire test sets of datasets. I did not find the SReLU slopes smaller than one very interesting but I am including them here for the sake of comparison. Their effect is not consistent. They are useful against targeted attacks but almost always hinder both classifier accuracy and robustness in untargeted attack settings.</p><p>Over MNIST, at α = 1 and = 0 (top left panel in Fig. <ref type="figure" target="#fig_1">2</ref>) classifier accuracy is around 98% and gradually falls as grows. For α ∈ {2, 5}, the performance loss starts to recover. Finally, for α ∈ {10, 100} the classifier has not been impacted at all, hence completely failing the attack. Classifier performance as a function of slope (averaged over epsilons) for each attack type is shown in the middle row of Fig. <ref type="figure" target="#fig_1">2</ref>. The ReLU defense does very well against FGSM, BIM, RFGSM and PGD attacks. Over StepLL and DeepFool, however, it neither helped nor harmed the classifier. Results over the CIFAR-10 dataset are consistent with the MNIST results (middle and right columns in Fig. <ref type="figure" target="#fig_1">2</ref> corresponding to CIFAR10-CNN1 and CIFAR10-CNN2, respectively). On this dataset, the classifier had around 60% accuracy on the clean test set ( = 0) and went down with more perturbation. Here again increasing α led to better defense, although sometimes at the expense of accuracy. Please see Fig. <ref type="figure" target="#fig_2">3</ref> for the performance of each attack type as a function of perturbation magnitude.</p><p>Are these findings specific to SReLU function? To answer this question, I swapped the SReLU with other activation functions and measured the FGSM performance. Now the attacks became even more damaging (Fig. <ref type="figure" target="#fig_1">2</ref>; bottom row) indicating that SReLU does better.</p><p>To assess this defense against a wider range of adversarial attacks, I conducted a larger scale analysis using the Foolbox code repository <ref type="bibr" target="#b7">[8]</ref> over the first 2K images of each dataset. The proposed defense was tested against 25 attacks utilizing different L norms or image transformations (e.g. Gaussian blurring). The top row in Fig. <ref type="figure" target="#fig_3">4</ref> shows the mean classifier accuracy averaged over all attacks. Increasing α improved the defense, although there seems to be a trade-off between accuracy and robustness. Please compare the last vs. = 0 in the top row of Fig. <ref type="figure" target="#fig_3">4</ref>. Looking across the attacks (bottom row in Fig. <ref type="figure" target="#fig_3">4</ref> shown only for the last ; 0.3 over MNIST and 0.1 over CIFAR), however, reveals that the boost in improvement comes only from few attacks against which the defense has done a great job, in particular FGSM and its variants (e.g. PGD). Against the VirtualAdversarial attack, the proposed defense made the situation much worse. Overall, it seems that this defense works better against gradient based attacks as opposed to image degradation ones such as additive noise, salt and pepper noise, and inversion attack. Nonetheless, the ReLU defense was able to recover performance loss around 15% over MNIST and around 10-15% over CIFAR-10 (averaged over attacks; compare α = 100 vs. α = 1 in the bottom row of Fig. <ref type="figure" target="#fig_3">4</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Performance against targeted attacks</head><p>Performance of the ReLU defense against the targeted FGSM attack is reported in Fig. <ref type="figure" target="#fig_4">5</ref> over the full test sets of MNIST and CIFAR-10 datasets. The y axis here shows the success rate of the attack in fooling the CNN to classify an image as the desired target class (if it has not already been classified so). As expected, increasing (perturbation magnitude) leads to higher attack accuracy. Over both datasets, increasing the SReLU slope (above 1) reduces the attack success rate which means better defense. Here, ReLU defense seems to perform about the same for slopes greater than one.</p><p>Defense results over individual classes are shown in Fig. <ref type="figure" target="#fig_5">6</ref>. Increasing the slope helped in 6 classes, out of 10, using both MNIST CNN and CIFAR10-CNN1. Using CIFAR10-CNN2, 4 classes were defended better. In the remaining cases, the defense tied with the slope of 1 (i.e. no defense) or slightly deteriorated the robustness. On average, it seems that ReLU defense is effective against the targeted FGSM attack, although not as good as its performance against the untargeted FGSM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Discussion and Conclusion</head><p>What is happening? I suspect the reason why this defense works is because it enhances the signal-tonoise ratio. In the untargeted attack setting, increasing the loss leads to suppressing pixels that are important in making the correct prediction while boosting irrelevant features. Since relevant features are associated with higher weights, increasing the SReLU slope will enhance those feature more compared to the irrelevant ones, hence recovering good features. For the same reason, this defense is not very effective against targeted attacks. In this setting, the attacker aims to lower the loss in favor of the target class. Raising the SReLU slope results in enhancing features that are important to the target class, as well as the true class. This ignites a competition between two sets of features which can go either way. This explanation is reminiscent of attention mechanisms in human vision (and also in computer vision), in particular neural gain modulation theories. These theories explain behavioral and neural data in a variety of visual tasks such as discrimination and visual search <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. On a related note, elevating the bias of neurons in different layers, as is done in <ref type="bibr" target="#b10">[11]</ref>, may lead to similar observations. Gradient obfuscation. An alternative, and perhaps more plausible, explanation is gradient explosion.</p><p>Increasing the SReLU slope causes the gradients to explode during back-propagation. Strangely, however, this does not stop the targeted attacks! In this regard, the ReLU defense falls under the category of defenses that aim to mask or obfuscate gradients (e.g. <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref>). Some works have shown that these methods give a false sense of security and are thus breakable <ref type="bibr" target="#b15">[16]</ref>. Training and testing the network with the same slope (greater than one) did not improve the robustness, which could strengthen the gradient explosion reasoning.</p><p>To further investigate gradient obfuscation, I followed the approach proposed in <ref type="bibr" target="#b15">[16]</ref> known as Backward Pass Differentiable Approximation (BPDA). A second CNN, with the same architecture used in experiments, was trained. Instead of the cross entropy loss over predictions and ground truth labels, the model was trained with the cross entropy over logits of the two networks. The adversarial examples crafted for the second network were then applied to the first one. Results are shown in Fig. <ref type="figure">8</ref>. As it can be seen, the original models are severely hindered by the transfer attack using both FGSM and PGD attacks. The accuracy, however, is still higher than accuracy using attacks directly on the original models.</p><p>Fig. <ref type="figure">9</ref> shows tSNE illustration of the last fully connected layer over the MNIST dataset for different ReLU slopes. As the slope increases, the clusters become less compact. However, the distance between digits increases because of the higher magnitude of neuron activations due to the higher slopes.</p><p>Is increasing the SReLU slope the same as scaling up the image by a factor? In general, No. Notice that αmax(0, x) = max(0, αx), for α ≥ 0 and x ≥ 0. For a linear network with positive weights, the answer is yes, but for non-linear CNNs comprised of positive and negative weights the answer is no. Just to make sure, I ran an experiment in which I multiplied the pixel values by α and computed the classifier performance under the untargeted FGSM attack. Results are shown in Fig. <ref type="figure" target="#fig_6">7</ref>. Clipping the pixel values to the [0,1] range (after scaling) did not improve the robustness. Interestingly, without clipping the pixel values, results resemble those obtained by increasing the SReLU slope (top right panel in Fig. <ref type="figure" target="#fig_6">7</ref>; compare with Fig. <ref type="figure" target="#fig_1">2</ref>.). This suggests that maybe instead of increasing the SReLU slope we can just scale the pixel values! Results over the CIFAR-10 dataset, however, do not support this hypothesis. On this dataset, scaling does not help robustness with or without clipping. The discrepancy of results over two datasets can be attributed to the fact that MNIST digits are gray level whereas CIFAR-10 images are RGB. Increasing the pixel intensity of MNIST digits leads to maintaining high classifier accuracy while at the same time making the job of the attacker harder since now he has to increase the magnitude of the perturbation. In conclusion, this analysis suggests that increasing pixel values is not as effective as increasing the SReLU slope. This, however, needs further investigation. If the opposite is true, then it will have the following unfortunate consequence. To counter the ReLU defense, the attacker can simply submit the scaled down version of the input image.</p><p>Last but not least, that fact that this simple defense consistently works against some strong attacks (e.g. PGD) is surprising. But please take these findings with a healthy dose of scepticism as I am still investigating them. A similar work has also been reported in <ref type="bibr" target="#b16">[17]</ref>. Future work should evaluate the proposed approach on other models (e.g. ResNet), datasets (e.g. ImageNet), and against black-box attacks. Further, saliency and interpretability tools such as <ref type="bibr" target="#b17">[18]</ref> can be used to explain the observed phenomenon.    For some classes, the defense exacerbates the problem and helps the attack. On average, however, the defense seems to be working, although not as effective as in the untargeted setting. Lower y value here means better defense.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sketch of the the proposed defense. Just increase the ReLU slope at the test time!</figDesc><graphic url="image-1.png" coords="1,209.16,556.55,195.52,104.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Performance of the ReLU defense against untargeted attacks. Left column: MNIST. Middle column: CIFAR-10 with all ReLUs replaced with SReLU, i.e. CIFAR10-CNN1. Right column: CIFAR-10 with SReLUs only after the first two FC layers, i.e. CIFAR10-CNN2. All 10K test images of each dataset were used. Rows from top to bottom: ReLU defense against the FGSM attack, the defense success rate (i.e. accuracy loss recovery) as a function of SReLU slope averaged over all epsilons (iterations for DeepFool), and the effect of switching to a different activation function against the FGSM attack. Increasing the SReLU slope improves the performance significantly, except over StepLL and DeepFool attacks. = 0 corresponds to the classifier accuracy with no input perturbation. Higher y value here means better defense.</figDesc><graphic url="image-8.png" coords="3,108.00,325.18,130.68,130.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Results of the ReLU defense against untargeted attacks over MNIST (left column), CIFAR10-CNN1 (middle), and CIFAR10-CNN2 (right). Higher y value here means better defense.</figDesc><graphic url="image-26.png" coords="6,112.85,586.30,118.80,118.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Results of ReLU defense against 25 untargeted attacks using the Foolbox code repository, over MNIST (left column), CIFAR10-CNN1 (middle), and CIFAR10-CNN2 (right). The first 2K images from each dataset were used. Top row shows the average performance over all attacks. Bottom row shows the performance at the last epsilon ( = 0.3 over MNIST and = 0.1 over CIFAR-10), for each attack. Higher y value here means better defense.</figDesc><graphic url="image-32.png" coords="7,8.37,381.81,203.75,273.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Performance of the ReLU defense against the targeted FGSM attack over MNIST (top row), CIFAR10-CNN1 (middle row), and CIFAR10-CNN2 (bottom row). Lower y value here means better defense.</figDesc><graphic url="image-37.png" coords="8,207.00,458.71,198.00,198.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: Performance of the ReLU defense against the targeted FGSM attack over MNIST (top rows), CIFAR10-CNN1 (middle rows), and CIFAR10-CNN2 (bottom rows). Each plot corresponds to converting images to a different target class; if they have not already been classified as the target class). For some classes, the defense exacerbates the problem and helps the attack. On average, however, the defense seems to be working, although not as effective as in the untargeted setting. Lower y value here means better defense. 9</figDesc><graphic url="image-63.png" coords="9,61.35,578.58,101.88,101.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Impact of scaling up pixel values against the untargeted FGSM attak over MNIST (top) and CIFAR-10 (bottom; using CIFAR10-CNN1) datasets. The left column shows results with clipping the pixel values to [0,1] range after scaling. The right column shows results without clipping. The legend represents the magnitude of scaling. Higher y value here means better defense.</figDesc><graphic url="image-70.png" coords="10,146.36,368.70,158.40,159.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 : 11 Figure 9 :</head><label>8119</label><figDesc>Figure 8: Results of the BPDA attack (untargeted attack). Lower y value here means better defense.</figDesc><graphic url="image-74.png" coords="11,146.36,379.61,158.40,211.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-82.png" coords="13,108.00,167.03,396.00,435.08" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. I would like to thank Google for making the Colaboratory platform available.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
				<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR, abs/1607.02533</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Ensemble adversarial training: Attacks and defenses</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">D</forename><surname>Mcdaniel</surname></persName>
		</author>
		<idno>CoRR, abs/1705.07204</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
		<idno>CoRR, abs/1706.06083</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deepfool: A simple and accurate method to fool deep neural networks</title>
		<author>
			<persName><forename type="first">Seyed-Mohsen</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2574" to="2582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Foolbox v0.8.0: A python toolbox to benchmark the robustness of machine learning models</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<idno>CoRR, abs/1707.04131</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Basing perceptual decisions on the most informative sensory neurons</title>
		<author>
			<persName><forename type="first">Miranda</forename><surname>Scolari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">T</forename><surname>Serences</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of neurophysiology</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2266" to="2273" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Optimal attentional modulation of a neural population</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in computational neuroscience</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">34</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">White noise analysis of neural networks</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sikun</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Practical black-box attacks against machine learning</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">D</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Berkay</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security</title>
				<meeting>the 2017 ACM on Asia Conference on Computer and Communications Security<address><addrLine>Abu Dhabi, United Arab Emirates</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-02">2017. April 2-6, 2017. 2017</date>
			<biblScope unit="page" from="506" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.01991</idno>
		<title level="m">Mitigating adversarial effects through randomization</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Pixeldefend: Leveraging generative models to understand and defend against adversarial examples</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10766</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Stochastic activation pruning for robust adversarial defense</title>
		<author>
			<persName><forename type="first">Kamyar</forename><surname>Guneet S Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aran</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anima</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName><surname>Anandkumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01442</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples</title>
		<author>
			<persName><forename type="first">Anish</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Wagner</surname></persName>
		</author>
		<idno>CoRR, abs/1802.00420</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Jumprelu: A retrofit defense strategy for adversarial attacks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Benjamin Erichson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03750</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2017-10">Oct 2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
