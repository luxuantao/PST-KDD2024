<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Knockoff Nets: Stealing Functionality of Black-Box Models (Supplementary material)</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tribhuvanesh</forename><surname>Orekondy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mario</forename><surname>Fritz</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">CISPA Helmholtz Center for Information Security Saarland Informatics Campus</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">A</forename><surname>Contents</surname></persName>
						</author>
						<title level="a" type="main">Knockoff Nets: Stealing Functionality of Black-Box Models (Supplementary material)</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Transfer Set Test Set Gadwall: 0</term>
					<term>65 Nighthawk: 0</term>
					<term>06 Horned Lark: 0</term>
					<term>05 Gadwall: 0</term>
					<term>31 B</term>
					<term>Swallow: 0</term>
					<term>14 N</term>
					<term>Flicker: 0</term>
					<term>12 Gadwall: 0</term>
					<term>14 Chuck</term>
					<term>Widow: 0</term>
					<term>13 Swain</term>
					<term>Warbler: 0</term>
					<term>1 Gadwall: 0</term>
					<term>95 Mallard: 0</term>
					<term>01 Rb</term>
					<term>Merganser: 0</term>
					<term>00 Gadwall: 0</term>
					<term>44 Mallard: 0</term>
					<term>15 Rb</term>
					<term>Merganser: 0</term>
					<term>11 Pom</term>
					<term>Jaeger: 0</term>
					<term>16 Black Tern: 0</term>
					<term>11 Herring Gull: 0</term>
					<term>07</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A. Contents (this section) B. Extended Descriptions 1. Blackbox Models 2. Overlap Between P V and P A 3. Aggregating OpenImages and OpenImages-Faces 4. Additional Implementation Details C. Extensions of Existing Results 1. Qualitative Results 2. Sample-efficiency of GT 3. Policies Learnt by adaptive Strategy 4. Reward Ablation D. Auxiliary experiments 1. Effect of CNN initialization 2. Seen and Unseen Classes 3. Adaptive Strategy: With/without hierarchy 4. Semi-open World: ? D 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Extended Descriptions</head><p>In this section, we provide additional detailed descriptions and implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Sample Efficiency: Training Knockoffs on GT</head><p>We extend Figure <ref type="figure">5</ref> in the main paper to include training on the same ground-truth data used to train the blackboxes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Black-box Models</head><p>We supplement Section 5.1 by providing extended descriptions of the blackboxes listed in Table <ref type="table">1</ref> of the main paper. Each blackbox F V is trained on one particular image classification dataset.</p><formula xml:id="formula_0">P V P A Caltech256 (K=256) CUBS200 (K=200) Indoor67 (K=67) Diabetic5 (K=5)</formula><p>ILSVRC (Z=1000) 108 (42%) 2 (1%) 10 (15%) 0 (0%) OpenImages (Z=601) 114 (44%) 1 (0.5%) 4 (6%) 0 (0%)</p><p>Table <ref type="table">S1</ref>: Overlap between PA and PV .</p><p>Black-box 1: Caltech256 <ref type="bibr" target="#b4">[5]</ref>. Caltech-256 is a popular dataset for general object recognition gathered by downloading relevant examples from Google Images and manually screening for quality and errors. The dataset contains 30k images covering 256 common object categories.</p><p>Black-box 2: CUBS200 <ref type="bibr" target="#b13">[14]</ref>. A fine-grained bird-classifier is trained on the CUBS-200-2011 dataset. This dataset contains roughly 30 train and 30 test images for each of 200 species of birds. Due to the low intra-class variance, collecting and annotating images is challenging even for expert bird-watchers.</p><p>Black-box 3: Indoor67 <ref type="bibr" target="#b10">[11]</ref>. We introduce another finegrained task of recognizing 67 types of indoor scenes. This dataset consists of 15.6k images collected from Google Images, Flickr, and LabelMe.</p><p>Black-box 4: Diabetic5 <ref type="bibr" target="#b0">[1]</ref>. Diabetic Retinopathy (DR) is a medical eye condition characterized by retinal damage due to diabetes. Cases are typically determined by trained clinicians who look for presence of lesions and vascular abnormalities in digital color photographs of the retina captured using specialized cameras. Recently, a dataset of such 35k retinal image scans was made available as a part of a Kaggle competition <ref type="bibr" target="#b0">[1]</ref>. Each image is annotated by a clinician on a scale of 0 (no DR) to 4 (proliferative DR). This highly-specialized biomedical dataset also presents challenges in the form of extreme imbalance (largest class contains 30? as the smallest one).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Overlap: Open-world</head><p>In this section, we supplement Section 5.2.1 in the main paper by providing more details on how overlap was calculated in the open-world scenarios. We manually compute overlap between labels of the blackbox (K, e.g., 256 Caltech classes) and the adversary's dataset (Z, e.g., 1k ILSVRC classes) as: 100 ? |K ? Z|/|K|. We denote two labels k ? K and z ? Z to overlap if: (a) they have the same semantic meaning; or (b) z is a type of k e.g., z = "maltese dog" and k = "dog". The exact numbers are provided in Table <ref type="table">S1</ref>. We remark that this is a soft-lower bound. For instance, while ILSVRC contains "Hummingbird" and CUBS-200-2011 contains three distinct species of hummingbirds, this is not counted towards the overlap as the adversary lacks annotated data necessary to discriminate among the three species.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Dataset Aggregation</head><p>All datasets used in the paper (expect OpenImages) have been used in the form made publicly available by the authors. We use a subset of OpenImages due to storage con-  straints imposed by its massive size (9M images). The description to obtain these subsets are provided below.</p><formula xml:id="formula_1">? F V =LeNet EMNIST EMNISTLetters FashionMNIST KMNIST 0k 1k 10k Budget B CIFAR10 ? F V =Resnet-18 CIFAR100 TinyImageNet200 CIFAR100 (pt) TinyImageNet200 (pt) 0k 1k 10k Budget B CIFAR100 ? F V =Resnet-18 CIFAR10 TinyImageNet200 CIFAR10 (pt) TinyImageNet200 (pt)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OpenImages.</head><p>We retrieve 2k images for each of the 600 OpenImages <ref type="bibr" target="#b7">[8]</ref> "boxable" categories, resulting in 554k unique images. ?19k images are removed for either being corrupt or representing Flickr's placeholder for unavailable images. This results in a total of 535k unique images. OpenImages-Faces. We download all images (422k) from OpenImages <ref type="bibr" target="#b7">[8]</ref> with label "/m/0dzct: Human face" using the OID tool <ref type="bibr" target="#b12">[13]</ref>. The bounding box annotations are used to crop faces (plus a margin of 25%) containing at least 180?180 pixels. We restrict to at most 5 faces per image to maintain diversity between train/test splits. This results in a total of 98k faces images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Additional Implementation Details</head><p>In this section, we provide implementation details to supplement discussions in the main paper. Input Transformations.</p><p>While training the blackbox models F V we augment training data by applying input transformations: random 224?224 crops and horizontal flips. This is followed by performing normalizing the image using standard Imagenet mean and standard deviation values. While training the knockoff model F A and for evaluation, we resize the image to 256?256, obtain a 224?224 center crop and normalize as before.</p><p>Training F V = Diabetic5. We train this model using a learning rate of 0.01 (while this is 0.1 for the other models) and a weighted loss. Due to the extreme imbalance between classes of the dataset, we weigh each class as follows. Let n k denote the number of images belonging to class k and let n min = min k n k . We weigh the loss for each class k as n min /n k . From our experiments with weighted loss, we found approximately 8% absolute improvement in overall accuracy on the test set. However, the training of knockoffs of all blackboxes are identical in all aspects, including a non-weighted loss irrespective of the victim blackbox targeted.</p><p>Creating ILSVRC Hierarchy. We represent the 1k labels of ILSVRC as a hierarchy (Figure <ref type="figure">4b</ref>) in the form: root node "entity" ? N coarse nodes ? 1k leaf nodes. We obtain N (30 in our case) coarse labels as follows: (i) a 2048-d mean feature vector representation per 1k labels is obtained using an Imagenet-pretrained ResNet ; (ii) we cluster the 1k features into N clusters using scikit-learn's <ref type="bibr" target="#b9">[10]</ref> implementation of agglomerative clustering; (iii) we obtain semantic labels per cluster (i.e., coarse node) by finding the common parent in the Imagenet semantic hierarchy.</p><p>Adaptive Strategy. Recall from Section 6, we train the knockoff in two phases: (a) Online: during transfer set construction; followed by (b) Offline: the model is retrained using transfer set obtained thus far. In phase (a), we train F A with SGD (with 0.5 momentum) with a learning rate of 0.0005 and batch size of 4 (i.e., 4 images sampled at each t). In phase (b), we train the knockoff F A from scratch on the transfer set using SGD (with 0.5 momentum) for 100 epochs with learning rate of 0.01 decayed by a factor of 0.1 every 60 epochs. We used ?=25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Extensions of Existing Results</head><p>In this section, we present extensions of existing results discussed in the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Qualitative Results</head><p>Qualitative results to supplement Figure <ref type="figure">6</ref> are provided in Figures <ref type="figure" target="#fig_3">S4-S7</ref>. Each row in the figures correspond to an output class of the blackbox whose images the knockoff has never encountered before. Images in the "transfer set" column were randomly sampled from ILSVRC <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12]</ref>. In contrast, images in the "test set" belong to the victim's test set (Caltech256, CUBS-200-2011, etc.).   This extension "P V (F V )" is illustrated in Figure <ref type="figure">S2</ref>, displayed alongside KD approach. The figure represents the sample-efficiency of the first two rows of Table <ref type="table" target="#tab_0">2</ref>. Here we observe: (i) comparable performance in all but one case (Diabetic5, discussed shortly) indicating KD is an effective approach to train knockoffs; (ii) we find KD achieve better performance in Caltech256 and Diabetic5 due to regularizing effect of training on soft-labels <ref type="bibr" target="#b5">[6]</ref> on an imbalanced dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Policies learnt by Adaptive</head><p>We inspected the policy ? learnt by the adaptive strategy in Section 6.1. In this section, we provide policies over all blackboxes in the closed-and open-world setting. Since the distribution of rewards is non-stationary, we visualize the policy over time in Figure <ref type="figure" target="#fig_4">S8b</ref> for CUBS200 in a closed-world setup. From this figure, we observe an evolution where: (i) at early stages (t ? [0, 2000]), the approach samples (without replacement) images that overlaps with the victim's train data; and (ii) at later stages (t ? [2000, 4000]), since the overlapping images have been exhausted, the approach explores related images from other datasets e.g., "ostrich", "jaguar".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4. Reward Ablation</head><p>The reward ablation experiment (Figure <ref type="figure">8</ref> in the main paper) for the remaining datasets are provided in Figure <ref type="figure">S9</ref>. We make similar observations as before for Indoor67. However, since F V = Diabetic5 demonstrates confident predictions in all images, we find little-to-no improvement for knockoffs of this victim model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure S1 :Figure S2 :</head><label>S1S2</label><figDesc>Figure S1: Performance of the knockoff at various budgets. (Enlarged version of Figure 5) Presented for various choices of adversary's image distribution (PA) and sampling strategy ?.represents accuracy of blackbox FV and represents chance-level performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure S3 :</head><label>S3</label><figDesc>Figure S3: Training with non-ImageNet initializations of knockoff models. Shown for various choices of blackboxes FV (subplots) and adversary's image distribution PA (lines). All victim blackbox models are trained from scratch; test accuracy indicated by. All knockoff models are either trained from scratch, or pretrained on the corresponding PA task (suffixed with '(pt)').</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure S7 :</head><label>S7</label><figDesc>Figure S7: Qualitative results: Diabetic5. Extends Figure 6 in the main paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure S8 :</head><label>S8</label><figDesc>Figure S8: Policies learnt by adaptive strategy. Supplements Figure 7 the main paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure S10 :</head><label>S10</label><figDesc>Figure S9: Reward Ablation. Supplements Figure 8 in the main paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Figures S8a and S8c display probabilities of each action z ? Z at t = 2500.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2</head><label>2</label><figDesc></figDesc><table /><note><p>: "PV (FV )" (training with GT data) and "PV (KD)" (training with soft-labels of GT images produced by FV )</p></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Auxiliary Experiments</head><p>In this section, we present experiments to supplement existing results in the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Effect of CNN Initialization</head><p>In our experiments (Section 6), the victim and knockoff models are initialized with ImageNet pretrained weights 1 , a de facto when training CNNs with a limited amount of data. In this section, we study influence of different initializations of the victim and adversary models.</p><p>To achieve reasonable performance in our limited data setting, we perform the following experiments on comparatively smaller models and datasets. We choose three victim blackboxes (all trained after random initialization) using the following datasets: MNIST <ref type="bibr" target="#b8">[9]</ref>, CIFAR10 <ref type="bibr" target="#b6">[7]</ref>, and CI-FAR100 <ref type="bibr" target="#b6">[7]</ref>. We train a LeNet-like model 2 for MNIST, and Resnet-18 models for CIFAR-10 and CIFAR-100.</p><p>While we use the same blackbox model architecture for the knockoff, we either randomly initialize them or pretrain them on a different task. Consequently, in the following experiments, both the victim and knockoff have different initializations. We repeat our experiment using random policy (Section 4.1.1) and using as the query set P A : (a) when P V =MNIST: EMNIST <ref type="bibr" target="#b2">[3]</ref> (superset of MNIST containing alpha numeric characters [A-Z, az, 0-9]), EMNISTLetters ([A-Z, a-z]), FashionMNIST <ref type="bibr" target="#b14">[15]</ref> (fashion items spanning 10 classes e.g., trouser, coat) and KMNIST <ref type="bibr" target="#b1">[2]</ref> (Japanese Hiragana characters spanning 10 classes); (b) when P V =CIFAR10: CIFAR100 <ref type="bibr" target="#b6">[7]</ref> and Tiny-ImageNet200 3 (subset of ImageNet with 500 images per each of 200 classes); and (c)when P V =CIFAR100: CI-FAR10 and TinyImageNet200. Note that the output classes between CIFAR10 and CIFAR100 are disjoint.</p><p>From Figure <ref type="figure">S3</ref>, we observe: (i) model stealing is possible even when the knockoffs are randomly initialized. For instance, when stealing MNIST, we recover 0.98? victim accuracy across all choices of P A ; (ii) pretraining the knockoff model -even on a different task -improves sample efficiency of model stealing attacks e.g., when F V =CIFAR10-resnet18, querying images from P V improves the knockoff accuracy after 50k queries from 46.5% to 78.9%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Seen and Unseen classes</head><p>We now discuss evaluation to supplement Section 5.2.1 and Section 6.1.</p><p>In Section 6.1, we highlighted strong performance of the knockoff even among classes that were never encountered (see Table <ref type="table">S1</ref> for exact numbers) during training. To elaborate, we split the blackbox output classes into "seen" and "unseen" categories and present mean per-class accuracies in Figure <ref type="figure">S10</ref>. Although we find better performance on 1 Alternatives for ImageNet pretrained models across a wide range of architectures were not available at the time of writing classes seen while training the knockoff, performance of unseen classes is remarkably high, with the knockoff achieving &gt;70% performance in both cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Adaptive: With and without hierarchy</head><p>The adaptive strategy presented in Section 4.1.2 uses a hierarchy discussed in Section 5.2.2. As a result, we approached this as a hierarchical multi-armed bandit problem. Now, we present an alternate approach adaptive-flat, without the hierarchy. This is simply a multi-armed bandit problem with |Z| arms (actions).</p><p>Figure <ref type="figure">S11</ref> illustrates the performance of these approaches using P A = D 2 (|Z| = 2129) and rewards {certainty, diversity, loss}. We observe adaptive consistently outperforms adaptive-flat. For instance, in CUBS200, adaptive is 2? more sample-efficient to reach accuracy of 50%. We find the hierarchy helps the adversary (agent) better navigate the large action space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4. Semi-open World</head><p>The closed-world experiments (P A = D 2 ) presented in Section 6.1 and discussed in Section 5.2.1 assumed access to the image universe. Thereby, the overlap between P A and P V was 100%. Now, we present an intermediate overlap scenario semi-open world by parameterizing the overlap as: (i) ? d : The overlap between images P A and P V is 100 ? ? d ; and (ii) ? k : The overlap between labels K and Z is 100? ? k . In both these cases ? d , ? k ? (0, 1] represents the fraction of P A used. ? d = ? k = 1 depicts the closed-world scenario discussed in Section 6.1.</p><p>From Figure <ref type="figure">S12</ref>, we observe: (i) the random strategy is unaffected in the semi-open world scenario, displaying comparable performance for all values of ? d and ? k ; (ii) ? d : knockoff obtained using adaptive obtains strong performance even with low overlap e.g., a difference of at most 3% performance in Caltech256 even at ? d = 0.1; (iii) ? k : although the adaptive strategy is minimally affected in few cases (e.g., CUBS200), we find the performance drop due to a pure exploitation (certainty) that is used. We observed recovery in performance by using all rewards indicating exploration goals (diversity, loss) are necessary when transitioning to an open-world scenario.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>Eyepacs</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/c/diabetic-retinopathy-detection" />
		<imprint>
			<date type="published" when="2018-11">2018-11-08. 1</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep learning for classical japanese literature</title>
		<author>
			<persName><forename type="first">Tarin</forename><surname>Clanuwat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Bober-Irizar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asanobu</forename><surname>Kitamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuaki</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Emnist: an extension of mnist to handwritten letters</title>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saeed</forename><surname>Afshar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tapson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andr?</forename><surname>Van Schaik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05373</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Caltech-256 object category dataset</title>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Citeseer</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<author>
			<persName><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00982</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mnist handwritten digit database</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><surname>Burges</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist" />
	</analytic>
	<monogr>
		<title level="j">AT&amp;T Labs</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recognizing indoor scenes</title>
		<author>
			<persName><forename type="first">Ariadna</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Toolkit to download and visualize single or multiple classes from the huge open images v4 dataset</title>
		<author>
			<persName><forename type="first">Angelo</forename><surname>Vittorio</surname></persName>
		</author>
		<ptr target="https://github.com/EscVM/OIDv4_ToolKit" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Fashionmnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
