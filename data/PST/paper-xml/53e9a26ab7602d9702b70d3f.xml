<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Quantitative Analysis of Human-Model Agreement in Visual Saliency Modeling: A Comparative Study</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2012-12-20">December 20, 2012</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Ali</forename><surname>Borji</surname></persName>
							<email>aliborji@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<postCode>90089</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dicky</forename><forename type="middle">N</forename><surname>Sihite</surname></persName>
							<email>sihite@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<postCode>90089</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Laurent</forename><surname>Itti</surname></persName>
							<email>itti@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<postCode>90089</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Quantitative Analysis of Human-Model Agreement in Visual Saliency Modeling: A Comparative Study</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2012-12-20">December 20, 2012</date>
						</imprint>
					</monogr>
					<idno type="MD5">7E0787A12419BCEF5DF36DC618610EE4</idno>
					<idno type="DOI">10.1109/TIP.2012.2210727</idno>
					<note type="submission">received August 19, 2011; revised January 31, 2012; accepted May 3, 2012. Date of publication July 30, 2012; date of current version</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Bottom-up attention</term>
					<term>eye movement prediction</term>
					<term>model comparison</term>
					<term>visual attention</term>
					<term>visual saliency</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visual attention is a process that enables biological and machine vision systems to select the most relevant regions from a scene. Relevance is determined by two components: 1) top-down factors driven by task and 2) bottom-up factors that highlight image regions that are different from their surroundings. The latter are often referred to as "visual saliency." Modeling bottom-up visual saliency has been the subject of numerous research efforts during the past 20 years, with many successful applications in computer vision and robotics. Available models have been tested with different datasets (e.g., synthetic psychological search arrays, natural images or videos) using different evaluation scores (e.g., search slopes, comparison to human eye tracking) and parameter settings. This has made direct comparison of models difficult. Here, we perform an exhaustive comparison of 35 state-of-the-art saliency models over 54 challenging synthetic patterns, three natural image datasets, and two video datasets, using three evaluation scores. We find that although model rankings vary, some models consistently perform better. Analysis of datasets reveals that existing datasets are highly center-biased, which influences some of the evaluation scores. Computational complexity analysis shows that some models are very fast, yet yield competitive eye movement prediction accuracy. Different models often have common easy/difficult stimuli. Furthermore, several concerns in visual saliency modeling, eye movement datasets, and evaluation scores are discussed and insights for future work are provided. Our study allows one to assess the state-of-the-art, helps to organizing this rapidly growing field, and sets a unified comparison framework for gauging future efforts, similar to the PASCAL VOC challenge in the object recognition and detection domains.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>higher-level cognitive areas that perform complex processes such as scene understanding, action selection, and decision making. In addition to being an interesting scientific challenge, modeling visual attention has many engineering applications, including in: computer vision (e.g., object recognition <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>, object detection <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, target tracking <ref type="bibr" target="#b5">[6]</ref>, image compression <ref type="bibr" target="#b6">[7]</ref>, and video summarization <ref type="bibr" target="#b7">[8]</ref>); computer graphics (e.g., image rendering <ref type="bibr" target="#b8">[9]</ref>, image thumb-nailing <ref type="bibr" target="#b9">[10]</ref>, automatic collage creation <ref type="bibr" target="#b10">[11]</ref>, and dynamic lighting <ref type="bibr" target="#b11">[12]</ref>); robotics (e.g., active gaze control <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, robot localization and navigation <ref type="bibr" target="#b14">[15]</ref>, and human-robot interaction <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>); and others (e.g., advertising <ref type="bibr" target="#b16">[17]</ref> and retinal prostheses <ref type="bibr" target="#b18">[19]</ref>).</p><p>Modeling visual saliency has attracted much interest recently and there are now several frameworks and computational approaches available. Some are inspired by cognitive findings, some are purely computational, and others are in between. However, since models have used different evaluation scores and datasets while applying various parameters, model evaluation against the state-of-the-art is becoming an increasingly complex challenge. In this paper, inspired by the PAS-CAL VOC object detection/recognition challenge <ref type="bibr" target="#b19">[20]</ref>, we aim to compare visual attention models in a unified framework over several scoring methods and datasets. Such a comparison helps better understand modeling parameters and provides insights towards further developing more effective models. It also helps better focus and calibrate the research effort by avoiding repetitive work and discarding less promising directions. It will also benefit experimentalists to choose the right tool/model for their applications. Since our main purpose is to compare models, rather than discuss attention concepts and models in detail, we refer the interested reader to general reviews for more information (e.g., Itti and Koch <ref type="bibr" target="#b20">[21]</ref>, Heinke and Humphreys <ref type="bibr" target="#b21">[22]</ref>, Frintrop et al. <ref type="bibr" target="#b22">[23]</ref>, and Borji and Itti <ref type="bibr" target="#b23">[24]</ref>).</p><p>There is often a confusion between saliency and attention. Visual attention is a broad concept covering many topics (e.g., bottom-up/top-down, overt/covert, spatial/spatio-temporal, and space-based/object-based attention). Visual saliency, on the other hand, has been mainly referring to bottom-up processes that render certain image regions more conspicuous: For instance, image regions with different features from their surroundings (e.g., a single red dot among several blue dots). Bottom-up saliency has been studied in search tasks such as finding an odd item among distractors in pop-out and conjunction search arrays, as well as in eye movement prediction on free-viewing of images or videos. In contrast to bottom-up, top-down attention deals with high-level cognitive factors that make image regions relevant, such as task demands, emotions, and expectations. It has been studied in natural behaviors such as sandwich making <ref type="bibr" target="#b24">[25]</ref>, driving <ref type="bibr" target="#b25">[26]</ref>, and interactive game playing <ref type="bibr" target="#b26">[27]</ref>. In the real-world, bottom-up and topdown mechanisms are combined to direct visual attention. Correspondingly, models of visual attention often focus either on bottom-up (known as saliency models) or on top-down factors of visual attention. Due to the relative simplicity of bottom-up processing (compared to top-down), the majority of existing models has focused on bottom-up attention. For a review on attention in natural behavior, please refer to <ref type="bibr" target="#b27">[28]</ref>.</p><p>In addition to the dissociation between bottom-up and topdown, visual attention studies (and likewise models) can be categorized based on several other factors. Some studies have addressed explaining fixations/saccades in free viewing of static images while others have approached dynamic stimuli, such as observing movies or playing video games <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b27">[28]</ref>. This distinction has divided models into spatial (still images) or spatio-temporal models (over video stimuli). The majority of spatio-temporal models are also applicable to saliency estimation over static images. Although static models are also applicable to videos by processing each single frame, they have not been fundamentally built to account for such stimuli.</p><p>Models can be categorized as being space-based or objectbased. Object-based models try to segment or detect objects to predict salient regions. This is supported by the finding that objects predict fixations better than early saliency <ref type="bibr" target="#b29">[30]</ref>. In contrast, in space-based models, all operations happen at the image level (pixels or image patches), or in the image spectral phase domain. For these space-based models, the goal is to create saliency maps that may predict which locations have higher probability of attracting human attention (as measured, e.g., by subjective rankings of interesting and salient locations, reaction times in visual search, or eye movements). Salient region detection in object-based models adds a segmentation problem where the goal is to not only locate but also segment the most salient objects within a scene from the background. Perhaps because object segmentation remains a difficult machine vision problem, there are not as many objectbased models as space-based models.</p><p>Another distinction is between overt and covert attention. Overt attention is the process of directing the the eyes towards a stimulus, while covert attention is that of mentally focusing onto one of several possible sensory stimuli (without necessarily moving the eyes). Many bottom-up saliency models have blurred the distinction between overt and covert attention and have focused onto detecting salient image regions, which in turn could attract one or both types of attention. Indeed, as detailed below, few models offer explicit mechanisms for the control of head/body/gaze movements.</p><p>Considering the above definitions, here we compare those visual saliency models that belong to the majority class of models, namely, those models that are: 1) bottom-up; 2) spatial or spatio-temporal; 3) space-based; 4) able to generate a topographic saliency map for an arbitrary digital image or a video; and 5) addressing free-viewing of images or videos (not solely visual search or salient object segmentation). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. COMPARISON PLAN</head><p>First, we briefly explain experimental settings in Sec. II-A. Then, datasets including widely-used synthetic patterns and eye movement datasets over static scenes (natural, abstract, and cartoon images) and videos are described in Sec. II-B. Next, in Sec. II-C, three popular evaluation scores are explained. We then discuss some challenges in model comparison and our way to tackle them (Sec. II-D). Finally, experimental results of thorough model evaluation are shown in Sec. III followed by learned lessons in Sec. IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Settings</head><p>The first step in this study was to collect saliency models. Some models were already shared online. For others, we contacted their creators for software; the authors then either sent source code for us to compile or sent executables. Some authors, however, preferred to run their models on our stimuli and to send back saliency maps. In the end, we were able to evaluate the 35 models listed in Table <ref type="table" target="#tab_0">I</ref>, sorted by publication year. This table also shows stimulus types that models are applicable to and their implementation language. In addition to developed models by the authors, we also implemented two other simple yet powerful models, to serve as baseline: The Gaussian Blob (Gauss) and Human Inter-Observer (IO) models. The Gaussian blob model is simply a 2D Gaussian shape drawn at the center of the image; it is expected to predict human gaze well if such gaze is strongly clustered around the image center. The human inter-observer model outputs, for a given stimulus, a map built by integrating eye fixations from other subjects than the one under test, while they watched that stimulus. The map is then smoothed by convolving with a Gaussian filter. This inter-observer "model" is expected to provide an upper bound on prediction accuracy of computational models, to the extent that different humans may be the best predictors of each other. Since maps made by models have different resolutions, we resized them (using nearest neighbor interpolation) to the size of the original images onto which eye movements have been recorded. Map resolutions as well as model acronyms used in the rest of the paper are listed in Table <ref type="table" target="#tab_0">I</ref>. Please note that, besides models compared here, some other models may exist that might perform well, but are not publicly available or easily accessible (e.g., <ref type="bibr" target="#b57">[58]</ref>). We leave such models for future investigations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Stimuli</head><p>Attention models have first been validated by predicting accuracy and reaction times of human subjects in target detection in visual search arrays. In addition, many models have commonly been validated against eye movement data.</p><p>1) Synthetic Stimuli: Early attention studies and models used simple synthetic patterns such as searching for a target or detecting an odd item among distractor items to find out important feature channels in directing attention and how they are combined <ref type="bibr" target="#b58">[59]</ref>. For instance, it has been shown that reaction time for a simple pop-out search task remains constant as a function of set size (number of all items on the screen), while in conjunction search tasks (searching for a target that is different in two features) reaction time increases linearly with set size <ref type="bibr" target="#b58">[59]</ref>. In <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b59">[60]</ref>, authors enumerate and discuss features that influence attention. For a computational perspective on implementation of these features in saliency models, please refer to <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b23">[24]</ref>.</p><p>Fig. <ref type="figure" target="#fig_0">1</ref>, shows a collection of 54 diverse synthetic patterns where one item (a target) differs from all other (distractor) items (pop-out, search asymmetry, texture, semantics, size, grouping, curvature, etc.). Such stimuli have been widely used for qualitative evaluation of saliency and attention models in the past. Patterns are sorted from easy to hard for models (Fig. <ref type="figure" target="#fig_4">5</ref>) from left to right and top to bottom. They can be categorized into: orientation pop-out <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b53">54)</ref>, texture pop-out <ref type="bibr" target="#b5">(6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b46">47)</ref>, curvature popout <ref type="bibr" target="#b34">(35,</ref><ref type="bibr" target="#b47">48)</ref>, size pop-out <ref type="bibr" target="#b7">(8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b51">52)</ref>, grouping <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34)</ref>, color pop-out <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b52">53)</ref>, intensity pop-out <ref type="bibr" target="#b10">(11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b41">42)</ref>, search asymmetry (5;15, 22;46, 40;49), and other complex search arrays <ref type="bibr" target="#b6">(7,</ref><ref type="bibr" target="#b22">23)</ref>. In some patterns, targets are embedded in noise (e.g., speckle noise: 11, 20, 31 and orientation noise: <ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b40">41)</ref>. We aimed to assess the pure target detection performance of models. This is why we included harder displays, even though humans may perform poorly on them (hence a great model of human attention should also perform poorly, but some models might transcend human abilities with such images).</p><p>2) Natural Scenes: Space-based models have often been tested for eye fixation prediction over still image datasets and spatio-temporal models have been evaluated against video data. a) Image datasets: Since statistics of different datasets vary, we employed three popular image datasets often used for saliency evaluation: 1) Bruce and Tsotsos <ref type="bibr" target="#b36">[37]</ref> (one of the earliest and most widely used datasets). It contains 120 images mainly indoor and in-city scenes. Due to the small size of this dataset and the small number of subjects, its sole usage is less encouraged; 2) Kootstra and Shomacker <ref type="bibr" target="#b43">[44]</ref> (which contains a wide variety of images); and 3) Judd et al. <ref type="bibr" target="#b49">[50]</ref> (which is the largest dataset available to date containing 1003 images). It contains many images with human faces and has a high degree of photographer bias and a smaller number of subjects. Le Meur <ref type="bibr" target="#b37">[38]</ref> dataset has only 27 images with the highest number of eye-tracking subjects <ref type="bibr" target="#b39">(40)</ref>. We avoided to use this dataset as its images are highly center-biased (See Sec. II-D).</p><p>Because of the specialty of datasets (different optimal weights for features over different datasets <ref type="bibr" target="#b60">[61]</ref>), a fair evaluation is to compare models over several datasets (Sec. III).</p><p>b) Video datasets: Unfortunately, there are not many publicly available video datasets with associated eye-tracking data. This calls for collecting more eye movement data over videos. Here, we run models over two datasets: 1) A large popular benchmark dataset for comparison of spatio-temporal saliency, called CRCNS-ORIG <ref type="bibr" target="#b61">[62]</ref>, which is freely accessible. Fig. <ref type="figure" target="#fig_1">2</ref> shows a sample frame from each video of CRCNS-ORIG dataset embedded with eye fixations. 2) A recent project called DIEM (Dynamic Images and Eye Movements) has investigated where people look during dynamic scene viewing such as during film trailers, music videos, or advertisements <ref type="bibr" target="#b62">[63]</ref>  <ref type="foot" target="#foot_0">1</ref> . Fig. <ref type="figure" target="#fig_2">3</ref> shows sample frames of DIEM with fixations overlaid.</p><p>Please refer to <ref type="bibr" target="#b23">[24]</ref> for more details on available datasets. Our choice of datasets emphasizes popularity, thoroughness, and variety in the stimuli.</p><p>We applied spatial and spatio-temporal models over static (still images) and dynamic (video) stimuli to compare accuracy of both types of models over both types of stimuli. This way we can analyze the usefulness of temporal information by comparing accuracy of models built from simple features plus the motion channel (e.g., the Itti-CIOFM model) with other high-performing models without temporal information.   Another approach will be extending all spatial models to the temporal domain before comparison. This, however goes beyond our scope in this paper and should be addressed by the model creators.</p><formula xml:id="formula_0">; 28) T v -action01; 29) T v -ads01; 30) T v -ads02; 31) T v -ads03; 32) T v -ads04; 33) T v -announce01; 34) T v -music01; 35) T v -news01; 36) T v -news02; 37) T v -news03; 38) T v -news04; 39) T v -news05; 40) T v -news06; 41) T v -news09; 42) T v -sports01; 43) T v -sports02; 44) T v -sports03; 45) T v -sports04; 46) T v -sports05; 47) T v -talk01; 48) T v -talk03; 49) T v -talk04;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation Scores</head><p>Here, three evaluation scores for comparison of models are explained. The motivation for analyzing models with more than one metric is to ensure that the main qualitative conclusions are independent of the choice of metric. In the following, G denotes a ground-truth saliency map which is a map built by inserting 1's at fixation locations and convolving the result with a Gaussian for smoothing. An estimated saliency map which is computed by a saliency model is denoted by S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Linear Correlation Coefficient (CC):</head><p>The linear correlation coefficient measures the strength of a linear relationship between two variables: CC(G,S) = cov(G,S) σ G σ S where σ G and σ S are the standard deviations of the G and S maps, respectively <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b64">[65]</ref>. When CC is close to +1/ -1 there is almost a perfectly linear relationship between the two variables.</p><p>2) Normalized Scanpath Saliency (NSS): NSS <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b66">[67]</ref> is the average of the response values at human eye positions (x i h , y i h ) in a model's saliency map (S) that has been normalized to have zero mean and unit standard deviation. N SS = 1 indicates that the subjects' eye positions fall in a region whose predicted saliency is one standard deviation above average. Thus, when N SS ≥ 1 the saliency map exhibits significantly higher saliency values at human fixated locations compared to other locations. Meanwhile N SS ≤ 0 indicates that the model performs no better than picking a random position, and hence is at chance in predicting human gaze.</p><p>3) Area Under Curve (AUC): AUC is the area under the Receiver Operating Characteristics (ROC) curve <ref type="bibr" target="#b67">[68]</ref>. Using this score, human fixations are considered as the positive set and some points from the image are sampled, either uniformly or non-uniformly <ref type="bibr" target="#b44">[45]</ref> (for discounting center-bias), to form the negative set. The saliency map S is then treated as a binary classifier to separate the positive samples from the negatives. By thresholding over the saliency map and plotting true positive rate vs. false positive rate an ROC curve is achieved for each image. Then ROC curves are averaged over all images and the area underneath the final ROC curve is calculated <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b68">[69]</ref>. Perfect prediction corresponds to a score of 1 while a score of 0.5 indicates chance level.</p><p>For more details on evaluation scores please refer to <ref type="bibr" target="#b23">[24]</ref> <ref type="foot" target="#foot_2">2</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Challenges and Open Problems</head><p>Here we discuss challenges that have emerged as more saliency models have been proposed. These are open issues that must be considered, not only for research but also for performing a fair comparison of all models.</p><p>1) Center Bias (CB): Perhaps the biggest challenge in model comparison is the issue of center-bias. Center-bias means that a majority of fixations happen to be near the image center. Several reasons for this have previously been proposed. For instance, it could be due to a tendency of photographers to put interesting (and hence salient <ref type="bibr" target="#b69">[70]</ref>) objects at the image center; or it could be because of a viewing strategy by which subjects first inspect the image center, maybe to rapidly gather a global view of the scene <ref type="bibr" target="#b70">[71]</ref>, <ref type="bibr" target="#b71">[72]</ref>. Some models have implicitly (e.g., GBVS <ref type="bibr" target="#b39">[40]</ref>) or explicitly (e.g., Judd <ref type="bibr" target="#b49">[50]</ref>) used center-preference (location prior) to better account for eye movements. This, however, makes fair comparison challenging. Three remedies are possible: 1) Every model adds a Gaussian of a certain size to its output. This approach has the drawback that it is hard to impose to the large community of researchers. 2) Collecting a dataset with no center-bias. This is difficult because even if we have an approach to uniformly distribute image content, viewing strategy still exists. 3) Designing suitable evaluation metrics, which is what we consider as the most reasonable approach, and which we use here.</p><p>To eliminate center-bias effects, Zhang et al. <ref type="bibr" target="#b44">[45]</ref> used the shuffled AUC metric instead of the uniform AUC metric. They defined shuffled AUC as: For an image and a human subject, the positive sample set is composed of the fixations of that subject on that image, while the negative set, instead of uniformly random points, is composed of the union of all fixations of all subjects across all other images, except for the positive set. This score allows for a stronger assessment of the non-trivial off-center fixations, which are the ones that are more challenging and more interesting to predict. Alternatively, Qi and Koch <ref type="bibr" target="#b60">[61]</ref>, defined an unbiased AUC score as the ratio of normal AUC to the AUC score of the inter-observer model.</p><p>Here, along with using the shuffled AUC score, we apply models to images with low center-bias. This second-order study provides another way of differentiating models behavior over (difficult) fixations which deviate from center. Please note that this does not necessarily mean that center-bias is not a fact of human attention behavior. To this end, we propose a new measure called Center-Bias Ratio (CBR) to quantify the amount of center-bias in an image or a set of images. First, for an image, a heat map is generated by pooling fixations from all subjects without Gaussian smoothing. Then, the ratio of fixations inside each central circle to the overall number of fixations in the image is calculated. By varying the radius, a vector of ratios is derived. If there are more fixations at the center, the first values of this vector should be very high. By applying a fixed threshold, one can make a decision whether an image is center-biased or not.</p><p>Fig. <ref type="figure" target="#fig_3">4</ref> shows distribution of fixations for three datasets and their center-bias ratio. The five most and five least centerbiased images from datasets are also shown. Judd et al., and Bruce and Tsotsos datasets are highly center-biased (at 40% circle, from center to image corner, they explain more than 80% of fixations) and Kootstra and Shomacker has the least center-bias amongst three. This might be because this dataset has many symmetric objects (e.g., flowers) off the center.</p><p>To test how many images pass a CB criterion, at the radius level of 40%, we selected an image from a dataset if its CBR was less than 0.7. This way, 10, 58, and 120 images from Bruce and Tsotsos, Kootstra and Schomaker, and Judd et al. datasets passed the selection criteria, respectively (Overall 15% of 1250 images) <ref type="foot" target="#foot_3">3</ref> . 2) Border Effect: Another challenge is the treatment of image borders. Zhang et al. <ref type="bibr" target="#b44">[45]</ref> showed that KL and ROC scores are corrupted by edge effects. When an image filter lies partially off the edge of an image, the filter response is not well defined. They varied the size of a black border added around a dummy white saliency map (of size 120×160 pixels) and showed that as the border size increases, ROC and KL scores increase as well. Since human eye fixations are rarely near the edges of test images, edge effects primarily change the distribution of saliency of the random samples. For the dummy saliency map, a baseline map (uniform white) gives a ROC value of 0.5, adding a four-pixel black border yields 0.62, and an eight-pixel black border yields 0.73. The same 3 border sizes would yield KL scores of 0, 0.12, and 0.25. Note that a black border effect due to variations in handling invalid filter responses at the image borders is similar to the center-bias issue and could be handled the same way. But the first is a problem with datasets while the second one regards a problem in modeling.</p><p>3) Scores: Some issues concern scores. For instance, as a limitation of ROC, Qi and Koch <ref type="bibr" target="#b60">[61]</ref>, compared two saliency maps with different degrees of false alarm rates. Interestingly, while one map had a clear dense activation at fixations (with almost no background activation), its standard AUC (=0.975) was not dramatically better compared to the other map (with activations at both fixations and background) with much higher false alarm rate (AUC = 0.973). Because of the normalization to the entire map, this problem did not affect NSS score.</p><p>4) Model Parameters: Another problem regarding fair model comparison is adjusting parameters in models. For instance, it has been shown that smoothing the final saliency map of a model affects the scores <ref type="bibr" target="#b74">[75]</ref>. In models described in Table <ref type="table" target="#tab_0">I</ref>, some authors mentioned the best set of parameters, and some manually tunned their model on our stimuli and sent back the saliency maps.</p><p>To tackle center-bias, border effects, and scoring issues, instead of only using one score, we decided to use three, with an emphasis on analysis of results using the shuffled AUC score which is more robust to center-bias and borders. A model that works well should score high (if not the best) at almost any score. Regarding model parameters, over some crossvalidation data, we tried to tune models for best performance by qualitatively checking saliency maps or quantitatively by calculating scores. However, as further discussed in section IV, ultimately the model parameter issue will be best handled through an online challenge where participants can tune their own models before submitting results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTAL RESULTS</head><p>Having laid out the evaluation framework, we are ready to compare saliency models in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Results Over Synthetic Images</head><p>Fig. <ref type="figure" target="#fig_4">5</ref> shows ranking of models over synthetic patterns. The location of each target in each stimulus was tagged manually (bottom-right panel). Then the accuracy of a saliency model to capture the target was calculated using the NSS score. This score is more suitable here because there is only one target position in each image and if a model could accurately predict that location it would get a high score. The higher NSS thus means better target detection. The top-left panel in this figure shows performance of all models over all stimuli sorted both ways. The bottom-left panel ranks stimuli in terms of simplicity of target detection averaged over models (see also Fig. <ref type="figure" target="#fig_0">1</ref>). The top-right panel shows sorted NSS scores (averaged over stimuli) of models.</p><p>On average (over stimuli), all models performed significantly above chance. Overall, models based on FIT theory performed higher on synthetic patterns (e.g., compared to statistical and information-theoretic models). STB, AWS, GBVS, VOCUS, Bian, and Itti-CIO models achieved the best NSS scores. From these models, Itti-CIO and hence its descendants STB and VOCUS are directly based on the FIT framework. Similar to these, AWS and GBVS models have used multiscale color, intensity and orientation channels. One highperforming model which is not based on FIT is Bian's model, which works in the frequency domain. Inspecting the highperforming models, we noticed that they all generate maps with a high peak at the target location and less activation elsewhere, which results in high NSS values. Models including AIM and HouNIPS seem impaired by the border effect, which affects their normalization; indeed, these models perform poorly on all our search-array stimuli. We expected that some models might actually surpass human vision in some of these images, i.e., they might mark as salient some targets which are hard to be immediately seen by humans. For example, AWS is doing quite well on hard image 23. Although some stimuli were easier for many models, no single stimulus was easy for all models. For example, stimulus 1, a simple red/green color pop-out was easy for models which include a separate color channel but remained challenging for several statistical models which are based on natural scene statistics (AIM, HouNIPS, Rarity-G). One important conclusion of our study therefore is that to date no model performs perfectly over all synthetic stimuli tested here. Fig. <ref type="figure" target="#fig_5">6</ref> illustrates saliency maps of models over the best and worst synthetic stimuli (averaged over all models) as well as some other sample synthetic stimuli.</p><p>Although in this section we focused on evaluating the consistency of saliency models with a number of classic psychophysical results related to bottom-up attention, there are several other tests that a model could be verified against, including: nonlinearity against orientation contrast, efficient (parallel) and inefficient (serial) search, orientation asymmetry, presence-absence asymmetry and Weber's law, and influence of background on color asymmetries (see <ref type="bibr" target="#b75">[76]</ref>, <ref type="bibr" target="#b76">[77]</ref>). Some models have been partially tested against such stimuli <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b76">[77]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results Over Natural Scenes</head><p>Fig. <ref type="figure" target="#fig_6">7</ref>(a) shows ranking of models for fixation prediction over still images. For statistical significance testing of mean scores between two models, we used the t-test at the significance level of p ≤ 0.05. Although the ranking order is not exactly the same over all three datasets, some general patterns can be observed. Using the CC score, over all three datasets, GBVS works the best. The Yan, Kootstra, and Gauss models are among the best six. High CC scores for the Gauss model indicate that there is high density of fixations at the image center over all three datasets. Higher CC for Gauss over the Judd et al. dataset (no significant difference between Gauss and GBVS; p = 0.1) means higher central eye concentration over this dataset. Similarly, using NSS, GBVS did the best and the Yan, Judd, AWS, and Kootstra models were among the six best. High performance for Gauss with NSS again indicates a high center-preference over datasets (Gauss ranked third over the Judd et al. dataset). Scores of models over the Kootstra and Shomacker dataset are smaller than over other datasets. This might be partially due to difficulty of stimuli in this dataset. For instance, many of them are outdoor natural scenes as opposed to close-up shots of objects or animals. Consistent with previous research, an important point here is that CC and NSS scores are sensitive to center-preference (high scores for Gauss model), therefore their usage is not encouraged for future work. Using shuffled AUC, the Gauss model is the worst (not significantly higher than chance) over all three datasets as we expected. Indeed, the shuffled AUC measure explicitly discounts center bias by sampling random points from human fixations. With shuffled AUC, the AWS model is significantly better than all other models over the three datasets, followed by HouNIPS model. The AIM and Judd models were the other two models that did well. One interesting observation is that AWS is able to predict human fixations over the Kootstra and Shomacker dataset at the level of human inter-observer similarity (no significant difference between model's score and Human inter-observer score). Rarity-L, Entropy, and STB are three models that did worst over CC and NSS scores. In terms of AUC scores, Gauss, STB, and Marat are at the bottom.</p><p>Except for the aforementioned case of AWS over the Kootstra and Shomacker dataset, the main conclusion of this study is that a significant gap still exists between the best models and human inter-observer agreement. The spread of models scores is also quite narrow, and for NSS over the Kootstra and Shomacker and Judd et al. datasets the gap between IO and the best model is greater than that between the best and worst models. This indicates that even though much progress has been made in modeling saliency over the past 13 years, dramatic and qualitatively better new models still remain to be discovered that will better approach human eye fixations. To the disappointment of the authors, many recent models overall perform worse that the Itti-CIO2 model published in 1998 <ref type="bibr" target="#b30">[31]</ref>, indicating the importance of using a comprehensive comparison framework for measuring progress. We further examine these issues in the Discussion section (Sec. IV).</p><p>An important note from our comparisons is that most models that did well overall, performed reasonably well over every combination of dataset and score. An exception is GBVS which performed the best over three datasets using the CC and NSS scores but not as well (though still quite well) with AUC. The performance drop of the GBVS model could be because it takes advantage of center-bias. Some of the models which scored well on the synthetic patterns (Fig. <ref type="figure" target="#fig_4">5</ref>) scored poorly on natural image datasets (e.g., STB and Itti-CIO). To some extent, we find that this may be due to the fact that these models are developed based on FIT framework which has been originally proposed to explain synthetic patterns. The Itti-CIO model also generates very sparse maps which do not reflect well the substantial inter-observer variations present in the human eye movement data.</p><p>In addition to using the shuffled AUC score, we conducted another experiment to compare models over stimuli with less center-bias. We selected 100 images from the Judd et al. dataset with least center-bias ratio (using 40% circle) and calculated scores for those images. Results are shown in Fig. <ref type="figure" target="#fig_6">7(b)</ref>. The rationale for focusing on images that yield many off-center fixations is that such fixations may convey more information about the processes by which attention is drawn to salient peripheral stimuli (as opposed to central fixations, which may be stimulus-driven or part of a viewing strategy; see section II-D). Indeed, we verified that the Gauss model performed poorly on this dataset. Consistent with CC and NSS scores over three datasets, here GBVS again scored the best, and the ranking of models is almost the same as when using all images across these two scores. With shuffled AUC, the ranking is almost the same as with the original datasets, with AWS, HouNIPS, and AIM at the top. Similar to the original datasets, the AUC performance of GBVS is not among the best. Note how, with shuffled AUC (which is emerging as the most reliable score), all models are closer to the IO performance in the least center-biased dataset. This new approach to dataset design helps us mitigate the above remark about the need for a qualitative jump in eye movement prediction: The off-center fixations, which arguably are the most important and difficult to predict, are captured quite well by many models.</p><p>Our next analysis is ranking models over different classes of stimuli from the Kootstra and Shomacker dataset. The intuition behind this experiment is that since different models use different features, and different classes of images may exhibit different feature distributions, it is likely that models may selectively perform higher over different types of images. Fig. <ref type="figure" target="#fig_3">4</ref>, middle column, shows sample images from the Kootstra and Shomacker dataset. Images of this dataset fall into 5 categories: 1) Animals, 2) Automan (cars and humans), 3) Buildings, 4) Flowers, and 5) Nature. The shuffled AUC scores of all models are shown in Table <ref type="table" target="#tab_1">II</ref> for each category. This table also shows scores of the inter-observer (IO) model as well as average scores of models (using three scores) across 5 categories. Interestingly, again the AWS model did the best over all categories (it was only significantly better than other models in the Flowers category). HouNIPS, Judd, SDSR, Yan, and AIM were also at the top. Gauss, STB, Marat, and Entropy ranked at the bottom. The least performance among categories belongs to Nature stimuli (using all 3 scores), probably because stimuli in the Nature category are more noisy and there are less solid objects or dense salient regions. All models scored below AUC = 0.6 in that category, and humans are also less consistent over nature stimuli (smaller AUC score for IO model). The best performance of models is over the Automan category, which consists of in-city scenes containing cars and humans, and IO also scored highest in this category. Model performance differences over categories suggests that  <ref type="bibr" target="#b36">[37]</ref>. Middle column: Kootstra and Shomacker <ref type="bibr" target="#b43">[44]</ref>. Right column: Judd et al. <ref type="bibr" target="#b49">[50]</ref>. Stars indicate statistical significance using t-test (95%, p ≤ 0.05) between consecutive models. Note that no star between two models that are not immediately close to each other does not necessarily mean that they are not significantly different. In fact, it is highly probable that a model that is significantly better than the one in its left, also scores significantly better than all other models on its left. Error bars indicate standard error of the mean (SEM): (σ/ √ N ), where σ is the standard deviation and N is the number of images. We do not show CC results for IO model because comparing the map built from fixations of one subject with the map built from fixations of all other subjects using CC, does not generate a high value (both maps are convolved with a Gaussian). This is because few fixations of only one subject do not generate a diffused map, which is favored by CC score. We also could not calculate IO score over Bruce and Tsotsos dataset since fixations are not separated for each subject. (b) This column sorts models over 100 least center-biased images from the Judd et al. dataset (see Section II-D). The heatmap at the top-most panel shows distribution of fixations over selected images. Judd model uses center feature, gist, and horizon line, and object detectors for cars, faces, and human body. Itti-CIO2 is the approach proposed by Itti et al. <ref type="bibr" target="#b30">[31]</ref> that uses the following normalization scheme: For each feature map, find the global max M and find the average m of all other local maxima. Then just weight the map by (Mm) 2 . In the Itti-CIO method <ref type="bibr" target="#b32">[33]</ref>, normalization is: Convolve each map by a difference of Gaussian(DoG) filter, cut off negative values, and iterate this process for a few times. This normalization operation results in sparse saliency maps. In the literature, majority of models have been compared against Itti-CIO.</p><p>customizing models based on image category might further improve fixation prediction accuracy. Some models indeed rely on detecting the "gist" of a scene (e.g., whether it is indoors or outdoors) to establish a spatial prior on saliency <ref type="bibr" target="#b34">[35]</ref>; these could be further combined with learning techniques (e.g., <ref type="bibr" target="#b60">[61]</ref>) to modulate features contributing to saliency based on a scene's gist or category. Such research might benefit from deeper psychological studies of eye movement patterns over different categories of scenes. Fig. <ref type="figure" target="#fig_7">8</ref>(a) sorts models over the CRCNS-ORIG dataset using three scores. Rankings are almost the same over CC and NSS scores with GBVS, Gauss, Marat, HouNIPS, Judd, and Bian models at the top. Using the AUC score, AWS, HouNIPS, Bian and Human inter-observer are the best. The reason why, when using shuffled AUC, the inter-observer model is slightly lower than the three mentioned models is likely because the number of subjects is small and hence a map from other subjects may not be a good predictor of the remaining test subject.  <ref type="bibr" target="#b61">[62]</ref>. (b) Ranking models over DIEM dataset <ref type="bibr" target="#b62">[63]</ref>. Only these models had motion channel: Itti-M, Itti-CIOFM, Surprise-CIOFM, Marat, and PQFT.</p><p>Why then is the human inter-observer significantly better than other models when using NSS? This is likely because even if in few occasions humans look at the same location, this generates a very large NSS value. The human inter-observer map in this dataset is a very sparse map and a hit results in a very large NSS score. Also, note that the inter-observer model is not significantly better than the three best computational models using AUC. Interestingly, only the motion channel of the Itti model (Itti-M) worked better than many models over video stimuli (specially using CC and NSS scores). Itti-Int was the worst among all models with STB, Entropy, Itti-CIO, Variance, VOCUS, and Surprise-CIO: all these models indeed only use static features. CC values are smaller here compared with still images because there are fewer fixations (due to smaller numbers of subjects).</p><p>All models achieved higher scores (all three) over the saccadetest video clip, which is a circular moving blob on a static blue background (see Fig. <ref type="figure" target="#fig_1">2</ref>). Other stimuli on which models did well include gamecube05, gamecube17, tv-news04, gamecube06, and gamecube23, which tend to depict only one central moving actor of interest. Lowest scores belong to standard04, tv-announce01, tv-talk05, and standard03, which are very cluttered scenes with many actors and moving objects. Inspecting the difficult video clips suggests that eye fixations in these clips are often driven by complex cognitive processes; for instance, in tv-talk-05, fixations switch from one speaker to the other following their subtle lip movements, while the overall saliency of both their faces remains high throughout the clip. Much more thus needs to be studied in modeling such cognitive influences on saliency, as small dynamic changes pixel-wise (like moving lips) can yield dramatic differences in human gaze allocation (see, e.g., <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>). Eye fixation distributions of CRCNS-ORIG dataset shows higher density at the center compared to still image datasets (about 42% at the inner-most circle (10% radius) and about 83% at 40% radius). This could also be verified by the high scores of the Gauss model over CC and NSS scores. Over this dataset, similar to image datasets, NSS and AUC scores of many models are much smaller than human inter-observer scores. Generally, models that performed well over static images also achieved higher accuracies over the CRCNS-ORIG dataset. Interestingly, overall, models with a motion channel rank towards the middle, i.e., they do not seem to work better than the best models which only use static features, though they still work better than the lowest-performing static models.</p><p>Ranking of models over DIEM video dataset is shown in Fig. <ref type="figure" target="#fig_7">8(b</ref>). The IO, Tavakoli, Gauss, GBVS, HouNIPS, Bian, and Judd models ranked on top using CC and NSS scores. Using shuffled AUC, however, AWS, Bian,  Murray, Judd, AIM, and HouNIPS scored best. The sport_scramblers_1280x720 video was the easiest on average for models over three scores because it has mainly one moving object. Models that performed poorly over the CRCNS-ORIG dataset are also ranked at the bottom on DIEM dataset. Several videos clips in this dataset yield very poor model scores for all models. Here again, those clips include significant cognitive factors; for example, in the ping-pong videos, a reactive saliency model often trails behind human fixations which tend to be more predictive <ref type="bibr" target="#b77">[78]</ref>. Adding stronger predictive abilities to models is a very hard problem as the predictions occur in the 3D world, thus requiring extensive machine vision to recover 3D structure from videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Analysis of Gaussian Blob Size</head><p>Another important factor in model comparison is the size of the Gaussian blob. We changed the sigma (σ ) parameter of the Gauss model and evaluated the scores over three datasets shown in Fig. <ref type="figure" target="#fig_8">9</ref>. Two points should be noticed here: 1) Using all three scores, maximum performance happens for the Gaussian σ equal to 6, 7, or 8. In our experiments, Gaussian σ = 10 was used for model comparison and 2) Over shuffled AUC, as it was expected, values do not change for different Gaussians over three datasets (between 0.5 and 0.512). This again shows that shuffled AUC is invariant to center-bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Time Complexity Analysis of Models</head><p>In addition to correctly predicting atypical image locations attracting human attention, a saliency model should be also very fast. For some species, attention is tightly linked to their survival (e.g., quick detection and response to a predator). Some complex processes such as cluttered scene understanding will not be feasible or will be very slow without employing an effective attentional strategy. Thus, it is important that attention should kicky orient other complex processes to important dimensions of stimuli. The average time required to compute saliency map of an image for models is shown in Table <ref type="table" target="#tab_3">III</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Illustrative Figures</head><p>The three best and three worst stimuli (measured by shuffled AUC score) for each model are shown in Fig. <ref type="figure" target="#fig_0">10</ref>. Many models share their three best and three worst images. For the Gauss model stimuli that have fixations at the center happen to be the best and those that have fixations off the center are the worst. Since no model uses face detection (except for Judd et al.) and text detection channels, most models have difficulty predicting fixations over stimuli with these types of features. This means that an important point in building more successful models is to look for cognitive factors that drive visual attention (e.g., gaze direction of human characters in images, meaning of text messages <ref type="bibr" target="#b68">[69]</ref>, etc.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DISCUSSION AND CONCLUSION</head><p>In this paper, we briefly reviewed several state-of-the-art visual saliency models and quantitatively compared them over 54 synthetic patterns, three radically different still image datasets, and two benchmark video datasets. We also analyzed datasets in terms of center-bias and models in terms of time complexity. Here, we list the main conclusions of our comparison study:</p><p>1) All existing datasets are highly center-biased. Developing less center-biased datasets in the future can help fair model comparison <ref type="foot" target="#foot_4">4</ref> .</p><p>2) The majority of existing eye movements datasets are small with small numbers of subjects. Further attempts are necessary to collect larger datasets with more observers (to obtain a better notion of average human performance) with higher stimulus variability. This need is more pressing for collecting fixations over videos.</p><p>3) The CC and NSS scores suffer from the center-bias issue and their use in future model comparisons is not encouraged. On the other hand, the shuffled AUC score tackles center bias and border effects, and is the best option for model comparison. 4) There is still a gap between current models and human performance. This gap is smaller for off-center fixations and for some datasets, but overall exists. As discussed above, discovering and adding more top-down features to models will hopefully boost their performance. 5) Saliency models based on FIT theory work better in locating a target over synthetic patterns. 6) Models that did well over static natural scenes in general also did well over the video datasets. The majority of these models are based on statistical techniques. 7) The top performing model in our experiments with static and dynamic natural scenes is AWS (focusing on the shuffled AUC score); it also performed second best with synthetic images. 8) Consistent with <ref type="bibr" target="#b78">[79]</ref>, we also noticed that models that generate blurrier maps achieve higher scores (e.g., GBVS, AIM, and Itti-CIO2). This should be considered by authors and future comparisons. 9) Models incorporating motion did not perform better than the best static models over video datasets. Extension of the best existing static models to the spatio-temporal domain may further scale up those models. 10) Some categories are harder for models (e.g., Nature stimuli) while some others containing less cluttered scenes and scenes with fewer objects are easier (e.g., scenes containing humans and cars). 11) Best and worst stimuli are the same for many models, which means that models have common difficulty in prediction of saliency over specific classes of stimuli (Fig. <ref type="figure" target="#fig_0">10</ref>). This suggests some hints for future research. 12) Some models are fast and effective (e.g., HouNIPS, Bian, HouCVPR, Torralba, and Itti-CIO2) providing a tradeoff between accuracy and speed necessary for many applications. One remaining problem in fair model evaluation is the effect of internal model parameters, such as number of filters, type of filters, Gabor or DoG filter parameters, choice of the nonlinearities, blurring, and normalization schemes. Proper tuning of these parameters is important, and doing so may dramatically affect the performance of a system. Here, we tried our best to produce highly predictive maps for models.</p><p>Despite significant success of the models evaluated here, there is still significant room to further improve attention accuracy due to a remaining large gap between models and human observer agreement, as has also been shown using smaller datasets and less systematic comparisons in previous studies (e.g., <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b44">[45]</ref>). Here, we suggest several directions that could help bridge this gap.</p><p>One direction to extend current models is adding top-down factors. Context <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b44">[45]</ref>, gain modulation of features for target detection <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b79">[80]</ref>, and use of target detectors tuned to specific objects <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b68">[69]</ref> has been used for modeling topdown attentional effects. Here, we comment more on these factors. While almost all bottom-up models have employed simple feature channels believed to be computed by early visual areas, they do not rule out the existence of top-down influences in free-viewing tasks where these models have been applied to. For instance, in free viewing of spatio-temporal stimuli, such as videos, semantic processing of scenes, and extraction of high-level knowledge plays a significant role in guiding attention and eye movements. Some semantic cues involve social interactions in images, living beings, faces (and eyes, nose, and mouth within faces), text, etc. Also it appears that attentional-bias is independent of illumination, orientation as well as scale of the salient object/concept <ref type="bibr" target="#b80">[81]</ref>. A large dataset containing many example images with such factors (758 images viewed by on average 25.3 viewers) has recently Fig. <ref type="figure" target="#fig_0">10</ref>. Three best and three worst stimuli using shuffled AUC score for all models over Judd et al. dataset. Note that some images are best for many models and at the same time some worst cases repeat across many models. Yellow dots represent human fixations. been collected by Ramanathan et al. <ref type="bibr" target="#b81">[82]</ref>. They also observed that unpleasant concepts, such as reptiles or blood and injury, considerably influence visual attention whenever present. The fact that recognized concepts drive visual attention adds support to the theory that visual attention and object recognition are concurrent processes, and this is an interesting topic of research in the cognitive science community. Therefore, adding top-down factors to bottom-up models can be an important topic for future research in saliency modeling. Indeed the list of top-down factors is not limited to the above factors and several others, including task demands (e.g., real-world tasks), memories, experiences, expectations, and internal states play an important roles in directing overt attention and gaze.</p><p>We suggest taking inspiration from early visual cortex for developing more biologically inspired models of attention. For instance, the AWS model takes advantage of a basic idea, decorrelation of neural responses in representing a visual stimuli <ref type="bibr" target="#b82">[83]</ref>, <ref type="bibr" target="#b83">[84]</ref>. In this regard, shown by our results, having many features (similar to Judd <ref type="bibr" target="#b49">[50]</ref>) might not be as efficient as discovering the basic principles/features of attention guidance (employed by models, such as Itti-CIO, AWS, HouNIPS, and GBVS). An idea in this direction is validating models of saliency against eye movements of humans over distorted images (e.g., rotated, mirrored, or inversed images) or by considering detailed low-level neural findings revealed by neurophysiological studies (e.g., <ref type="bibr" target="#b84">[85]</ref>).</p><p>Another future direction will be combining several different saliency models to achieve higher performances. Since each of these models is based on different mechanisms, it is likely that combining them may result in higher fixation prediction. This trend has been followed previously in biometrics (e.g., face identifications) as well as character recognition <ref type="bibr" target="#b85">[86]</ref>. Such direction may not extend our understanding of visual attention, but if successful it may have several practical applications.</p><p>There are several other open questions for future investigations. As already mentioned, text is an important feature that is proven to attract attention <ref type="bibr" target="#b68">[69]</ref>. But since text detection in natural scenes is an open problem and few approaches exist for that, it has not been added to current models. Basically using more features leads to better fixation prediction performance with the cost of lowering speed. One solution is parallel implementation of models (e.g., feature extraction on GPU (e.g., <ref type="bibr" target="#b86">[87]</ref>, <ref type="bibr" target="#b87">[88]</ref>). Most models have focused on predicting locations that human observers look at, while few (e.g., <ref type="bibr" target="#b33">[34]</ref>) have investigated other aspects of eye fixations, such as saccade dynamics, sequencing (Wang et al. <ref type="bibr" target="#b88">[89]</ref>), retinal sampling, inhibition of return, the role of context, etc <ref type="bibr" target="#b89">[90]</ref>. More work needs to be done in this direction.</p><p>A less explored application of saliency modeling is using it for understanding cluttered scenes by sequentially processing important regions or objects. Another promising direction is in developing models that can predict locations that humans find interesting, for instance by clicking and see how such models differ from traditional saliency models for fixation prediction <ref type="bibr" target="#b90">[91]</ref>. Also, more attempts are still needed to determine important features attracting eye fixations. Extending models to include some understanding of 3D scene structure is a challenging yet pressing problem, as solving it may allow the creation of new models with significantly better predictive abilities (e.g., the expected landing point of a ball might be more salient than the ball itself). It would be also interesting to customize a saliency model for each person. For instance, by learning habits, preferences, etc. of each human subject. This way, it is theoretically possible to surpass the human inter-observer model. Interaction between attention and object recognition and their mutual benefit has been overstated, but still there are not many works in this area.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Synthetic patterns. Stimuli are numbered in blue/yellow from 1 to 54 in row-first order. Numbers are positioned close to the target locations and are for illustration purposes only. Stimuli are sorted according to their average easiness of oddity detection for saliency models (see Fig. 5).</figDesc><graphic coords="4,65.03,288.17,481.34,192.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. One sample frame (frame no. 100) from 50 videos of CRCNS-ORIG eye movement dataset. Eye movements are embedded on images in yellow. For some videos, eye fixations are shown in blue for better illustration. Video names in order (from left to right, top to bottom) are: 1) Beverly01; 2) Beverly03; 3) Beverly05; 4) Beverly06; 5) Beverly07; 6) Beverly08; 7) Gamecube02; 8) Gamecube04; 9) Gamecube05; 10) Gamecube06; 11) Gamecube13; 12) Gamecube6; 13) Gamecube7; 14) Gamecube18; 15) Gamecube23; 16) Monica03; 17) Monica04; 18) Monica05; 19) Monica06; 20) Saccadetest; 21) Standard01; 22) Standard02; 23) Standard03; 24) Standard04; 25) Standard05; 26) Standard06; 27) Standard07; 28) Tv -action01; 29) T v -ads01; 30) T v -ads02; 31) T v -ads03; 32) T v -ads04; 33) T v -announce01; 34) T v -music01; 35) T v -news01; 36) T v -news02; 37) T v -news03; 38) T v -news04; 39) T v -news05; 40) T v -news06; 41) T v -news09; 42) T v -sports01; 43) T v -sports02; 44) T v -sports03; 45) T v -sports04; 46) T v -sports05; 47) T v -talk01; 48) T v -talk03; 49) T v -talk04;and 50) T v -talk05. Note that a different number of subjects observed videos. For results of model comparisons on these videos, please see Fig.8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 2. One sample frame (frame no. 100) from 50 videos of CRCNS-ORIG eye movement dataset. Eye movements are embedded on images in yellow. For some videos, eye fixations are shown in blue for better illustration. Video names in order (from left to right, top to bottom) are: 1) Beverly01; 2) Beverly03; 3) Beverly05; 4) Beverly06; 5) Beverly07; 6) Beverly08; 7) Gamecube02; 8) Gamecube04; 9) Gamecube05; 10) Gamecube06; 11) Gamecube13; 12) Gamecube6; 13) Gamecube7; 14) Gamecube18; 15) Gamecube23; 16) Monica03; 17) Monica04; 18) Monica05; 19) Monica06; 20) Saccadetest; 21) Standard01; 22) Standard02; 23) Standard03; 24) Standard04; 25) Standard05; 26) Standard06; 27) Standard07; 28) Tv -action01; 29) T v -ads01; 30) T v -ads02; 31) T v -ads03; 32) T v -ads04; 33) T v -announce01; 34) T v -music01; 35) T v -news01; 36) T v -news02; 37) T v -news03; 38) T v -news04; 39) T v -news05; 40) T v -news06; 41) T v -news09; 42) T v -sports01; 43) T v -sports02; 44) T v -sports03; 45) T v -sports04; 46) T v -sports05; 47) T v -talk01; 48) T v -talk03; 49) T v -talk04; and 50) T v -talk05.Note that a different number of subjects observed videos. For results of model comparisons on these videos, please see Fig.8.BBC-life-in-cold-blood -1278x710</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Results of center-bias analysis over three datasets. The first row shows the heatmap of all fixations over all images for each dataset. White rings show 10% increase in radius from the image center, and the bar chart at the right of the heatmap shows the percentage of fixations that happen in each ring. The red horizontal bar shows the 80% density level. Five most and least center-biased images from each dataset along with eye fixations are shown at the bottom.</figDesc><graphic coords="6,50.87,53.69,510.02,192.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Ranking models over synthetic patterns shown in Fig. 1. (a) Individual NSS scores of models for stimuli. (b) Sorts models averaged over stimuli and (c) sorts stimuli averaged over models. Error bars indicate standard error of the mean (SEM). (d) Spatial distribution of target locations.</figDesc><graphic coords="7,217.31,188.45,66.14,54.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Prediction maps of saliency models for the best, worst, and other sample synthetic stimuli. Best and worst stimuli are determined based on difficulty of models (on average) to detect the odd item among distractors in a search array.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig.7. (a) Ranking visual saliency models over three image datasets using three evaluation scores: Correlation coefficient (CC), normalized scanpath saliency (NSS), and shuffled AUC. Left column: Bruce and Tsotsos<ref type="bibr" target="#b36">[37]</ref>. Middle column: Kootstra and Shomacker<ref type="bibr" target="#b43">[44]</ref>. Right column: Judd et al.<ref type="bibr" target="#b49">[50]</ref>. Stars indicate statistical significance using t-test (95%, p ≤ 0.05) between consecutive models. Note that no star between two models that are not immediately close to each other does not necessarily mean that they are not significantly different. In fact, it is highly probable that a model that is significantly better than the one in its left, also scores significantly better than all other models on its left. Error bars indicate standard error of the mean (SEM): (σ/ √ N ), where σ is the standard deviation and N is the number of images. We do not show CC results for IO model because comparing the map built from fixations of one subject with the map built from fixations of all other subjects using CC, does not generate a high value (both maps are convolved with a Gaussian). This is because few fixations of only one subject do not generate a diffused map, which is favored by CC score. We also could not calculate IO score over Bruce and Tsotsos dataset since fixations are not separated for each subject. (b) This column sorts models over 100 least center-biased images from the Judd et al. dataset (see Section II-D). The heatmap at the top-most panel shows distribution of fixations over selected images. Judd model uses center feature, gist, and horizon line, and object detectors for cars, faces, and human body. Itti-CIO2 is the approach proposed by Itti et al.<ref type="bibr" target="#b30">[31]</ref> that uses the following normalization scheme: For each feature map, find the global max M and find the average m of all other local maxima. Then just weight the map by (Mm)2 . In the Itti-CIO method<ref type="bibr" target="#b32">[33]</ref>, normalization is: Convolve each map by a difference of Gaussian(DoG) filter, cut off negative values, and iterate this process for a few times. This normalization operation results in sparse saliency maps. In the literature, majority of models have been compared against Itti-CIO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. (a) Ranking visual saliency models over CRCNS-ORIG dataset<ref type="bibr" target="#b61">[62]</ref>. (b) Ranking models over DIEM dataset<ref type="bibr" target="#b62">[63]</ref>. Only these models had motion channel: Itti-M, Itti-CIOFM, Surprise-CIOFM, Marat, and PQFT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Analysis of Gaussian blob size parameter. CC, NSS, and shuffled AUC scores over Gaussian blobs at the image center with increasing size from small to large (bottom-row). Size of each blob is 50 × 50 pixels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="13,64.79,57.65,481.70,354.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I COMPARED</head><label>I</label><figDesc>VISUAL SALIENCY MODELS.</figDesc><table><row><cell cols="2">No. Acronym: Model</cell><cell cols="2">Year S P</cell><cell>Resolution</cell></row><row><cell>1</cell><cell>Gauss: Gaussian-blob</cell><cell>-</cell><cell>I M</cell><cell>51 × 51</cell></row><row><cell>2</cell><cell>IO: Human inter-observer</cell><cell>-</cell><cell>I M</cell><cell>W × H</cell></row><row><cell>3 4</cell><cell>Variance: [31] Entropy: [74]</cell><cell>--</cell><cell>I C I C</cell><cell>1 16 W × 1 16 H 1 16 W × 1 16 H</cell></row><row><cell>5 6 7</cell><cell>Itti-CIO2: Itti et al. [31], [32] Itti-Int: Itti et al. [31], [32] Itti-CIO: Itti et al. [33], [32]</cell><cell></cell><cell>I C I C I C</cell><cell>1 16 W × 1 16 H 1 16 W × 1 16 H 1 16 W × 1 16 H</cell></row><row><cell>8 9</cell><cell>Itti-M: Itti et al. [34] Itti-CIOFM: Itti et al. [34]</cell><cell></cell><cell>V C B C</cell><cell>1 16 W × 1 16 H 1 16 W × 1 16 H</cell></row><row><cell cols="2">10 Torralba: [35]</cell><cell></cell><cell>I M</cell><cell>W × H</cell></row><row><cell cols="2">11 VOCUS: Frintrop et al. [4] 12 Surprise-CIO: [36] 13 Surprise-CIOFM: [36]</cell><cell></cell><cell>B C I C B C</cell><cell>1 4 W × 1 4 H 1 16 W × 1 16 H 1 16 W × 1 16 H</cell></row><row><cell cols="2">14 AIM: Bruce and Tsotsos [37] 15 STB: saliency toolbox [1]</cell><cell></cell><cell>I M I M</cell><cell>1 2 W × 1 2 H 1 16 W × 1 16 H</cell></row><row><cell cols="2">16 Le Meur: Le Meur et al. [38], [39]</cell><cell></cell><cell>B X</cell><cell>W × H</cell></row><row><cell cols="2">17 GBVS: Harel et al. [40]</cell><cell></cell><cell>I M</cell><cell>W × H</cell></row><row><cell cols="2">18 HouCVPR: Hou et al. [41]</cell><cell></cell><cell>I M</cell><cell>64 × 64</cell></row><row><cell cols="2">19 Rarity-L: local rarity [42]</cell><cell></cell><cell>I M</cell><cell>W × H</cell></row><row><cell cols="2">20 Rarity-G: global rarity [42]</cell><cell></cell><cell>I M</cell><cell>W × H</cell></row><row><cell></cell><cell>51]</cell><cell></cell><cell>I M</cell><cell>1 16 W × 1 16 H</cell></row><row><cell cols="2">30 E-Saliency: Avraham et al. [52]</cell><cell></cell><cell>I X</cell><cell>W × H</cell></row><row><cell cols="2">31 Yan: Yan et al. [53]</cell><cell></cell><cell>I M</cell><cell>W × H</cell></row><row><cell cols="2">32 AWS: Diaz et al. [54]</cell><cell></cell><cell>I E</cell><cell>1 2 W × 1 2 H</cell></row><row><cell cols="2">33 Jia Li: Jia Li et al. [55]</cell><cell></cell><cell>I E</cell><cell></cell></row></table><note><p>21 HouNIPS: Hou et al. [43] I M W × H 22 Kootstra: Kootstra and Shomacker [44] I E W × H 23 SUN: Zhang et al. [45] I M 246 × 331 24 Marat: Marat et al. [46] B X W × H 25 PQFT: Guo et al. [47] I M 400 × 400 26 Yin Li: Yin Li et al. [48] I M W × H 27 SDSR: Seo and Milanfar [49] B M W × H 28 Judd: Judd et al. [50] I M W × H 29 Bian: Bian et al. [1 16 W × 1 16 H 34 Tavakoli: Tavakoli et al. [56] I M W/16 × H /16 35 Murray: Murray et al. [57] I M W × H S: Stimuli I: Image, V: Video, B: Both Image and Video. P: Programming Language M: MATLAB, C: C/C++, E: Executables, X: Sent Saliency Maps. W: Image Width and H: Image Height.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II MODEL</head><label>II</label><figDesc>COMPARISON OVER CATEGORIES OF KOOTSTRA AND SHOMACKER DATASET USING SHUFFLED AUC SCORE. SECOND NUMBER IN EACH PAIR OF VALUES IS SEM. THE THREE BEST MODELS FOR EACH CATEGORY ARE SHOWN IN BOLD. LAST THREE ROWS SHOW THE AVERAGE PERFORMANCE OF ALL MODELS USING THREE SCORES</figDesc><table><row><cell></cell><cell>Buildings</cell><cell>Nature</cell><cell>Animals</cell><cell>Flowers</cell><cell>Automan</cell></row><row><cell>Size</cell><cell>16</cell><cell>40</cell><cell>12</cell><cell>20</cell><cell>12</cell></row><row><cell>IO</cell><cell>0.62 ± 0.03</cell><cell>0.58 ± 0.04</cell><cell>0.65 ± 0.04</cell><cell>0.62 ± 0.04</cell><cell>0.70 ± 0.03</cell></row><row><cell>Gauss</cell><cell>0.50 ± 0.04</cell><cell>0.50 ± 0.04</cell><cell>0.50 ± 0.07</cell><cell>0.50 ± 0.07</cell><cell>0.50 ± 0.07</cell></row><row><cell>AIM</cell><cell>0.58 ± 0.02</cell><cell>0.55 ± 0.05</cell><cell>0.58 ± 0.05</cell><cell>0.58 ± 0.06</cell><cell>0.63 ± 0.05</cell></row><row><cell>AWS</cell><cell>0.60 ± 0.04</cell><cell>0.58 ± 0.06</cell><cell>0.63 ± 0.07</cell><cell>0.62 ± 0.06</cell><cell>0.68 ± 0.05</cell></row><row><cell>E-Saliency</cell><cell>0.56 ± 0.04</cell><cell>0.53 ± 0.05</cell><cell>0.57 ± 0.06</cell><cell>0.54 ± 0.07</cell><cell>0.63 ± 0.06</cell></row><row><cell>Bian</cell><cell>0.52 ± 0.07</cell><cell>0.55 ± 0.05</cell><cell>0.60 ± 0.08</cell><cell>0.56 ± 0.08</cell><cell>0.61 ± 0.09</cell></row><row><cell>Entropy</cell><cell>0.54 ± 0.04</cell><cell>0.52 ± 0.03</cell><cell>0.51 ± 0.05</cell><cell>0.56 ± 0.04</cell><cell>0.57 ± 0.04</cell></row><row><cell>GBVS</cell><cell>0.56 ± 0.03</cell><cell>0.55 ± 0.05</cell><cell>0.57 ± 0.04</cell><cell>0.55 ± 0.06</cell><cell>0.60 ± 0.07</cell></row><row><cell>Kootstra</cell><cell>0.56 ± 0.03</cell><cell>0.53 ± 0.04</cell><cell>0.54 ± 0.06</cell><cell>0.54 ± 0.07</cell><cell>0.58 ± 0.05</cell></row><row><cell>HouCVPR</cell><cell>0.58 ± 0.03</cell><cell>0.54 ± 0.05</cell><cell>0.59 ± 0.05</cell><cell>0.55 ± 0.06</cell><cell>0.62 ± 0.05</cell></row><row><cell>HouNIPS</cell><cell>0.58 ± 0.03</cell><cell>0.56 ± 0.05</cell><cell>0.59 ± 0.07</cell><cell>0.59 ± 0.06</cell><cell>0.66 ± 0.07</cell></row><row><cell>Itti-CIO</cell><cell>0.52 ± 0.02</cell><cell>0.52 ± 0.03</cell><cell>0.54 ± 0.03</cell><cell>0.51 ± 0.03</cell><cell>0.54 ± 0.02</cell></row><row><cell>Itti-CIO2</cell><cell>0.55 ± 0.04</cell><cell>0.55 ± 0.03</cell><cell>0.58 ± 0.05</cell><cell>0.54 ± 0.04</cell><cell>0.64 ± 0.03</cell></row><row><cell>Jia Li</cell><cell>0.56 ± 0.04</cell><cell>0.53 ± 0.04</cell><cell>0.57 ± 0.06</cell><cell>0.52 ± 0.08</cell><cell>0.60 ± 0.05</cell></row><row><cell>Judd</cell><cell>0.57 ± 0.04</cell><cell>0.56 ± 0.05</cell><cell>0.58 ± 0.06</cell><cell>0.58 ± 0.06</cell><cell>0.63 ± 0.05</cell></row><row><cell>Le Meur</cell><cell>0.55 ± 0.05</cell><cell>0.55 ± 0.05</cell><cell>0.55 ± 0.05</cell><cell>0.55 ± 0.05</cell><cell>0.62 ± 0.07</cell></row><row><cell>Marat</cell><cell>0.51 ± 0.02</cell><cell>0.50 ± 0.02</cell><cell>0.51 ± 0.02</cell><cell>0.51 ± 0.02</cell><cell>0.51 ± 0.01</cell></row><row><cell>PQFT</cell><cell>0.53 ± 0.06</cell><cell>0.53 ± 0.05</cell><cell>0.52 ± 0.06</cell><cell>0.58 ± 0.05</cell><cell>0.58 ± 0.05</cell></row><row><cell>Rarity-G</cell><cell>0.53 ± 0.03</cell><cell>0.53 ± 0.03</cell><cell>0.55 ± 0.02</cell><cell>0.56 ± 0.04</cell><cell>0.57 ± 0.04</cell></row><row><cell>Rarity-L</cell><cell>0.54 ± 0.02</cell><cell>0.53 ± 0.03</cell><cell>0.54 ± 0.04</cell><cell>0.53 ± 0.04</cell><cell>0.57 ± 0.05</cell></row><row><cell>SDSR</cell><cell>0.58 ± 0.04</cell><cell>0.56 ± 0.05</cell><cell>0.62 ± 0.06</cell><cell>0.55 ± 0.06</cell><cell>0.65 ± 0.07</cell></row><row><cell>SUN</cell><cell>0.53 ± 0.06</cell><cell>0.53 ± 0.05</cell><cell>0.50 ± 0.06</cell><cell>0.58 ± 0.05</cell><cell>0.59 ± 0.07</cell></row><row><cell>Surprise-CIO</cell><cell>0.53 ± 0.03</cell><cell>0.54 ± 0.04</cell><cell>0.55 ± 0.03</cell><cell>0.53 ± 0.05</cell><cell>0.55 ± 0.02</cell></row><row><cell>Torralba</cell><cell>0.56 ± 0.03</cell><cell>0.54 ± 0.04</cell><cell>0.55 ± 0.05</cell><cell>0.58 ± 0.06</cell><cell>0.62 ± 0.05</cell></row><row><cell>Variance</cell><cell>0.54 ± 0.03</cell><cell>0.53 ± 0.04</cell><cell>0.52 ± 0.05</cell><cell>0.57 ± 0.06</cell><cell>0.59 ± 0.04</cell></row><row><cell>VOCUS</cell><cell>0.56 ± 0.03</cell><cell>0.54 ± 0.04</cell><cell>0.58 ± 0.05</cell><cell>0.56 ± 0.06</cell><cell>0.63 ± 0.06</cell></row><row><cell>STB</cell><cell>0.51 ± 0.01</cell><cell>0.51 ± 0.01</cell><cell>0.53 ± 0.04</cell><cell>0.51 ± 0.02</cell><cell>0.51 ± 0.01</cell></row><row><cell>Yan</cell><cell>0.57 ± 0.03</cell><cell>0.55 ± 0.06</cell><cell>0.60 ± 0.05</cell><cell>0.56 ± 0.06</cell><cell>0.65 ± 0.06</cell></row><row><cell>Yin Li</cell><cell>0.55 ± 0.03</cell><cell>0.55 ± 0.05</cell><cell>0.59 ± 0.06</cell><cell>0.57 ± 0.06</cell><cell>0.60 ± 0.07</cell></row><row><cell>Average-AUC</cell><cell>0.55 ± 0.02</cell><cell>0.54 ± 0.01</cell><cell>0.56 ± 0.3</cell><cell>0.55 ± 0.02</cell><cell>0.60 ± 0.04</cell></row><row><cell>Average-CC</cell><cell>0.17 ± 0.06</cell><cell>0.17 ± 0.07</cell><cell>0.22 ± 0.84</cell><cell>0.19± 0.82</cell><cell>0.24 ± 0.07</cell></row><row><cell>Average-NSS</cell><cell>0.33 ± 0.12</cell><cell>0.30 ± 0.13</cell><cell>0.57 ± 0.2</cell><cell>0.46 ± 0.20</cell><cell>0.54 ± 0.18</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>. Average time was calculated over 100 images with resolution 511 × 681 from Bruce and Tsotsos dataset. All models were executed on a computer running Linux Mandriva with 4GB RAM and Quad core 2.8 GHz Intel CPU. The Itti-CIO model is the fastest (∼17 ms/image) followed by VOCUS and HouCVPR models (less than 300 ms/image). Note that in this table, what matters is ranking, while absolute durations may be reduced with more powerful machines. The Judd et al., model has high saliency prediction accuracy but is very slow (about 100 sec/image) since it needs to calculate several fairly complex channels (person, face, car, gist, horizontal line, etc). Most of the models need less than 16 sec to calculate saliency.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III AVERAGE</head><label>III</label><figDesc>SALIENCY COMPUTATION TIME (SORTED) FOR MODELS IN SECONDS FOR A 511 × 681 IMAGE. THE TWO FASTEST MODELS ARE WRITTEN IN C++ CODE. HOUCVPR IS IN MATLAB</figDesc><table><row><cell>Model</cell><cell>Judd</cell><cell>Yin Li</cell><cell>SUN</cell><cell>AIM</cell><cell>Yan</cell><cell>AWS</cell><cell>GBVS</cell><cell>Rarity-L</cell><cell>Rarity-G</cell><cell>SDSR</cell><cell>STB</cell><cell>Torralba</cell><cell>PQFT</cell><cell>HouNIPS</cell><cell>Bian</cell><cell>HouCVPR</cell><cell>VOCUS</cell><cell>Itti-CIO</cell></row><row><cell>Time</cell><cell>98.58</cell><cell>55.07</cell><cell>51.92</cell><cell>15.6</cell><cell>13.05</cell><cell>12.08</cell><cell>10.14</cell><cell>4.14</cell><cell>3.6</cell><cell>2.35</cell><cell>2.29</cell><cell>2.24</cell><cell>1.7</cell><cell>1.14</cell><cell>1.1</cell><cell>0.30</cell><cell>0.025</cell><cell>0.017</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>DIEM has so far collected data from over</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="250" xml:id="foot_1"><p>participants watching 85 different videos. All of this data is freely available. We selected 20 videos and about 1,000 frames from each to make a benchmark for model comparison. Selected videos cover different concepts/topics. We only used right-eye positions of subjects to make model evaluation tractable. Frames of this dataset were scaled down to 640 × 480 while maintaining aspect ratio.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>In addition to above scores, Kullback-Leibler (KL) (the divergence between the saliency distributions at human fixations and at randomly shuffled fixations; used in<ref type="bibr" target="#b35">[36]</ref>,<ref type="bibr" target="#b44">[45]</ref>),<ref type="bibr" target="#b60">[61]</ref>, and the string-edit distance (difference between the sequence of fixations generated by a saliency model versus human fixations)<ref type="bibr" target="#b72">[73]</ref>,<ref type="bibr" target="#b73">[74]</ref>) have also been used for model evaluation. Note that all of these scores (except the Shuffled AUC) are influenced by the center-bias. We draw conclusions based on the average model behavior on these scores.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>We also used another dataset from Le Meur et al.<ref type="bibr" target="#b37">[38]</ref> but none of the images passed the threshold. Link: http://www.irisa.fr/temics/staff/lemeur/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>We share a dataset at: https://sites.google.com/site/saliencyevaluation/.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>. This work was supported in part by the Defense Advanced Research Projects Agency under Contract HR0011-10-C-0034, the National Science Foundation (CRCNS) under Grant BCS-0827764, the General Motors Corporation, and the Army Research Office under Grant W911NF-08-1-0360 and Grant W911NF-11-1-0046. The associate editor coordinating the review of this manuscript and approving it for publication was Prof. Alex ChiChung Kot.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Modeling attention to salient proto-objects</title>
		<author>
			<persName><forename type="first">D</forename><surname>Walther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1395" to="1407" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A selective attention-based method for visual pattern recognition with application to handwritten digit recognition and face recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Salah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Alpaydin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Akrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="420" to="425" />
			<date type="published" when="2002-03">Mar. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Robust classification of objects, faces, and flowers using national image</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. Vis. Pattern Recognit</title>
		<meeting>Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">VOCUS: A visual attention system for object detection and goal-directed search</title>
		<author>
			<persName><forename type="first">S</forename><surname>Frintrop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dept. Comput. Sci., Univ. Bonn</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<pubPlace>Bonn, Germany</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An integrated model of top-down and bottom-up attention for optimizing detection speed</title>
		<author>
			<persName><forename type="first">V</forename><surname>Navalpakkam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="2049" to="2056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">General object tracking with a component-based target descriptor</title>
		<author>
			<persName><forename type="first">S</forename><surname>Frintrop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Robot. Autom</title>
		<meeting>Int. Conf. Robot. Autom</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="4531" to="4536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic foveation for video compression using a neurobiological model of visual attention</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1304" to="1318" />
			<date type="published" when="2004-10">Oct. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A generic framework of user attention model and its application in video summarization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="907" to="919" />
			<date type="published" when="2005-10">Oct. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Stylization and abstraction of photographs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Decarlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Santella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="769" to="776" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A framework for visual saliency detection with applications to image thumbnailing</title>
		<author>
			<persName><forename type="first">L</forename><surname>Marchesotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cifarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2232" to="2239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Picture collage</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. Vis. Pattern Recognit</title>
		<meeting>Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="347" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dynamic intelligent lighting for directing visual attention in interactive 3-D scenes</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>El-Nasr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vasilakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zupko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput. Intell. AI Games</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="145" to="153" />
			<date type="published" when="2009-06">Jun. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The neural active vision system</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mertsching</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bollmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hoischen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schmalz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Computer Vision and Applications</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="543" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Online learning of task-driven object-based visual attention control</title>
		<author>
			<persName><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Ahmadabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Araabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hamidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1130" to="1145" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Biologically inspired mobile robot vision localization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Siagian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Robot</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="861" to="873" />
			<date type="published" when="2009-08">Aug. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A context-dependent attention system for a social robot</title>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf</title>
		<meeting>Int. Joint Conf</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="1146" to="1151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Do predictions of visual perception aid design?</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rosenholtz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dorai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Appl. Percept</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast hand gesture recognition based on saliency maps: An application to interactive robotic marionette playing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ajallooeian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Araabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Ahmadabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Moradi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Robot Human Interact</title>
		<meeting>IEEE Conf. Robot Human Interact</meeting>
		<imprint>
			<date type="published" when="2009-10">Oct. 2009</date>
			<biblScope unit="page" from="841" to="847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Saliency-based image processing for retinal prostheses</title>
		<author>
			<persName><forename type="first">N</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weiland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neural Eng</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="16006" to="16007" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<ptr target="http://pascallin.ecs.soton.ac.uk/challenges/VOC/" />
		<title level="m">The PASCAL Visual Object Classes</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Computational modelling of visual attention</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Rev. Neurosci</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="194" to="203" />
			<date type="published" when="2001-03">Mar. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Computational models of visual selective attention: A review</title>
		<author>
			<persName><forename type="first">D</forename><surname>Heinke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Humphreys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Connectionist Models in Psychology</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Houghton</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ed</forename><surname>Florence</surname></persName>
		</editor>
		<meeting><address><addrLine>KY</addrLine></address></meeting>
		<imprint>
			<publisher>Psychology Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="273" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Computational visual attention systems and their cognitive foundations: A survey</title>
		<author>
			<persName><forename type="first">S</forename><surname>Frintrop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">I</forename><surname>Christensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Appl. Percept</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="46" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">State-of-the-art in visual attention modeling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint/>
	</monogr>
	<note>to be published</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Memory representations in natural tasks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hayhoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pelz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Cognit. Neurosci</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="66" to="80" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Where we look when we steer</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Land</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">369</biblScope>
			<biblScope unit="page" from="742" to="744" />
			<date type="published" when="1994-06">Jun. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Beyond bottom-up: Incorporating taskdependent influences into a computational model of spatial attention</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. Vis. Pattern Recognit</title>
		<meeting>Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Eye movements in natural behavior</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hayhoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends Cognit. Sci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="188" to="194" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">High-level scene perception</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hollingworth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Psychol</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="243" to="271" />
			<date type="published" when="1999-02">Feb. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Objects predict fixations better than early saliency</title>
		<author>
			<persName><forename type="first">W</forename><surname>Einhauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Spain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998-11">Nov. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<ptr target="http://ilab.usc.edu/toolkit/" />
		<title level="m">iLab Neuromorphic Vision C++ Toolkit (iNVT)</title>
		<imprint>
			<date type="published" when="2010-11">2010, Nov. 17</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A saliency-based search mechanism for overt and covert shifts of visual attention</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<idno>nos. 10-12</idno>
	</analytic>
	<monogr>
		<title level="j">Vis. Res</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1489" to="1506" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Realistic avatar eye and head animation using a neurobiological model of visual attention</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dhavale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pighin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="2003-12">Dec. 2003</date>
			<biblScope unit="volume">5200</biblScope>
			<biblScope unit="page" from="64" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Modeling global scene factors in attention</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1407" to="1418" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Bayesian surprise attracts human attention</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Saliency based on information maximization</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D B</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A coherent computational approach to model bottom-up visual attention</title>
		<author>
			<persName><forename type="first">O</forename><surname>Le Meur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Le</forename><surname>Callet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thoreau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="802" to="817" />
			<date type="published" when="2006-05">May 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Predicting visual fixations on video based on low-level visual features</title>
		<author>
			<persName><forename type="first">O</forename><surname>Le Meur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Le</forename><surname>Callet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Res</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="2483" to="2493" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Graph-based visual saliency</title>
		<author>
			<persName><forename type="first">J</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Saliency detection: A spectral residual approach</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. Vis. Pattern Recognit</title>
		<meeting>Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Computational attention: Modelisation and application to audio and image processing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mancas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<pubPlace>Belgium</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Faculté Polytechnique de Mons, Arrondissement of Mons</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dynamic visual attention: Searching for coding length increments</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Prediction of human eye fixations using symmetry</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kootstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R B</forename><surname>Schomaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 31st Annu. Conf. Cognit</title>
		<meeting>31st Annu. Conf. Cognit</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">SUN: A Bayesian framework for saliency using natural statistics</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Modelling spatio-temporal saliency to predict gaze direction for short videos</title>
		<author>
			<persName><forename type="first">S</forename><surname>Marat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ho-Phuoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Granjon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Guyader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pellerin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guérin-Dugué</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="231" to="243" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A novel multiresolution spatiotemporal saliency detection model and its applications in image and video compression</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="185" to="198" />
			<date type="published" when="2010-01">Jan. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Visual saliency based on conditional entropy</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Asian Conf. Comput. Vis</title>
		<meeting>Asian Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="246" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Static and space-time visual saliency detection by self-resemblance</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning to predict where humans look</title>
		<author>
			<persName><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Biological plausibility of spectral domain approach for spatiotemporal visual saliency</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neuro-Information Processing</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Esaliency (extended saliency): Meaningful attention using stochastic image modeling</title>
		<author>
			<persName><forename type="first">T</forename><surname>Avraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lindenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="693" to="708" />
			<date type="published" when="2010-04">Apr. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Visual saliency detection via ranksparsity decomposition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Image Process</title>
		<meeting>Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1089" to="1092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Saliency from hierarchical adaptation through decorrelation and variance normalization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Garcia-Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">R</forename><surname>Fdez-Vidal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">M</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dosil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="51" to="64" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Probabilistic multi-task learning for visual saliency estimation in video</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. comput. Vis</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="150" to="165" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Fast and efficient saliency detection using sparse sampling and kernel density estimation</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Tavakoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heikkilä</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 17th Scandin. Conf. Image Anal</title>
		<meeting>17th Scandin. Conf. Image Anal</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="666" to="675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Saliency estimation using a non-parametric low-level vision model</title>
		<author>
			<persName><forename type="first">N</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vanrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Otazu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Parraga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. Vis</title>
		<meeting>Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2011-06">Jun. 2011</date>
			<biblScope unit="page" from="433" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">The discriminant centersurround hypothesis for bottom-up saliency</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A feature integration theory of attention</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Treisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gelade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognit. Psychol</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="136" />
			<date type="published" when="1980-01">Jan. 1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">What attributes guide the deployment of visual attention and how do they do it?</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wolfe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Horowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Rev. Neurosci</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2004-06">Jun. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning a saliency map using fixated locations in natural scenes</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<ptr target="http://crcns.org/data-sets/eye/eye-1" />
		<title level="m">CRCNS -Collaborative Research in Computational Neuroscience -Data Sharing</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title/>
		<author>
			<persName><surname>Dataset</surname></persName>
		</author>
		<ptr target="http://thediemproject.wordpress.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Assessing the contribution of color in visual attention</title>
		<author>
			<persName><forename type="first">T</forename><surname>Jost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ouerhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wartburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mäuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Häugli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Understand</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107" to="123" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Visual search in noise: Revealing the influence of structural cues by gaze-contingent classification image analysis</title>
		<author>
			<persName><forename type="first">U</forename><surname>Rajashekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Cormack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="379" to="386" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Components of bottom-up gaze allocation in natural images</title>
		<author>
			<persName><forename type="first">R</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Res</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="2397" to="2416" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Modeling the role of salience in the allocation of overt visual attention</title>
		<author>
			<persName><forename type="first">D</forename><surname>Parkhurst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Res</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="123" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Signal Detection Theory and Psychophysics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Swets</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1966">1966</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Faces and text attract gaze independent of the task: Experimental data and computer model</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cerf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A Bayesian model for efficient visual search and recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Elazary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Res</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="1338" to="1352" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">The central fixation bias in scene viewing: Selecting an optimal viewing position independently of motor biases and image feature distributions</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Tatler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Quantifying center bias of observers in free viewing of dynamic natural scenes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Carmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">G M</forename><surname>Cameron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Spontaneous eye movements during visual imagery reflect the content of the visual scene</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">W</forename><surname>Stark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Cognit. Neurosci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="38" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Algorithms for defining visual regionsof-interest: Comparison with eye fixations</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Privitera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">W</forename><surname>Stark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="970" to="982" />
			<date type="published" when="2000-09">Sep. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Image signature: Highlighting sparse salient regions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="194" to="201" />
			<date type="published" when="2012-01">Jan. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Salience of feature contrast</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Nothdurft</surname></persName>
		</author>
		<editor>Neurobiology of Attention, L. Itti, G. Rees, and J. K. Tsotsos</editor>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Elsevier</publisher>
			<pubPlace>San Diego, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Biologically plausible saliency mechanisms improve feedforward object recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Res</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="2295" to="2307" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Look-ahead fixations: Anticipatory eye movements in natural tasks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hayhoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Experim. Brain Res</title>
		<imprint>
			<biblScope unit="volume">179</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="427" to="442" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Understanding and predicting where people look</title>
		<author>
			<persName><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dept. Electr. Eng. Comput. Sci</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>MIT, Cambrige</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Cost-sensitive learning of top-down modulation for attentional control</title>
		<author>
			<persName><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Ahmadabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Araabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Vis. Appl</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="76" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Fixations on low-resolution images</title>
		<author>
			<persName><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">An eye fixation database for saliency detection in images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Katti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kankanhalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="30" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">How close are we to understanding V1?</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Field</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1665" to="1699" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Natural image statistics and neural representation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Olshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Neurosci</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1193" to="1216" />
			<date type="published" when="2001-03">Mar. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">A saliency map in primary visual cortex</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends Cognit. Sci</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="16" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">On combining classifiers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hatef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P W</forename><surname>Duin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="226" to="239" />
			<date type="published" when="1998-03">Mar. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">High speed visual saliency computation on GPU</title>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Image Process</title>
		<meeting>Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="361" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">A high-speed multi-GPU implementation of bottom-up attention using CUDA</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pototschnig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kühnlenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Buss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Robot. Auotm</title>
		<meeting>Int. Conf. Robot. Auotm</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="41" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Simulating human saccadic scanpaths on natural images</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. Vis. Pattern Recognit</title>
		<meeting>Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2011-06">Jun. 2011</date>
			<biblScope unit="page" from="441" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Eye guidance in natural vision: Reinterpreting salience</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Tatler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Hayhoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Land</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Everyone knows what is interesting: Salient locations which should be fixated</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Masciocchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mihalas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parkhurst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Shifts in selective visual attention: Toward the underlying neural circuitry</title>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Neurobiol</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="219" to="227" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Context-free attentional operators: The generalized symmetry transform</title>
		<author>
			<persName><forename type="first">D</forename><surname>Reisfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wolfson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yeshurun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. Vis</title>
		<meeting>Int. Joint Conf. Vis</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="119" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Topdown control of visual attention in object detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Castelhano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Image Proces</title>
		<meeting>Int. Conf. Image es</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">SUNDAy: Saliency using natural statistics for dynamic analysis of scenes</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 31st Annu</title>
		<meeting>31st Annu</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2944" to="2949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Centersurround patterns emerge as optimal predictors for human saccade targets</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kienzle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Focus-of-attention from local color symmetries</title>
		<author>
			<persName><forename type="first">G</forename><surname>Heidemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="817" to="830" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">LabelMe: A database and web-based tool for image annotation</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="157" to="173" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">LabelMe video: Building a video database with human annotations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Natural scene statistics at the centre of gaze</title>
		<author>
			<persName><forename type="first">P</forename><surname>Reinagel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zador</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="341" to="350" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Gorillas in our midst: Sustained inattentional blindness for dynamic events</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Simons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Chabris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1059" to="1074" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
