<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Arbitrary-Oriented Scene Text Detection via Rotation Proposals</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jianqi</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>Li Wang, Hong Wang, Yingbin Zheng</addrLine>
									<settlement>Hao Ye, Xiangyang Xue</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weiyuan</forename><surname>Shao</surname></persName>
							<email>shaowy@sari.ac.cn</email>
							<affiliation key="aff0">
								<address>
									<addrLine>Li Wang, Hong Wang, Yingbin Zheng</addrLine>
									<settlement>Hao Ye, Xiangyang Xue</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">are with Shanghai Advanced Research Institute</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>201210</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yingbin</forename><forename type="middle">J</forename><surname>Zheng</surname></persName>
							<email>zhengy-b@sari.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">are with Shanghai Key Lab of Intelligent In-formation Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<addrLine>Shang-hai 200433</addrLine>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">are with Shanghai Advanced Research Institute</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>201210</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">L</forename><surname>Ma</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">are with Shanghai Key Lab of Intelligent In-formation Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<addrLine>Shang-hai 200433</addrLine>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">X</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">are with Shanghai Key Lab of Intelligent In-formation Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<addrLine>Shang-hai 200433</addrLine>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Xue</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">are with Shanghai Key Lab of Intelligent In-formation Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<addrLine>Shang-hai 200433</addrLine>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">H</forename><surname>Ye</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">are with Shanghai Advanced Research Institute</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>201210</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">H</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">are with Shanghai Advanced Research Institute</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>201210</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Arbitrary-Oriented Scene Text Detection via Rotation Proposals</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8E927B900EF0DA7168E88EDA9F705602</idno>
					<idno type="DOI">10.1109/TMM.2018.2818020</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2018.2818020, IEEE Transactions on Multimedia IEEE TRANSACTIONS ON MULTIMEDIA 1 This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2018.2818020, IEEE Transactions on Multimedia IEEE TRANSACTIONS ON MULTIMEDIA</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Scene text detection</term>
					<term>arbitrary oriented</term>
					<term>rotation proposals</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper introduces a novel rotation-based framework for arbitrary-oriented text detection in natural scene images. We present the Rotation Region Proposal Networks (RRPN), which are designed to generate inclined proposals with text orientation angle information. The angle information is then adapted for bounding box regression to make the proposals more accurately fit into the text region in terms of the orientation. The Rotation Region-of-Interest (RRoI) pooling layer is proposed to project arbitrary-oriented proposals to a feature map for a text region classifier. The whole framework is built upon a regionproposal-based architecture, which ensures the computational efficiency of the arbitrary-oriented text detection compared with previous text detection systems. We conduct experiments using the rotation-based framework on three real-world scene text detection datasets and demonstrate its superiority in terms of effectiveness and efficiency over previous approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Text detection aims to identify text regions of given images and is an important prerequisite for many multimedia tasks, such as visual classification <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, video analysis <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> and mobile applications <ref type="bibr" target="#b4">[5]</ref>. Although there are a few commercial optical character recognition (OCR) systems for documentary texts or internet content, the detection of text in a natural scene image is challenging due to complex situations such as uneven lighting, blurring, perspective distortion, orientation, etc.</p><p>In recent years, much attention has been paid to the text detection task (e.g., <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b15">[16]</ref>). Although these approaches have shown promising results, most of them rely on horizontal or nearly horizontal annotations and return the detection of horizontal regions. However, in real-world applications, a larger number of the text regions are not horizontal, and even applying non-horizontal aligned text lines as the axis-aligned proposals may not be accurate. Thus, the horizontal-specific methods cannot be widely applied in practice.</p><p>Recently, a few works have been proposed to address arbitrary-oriented text detection <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b18">[19]</ref>. In general, these methods mainly involve two steps, i.e., segmentation networks, such as the fully convolutional network (FCN), are used to generate text prediction maps, and geometric approaches are used for inclined proposals. However, prerequisite segmentation is usually time-consuming. In addition, some systems require several post-processing steps to generate the final text region proposals with the desired orientation and are thus not as efficient as those directly based on a detection network.</p><p>In this paper, we develop a rotation-based approach and an end-to-end text detection system for arbitrary-oriented text detection. Particularly, orientations are incorporated so that the detection system can generate proposals for arbitrary orientation. A comparison between the previous horizontalbased approach and ours is illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. We present the Rotation Region Proposal Networks (RRPN), which are designed to generate inclined proposals with text orientation angle information. The angle information is then adapted for bounding box regression to make the proposals more accurately fit the text region. The Rotation Region-of-Interest (RRoI) pooling layer is proposed to project arbitrary-oriented proposals to a feature map. Finally, a two-layer network is deployed to classify the regions as either text or background. The main contributions of this paper include the following:</p><p>• Different from previous segmentation-based frameworks, ours has the ability to predict the orientation of a text line using a region-proposal-based approach; thus, the proposals can better fit the text region, and the ranged text region can be easily rectified and is more convenient for text reading. New components, such as the RRoI pooling layer and learning of the rotated proposal, are incorporated into the region-proposal-based architecture <ref type="bibr" target="#b19">[20]</ref>, which ensures the computational efficiency of text detection compared with segmentation-based text detection systems. • We also propose novel strategies for the refinement of region proposals with arbitrary orientation to improve the performance of arbitrary-oriented text detection. • We apply our framework to three real-world text detection datasets, i.e., MSRA-TD500 <ref type="bibr" target="#b20">[21]</ref>, ICDAR2013 <ref type="bibr" target="#b21">[22]</ref> and ICDAR2015 <ref type="bibr" target="#b22">[23]</ref>, and find that it is more accurate and significantly efficient compared to previous approaches. The rest of this paper is organized as follows. Section II introduces the background of scene text detection and related work. Section III briefly reviews the horizontal region proposal approach. Section IV discusses our framework in detail. In Section V, we demonstrate the quantitative study on three datasets. We conclude our work in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>The reading of text in the wild has been studied over the last few decades; comprehensive surveys can be found in <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b26">[27]</ref>. Methods based on the sliding window, connected components and the bottom-up strategy are designed to handle horizontal-based text detection. Sliding window-based methods <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b27">[28]</ref>- <ref type="bibr" target="#b29">[30]</ref> tend to use a sliding window of a fixed size to slide the text area and find the region most likely to include text. To consider more precise styles of text, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b30">[31]</ref> apply multiple scales and ratios to the sliding window methods. However, the sliding window process leads to a large computational cost and inefficiency. Representative connected-component-based approaches such as the Stroke Width Transform (SWT) <ref type="bibr" target="#b31">[32]</ref> and Maximally Stable Extremal Regions (MSER) <ref type="bibr" target="#b32">[33]</ref> exhibited superior performances in the ICDAR 2011 <ref type="bibr" target="#b33">[34]</ref> and ICDAR 2013 <ref type="bibr" target="#b21">[22]</ref> robust text detection competitions. They mainly focus on the edge and pixel point of an image by detecting the character via edge detection or extreme region extraction and then combining the sub-MSER components into a word or text-line region. The capabilities of these methods are limited in some difficult situations involving multiple connected characters, segmented stroke characters and non-uniform illumination <ref type="bibr" target="#b34">[35]</ref>.</p><p>Scene text in the wild is usually aligned from any orientation in real-world applications, and approaches for arbitrary orientations are needed. For example, <ref type="bibr" target="#b35">[36]</ref> uses mutual magnitude symmetry and gradient vector symmetry to identify text pixel candidates regardless of the orientation, including curves from natural scene images, and <ref type="bibr" target="#b36">[37]</ref> designs a Canny text detector by taking the similarity between an image edge and text to detect text edge pixels and perform text localization. Recently, convolution-network-based approaches were proposed to perform text detection, e.g., Text-CNN <ref type="bibr" target="#b37">[38]</ref>, by first using an optimized MSER detector to find the approximate region of the text and then sending region features into a characterbased horizontal text CNN classifier to further recognize the character region. In addition, the orientation factor is adopted in the segmentation models developed by Yao et al. <ref type="bibr" target="#b17">[18]</ref>. Their model aims to predict more accurate orientations via an explicit manner of text segmentation and yields outstanding results on the ICDAR2013 <ref type="bibr" target="#b21">[22]</ref>, ICDAR2015 <ref type="bibr" target="#b22">[23]</ref> and MSRA-TD500 <ref type="bibr" target="#b20">[21]</ref> benchmarks.</p><p>A technique similar to text detection is generic object detection. The detection process can be made faster if the number of proposals is largely reduced. There is a wide variety of region proposal methods, such as Edge Boxes <ref type="bibr" target="#b38">[39]</ref>, Selective Search <ref type="bibr" target="#b39">[40]</ref>, and Region Proposal Networks (RPNs) <ref type="bibr" target="#b19">[20]</ref>. For example, Jaderberg et al. <ref type="bibr" target="#b40">[41]</ref> extends the region proposal method and applies the Edge Boxes method <ref type="bibr" target="#b38">[39]</ref> to perform text detection. Their text spotting system achieves outstanding results on several text detection benchmarks. The Connectionist Text Proposal Network (CTPN) <ref type="bibr" target="#b41">[42]</ref> is also a detection-based framework for scene text detection. It employs the image feature from the CNN network in LSTM to predict the text region and generate robust proposals.</p><p>This work is inspired by the RPN detection pipeline in regards to the dense-proposal based approach used for detection and RoI pooling operation used to further accelerate the detection pipeline. Detection pipelines based on RPN are widely used in various computer vision applications <ref type="bibr" target="#b42">[43]</ref>- <ref type="bibr" target="#b44">[45]</ref>. The idea is also similar to that of Spatial Transformer Networks (STN) <ref type="bibr" target="#b45">[46]</ref>, i.e., a neural network model can rectify an image by learning its affine transformation matrix. Here, we try to extend the model to multi-oriented text detection by injecting angle information. Perhaps the work most related to ours is <ref type="bibr" target="#b42">[43]</ref>, where the authors proposed an inception-RPN and made further text detection-specific optimizations to adapt the text detection. We incorporate the rotation factor into the region proposal network so that it is able to generate arbitrary-oriented proposals. We also extend the RoI pooling layer into the Rotation RoI (RRoI) pooling layer and apply angle regression in our framework to perform the rectification process and finally achieve outstanding results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. HORIZONTAL REGION PROPOSAL</head><p>We begin with a brief review of RPN <ref type="bibr" target="#b19">[20]</ref>. As mentioned in the previous section, an RPN is able to further accelerate the process of proposal generation. Part of VGG-16 <ref type="bibr" target="#b46">[47]</ref> is employed as sharable layers, and the horizontal region proposals are generated by sliding over the feature map of the last convolutional layer. The features extracted from each sliding window are fed into two sibling layers (a boxregression (reg) layer and a box-classification (cls) layer), with 4k (4 coordinates per proposal) outputs from the reg layer representing coordinates and 2k (2 scores per proposal) scores from the cls layer for k anchors of each sliding position.</p><p>To fit the objects to different sizes, the RPN uses two parameters to control the size and shape of anchors, i.e., scale and aspect ratio. The scale parameter determines the size of the anchor, and the aspect ratio controls the ratio of the width to the height for the anchor box. In <ref type="bibr" target="#b19">[20]</ref>, the authors set the scale as 8, 16 and 32 and the ratio as 1:1, 1:2 and 2:1 for a generic object detection task. This anchor selection strategy can cover the shapes of nearly all natural objects and keep the total number of proposals low. However, in the text detection task, especially for scene images, texts are usually presented in an unnatural shape with different orientations; axis-aligned proposals generated by RPN are not robust for scene text detection. To make a network more robust for text detection and maintain its efficiency, we think that it is necessary to build a detection framework, which encodes the rotation information with the region proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. APPROACH</head><p>We now elaborate the construction of the rotation-based framework; the architecture is illustrated in Figure <ref type="figure" target="#fig_1">2</ref>. We employ the convolutional layers of VGG-16 <ref type="bibr" target="#b46">[47]</ref> in the front of the framework, which are shared by two sibling branches, i.e., the RRPN and a clone of the feature map of the last convolutional layer. The RRPN generates arbitrary-oriented proposals for text instances and further performs bounding box regression for proposals to better fit the text instances. The sibling layers branching out from the RRPN are the classification layer (cls) and the regression layer (reg) of the RRPN. The outputs from these two layers are the scores from the cls and proposal information from the reg, and their losses are computed and summed to form a multitask loss. Then, the RRoI pooling layer acts as a max pooling layer by projecting arbitrary-oriented text proposals from the RRPN onto the feature map. Finally, a classifier formed by two fully connected layers is used, and the region with the RRoI features is classified as either text or background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Rotated Bounding Box Representation</head><p>In the training stage, the ground truth of a text region is represented as rotated bounding boxes with 5 tuples (x, y, h, w, θ). The coordinate (x, y) represents the geometric center of the bounding box. The height h is set as the short side of the bounding box, and the width w, as the long side. The orientation θ is the angle from the positive direction of the x-axis to the direction parallel to the long side of the rotated bounding box. Because of the special ability of scene text detection, the direction of reading and its opposite do not influence the detected region. Here, we simply maintain the orientation parameter θ such that it covers half the angular space. Suppose the orientation of a rotated box is θ; there exists one and only one integer k ensuring that θ+kπ is within</p><formula xml:id="formula_0">the interval [-π 4 , 3π<label>4</label></formula><p>), and we update θ + kπ as θ. There are three benefits of the tuple representation (x, y, h, w, θ). First, it is easy to calculate the angle difference between two different rotated boxes. Second, this is a rotation-friendly representation for the angle regression of each rotated bounding box. Third, compared with the traditional 8-point representation (x 1 , y 1 , x 2 , y 2 , x 3 , y 3 , x 4 , y 4 ) of a rotated bounding box, this representation can be used to easily calculate the new ground truth after we rotate a training image.</p><p>Suppose the size of a given image is I H × I W and the original text region is represented as (x, y, h, w, θ). If we rotate the image by an angle α ∈ [0, 2π) around its center, the center of the anchor can be calculated as</p><formula xml:id="formula_1">  x y 1   = T( I W 2 , I H 2 )R(α)T(- I W 2 , - I H 2 )   x y 1  <label>(1)</label></formula><p>where T and R are the translation matrix and rotation matrix, respectively,</p><formula xml:id="formula_2">T(δ x , δ y ) =   1 0 δ x 0 1 δ y 0 0 1   (2) R(α) =   cos α sin α 0 -sin α cos α 0 0 0 1  <label>(3)</label></formula><p>The width w and height h of the rotated bounding box do not change, and the orientation is</p><formula xml:id="formula_3">θ = θ + α + kπ (θ ∈ [-π 4 , 3π<label>4</label></formula><p>)). We employed this image rotation strategy for data augmentation during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Rotation Anchors</head><p>Traditional anchors, which use scale and aspect ratio parameters, are not sufficient for in-the-wild text detection. Therefore, we design the rotation anchors (R-anchors) by making several adjustments. First, an orientation parameter is added to control the orientation of a proposal. Six different orientations, i.e., -π 6 , 0, π 6 , π 3 , π 2 and 2π 3 , are used, which are trade-offs between orientation coverage and computational efficiency. Second, as text regions usually have special shapes, 1520-9210 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.  the aspect ratio is changed to 1:2, 1:5 and 1:8 to cover a wide range of text lines. In addition, the scales of 8, 16 and 32 are kept. The anchor strategy is summarized in Figure <ref type="figure" target="#fig_2">3</ref>. Following our data representation step, a proposal is generated from the R-anchors with 5 variables (x, y, h, w, θ). For each point on the feature map, 54 R-anchors (6 orientations, 3 aspect ratios, and 3 scales) are generated, as well as 270 outputs (5×54) for the reg layer and 108 score outputs (2×54) for the cls layer at each sliding position. Then, we slide the feature map with the RRPN and generate H ×W ×54 anchors in total for the feature map, with width W and height H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Learning of Rotated Proposal</head><p>As the R-anchors are generated, a sampling strategy for the R-anchors is needed to perform network learning. We first define the intersection-over-union (IoU) overlap as the overlap between the skew rectangles of the ground truth and R-anchor. Then, positive R-anchors feature the following: (i) the highest IoU overlap or an IoU larger than 0.7 with respect to the ground truth, and (ii) an intersection angle with respect to the ground truth of less than π 12 . Negative R-anchors are characterized by the following: (i) an IoU lower than 0.3, or (ii) an IoU larger than 0.7 but with an intersection angle with a ground truth larger than π 12 . Regions that are not selected as either positive or negative are not used during training.</p><p>Our loss function for the proposal takes the form of multitask loss <ref type="bibr" target="#b47">[48]</ref>, which is defined as:</p><formula xml:id="formula_4">L(p, l, v * , v) = L cls (p, l) + λlL reg (v * , v)<label>(4)</label></formula><p>where l is the indicator of the class label (l = 1 for text and l = 0 for background; no regression for the background), the parameter p = (p 0 , p 1 ) is the probability over classes computed by the softmax function</p><formula xml:id="formula_5">, v = (v x , v y , v h , v w , v θ )</formula><p>denotes the predicted tuple for the text label, and</p><formula xml:id="formula_6">v * = (v * x , v * y , v * h , v * w , v * θ )</formula><p>denotes the ground truth. The trade-off between two terms is controlled by the balancing parameter λ. We define the classification loss for class l as:</p><formula xml:id="formula_7">L cls (p, l) = -log p l (5)</formula><p>For the bounding box regression, the background RoIs are ignored, and we adopt smooth-L 1 loss for the text RoIs:</p><formula xml:id="formula_8">L reg (v * , v) = i∈{x,y,h,w,θ} smooth L1 (v * i -v i ) (6) smooth L1 (x) = 0.5x 2 if |x| &lt; 1 |x| -0.5 otherwise<label>(7)</label></formula><p>The scale-invariant parameterizations tuple v and v * are calculated as follows:</p><formula xml:id="formula_9">v x = x-xa wa , v y = y-ya ha v h = log h ha , v w = log w wa , v θ = θ θ a (8) v * x = x * -xa wa , v * y = y * -ya ha v * h = log h * ha , v * w = log w * wa , v * θ = θ * θ a<label>(9)</label></formula><p>where x, x a and x * are for the predicted box, anchor and ground truth box, respectively; the same is for y, h, w and θ.</p><formula xml:id="formula_10">The operation a b = a -b + kπ, where k ∈ Z to ensure that a b ∈ [-π 4 , 3π<label>4</label></formula><p>). As described in the previous section, we give R-anchors fixed orientations within the range [-π 4 , 3π 4 ), and each of the 6 orientations can fit the ground truth that has an intersection angle of less than π 12 . Thus, every R-anchor has its fitting range, which we call its fit domain. When an orientation of a ground truth box is in the fit domain of an R-anchor, this R-anchor is most likely to be a positive sample of the ground truth box. As a result, the fit domains of the 6 orientations divide the angle range [-π 4 , 3π 4 ) into 6 equal parts. Thus, a ground truth in any orientation can be fitted with an R-anchor of the appropriate fit domain. Figure <ref type="figure" target="#fig_3">4</ref> shows a comparison of the utility of the regression terms. We can observe that the orientations of the regions are similar in a neighborhood region.</p><p>To verify the ability of a network to learn the text region orientation, we visualize the intermediate results in Add the vertices of R j inside R i to P Set on the feature map represents the R-anchor with the highest response to the text instance. The orientation of the short line is the orientation of this R-anchor, while the length of the short line indicates the level of confidence. We can observe that the brighter field of the feature map focuses on the text region, while the other region becomes darker after 150,000 iterations. Moreover, the orientations of the regions become closer to the orientation of the text instance as the number of iterations increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Accurate Proposal Refinement</head><p>Skew IoU Computation The rotation proposals can be generated in any orientations. Thus, the IoU computation for axis-aligned proposals may lead to an inaccurate IoU of skew interactive proposals and further ruin the proposal learning. As shown in Algorithm 1, we design an implementation 1 for the skew IoU computation with consideration of the triangulation <ref type="bibr" target="#b48">[49]</ref>; Figure <ref type="figure" target="#fig_6">6</ref> shows the geometric principles. Given a set of skew rectangles R 1 , ..., R n , our goal is to compute the IoU for each pair R i , R j . The first step is to generate the intersection point set P Set of R i and R j (Lines 4-7 in Algorithm 1). The intersection points of the two rectangles and the vertices of one rectangle inside another rectangle are calculated and inserted into P Set. Then, the intersection area of P Set is computed (Lines 8-10 in Algorithm 1). The points in P Set are sorted into anticlockwise order according to their positions in the image, and a convex polygon is generated based on the ordered points. By the triangulation, we can obtain the triangle 1 Here, we use the GPU to accelerate the computation speed. </p><formula xml:id="formula_11">L rotate ← (L -x) cos θ + (T -y) sin θ + x 7: T rotate ← (T -y) cos θ -(L -x) sin θ + y 8:</formula><p>value ← 0</p><formula xml:id="formula_12">9:</formula><p>for k, l ∈ {0, ..., Grid h •SS -1 }×{0, ..., Grid w • SS -1 } do 10: set (e.g., {∆AIJ, ∆AJC, ∆ACK, ∆AKL} in Figure <ref type="figure" target="#fig_6">6</ref>(b)). The area of the polygon is the sum of the areas of the triangles. Finally, the IoU value is computed. Skew Non-Maximum Suppression (Skew-NMS) Traditional NMS takes only the IoU factor into consideration (e.g., the IoU threshold is 0.7), but it is insufficient for arbitraryoriented proposals. For instance, an anchor with a ratio of 1:8 and an angle difference of π 12 has an IoU of 0.31, which is less than 0.7; however, it may be regarded as a positive sample. Therefore, the Skew-NMS consists of 2 phases: (i) keep the max IoU for proposals with an IoU larger than 0.7; (ii) if all proposals have an IoU in the range [0.3, 0.7], keep the proposal with the minimum angle difference with respect to the ground truth (the angle difference should be less than π 12 ).</p><formula xml:id="formula_13">P x ← L rotate • SS + l cos θ + k sin θ + 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. RRoI Pooling Layer</head><p>As presented for the Fast-RCNN <ref type="bibr" target="#b47">[48]</ref>, the RoI pooling layer extracts a fixed-length feature vector from the feature map for each proposal. Each feature vector is fed into fully connected layers that finally branch into the sibling cls and reg layers, and the outputs are the predicted localization and class of an object in an input image. As the feature map of image needs to be computed only once per image rather than computed for every generated proposal, the object detection framework is accelerated. The RoI pooling layer uses max pooling to convert the feature inside any valid RoI into a small feature map with a fixed spatial extent of h r × w r , where h r and w r are layer hyperparameters that are independent of any RoI.</p><p>For the arbitrary-oriented text detection task, the traditional RoI pooling layer can only handle axis-aligned proposals. Thus, we present the rotation RoI (RRoI) pooling layer to adjust arbitrary-oriented proposals generated by RRPNs. We first set the RRoI layer hyperparameters to H r and W r for the RRoIs. The rotated proposal region can be divided into H r ×W r subregions of h Hr × w Wr size for a proposal with height h and width w (as shown in Figure <ref type="figure">7(a)</ref>). Each subregion have the same orientation as that of the proposal. Figure <ref type="figure">7</ref>(b) displays an example with 4 vertices (A, B, C, and D) of the subregion on the feature map. The 4 vertices are calculated using a similarity transformation (shift, scale, and rotate) and grouped to range the border of the subregion. Then, max pooling is performed in every subregion, and max-pooled values are saved in the matrix of each RRoI; the pseudo-code for RRoI pooling is shown in Algorithm 2. Compared with RoI pooling, RRoI pooling can pool any regions, with various angles, aspect ratios, or scales, into a fixed-size feature map. Finally, the proposals are transferred into RRoIs and sent to the classifiers to give the result, i.e., either text or background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>We evaluate the rotation-based framework on three popular text detection benchmarks: MSRA-TD500 <ref type="bibr" target="#b20">[21]</ref>, ICDAR2015 <ref type="bibr" target="#b22">[23]</ref> and ICDAR2013 <ref type="bibr" target="#b21">[22]</ref>. We follow the evaluation protocols of these benchmarks. The MSRA-TD500 dataset contains 300 training images and 200 testing images. Annotations of the images consist of both the position and orientation of each text instance, and the benchmark can be used to evaluate the text detection performance over the multi-oriented text instance. As the dataset of MSRA-TD500 is relatively smaller, its experiments are designed to exploit alternative settings. ICDAR2015 was released for the text localization of the incidental scene text challenge (Task 4.1) of the ICDAR 2015 Robust Reading Competition; it has 1,500 images in total. Different from previous ICDAR robust reading competitions, the text instance annotations have four vertices, which form an irregular quadrilateral bounding box with orientation information. We roughly generate an inclined rectangle to fit the quadrangle and its orientation. The ICDAR2013 dataset is from the ICDAR 2013 Robust Reading Competition. There are 229 natural images for training and 233 natural images for testing. All the text instances in this dataset are horizontally aligned, and we conduct experiments on this horizontal benchmark to determine the adaptability of our approach to specific orientations.</p><p>Implementation Details. Our network is initialized by pretraining a model for ImageNet classification <ref type="bibr" target="#b46">[47]</ref>. The weights of the network are updated by using a learning rate of 10 -3 for the first 200,000 iterations and 10 -4 for the next 100,000 iterations, with a weight decay of 5 × 10 -4 and a momentum of 0.9. We use the rotation of an image with a  random angle for the data augmentation, as their efficiency and measurements are improved when the augmentation is used (see Table <ref type="table" target="#tab_1">I</ref>). Due to our different R-anchor strategy, the total number of proposals for each image is nearly 6 times that of previous approaches such as the Faster-RCNN. To ensure efficient detection, we filter the R-anchors to remove those passing through the border of an image. Therefore, the speed of our system is similar to that of previous works in both the training and testing stages; a comparison with the state-ofthe-art approaches on MSRA-TD500 is presented in Table VI-Left. Table <ref type="table" target="#tab_1">II</ref> shows the runtime speed of our proposed framework and that of the original Faster-RCNN under the baseline settings and with border padding. We can observe that our approach takes two times as much as the Faster-RCNN approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Ablation Study</head><p>We first perform an ablation study on the smaller dataset, i.e., MSRA-TD500. The baseline system is trained using 300 images from the MSRA-TD500 training set; the input image is resized, with long side being 1,000 pixels. The evaluation result is a precision of 57.4%, recall of 54.5%, and F-measure of 55.9%, which reflects a much better performance compared to that of the original Faster-RCNN, the P, R and F of which were 38.7%, 30.4% and 34.0%, respectively. We make a comparison between rotation and horizontal region proposals, with some detection results illustrated in Figure <ref type="figure" target="#fig_9">8</ref>. The rotationbased approach is able to achieve accurate detection with less background area, which indicates the effectiveness of incorporating the rotation strategy.</p><p>Further analysis of the baseline results give us the following insights: (i) the difficult situations (e.g., blur and uneven lighting) in the image can hardly be detected; (ii) some text instances of extremely small size cannot be properly detected, resulting in a large recall loss regarding the performance; (iii) the extremely long text line, i.e., a height-width ratio of the bounding box larger than 1:10, cannot be correctly detected and is often split into several shorter proposals; hence, all the proposals become instances of false detection according to the evaluation of MSRA-TD500 (some failed detection instances are shown in Figure <ref type="figure" target="#fig_8">9</ref>). A few alternative strategies and settings from the baseline approach are tested; a summary is given in Table <ref type="table" target="#tab_3">III</ref>.  Context of the Text Region. Incorporating the contextual information has been proven to be useful for the general object detection task (e.g., <ref type="bibr" target="#b49">[50]</ref>), and we wonder whether it can promote a text detection system. We retain the center of the rotated bounding box and its orientation and enlarge both the width and height by a factor of 1.X in the data preprocessing step. During the testing phase, we divide the enlargement for every proposal. As shown in Table IV, all the experiments exhibit an obvious increase in the F-measure. The reason may be that as the bounding box becomes larger, more context information of the text instance is obtained, and the information regarding the orientation can be better captured. Thus, the orientation of the proposals can be more precisely predicted.</p><p>Training Dataset Enlargement. We adopt HUST-TR400 (contains 400 images, with text instances annotated using the  same parameters as for MSRA-TD500) <ref type="bibr" target="#b50">[51]</ref> as an additional dataset and form a training set of 700 images from both datasets. There is a significant improvement in all the measurements, and the F-measure is 60.8%, showing that the network is better trained and more robust when addressing noisy inputs. Border Padding. Using our filtering strategy, most of the boundary breaking R-anchors are eliminated. However, as the bounding box is rotated by certain angles, it may still exceed the image border, especially when we enlarge the text region for the contextual information. Thus, we set a border padding of 0.25 times each side to reserve more positive proposals. The experiment shows that adding border padding to an image improves the detection results. The border padding increases the amount of computation for our approach by approximately 5% (Table <ref type="table" target="#tab_1">II</ref>). In addition, combining border padding with enlargement of the text region and the training dataset yields a further improvement in the F-measure of 63.3%.</p><p>Scale Jittering. There are still a number of small text regions in both training datasets, and we would like to improve  the robustness of our system. One approach is to rescale the input images to a fixed larger size, and another is to perform scale jittering, i.e., rescaling with a long side of a random size before sending the image into the network. Figure <ref type="figure" target="#fig_0">10</ref> shows that the inputs with a long side of 1300 pixels outperform those with other fixed settings (precision: 71.1%, recall: 65.3%, and F-measure: 68.1%). When we apply scale jittering with a long side of a random size less than 1300 pixels, a better result is achieved compared to that of the experiment without jittering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance on Benchmarks</head><p>We use the best settings from the ablation study. The annotation of MSRA-TD500 prefers to label the region of a whole text line. Thus, the length of a text line does not have a fixed range, sometimes being very long.</p><p>the ratios for the R-anchor are fixed and may not be large enough to cover all the lengths, which leads to several short bounding box results for a single text region. To address this extremely long text line issue, a post-processing step is incorporated by linking multiple short detection segments into a finer proposal, as detailed in Algorithm 3. With this postprocessing, the performance is further boosted, with the Fmeasure being 74.2% and the time cost being only 0.3 s. We also conduct an experiment that incorporates the postprocessing as well as the strategies presented in Section V-A on the Faster-RCNN <ref type="bibr" target="#b19">[20]</ref>; the results are a precision of 42.7%, recall of 37.6%, and F-measure of 40.0%. The comparison verifies that using a rotation-based framework is necessary to achieve a more robust text detector. Note that the postprocessing is applied for the text line detection benchmark, i.e., MSRA-TD500, only and that we do not apply the algorithm Algorithm 3 Text-Linking Processing </p><formula xml:id="formula_14">P Set ← {P k |Valid[k]==1}</formula><p>to the ICDAR benchmarks. The results (RRPN) on MSRA-TD500 are shown in the left-most column of Table <ref type="table" target="#tab_6">VI</ref>.</p><p>ICDAR 2015. We train a baseline experiment on the ICDAR2015 benchmark using the same strategy used for MSRA-TD500. The evaluation result is a precision of 45.42%, recall of 72.56%, and F-measure of 55.87%. There are some differences between these two datasets. MSRA-TD500 tends to provide a text line ground truth, while ICDAR provides word-level annotations. Thus, the precision of our approach is lower than that of other methods that achieve the same Fmeasure. This issue may originate from three aspects. First, some of the incidental text regions are still too small for our detector to find. Second, there exist some small unreadable text instances (labeled '###') in the ICDAR2015 training set, which may lead to the false detection of text-like instances. Finally, our training set is insufficient (containing only 1,000 images) compared with those of previous approaches, such as <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b41">[42]</ref>. Some of the detection results obtained based on the ICDAR2015 training set are shown in Figure <ref type="figure" target="#fig_1">12</ref>.</p><p>To address the small text region issue, we create a larger scale by jittering the image patch with a long side of a random size less than 1,700 pixels before sending it into the network. We also check the impact of small unreadable text instances by randomly removing these instances from the training set. Figure <ref type="figure" target="#fig_2">13</ref> displays the curves of the measurements. The recall rate remains the same, i.e., approximately 72%-73%, unless we remove all the unreadable instances, while the precision significantly increases with the proportion. Therefore, we randomly remove 80% of the unreadable text instances in the training set and keep the whole testing set. To further improve our detection system, we incorporate a few text datasets for training, i.e., ICDAR2013 <ref type="bibr" target="#b21">[22]</ref>, ICDAR2003 <ref type="bibr" target="#b51">[52]</ref> and SVT [7]. As listed in Table <ref type="table" target="#tab_6">V</ref>, the training images for different approaches are of the same order of magnitude, and ours achieves better performance. ICDAR 2013. To examine the adaptability of our approach, we also conduct experiments on the horizontal-based ICDAR2013 benchmark. We reuse the model trained for ICDAR2015, and the 5-tuple rotation proposals are fit into horizontal-aligned rectangles. The result is a precision of recall of 71.89%, and F-measure of 80.02% under the ICDAR 2013 evaluation protocol. As shown in Table <ref type="table" target="#tab_6">VI</ref>, there is a 7% compared with the Faster-RCNN, which confirms the robustness of our detection framework with the rotation factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. More Results</head><p>The experimental results of our method compared with those of the state-of-the-art approaches are given in Table <ref type="table" target="#tab_6">VI</ref>. As the RRPN models are trained separately for MSRA-TD500 and ICDAR, we also train a unified model (RRPN*) trained on all of the training sets to consider the generalization issue. The precision-recall curves of RRPN and RRPN* on the three datasets are illustrated in Figure <ref type="figure" target="#fig_12">15</ref>. For the MSRA-TD500 dataset, the performance of our RRPN reaches the same magnitude of that of the state-of-the-art approaches, such   as <ref type="bibr" target="#b17">[18]</ref> and <ref type="bibr" target="#b16">[17]</ref>. When our system achieves text detection, it is more efficient than others, requiring a processing time of only 0.3 s per testing image. For the ICDAR benchmarks, the substantial performance gains over the published works confirm the effectiveness of using a rotation region proposal and rotation RoI for the text detection task. The recently developed DeepText <ref type="bibr" target="#b42">[43]</ref> is also a detection-based approach, but it is based on the Inception-RPN structure. Both our approach and DeepText are evaluated on the ICDAR2013 benchmark. The evaluation results in Table <ref type="table" target="#tab_6">VI</ref> and detection examples in Figure <ref type="figure" target="#fig_11">14</ref> demonstrate that our approach performs better in terms of different evaluation measurements. We believe that our rotation-based framework is also complementary to the Inception-RPN structure, as they both focus on different levels of information. Some detection results obtained on the benchmarks are illustrated in Figure <ref type="figure" target="#fig_0">11</ref>, and we have released the code and trained models for future research <ref type="foot" target="#foot_0">3</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>In this paper, we introduced a rotation-based detection framework for arbitrary-oriented text detection. Inclined rectangle proposals were generated with the text region orientation angle information from higher convolutional layers of network, resulting in the detection of text with multiple orientations. A novel RRoI pooling layer was also designed and adapted to the rotated RoIs. Experimental comparisons with the stateof-the-art approaches on MSRA-TD500, ICDAR2013 and ICDAR2015 showed the effectiveness and efficiency of our proposed RRPN and RRoI for the text detection task. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Rotation-based text detection pipeline.</figDesc><graphic coords="3,114.83,130.84,61.13,53.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Anchor strategy used in our framework.</figDesc><graphic coords="4,80.90,68.45,58.02,58.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Visualization of the impact on regression: input images (a); orientation and response of the anchors without regression term (b) and with regression (c). The orientation of the R-anchor is the direction of the white line at each point, with longer lines indicating a higher response score for text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Visualization of different multitask loss values. (a) Input images; (b) 0 iterations; (c) 15,000 iterations; (d) 150,000 iterations.</figDesc><graphic coords="4,311.98,56.07,251.05,102.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 . 4 : 5 :</head><label>545</label><figDesc>For an input image, the feature maps of RRPN training after different iterations are visualized. The short white line Algorithm 1 IoU computation 1: Input: Rectangles R 1 , R 2 , ..., R N 2: Output: IoU between rectangle pairs IoU 3: for each pair R i , R j (i &lt; j) do Point set P Set ← ∅ Add intersection points of R i and R j to P Set 6:Add the vertices of R i inside R j to P Set 7:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Examples of skew IoU computation: (a) 4 points, (b) 6 points, (c) 8 points (vertices of rectangle are in black, while intersection points are in green). Considering example (b), first add intersection points I, J, L, and K and inner vertices A and C to P Set, sort P Set to obtain convex polygon AIJCKL, and then calculate the intersection area Area(AIJCKL) = Area(∆AIJ) + Area(∆AJC)+ Area(∆ACK)+ Area(∆AKL).</figDesc><graphic coords="5,329.19,313.57,65.59,73.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>2 11 : 12 :Fig. 7 .</head><label>211127</label><figDesc>Fig. 7. RRoI pooling layer: (a) divide arbitrary-oriented proposal into subregions; (b) max pooling of a single region from an inclined proposal to a point in the RRoI.</figDesc><graphic coords="5,329.41,309.64,70.17,82.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Examples of failed detection on MSRA-TD500: (a) blur and uneven lighting situations; (b) extremely small text instances; (c) extremely long text line. The red boxes indicate instances of negative detection, i.e., either IoU&lt;0.5 or failed to detect; the yellow boxes indicate instances of positive detection with respect to the ground truth.</figDesc><graphic coords="7,48.96,243.40,251.05,249.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Comparison of rotation and horizontal region proposals. Left: original images; middle: text detection based on horizontal region proposal; right: text detection based on rotation region proposal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 .Fig. 13 .</head><label>1213</label><figDesc>Fig. 12. Text detection on ICDAR2015, with the model trained on the ICDAR2015 training set (including all text instances). The yellow areas denote instances of positive detection, with IoU &gt; 0.5, while red areas represent text regions that were not detected.</figDesc><graphic coords="9,74.07,56.07,200.84,136.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Detection results of the proposed approach and DeepText [43], downloaded from the ICDAR evaluation website 2 . The green and red boxes indicate instances of positive and false detection, respectively, the orange box refers to "one box covering multiple instances", and the blue box indicates multiple occurrences of detection for one instance.</figDesc><graphic coords="9,311.98,56.07,251.05,121.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Precision-recall curve of the benchmarks. The red and blue curves represent the results of RRPN and RRPN*, respectively.</figDesc><graphic coords="9,311.98,248.16,251.07,70.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Li</head><label></label><figDesc>Wang received the B.S. degree in software engineering from Hubei University, Wuhan, China, in 2010. She is currently pursuing the M.S. degree with School of Computer Science, Fudan University, Shanghai, China. Her research interests include computer vision and machine learning. Hong Wang received the B.S. degree from University of Science and Technology of China, Hefei, China, in 2005, and the Ph.D. degree from Univerisity of Chinese Academy of Sciences, Beijing, China, in 2011. He is currently an associate professor of Shanghai Advanced Research Institute, Chinese Academy of Science, Shanghai, China. His research interests include computer vision and signal processing. Yingbin Zheng received the B.S. degree and the Ph.D. degree in computer science from Fudan University, Shanghai, China, in 2008 and 2013, respectively. He was a research scientist with SAP Labs China from 2013 to 2015. Since 2015, he has been in Shanghai Advanced Research Institute, Chinese Academy of Sciences, Shanghai, China, where he is currently an associate professor. His research interests are in computer vision, especially in scene understanding and video data analysis. Xiangyang Xue received the B.S., M.S., and Ph.D. degrees in communication engineering from Xidian University, Xi'an, China, in 1989, 1992 and 1995, respectively. He is currently a professor of computer science with Fudan University, Shanghai, China. His research interests include computer vision and multimedia information processing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 2 RRoI pooling 1: Input: Proposal (x, y, h, w, θ), pooled size (H r ,W r ), input feature map InF eatM ap, spatial scale SS 2: Output: Output feature map OutF eatM ap 3: Grid w , Grid h ← w Wr , h Hr 4: for i, j ∈ {0, ..., H r -1} × {0, ..., W r -1} do</figDesc><table><row><cell>5:</cell><cell>L, T ← x -w 2 + jGrid w , y -h 2 + iGrid h</cell></row><row><cell>6:</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I EFFECT</head><label>I</label><figDesc>OF DATA AUGMENTATION.</figDesc><table><row><cell cols="2">Data Augmentation Precision</cell><cell>Recall</cell><cell>F-measure</cell></row><row><cell>Without rotation</cell><cell>44.5%</cell><cell>38.9%</cell><cell>41.5%</cell></row><row><cell>With rotation</cell><cell>68.4%</cell><cell>58.9%</cell><cell>63.3%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III EVALUATION</head><label>III</label><figDesc>ON MSRA-TD500 WITH DIFFERENT STRATEGIES AND SETTINGS. EXPERIMENTS ON FASTER-RCNN ARE BASED ON THE ORIGINAL SOURCE CODE. P, R AND F DENOTE THE PRECISION, RECALL,</figDesc><table><row><cell>a.</cell><cell>b.</cell><cell>c.</cell><cell>d.</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>∆F</cell></row><row><cell cols="4">Faster-RCNN [20]</cell><cell>38.7%</cell><cell>30.4%</cell><cell>34.0%</cell><cell>-</cell></row><row><cell>√ √ √</cell><cell cols="2">Baseline √ √ √ √ √ √</cell><cell>√</cell><cell>57.4% 65.6% 63.3% 63.1% 68.4% 71.8%</cell><cell>54.5% 58.4% 58.5% 55.4% 58.9% 67.0%</cell><cell>55.9% 61.8% 60.8% 59.0% 63.3% 69.3%</cell><cell>-5.9% 4.9% 3.1% 7.4% 13.4%</cell></row></table><note><p>AND F-MEASURE, RESPECTIVELY. ∆F IS THE IMPROVEMENT OF THE F-MEASURE OVER THE BASELINE. THE STRATEGIES INCLUDE THE FOLLOWING: A. CONTEXT OF THE TEXT REGION; B. TRAINING DATASET ENLARGEMENT; C. BORDER PADDING; AND D. SCALE JITTERING.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV EXPLOITATION</head><label>IV</label><figDesc>OF THE TEXT REGION CONTEXT BY ENLARGING THE TEXT BOUNDING BOX BY DIFFERENT FACTORS OF THE ORIGINAL SIZE.</figDesc><table><row><cell>Factor</cell><cell>Precision</cell><cell>Recall</cell><cell>F-measure</cell></row><row><cell>1.0</cell><cell>57.4%</cell><cell>54.5%</cell><cell>55.9%</cell></row><row><cell>1.2</cell><cell>59.3%</cell><cell>57.0%</cell><cell>58.1%</cell></row><row><cell>1.4</cell><cell>65.6%</cell><cell>58.4%</cell><cell>61.8%</cell></row><row><cell>1.6</cell><cell>63.8%</cell><cell>56.8%</cell><cell>60.1%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>1520-9210 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2018.2818020, IEEE Transactions on Multimedia</figDesc><table><row><cell cols="3">IEEE TRANSACTIONS ON MULTIMEDIA</cell><cell></cell><cell></cell><cell>8</cell></row><row><cell>80%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Precision</cell><cell>Recall</cell><cell>F-measure</cell></row><row><cell>70%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>60%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>50%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>40%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1000</cell><cell>1100</cell><cell>1200</cell><cell>1300</cell><cell>1400</cell><cell>SJ</cell></row><row><cell cols="6">Fig. 10. Evaluation on MSRA-TD500 for different input scales. 1X00</cell></row><row><cell cols="6">(X=0,1,2,3,4) denotes those inputs with a long side of 1X00 pixels, and SJ is</cell></row><row><cell cols="6">the result with scale jittering. The experiments are conducted using strategies</cell></row><row><cell cols="6">of context of the text region, training dataset enlargement, and border padding.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V TRAINING</head><label>V</label><figDesc>SET AND RESULTS FOR ICDAR2015. IXX INDICATE ICDAR20XX TRAINING SET, M500 INDICATES MSRA-TD500, SVT INDICATES SVT DATASET<ref type="bibr" target="#b6">[7]</ref>, AND CA INDICATES DATA COLLECTED BY THE AUTHORS OF<ref type="bibr" target="#b41">[42]</ref>.</figDesc><table><row><cell>Approach</cell><cell cols="2">RRPN</cell><cell>[18]</cell><cell>[42]</cell><cell>[17]</cell></row><row><cell># of images</cell><cell>2077</cell><cell>1229</cell><cell>1529</cell><cell>3000</cell><cell>1529</cell></row><row><cell>Training</cell><cell>I13 I15</cell><cell>I13 I15</cell><cell>I13 I15</cell><cell>I13</cell><cell>I13 I15</cell></row><row><cell>set</cell><cell>I03 SVT</cell><cell></cell><cell>M500</cell><cell>CA</cell><cell>M500</cell></row><row><cell>Precision</cell><cell>82.17%</cell><cell>79.41%</cell><cell>72.26%</cell><cell>74.22%</cell><cell>70.81%</cell></row><row><cell>Recall</cell><cell>73.23%</cell><cell>70.00%</cell><cell>58.69%</cell><cell>51.56%</cell><cell>43.09%</cell></row><row><cell>F-measure</cell><cell>77.44%</cell><cell>74.41%</cell><cell>64.77%</cell><cell>60.85%</cell><cell>53.58%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>1 :</head><label>1</label><figDesc>Input: Proposal P 1 , ..., P N (P k = x k , y k , h k , w k , θ k ) 2: Output: Merged Proposal Set P Set 3: Angle Threshold T ← 10 4: if N == 1 then CenterDistance Dis ← (x i -x j ) 2 + (y i -y j ) 2</figDesc><table><row><cell>5:</cell><cell>P Set ← {P 1 }</cell><cell></cell></row><row><cell cols="2">6: end if</cell><cell></cell></row><row><cell cols="2">7: for k ∈ {1, ..., N } do</cell><cell></cell></row><row><cell>8:</cell><cell>Valid[k] ← 1</cell><cell></cell></row><row><cell cols="2">9: end for</cell><cell></cell></row><row><cell cols="3">10: for each pair P i , P j (i &lt; j) do</cell></row><row><cell>11:</cell><cell cols="3">if Valid[i] == 0 or Valid[j] == 0 then</cell></row><row><cell>12:</cell><cell>Continue</cell><cell></cell></row><row><cell>13:</cell><cell>end if</cell><cell></cell></row><row><cell>14:</cell><cell>MeanWidth W idth ←</cell><cell>wi+wj 2</cell></row><row><cell>15:</cell><cell></cell><cell></cell></row><row><cell>16:</cell><cell cols="2">CenterGrad Grad ← |arctan(</cell><cell>yj -yi xj -xi )|</cell></row><row><cell>17: 18:</cell><cell cols="3">if Dis &lt; W idth and |Grad -θ i | &lt; T then P i ← xi+xj 2 , yi+yj 2 , hi+hj θi+θj 2 , w i + w j , 2</cell></row><row><cell>19:</cell><cell>Valid[j] ← 0</cell><cell></cell></row><row><cell>20:</cell><cell>end if</cell><cell></cell></row><row><cell cols="2">21: end for</cell><cell></cell></row><row><cell>22:</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>https://github.com/mjq11302010044/RRPN</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by the National Key R&amp;D Program of China (No. 2017YFC0803700), the National Natural Science Foundation of China (No. 61602459, No. 61572138 and No. U1611461), and the Science and Technology Commission of Shanghai Municipality (No. 17511101902 and No.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Words matter: Scene text for image classification and retrieval</title>
		<author>
			<persName><forename type="first">S</forename><surname>Karaoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1063" to="1076" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Integrating scene text and visual appearance for fine-grained image classification with convolutional neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04613</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Text detection, tracking and recognition in video: A comprehensive survey</title>
		<author>
			<persName><forename type="first">X.-C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Y</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2752" to="2773" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robustly extracting captions in videos based on stroke-like edges and spatio-temporal analysis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="482" to="489" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A low complexity sign detection and text localization method for mobile applications</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Bouman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Abdollahian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Boutin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Delp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="922" to="934" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detecting and reading text in natural scenes</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="366" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Word spotting in the wild</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="591" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A method for text localization and recognition in real-world images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Asian Conf. Comp. Vis</title>
		<meeting>Asian Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Photoocr: Reading text in uncontrolled conditions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="785" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep features for text spotting</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="512" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robust scene text detection with convolution neural network induced mser trees</title>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="497" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A new technique for multi-oriented scene text line detection and tracking in video</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shivakumara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1137" to="1152" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Text flow: A unified text detection system in natural scene images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Improving text proposals for scene images with fully convolutional networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bazazian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Bigorda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05089</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A convolutional neural network-based chinese text detection algorithm via text structure modeling</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="506" to="518" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Textboxes: A fast text detector with a single deep neural network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf</title>
		<meeting>AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multioriented text detection with fully convolutional networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Scene text detection via holistic, multi-channel prediction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.09002</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Accurate text localization in natural image with cascaded convolutional text network</title>
		<author>
			<persName><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.09423</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Detecting texts of arbitrary orientations in natural images</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1083" to="1090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Icdar 2013 robust reading competition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Document Anal</title>
		<meeting>Int. Conf. Document Anal</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Icdar 2015 competition on robust reading</title>
		<author>
			<persName><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nicolaou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Document Anal</title>
		<meeting>Int. Conf. Document Anal</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1156" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A survey of text detection and recognition in images and videos</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luettin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Text information extraction in images and video: a survey</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Text localization and recognition in images and video</title>
		<author>
			<persName><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Document Image Processing and Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="843" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Text detection and recognition in imagery: A survey</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1480" to="1500" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Texture-based approach for text detection in images using support vector machines and continuously adaptive mean shift algorithm</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1631" to="1639" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Detecting and reading text in natural scenes</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">366</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Scene text localization and recognition with oriented stroke detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">End-to-end text recognition with convolutional neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3304" to="3308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Detecting text in natural scenes with stroke width transform</title>
		<author>
			<persName><forename type="first">B</forename><surname>Epshtein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ofek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wexler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Robust wide-baseline stereo from maximally stable extremal regions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image &amp; Vision Computing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="761" to="767" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Icdar 2011 robust reading competition challenge 2: Reading text in scene images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shahab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Document Anal</title>
		<meeting>Int. Conf. Document Anal</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1491" to="1496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Character proposal network for robust text extraction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech, Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust., Speech, Signal ess</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2633" to="2637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A robust arbitrary text detection system for natural scene images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Risnumawan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shivakumara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="8027" to="8048" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Canny text detector: Fast and robust scene text localization algorithm</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Text-attentional convolutional neural network for scene text detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2529" to="2541" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E A V D</forename><surname>Sande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Reading text in the wild with convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Detecting text in natural image with connectionist text proposal network</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="56" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Deeptext: A unified framework for text proposal generation and text detection in natural images</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07314</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Face detection with the faster R-CNN</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03473</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Evolving boxes for fast vehicle detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Multimedia Expo</title>
		<meeting>IEEE Int. Conf. Multimedia Expo</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1135" to="1140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent</title>
		<meeting>Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A heuristic triangulation algorithm</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Plaisted</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Algorithms</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="405" to="437" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A unified framework for multioriented text detection and recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4737" to="4749" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">ICDAR 2003 robust reading competitions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Panaretos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Document Anal. Recog</title>
		<meeting>Int. Conf. Document Anal. Recog</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Robust text detection in natural scene images</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="970" to="983" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Orientation robust text line detection in natural images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2315" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Multi-orientation scene text detection with adaptive clustering</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Deep matching prior network: Toward tighter multioriented text detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01425</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Cascaded segmentation-detection networks for word-level text spotting</title>
		<author>
			<persName><forename type="first">S</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manduchi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00834</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Jianqi Ma received the B.S. degree in software engineering from School of Software Engineering</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06520</idno>
	</analytic>
	<monogr>
		<title level="m">His research interests include computer vision, signal processing, and machine learning</title>
		<meeting><address><addrLine>Shanghai, China; Shanghai</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2017. 2015</date>
		</imprint>
		<respStmt>
			<orgName>Fudan University ; Computer Science, Fudan University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>He is currently pursuing the M.S. degree with School of</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">His research interests are in computer vision, especially in scene text detection and face recognition. Hao Ye received the Ph.D. degree in computer science from Fudan University</title>
	</analytic>
	<monogr>
		<title level="m">His research interests include computer vision, multimedia information processing, and deep learning</title>
		<meeting><address><addrLine>Shanghai, China; Shanghai, China; Shanghai, China; Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016. 2016</date>
		</imprint>
		<respStmt>
			<orgName>Weiyuan Shao received the B.S degree from Shanghai Normal University</orgName>
		</respStmt>
	</monogr>
	<note>2012, and the M.S. degree in computer science from Fudan University</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
