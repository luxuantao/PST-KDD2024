<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TARNet: Task-Aware Reconstruction for Time-Series Transformer</title>
				<funder>
					<orgName type="full">Google</orgName>
				</funder>
				<funder ref="#_YUSRraC">
					<orgName type="full">Adobe</orgName>
				</funder>
				<funder>
					<orgName type="full">Hal?c?o?lu Data Science Institute</orgName>
				</funder>
				<funder>
					<orgName type="full">National Science Foundation Convergence Accelerator</orgName>
				</funder>
				<funder ref="#_W3ZyQzM">
					<orgName type="full">CONIX Research Center</orgName>
				</funder>
				<funder ref="#_QsSqYjC">
					<orgName type="full">DARPA</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ranak</forename><forename type="middle">Roy</forename><surname>Chowdhury</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiyuan</forename><surname>Zhang</surname></persName>
							<email>xiyuanzh@ucsd.edu</email>
						</author>
						<author>
							<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
							<email>jshang@eng.ucsd.edu</email>
						</author>
						<author>
							<persName><forename type="first">Rajesh</forename><forename type="middle">K</forename><surname>Gupta</surname></persName>
							<email>gupta@eng.ucsd.edu</email>
						</author>
						<author>
							<persName><forename type="first">Dezhi</forename><surname>Hong</surname></persName>
							<email>hondezhi@amazon.com</email>
						</author>
						<author>
							<persName><forename type="first">?</forename><forename type="middle">2022</forename><surname>Tarnet</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>San Diego La Jolla</addrLine>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>San Diego La Jolla</addrLine>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>San Diego La Jolla</addrLine>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>San Diego La Jolla</addrLine>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TARNet: Task-Aware Reconstruction for Time-Series Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3534678.3539329</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Time series</term>
					<term>self-supervision</term>
					<term>data reconstruction</term>
					<term>self-attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Time-series data contains temporal order information that can guide representation learning for predictive end tasks (e.g., classification, regression). Recently, there are some attempts to leverage such order information to first pre-train time-series models by reconstructing time-series values of randomly masked time segments, followed by an end-task fine-tuning on the same dataset, demonstrating improved end-task performance. However, this learning paradigm decouples data reconstruction from the end task. We argue that the representations learnt in this way are not informed by the end task and may, therefore, be sub-optimal for the end-task performance. In fact, the importance of different timestamps can vary significantly in different end tasks. We believe that representations learnt by reconstructing important timestamps would be a better strategy for improving end-task performance. In this work, we propose TARNet 1 , Task-Aware Reconstruction Network, a new model using Transformers to learn task-aware data reconstruction that augments end-task performance. Specifically, we design a datadriven masking strategy that uses self-attention score distribution from end-task training to sample timestamps deemed important by the end task. Then, we mask out data at those timestamps and reconstruct them, thereby making the reconstruction task-aware. This reconstruction task is trained alternately with the end task at every epoch, sharing parameters in a single model, allowing the representation learnt through reconstruction to improve end-task performance. Extensive experiments on tens of classification and regression datasets show that TARNet significantly outperforms state-of-the-art baseline models across all evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Mathematics of computing ? Time series analysis; ? Computing methodologies ? Supervised learning by classification; Supervised learning by regression.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Time-series data has domain-specific structural properties encoded in the temporal ordering of events. These intrinsic properties can provide a rich source of supervision besides target labels, which the state-of-the-art time-series models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b38">38]</ref> often neglect. Recently, time-series Transformer <ref type="bibr" target="#b36">[37]</ref> leveraged this unlabeled data to craft a reconstruction task that masks time-series values of randomly chosen time segments and reconstructs them. The pre-trained model is then fine-tuned on an end task, by reusing the same data samples along with their labels, leading to improved performance over exclusively doing supervised learning on the end task.</p><p>However, this data reconstruction task precedes fine-tuning as a decoupled step, which means the representation learnt during reconstruction is not informed about the end task. Hence, such learnt representation may not be fully leveraged to perform optimally on the end task.</p><p>Depending on the end task, different properties of the given data may be useful for different end tasks. For example, consider the following end tasks using the same data collected from sensors in a building: predict the level of energy consumption (high, medium, low) and the occupancy status (occupied or not) of a room based on outdoor temperature and humidity, and light intensity and ?? 2 readings from a room. Energy consumption prediction task may be highly correlated to times when temperature is high (air conditioning stays on) or light intensity is high (lights are switched ON) while occupancy status may correlate to timestamps when ?? 2 level is high. Hence, depending on the end task, certain timestamps in the data may be more important than others for that task.</p><p>Generic learnt representations typically result from decoupled data reconstruction and end tasks. To optimize the performance for an end task, we customize the learnt representation for the end task in TARNet. We test and validate the hypothesis that a representation learnt by reconstructing data from timestamps important to the end task will yield improved performance over reconstruction on random time segments. Therefore, we design a data reconstruction task which masks data from those important timestamps and reconstructs them. In the process, the model learns a task-specific representation, resulting in improved end task performance.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> shows TARNet's learning process. Using a transformer encoder <ref type="bibr" target="#b28">[29]</ref> as the backbone model, we train for the end task (Figure <ref type="figure" target="#fig_0">1(a)</ref>) and the data reconstruction task (Figure <ref type="figure" target="#fig_0">1</ref>(c)) alternately on the same model. In order to compute the timestamps to mask during data reconstruction, we design a data-driven masking strategy (Figure <ref type="figure" target="#fig_0">1(b)</ref>). It uses the self-attention score distribution generated by transformer encoder during the end task training and determines the set of timestamps to mask. Since the two tasks share parameters, the representation learnt during reconstruction can be effectively leveraged by the end task to improve performance.</p><p>We conducted experiments on 34 classification datasets from UEA Archive <ref type="bibr" target="#b0">[1]</ref>, UCI Machine Learning Repository <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15]</ref> and 6 regression datasets from Monash University, UEA, UCR Time Series Regression Archive <ref type="bibr" target="#b26">[27]</ref>. Time Series Transformer (TST) <ref type="bibr" target="#b36">[37]</ref>, the current state-of-the-art for time-series, achieved the best accuracy on 6 out of 10 datasets, when compared with 5 baselines. We compared TARNet with 14 state-of-the-art baselines and it performed the best on 17 out of 34 datasets, being 2.7% higher in average accuracy than TST, which now performs best on 7 datasets. Similarly, TST achieved the lowest error on 3 out of 6 datasets for regression when compared with 11 state-of-the-art baselines. TARNet achieved the lowest error on 3 and 2 nd lowest error on 2 datasets when compared with the same baselines, whereas TST now achieves the lowest error on 2 and 2 nd lowest error on 1 dataset. We conducted case studies to show how TARNet's data-driven masking strategy learns task-specific representations, consistent with domain characteristics, thereby boosting end-task performance.</p><p>In summary, our main contributions are: ? We propose TARNet to learn task-aware reconstruction from time-series data to augment end-task performance.</p><p>? We design a data-driven masking strategy to determine important timestamps to an end task and learn to reconstruct them. ? We evaluate TARNet on numerous real-world datasets to validate and quantify its efficacy compared with state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Non-Deep Learning Methods</head><p>ROCKET <ref type="bibr" target="#b4">[5]</ref> and MiniROCKET <ref type="bibr" target="#b5">[6]</ref> recently produced state-of-the art results for time-series. They learn features extracted by numerous and various random convolutional kernels. Other relevant directions include: (1) time series shapelet, (2) bag-of-patterns, and</p><p>(3) distance-based models. Baydogan et al. <ref type="bibr" target="#b2">[3]</ref> introduced Symbolic Representation to learn local relationships between different dimensions. Shapelets <ref type="bibr" target="#b32">[33]</ref> are short discriminative time series sub-sequences, e.g. dynamic shapelets <ref type="bibr" target="#b22">[23]</ref>, efficient shapelets <ref type="bibr" target="#b15">[16]</ref>. WEASEL-MUSE <ref type="bibr" target="#b23">[24]</ref> utilizes bag of SFA (Symbolic Fourier Approximation). Distance-based methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b30">31]</ref> use distance metric to measure similarity of a pair of time series. Among limitations of these approaches are that they incorporate expert insights, consist of large, heterogeneous ensembles of classifiers, scale poorly to long time-series, and many apply to only uni-variate time-series. TARNet can be applied to both uni-and multi-variate time-series, automatically extracts features, and handles long time-series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep Learning Methods</head><p>Using labeled data. Fawaz et al. <ref type="bibr" target="#b11">[12]</ref> summarize many neural networks-based methods for time-series. Most neural networksbased methods use some arrangement of LSTM, CNN or both <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b39">39]</ref>. Others use different components of neural models, e.g., learnable temporal pooling <ref type="bibr" target="#b18">[19]</ref>, correlative channel-aware learnable fusion <ref type="bibr" target="#b1">[2]</ref>, label-learning <ref type="bibr" target="#b21">[22]</ref>, attentional prototype network <ref type="bibr" target="#b38">[38]</ref>, and shapelet embedding <ref type="bibr" target="#b19">[20]</ref>. TARNet proposes a subsidiary data reconstruction technique that utilizes knowledge from the end task to learn a task-specific data representation. Sharing parameters of this reconstruction task with the end task in a single architecture allows the learnt representation to improve end task performance. Using both unlabeled and labeled data. Unsupervised representation learning for time-series uses triplet loss with negative sampling <ref type="bibr" target="#b13">[14]</ref>, hierarchical contrastive loss <ref type="bibr" target="#b35">[36]</ref>, temporal and contextual contrasting <ref type="bibr" target="#b10">[11]</ref>, local smoothness to define neighborhoods in time <ref type="bibr" target="#b27">[28]</ref>, and reprogramming acoustic models <ref type="bibr" target="#b31">[32]</ref>. TST <ref type="bibr" target="#b36">[37]</ref> first pre-trains a transformer model by an unsupervised objective; masks out time-series values at random time segments from data and reconstructs them. It then reuses the same training samples to fine-tune the model on an end task. This gave improved performance than using the data once to train a fully supervised model.</p><p>However, decoupling the data reconstruction from the end task makes the representation learnt during reconstruction uninformed about the end task. Depending on the end task, certain timestamps in time-series data may be more important than others <ref type="bibr" target="#b20">[21]</ref>, which the learnt representation ignores. TARNet aims to learn a taskaware data reconstruction by masking important timestamps with respect to the end task. Hence, the learnt representation is better suited for improving end task performance than the representation learnt from reconstructing randomly masked time segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">TARNET</head><p>In Figure <ref type="figure" target="#fig_0">1</ref>, we show a schematic diagram of TARNet common across all considered tasks. In this section, we first present the problem setting and base model architecture shared by the two tasks. Then, we explain the end-task? ?? ? (i.e., Figure <ref type="figure" target="#fig_0">1(a)</ref>) and taskaware reconstruction ? ? ?? (i.e, Figure <ref type="figure" target="#fig_0">1(c)</ref>). Finally, we present our data-driven masking strategy (i.e., Figure <ref type="figure" target="#fig_0">1(b)</ref>) that uses information from ? ?? ? to decide which timestamps to mask for ? ? ?? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Description and Notations</head><p>Each training sample X ? R ??? denotes a multivariate time-series of length ? and ? variables. Specifically, it comprises a sequence of ? ? -dimensional feature vectors, ? ? ? R ? : X ? R ??? . This formulation also covers the uni-variate case when ? = 1. All the training samples come together with a target label ?, which is an integer class id for a classification task or a real-valued number for a regression task. The full training dataset is labeled, i.e. we do not leverage any additional unlabeled data. Based on these training samples, we build a model to predict the label ? of unseen data X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Base Model</head><p>We opt to use Transformer Encoders <ref type="bibr" target="#b28">[29]</ref> as the backbone model, as we aim to develop a general framework to learn task-specific reconstruction that can be applied for a multitude of tasks. An architecture consisting of an encoder provides flexibility as it can not only handle tasks like classification, regression, imputation, but also handle generative tasks such as forecasting. One can plug in a task of interest by replacing the Fully Connected (FC) Layer in Figure <ref type="figure" target="#fig_0">1</ref>(a) by task-specific layers (e.g., decoder for forecasting).</p><p>The feature vectors ? ? are first mean-standardized per variable dimension. Then ? ? is linearly projected onto a ?-dimensional vector space, where ? is the dimension of the Transformer model sequence element representations (typically called embedding dimension):</p><formula xml:id="formula_0">? ? = W ? ? ? + ? ? ,<label>(1)</label></formula><p>where W ? ? R ??? , ? ? ? R ? are learnable parameters and ? ? ? R ? , ? = 1, 2, ..., ? are the model input vectors. The Transformer is a feed-forward architecture insensitive to the ordering of input. Therefore, we add positional encoding to these input vectors in order to make it aware of the sequential nature of the time series. The resultant vectors become the queries, keys and values of the self-attention layer in the encoder block. We pass data through several layers of such Transformer encoder blocks. Then, we pass the output values weighted by self-attention scores through a fully connected feed-forward network. We refer the reader to the original work <ref type="bibr" target="#b28">[29]</ref> for a detailed description of the Transformer model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">End Task (? ?? ? )</head><p>For clarity, we use classification and regression as example end tasks here. Please note that TARNet can be easily extended to other tasks such as anomaly detection and time-series forecasting, by tweaking the FC Layer in Figure <ref type="figure" target="#fig_0">1(a)</ref>.</p><p>We modify the base model architecture presented in Section 3.2 for regression and classification in the following way:</p><p>The data fed to ? ?? ? is not masked, as illustrated by the frozen Masking Layer in Figure <ref type="figure" target="#fig_0">1</ref>(a). The vector corresponding to the last timestamp from Transformer Encoders ? ? ? R ? is fed through 2 FC layers and ???? activation (represented as ? ), with parameters</p><formula xml:id="formula_1">W ?1 ? R ? ? ?? , ? ?1 ? R ? ? , W ?2 ? R ? ? ?? ? , ? ?2 ? R ? ? ,</formula><p>followed by the output layer with parameters</p><formula xml:id="formula_2">W ? ? ? R ??? ? , ? ? ? ? R ?</formula><p>, where ? ? is the feed-forward dimension of FC Layer for ? ?? ? and ? is the number of classes for classification or number of scalars to be estimated for regression (typically ? = 1):</p><formula xml:id="formula_3">? = W ? ? ? (W ?2 ? (W ?1 ? ? + ? ?1 ) + ? ?2 ) + ? ? ? .<label>(2)</label></formula><p>For classification, predictions ? are passed through a softmax to give probability distribution, ?, over ? classes. We use cross-entropy loss with categorical ground truth labels, L ?? ? = ? ?=1 ? ? ???(? ? ). For regression, we use squared error,</p><formula xml:id="formula_4">L ?? ? = ? ? -? ? 2 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Task-aware Reconstruction (? ? ?? )</head><p>Learning data representation through reconstruction has been explored in natural language processing <ref type="bibr" target="#b6">[7]</ref> and time-series <ref type="bibr" target="#b36">[37]</ref>. The goal of ? ? ?? , illustrated in Figure <ref type="figure" target="#fig_0">1</ref>(c), is to learn a data representation by reconstructing the input data X after it has been appropriately masked by the Data-driven Masking Strategy, ?.</p><p>The role of TARNet's masking strategy ?, elaborated in Section 3.5, is to generate a new binary training data mask ? ? R ? for each training sample at every epoch. It is a boolean array with ???? number of 1's, where ? is a hyper-parameter 0 &lt; ? &lt; 1, to select the timestamps to be masked from X for the reconstruction task. Let ? ? represent the value of ? at timestamp ?. If ? ? = 1 we mask ? ? , otherwise we do not. Masking a particular timestamp, ?, involves replacing the ? -dimensional feature vector ? ? with zeros. X passes through Transformer Encoder layers after being masked by ?. The final representation vectors Z ? R ??? is fed through 2 FC layers and ???? activation, with parameters</p><formula xml:id="formula_5">W ?3 ? R ? ? ?? , ? ?3 ? R ? ? , W ?4 ? R ? ? ?? ? , ? ?4 ? R ? ? ,</formula><p>followed by the output layer with parameters</p><formula xml:id="formula_6">W ? ? ? R ? ?? ? , ? ? ? ? R ?</formula><p>, where ? ? is the feed-forward dimension of FC Layer for ? ? ?? and ? is the number of variables:</p><formula xml:id="formula_7">X = W ? ? ? (W ?4 ? (W ?3 ? + ? ?3 ) + ? ?4 ) + ? ? ? .<label>(3)</label></formula><p>The label for this task is the raw input data X. To ensure accurate reconstruction, we calculate Mean Square Error (MSE) between the ground truth X and prediction X. We calculate the average MSE loss for masked and unmasked part of the data as follows:</p><formula xml:id="formula_8">L ?????? = 1 ? ? ? =1 ? ? ? ?? ? =1 ? ? ? x? -x ? ? 2 2 ,<label>(4)</label></formula><formula xml:id="formula_9">L ???????? = 1 ? (? -? ? =1 ? ? ) ? ?? ? =1 (1 -? ? ) ? x? -x ? ? 2 2 .<label>(5)</label></formula><p>Unlike TST, which only considers MSE loss for reconstructing the masked portion of the data, L ?????? , we include loss incurred for replicating the unmasked, observed portion of the input data, L ???????? , as well. Time-series data is auto-regressive with strong correlation across time. Therefore, the ability to reconstruct the masked data at a given timestamp depends on how effectively the model learns to reconstruct the unmasked data and use that as context to infer the masked data. Including the loss for the unmasked data ensures its accurate reconstruction.</p><p>The combined reconstruction loss L ? ?? is a weighted sum of L ?????? and L ???????? , given by</p><formula xml:id="formula_10">L ? ?? = ? L ?????? + (1 -?) L ???????? ,<label>(6)</label></formula><p>where ? is a hyper-parameter 0 &lt; ? &lt; 1 that controls the relative weights between the two losses. It is advisable to keep ? &gt; 0.5 because the masked timestamps are more important for the end task than the unmasked ones. With L ?? ? as the end task loss, the total loss becomes</p><formula xml:id="formula_11">L ? ???? = ?L ? ?? + (1 -?)L ?? ? ,<label>(7)</label></formula><p>where ? is a hyper-parameter (0 &lt; ? &lt; 1) that controls the relative weights between the two task losses. We train ? ?? ? and ? ? ?? end-to-end alternately at every epoch, until convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Data-driven Masking Strategy (?)</head><p>Data reconstruction in Time-series Transformer <ref type="bibr" target="#b36">[37]</ref> involves masking segment of time-series data at randomly chosen timestamps and reconstructing them. However, different timestamps in the data may have different levels of importance to the end task. Therefore, we eschew random reconstruction of data in favor of a strategy that uses end task characteristics. Specifically, we identify timestamps that the end task deemed important during learning. We will then mask ? ? from X corresponding to those timestamps and reconstruct them during ? ? ?? . We hypothesize that reconstructing data at timestamps identified to be important by the end task will generate a data representation that benefits the end task. This is in contrast to a random masking based data reconstruction, which does not consider any such information. X, ?, A = ?????.train(X, ?) # A ? Self-Attention Scores 7:</p><p>Compute L ? ?? ( X, X, ?) and L ?? ? ( ?, ?)</p><p>8:</p><formula xml:id="formula_12">L ? ???? = ?L ? ?? + (1 -?)L ?? ? 9:</formula><p>? = add_and_normalize(A) 10: end while 11: return ????? To define the notion of an "important" timestamp, we use selfattention weights generated by Transformer Encoder in the forward pass of ? ?? ? . Attention weights indicate how much weight should be assigned to each ? ? to compute representation for a given ? ? . We compute aggregate attention map A ? R ??? by summing the attention maps generated by each layer of Transformer Encoder. Let A ?? be the attention weight assigned to ? ? during update of ? ? , where ? = ? = 1, 2, ..., ?, and ? ?=1 A ?? = 1 for all ?. Therefore, the update to ? ? is a weighted sum of ? 1,2,...,? , where the weights are A ?,?=1,2,...? . We compute ? ? R ? , where ? ? = ? ?=1 A ?? ? ?=1 ? ?=1 A ?? for ? = 1, 2, ..., ?. ? ? represents the normalized aggregate attention weight of timestamp ? to the computation of ? 1 , ? 2 , ..., ? ? . We define the importance of each timestamp by its magnitude in ?, i.e. the higher ? ? is, the more important timestamp ? is for ? ?? ? .</p><p>We then select the timestamps corresponding to the top ???? values in ? and mask them from X for reconstruction. Since the same training data is fed at every epoch, the set of important timestamps computed from a given sample will not vary across epochs. Hence, the model may memorize reconstructing a few selected timestamps from the sample, leading to overfitting. Considering the heterogeneity in time-series data due to irregular sampling frequency or uncertainty about feature availability, it is probable that real-world data may have a different set of important timestamps compared to those seen in training data. Therefore, not exploring enough timestamps to approximate the training data distribution may lead to poor generalization on the real-world data.</p><p>Hence, we ensure that for every sample, at each epoch the model explores a random set of timestamps among those that are important. Therefore, we introduce an attention regularization parameter, ?, where ? &gt; ? and 0 &lt; ? &lt; 1. We, therefore, compute set ? ? to choose the top ???? values in ?. Then we randomly sample ???? timestamps without replacement from ? ? to generate the training data mask ?. ? ? = 1 if ? is sampled from ? ? , otherwise 0.</p><p>Although we still choose an important set of timestamps to mask, the use of randomization through sampling ensures that the model does not always mask the same set of timestamps for a sample throughout its entire training regime. This gives the model a more versatile representation of the underlying data distribution, yet, one that is important for the end task. This data-driven masking strategy makes the model learn task-specific data representation by reconstructing data at those timestamps deemed important by the end task. Algorithm 1 outlines the training procedure of TARNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We present the datasets, baselines, training settings, followed by the evaluation metrics. We then show and analyze classification and regression results of TARNet. We also conduct an ablation study, few-shot training experiments and case studies to justify TARNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>We use benchmark time-series datasets with detailed information available in UEA Archive <ref type="bibr" target="#b0">[1]</ref>, UCI Machine Learning Repository <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15]</ref>, and Monash University, UEA, UCR Time Series Regression Archive <ref type="bibr" target="#b26">[27]</ref>. These datasets represent an assortment of domains (Motion, Audio, EEG, HAR), sensor type, and sampling frequency. The number of training data points varies from 15 to over one million, the length of the time series, ?, varies between 8 to 17, 984, the number of features, ? , varies between 1 to 1, 345, and the number of target classes, ?, varies between 2 to 39. ? = 1 covers the uni-variate case. ? &gt; 1 refers to the multi-variate case.</p><p>We compare TARNet with statistical [1, 4-6, 9, 24-26] and deep learning <ref type="bibr">[11-14, 18, 20, 28, 30, 35, 37, 38]</ref> baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Statistical Baselines.</head><p>(1) Distance-based method <ref type="bibr" target="#b0">[1]</ref>. Euclidean Distance (ED), dimension independent dynamic time warping (DTWI), and dimensiondependent dynamic time warping (DTWD) <ref type="bibr" target="#b24">[25]</ref>. (2) SVR: <ref type="bibr" target="#b8">[9]</ref> Support Vector Regression.</p><p>(3) Tree-based methods: Random Forest <ref type="bibr" target="#b25">[26]</ref> and XGBoost <ref type="bibr" target="#b3">[4]</ref>. (4) WEASEL-MUSE <ref type="bibr" target="#b23">[24]</ref> is a bag-of-pattern based sliding-window approach with statistical feature extraction and filtration. (5) Rocket <ref type="bibr" target="#b4">[5]</ref> convolves time series with random convolutional kernels and applies global max pooling to extract features. (6) MiniRocket <ref type="bibr" target="#b5">[6]</ref> upgrades Rocket by speeding it up, using a small, fixed set of kernels, and is almost entirely deterministic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Deep Learning Baselines.</head><p>(1) FCN <ref type="bibr" target="#b29">[30]</ref> Fully Convolutional Networks. Replaces traditional final FC layer with a Global Average Pooling (GAP) layer. (2) MLSTM-FCNs <ref type="bibr" target="#b17">[18]</ref> expands LSTM-FCN and Attention LSTM-FCN by adding squeeze-and-excitation blocks. (3) Negative samples (NS) <ref type="bibr" target="#b13">[14]</ref> generates negative samples and trains a dilated causal convolution encoder with triplet loss. (4) TapNet <ref type="bibr" target="#b38">[38]</ref> designs random group permutation method with multi-layer convolutional and attentional prototype network. (5) ShapeNet <ref type="bibr" target="#b19">[20]</ref> extends shapelet <ref type="bibr" target="#b32">[33]</ref> for multivariate timeseries. Learns shared embedding space across different shapelet candidates, trains a dilated causal CNN, followed by an SVM. (6) Time Series Transformer (TST) <ref type="bibr" target="#b36">[37]</ref> pre-trains Transformer Encoder by masking random time segments and reconstructing them. Reuses the same data to fine-tune the model. (7) TS2Vec <ref type="bibr" target="#b34">[35]</ref> performs hierarchical contrastive learning over augmented context views. Builds representation of an arbitrary sub-sequence by aggregating representations of timestamps.</p><p>(8) TNC <ref type="bibr" target="#b27">[28]</ref> leverages local smoothness of a signal to define temporal neighborhoods and learns generalizable representations. (9) TS-TCC <ref type="bibr" target="#b10">[11]</ref> encourages consistency of different data augmentations to learn transformation-invariant representations. (10) ResNet <ref type="bibr" target="#b11">[12]</ref> uses convolutional followed by a GAP layer. Adds shortcut residual connection between convolutional layers. (11) Inception <ref type="bibr" target="#b12">[13]</ref> is an ensemble of deep CNN models, inspired by the Inception-v4 architecture. We normalize the datasets for each of our experiments. For datasets on which the accuracies of the baselines have been reported, we present the same results according to their papers. For the remaining datasets, we train all the baseline models with sufficient hyper-parameter tuning to produce results. Since our benchmark datasets are widely heterogeneous in terms of number of data points, features, sequence length, and sampling frequency, as well as the physical nature of the data itself, we obtain better performance via cursory tuning of architecture-specific hyper-parameters. To select hyper-parameters, we do a random 80%-20% split of the training set and used the 20% as a validation set for hyper-parameter tuning. After fixing the hyper-parameters, we train the model again using the entire training set and save the model with the lowest training loss. We use the saved model to evaluate on the official test set and report our evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metrics</head><p>We use accuracy and Root Mean Squared Error (RMSE) error as our performance metric for classification and regression, respectively. Considering the large number of datasets and baselines used, it is highly unlikely for a single model to outperform all other methods on every datasets. Therefore, we also present some summary statistics to present a holistic and a fairer comparison of the methods. The evaluation metrics are as follows:</p><p>? Ours 1-to-1 Wins/Draws/Losses: Number of datasets for which TARNet's accuracy or RMSE is better/same/worse than the corresponding baselines, respectively. Higher wins, lower draws and lower losses are better. This is useful to draw a one-on-one comparison between TARNet and a given model. </p><formula xml:id="formula_13">? ? = 1 ? ? ?? ?=1 ?(?, ?) -R? R? , R? = 1 ? ? ?? ?=1 ?(?, ?),<label>(8)</label></formula><p>where ?(?, ?) is the RMSE of model ? on dataset ? and ? is the number of models. ? ? = -0.3 means that the model on average attains 30% lower RMSE on a dataset than the average model performance on the same dataset. Lower value is better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Classification</head><p>Table <ref type="table" target="#tab_2">1</ref> shows the accuracy of the models. According to Table <ref type="table" target="#tab_2">1</ref>, the overall accuracy of TARNet is the best among all compared methods. TARNet performs the best on 17 datasets, as compared to 7 and 6 by the next best baselines TST <ref type="bibr" target="#b36">[37]</ref> and Rocket <ref type="bibr" target="#b4">[5]</ref>, respectively. TARNet achieves a 2.7-point higher average accuracy across all datasets over TST. The closest competitors of TARNet are TST and Rocket, but TARNet still outperforms them on 20 datasets while losing on 14 and 12, respectively. TARNet ranks 1 st (lowest "Mean Rank") on average, having a 0.71-point lower average than the 2 nd best MiniRocket. Rocket and ShapeNet ranks 3 rd and 4 th with a 1.18 and 1.47-point higher average, respectively, than TARNet. The large number of datasets and baselines used makes it highly unlikely for a single model to outperform all other methods on every dataset. For example, TST had the 2 nd best "Total best Accuracy" <ref type="bibr" target="#b6">(7)</ref> and "Average Accuracy" (0.745), but it ranks 5 th across all models, with a 1.74-point higher average than TARNet. This means that for the datasets where TST under-performs, its performance metrics are significantly below those of other baselines, pushing down its "Mean Rank. " However, TARNet performs well across all evaluation metrics. Not only does it have the highest "Total best Accuracy" <ref type="bibr" target="#b16">(17)</ref> and "Average Accuracy" (0.772), but it also ranks 1 st , meaning that for the datasets where TARNet under-performs, it still generates better performance than most of its baselines, pushing up its "Mean Rank". Moreover, from Table <ref type="table" target="#tab_2">1</ref>, we find that on datasets where TARNet under-performs, the winning methods are in fact different. Considering that no single baseline is consistently better than TARNet, as illustrated by the baselines' low number of best accuracies, low average accuracies and high mean rank, we argue that TARNet is the new benchmark for time-series classification.</p><p>Moreover, TARNet achieves the best accuracy across a diverse set of data characteristics. For example, TARNet has the best accuracy for Atrial Fibrillation and Occupancy with 15 and 1.2?+ training data points, respectively, for RacketSports and Cricket with sequence length of 30 and 1197, respectively, for Epilepsy and FaceDetection with 3 and 44 features, respectively and for MotorImagery and OpportunityGestures with 2 and 17 classes, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Regression</head><p>We compare regression results against all the baselines reported by TST <ref type="bibr" target="#b36">[37]</ref>. Table <ref type="table" target="#tab_3">2</ref> shows the Root Mean Squared Error of the models. TARNet ranks 1 st on three and 2 nd on two datasets, which Although TST <ref type="bibr" target="#b36">[37]</ref> pretrains and finetunes on the same dataset, the data reconstruction and the supervised end-task runs sequentially, slowing down training time. However, TARNet trains both tasks, ? ? ?? and ? ?? ? parallely. Hence, not only TARNet outperforms TST on the end-task but it also trains faster than TST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head><p>We justify our design choices of ? through ablation study results on classification and regression tasks in Table <ref type="table">3</ref>. TARNet-Random uses the same architecture as TARNet but instead masks timestamps randomly and reconstructs them, giving substandard performance. TARNet-Top ? selects timestamps corresponding to the top ???? values in ? and masks them from X for reconstruction. This does not lead to a clear improvement which may be attributed to overfitting, as explained in Section 3.5. This prompts sampling to TARNet-Top ? while selecting the timestamps to mask from the set of important timestamps, resulting in TARNet. To ensure a fair comparison, we maintain the same set of hyper-parameters across all ablation models for each dataset. Table <ref type="table">3</ref> shows that TARNet has the highest average accuracy, most number of datasets with highest accuracy and lowest loss, and lowest mean rank. TARNet combines ideas from both TARNet-Random and TARNet-Top ? to counter their individual drawbacks and yields better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Can ? ? ?? compensate for limited labeled training data?</head><p>We study whether under data-deficient environments TARNet can make better use of limited data compared to baselines. This will illustrate if the knowledge gained during reconstruction, ? ? ?? , can compensate for a lack of labeled data to train the end task, ? ?? ? . We choose occupancy and human gestures datasets for classification. As Figure <ref type="figure" target="#fig_2">2</ref>  Both TST and TARNet can leverage additional information learnt though reconstruction to compensate for the lack of labeled data, resulting in better performance over other baselines. However, making the reconstruction task-aware improves the performance of TARNet over TST. For example, in Occupancy, TARNet achieves the same performance with 50% training data, which TST and ShapeNet require 75% training data to achieve. Similarly, for LiveFuelMoisture  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Explaining Masking Strategy, ?</head><p>We provide two real-world case studies to show why a task-aware reconstruction learnt through a data-driven masking strategy, ?, is superior to a reconstruction learnt through random masking. For qualitative analysis, we show normalized aggregate attention, ?, computed from attention maps of Transformer during ? ?? ? .</p><p>Case Study I: Epilepsy. Figure <ref type="figure" target="#fig_3">3</ref> shows a time-series plot of an accelerometer data from a person conducting the activity of "Sawing" (classification label). Following the time-series plot are the ? scores, as discovered by TARNet and TARNet-Random. Sawing involves strong periodic motion of the hand as the time-series plot. Figure <ref type="figure" target="#fig_3">3</ref> shows that a random-masking based autoregressive task (TARNet-Random) could not capture this inherent TARNet discriminates between the "unimportant" and "important" timestamps for classification by assigning higher average attention per timestamp for times greater than 20 than to those before 20. However, TARNet-Random fails to infer such task-specific domain properties from the data and assigns attention weights randomly across time. Hence, TARNet-Random achieves an accuracy of 0.607, whereas TARNet achieves 0.641. The two case studies substantiate why using ? to decide which timestamps to mask during reconstruction is important. Representations learnt through reconstructing "important" timestamps reflect some domain-specific inherent properties in the data, as illustrated by how the attention scores have been assigned. Such domain properties are relevant to the end task and can clearly lead to performance improvement on the end task, as illustrated in Table <ref type="table" target="#tab_2">1</ref> and 2. We also highlight that the utility of self-attention goes beyond computing internal data representation of a model to improve performance <ref type="bibr" target="#b28">[29]</ref> or providing meaningful explanations <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b33">34]</ref>. In addition, self-attention can also be used to integrate simple and intuitive data-driven techniques into deep learning frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION AND CONCLUSIONS</head><p>We have proposed a task-aware reconstruction technique to improve end-task performance for a time series. In particular, we use attention score distribution to identify timestamps important to an end task. We then sample from those important timestamps and mask them from the data for reconstruction, making the reconstruction end task-aware. These tasks are trained alternately, sharing parameters in the same model, thereby enabling the representation learnt through reconstruction to improve end-task performance. Experimental results show that TARNet outperforms the state-ofthe-art baselines for both classification and regression tasks. The ablation study highlights the essence of our design choices for the data masking technique, and the case study observations show how TARNet captures the intrinsic task-specific properties of data.</p><p>Additional unlabeled data can help to improve TARNet. Although the data reconstruction task is fully unsupervised, it is driven by the end task that requires labeled data. In the future, we wish to explore such task-aware representations under data shift problem and in the presence of outliers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>?Figure 1 :</head><label>1</label><figDesc>Figure 1: TARNet Overview: (a) Task of interest / End Task, ? ?? ? : Data is mean-standardized, then passed through an Embedding and a Positional Encoding layer (not shown for simplicity), followed by the N-layer Transformer Encoders and Fully Connected (FC) Layer; (b) Data-driven Masking Strategy, ?: For every time-series data, we collect attention maps generated by Transformer Encoders in ? ?? ? and then compute the set of important timestamps to be masked in task-aware reconstruction; and (c) Task-aware Reconstruction, ? ? ?? : Input data are masked at timestamps computed by ? and reconstructed. Transformer Encoder parameters are shared between ? ?? ? and ? ? ?? , but the FC layers are different (highlighted by different colors).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) and (b) show, the accuracy of all models increases as the amount of training data increases. Particularly, TARNet has a steep rise for both datasets, signifying that the greatest improvement occurs with low quantity of training data. Similarly, we choose LiveFuelMoisture and IEEEPPG datasets for regression. As Figure 2 (c) and (d) show, the RMSE Loss of all models decreases as the amount of training data increases. Even with just 25% training data, TARNet achieves significantly lower loss than any baselines. It achieves superior performance over all baselines at all quantities of training data, for both classification and regression.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) and (b) show classification accuracy, and (c) and (d) show regression RMSE Loss against % of training data.</figDesc><graphic url="image-10.png" coords="8,61.72,215.57,107.72,101.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: ? plotted as heatmap for Epilepsy.</figDesc><graphic url="image-12.png" coords="8,53.80,373.13,240.25,130.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: ? plotted as heatmap for Face Detection.</figDesc><graphic url="image-13.png" coords="8,317.96,83.68,240.25,130.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>?</head><label></label><figDesc>Mean Rank: Average rank of a model across all datasets. Lowest rank is assigned to model with highest accuracy for classification and lowest RMSE for regression. Lower mean rank is better.? Avg.Rel.Diff.Mean<ref type="bibr" target="#b36">[37]</ref>: We report the "average relative difference from mean" metric ? ? for each model ?, over ? datasets:</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Accuracy of TARNet and baselines on classification datasets from UEA Archive and UCI Machine Learning Repository. We mark the best and second best values. Baselines are presented in ascending order (left to right) by average accuracy. A dash indicates that the corresponding method failed to run on this dataset. Higher Total best accuracy, average accuracy, and Ours 1-to-1 Wins is better. Lower Ours 1-to-1 Draws, Ours 1-to-1 Losses, and Mean Rank is better.</figDesc><table><row><cell cols="15">Dataset ED MLSTM-FCNs DTWD TapNet DTWI NS WEASEL-MUSE TS-TCC TNC ShapeNet TS2Vec Rocket MiniRocket TST TARNet</cell></row><row><cell>ArticularyWordRecognition 0.970</cell><cell>0.973</cell><cell cols="4">0.987 0.987 0.980 0.987</cell><cell>0.990</cell><cell cols="3">0.953 0.973 0.987</cell><cell cols="2">0.987 0.993</cell><cell>0.993</cell><cell cols="2">0.947 0.977</cell></row><row><cell>AtrialFibrillation 0.267</cell><cell>0.267</cell><cell cols="4">0.220 0.333 0.267 0.133</cell><cell>0.333</cell><cell cols="3">0.267 0.133 0.400</cell><cell cols="2">0.200 0.067</cell><cell>0.133</cell><cell cols="2">0.533 1.000</cell></row><row><cell>BasicMotions 0.676</cell><cell>0.950</cell><cell cols="4">0.975 1.000 1.000 1.000</cell><cell>1.000</cell><cell cols="3">1.000 0.975 1.000</cell><cell cols="2">0.975 1.000</cell><cell>1.000</cell><cell cols="2">0.925 1.000</cell></row><row><cell>CharacterTrajectories 0.964</cell><cell>0.985</cell><cell cols="4">0.989 0.997 0.969 0.994</cell><cell>0.990</cell><cell cols="3">0.985 0.967 0.980</cell><cell cols="2">0.995 0.991</cell><cell>0.990</cell><cell cols="2">0.971 0.994</cell></row><row><cell>Cricket 0.944</cell><cell>0.917</cell><cell cols="4">1.000 0.958 0.986 0.986</cell><cell>1.000</cell><cell cols="3">0.917 0.958 0.986</cell><cell cols="2">0.972 1.000</cell><cell>0.986</cell><cell cols="2">0.847 1.000</cell></row><row><cell>DuckDuckGeese 0.275</cell><cell>0.675</cell><cell cols="4">0.600 0.575 0.550 0.675</cell><cell>0.575</cell><cell cols="3">0.380 0.460 0.725</cell><cell cols="2">0.680 0.500</cell><cell>0.750</cell><cell cols="2">0.300 0.750</cell></row><row><cell>EigenWorms 0.549</cell><cell>0.504</cell><cell cols="2">0.618 0.489</cell><cell>-</cell><cell>0.878</cell><cell>0.890</cell><cell cols="3">0.779 0.840 0.878</cell><cell cols="2">0.847 0.650</cell><cell>0.790</cell><cell cols="2">0.720 0.420</cell></row><row><cell>Epilepsy 0.666</cell><cell>0.761</cell><cell cols="4">0.964 0.971 0.978 0.957</cell><cell>1.000</cell><cell cols="3">0.957 0.957 0.987</cell><cell cols="2">0.964 0.986</cell><cell>1.000</cell><cell cols="2">0.775 1.000</cell></row><row><cell>ERing 0.133</cell><cell>0.133</cell><cell cols="4">0.133 0.133 0.133 0.133</cell><cell>0.133</cell><cell cols="3">0.904 0.852 0.133</cell><cell cols="2">0.874 0.989</cell><cell>0.974</cell><cell cols="2">0.930 0.919</cell></row><row><cell>EthanolConcentration 0.293</cell><cell>0.373</cell><cell cols="4">0.323 0.323 0.304 0.236</cell><cell>0.430</cell><cell cols="3">0.285 0.297 0.312</cell><cell cols="2">0.308 0.450</cell><cell>0.430</cell><cell cols="2">0.337 0.323</cell></row><row><cell>FaceDetection 0.519</cell><cell>0.545</cell><cell cols="2">0.529 0.556</cell><cell>-</cell><cell>0.528</cell><cell>0.545</cell><cell cols="3">0.544 0.536 0.602</cell><cell cols="2">0.501 0.638</cell><cell>0.612</cell><cell cols="2">0.625 0.641</cell></row><row><cell>FingerMovements 0.550</cell><cell>0.580</cell><cell cols="4">0.530 0.530 0.520 0.540</cell><cell>0.490</cell><cell cols="3">0.460 0.470 0.580</cell><cell cols="2">0.480 0.520</cell><cell>0.550</cell><cell cols="2">0.590 0.620</cell></row><row><cell>HandMovementDirection 0.278</cell><cell>0.365</cell><cell cols="4">0.231 0.378 0.306 0.270</cell><cell>0.365</cell><cell cols="3">0.243 0.324 0.338</cell><cell cols="2">0.338 0.486</cell><cell>0.392</cell><cell cols="2">0.675 0.392</cell></row><row><cell>Handwriting 0.200</cell><cell>0.286</cell><cell cols="4">0.286 0.357 0.316 0.533</cell><cell>0.605</cell><cell cols="3">0.498 0.249 0.451</cell><cell cols="2">0.515 0.596</cell><cell>0.520</cell><cell cols="2">0.359 0.281</cell></row><row><cell>Heartbeat 0.619</cell><cell>0.663</cell><cell cols="4">0.717 0.751 0.658 0.737</cell><cell>0.727</cell><cell cols="3">0.751 0.746 0.756</cell><cell cols="2">0.683 0.741</cell><cell>0.771</cell><cell cols="2">0.782 0.780</cell></row><row><cell>InsectWingbeat 0.128</cell><cell>0.167</cell><cell>-</cell><cell>0.208</cell><cell>-</cell><cell>0.160</cell><cell>-</cell><cell cols="3">0.264 0.469 0.250</cell><cell cols="2">0.466 0.179</cell><cell>0.229</cell><cell cols="2">0.687 0.137</cell></row><row><cell>JapaneseVowels 0.924</cell><cell>0.976</cell><cell cols="4">0.949 0.965 0.959 0.989</cell><cell>0.973</cell><cell cols="3">0.930 0.978 0.984</cell><cell cols="2">0.984 0.978</cell><cell>0.986</cell><cell cols="2">0.995 0.992</cell></row><row><cell>Libras 0.833</cell><cell>0.856</cell><cell cols="4">0.870 0.850 0.894 0.867</cell><cell>0.878</cell><cell cols="3">0.822 0.817 0.856</cell><cell cols="2">0.867 0.906</cell><cell>0.922</cell><cell cols="2">0.861 1.000</cell></row><row><cell>LSST 0.456</cell><cell>0.373</cell><cell cols="4">0.551 0.568 0.575 0.558</cell><cell>0.590</cell><cell cols="3">0.474 0.595 0.590</cell><cell cols="2">0.537 0.635</cell><cell>0.653</cell><cell cols="2">0.576 0.976</cell></row><row><cell>MotorImagery 0.510</cell><cell>0.510</cell><cell cols="2">0.500 0.590</cell><cell>-</cell><cell>0.540</cell><cell>0.500</cell><cell cols="3">0.610 0.500 0.610</cell><cell cols="2">0.510 0.460</cell><cell>0.610</cell><cell cols="2">0.610 0.630</cell></row><row><cell>NATOPS 0.850</cell><cell>0.889</cell><cell cols="4">0.883 0.939 0.850 0.944</cell><cell>0.870</cell><cell cols="3">0.822 0.911 0.883</cell><cell cols="2">0.928 0.872</cell><cell>0.933</cell><cell cols="2">0.939 0.911</cell></row><row><cell>PEMS-SF 0.705</cell><cell>0.699</cell><cell cols="4">0.711 0.751 0.734 0.688</cell><cell>-</cell><cell cols="3">0.734 0.699 0.751</cell><cell cols="2">0.682 0.832</cell><cell>0.809</cell><cell cols="2">0.930 0.936</cell></row><row><cell>PenDigits 0.973</cell><cell>0.978</cell><cell cols="4">0.977 0.980 0.939 0.983</cell><cell>0.948</cell><cell cols="3">0.974 0.979 0.977</cell><cell cols="2">0.989 0.981</cell><cell>0.967</cell><cell cols="2">0.981 0.976</cell></row><row><cell>Phoneme 0.104</cell><cell>0.110</cell><cell cols="4">0.151 0.175 0.151 0.246</cell><cell>0.190</cell><cell cols="3">0.252 0.207 0.298</cell><cell cols="2">0.233 0.273</cell><cell>0.291</cell><cell cols="2">0.111 0.165</cell></row><row><cell>RacketSports 0.868</cell><cell>0.803</cell><cell cols="4">0.803 0.868 0.842 0.862</cell><cell>0.934</cell><cell cols="3">0.816 0.776 0.882</cell><cell cols="2">0.855 0.901</cell><cell>0.868</cell><cell cols="2">0.796 0.987</cell></row><row><cell>SelfRegulationSCP1 0.771</cell><cell>0.874</cell><cell cols="4">0.775 0.652 0.765 0.846</cell><cell>0.710</cell><cell cols="3">0.823 0.799 0.782</cell><cell cols="2">0.812 0.867</cell><cell>0.915</cell><cell cols="2">0.961 0.816</cell></row><row><cell>SelfRegulationSCP2 0.483</cell><cell>0.472</cell><cell cols="4">0.539 0.550 0.533 0.556</cell><cell>0.460</cell><cell cols="3">0.533 0.550 0.578</cell><cell cols="2">0.578 0.555</cell><cell>0.506</cell><cell cols="2">0.604 0.622</cell></row><row><cell>SpokenArabicDigits 0.967</cell><cell>0.990</cell><cell cols="4">0.963 0.983 0.959 0.956</cell><cell>0.982</cell><cell cols="3">0.970 0.934 0.975</cell><cell cols="2">0.988 0.997</cell><cell>0.963</cell><cell cols="2">0.998 0.985</cell></row><row><cell>StandWalkJump 0.200</cell><cell>0.067</cell><cell cols="4">0.200 0.400 0.333 0.400</cell><cell>0.333</cell><cell cols="3">0.333 0.400 0.533</cell><cell cols="2">0.467 0.467</cell><cell>0.333</cell><cell cols="2">0.600 0.533</cell></row><row><cell>UWaveGestureLibrary 0.881</cell><cell>0.891</cell><cell cols="4">0.903 0.894 0.868 0.884</cell><cell>0.916</cell><cell cols="3">0.753 0.759 0.906</cell><cell cols="2">0.906 0.931</cell><cell>0.785</cell><cell cols="2">0.913 0.878</cell></row><row><cell>PAMAP2 0.718</cell><cell>0.949</cell><cell cols="4">0.683 0.865 0.769 0.885</cell><cell>0.928</cell><cell cols="3">0.942 0.938 0.948</cell><cell cols="2">0.941 0.931</cell><cell>0.962</cell><cell cols="2">0.948 0.974</cell></row><row><cell>OpportunityGestures 0.655</cell><cell>0.768</cell><cell cols="4">0.762 0.574 0.715 0.689</cell><cell>0.553</cell><cell cols="3">0.791 0.821 0.730</cell><cell cols="2">0.771 0.813</cell><cell>0.809</cell><cell cols="2">0.732 0.830</cell></row><row><cell>OpportunityLocomotion 0.845</cell><cell>0.900</cell><cell cols="4">0.859 0.850 0.868 0.859</cell><cell>0.634</cell><cell cols="3">0.881 0.874 0.874</cell><cell cols="2">0.842 0.875</cell><cell>0.886</cell><cell cols="2">0.907 0.908</cell></row><row><cell>Occupancy [15] 0.496</cell><cell>0.873</cell><cell cols="4">0.517 0.844 0.526 0.817</cell><cell>0.556</cell><cell cols="3">0.865 0.828 0.852</cell><cell cols="2">0.876 0.832</cell><cell>0.878</cell><cell cols="2">0.881 0.883</cell></row><row><cell>Total best accuracy 0</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>1</cell><cell>2</cell><cell>5</cell><cell>1</cell><cell>0</cell><cell>2</cell><cell>1</cell><cell>6</cell><cell>4</cell><cell>7</cell><cell>17</cell></row><row><cell>Average accuracy 0.596</cell><cell>0.651</cell><cell cols="4">0.658 0.672 0.675 0.686</cell><cell>0.688</cell><cell cols="3">0.692 0.693 0.717</cell><cell cols="2">0.722 0.732</cell><cell>0.741</cell><cell cols="2">0.745 0.772</cell></row><row><cell>Ours 1-to-1 Wins 32</cell><cell>26</cell><cell>27</cell><cell>23</cell><cell>31</cell><cell>23</cell><cell>25</cell><cell>28</cell><cell>29</cell><cell>25</cell><cell>24</cell><cell>20</cell><cell>21</cell><cell>20</cell><cell>-</cell></row><row><cell>Ours 1-to-1 Draws 0</cell><cell>0</cell><cell>2</cell><cell>2</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>1</cell><cell>1</cell><cell>2</cell><cell>0</cell><cell>2</cell><cell>4</cell><cell>0</cell><cell>-</cell></row><row><cell>Ours 1-to-1 Losses 2</cell><cell>8</cell><cell>5</cell><cell>9</cell><cell>2</cell><cell>9</cell><cell>6</cell><cell>5</cell><cell>4</cell><cell>7</cell><cell>10</cell><cell>12</cell><cell>9</cell><cell>14</cell><cell>-</cell></row><row><cell>Mean Rank 12.15</cell><cell>8.79</cell><cell>9.65</cell><cell cols="3">7.44 10.44 7.59</cell><cell>7.79</cell><cell cols="2">9.03 9.41</cell><cell>5.47</cell><cell>7.18</cell><cell>5.18</cell><cell>4.71</cell><cell cols="2">5.74 4.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Root Mean Squared Error (RMSE) Performance of TARNet and baselines on regression datasets from Monash University, UEA, UCR Time Series Regression Archive<ref type="bibr" target="#b26">[27]</ref>. We mark the best and second best values. Baselines are presented in descending order (left to right) by mean rank. Avg.Rel.Diff.Mean: Average Relative Difference from Mean over all models, e.g. -0.3 means that the model on average attains 30% lower RMSE than the average model performance. Higher Total best loss and Ours 1-to-1 Wins is better. Lower Ours 1-to-1 Draws, Ours 1-to-1 Losses, Mean Rank, and Avg.Rel.Diff.Mean is better. Rel.Diff.Mean scores. However, TARNet still outperforms TST and all other baseline models by attaining 31.3% lower RMSE on average than the mean RMSE among all models. Considering that TARNet achieves the highest number of best losses, lowest mean rank, and lowest Avg.Rel.Diff.Mean in Table2, we argue that TARNet is the new benchmark for time-series regression.</figDesc><table><row><cell cols="14">Dataset 1-NN-DTWD 1-NN-ED 5-NN-ED 5-NN-DTWD SVR ResNet FCN Rocket Inception RF XGB TST TARNet</cell></row><row><cell>AppliancesEnergy</cell><cell>6.036</cell><cell>5.231</cell><cell>4.227</cell><cell>4.019</cell><cell cols="4">3.457 3.065 2.865 2.299</cell><cell cols="5">4.435 3.455 3.489 2.375 2.173</cell></row><row><cell>BenzeneConcentration</cell><cell>4.983</cell><cell>6.535</cell><cell>5.844</cell><cell>4.868</cell><cell cols="4">4.790 4.061 4.988 3.360</cell><cell cols="5">1.584 0.855 0.637 0.494 0.481</cell></row><row><cell>BeijingPM10</cell><cell>139.134</cell><cell cols="2">139.229 115.502</cell><cell>115.502</cell><cell cols="9">110.574 95.489 94.438 120.057 96.749 94.072 93.138 86.866 90.482</cell></row><row><cell>BeijingPM25</cell><cell>88.256</cell><cell>88.193</cell><cell>74.156</cell><cell>72.717</cell><cell cols="9">75.734 64.462 59.726 62.769 62.227 63.301 59.495 53.492 60.271</cell></row><row><cell>LiveFuelMoisture</cell><cell>57.111</cell><cell>58.238</cell><cell>46.331</cell><cell>46.290</cell><cell cols="9">43.021 51.632 47.877 41.829 51.539 44.657 44.295 43.138 41.091</cell></row><row><cell>IEEEPPG</cell><cell>37.140</cell><cell>33.208</cell><cell>27.111</cell><cell>33.572</cell><cell cols="9">36.301 33.150 34.325 36.515 23.903 32.109 31.487 27.806 26.372</cell></row><row><cell>Total best loss</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>2</cell><cell>3</cell></row><row><cell>Ours 1-to-1 Wins</cell><cell>6</cell><cell>6</cell><cell>6</cell><cell>6</cell><cell>6</cell><cell>6</cell><cell>5</cell><cell>6</cell><cell>5</cell><cell></cell><cell>5</cell><cell>4</cell><cell>-</cell></row><row><cell>Ours 1-to-1 Draws</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell></cell><cell>0</cell><cell></cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>-</cell></row><row><cell>Ours 1-to-1 Losses</cell><cell>0</cell><cell></cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>-</cell></row><row><cell>Mean Rank</cell><cell>12.167</cell><cell>11.833</cell><cell>8.833</cell><cell>8.833</cell><cell cols="4">8.000 7.333 7.000 6.500</cell><cell cols="5">6.500 5.500 4.333 2.500 1.833</cell></row><row><cell>Avg.Rel.Diff.Mean</cell><cell>0.355</cell><cell>0.379</cell><cell>0.153</cell><cell>0.125</cell><cell cols="9">0.097 0.006 0.022 -0.047 -0.107 -0.171 -0.196 -0.302 -0.313</cell></row><row><cell cols="4">Table Ablation study of TARNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">TARNet-Random TARNet-Top ? TARNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Results on 34 classification datasets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Total best accuracy</cell><cell>6</cell><cell>9</cell><cell>31</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Average accuracy</cell><cell>0.752</cell><cell>0.741</cell><cell cols="2">0.772</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours 1-to-1 Wins</cell><cell>28</cell><cell>25</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours 1-to-1 Draws</cell><cell>5</cell><cell>7</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours 1-to-1 Losses</cell><cell>1</cell><cell>2</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mean Rank</cell><cell>2.206</cell><cell>2.176</cell><cell cols="2">1.088</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Results on 6 regression datasets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Total best loss</cell><cell>0</cell><cell>1</cell><cell>5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours 1-to-1 Wins</cell><cell>6</cell><cell>5</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours 1-to-1 Draws</cell><cell>0</cell><cell>0</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours 1-to-1 Losses</cell><cell>0</cell><cell>1</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mean Rank</cell><cell>2.667</cell><cell>2.167</cell><cell cols="2">1.167</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Avg.Rel.Diff.Mean</cell><cell>0.046</cell><cell>0.014</cell><cell cols="2">-0.060</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">is better than what any of the baseline models achieve. For the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">overall rank, TARNet achieves an average rank of 1.833, setting it</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">clearly apart from all other models; the overall second best model,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">TST [37] has an average rank of 2.5; XGB, Inception, and FCN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">(which outperformed TARNet on one dataset) on average ranks</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">4.333, 6.5, and 7, respectively. Both TST [37] and TARNet use a simi-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">lar transformer backbone model which explains the small difference</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>in Avg.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>TARNet: Task-Aware Reconstruction for Time-Series Transformer KDD '22, August 14-18, 2022, Washington, DC, USA</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work was supported in part by the <rs type="funder">CONIX Research Center</rs>, one of six centers in JUMP, a <rs type="grantName">Semiconductor Research Corporation</rs> (SRC) program sponsored by <rs type="funder">DARPA</rs>, under award <rs type="grantNumber">2018-JU-2779</rs>. This work was also sponsored in part by <rs type="funder">National Science Foundation Convergence Accelerator</rs> under award OIA-2040727 as well as generous gifts from <rs type="funder">Google</rs>, <rs type="funder">Adobe</rs>, and <rs type="person">Teradata. Ranak</rs> is partially funded by a <rs type="grantName">Graduate Prize Fellowship</rs> from <rs type="funder">Hal?c?o?lu Data Science Institute</rs>. Any opinions, findings, and conclusions or recommendations expressed herein are those of the authors and should not be interpreted as necessarily representing the views, either expressed or implied, of the <rs type="institution">U.S. Government</rs>. The U.S. Government is authorized to reproduce and distribute reprints for government purposes not withstanding any copyright annotation hereon.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_W3ZyQzM">
					<orgName type="grant-name">Semiconductor Research Corporation</orgName>
				</org>
				<org type="funding" xml:id="_QsSqYjC">
					<idno type="grant-number">2018-JU-2779</idno>
				</org>
				<org type="funding" xml:id="_YUSRraC">
					<orgName type="grant-name">Graduate Prize Fellowship</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Bagnall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Dau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Lines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Large</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Bostrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eamonn</forename><surname>Southam</surname></persName>
		</author>
		<author>
			<persName><surname>Keogh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00075</idno>
		<title level="m">The UEA multivariate time series classification archive</title>
		<imprint>
			<date type="published" when="2018">2018. 2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Yue</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<title level="m">Correlative Channel-Aware Fusion for Multi-View Time Series Classification</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>AAAI</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning a symbolic representation for multivariate time series classification</title>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Gokce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baydogan</forename></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Runger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="400" to="422" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Xgboost: A scalable tree boosting system</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ROCKET: exceptionally fast and accurate time series classification using random convolutional kernels</title>
		<author>
			<persName><forename type="first">Angus</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fran?ois</forename><surname>Petitjean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">I</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1454" to="1495" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">MINIROCKET: A very fast (almost) deterministic transform for time series classification</title>
		<author>
			<persName><forename type="first">Angus</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">F</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">I</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning Discriminative Virtual Sequences for Time Series Classification</title>
		<author>
			<persName><forename type="first">Abhilash</forename><surname>Dorle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenzhan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2001">2020. 2001-2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Support vector regression machines</title>
		<author>
			<persName><forename type="first">Harris</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Chris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linda</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="155" to="161" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Dheeru</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Casey</forename><surname>Graff</surname></persName>
		</author>
		<ptr target="http://archive.ics.uci.edu/ml" />
		<title level="m">UCI Machine Learning Repository</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Emadeldeen</forename><surname>Eldele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Ragab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenghua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chee</forename><surname>Keong Kwoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoli</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cuntai</forename><surname>Guan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.14112</idno>
		<title level="m">Time-Series Representation Learning via Temporal and Contextual Contrasting</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep learning for time series classification: a review</title>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Ismail Fawaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Germain</forename><surname>Forestier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lhassane</forename><surname>Idoumghar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Alain</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data mining and knowledge discovery</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="917" to="963" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Inceptiontime: Finding alexnet for time series classification</title>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Ismail Fawaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Germain</forename><surname>Forestier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charlotte</forename><surname>Pelletier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">F</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">I</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lhassane</forename><surname>Idoumghar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Alain</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fran?ois</forename><surname>Petitjean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unsupervised scalable representation learning for multivariate time series</title>
		<author>
			<persName><forename type="first">Jean-Yves</forename><surname>Franceschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aymeric</forename><surname>Dieuleveut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.10738</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards automatic spatial verification of sensor placement in buildings</title>
		<author>
			<persName><forename type="first">Dezhi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamin</forename><surname>Whitehouse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Culler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th ACM Workshop on Embedded Systems For Energy-Efficient Buildings</title>
		<meeting>the 5th ACM Workshop on Embedded Systems For Energy-Efficient Buildings</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient learning of timeseries shapelets</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacek</forename><surname>Zurada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Explainable Multivariate Time Series Classification: A Deep Neural Network Which Learns to Attend to Important Variables As Well As Time Intervals</title>
		<author>
			<persName><forename type="first">Tsung-Yu</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasant</forename><surname>Honavar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multivariate LSTM-FCNs for time series classification</title>
		<author>
			<persName><forename type="first">Fazle</forename><surname>Karim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somshubra</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houshang</forename><surname>Darabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Harford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="237" to="245" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Dongha</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seonghyeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwanjo</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02577</idno>
		<title level="m">Learnable Dynamic Temporal Pooling for Time Series Classification</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Shapenet: A shapelet-neural network approach for multivariate time series classification</title>
		<author>
			<persName><forename type="first">Guozhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byron</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sourav</surname></persName>
		</author>
		<author>
			<persName><surname>Bhowmick</surname></persName>
		</author>
		<author>
			<persName><surname>Kwok-Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grace Lh</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="8375" to="8383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robust unsupervised anomaly detection via multi-time scale DCGANs with forgetting mechanism for industrial multivariate time series</title>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lili</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">423</biblScope>
			<biblScope unit="page" from="444" to="462" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Joint-Label Learning by Dual Augmentation for Time Series Classification</title>
		<author>
			<persName><forename type="first">Qianli</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenjing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanqing</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><surname>Garrison</surname></persName>
		</author>
		<author>
			<persName><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="8847" to="8855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adversarial dynamic shapelet networks</title>
		<author>
			<persName><forename type="first">Qianli</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanqing</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Desen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Garrison</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="5069" to="5076" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Sch?fer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulf</forename><surname>Leser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.11343</idno>
		<title level="m">Multivariate time series classification with WEASEL+ MUSE</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On the nontrivial generalization of dynamic time warping to the multi-dimensional case</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shokoohi-Yekta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eamonn</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM. SIAM</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Random forest: a classification and regression tool for compound classification and QSAR modeling</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Svetnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Liaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Christopher Culberson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">P</forename><surname>Sheridan</surname></persName>
		</author>
		<author>
			<persName><surname>Bradley P Feuston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and computer sciences</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1947" to="1958" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Chang</forename><surname>Wei Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Bergmeir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francois</forename><surname>Petitjean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">I</forename><surname>Webb</surname></persName>
		</author>
		<title level="m">Monash university, uea, ucr time series regression archive</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page">2006</biblScope>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning for time series with temporal neighborhood coding</title>
		<author>
			<persName><forename type="first">Sana</forename><surname>Tonekaboni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><surname>Eytan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Goldenberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.00750</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Time series classification from scratch with deep neural networks: A strong baseline</title>
		<author>
			<persName><forename type="first">Zhiguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Oates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International joint conference on neural networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1578" to="1585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fast time series classification using numerosity reduction</title>
		<author>
			<persName><forename type="first">Xiaopeng</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eamonn</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Shelton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>Chotirat</surname></persName>
		</author>
		<author>
			<persName><surname>Ratanamahatana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1033" to="1040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Voice2series: Reprogramming acoustic models for time series classification</title>
		<author>
			<persName><surname>Chao-Han Huck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun-Yun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pin-Yu</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Time series shapelets: a new primitive for data mining</title>
		<author>
			<persName><forename type="first">Lexiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eamonn</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="947" to="956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Muvan: A multi-view attention network for multivariate temporal data</title>
		<author>
			<persName><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangxu</forename><surname>Xun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fenglong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kebin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="717" to="726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Zhihan</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanyong</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianmeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congrui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhai</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bixiong</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10466</idno>
		<title level="m">TS2Vec: Towards Universal Representation of Time Series</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Zhihan</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanyong</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianmeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congrui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bixiong</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10466</idno>
		<title level="m">Learning Timestamp-Level Representations for Time Series with Hierarchical Contrastive Loss</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">George</forename><surname>Zerveas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srideepika</forename><surname>Jayaraman</surname></persName>
		</author>
		<title level="m">Dhaval Patel, Anuradha Bhamidipaty, and Carsten Eickhoff</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A Transformer-based Framework for Multivariate Time Series Representation Learning</title>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2021">pages=2114-2124, year=2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Tapnet: Multivariate time series classification with attentional prototypical network</title>
		<author>
			<persName><forename type="first">Xuchao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang-Tien</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Time series classification using multi-channels deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leon</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on web-age information management</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="298" to="310" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
