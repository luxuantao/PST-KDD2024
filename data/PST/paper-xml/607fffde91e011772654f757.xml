<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Solving Inefficiency of Self-supervised Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-04-18">18 Apr 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-sen University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Univeristy of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Keze</forename><surname>Wang</surname></persName>
							<email>kezewang@gmail.com</email>
							<affiliation key="aff3">
								<orgName type="department">DarkMatter AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guangcong</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Phillip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
							<email>philip.torr@eng.ox.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">Univeristy of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liang</forename><surname>Lin</surname></persName>
							<email>linliang@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Solving Inefficiency of Self-supervised Representation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-04-18">18 Apr 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2104.08760v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-supervised learning has attracted great interest due to its tremendous potentials in learning discriminative representations in an unsupervised manner. Along this direction, contrastive learning achieves current state-of-the-art performance. Despite the acknowledged successes, existing contrastive learning methods suffer from very low learning efficiency, e.g., taking about ten times more training epochs than supervised learning for comparable recognition accuracy. In this paper, we discover two contradictory phenomena in contrastive learning that we call under-clustering and over-clustering problems, which are major obstacles to learning efficiency. Under-clustering means that the model cannot efficiently learn to discover the dissimilarity between inter-class samples when the negative sample pairs for contrastive learning are insufficient to differentiate all the actual object categories. Over-clustering implies that the model cannot efficiently learn the feature representation from excessive negative sample pairs, which include many outliers and thus enforce the model to over-cluster samples of the same actual categories into different clusters. To simultaneously overcome these two problems, we propose a novel self-supervised learning framework using a median triplet loss. Precisely, we employ a triplet loss tending to maximize the relative distance between the positive pair and negative pairs to address the under-clustering problem; and we construct the negative pair by selecting the negative sample of a median similarity score from all negative samples to avoid the over-clustering problem, guaranteed by the Bernoulli Distribution model. We extensively evaluate our proposed framework in several large-scale benchmarks (e.g., ImageNet, SYSU-30k, and COCO). The results demonstrate the superior performance (e.g., the learning efficiency) of our model over the latest state-of-the-art methods by a clear margin.</p><p>1 Under-clustering refers to that the clusters are insufficient for the model to differentiate all the object categories, resulting in overlapping among different actual object categories. In contrast, over-clustering means that there are excessive clusters that the model is enforced to overcluster samples of the same actual categories into different clusters</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently, self-supervised learning has shown remarkable results in representation learning. Among them, the results Here, the x-axis represents the training epochs of self-supervised learning, and the y-axis stands for the top-1 accuracy of ImageNet linear evaluation. All methods have lower learning efficiency than supervised learning, but our approach has a significantly higher learning efficiency than the existing self-supervised methods. (best view in color) of contrastive learning are most promising in the computer vision task. Notable works include MoCo v1/v2 <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b5">6]</ref>, SimCLR <ref type="bibr" target="#b4">[5]</ref>, BYOL <ref type="bibr" target="#b18">[19]</ref>, and SimSiam <ref type="bibr" target="#b6">[7]</ref>. For example, on ImageNet <ref type="bibr" target="#b31">[31]</ref>, the top-1 accuracy of BYOL is 74.3%, which is close to that of supervised learning, i.e., 76.4% <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b28">28]</ref> (see "goal line" in Figure <ref type="figure" target="#fig_0">1</ref>). Despite the promising performance and high expectations, the learning efficiency of the state-of-the-art self-supervised learning methods is about ten times lower than the supervised learning methods. For instance, the supervised learning method usually takes about 100 epochs to train a ResNet50 on Im-ageNet. In comparison, SimCLR and BYOL have to cost 1,000 epochs, and MoCo v2 needs to cost 800 epochs (See Figure <ref type="figure" target="#fig_0">1</ref> for details).</p><p>Attempting to address this issue, we rethink the mechanism of the existing self-supervised learning methods and attribute the inherited drawback of contrastive learning to two opposing problems, i.e., under-clustering and overclustering 1 . Specifically, contrastive learning considers each image a class (e.g., there would be 1.28 Million classes on ImageNet) and constructs positive and negative sample pairs 2 for these images. The optimization objective is to reduce the distance between positive sample pairs and enlarge the distance between negative sample pairs. As suggested by distance metric learning <ref type="bibr" target="#b8">[9]</ref>, a sufficient number of negative sample pairs are required to guarantee the learning efficiency. Otherwise, lacking negative samples -whether due to the GPU memory constraints like SimCLR or (ii) algorithm design like BYOL and SimSiam <ref type="bibr" target="#b6">[7]</ref> -can result in lower learning efficiency because the model cannot efficiently discover the dissimilarity between inter-class samples. This is identified as the under-clustering problem. As a result of under-clustering, SimCLR and BYOL converge slowly to an imperfect local optimum. On the contrary, excessive negative samples can lead to an opposite problem, i.e., over-clustering, which implies the model is enforced to learn dissimilarity between intra-class samples in vain. As the number of negative samples increases, some negative samples have an increased possibility to be actual positive samples, i.e., obtaining different pseudo training labels but belonging to the same object category. This also results in a lower learning efficiency since it essentially misleads the model to identify/memorize every specific training sample incompetently rather than abstract its semantic information in an efficient fashion. As reported by <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b1">2]</ref>, this overclustering problem can lead to unnecessary harmful representation learning. For example, Exemplar-CNN <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11]</ref> obtains an unsatisfied performance due to directly clarifying CIFAR-10 into 50,000 classes. MoCo v1 and v2 cannot further increase the accuracy, even leveraging the momentum to store plenty of negative samples. In summary, existing contrastive learning cannot avoid the under-clustering or over-clustering problems, so that their learning efficiency is still low.</p><p>To tackle the above under-clustering and over-training problems, we propose a novel self-supervised learning framework using a median triplet loss. Specifically, a triplet loss tends to maximize the relative distance between the positive pair and the negative pair for each triplet unit. Having plenty of triplets, we can address the under-clustering problem because wealthy triplets contain rich negative pairs that guarantee a considerable distance between negative sample pairs. Triplet loss largely addresses the underclustering issue but raises the over-clustering problem. Hence, we propose novel triplet loss with median triplets to avoid over-clustering samples from the same category into different clusters. The median triplets only contain negative samples with the median similarity score but with almost absolute confidence guaranteed by the Bernoulli Distribution model. This significantly improves the learning effi- 2 Every two samples augmented from the same image are considered a positive sample pair; every two samples augmented from different photos are considered as a negative sample pair. ciency of self-supervised learning and leads to state-of-theart performance (See Figure <ref type="figure" target="#fig_0">1</ref> for detail).</p><p>In summary, our contribution is three-fold.</p><p>• We analyze the existing best-performing contrastive learning methods and attribute their learning inefficiency to the inappropriate use/unuse of negative samples. Specifically, insufficient negative samples can lead to an under-clustering problem because sufficient negative samples are needed to guarantee a considerable intra-class distance. Excessive negative instances can lead to an over-clustering issue because it overclusters examples of the same actual label into different clusters, resulting in unnecessary harmful representation learning just to memorize the data.</p><p>• To address the under-clustering and over-training problem, we propose a novel self-supervised representation learning framework using a median triplet loss. Precisely, we employ a triplet loss containing rich negative samples to address the under-clustering problem, and our triplet loss uses median triplets that have negative pairs with median similarity scores to avoid the over-clustering problem, guaranteed by the Bernoulli Distribution model.</p><p>• Our method significantly improves the learning efficiency of self-supervised learning and thus leads to state-of-the-art performance in several large-scale benchmarks (e.g., ImageNet <ref type="bibr" target="#b31">[31]</ref>, SYSU-30k <ref type="bibr" target="#b39">[40]</ref>, and COCO 2017 <ref type="bibr" target="#b24">[25]</ref>) and varieties of downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Vanilla self-supervised learning. The recent renaissance of self-supervised learning originated from pretext tasks, which were simple and straightforward. Typical pretext tasks included image denoising <ref type="bibr" target="#b35">[36]</ref>, image inpainting <ref type="bibr" target="#b29">[29]</ref>, patch ordering <ref type="bibr" target="#b9">[10]</ref>, solving jigsaw puzzles <ref type="bibr" target="#b27">[27]</ref>, color jittering <ref type="bibr" target="#b48">[48]</ref>, and rotation prediction <ref type="bibr" target="#b15">[16]</ref>. Although these methods contributed to the renaissance of self-supervised learning, their learned representations had a weak generalization ability in computer vision.</p><p>Contrastive learning. Currently, the most effective method for self-supervised learning in computer vision is contrastive learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b3">4]</ref>. Contrastive learning is to learn the appropriate distance for different classes. The intra-class distances are encouraged to be small, and the inter-class distances are forced to be large. Plenty of positive and negative samples are needed to discover the similarity and dissimilarity, which requires large GPU memories <ref type="bibr" target="#b4">[5]</ref>. To address this problem, SimCLR <ref type="bibr" target="#b4">[5]</ref> employs distributed computation to enlarge the batch using many computing machines. Nevertheless, due to GPU memory  limitation, further enlarging the number of positive/negative samples is prohibitive in practice, which forms a barrier to improving self-supervised learning. We identify this as an under-clustering problem.</p><p>To overcome the under-clustering problem, more elegantly, the mean teacher approach <ref type="bibr" target="#b33">[33]</ref> is applied to produce sufficient number of negative samples <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b5">6]</ref> and positive samples <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b3">4]</ref>. One special method, called Exemplar-CNN <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11]</ref>, directly clarifies each all the images in a dataset into a class, i.e., it categorizes CIFAR-10 into 50,000 classes. However, it obtains an unsatisfied performance. We identify this as an over-clustering problem. Specifically, since each image can be regarded as a class, excessive negative sample pairs can over-cluster samples from the same category into different clusters. This overclustering can lead to bad representation learning since the network just memorizes the data instead of learning from the data <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b1">2]</ref>.</p><p>To overcome the over-clustering problem, recent works rethink the necessity of negative samples and propose to remove negative samples at all. Notable works include BYOL <ref type="bibr" target="#b18">[19]</ref> and SimSiam <ref type="bibr" target="#b6">[7]</ref>. However, once the negative samples are removed, under-clustering problems reoccur because the model cannot efficiently discover the dissimilarity between inter-class samples.</p><p>Triplet loss. Triplet loss was proposed by Ding et al. <ref type="bibr" target="#b8">[9]</ref> and Schroff et al. <ref type="bibr" target="#b32">[32]</ref> independently for person reidentification and face recognition, respectively. It tends to maximize the relative distance between the positive pair and the negative pair for each triplet unit. Several improvements over triplet loss are conducted to discover the valuable triplets <ref type="bibr" target="#b22">[23]</ref>, to perform cross-batch triplet loss <ref type="bibr" target="#b41">[42]</ref>, and to apply to weakly supervised scenario <ref type="bibr" target="#b39">[40]</ref>. However, these classical triplet losses can also result in overclustering. In contrast, we propose a median triplet loss to address the over-clustering problem guaranteed by the Bernoulli Distribution model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Self-supervised Representation Learning</head><p>We first present the problems of existing contrastive learning methods in Section 3.1, including under-clustering and over-clustering. Then, we present our method in Section 3.2. The analysis of the effectiveness of our approach is presented in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Under-clustering and Over-clustering Problems</head><p>Contrastive learning is proposed by <ref type="bibr" target="#b19">[20]</ref> and is widely used in self-supervised learning, achieving best-performing results on ImageNet. The most widely-adopted loss for contrastive learning is InfoNCE <ref type="bibr">[35]</ref>. Let x be a query image which has 1 positive sample x + and l negative samples {x − j } j=1,••• ,m . InfoNCE calculates their inner products and normalize the products using softmax and have:</p><formula xml:id="formula_0">{ x T x + , x T x − 1 , x T x − 2 , • • • , x T x − m} = sof tmax({x T x + , x T x − 1 , x T x − 2 , • • • , x T x − m }).</formula><p>Then, the goal of contrastive learning is to minimize 3 for detail), which can be interpreted as forcing x T x + to be close to 1 and forcing</p><formula xml:id="formula_1">−1 log x T x + − 0 log x T x − 1 − 0 log x T x − 2 • • • − 0 log x T x − m (see footnote</formula><formula xml:id="formula_2">x T x − 1 , x T x − 2 , ..., x T x −</formula><p>m to be close to 0. This indicates that plenty of negative sample pairs are needed to guarantee the learning efficiency because the model needs sufficient negative samples to discover the dissimilarity between inter-class samples. Especially, plenty of positive samples and negative samples are needed to enrich the similarity and dissimilarity in each batch of data.</p><p>Under-clustering. Insufficient positive and negative examples can lead to under-clustering. Under-clustering is a critical problem in which different categories have a valid (but unwelcome) overlap. For example, in Figure <ref type="figure" target="#fig_2">2</ref> (a), a cluster may contains {dog, horse} or {cat, cow}, i.e., dogs 3 Generally, it's written as: − log</p><formula xml:id="formula_3">exp(x T x + )/τ exp(x T x + )/τ + m j=1 exp(x T x − j )/τ</formula><p>, where τ is a temperature. Ideally, we would like to use just the right amount of negative sample pairs to ensure that the images from the same category are close to each other and ensure that the images from different classes are far away. As shown in Figure <ref type="figure" target="#fig_2">2</ref> (b), all the dogs, cats, and cows are clustered correctly. Note that this is achieved in an unsupervised manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Median Triplet Loss</head><p>Triplet loss. Inspired by relative distance comparison <ref type="bibr" target="#b50">[50]</ref>, triplet loss was proposed by Ding et al. <ref type="bibr" target="#b8">[9]</ref> and Schroff et al. <ref type="bibr" target="#b32">[32]</ref> independently for person re-identification and face recognition, respectively. In a triplet-loss method, a set of triplets, i.e., {(</p><formula xml:id="formula_4">x i , x + i , x − i )} i=1,••• ,m</formula><p>, are first generated. In general, a query image will have far more negative sam-ples than positive samples (see Figure <ref type="figure" target="#fig_3">3</ref> (a) for detail). For presentation simplicity, we use only one query image and one positive sample for illustration, i.e., we have a triplet set {(x,</p><formula xml:id="formula_5">x + , x − i )} i=1,••• ,m . The oldest triplet loss is defined as: Loss = m i=1 max d(x, x + ) − d(x, x − i ), C</formula><p>, where d is a distance metric (e.g., cosine distance or Euclidean distance). Here, C is a margin deciding whether or not to drop a triplet. This is critical in machine learning algorithms since we usually drop the simple data and focus on the hard data near the decision boundary, as support vector machine <ref type="bibr" target="#b7">[8]</ref> suggests. To improve the learning efficiency of triplet loss, in practice, we usually use the hardest triplet to represent the overall triplets, i.e., only the triplet containing the negative sample of the highest similarity score overall negative samples are used (please refer to Figure <ref type="figure" target="#fig_3">3</ref> (b) for detail). Finally, a triplet is formally defined as:</p><formula xml:id="formula_6">Loss = max d(x, x + ) − d(x, x − hardest ), C .<label>(1)</label></formula><p>Since x − hardest is the hardest negative sample, we have d(x, x − hardest ) &lt; d(x, x − i ) for all i. This indicates that when the hardest triplet loss meets the condition d(x, x + ) &lt; d(x, x − hardest ), all others triplets meet the condition. Therefore, the hardest triplet loss guarantees a considerable distance between negative sample pairs. Using triplet loss, we can address the under-clustering problem.</p><p>Although triplet loss largely addresses the underclustering issue, it also raises the over-clustering problem. Specifically, since contrastive learning can be considered a classification problem that identifies each image as a class, using the hardest triplet loss can lead to over-clustering. For example, in Figure <ref type="figure" target="#fig_3">3</ref> (a), the two dogs belong to the same object category. Unsurprisingly, their feature similarity score is very high. But in self-supervised learning, the actual category labels are absent; thus, these two dogs can be reluctantly considered negative sample pairs (Figure <ref type="figure" target="#fig_3">3</ref> (b) and (c), top). This indicates they are the hardest negative sample pairs. Using the hardest triplet loss, the distance between these two dogs is enlarged. This results in an over-clustering problem that the two dogs from the same category are over-clustered into two different clusters.</p><p>Median triplet loss. To avoid over-clustering, we construct the negative pair by selecting the negative sample of a rank-k similarity score throughout negative samples rather than the hardest negative sample, i.e., we have:</p><formula xml:id="formula_7">Loss = max γd(x, x + ) − d(x, x − rank−k ), C .<label>(2)</label></formula><p>Specifically, d(x, x − rank−k ) is obtained using the following steps. First, we compute the distance {d(x, x − i )}, ∀i. Then, we sort {d(x, x − i )} by ascending. Finally, the k-th element are selected from {d(x, x − i )}, forming d(x, x − rank−k ). Note that when k = 1, the median triplet loss reduces to the hardest triplet loss. We can show in Section 3.3 that replacing the hardest triplet with the rank-k triplet can indeed reduce the risk of over-clustering, guaranteed by the Bernoulli Distribution model. By default, we use k = m 2 (i.e., the median triplet) for most of the experiments, although using other values (e.g., k = 5) also yields good performance. We use the widely-used cosine distance for d, i.e., d(x, y) = − xy x 2 y 2 , where • 2 represents the L2 norm. The negative sign ("-") is used here because we usually consider that a higher similarity indicates a smaller distance. We set γ to be 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Analysis with Bernoulli Distribution Model</head><p>In self-supervised learning, the actual category labels are absent; the images from the same actual category are unavoidably regarded as negative samples (e.g., the two dogs in Figure <ref type="figure" target="#fig_3">3 (a)</ref>). Unsurprisingly, these kinds of negative sample pairs have a high feature similarity. With a high probability, they will be in the hardest triplets (see Figure <ref type="figure" target="#fig_3">3 (b)</ref>). Using the hardest triplet loss, the distance between these "pseudo" negative pairs is enlarged. This results in an over-clustering problem that the "pseudo" negative pairs from the same category are over-clustered into two different clusters (see Figure <ref type="figure" target="#fig_3">3</ref> </p><formula xml:id="formula_8">(c) top).</formula><p>But in our median triplet loss, we sort {d(x, x − i )} by ascending and select the k-th element from {d(x,</p><formula xml:id="formula_9">x − i )}, form- ing d(x, x − rank−k ).</formula><p>If this rank-k negative sample belongs to the same actual category as the query image, the overclustering risk still exists; otherwise, the over-clustering risk is reduced. We need to estimate the probability that the rank-k negative sample belongs to the same category as the query image. We first have a reasonable assumption: with a high probability, the image pairs from the same actual category have higher feature similarities than other pairs, and the distances between these pairs are smaller than other pairs. Because we have sorted {d(x, x − i )} by ascending, the event that the rank-k negative sample belongs to the same category as the query image indicates an event that at least k negative samples belong to the same category as the query image. The probability of this event can be computed by using the Bernoulli Distribution model, i.e.,</p><formula xml:id="formula_10">P r = C k m p k (1 − p) m−k + C k+1 m p k+1 (1 − p) m−k−1 + • • • + C m−1 m p m−1 (1 − p) 1 + C m m p m (1 − p) 0 .</formula><p>Here, C is used to denote the combinations, and p is used to denote the probability that a negative sample belongs to the same category as the query image. For example, on ImageNet, we have p = 1 1000 . In our experiment, we let m be 104 and k be m 2 . Putting m, k into the above equation, we have P r = 6.53e −121 , which is almost zero. Even we let m be 104 and k be 5, we have P r = 3.03e −94 . This indicates it is a rare event that the rank-k negative sample belongs to the same category as the query image. Thus, our triplet loss uses median triplets can avoid the over-clustering problem, guaranteed by the Bernoulli Distribution model.</p><p>Thanks to the median triplet loss, we can avoid the overclustering problem, e.g., the two dogs at the bottom of Figure <ref type="figure" target="#fig_3">3 (c</ref>) can be identified correctly. Experimental results in Section 5 also verify the effectiveness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Main results</head><p>Our self-supervised training protocols are as follows. Data augmentation protocol. Our augmentations are straightforward, including randomly cropping, randomly resizing, randomly flipping horizontally, arbitrary gray scaling, stochastic color jittering, Gaussian blurring, and solarization.</p><p>Other protocols. For the unsupervised learning stage, the batch size is 104 images per GPU, and we use eight GPUs. The gradient update interval is five steps. The maximum epoch is 200. The learning rate starts from 4.8 and gradually decreases with cosine annealing. The weight decay factor is 1e −6 . The optimizer is LARS <ref type="bibr" target="#b16">[17]</ref> with a momentum 0.9. The backbone is ResNet-50, which is the same as the previous methods. The models are trained by using the 1.28M training images of ImageNet but without their annotations. All the protocols are in line with <ref type="bibr" target="#b49">[49]</ref>. Using the same protocols as <ref type="bibr" target="#b49">[49]</ref> makes it feasible to present comparisons on multiple datasets/tasks without the extra hyperparameter search.</p><p>We evaluate our method by comparing with state-of-theart methods in four tasks, involving linear evaluation on Im-ageNet, person re-identification on SYSU-30k, and object detection on both COCO 2017 and "VOC07+12".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Linear evaluation on ImageNet</head><p>Linear evaluation is the most widely-adopted evaluation protocol for validating the representation ability of different self-supervised methods. Standardly, the backbones of  <ref type="bibr" target="#b15">[16]</ref> 47.0 200 DeepCluster <ref type="bibr" target="#b2">[3]</ref> 46.9 200 NPID <ref type="bibr" target="#b43">[44]</ref> 56.6 200 ODC <ref type="bibr" target="#b46">[46]</ref> 53.4 200 SimCLR <ref type="bibr" target="#b4">[5]</ref> 60.6 200 SimCLR-1000 <ref type="bibr" target="#b4">[5]</ref> 69.3 1000 MoCo <ref type="bibr" target="#b20">[21]</ref> 61.9 200 MoCo v2 <ref type="bibr" target="#b5">[6]</ref> 67.0 200 MoCo v2-800 <ref type="bibr" target="#b5">[6]</ref> 71.1 800 BYOL <ref type="bibr" target="#b18">[19]</ref> ResNet-50 are trained by using the above self-supervised training protocols and are frozen. A linear classifier is then added to the top of the frozen representation and is trained for each method. All methods are trained using the 1.28M training images of ImageNet and are evaluated using the 50K validation images of ImageNet. For the linear classification stage, the batch size is 256. The maximum epoch is 100. There is no weight decay in linear classification training. The optimizer is SGD. Single-scale center-crop top-1 accuracy is used. Currently, the widely-used evaluation standard of selfsupervised learning values the accuracy most but regardless of the training epochs. Following this standard, we first compare our method with the competitors without considering the training epochs. The results are reported in Table <ref type="table" target="#tab_0">1</ref>. As shown, our method achieves a promising result on Ima-geNet, i.e., 75.9%, outperforming the latest state-of-the-art methods by a clear margin.</p><p>Regarding learning efficiency, the computational efficiency of existing self-supervised learning methods is far from supervised learning. As shown in Table <ref type="table" target="#tab_0">1</ref>, the selfsupervised models are trained for about 1,000 epochs, while the supervised counterpart is trained for only about 100 epochs. SimCLR <ref type="bibr" target="#b4">[5]</ref> explains that training for a longer time does not bring gain for the supervised learning model (i.e., it reported a result of 76.4% vs. 76.3% for supervised-1000 vs. supervised-100). But our observation is opposite, i.e., our reproduction of a supervised model trained for 270 epochs can achieve 78.4% top-1 accuracy, which is signifi-  cantly higher than all of the self-supervised models. Put together, a complete comparison regarding both the accuracies and training epochs is presented in Figure <ref type="figure" target="#fig_0">1</ref> and Table <ref type="table" target="#tab_0">1</ref>, from which we have two observations. First, previous self-supervised methods still have a long way to go. All methods have significantly lower learning efficiency than supervised learning. Second, as shown in Figure <ref type="figure" target="#fig_0">1</ref>, our approach lies in the top-left corner of the figure, which indicates that our method achieves the best performance among the compared self-supervised methods. This comparison verifies the effectiveness and efficiency of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Transferring to downstream tasks</head><p>Transferring to COCO 2017 object detection. One of the self-supervised learning's goals is to learn transferrable features. We test our self-supervised learned representation's generalization ability by transferring to COCO 2017 object detection <ref type="bibr" target="#b24">[25]</ref>. Specifically, the backbones of ResNet-50 are trained by using the above self-supervised training protocols, and the trained network weights serve as the initialization of Mask-RCNN <ref type="bibr" target="#b21">[22]</ref> with C4. We finetune all layers on the train2017 set of COCO 2017, which has about 119K training images. The training schedule is the default 2× schedule in <ref type="bibr" target="#b17">[18]</ref>. Following <ref type="bibr" target="#b20">[21]</ref>, we finetune BN instead of freezing it. Overall, the self-supervised pretraining methods use the same training protocol as the ImageNet supervised counterpart. The accuracy is tested on the val2017 set of COCO 2017. We report the standard metric for the detection and instance segmentation: AP Box and AP Mask .</p><p>Table <ref type="table" target="#tab_2">2</ref> shows that using our approach for pretraining surpasses other self-supervised ImageNet pretraining on COCO 2017 detection. Moreover, our self-supervised pretraining even outperforms the supervised ImageNet pretraining, implying that self-supervised learning can obtain more universal representations. This is in line with previous works that also show that self-supervised pretraining can outperform supervised pretraining on object detection <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>Transferring to VOC07+12 object detection. In addition to COCO 2017, we also evaluate our method's transferability in PASCAL VOC object detection. Following <ref type="bibr" target="#b20">[21]</ref> and <ref type="bibr" target="#b49">[49]</ref>, the backbones of ResNet-50 are trained by using the above self-supervised training protocols, and the trained network weights serve as the initialization of Faster R-CNN <ref type="bibr" target="#b30">[30]</ref> with C4. Then, we fine-tune all layers on the train-val07+12 set of PASCAL. The image scale is [480, 800] pixels during training and 800 in the testing. We report the default VOC metric of AP 50 and the COCO-style AP and AP 75 . The evaluation is on the VOC test2017 set.</p><p>We show the performance of different methods in Table <ref type="table" target="#tab_3">3</ref>. As shown, only MoCo v2 and our approach can catch up with the supervised pretraining counterpart that performs pretraining for 100 epochs. Our approach is comparable to MoCo v2. This verifies the effectiveness of our method and implies that our self-supervised learning can obtain universal representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Person re-identification on SYSU-30k</head><p>In a general sense, all the above tasks (image classification, object detection, and segmentation) belong to visual categorization since detection and segmentation can be considered categorizing regions and pixels for a given image. The effectiveness of our approach beyond visual classification remains uninvestigated. In the following, we investigate a completely different task, i.e., person re-identification (re-ID), which is fundamental in video surveillance <ref type="bibr" target="#b13">[14]</ref>. Re-ID refers to the problem of re-identifying individuals across cameras. Mathematically, re-ID is a matching problem rather than a classification problem because it requires calculating distance metric between two given images. As is proved by <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b36">37]</ref>, unsupervised representation learning is critical to visual matching, therefore validating the effectiveness of our approach in re-ID is nontrivial.</p><p>Dataset and protocol. We conduct experiments on the SYSU-30k dataset <ref type="bibr" target="#b39">[40]</ref>, which is the largest database for re-ID. This database contains 29,606,918 images of 30,508 pedestrians, which is about 30 times larger than ImageNet in terms of category number. Please note that the exact  <ref type="bibr" target="#b44">[45]</ref> 23.0 MGN <ref type="bibr" target="#b40">[41]</ref> 23.6</p><p>Weakly supervised W-Local CNN <ref type="bibr" target="#b39">[40]</ref> 28.8 W-MGN <ref type="bibr" target="#b39">[40]</ref> 29.5</p><p>Self-supervised SimCLR <ref type="bibr" target="#b4">[5]</ref> 10.9 MoCo v2 <ref type="bibr" target="#b5">[6]</ref> 11.6 BYOL <ref type="bibr" target="#b18">[19]</ref> 12.7 median triplet 14.8</p><p>label of each image is unknown in this dataset. Both the lack of precise annotation and the massive number of images make this dataset set very suitable for unsupervised learning, especially self-supervised learning. Since we are the first to perform self-supervised learning in this database, no previous work provides an evaluation protocol for this dataset. We propose a new evaluation protocol for it as follows: the training set of SYSU-30k is employed to perform self-supervised representation learning. Once the model is learned, we directly use it to extract features for visual matching on the test set of SYSU-30k without any finetuning. This is even more challenging than linear evaluation on ImageNet because linear evaluation learns an extra classifier for recognition, but no extra classifier is learned here. Regarding the superiority of using SYSU-30k mentioned above, we believe SYSU-30k is a perfect database to evaluate the effectiveness of self-supervise learning and recommend it to future self-supervised learning researchers. Result analysis. In Table <ref type="table" target="#tab_4">4</ref>, we compare with Sim-CLR, MoCo-v2, BYOL, and current state-of-the-art results (not self-supervised methods). We use ResNet-50 as the feature extractors. The results in Table <ref type="table" target="#tab_4">4</ref> show that our models achieve a new state-of-the-art performance, i.e., a rank-1 accuracy of 14.8%. Please note that this number is so low, i.e., even lower than existing transfer learning and weakly-supervised learning methods. This is attributed to the challenge of the SYSU-30k test set, which contains about 480,000 testing images. More importantly, there are 478,730 mismatching images as the wrong answer in the gallery. Thus, evaluation using the SYSU-30k test set is like searching for a needle in the ocean. We encourage future self-supervised researchers to use this dataset to evaluate the effectiveness of self-supervised learning. We can also observe that our approach surpasses other self-supervised learning methods by a clear margin (14.8 vs. 12.7 for ours vs. BYOL). This verifies the effectiveness of our approach on visual matching tasks like re-ID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Ablation studies</head><p>We present ablation studies on our method to provide insights into how it works. To reduce the training time and Rank-k rank-1 rank-5 rank-52 Top-1 accuracy 28.9 29.5 30.0 fast access to the results, we perform ablation studies using 20 training epochs. Please note that, due to our method's high learning efficiency, training for 20 epochs is sufficient for ablation studies. Previous also uses few training epochs for ablation studies, e.g., <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b34">[34]</ref>. All the training protocols are the same as Section 4, except that we take the 20th epoch's checkpoint for evaluation. This section only reports the results of top-1 accuracy on ImageNet under the linear evaluation protocol since it is the most widely-adopted metric for validating the effectiveness of self-supervised methods. The linear evaluation training is also reduced to one epoch because, empirically, we find a strong correlation between the one-epoch training and the complete training.</p><p>Effect of avoiding over-clustering. As we discussed in Section 3.2, thanks to the median triplet loss, we can avoid the over-clustering problem. For example, if k = 5, the probability of over-clustering is 3.03e −94 . If k = m 2 = 52, the probability of over-clustering is 6.53e −121 . However, whether this analysis is correct remains unclear. In the following, we provide empirical evidence to support our analysis. During batch training, all the batch samplings are considered the total event A. If a batch contains at least two images belonging to the same actual category (e.g., there are two dogs in a batch), we call it an event B. If the rank-k negative sample belongs to the same category as the query image in a batch (e.g., the two dogs are considered rank-k negative sample pairs), we call it an event Ω. We report the frequency P r(Ω|A) and P r(Ω|B) in Table <ref type="table" target="#tab_5">5</ref> for different training epochs and different ks.</p><p>We have three observations from Table <ref type="table" target="#tab_5">5</ref>. First, the rank-52 and rank-5 negative sample rarely belong to the same category as the query image. Both P r(Ω|A) and P r(Ω|B) are low for rank-52. Second, with the training epochs increasing, the probability that the rank-k negative sample belongs to the same category as the query image decreases. This is attributed to the more and more discriminative features that have been learned as the training goes. Third, with k increasing, the probability that the rank-k negative sample belongs to the same category as the query image decreases. Especially when k = 1, our median triplet loss reduces to the hardest triplet loss. As shown, the hardest triplet loss indeed has a risk of over-clustering because the probability P r(Ω|B) is high. With k increasing, the probability P r(Ω|B) decreases. This indicates that our triplet loss uses median triplets can avoid the over-clustering problem, guaranteed by the Bernoulli Distribution model.</p><p>Please note that if a batch contains even one negative sample pair that belongs to the same actual category but is clustered into two different clusters, we consider the whole batch has the over-clustering risk. Therefore, the probability in Table <ref type="table" target="#tab_5">5</ref> (e.g., 0.0110 or 0.2105) is higher than that in the analysis (e.g.,3.03e −94 or 6.53e −121 ).</p><p>Impact of margin. As we discussed in Section 3.2, there is a margin C deciding whether or not to drop a triplet. This is critical in machine learning algorithms since we usually drop the simple data and focus on the complex data near the decision boundary, as support vector machine <ref type="bibr" target="#b7">[8]</ref> suggests. To empirically verify this hypothesis, we train our method using different margins. The results are shown in Table <ref type="table" target="#tab_6">6</ref>. As shown, different margins lead to a performance fluctuation. When C = 100 or C = 1.2, the performance is best. Hence, we use C = 100 for default in all of the experiments if no otherwise specified.</p><p>Impact of rank-k. As we analyze in Section 3.3, we can use different ks for our median triplet loss. When k = 1, our median triplet loss reduces to the traditional hardest triplet loss. When k increases, the risks of over-clustering reduces exponentially. To know the impact of using different ks, we train our method using different ks. The results are shown in Table <ref type="table" target="#tab_7">7</ref>. As shown, different ks lead to a performance fluctuation. When k = 5 and k = 52, the performances are satisfied. Hence, we use k = 52 for default in all of the experiments if no otherwise specified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Although self-supervised learning has shown promising results on ImageNet, its learning efficiency is still low. We attribute the inherited drawback of contrastive learning to under-clustering and over-clustering. To overcome these two problems, we propose a novel self-supervised representation framework using a median triplet loss. We employ triplet loss containing rich negative sample information to address the under-clustering problem, and we construct negative sample pairs by selecting the negative samples with the median similarity score to prevent the overclustering problem, guaranteed by the Bernoulli Distribution model. Our method significantly improves the learning efficiency of self-supervised learning, leading to state-ofthe-art performance in several large-scale benchmarks and varieties of downstream tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>SimSiamFigure 1 .</head><label>1</label><figDesc>Figure 1. A comparison of learning efficiency among different self-supervised methods.Here, the x-axis represents the training epochs of self-supervised learning, and the y-axis stands for the top-1 accuracy of ImageNet linear evaluation. All methods have lower learning efficiency than supervised learning, but our approach has a significantly higher learning efficiency than the existing self-supervised methods. (best view in color)</figDesc><graphic url="image-1.png" coords="1,334.37,209.24,173.19,125.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Illustration of the under-clustering and over-clustering problems. Here, each pair of samples connected by a yellow line represent a negative sample pair.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Illustration of our median triplet loss. Here, each pair of samples connected by a red line represent a negative sample pair. Please note that although labeled by a green word "Neg", in fact, the dog is a positive sample to the query image because they belong to the same category, i.e., "a dog". Traditional triplet loss using the hardest triplet (see Figure (b)) will result in an over-clustering problem, in which the distance between these two dogs will be enlarged. The over-clustering result is shown at (c) top, and the perfect learning results is shown at (c) bottom. and horse are mixed up. Without the annotation, we cannot identify the actual label of each data point. In other words, the dogs and horses have an overlap. An underclustering problem occurs when insufficient positive and negative samples are present.Over-clustering. As opposed to under-clustering caused by lacking negative samples, over-clustering is a problem caused by overwhelming negative samples. Although contrastive learning implicitly regards each image as a class, we do not expect over-clustering. Excessive negative sample pairs can result in over-clustering that forces samples from the same category into different clusters. As is shown in Figure2(c), if excessive negative examples are provided, the two dogs that belong to the same category are now assigned to two clusters. Similar phenomena also appear in cats and cows. This non-ideal over-clustering would prevent the model from learning discriminative representations summarizing fundamental features of a category since the network just memorizes the data instead of learning from the data<ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b1">2]</ref>.Ideally, we would like to use just the right amount of negative sample pairs to ensure that the images from the same category are close to each other and ensure that the images from different classes are far away. As shown in Figure2(b), all the dogs, cats, and cows are clustered correctly. Note that this is achieved in an unsupervised manner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Top-1 accuracy and training epochs of state-of-the-art methods on ImageNet using linear classification for evaluation.</figDesc><table><row><cell>Method</cell><cell cols="2">top-1 acc. train epochs</cell></row><row><cell>Random</cell><cell>4.4</cell><cell>0</cell></row><row><cell>Relative-Loc [10]</cell><cell>38.8</cell><cell>200</cell></row><row><cell>Rotation-Pred</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>The object detection accuracy on COCO 2017 using Mask-RCNN.</figDesc><table><row><cell>Method</cell><cell cols="2">AP Box AP M ask</cell></row><row><cell>Random</cell><cell>35.6</cell><cell>31.4</cell></row><row><cell>Relative-Loc [10]</cell><cell>40.0</cell><cell>35.0</cell></row><row><cell>Rotation-Pred [16]</cell><cell>40.0</cell><cell>34.9</cell></row><row><cell>NPID [44]</cell><cell>39.4</cell><cell>34.5</cell></row><row><cell>MoCo [21]</cell><cell>40.9</cell><cell>35.5</cell></row><row><cell>MoCo v2 [6]</cell><cell>40.9</cell><cell>35.5</cell></row><row><cell>SimCLR [5]</cell><cell>39.6</cell><cell>34.6</cell></row><row><cell>BYOL [19]</cell><cell>40.3</cell><cell>35.1</cell></row><row><cell>median triplet</cell><cell>41.3</cell><cell>37.3</cell></row><row><cell>supervised-100</cell><cell>40.0</cell><cell>34.7</cell></row><row><cell>supervised-270</cell><cell>42.0</cell><cell>37.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="4">. The object detection accuracy on "VOC07+12" using</cell></row><row><cell>Faster-RCNN. Method</cell><cell cols="3">AP50 Box AP Box AP75 Box</cell></row><row><cell>Random</cell><cell>59.0</cell><cell>32.8</cell><cell>31.6</cell></row><row><cell>Relative-Loc [10]</cell><cell>80.4</cell><cell>55.1</cell><cell>61.2</cell></row><row><cell>Rotation-Pred [16]</cell><cell>80.9</cell><cell>55.5</cell><cell>61.4</cell></row><row><cell>NPID [44]</cell><cell>80.0</cell><cell>54.1</cell><cell>59.5</cell></row><row><cell>MoCo [21]</cell><cell>81.4</cell><cell>56.0</cell><cell>62.2</cell></row><row><cell>MoCo v2 [6]</cell><cell>82.0</cell><cell>56.6</cell><cell>62.9</cell></row><row><cell>SimCLR [5]</cell><cell>79.4</cell><cell>51.5</cell><cell>55.6</cell></row><row><cell>BYOL [19]</cell><cell>81.0</cell><cell>51.9</cell><cell>56.5</cell></row><row><cell>median triplet</cell><cell>81.8</cell><cell>56.4</cell><cell>62.9</cell></row><row><cell>supervised-100</cell><cell>81.6</cell><cell>54.2</cell><cell>59.8</cell></row><row><cell>supervised-270</cell><cell>82.2</cell><cell>56.9</cell><cell>63.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Comparison with state-of-the-art methods on SYSU-30k.</figDesc><table><row><cell>supervision</cell><cell>method</cell><cell>rank-1</cell></row><row><cell></cell><cell>DARI [39]</cell><cell>11.2</cell></row><row><cell>Transfer learning</cell><cell>DF [9] Local CNN</cell><cell>10.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Effect of avoiding over-clustering.</figDesc><table><row><cell>Training epoch</cell><cell>event</cell><cell>0</cell><cell>180</cell></row><row><cell>Top-1 accuracy, k = 1</cell><cell cols="3">Pr(ω|A) 0.1538 0.9656 Pr(ω|B) 0.1618 0.9948</cell></row><row><cell>Top-1 accuracy, k = 5</cell><cell cols="3">Pr(ω|A) 0.1167 0.2105 Pr(ω|B) 0.1230 0.2132</cell></row><row><cell>Top-1 accuracy, k = 52</cell><cell cols="3">Pr(ω|A) 0.1220 0.0110 Pr(ω|B) 0.1288 0.0233</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Impact of margin.</figDesc><table><row><cell>Margin</cell><cell cols="3">C = 0.3 C = 1.2 C = 100</cell></row><row><cell>Top-1 accuracy</cell><cell>28.3</cell><cell>29.8</cell><cell>30.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Impact of rank-k.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensor-Flow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas</title>
				<meeting><address><addrLine>Pete Warden, Martin Wattenberg, Martin Wicke</addrLine></address></meeting>
		<imprint>
			<publisher>Yuan Yu, and Xiaoqiang Zheng</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A corrective view of neural networks: Representation, memorization and learning</title>
		<author>
			<persName><forename type="first">Guy</forename><surname>Bresler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dheeraj</forename><surname>Nagaraj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="848" to="901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
				<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 8-14. 2018. 2018</date>
			<biblScope unit="page" from="139" to="156" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XIV</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno>CoRR, abs/2006.09882</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>CoRR, abs/2002.05709</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Improved baselines with momentum contrastive learning</title>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno>CoRR, abs/2003.04297</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2020, virtual conference online</title>
				<imprint>
			<date type="published" when="2021">June 19-25, 2021, page , 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep feature learning with relative distance comparison for person re-identification</title>
		<author>
			<persName><forename type="first">Shengyong</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2993" to="3003" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision, ICCV 2015</title>
				<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">December 7-13, 2015. 2015</date>
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with exemplar convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1734" to="1747" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Jost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">A</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08">2014. December 8-13 2014. 2014</date>
			<biblScope unit="page" from="766" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification: Clustering and finetuning</title>
		<author>
			<persName><forename type="first">Hehe</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenggang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Person re-identification by symmetry-driven accumulation of local features</title>
		<author>
			<persName><forename type="first">Michela</forename><surname>Farenzena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loris</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vittorio</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Cristani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE computer society conference on computer vision and pattern recognition</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2360" to="2367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning representations by predicting bags of visual words</title>
		<author>
			<persName><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020</title>
				<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">June 13-19, 2020. 2020</date>
			<biblScope unit="page" from="6926" to="6936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno>ICLR 2018</idno>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<title level="s">Conference Track Proceedings</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-05-03">April 30 -May 3, 2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Large batch training of convolutional networks with layer-wise adaptive rate scaling</title>
		<author>
			<persName><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><surname>Detectron</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Ávila Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaohan</forename><surname>Daniel Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
		<idno>CoRR, abs/2006.07733</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-06">2006. June 2006. 2006</date>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2020</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision, ICCV 2017</title>
				<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">October 22-29, 2017. 2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">In defense of the triplet loss for person re-identification</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno>CoRR, abs/1703.07737</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia, MM &apos;14</title>
				<meeting>the ACM International Conference on Multimedia, MM &apos;14<address><addrLine>Orlando, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">November 03 -07, 2014. 2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014 -13th European Conference</title>
				<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">September 6-12, 2014. 2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
	<note>Proceedings, Part V</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020</title>
				<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">June 13-19, 2020. 2020</date>
			<biblScope unit="page" from="6706" to="6716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 -14th European Conference, Amsterdam</title>
				<meeting><address><addrLine>The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">October 11-14, 2016. 2016</date>
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VI</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Köpf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-14">2019. 2019, 8-14 December 2019. 2019</date>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas</title>
				<meeting><address><addrLine>NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">June 27-30, 2016. 2016</date>
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07">2015. December 7-12, 2015. 2015</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015</title>
				<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">June 7-12, 2015. 2015</date>
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09">2017, 4-9 December 2017. 2017</date>
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Aäron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno>CoRR, abs/1807.03748</idno>
		<imprint>
			<date type="published" when="2018">2020. 2018</date>
		</imprint>
	</monogr>
	<note>What makes for good views for contrastive learning</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning, Proceedings of the Twenty-Fifth International Conference (ICML 2008)</title>
				<meeting><address><addrLine>Helsinki, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">June 5-9, 2008. 2008</date>
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Spatial-temporal person re-identification</title>
		<author>
			<persName><forename type="first">Guangcong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhuang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peigen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019</title>
				<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019-02-01">January 27 -February 1, 2019. 2019</date>
			<biblScope unit="page" from="8933" to="8940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Smoothing adversarial domain attack and p-memory reconsolidation for cross-domain person reidentification</title>
		<author>
			<persName><forename type="first">Guangcong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Huang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10568" to="10577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">DARI: distance metric and representation integration for person verification</title>
		<author>
			<persName><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengyong</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
				<editor>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michael</forename><forename type="middle">P</forename><surname>Wellman</surname></persName>
		</editor>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence<address><addrLine>Phoenix, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016">February 12-17, 2016. 2016</date>
			<biblScope unit="page" from="3611" to="3617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Weakly supervised person re-id: Differentiable graphical learning and a new benchmark</title>
		<author>
			<persName><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangcong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xujie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhuang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengtao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Neural Networks and Learning Systems (T-NNLS)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning discriminative features with multiple granularities for person re-identification</title>
		<author>
			<persName><forename type="first">Guanshuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufeng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Kyoung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeran</forename><surname>Byun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM Multimedia Conference on Multimedia Conference, MM 2018</title>
				<editor>
			<persName><forename type="first">Chang</forename><surname>Wen Chen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rainer</forename><surname>Lienhart</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</editor>
		<meeting><address><addrLine>Seoul, Republic of Korea</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">October 22-26. 2018. 2018</date>
			<biblScope unit="page" from="274" to="282" />
		</imprint>
	</monogr>
	<note>Susanne Boll</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Cross-batch memory for embedding learning</title>
		<author>
			<persName><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haozhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020</title>
				<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">June 13-19, 2020. 2020</date>
			<biblScope unit="page" from="6387" to="6396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<ptr target="https://github.com/tensorpack/" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Unsupervised feature learning via non-parametric instancelevel discrimination</title>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<idno>CoRR, abs/1805.01978</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Local convolutional neural networks for person re-identification</title>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Jiebo Luo</title>
		<editor>Susanne Boll, Kyoung Mu Lee</editor>
		<imprint>
			<publisher>Hyeran Byun</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m">2018 ACM Multimedia Conference on Multimedia Conference, MM 2018</title>
				<editor>
			<persName><forename type="first">Chang</forename><surname>Wen Chen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rainer</forename><surname>Lienhart</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</editor>
		<meeting><address><addrLine>Seoul, Republic of Korea</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">October 22-26. 2018. 2018</date>
			<biblScope unit="page" from="1074" to="1082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Online deep clustering for unsupervised representation learning</title>
		<author>
			<persName><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020</title>
				<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">June 13-19, 2020. 2020</date>
			<biblScope unit="page" from="6687" to="6696" />
		</imprint>
	</monogr>
	<note>Yew-Soon Ong, and Chen Change Loy</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<title level="s">Conference Track Proceedings</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. April 24-26, 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 -14th European Conference</title>
				<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">October 11-14, 2016. 2016</date>
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Xiaohang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Openselfsup</surname></persName>
		</author>
		<ptr target="https://github.com/open-mmlab/OpenSelfSup" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Reidentification by relative distance comparison</title>
		<author>
			<persName><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="653" to="668" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Occluded person re-identification</title>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhuang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangcong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Multimedia and Expo, ICME 2018</title>
				<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018">July 23-27. 2018. 2018</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
