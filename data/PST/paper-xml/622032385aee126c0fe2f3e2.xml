<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HyperPrompt: Prompt-based Task-Conditioning of Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-01">1 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yun</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Steven</forename><surname>Huaixiu</surname></persName>
							<email>stevenzheng@google.com</email>
						</author>
						<author>
							<persName><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
							<email>yitay@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Jai</forename><surname>Gupta</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Du</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vamsi</forename><surname>Aribandi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yaguang</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhao</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Heng-Tze</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Waymo</forename><surname>Llc</surname></persName>
						</author>
						<title level="a" type="main">HyperPrompt: Prompt-based Task-Conditioning of Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-01">1 Mar 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2203.00759v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Prompt-Tuning is a new paradigm for finetuning pre-trained language models in a parameter-efficient way. Here, we explore the use of HyperNetworks to generate hyper-prompts: we propose Hyper-Prompt, a novel architecture for prompt-based taskconditioning of self-attention in Transformers. The hyper-prompts are end-to-end learnable via generation by a HyperNetwork. HyperPrompt allows the network to learn task-specific feature maps where the hyper-prompts serve as task global memories for the queries to attend to, at the same time enabling flexible information sharing among tasks. We show that HyperPrompt is competitive against strong multi-task learning baselines with as few as 0.14% of additional task-conditioning parameters, achieving great parameter and computational efficiency. Through extensive empirical experiments, we demonstrate that HyperPrompt can achieve superior performances over strong T5 multi-task learning baselines and parameter-efficient adapter variants including Prompt-Tuning and HyperFormer++ on Natural Language Understanding benchmarks of GLUE and SuperGLUE across many model sizes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Prompt-Tuning <ref type="bibr" target="#b12">(Lester et al., 2021)</ref>, learning to condition large language models with soft learnable memory tokens, have recently garnered attention owing to their ability for parameter-efficient finetuning. Prompts are lightly tuned, allowing the model to be trained quickly since the main body of the * Equal contribution. Yun returned to TAMU, work done as an Intern at Google.  pretrained model is kept frozen. To this end, this paradigm is strongly reminiscent of adapter layers <ref type="bibr">(Houlsby et al., 2019a;</ref><ref type="bibr" target="#b10">Karimi Mahabadi et al., 2021;</ref><ref type="bibr">Zaken et al., 2021;</ref><ref type="bibr" target="#b7">He et al., 2021)</ref> which are also efficiently finetuned.</p><p>We introduce HyperPrompt, a natural but novel extension of Prompt-Tuning to multi-task learning (MTL) for language. HyperPrompt introduces taskconditioned hyper-prompts that conditions the model on task-specific information for constructing these prompts. Hyper-prompts are injected to the keys and values in the self-attention module, reminiscent of memory augmented Transformers <ref type="bibr" target="#b23">(Sukhbaatar et al., 2019)</ref>. This mitigates the cost of having prompts pass through the standard FFN layers in Transformers and serves as additional task-specific memory tokens for queries to attend to.</p><p>We further improve upon this by introducing taskaware and layer-aware HyperNetworks <ref type="bibr" target="#b5">(Ha et al., 2017)</ref> that parameterize and generate weights for the prompt generation process. The usage of HyperNetwork imbues our model with the necessary flexibility and expressiveness, especially when it comes to incorporating task-specific and layer-specific information to the network. Meanwhile, HyperPrompt remains very parameter and computational efficient and friendly to multi-task scaling: the additional parameters scale sub-linearly with, and are independent of the number of tasks in practice. While Hypernetworks have enjoyed some success in learning adapters <ref type="bibr" target="#b10">(Karimi Mahabadi et al., 2021;</ref><ref type="bibr" target="#b24">Tay et al., 2020)</ref> and/or continual learning <ref type="bibr" target="#b26">(von Oswald et al., 2019)</ref>, we note that this is the first exploration of HyperNetworks as a prompt generator.</p><p>Contrary to prior work, we additionally propose to finetune the entire network instead of only the hyperprompts. We make several compelling arguments for this. Firstly, <ref type="bibr" target="#b12">Lester et al. (2021)</ref> shows that parameter efficient Prompt-Tuning only shines for large (e.g., 11B) models and substantially pales in comparison to fine-tuning when the model is moderately parameterized (e.g., 220M). Secondly, finetuning only adaptive parameters (e.g., prompts/adapters) simply presents an illusion of efficiency <ref type="bibr" target="#b3">(Dehghani et al., 2021)</ref>. In reality, the FLOPs incurred by the model is still identical on the forward pass, which saves no compute during inference. Parameter counts, especially when including only prompts and adapters, are not the only measurement of computational efficiency. Instead, the FLOPs and training time should be considered together to provide a holistic view.</p><p>Our Contributions Overall, the main contributions include:</p><p>? We propose a novel HyperPrompt Transformer architecture with learnable hyper-prompts for multi-task fine-tuning with great parameter and computational efficiency. ? We demonstrate that for difficult tasks, it is crucial to fine-tune the task-specific parameters together with the backbone model to achieve Pareto efficiency on all tasks. ? We explore HyperNetworks as a prompt generator, and inject hyper-prompts into the selfattention module as global task memory tokens.</p><p>? HyperPrompt outperforms state-of-the-art parameter-efficient T5 models <ref type="bibr" target="#b18">Raffel et al. (2019)</ref> using Prompt-Tuning or adapters on well-established benchmarks such as Super-GLUE and GLUE, across all explored model sizes (see Figure <ref type="figure" target="#fig_1">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Statement</head><p>We consider the general setting of multi-task learning for a set of tasks {D ? } T ? =1 , where T is the total number of tasks and</p><formula xml:id="formula_0">{D ? } = {x (n) ? , y (n) ? } N?</formula><p>n=1 indicates the corresponding training set of the ? -th task with N ? samples. We assume that a pre-trained Transformer model f ? (?) (e.g., T5) is given, where the model is parameterized by ?. To tackle such multi-task learning problem with f ? (?), we minimize the following objective function</p><formula xml:id="formula_1">L(?) = T ? =1 N? n=1 C(f ? (x (n) ? ), y (n) ? ), where C(?, ?) is typically the cross-entropy loss and f ? (x (n) ? ) is the output for training sample x (n) ? .</formula><p>Transformer-based pre-trained language models such as <ref type="bibr">T5 Raffel et al. (2019)</ref> and <ref type="bibr" target="#b13">BART Lewis et al. (2020)</ref> are unified text-to-text frameworks where all tasks share the same encoder-decoder architecture -{{x</p><formula xml:id="formula_2">(n) ? } N? n=1 } T ? =1</formula><p>are fed into the same encoder and {{?</p><formula xml:id="formula_3">(n) ? } N? n=1 } T ? =1</formula><p>are generated by the same decoder. For such universal modules, multi-task learning simply corresponds to mixing task data sets together and there is no task-specific classification or regression networks for each task as in encoder-only modules <ref type="bibr" target="#b4">Devlin et al. (2019)</ref>; <ref type="bibr">Liu et al. (2019b)</ref>.</p><p>Previous work <ref type="bibr" target="#b18">Raffel et al. (2019)</ref> shows that colearning all tasks together on a pre-trained Transformer model is inferior to fine-tuning on each task separately. A possible reason is that ? is taskagnostic (i.e., all parameters are shared) and hence task-specific information is not well captured which can be especially true for low-resource tasks. Therefore, a natural way to improve the performance of Transformers on multi-task learning is to introduce a set of task-conditioned parameters {? ? } T ? =1 into f ? (.). The objective function can be updated as</p><formula xml:id="formula_4">L(?, {? ? } T ? =1 ) = T ? =1 N? n=1 C(f ?,?? (x (n) ? ), y (n) ? )</formula><p>, where ? ? is the task-specific parameterization for the ? -th task. During training, both ? and {? ? } T ? =1 are updated via back-propagation because we observe a large performance drop in SuperGLUE when backbone model ? is frozen and only task-conditioned parameters are tuned, as done in Karimi Mahabadi et al. ( <ref type="formula">2021</ref>), which will be detailed in Section 4.3.</p><p>To this end, our goal is to design task-conditioned parameterization of Transformer models to achieve greater parameter and computational efficiency as well as Pareto efficiency for multi-task learning.</p><p>More explicitly, we have two goals: (1) improving the finetuning performance of most tasks in {D ? } T ? =1 by introducing task-conditioned parameters {? ? } T ? =1 into f ? (.) and ( <ref type="formula">2</ref>) under the constraint that ? {? ? } T ? =1 0 ? 0 , which means that the model capacity will not be significantly increased. And the computational cost would not increase substantially either.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>In this section, we introduce MTL-Prompt and Hy-perPrompt (Figure <ref type="figure">2</ref>). We follow two key design principles to formulate HyperPrompt: (1) injecting task-conditioning into self-attention module for better computational efficiency and more expressive power, and (2) using HyperNetworks to simultaneously improve the parameter efficiency and allow a flexible degree of task sharing for better generalization. The intuition behind the first design principle is based on the fact that self-attention is the key to the power of Transformers via token-level interactions. Injecting task-conditioning into self-attention directly allows the learning of more expressive task-specific representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Prompt-Based Task-Conditioned Transformer</head><p>Previous adapter-based methods Karimi Mahabadi et al. (2021); <ref type="bibr" target="#b24">Tay et al. (2020)</ref> for multi-task learning normally add an adapter (i.e., dense-relu-dense network) for each task after the feed-forward layers at every Transformer block. Instead, the key idea of our approach is to prepend l task-conditioned trainable vectors to the keys and values of the multihead selfattention layer at every Transformer block, where the task-specific attention feature maps are jointly learned with the task-agnostic representation.</p><p>The idea of prepending learnable prompts to the network is explored before by <ref type="bibr" target="#b14">Li &amp; Liang (2021)</ref>; <ref type="bibr" target="#b12">Lester et al. (2021)</ref>; <ref type="bibr">Liu et al. (2021)</ref> for singletask fine-tuning. We first introduce and expand this idea for multi-task learning in this subsection. In Sec.3.2, we design a novel method called MTL-Prompt following the design principle #1 of injecting hyper-prompts into self-attention. In Sec.3.3, we further add on-top design principle #2 to improve MTL-Prompt using HyperNetworks as generators for hyper-prompts.</p><p>At a multihead self-attention layer, the original key, value and query are calculated as</p><formula xml:id="formula_5">K ? = X ? W k , V ? = X ? W v , Q ? = X ? W q , where X ? ? R L?d is the input sequence of a training sample from the ? -th task, L is the sequence length, d is the model dimension. W k ? R d?h?d h , W v ? R d?h?d h and W q ? R d?h?d h project the input into original key K ? ? R L?h?d h , value V ? ? R L?h?d h and query Q ? ? R L?h?d h ,</formula><p>h is the number of heads, d h is the dimension of each head and typically set to d/h to save parameters.</p><p>To learn the task-specific information for the ? -th task, we have l trainable d-dimensional vectors as the hyper-prompts for the key and the value respectively, denoted as P ?,k ? R l?h?d h and P ?,v ? R l?h?d h , as shown in Figure <ref type="figure">2(a)</ref>. Then, the hyper-prompts are concatenated with the original key and value:</p><formula xml:id="formula_6">K ? = concat(P ?,k , K ? )</formula><p>(1)</p><formula xml:id="formula_7">V ? = concat(P ?,v , V ? ) (2)</formula><p>where the new key (value) K ? (V ? ) ? R (l+L)?h?d h are used to compute the multihead self-attention.</p><p>The hyper-prompts benefit Transformers for multitask learning in two ways: (1) Prompt for key P ?,k is prepended with the original key and will participate in the calculation of attention feature map: softmax(Q ? K T ? ). P ?,k directly interacts (matrix multiplication) with the original query Q ? , allowing tokens to acquire task-specific semantics. (2) Prompt for value P ?,v is prepended with the original value and will be absorbed into the self-attention output O ? , where each position in O ? is the weighted-sum of vectors in V ? with weights from the attention scores. This way, P ?,v can serve as task-specific memories for multihead attention to retrieve information from.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MTL-Prompt</head><p>How to obtain the prompts for the m-th Transformer block? A straightforward way is to directly initialize P m ?,k and P m ?,v . However, this way is parameterinefficient, as it scales linearly with both the number of tasks T and the number layers M as O(T ? M ).</p><p>Instead, we initialize a global 1 prompt P ? for each task and apply projection networks at every Transformer block to project this prompt into {P m ?,k } M m=1 and {P m ?,v } M m=1 . Global Prompts. Specifically, we initialize a set of global prompts {P ? } T ? =1 , where P ? ? R l?d is a trainable matrix to learn the task-specific information 1 we term it global because it is independent of the layer number as opposed to layer-dependent prompt P m ? .</p><p>Scaled Dot-Product Attention</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concat</head><p>Linear</p><formula xml:id="formula_8">Q K V P V P K Multi-Head Attention U K,V RELU P D K,V P V P K Projection Network Hyper Prompts Global Prompts (a) Global HyperNetwork h k,v U K,V D K,V I Layer-Aware Task Embedding MTL-Prompt HyperPrompt (b) (c)</formula><p>Figure <ref type="figure">2</ref>: HyperPrompt framework: (a) in each Transformer block, task-specific hyper-prompts P K,V are prepended to the original key K and value V for the query Q to attend to, (b) in MTL-Prompt, global prompts P is used to generate the hyper-prompts P K,V through a projection network, which consists of a down-projection matrix D K,V , a RELU layer and a up-project matrix U K,V , (c) in HyperPrompt, all the projection matrices are generated by global HyperNetworks h k,v using layer-aware task embeddings I as task-specific inputs (see Section 3.3 for details).</p><p>of the ? -th task, d is the model dimension and l is the length of the prompt. Projection Networks. At the m-th Transformer block, we apply two projection networks to transform the global prompt P ? into layer-specific and taskspecific prompts as shown in Figure <ref type="figure">2(b)</ref>:</p><formula xml:id="formula_9">P m ?,k = U m k (Relu(D m k (P ? ))),<label>(3)</label></formula><formula xml:id="formula_10">P m ?,v = U m v (Relu(D m v (P ? ))),<label>(4)</label></formula><p>where P m ?,k/v ? R l?h?d h . We call these generated prompts hyper-prompts to distinguish from global prompts.</p><p>In particular, to limit the number of parameters, the projection networks are designed using a bottleneck architecture: Despite the saving of parameters, one drawback of MTL-Prompt-Share is that the task conflicts could arise given the limited model capacity <ref type="bibr">Wu et al. (2020)</ref>; <ref type="bibr" target="#b29">Wang et al. (2020)</ref> of the shared projection networks.</p><formula xml:id="formula_11">D m k/v ? R d?b and U m k/v ? R b?h?d h are</formula><p>MTL-Prompt-Sep. In the opposite extreme of MTL-Prompt-Share, each task can have its own pro-jection networks as following:</p><formula xml:id="formula_12">P m ?,k = U m ?,k (Relu(D m ?,k (P ? ))),<label>(5)</label></formula><formula xml:id="formula_13">P m ?,v = U m ?,v (Relu(D m ?,v (P ? ))),<label>(6)</label></formula><p>where D m ?,k/v and U m ?,k/v are down-projection and up-projection matrices for the ? task, respectively. In this case, each task hyper-prompt is trained independently and hence there is no information sharing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">HyperPrompt</head><p>We further propose a novel design of HyperPrompt to flexibly share information among tasks while maintaining a low parameter cost, inspired by <ref type="bibr">Hyper-Former++ Karimi Mahabadi et al. (2021)</ref>. As shown in Figure <ref type="figure">2</ref>(c), the key idea of HyperPrompt is to generate the project networks using the same shared global HyperNetwork for all tasks.</p><p>Layer-Aware Task Embedding. Following the same recipe in Karimi Mahabadi et al. ( <ref type="formula">2021</ref>), we define a layer-aware task embedding for better generalization. Let k ? ? R t denote the task embedding for the ? task and t is the dimension. To capture the layer-specific information, layer embedding z m ? R t is introduced. After that, a task projection network h t (?, ?) is applied to fuse the task embedding and the layer embedding into the final layer-awared task embedding I m ? = h t (k ? , z m ), where I m ? is the input to the shared global HyperNetworks as shown in Figure <ref type="figure" target="#fig_4">1(c</ref>). h t is a MLP consisting of two feed-forward layers and a ReLU non-linearity, which takes the concatenation of k ? and z m as input.</p><p>Global HyperNetworks. h k (?) generates the weight matrices in the projection networks of key prompts (U m ?,k , D m ?,k ) and another HyperNetwork</p><formula xml:id="formula_14">h v (?) generates (U m ?,v , D m ?,v ) of value prompts: (U m ?,k , D m ?,k ) = h k (I m ? ) = (W U k , W D k )I m ? , (7) (U m ?,v , D m ?,v ) = h v (I m ? ) = (W Uv , W Dv )I m ? ,<label>(8)</label></formula><p>where I m ? ? R t is the layer-aware task embedding for the ? task at the m-th block.</p><formula xml:id="formula_15">W D k ? R (d?b)?t , W Dv ? R (d?b)?t , W U k ? R (b?h?d h )?t and W Uv ? R (b?h?d h )?t are the weight matrices of h k (?) and h v (?).</formula><p>Given that U m ?,k/v , and D m ?,k/v are generated by the global HyperNetworks, we project the global prompts P ?,k/v into hyper-promtps P m ?,k/v following Eqs. 5 and 6. Finally, the hyper-prompts P m ?,k/v are prepended with original key and value at every selfattention layer as shown in Figure <ref type="figure" target="#fig_1">1</ref>(a) to calculate the task-conditioned attention scores.</p><p>Using global HyperNetworks to generate the projection networks has two benefits:</p><p>1. It enables a more flexible way to share information across tasks and layers: the transformation matrices are decomposed into h k/v (?) that are shared by all tasks and all layers. Therefore, the model can adjust the degree of information sharing across tasks and layers through learning the appropriate parameter values in h k/v (?) during the end-to-end training.</p><p>2. A parameter-efficient task conditioned parameterization is enabled. The number of extra task-conditioned parameters doesn't depend on the number of layers M , and scales sublinearly with respect to the total number of tasks T . In practice, since task embeddings and task prompts have far fewer parameters than the global HyperNetworks, the additional task-conditioned parameters is almost independent of T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Parameter Efficiency of Hyper-Prompt</head><p>As shown in A.1, the total number of additional parameters from HyperPrompt is dlT + 4(bdt) + T t + M t + (2t + t)e, where d is the model dimension, l is the length of the prompts, T is the total number of tasks, b is the bottleneck dimension of the projection networks, d is the model dimension, t /t is the dimension of the raw/final layer-aware task embedding, and e is the hidden dimension of h k/v . Therefore, the space complexity is O(d(lT + 4bt)), given that in practice M ? T , t dl, and e bd. This leads to a sub-linear scaling with respect to T . Furthermore, T is typical ? O(10) for multi-task learning. A reasonable l ? O( <ref type="formula">10</ref>) is required to achieve the optimal performance, which will be detailed in Section 4.7. On the other hand, typical values for b ? 24 and t ? 32, and therefore 4bt lT is satisfied in most cases. Hence, the space complexity could be further simplified as O(bdt). In conclusion, the space complexity of HyperPrompt mainly comes from the global HyperNetworks and is practically independent of the prompt length l, the number of Transformer layers M , and the number of tasks T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Datasets. We evaluate the performance of the models on GLUE <ref type="bibr" target="#b27">Wang et al. (2018)</ref> and SuperGLUE <ref type="bibr" target="#b28">Wang et al. (2019)</ref> respectively. Each of them is a collection of text classification tasks to test the general language understanding ability. Specifically, the tasks include: sentence acceptability (CoLA), sentiment analysis (SST-2), paraphrasing/sentence similarity (MRPC, STS-B and QQP), natural language inference (MNLI, QNLI, RTE and CB), coreference resolution (WSC), sentence completion (COPA), word sense disambiguation (WIC) and question answering (MultiRC and ReCoRD, BoolQ).</p><p>Transformers. Evaluation. We save a checkpoint every 2000 steps for all models and follow the same convention as <ref type="bibr" target="#b18">Raffel et al. (2019)</ref> in selecting the best checkpoint for each task. The emphasis of our evaluation is not to find the best single checkpoint for all tasks but to test the model's ability of transfer learning among the co-trained tasks. We first calculate the average of all metrics for each task and then report the average of all tasks for GLUE and SuperGLUE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Following previous work</head><p>Baselines. We compare our proposed MTL-  <ref type="formula">2021</ref>) is originally for parameter-efficient single-task fine-tuning and only prepends prompts to the input word embeddings in the first layer.</p><p>We slightly modify it by initializing and prepending prompts for each task respectively so that Prompt-Tuning can be applied to multi-task learning.</p><p>We defer additional details of the experiments to A.2  <ref type="figure" target="#fig_1">1</ref>), but substantially pales for moderate model sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Key Results</head><p>Our HyperPrompt architecture when fully finetuned achieves state-of-the-art performance on Su-perGLUE across four different model sizes. Competitive adapter-tuning variants including Prompt-Tuning and HyperFormer++ can either match or slightly improve upon the multi-task learning (MTL) baseline on the SuperGLUE dataset. In contrast, HyperPrompt outperforms the strong MTL baseline by a large margin on SuperGLUE score (78.9 vs 77.2 for T5 Base). Interestingly, such a performance gain continues all the way to model size as big as XXL (e.g. 91.3 vs 90.2) with only 0.14% additional parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Tuning all vs Task-Conditioned Parameters</head><p>Recently, Karimi <ref type="bibr" target="#b10">Mahabadi et al. (2021)</ref> show that only tuning adapters can be competitive against the full fine-tuning. However, the evaluation is conducted only on the GLUE with smaller models including T5 Small and Base. In the experiments, we first compare tuning the full model (the backbone model plus task-conditioned parameters) vs. only task-conditioned parameters.</p><p>Table <ref type="table">1</ref> shows the comparison on the GLUE and SuperGLUE average scores using T5 large (for pertask performance, please refer to A.4). For GLUE, the observation is consistent with Karimi Mahabadi et al. ( <ref type="formula">2021</ref>), where task-specific only fine-tuning of HyperFormer++ and HyperPrompt is comparable to the MTL baseline. However, on SuperGLUE, we observe a large performance drop: the average score drops by 5.5 and 5.9 for HyperPrompt and HyperFormer++, respectively.</p><p>Therefore, these experiments show that only tuning the task-conditioned parameters is not enough to achieve competitive results as full model training for multi-task learning on high-difficulty tasks such as SuperGLUE. This is consistent with the results of Prompt-Tuning <ref type="bibr" target="#b12">Lester et al. (2021)</ref>. Hence, the rest of the experiments are conducted with tuning all model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Computational Efficiency</head><p>Table <ref type="table">2</ref> presents the computational efficiency analysis of the Adapter/Prompt models. HyperPrompt (together with MTL-Prompt-Share) has the lowest # Ops since hyper-prompts are injected into selfattention and skip the standard FFN layers. In contrast, HyperFormer++ has ? 3x # Ops compared to other variants. Regarding training time, MTL-Prompt-Share is fastest given that the projection networks are shared across tasks. Vanilla Adapter and HyperPrompt are comparable while Hy-perFormer++ and Prompt-Tuning take significant longer to do the full fine-tuning. This shows the computational efficiency of HyperPrompt for both training and inference, in addition to the parameter efficiency. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head><p>Table <ref type="table">3</ref> presents the results on T5 Base and Table <ref type="table">4</ref> presents the results on T5 Large (see more detailed results in A.4). HyperPrompt outperforms all baselines in terms of the average score of GLUE and SuperGLUE.</p><p>HyperPrompt vs. Prompt-Tuning. The original Prompt-Tuning Lester et al. ( <ref type="formula">2021</ref>) is for singletask fine-tuning. To be parameter-efficient, it only trains the prompts but freezes the backbone T5. To make a fair comparison, we modify Prompt-Tuning by (1) training both prompts and backbone, and (2) adding prompt to each task and co-train all tasks together. As shown in Table <ref type="table">3</ref> and<ref type="table">4</ref>, HyperPrompt outperforms Prompt-Tuning by 2.0 (0.6) and 1.6 (1.4) on GLUE and SuperGLUE using T5 Base (Large), respectively. HyperPrompt improves upon Prompt-Tuning in two places: (1) Prompt-Tuning only adds prompts to the word embedding layer while Hyper-Prompt adds hyper-prompts at every Transformer layer and hence is more expressive; and (2) Prompts of tasks are trained independently in Prompt-Tuning while HyperPrompt enables a flexible information sharing via the HyperNetworks.</p><p>HyperPrompt vs. HyperFormer++. Hyper-Prompt is superior to the state-of-the-art baseline Hy-perFormer++ in the average score of GLUE and Su-perGLUE for both Base and Large T5 model. For ex-  <ref type="table">4</ref>). Note that the main difference between the two methods is that HyperPrompt inserts the taskconditioned parameters as prompts into self-attention layers while HyperFormer++ insert adapters after each block. We believe task-conditioning in selfattention gives more expressive power than in the feed-forward network as done in adapters. Hyperprompts that are prepended with the key and value participate in the attention interactions between different token positions, which helps the model to better capture the task-dependent semantics.</p><p>HyperPrompt vs. MTL Next, we observe that using HyperPrompt can greatly improve the performance upon the vanilla Transformer model (referred to MTL): 1.7 (1.1) gain on SuperGLUE score for T5 Base (Large) with 4% (2%) additional paramters. In conclusion, the experiments show that Hyper-Prompt is a parameter-efficient and effective taskconditioned parameterization of Transformers for multi-task learning.</p><p>HyperPrompt vs. MTL-Prompt. Interestingly, MTL-Prompt-Share is better than MTL-Prompt-Sep on the SuperGLUE on both Base and Large models while the opposite is true for GLUE. Notice that all tasks share the same two projection networks in MTL-Prompt-Share while each task has its own projection networks in MTL-Prompt-Sep. More importantly, we observe that HyperPrompt, where the projection networks are generated by the global HyperNetworks, always achieves the best performance on both GLUE and SuperGLUE. Hence, the experiments show that HyperPrompt can adjust the degree of information sharing for better multitask generalization, compared to the two variants of MTL-Prompt in the extremes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Peeking into Hyper-Prompts</head><p>To shed light on how hyper-prompts help improve the multi-task generalization via task-conditioning, we peek into HyperPrompt models by looking at the distribution of attention scores. We choose the GLUE task MRPC as an example. To avoid biasing on individual examples, we aggregate over 100 validation examples to compute the quantity of interest (see A.3 for details). First, we compute the attention mass on hyper-prompts for each encoder layer. Figure <ref type="figure" target="#fig_5">3</ref> (top) shows that the network has lower attention mass on hyper-prompts in the lower layers and gradually increases attention mass for higher layers. This phenomenon indicates that higher-levels of Transformer becomes more task-specialized while it is beneficial for the lower-levels to learn task-agnostic representation Yosinski et al. ( <ref type="formula">2014</ref>) by casting lower attention mass on hyper-prompts. Furthermore, we calculate the entropy of the attention scores on the tokens. For HyperPrompt, we remove the hyperprompts from the calculation and re-normalize the attention scores on the tokens to make a fair comparison with the MTL baseline. Figure <ref type="figure" target="#fig_5">3</ref> (bottom) shows a shift of entropy distribution towards higher values for HyperPrompt. This signifies that injecting hyper-prompts encourages a more diverse attention distribution, which seems to be beneficial to model generalization.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Impact of Hyper-Prompt Length</head><p>HyperPrompt prepends l trainable hyper-prompts to the keys and values of self-attention layer at every Transformer layer. In Figure <ref type="figure" target="#fig_6">4</ref>, we present the results of tuning the prompt length l on GLUE using T5 Base as the example (similar patterns are observed on T5 Large and SuperGLUE). We first add hyperprompts on the decoder and search the best l and then search the best l for the encoder with the fixed best decoder hyper-prompt length. As shown in Figure <ref type="figure" target="#fig_6">4</ref>(a), l = 6 is the best for the decoder. As shown in Figure <ref type="figure" target="#fig_6">4</ref>(b), HyperPrompt achieves the best result of 86.8 when l = 16 on the encoder with l = 6 fixed for the decoder. The experiments show that hyper-prompts with length l ? O(10) are good enough to achieve superior performance. Note that the original sequence length is 512 on the encoder and 32 on the decoder. Therefore, HyperPrompt does not substantially increase the time complexity of self-attention layers in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Encoder vs Decoder</head><p>To understand the effect of adding task-conditioned parameters to different parts of the network, we present the results of HyperPrompt and Hyper-Former++ with adding hyper-prompts/adapters to:</p><p>(1) encoder-only, (2) decoder-only, and (3) both encoder-decoder. As shown in Table <ref type="table">5</ref>, we observe adding task-conditioned parameters to encoder (encoder-only) performs better than decoder-only on GLUE. However, the opposite is true for Super-GLUE, where encoder-only is substantially worse than decoder-only. This potentially could be a trainability issue when prompts are inserted into encoders, i.e. a different learning rate might be required to learn the prompt parameters from scratch. We leave this investigation as a future work. Based on this experiment, we add task-conditioned parameters to the decoder for SuperGLUE in our experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Prompt-Tuning. Prompt tuning is becoming a new paradigm for adapting pre-trained generalpurpose language models to downstream tasks, as a lightweight alternative to the popular fine-tuning approach. Here, we use the term Prompt-Tuning to cover a general family of methods following the prompting idea in GPT-3 <ref type="bibr">Brown et al. (2020)</ref>. To avoid manually design the prompts, recent efforts have focused on search for discrete prommpting words automatically <ref type="bibr" target="#b22">Shin et al. (2020)</ref>  <ref type="formula">2021</ref>) is an alternative approach for parameter-efficient lightweight tuning of pre-trained langauge models for downstream tasks. Task-specific adapter layers <ref type="bibr">Houlsby et al. (2019a)</ref> are inserted into the Transformer block for fine-tuning while the rest of the backbone model is frozen. By adding only a few percent of additional parameters, Karimi <ref type="bibr" target="#b10">Mahabadi et al. (2021)</ref> show that competitive performance can be obtained on NLU benchmarks such as GLUE <ref type="bibr" target="#b27">Wang et al. (2018)</ref>. However, one limitation from the existing work is the evaluation of NLU on GLUE dataset, which is known to be no longer suitable for measuring the progress of language understanding <ref type="bibr" target="#b28">Wang et al. (2019)</ref>. In our work, we evaluate HyperPrompt on SuperGLUE in addition to GLUE dataset, and show that indeed higher-difficulty tasks such as SuperGLUE requires full-tuning of the model beyond adapter tuning, to be competitive against state-of-the-art multi-task baselines. We also demonstrate that it is advantageous to inject prompts into self-attention than adding adapters.</p><p>Multi-task Natural Language Understanding. Multi-task learning is an important and challenge research direction in both full fine-tuning and prompt-tuning paradigms because of the competing needs of training and serving a single model while achieving Pareto efficiency in all tasks.</p><p>The T5 model <ref type="bibr" target="#b18">Raffel et al. (2019)</ref> renders all NLP tasks as a Text-to-Text problem. However, the best results are obtained by task-specific finetuning. MTDNN (multi-task deep neural network) <ref type="bibr">Liu et al. (2019a)</ref> shares parameters between several NLP tasks, and achieves strong performance on the GLUE benchmark. <ref type="bibr" target="#b0">Aghajanyan et al. (2021)</ref> use around 50 tasks to boost the multi-task learning performance. <ref type="bibr" target="#b1">Aribandi et al. (2021)</ref> builds an extremely diverse set of 107 NLP tasks for extreme multi-task scaling and demonstrate superior performances on a wide range of benchmarks. Recently, <ref type="bibr" target="#b30">Wei et al. (2021)</ref>; <ref type="bibr" target="#b19">Sanh et al. (2021)</ref> also illustrated how a multitask learning stage can greatly improve the zero-shot prompting performance of large language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose a novel architecture for prompt-based task-conditioning of self-attention in Transformers. The hyper-prompts are generated by a HyperNetwork to enable flexible information sharing among tasks while remain efficient in parameters and computation. HyperPrompt allows the network to learn task-specific feature maps where the hyper-prompts serve as task global memories, encouraging a more diverse distribution of attention. Extensive experiments show that HyperPrompt can achieve superior performances over strong T5 multi-task learning Learning Representations, 2020. URL https: //openreview.net/forum?id=SylzhkBtDB. Yosinski, J., Clune, J., Bengio, Y., and Lipson, H.</p><p>How transferable are features in deep neural networks? In Advances in neural information processing systems, pp. 3320-3328, 2014.</p><p>Zaken, E. B., Ravfogel, S., and Goldberg, Y. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. arXiv preprint arXiv:2106.10199, 2021.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>This section covers the parameter count of HyperPrompt, the experimental details, the calculation of attention mass and entropy, and per-task performance of GLUE and SuperGLUE.</p><p>A.1 Parameter Count of HyperPrompt ( ?3.4)</p><p>Since the encoder and the decoder of Transformers have approximately the same capacity, the calculation considers only the decoder-side for simplicity. First, we have global task prompts P ? ? R l?d for the ? -th task, which contains dlT parameters for T tasks. The global HyperNetworks contain four weight matrices  <ref type="bibr">(2019)</ref>. Following <ref type="bibr" target="#b18">Raffel et al. (2019)</ref>, all data are preprocessed as into a "sequence-to-sequence" format. The length of the sequence is 512 at the encoder and 32 at the decoder. For all experiments, we train models 300K steps with a batch size of 128 and each batch is a mixture which samples each task proportionately to the number of examples in the dataset. Learning rate is a constant of 1e-3 with Adafactor optimizer <ref type="bibr" target="#b20">(Shazeer &amp; Stern, 2018)</ref>.</p><formula xml:id="formula_16">W D k ? R (d?b)?t , W Dv ? R (d?b)?t , W U k ? R (b?h?d h )?t and W Uv ? R (b?h?d h )?t ,</formula><p>For hyper-parameters tuning, the length of prompt l is selected from {12, 16, 20, 20, 24} at the encoder and {2, 4, 6, 8, 10, 12, 14, 16} at the decoder. The bottleneck dimension b in the transform matrices is set to d/r, where d is the model dimension of the T5 models and r is a reduction factor and selected from {16, 32, 64}. The dimension t of the layer-aware task embedding is selected from {32, 64, 128}. For a fair comparison, the hyper-parameters of baseline methods are set to have approximately the same numbers of parameters as HyperPrompt with the exception that Prompt-Tuning and MTL-Prompt-Share are extremely parameter-efficient with significantly fewer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Attention Mass and Entropy calculation ( ?4.6)</head><p>To calculate the attention mass over hyper-prompts per layer, we averaged the hyper-prompt attention softmax scores across 100 validation examples and each attention head in a layer, and summed across each query attending to the hyper-prompts. In other words, we aggregated the amount of attention given to hyper-prompts by queries. To calculate the attention entropy over tokens (other than hyper-prompts), we calculated the entropy of the attention distributions (averaged across attention heads) for 100 validation examples. This results in 100 n=1 12 L=1 |X n | entropies calculated and visualized in Figure <ref type="figure" target="#fig_5">3</ref> (bottom). For the HyperPrompt model, this involved re-normalizing the softmax distribution after removing hyper-prompts, as we wanted to understand how the original tokens are attended to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Per-Task Performance of GLUE and SuperGLUE</head><p>Table <ref type="table">6</ref> and 7 below show the comparison of fine-tuning the entire model against task-specific parameters only on GLUE and SuperGLUE datasets. Table <ref type="table" target="#tab_9">8</ref> and<ref type="table" target="#tab_10">9</ref> show the detailed results of full-tuning of HyperPrompt against baselines on T5 Base. Table <ref type="table" target="#tab_11">10</ref> and 11 show the detailed results of full-tuning of HyperPrompt against baselines on T5 Large. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: HyperPrompt achieves state-of-the-art performance on SuperGLUE for T5 models up to XXL. Prompt-tuning Lester et al. (2021) with tuning prompt parameters only achieves competitive performance against multi-task learning (MTL) baseline for the 11B parameter model with a big performance gap for smaller models. HyperPrompt outperforms the strong parameter-efficient adapter variant Hyper-Former++ Karimi Mahabadi et al. (2021), the MTL baseline, and the full fine-tuning of Prompt-Tuning (our implementation) across model sizes with a large margin [e.g. 91.3 vs 90.2 (MTL) for T5 XXL].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>down-projection and up-projection matrices, respectively. b is the bottleneck dimension satisfying b d. MTL-Prompt-Share. We first have all tasks share the same two projection networks defined by the down-project matrices D m k and D m v , and the up-project matrices U m k and U m v . We refer to this design choice as MTL-Prompt-Share.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc><ref type="bibr" target="#b10">Karimi Mahabadi et al. (2021)</ref> and<ref type="bibr" target="#b24">Tay et al. (2020)</ref>, our models are built on top of the state-of-the-art Transformer modelT5 Raffel et al. (2019), which uses encoder-decoder architecture from<ref type="bibr" target="#b25">Vaswani et al. (2017)</ref>. We use already pre-trained T5 with sizes from Base (220M parameters) to XXL (11B).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 1</head><label>1</label><figDesc>Figure1provides an overall summary of the results of HyperPrompt. Previous prompt-tuning<ref type="bibr" target="#b12">Lester et al. (2021)</ref>;<ref type="bibr" target="#b14">Li &amp; Liang (2021)</ref> methods focus on parameter-efficient single-task fine-tuning and hence freeze the backbone Transformers and only fine-tune the prompts. Their experiments show that the performance of only tuning the prompts can match the full model training with a very large 11B model (Figure1), but substantially pales for moderate model sizes.Our HyperPrompt architecture when fully finetuned achieves state-of-the-art performance on Su-perGLUE across four different model sizes. Competitive adapter-tuning variants including Prompt-Tuning and HyperFormer++ can either match or slightly improve upon the multi-task learning (MTL) baseline on the SuperGLUE dataset. In contrast, HyperPrompt outperforms the strong MTL baseline by a large margin on SuperGLUE score (78.9 vs 77.2 for T5 Base). Interestingly, such a performance gain continues all the way to model size as big as XXL (e.g. 91.3 vs 90.2) with only 0.14% additional parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Visualization of attention mass and entropy distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Impact of hyper-prompt length in Hyper-Prompt (GLUE score on T5 Base).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Prompt and HyperPrompt with vanilla T5 models<ref type="bibr" target="#b18">Raffel et al. (2019)</ref> for multi-task learning, which is referred to MTL. Another baseline is Vanilla Adapter proposed inHoulsby et al. (2019b)  that add adapters modules for each task after each of the the two feedforward modules in each Transformer block of the T5 model. The state-of-the-art adapter-based method for multi-task learning is HyperFormer++ proposed in Karimi Mahabadi et al. (2021) that use Hyper-Networks to generate adapters for each task and add them after the feed-forward modules followingHoulsby et al. (2019b). In addition, Prompt-TuningLester et al. (</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>which result in 4(bdt) parameters (we let d = h ? d h ). To obtain layer-aware task embedding, HyperPrompt learns task embedding k ? ? R t for the ? task and layer embedding z m ? R t for the m-th Transformer block, which in total results in T t + M t parameters. Besides, a task projection network h t is applied to fuse the task embedding and the layer embedding into the final layer-aware task embedding I m ? ? R t . h t is a two-layer feed-forward networks and contains (2t + t)e parameters, where e is the hidden dimension for h t .Our models were implemented using Mesh Tensorflow 2 Shazeer et al. (2018) with the T5 library 3 Raffel et al.</figDesc><table><row><cell>A.2 Experimental Details ( ?4.1)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Comparison of fine-tuning all vs task-specific parameters on SuperGLUE.</figDesc><table><row><cell>Model</cell><cell cols="3">#Params CoLA SST-2</cell><cell>MRPC</cell><cell>SST-B</cell><cell>QQP</cell><cell>MNLI</cell><cell cols="3">QNLI RTE AVG</cell></row><row><cell>MTL</cell><cell>1.0x</cell><cell>49.8</cell><cell>94.6</cell><cell>92.5/89.8</cell><cell>90.7/90.5</cell><cell cols="2">89.2/91.9 88.8/88.5</cell><cell>93.3</cell><cell>85.0</cell><cell>85.5</cell></row><row><cell>Vanilla Adapter</cell><cell>1.06x</cell><cell>60.0</cell><cell>95.4</cell><cell>92.7/89.8</cell><cell>90.2/90.2</cell><cell cols="2">89.3/91.9 88.5/88.1</cell><cell>93.5</cell><cell>84.4</cell><cell>86.7</cell></row><row><cell>HyperFormer++</cell><cell>1.04x</cell><cell>56.9</cell><cell>94.8</cell><cell>92.9/90.1</cell><cell cols="2">91.1/90.9 88.9/91.7</cell><cell>88.7/88.3</cell><cell>93.4</cell><cell>85.6</cell><cell>86.5</cell></row><row><cell>Prompt-Tuning</cell><cell>1.0003x</cell><cell>48.0</cell><cell>95.0</cell><cell>92.2/89.0</cell><cell>90.3/90.2</cell><cell cols="2">89.0/91.7 88.8/88.5</cell><cell>93.2</cell><cell>82.9</cell><cell>84.8</cell></row><row><cell>MTL-Prompt-Share (ours)</cell><cell>1.008x</cell><cell>56.2</cell><cell>94.7</cell><cell>93.0/90.4</cell><cell>90.6/90.4</cell><cell>89.2/91.9</cell><cell>88.7/88.4</cell><cell>93.4</cell><cell>85.2</cell><cell>86.4</cell></row><row><cell>MTL-Prompt-Sep (ours)</cell><cell>1.06x</cell><cell>57.2</cell><cell>94.6</cell><cell>93.8/91.4</cell><cell>91.0/90.8</cell><cell>89.2/91.9</cell><cell>88.5/88.4</cell><cell>93.4</cell><cell cols="2">86.6 86.8</cell></row><row><cell>HyperPrompt (ours)</cell><cell>1.04x</cell><cell>57.0</cell><cell>95.2</cell><cell>93.4/90.9</cell><cell>90.4/90.2</cell><cell cols="2">89.2/92.0 88.7/88.5</cell><cell>93.4</cell><cell cols="2">87.1 86.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Comparison of HyperPrompt with baselines on GLUE using T5 Base.</figDesc><table><row><cell>Model</cell><cell cols="2">#Params BoolQ</cell><cell>CB</cell><cell cols="2">COPA MultiRC</cell><cell>ReCoRD</cell><cell cols="4">RTE WIC WSC AVG</cell></row><row><cell>MTL</cell><cell>1.0x</cell><cell>82.6</cell><cell>93.4/93.5</cell><cell>65.7</cell><cell>76.7/39.7</cell><cell>80.9/80.2</cell><cell>85.6</cell><cell>70.5</cell><cell>81.4</cell><cell>77.2</cell></row><row><cell>Vanilla Adapter</cell><cell>1.03x</cell><cell>83.5</cell><cell>93.4/94.6</cell><cell>65.3</cell><cell cols="4">77.6/42.7 81.0/80.2 88.2 71.0</cell><cell>76.9</cell><cell>77.5</cell></row><row><cell>HyperFormer++</cell><cell>1.02x</cell><cell>83.5</cell><cell>96.2/97.0</cell><cell>66.3</cell><cell cols="2">77.8/41.9 81.2/80.4</cell><cell>87.4</cell><cell>71.0</cell><cell>80.1</cell><cell>78.2</cell></row><row><cell>Prompt-Tuning</cell><cell>1.0003x</cell><cell>82.5</cell><cell>94.0/95.8</cell><cell>68.0</cell><cell>76.9/40.2</cell><cell>80.9/80.2</cell><cell>84.1</cell><cell>69.3</cell><cell>80.8</cell><cell>77.3</cell></row><row><cell>MTL-Prompt-Share (ours)</cell><cell>1.004x</cell><cell>83.1</cell><cell>95.7/95.2</cell><cell>67.7</cell><cell cols="3">77.3/41.3 81.9/81.0 87.4</cell><cell>70.4</cell><cell>80.8</cell><cell>78.2</cell></row><row><cell>MTL-Prompt-Sep (ours)</cell><cell>1.03x</cell><cell>83.3</cell><cell>97.8/97.0</cell><cell>61.7</cell><cell>77.6/42.3</cell><cell>81.5/80.6</cell><cell cols="3">86.8 71.4 78.2</cell><cell>77.5</cell></row><row><cell>HyperPrompt (ours)</cell><cell>1.02x</cell><cell>83.3</cell><cell>96.6/96.4</cell><cell>69.7</cell><cell cols="3">77.5/41.0 81.7/80.9 86.8</cell><cell cols="3">70.5 83.7 78.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Comparison of HyperPrompt with baselines on SuperGLUE using T5 Base.</figDesc><table><row><cell>Model</cell><cell cols="3">#Params CoLA SST-2</cell><cell>MRPC</cell><cell>SST-B</cell><cell>QQP</cell><cell>MNLI</cell><cell cols="3">QNLI RTE AVG</cell></row><row><cell>MTL</cell><cell>1.0x</cell><cell>59.4</cell><cell>96.6</cell><cell>93.3/90.7</cell><cell>90.6/90.4</cell><cell>89.8/92.3</cell><cell>90.8/90.8</cell><cell>95.2</cell><cell>90.8</cell><cell>88.3</cell></row><row><cell>Vanilla Adapter</cell><cell>1.06x</cell><cell>63.8</cell><cell>96.5</cell><cell>93.7/91.3</cell><cell cols="3">92.0/91.9 90.0/92.5 90.6/90.5</cell><cell>94.9</cell><cell>88.7</cell><cell>88.8</cell></row><row><cell>HyperFormer++</cell><cell>1.02x</cell><cell>63.3</cell><cell>96.6</cell><cell cols="3">93.2/90.7 92.1/91.9 89.7/92.3</cell><cell>90.5/90.7</cell><cell>95.1</cell><cell>89.9</cell><cell>88.8</cell></row><row><cell>Prompt-Tuning</cell><cell>1.0001x</cell><cell>62.5</cell><cell>96.7</cell><cell>93.4/91.0</cell><cell>91.3/91.0</cell><cell cols="3">90.0/92.4 90.9/91.0 95.4</cell><cell>89.9</cell><cell>88.8</cell></row><row><cell>MTL-Prompt-Share (ours)</cell><cell>1.008x</cell><cell>65.0</cell><cell>96.7</cell><cell>93.8/91.6</cell><cell>91.1/90.8</cell><cell cols="2">90.0/92.4 90.8/91.1</cell><cell>95.3</cell><cell>91.3</cell><cell>89.3</cell></row><row><cell>MTL-Prompt-Sep (ours)</cell><cell>1.06x</cell><cell>63.9</cell><cell>96.6</cell><cell cols="2">94.6/92.6 92.0/91.7</cell><cell cols="2">90.0/92.4 90.9/91.0</cell><cell>95.2</cell><cell cols="2">91.6 89.4</cell></row><row><cell>HyperPrompt (ours)</cell><cell>1.02x</cell><cell>64.6</cell><cell>96.7</cell><cell>94.0/91.8</cell><cell>91.3/91.4</cell><cell cols="2">90.0/92.4 90.8/91.0</cell><cell cols="3">95.4 91.9 89.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Comparison of HyperPrompt with baselines on GLUE using T5 Large.</figDesc><table><row><cell>Model</cell><cell cols="2">#Params BoolQ</cell><cell>CB</cell><cell cols="2">COPA MultiRC</cell><cell>ReCoRD</cell><cell cols="4">RTE WIC WSC AVG</cell></row><row><cell>MTL</cell><cell>1.0x</cell><cell>88.5</cell><cell>95.8/98.2</cell><cell>87.0</cell><cell cols="2">85.5/56.3 89.2/88.6</cell><cell>91.7</cell><cell>74.0</cell><cell>89.4</cell><cell>85.9</cell></row><row><cell>Vanilla Adapter</cell><cell>1.03x</cell><cell>88.8</cell><cell>98.3/98.8</cell><cell>86.0</cell><cell>85.3/56.0</cell><cell>89.3/88.7</cell><cell>91.2</cell><cell>73.6</cell><cell>91.3</cell><cell>86.1</cell></row><row><cell>HyperFormer++</cell><cell>1.01x</cell><cell>88.9</cell><cell>98.7/98.2</cell><cell>86.7</cell><cell cols="5">85.4/56.7 89.4/88.8 92.1 74.5 90.7</cell><cell>86.4</cell></row><row><cell>Prompt-Tuning</cell><cell>1.0001x</cell><cell>88.5</cell><cell>97.6/98.8</cell><cell>85.0</cell><cell>84.9/55.2</cell><cell>89.0/88.4</cell><cell>91.5</cell><cell>72.8</cell><cell>90.1</cell><cell>85.6</cell></row><row><cell>MTL-Prompt-Share (ours)</cell><cell>1.004x</cell><cell>88.5</cell><cell>98.7/98.2</cell><cell>88.0</cell><cell>85.2/55.8</cell><cell>89.7/89.1</cell><cell>91.8</cell><cell cols="2">74.1 93.9</cell><cell>86.8</cell></row><row><cell>MTL-Prompt-Sep (ours)</cell><cell>1.03x</cell><cell>88.6</cell><cell>97.6/98.8</cell><cell>87.7</cell><cell>85.2/56.4</cell><cell>89.7/89.1</cell><cell>91.6</cell><cell>73.5</cell><cell>89.4</cell><cell>86.1</cell></row><row><cell>HyperPrompt (ours)</cell><cell>1.01x</cell><cell>88.7</cell><cell>99.1/98.8</cell><cell>91.0</cell><cell cols="3">85.0/55.6 89.8/89.1 91.3</cell><cell>74.2</cell><cell>92.0</cell><cell>87.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>Comparison of HyperPrompt with baselines on SuperGLUE using T5 Base.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://github.com/tensorflow/mesh</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>https://github.com/google-research/text-to-text-transfer-Transformer</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>baselines and parameter-efficient models including Prompt-Tuning and HyperFormer++ on GLUE and SuperGLUE benchmarks. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Massive multi-task representations with pre-finetuning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><surname>Muppet</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.468</idno>
		<ptr target="https://aclanthology.org/2021.emnlp-main.468" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-11">November 2021</date>
			<biblScope unit="page" from="5799" to="5811" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Ext5: Towards extreme multi-task scaling for transfer learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Aribandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Q</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.12894</idno>
		<title level="m">The efficiency misnomer</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno>doi: 10.18653</idno>
		<ptr target="https://aclanthology.org/N19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06">June 2019</date>
			<biblScope unit="page" from="19" to="1423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Hypernetworks</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rkpACe1lx" />
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. April 24-26, 2017. 2017</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">WARP: Word-level Adversarial ReProgramming</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hambardzumyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Khachatrian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>May</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.381</idno>
		<ptr target="https://aclanthology.org/2021.acl-long.381" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-08">August 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4921" to="4933" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Towards a unified view of parameterefficient transfer learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Parameter-efficient transfer learning for NLP</title>
		<author>
			<persName><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Giurgiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>De Laroussilhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Attariyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v97/houlsby19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<title level="s">Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019-06">Jun 2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Parameter-efficient transfer learning for nlp</title>
		<author>
			<persName><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Giurgiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>De Laroussilhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Attariyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2790" to="2799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Parameter-efficient multi-task finetuning for transformers via shared hypernetworks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Karimi Mahabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021-08">August 2021</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Crafting papers on machine learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Langley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Machine Learning (ICML 2000)</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Langley</surname></persName>
		</editor>
		<meeting>the 17th International Conference on Machine Learning (ICML 2000)<address><addrLine>Stanford, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1207" to="1216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Constant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08691</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><surname>Bart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Prefix-tuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00190</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multitask deep neural networks for natural language understanding</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1441</idno>
		<ptr target="https://aclanthology.org/P19-1441" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-07">July 2019</date>
			<biblScope unit="page" from="4487" to="4496" />
		</imprint>
	</monogr>
	<note>a. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-task deep neural networks for natural language understanding</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4487" to="4496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Gpt understands</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Alyafeai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chaffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stiegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Thakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Szczechla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chhablani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Manica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">X</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bawden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Neeraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rozen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Santilli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fevry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Teehan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>and Rush, A. M. Multitask prompted training enables zero-shot task generalization</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adaptive learning rates with sublinear memory cost</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName><surname>Adafactor</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4596" to="4604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Mesh-tensorflow: Deep learning for supercomputers</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Koanantakool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.02084</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Eliciting Knowledge from Language Models with Automatically Generated Prompts</title>
		<author>
			<persName><forename type="first">T</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Autoprompt</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.346</idno>
		<ptr target="https://aclanthology.org/2020.emnlp-main.346" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11">November 2020</date>
			<biblScope unit="page" from="4222" to="4235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Augmenting self-attention with persistent memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.01470</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-C</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName><surname>Hypergrid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.05891</idno>
		<title level="m">Efficient multi-task transformers with grid-wise decomposable hyper projections</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Von Oswald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Henning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sacramento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">F</forename><surname>Grewe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00695</idno>
		<title level="m">Continual learning with hypernetworks</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07461</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Superglue: A stickier benchmark for generalpurpose language understanding systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00537</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename></persName>
		</author>
		<title level="m">Small towers make big differences</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Finetuned language models are zero-shot learners</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Understanding and improving information transfer in multitask learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
