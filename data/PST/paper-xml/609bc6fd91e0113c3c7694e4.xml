<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Diffusion Models Beat GANs on Image Synthesis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
							<email>prafulla@openai.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Equal contribution 35th Conference on Neural Information Processing Systems</orgName>
								<address>
									<postCode>2021)</postCode>
									<settlement>Sydney</settlement>
									<region>NeurIPS</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Equal contribution 35th Conference on Neural Information Processing Systems</orgName>
								<address>
									<postCode>2021)</postCode>
									<settlement>Sydney</settlement>
									<region>NeurIPS</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Diffusion Models Beat GANs on Image Synthesis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128×128, 4.59 on ImageNet 256×256, and 7.72 on ImageNet 512×512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256×256 and 3.85 on ImageNet 512×512.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Over the past few years, generative models have gained the ability to generate human-like natural language <ref type="bibr" target="#b8">[9]</ref>, high-quality synthetic images <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b56">57]</ref> and highly diverse human speech and music <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b16">17]</ref>. These models can be used in a variety of ways, such as generating images from text prompts <ref type="bibr" target="#b77">[78,</ref><ref type="bibr" target="#b55">56]</ref> or learning useful feature representations <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b9">10]</ref>. While these models are already capable of producing realistic images and sound, there is still much room for improvement beyond the current state-of-the-art, and better generative models could have wide-ranging impacts on graphic design, games, music production, and countless other fields.</p><p>GANs <ref type="bibr" target="#b24">[25]</ref> currently hold the state-of-the-art on most image generation tasks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b33">34]</ref> as measured by sample quality metrics such as FID <ref type="bibr" target="#b28">[29]</ref>, Inception Score <ref type="bibr" target="#b60">[61]</ref> and Precision <ref type="bibr" target="#b37">[38]</ref>. However, some of these metrics do not fully capture diversity, and it has been shown that GANs capture less diversity than state-of-the-art likelihood-based models <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b47">48]</ref>. Furthermore, GANs are often difficult to train, collapsing without carefully selected hyperparameters and regularizers <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b6">7]</ref>. While GANs hold the state-of-the-art, their drawbacks make them difficult to scale and apply to new domains. As a result, much work has been done to achieve GAN-like sample quality with likelihood-based models <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b11">12]</ref>. While these models capture more diversity and are typically easier to scale and train than GANs, they still fall short in terms of visual fidelity. Furthermore, except for VAEs, sampling from these models is slower than GANs in terms of wall-clock time.</p><p>Diffusion models are a class of likelihood-based models which have recently been shown to produce high-quality images <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b48">49]</ref> while offering desirable properties such as distribution coverage, a stationary training objective, and easy scalability. These models generate samples by gradually removing noise from a signal, and their training objective can be expressed as a reweighted variational lower-bound <ref type="bibr" target="#b30">[31]</ref>. This class of models already holds the state-of-the-art <ref type="bibr" target="#b66">[67]</ref> on CIFAR-10 <ref type="bibr" target="#b36">[37]</ref>, but still lags behind GANs on difficult generation datasets like LSUN and ImageNet. We hypothesize that this gap exists for at least two reasons: first, that the model architectures used by recent GAN literature have been heavily explored and refined; second, that GANs are able to trade off diversity for fidelity, producing high quality samples but not covering the whole distribution. We aim to bring these benefits to diffusion models, first by improving model architecture and then by devising a scheme for trading off diversity for fidelity.</p><p>The rest of the paper is organized as follows. In Section 2, we give a brief background of diffusion models based on Ho et al. <ref type="bibr" target="#b30">[31]</ref> and the improvements from Nichol and Dhariwal <ref type="bibr" target="#b48">[49]</ref> and Song et al. <ref type="bibr" target="#b63">[64]</ref>, and we describe our evaluation setup. In Section 3, we introduce simple architecture improvements that give a substantial boost to FID. In Section 4, we describe a method for using gradients from a classifier to guide a diffusion model during sampling. Finally, in Section 5 we show that models with our improved architecture achieve state-of-the-art on unconditional image synthesis tasks, and with classifier guidance achieve state-of-the-art on conditional image synthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>In this section, we provide a brief overview of diffusion models. For a more detailed mathematical description, we refer the reader to Appendix C. On a high level, diffusion models sample from a distribution by reversing a gradual noising process. In particular, sampling starts with noise x T and produces gradually less-noisy samples x T −1 , x T −2 , ... until reaching a final sample x 0 . In particular, a diffusion model learns to produce a slightly more "denoised" x t−1 from x t . Ho et al. <ref type="bibr" target="#b30">[31]</ref> parameterize this model using a function ϵ θ (x t , t) which predicts the noise component of a noisy sample x t . To train this function, each sample in a minibatch is produced by randomly drawing a data sample x 0 , a timestep t, and noise ϵ, which together give rise to a noised sample x t (Equation 3, Appendix C). The training objective is then ||ϵ θ (x t , t) − ϵ|| 2 , i.e. a simple mean-squared error loss between the true noise and the predicted noise (Equation <ref type="formula">12</ref>, Appendix C).</p><p>Ho et al. <ref type="bibr" target="#b30">[31]</ref> show that, under reasonable assumptions, we can then model the denoising distribution p θ (x t−1 |x t ) of x t−1 given x t as a diagonal Gaussian N (x t−1 ; µ θ (x t , t), Σ θ (x t , t)), where the mean µ θ (x t , t) can be calculated as a function of ϵ θ (x t , t) (Equation <ref type="formula">13</ref>, Appendix C). Ho et al. <ref type="bibr" target="#b30">[31]</ref> observe that the simple mean-squared error objective, L simple , works better in practice than the actual variational lower bound L vlb that can be derived from interpreting the denoising diffusion model as a VAE. They also note that training with this objective and using their corresponding sampling procedure is equivalent to the denoising score matching model from Song and Ermon <ref type="bibr" target="#b64">[65]</ref>, who use Langevin dynamics to sample from a denoising model trained with multiple noise levels to produce high quality image samples. We often use "diffusion models" as shorthand to refer to both classes of models.</p><p>Following the breakthrough work of Song and Ermon <ref type="bibr" target="#b64">[65]</ref> and Ho et al. <ref type="bibr" target="#b30">[31]</ref>, several recent papers have proposed improvements to diffusion models. Nichol and Dhariwal <ref type="bibr" target="#b48">[49]</ref> find that fixing the variance Σ θ (x t , t) to a constant as done in Ho et al. <ref type="bibr" target="#b30">[31]</ref> is sub-optimal for sampling with fewer diffusion steps, and propose to parameterize Σ θ (x t , t) as a neural network whose output v is interpolated as Σ θ (x t , t) = exp(v log β t + (1 − v) log βt ). Here, β t and βt (Equation 5, Appendix C) are the variances in Ho et al. <ref type="bibr" target="#b30">[31]</ref> corresponding to upper and lower bounds for the reverse process variances. Additionally, Nichol and Dhariwal <ref type="bibr" target="#b48">[49]</ref> propose a hybrid objective for training both ϵ θ (x t , t) and Σ θ (x t , t) using the weighted sum L simple + λL vlb . Learning the reverse process variances with their hybrid objective allows sampling with fewer steps without much drop in sample quality. We adopt this objective and parameterization, and use it throughout our experiments.</p><p>Song et al. <ref type="bibr" target="#b63">[64]</ref> propose DDIM, which formulates an alternative non-Markovian noising process that has the same forward marginals as DDPM, but allows producing different reverse samplers by changing the variance of the reverse noise. By setting this noise to 0, they provide a way to turn any model ϵ θ (x t , t) into a deterministic mapping from latents to images, and find that this provides an alternative way to sample with fewer steps. We adopt this sampling approach when using fewer than 50 sampling steps, since Nichol and Dhariwal <ref type="bibr" target="#b48">[49]</ref> found it to be beneficial in this regime.</p><p>Sample Quality Metrics: For comparing sample quality across models, we perform quantitative evaluations using the following metrics. While these metrics are often used in practice and correspond well with human judgement, they are not a perfect proxy, and finding better metrics for sample quality evaluation is still an open problem.</p><p>We use FID <ref type="bibr" target="#b28">[29]</ref> as our default metric for overall sample quality comparisons as it captures both fidelity and diversity and has been the de facto standard metric for state-of-the-art generative models <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b30">31]</ref>. We use Precision and Recall <ref type="bibr" target="#b37">[38]</ref> as proxies for separately measuring fidelity and diversity, respectively. We include sFID <ref type="bibr" target="#b47">[48]</ref> as a metric that better captures spatial relationships than FID, and also include Inception Score (IS) <ref type="bibr" target="#b60">[61]</ref> as another proxy for fidelity. When comparing against other methods, we re-compute these metrics using public samples or models whenever possible. This is for two reasons: first, some papers <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b30">31]</ref> compare against arbitrary subsets of the training set which are not readily available; and second, subtle implementation differences can affect the resulting FID values <ref type="bibr" target="#b50">[51]</ref>. For consistent comparisons, we use the full training set as the reference batch <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b7">8]</ref>, and evaluate metrics for all models using the same codebase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Architecture Improvements</head><p>Ho et al. <ref type="bibr" target="#b30">[31]</ref> adopted the UNet architecture <ref type="bibr" target="#b57">[58]</ref> for diffusion models, which Jolicoeur-Martineau et al. <ref type="bibr" target="#b31">[32]</ref> found to substantially improve sample quality over the previous architectures <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b38">39]</ref> used for denoising score matching. The UNet model uses a stack of residual layers and downsampling convolutions, followed by a stack of residual layers with upsampling convolutions, with skip connections connecting the layers with the same spatial size. In addition, they use a global attention layer at the 16×16 resolution with a single head, and add a projection of the timestep embedding into each residual block. Song et al. <ref type="bibr" target="#b66">[67]</ref> found that further changes to the UNet architecture improved performance on the CIFAR-10 <ref type="bibr" target="#b36">[37]</ref> and CelebA-64 <ref type="bibr" target="#b39">[40]</ref> datasets. We show the same result on ImageNet 128×128, finding that architecture can indeed give a substantial boost to sample quality on a much larger and more diverse datasets at a higher resolution.</p><p>We explore the following architectural changes: increasing depth versus width, holding model size relatively constant; increasing the number of attention heads; using attention at 32×32, 16×16, and 8×8 resolutions rather than only at 16×16; using the BigGAN <ref type="bibr" target="#b7">[8]</ref> residual block for upsampling and downsampling the activations, following <ref type="bibr" target="#b66">[67]</ref>; and finally; rescaling residual connections with 1 √ 2 , following <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>We train models with the above architecture changes on ImageNet 128×128 and compare them on FID, evaluated at two different points of training, in Table <ref type="table" target="#tab_0">1</ref>. Aside from rescaling residual connections, all of the other modifications improve performance and have a positive compounding effect. On wall-clock (Figure <ref type="figure">5</ref>, Appendix A.1) we find that increased depth hurts training time most, so we opt not to use this change in further experiments. We also study other attention configurations that better match the Transformer architecture <ref type="bibr" target="#b71">[72]</ref>. We try two configurations: constant attention heads, or constant channels per head. Table <ref type="table" target="#tab_1">2</ref> shows our results, indicating that more heads or fewer channels per head improves FID. On wall-clock (Figure <ref type="figure">5</ref>, Appendix A.1), we see that 64 channels is best so we opt to use 64 channels per head as our default. We note that this choice also better matches modern transformer architectures, and is on par with our other configurations in terms of final FID. We also experiment with a layer <ref type="bibr" target="#b48">[49]</ref> that we refer to as adaptive group normalization (AdaGN), which incorporates the timestep and class embedding into each residual block after a group normalization operation <ref type="bibr" target="#b74">[75]</ref>, similar to adaptive instance norm <ref type="bibr" target="#b32">[33]</ref> and FiLM <ref type="bibr" target="#b53">[54]</ref>. We define this layer as AdaGN(h, y) = y s GroupNorm(h) + y b , where h is the intermediate activations of the residual block following the first convolution, and y = [y s , y b ] is obtained from a linear projection of the timestep and class embedding. We had already seen AdaGN improve our earliest diffusion models, and so had included it by default in all our runs. We explicitly ablate this choice (Table <ref type="table">6</ref>, Appendix A.1), and find that FID becomes worse by 2.02 when we remove the adaptive group normalization layer.</p><p>In the rest of the paper, we use this final improved model architecture as our default: variable width with 2 residual blocks per resolution, multiple heads with 64 channels per head, attention at 32, 16 and 8 resolutions, BigGAN residual blocks for up and downsampling, and adaptive group normalization for injecting timestep and class embeddings into residual blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Classifier Guidance</head><p>In addition to employing well designed architectures, GANs for conditional image synthesis <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b7">8]</ref> make heavy use of class labels. This often takes the form of class-conditional normalization statistics <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b13">14]</ref> as well as discriminators with heads explicitly designed to behave like classifiers p(y|x) <ref type="bibr" target="#b45">[46]</ref>.</p><p>As further evidence that class information is crucial to the success of these models, Lucic et al. <ref type="bibr" target="#b41">[42]</ref> find that it is helpful to generate synthetic labels when working in a label-limited regime. Given this observation for GANs, it makes sense to explore different ways to condition diffusion models on class labels. We already incorporate class information into adaptive group normalization layers (Section 3).</p><p>Here, we explore a different approach: exploiting a classifier p(y|x) to improve a diffusion generator. Sohl-Dickstein et al. <ref type="bibr" target="#b62">[63]</ref> and Song et al. <ref type="bibr" target="#b66">[67]</ref> show one way to achieve this, wherein a pre-trained diffusion model can be conditioned using the gradients of a classifier. In particular, we can train a classifier p ϕ (y|x t , t) on noisy images x t , and then use gradients ∇ xt log p ϕ (y|x t , t) to guide the diffusion sampling process towards an arbitrary class label y.</p><p>For class conditional diffusion sampling, we reproduce the derivation from Sohl-Dickstein et al. <ref type="bibr" target="#b62">[63]</ref> in Appendix D. Input: class label y, gradient scale s x T ← sample from N (0, I) for all t from T to 1 do p ϕ (y|x t , t) = p ϕ (y|x t ) and ϵ θ (x t , t) = ϵ θ (x t ) for brevity, noting that they refer to separate functions for each timestep t and at training time the models must be conditioned on the input t.</p><formula xml:id="formula_0">ε ← ϵ θ (x t ) − √ 1 − ᾱt ∇ xt log p ϕ (y|x t ) x t−1 ← √ ᾱt−1 xt− √ 1− ᾱtε √ ᾱt + √ 1 − ᾱt−1 ε end for return x 0</formula><p>To apply classifier guidance to a large scale generative task, we train classification models on ImageNet. Our classifier architecture is simply the downsampling trunk of the UNet model with an attention pool <ref type="bibr" target="#b54">[55]</ref> at the 8x8 layer to produce the final output. We train these classifiers on the same noising distribution as the corresponding diffusion model, and also add random crops to reduce overfitting.</p><p>In initial experiments with unconditional ImageNet models, we found it necessary to scale the classifier gradients by a constant factor larger than 1. When using a scale of 1, we observed that the classifier assigned reasonable probabilities (around 50%) to the desired classes for the final samples, but these samples did not match the intended classes upon visual inspection. Scaling up the classifier gradients remedied this problem, and the class probabilities from the classifier increased to nearly 100%. Figure <ref type="figure" target="#fig_1">2</ref> shows an example of this effect. To understand the effect of scaling classifier gradients, note that s • ∇ x log p(y|x) = ∇ x log 1 Z p(y|x) s , where Z is an arbitrary constant. As a result, the conditioning process is still theoretically grounded in a re-normalized classifier distribution proportional to p(y|x) s . When s &gt; 1, this distribution becomes sharper than p(y|x), since larger values are amplified by the exponent. In other words, using a larger gradient scale focuses more on the modes of the classifier, which is potentially desirable for producing higher quality (but less diverse) samples.</p><p>In the above derivations, we assumed that the underlying diffusion model was unconditional, modeling p(x). It is also possible to train conditional diffusion models, p(x|y), and use classifier guidance in  the exact same way. Table <ref type="table" target="#tab_3">3</ref> shows that the sample quality of both unconditional and conditional models can be greatly improved by classifier guidance. We see that, with a high enough scale, the guided unconditional model can get quite close to the FID of an unguided conditional model, although training directly with the class labels still helps. Guiding a conditional model further improves FID.</p><p>Table <ref type="table" target="#tab_3">3</ref> also shows that classifier guidance improves precision at the cost of recall, thus introducing a trade-off in sample fidelity versus diversity. We explicitly evaluate how this trade-off varies with the gradient scale in Figure <ref type="figure" target="#fig_2">3</ref>. We see that scaling the gradients beyond 1.0 smoothly trades off recall (a measure of diversity) for higher precision and IS (measures of fidelity). Since FID and sFID depend on both diversity and fidelity, their best values are obtained at an intermediate point. We also compare our guidance with the truncation trick from BigGAN (Figure <ref type="figure">6</ref>, Appendix A.2). We find that classifier guidance is strictly better than BigGAN-deep when trading off FID for Inception Score. Less clear cut is the precision/recall trade-off, which shows that classifier guidance is only a better choice up until a certain precision threshold, after which point it cannot achieve better precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>To evaluate our improved model architecture on unconditional image generation, we train separate diffusion models on three LSUN <ref type="bibr" target="#b76">[77]</ref> classes: bedroom, horse, and cat. To evaluate classifier guidance, we train conditional diffusion models on the ImageNet <ref type="bibr" target="#b58">[59]</ref> dataset at 128×128, 256×256, and 512×512 resolution.</p><p>Table <ref type="table" target="#tab_4">4</ref> summarizes our results. ADM refers to our ablated diffusion model, and ADM-G additionally uses classifier guidance. Our diffusion models can obtain the best FID on each task, and the best sFID on all but one task. With the improved architecture, we already obtain state-of-the-art image generation on LSUN and ImageNet 64×64. For higher resolution ImageNet, we observe that classifier guidance allows our models to substantially outperform the best GANs. These models obtain perceptual quality similar to GANs, while maintaining a higher coverage of the distribution as measured by recall, and can even do so using only 25 sampling steps. We also evaluate the computational requirements for training our models (Table <ref type="table" target="#tab_0">10</ref>, Appendix B), and find that we can obtain competitive sample quality while using the same or less compute than the corresponding BigGAN-deep or StyleGAN2 model.  tinca fish with no human holding it. We also check our generated samples for nearest neighbors in the Inception-V3 feature space in Appendix E, and we show additional samples in Appendices M-O.</p><p>We also compare guidance to using a two-stage upsampling stack. Nichol and Dhariwal <ref type="bibr" target="#b48">[49]</ref> and Saharia et al. <ref type="bibr" target="#b59">[60]</ref> train two-stage diffusion models by combining a low-resolution diffusion model with a corresponding upsampling diffusion model. In this approach, the upsampling model is trained to upsample images from the training set, and conditions on low-resolution images that are concatenated channel-wise to the model input using a simple interpolation (e.g. bilinear). During sampling, the low-resolution model produces a sample, and then the upsampling model is conditioned on this sample. This greatly improves FID on ImageNet 256×256, but does not reach the same performance as state-of-the-art models like BigGAN-deep <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b59">60]</ref>, as seen in Table <ref type="table" target="#tab_4">4</ref>.</p><p>In Table <ref type="table" target="#tab_5">5</ref>, we show that guidance and upsampling improve sample quality along different axes. We use the upsampling stack from Nichol and Dhariwal <ref type="bibr" target="#b48">[49]</ref> combined with our architecture improvements, which we refer to as ADM-U. While upsampling improves precision while keeping a high recall, guidance provides a knob to trade off diversity for much higher precision. We achieve the best FIDs by using guidance at a lower resolution before upsampling to a higher resolution, indicating that these approaches complement one another.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Score based generative models were introduced by Song and Ermon <ref type="bibr" target="#b65">[66]</ref> as a way of modeling a data distribution using its gradients, and then sampling using Langevin dynamics <ref type="bibr" target="#b72">[73]</ref>. Ho et al. <ref type="bibr" target="#b30">[31]</ref> found a connection between this method and diffusion models <ref type="bibr" target="#b62">[63]</ref>, and achieved excellent sample quality by leveraging this connection. After this breakthrough work, many works followed up with   <ref type="bibr" target="#b35">[36]</ref> and Chen et al. <ref type="bibr" target="#b10">[11]</ref> demonstrated that diffusion models work well for audio; Jolicoeur-Martineau et al. <ref type="bibr" target="#b31">[32]</ref> found that a GAN-like setup could improve samples from these models; Song et al. <ref type="bibr" target="#b66">[67]</ref> explored ways to leverage techniques from stochastic differential equations to improve the sample quality obtained by score-based models; Song et al. <ref type="bibr" target="#b63">[64]</ref> and Nichol and Dhariwal <ref type="bibr" target="#b48">[49]</ref> proposed methods to improve sampling speed; Nichol and Dhariwal <ref type="bibr" target="#b48">[49]</ref> and Saharia et al. <ref type="bibr" target="#b59">[60]</ref> demonstrated promising results on the difficult ImageNet generation task using upsampling diffusion models. Also related to diffusion models, and following the work of Sohl-Dickstein et al. <ref type="bibr" target="#b62">[63]</ref>, Goyal et al. <ref type="bibr" target="#b26">[27]</ref> described a technique for learning a model with learned iterative generation steps, and found that it could achieve good image samples when trained with a likelihood objective.</p><p>One missing element from previous work on diffusion models is a way to trade off diversity for fidelity. Other generative techniques provide natural levers for this trade-off. Brock et al. <ref type="bibr" target="#b7">[8]</ref> introduced the truncation trick for GANs, wherein the latent vector is sampled from a truncated normal distribution. They found that increasing truncation naturally led to a decrease in diversity but an increase in fidelity. More recently, Razavi et al. <ref type="bibr" target="#b56">[57]</ref> proposed to use classifier rejection sampling to filter out bad samples from an autoregressive likelihood-based model, and found that this technique improved FID. DeVries et al. <ref type="bibr" target="#b15">[16]</ref> found that filtering out low-density regions of the training set improves GAN training performance. Most likelihood-based models also allow for low-temperature sampling <ref type="bibr" target="#b0">[1]</ref>, which provides a natural way to emphasize modes of the data distribution (see Appendix I).</p><p>Other likelihood-based models have been shown to produce high-fidelity image samples. VQ-VAE <ref type="bibr" target="#b70">[71]</ref> and VQ-VAE-2 <ref type="bibr" target="#b56">[57]</ref> are autoregressive models trained on top of quantized latent codes, greatly reducing the computational resources required to train these models on large images. These models produce diverse and high quality images, but still fall short of GANs without expensive rejection sampling and special metrics to compensate for blurriness. DCTransformer <ref type="bibr" target="#b47">[48]</ref> is a related method which relies on a more intelligent compression scheme. VAEs are another promising class of likelihood-based models, and recent methods such as NVAE <ref type="bibr" target="#b68">[69]</ref> and VDVAE <ref type="bibr" target="#b11">[12]</ref> have successfully been applied to difficult image generation domains. Energy-based models are another class of likelihood-based models with a rich history <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b29">30]</ref>. Sampling from the EBM distribution is challenging, and Xie et al. <ref type="bibr" target="#b75">[76]</ref> demonstrate that Langevin dynamics can be used to sample coherent images from these models. Du and Mordatch <ref type="bibr" target="#b18">[19]</ref> further improve upon this approach, obtaining high quality images. More recently, Gao et al. <ref type="bibr" target="#b23">[24]</ref> incorporate diffusion steps into an energy-based model, and find that doing so improves image samples from these models.</p><p>Other works have controlled generative models with a pre-trained classifier. For example, an emerging body of work <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b1">2]</ref> aims to optimize GAN latent spaces for text prompts using pre-trained CLIP <ref type="bibr" target="#b54">[55]</ref> models. More similar to our work, Song et al. <ref type="bibr" target="#b66">[67]</ref> uses a classifier to generate class-conditional CIFAR-10 images with a diffusion model. In some cases, classifiers can act as stand-alone generative models. For example, Santurkar et al. <ref type="bibr" target="#b61">[62]</ref> demonstrate that a robust image classifier can be used as a stand-alone generative model, and Grathwohl et al. <ref type="bibr" target="#b27">[28]</ref> train a model which is jointly a classifier and an energy-based model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Limitations and Future Work</head><p>While we believe diffusion models are an extremely promising direction for generative modeling, they are still slower than GANs at sampling time due to the use of multiple denoising steps (and therefore forward passes). Since our diffusion models are also larger than the competing GAN generators, each forward pass takes anywhere from 5-20 times longer too. A promising direction to reduce this latency gap is Luhman and Luhman <ref type="bibr" target="#b42">[43]</ref>, who explore a way to distill the DDIM sampling process into a single step model. The samples from the single step model are not yet competitive with GANs, but are much better than previous single-step likelihood-based models. Future work in this direction might be able to completely close the sampling speed gap between diffusion models and GANs without sacrificing image quality.</p><p>Unlike GANs, Flows, and VAEs, diffusion models do not learn an explicit latent representation. While DDIM provides a way to encode images into an implicit latent space, it is unclear how semantically meaningful this latent representation is compared to those of other model classes. This could make it difficult to use diffusion models for representation learning or image editing applications.</p><p>The effectiveness of classifier guidance demonstrates that we can obtain powerful generative models from the gradients of a classification function. This could be used to condition an image generator with a text caption using a noisy version of CLIP <ref type="bibr" target="#b54">[55]</ref>, similar to recent methods that guide GANs using text prompts <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b1">2]</ref>. Our proposed classifier guidance technique is currently limited to labeled datasets. In the future, our method could be extended to unlabeled data by clustering samples to produce synthetic labels <ref type="bibr" target="#b41">[42]</ref> or by training discriminative models to use for guidance. This also suggests that large unlabeled datasets could be leveraged in the future to pre-train powerful diffusion models that can later be improved by using a classifier with desirable properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Societal Impact</head><p>Our proposed technique makes generative models more accessible in terms of compute costs, especially because new classifiers can be trained and used on top of existing high-quality diffusion models. While we believe this is generally a benefit of these models, it could also have negative societal implications. For example, cheaper generative models could enable bad actors to generate fake news, propaganda images, or doctored photos. Additionally, the wide-spread deployment of these models could displace jobs in art, graphic design, animation, and photography. One could imagine, however, that democratizing generative models could also have positive impacts in the long run, creating new types of jobs such as generative photo editing. Intentionally deceitful generated images are a more direct concern, and detecting and mitigating propaganda and fake news based on generative models is an ongoing area of research <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>We have shown that diffusion models, a class of likelihood-based models with a stationary training objective, can obtain better sample quality than state-of-the-art GANs. Our improved architecture is sufficient to achieve this on unconditional image generation tasks, and our classifier guidance technique allows us to do so on class-conditional tasks. In the latter case, we find that the scale of the classifier gradients can be adjusted to trade off diversity for fidelity. These guided diffusion models can reduce the sampling time gap between GANs and diffusion models, although diffusion models still require multiple forward passes during sampling. Finally, by combining guidance with upsampling, we can further improve sample quality on high-resolution conditional image synthesis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Selected samples from our best ImageNet 512×512 model (FID 3.85)</figDesc><graphic url="image-1.png" coords="1,108.00,419.36,395.98,197.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Samples from an unconditional diffusion model with classifier guidance to condition on the class "Pembroke Welsh corgi". Using classifier scale 1.0 (left; FID: 33.0) does not produce convincing samples in this class, whereas classifier scale 10.0 (right; FID: 12.0) produces much more class-consistent images.</figDesc><graphic url="image-2.png" coords="5,110.46,320.17,188.10,94.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Change in sample quality as we vary scale of the classifier gradients for a class-conditional ImageNet 128×128 model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4</head><label>4</label><figDesc>Figure 4 compares random samples from the best BigGAN-deep model to our guided diffusion model. While the samples are of similar perceptual quality, the diffusion model contains more modes than the GAN, such as zoomed ostrich heads, single flamingos, different orientations of cheeseburgers, and a</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Samples from BigGAN-deep with truncation 1.0 (FID 6.95, left) vs samples from our diffusion model with guidance (FID 4.59, middle) and samples from the training set (right).</figDesc><graphic url="image-4.png" coords="8,111.90,72.01,122.75,252.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Ablation of various architecture changes, evaluated at 700K and 1200K iterations</figDesc><table><row><cell cols="3">Channels Depth Heads</cell><cell cols="5">Attention resolutions up/downsample resblock 700K 1200K BigGAN Rescale FID FID</cell></row><row><cell>160</cell><cell>2</cell><cell>1</cell><cell>16</cell><cell>✗</cell><cell>✗</cell><cell>15.33</cell><cell>13.21</cell></row><row><cell>128</cell><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-0.21</cell><cell>-0.48</cell></row><row><cell></cell><cell></cell><cell>4</cell><cell></cell><cell></cell><cell></cell><cell>-0.54</cell><cell>-0.82</cell></row><row><cell></cell><cell></cell><cell></cell><cell>32,16,8</cell><cell></cell><cell></cell><cell>-0.72</cell><cell>-0.66</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>✓</cell><cell></cell><cell>-1.20</cell><cell>-1.21</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>✓</cell><cell>0.16</cell><cell>0.25</cell></row><row><cell>160</cell><cell>2</cell><cell>4</cell><cell>32,16,8</cell><cell>✓</cell><cell>✗</cell><cell>-3.14</cell><cell>-3.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation of attention heads. More heads or lower channels per heads both improve FID. The base model was a smaller version of the best model from Table1.</figDesc><table><row><cell>Number of heads Channels per head</cell><cell>FID</cell></row><row><cell>1</cell><cell>14.08</cell></row><row><cell>2</cell><cell>-0.50</cell></row><row><cell>4</cell><cell>-0.97</cell></row><row><cell>8</cell><cell>-1.17</cell></row><row><cell>32</cell><cell>-1.36</cell></row><row><cell>64</cell><cell>-1.03</cell></row><row><cell>128</cell><cell>-1.08</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Classifier guided diffusion sampling, given a diffusion model (µ θ (x t ), Σ θ (x t )), classifier p ϕ (y|x t ), and gradient scale s.Input: class label y, gradient scale s x T ← sample from N (0, I) for all t from T to 1 do µ, Σ ← µ θ (x t ), Σ θ (x t )x t−1 ← sample from N (µ + sΣ ∇ xt log p ϕ (y|x t ), Σ)Algorithm 2 Classifier guided DDIM sampling, given a diffusion model ϵ θ (x t ), classifier p ϕ (y|x t ), and gradient scale s.</figDesc><table><row><cell>end for</cell></row><row><cell>return x 0</cell></row></table><note>2. For DDIM, we perform a score-based derivation in Appendix D.3 inspired by Song et al.<ref type="bibr" target="#b66">[67]</ref>. The resulting sampling algorithms we use for guidance are Algorithms 1 and 2 respectively. Both algorithms incorporate class information by adding the gradients of a classifier to each sampling step with an appropriate step size. In these algorithms, we choose the notation Algorithm 1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Effect of classifier guidance on sample quality. Both conditional and unconditional models were trained for 2M iterations on ImageNet 256×256 with batch size 256.</figDesc><table><row><cell></cell><cell cols="6">Conditional Guidance Scale</cell><cell></cell><cell>FID</cell><cell cols="2">sFID</cell><cell>IS</cell><cell></cell><cell cols="4">Precision Recall</cell><cell></cell></row><row><cell></cell><cell></cell><cell>✗</cell><cell></cell><cell></cell><cell>✗</cell><cell></cell><cell></cell><cell>26.21</cell><cell cols="2">6.35</cell><cell>39.70</cell><cell></cell><cell>0.61</cell><cell></cell><cell cols="2">0.63</cell><cell></cell></row><row><cell></cell><cell></cell><cell>✗</cell><cell></cell><cell></cell><cell>✓</cell><cell>1.0</cell><cell></cell><cell>33.03</cell><cell cols="2">6.99</cell><cell>32.92</cell><cell></cell><cell>0.56</cell><cell></cell><cell cols="2">0.65</cell><cell></cell></row><row><cell></cell><cell></cell><cell>✗</cell><cell></cell><cell></cell><cell>✓</cell><cell>10.0</cell><cell></cell><cell cols="3">12.00 10.40</cell><cell>95.41</cell><cell></cell><cell>0.76</cell><cell></cell><cell cols="2">0.44</cell><cell></cell></row><row><cell></cell><cell></cell><cell>✓</cell><cell></cell><cell></cell><cell>✗</cell><cell></cell><cell></cell><cell>10.94</cell><cell cols="2">6.02</cell><cell>100.98</cell><cell></cell><cell>0.69</cell><cell></cell><cell cols="2">0.63</cell><cell></cell></row><row><cell></cell><cell></cell><cell>✓</cell><cell></cell><cell></cell><cell>✓</cell><cell>1.0</cell><cell></cell><cell>4.59</cell><cell cols="2">5.25</cell><cell>186.70</cell><cell></cell><cell>0.82</cell><cell></cell><cell cols="2">0.52</cell><cell></cell></row><row><cell></cell><cell></cell><cell>✓</cell><cell></cell><cell></cell><cell>✓</cell><cell>10.0</cell><cell></cell><cell>9.11</cell><cell cols="3">10.93 283.92</cell><cell></cell><cell>0.88</cell><cell></cell><cell cols="2">0.32</cell><cell></cell></row><row><cell></cell><cell></cell><cell>FID</cell><cell></cell><cell>sFID</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>IS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">precision</cell><cell></cell><cell>recall</cell></row><row><cell>16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>300</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>12 14</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>250</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.7 0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>200</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>6 8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>150</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.4 0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell><cell>0</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell><cell>0</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell></row><row><cell></cell><cell></cell><cell cols="2">gradient scale</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">gradient scale</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">gradient scale</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Sample quality comparison with state-of-the-art generative models for each task. LSUN diffusion models are sampled using 1000 steps (see Appendix L). ImageNet diffusion models are sampled using 250 steps, except when we use the DDIM sampler with 25 steps. *No BigGAN-deep model was available at this resolution, so we trained our own. † Values are taken from a previous paper, due to lack of public models or samples. ‡ Results use two-resolution stacks. § Results use compute-intensive classifier rejection sampling.</figDesc><table><row><cell>Model</cell><cell>FID sFID Prec Rec</cell><cell>Model</cell><cell>FID sFID Prec Rec</cell></row><row><cell cols="2">LSUN Bedrooms 256×256</cell><cell>ImageNet 128×128</cell><cell></cell></row><row><cell cols="2">DCTransformer  † [48] 6.40 6.66 0.44 0.56</cell><cell>BigGAN-deep [8]</cell><cell>6.02 7.18 0.86 0.35</cell></row><row><cell>DDPM [31]</cell><cell>4.89 9.07 0.60 0.45</cell><cell>LOGAN  † [74]</cell><cell>3.36</cell></row><row><cell>IDDPM [49]</cell><cell>4.24 8.21 0.62 0.46</cell><cell>ADM</cell><cell>5.91 5.09 0.70 0.65</cell></row><row><cell>StyleGAN [33]</cell><cell>2.35 6.62 0.59 0.48</cell><cell>ADM-G (25 steps)</cell><cell>5.98 7.04 0.78 0.51</cell></row><row><cell>ADM (dropout)</cell><cell>1.90 5.59 0.66 0.51</cell><cell>ADM-G</cell><cell>2.97 5.09 0.78 0.59</cell></row><row><cell></cell><cell></cell><cell>ImageNet 256×256</cell><cell></cell></row><row><cell cols="2">LSUN Horses 256×256</cell><cell>DCTransformer  † [48]</cell><cell>36.51 8.24 0.36 0.67</cell></row><row><cell>StyleGAN2 [34] ADM ADM (dropout)</cell><cell>3.84 6.46 0.63 0.48 2.95 5.94 0.69 0.55 2.57 6.81 0.71 0.55</cell><cell cols="2">VQ-VAE-2  † ‡ [57] VQ-VAE-2 (RS)  † ‡ § [57] ∼ 10 31.11 17.38 0.36 0.57 VQ-GAN  ‡ [21] 15.97 19.05 0.63 0.58</cell></row><row><cell></cell><cell></cell><cell>VQ-GAN (RS)  ‡ § [21]</cell><cell>5.06 7.34 0.79 0.48</cell></row><row><cell>LSUN Cats 256×256 DDPM [31] StyleGAN2 [34] ADM (dropout)</cell><cell>17.1 12.4 0.53 0.48 7.25 6.33 0.58 0.43 5.57 6.69 0.63 0.52</cell><cell>IDDPM  ‡ [49] SR3  † ‡ [60] BigGAN-deep [8] ADM ADM-G (25 steps)</cell><cell>12.26 5.42 0.70 0.62 11.30 6.95 7.36 0.87 0.28 10.94 6.02 0.69 0.63 5.44 5.32 0.81 0.49</cell></row><row><cell></cell><cell></cell><cell>ADM-G</cell><cell>4.59 5.25 0.82 0.52</cell></row><row><cell>ImageNet 64×64</cell><cell></cell><cell>ImageNet 512×512</cell><cell></cell></row><row><cell>BigGAN-deep* [8] IDDPM [49] ADM ADM (dropout)</cell><cell>4.06 3.96 0.79 0.48 2.92 3.79 0.74 0.62 2.61 3.77 0.73 0.63 2.07 4.29 0.74 0.63</cell><cell>BigGAN-deep [8] ADM ADM-G (25 steps) ADM-G</cell><cell>8.43 8.13 0.88 0.29 23.24 10.19 0.73 0.60 8.41 9.67 0.83 0.47 7.72 6.57 0.87 0.42</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparing our single, upsampling and classifier guided models. The upsamplers are 64→256 and 128→512. When combining guidance with upsampling, we only guide the lower resolution model. All models are sampled using 250 sampling steps.</figDesc><table><row><cell>Model</cell><cell>FID sFID</cell><cell>IS</cell><cell>Prec Rec</cell><cell>Model</cell><cell>FID sFID</cell><cell>IS</cell><cell>Prec Rec</cell></row><row><cell cols="2">ImageNet 256×256</cell><cell></cell><cell></cell><cell cols="2">ImageNet 512×512</cell><cell></cell><cell></cell></row><row><cell>ADM</cell><cell cols="3">10.94 6.02 100.98 0.69 0.63</cell><cell>ADM</cell><cell cols="3">23.24 10.19 58.06 0.73 0.60</cell></row><row><cell>ADM, ADM-U</cell><cell cols="3">7.49 5.13 127.49 0.72 0.63</cell><cell>ADM, ADM-U</cell><cell cols="3">9.96 5.62 121.78 0.75 0.64</cell></row><row><cell>ADM-G</cell><cell cols="3">4.59 5.25 186.70 0.82 0.52</cell><cell>ADM-G</cell><cell cols="3">7.72 6.57 172.71 0.87 0.42</cell></row><row><cell cols="4">ADM-G, ADM-U 3.94 6.14 215.84 0.83 0.53</cell><cell cols="4">ADM-G, ADM-U 3.85 5.86 221.72 0.84 0.53</cell></row><row><cell cols="3">more promising results: Kong et al.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Checklist</head><p>• Did you include the license to the code and datasets? <ref type="bibr">[No]</ref> We were unable to find the official license for either ImageNet or LSUN. We do include a license for our released source code.</p><p>1. For all authors...  <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b30">31]</ref>. We also cite PyTorch <ref type="bibr" target="#b51">[52]</ref>, which we use throughout our experiments. (b) Did you mention the license of the assets? [Yes] A license file is included with the code release. (c) Did you include any new assets either in the supplemental material or as a URL? <ref type="bibr">[Yes]</ref> The code is included in the supplemental material. (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A] (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A learning algorithm for boltzmann machines</title>
		<author>
			<persName><forename type="first">David</forename><surname>Ackley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terrence</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="147" to="169" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<ptr target="https://twitter.com/advadnoun/status/1351038053033406468" />
	</analytic>
	<monogr>
		<title level="j">Adverb. The big sleep</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Protecting world leaders against deep fakes</title>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hany</forename><surname>Farid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuming</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koki</forename><surname>Nagano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2019-06">June 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Detection of online fake news using n-gram analysis and machine learning techniques</title>
		<author>
			<persName><forename type="first">Hadeer</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Issa</forename><surname>Traore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherif</forename><surname>Saad</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-69155-8_9</idno>
		<imprint>
			<date type="published" when="2017">10 2017</date>
			<biblScope unit="page" from="127" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Detecting opinion spams and fake news using text classification</title>
		<author>
			<persName><forename type="first">Hadeer</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Issa</forename><surname>Traore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherif</forename><surname>Saad</surname></persName>
		</author>
		<idno type="DOI">10.1002/spy2.9</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/abs/10.1002/spy2.9" />
	</analytic>
	<monogr>
		<title level="j">Security and Privacy</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">e9</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A note on the inception score</title>
		<author>
			<persName><forename type="first">Shane</forename><surname>Barratt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Sharma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01973</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theodore</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07093</idno>
		<title level="m">Neural photo editing with introspective adversarial networks</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<title level="m">Ilya Sutskever, and Dario Amodei. Language models are few-shot learners</title>
				<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1691" to="1703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Nanxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><surname>Wavegrad</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.00713</idno>
		<title level="m">Estimating gradients for waveform generation</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Very deep vaes generalize autoregressive models and can outperform them on images</title>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10650</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The helmholtz machine</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Peter Dayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radford</forename><forename type="middle">M</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="889" to="904" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Florian</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jérémie</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.00683</idno>
		<title level="m">Modulating early visual processing by language</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><surname>Deepmind</surname></persName>
		</author>
		<ptr target="https://tfhub.dev/deepmind/biggan-deep-128/1" />
		<title level="m">Biggan-deep 128x128 on tensorflow hub</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.15255</idno>
		<title level="m">Instance selection for gans</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Payne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00341</idno>
		<title level="m">Jukebox: A generative model for music</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Large scale adversarial representation learning</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02544</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Implicit generation and generalization in energy-based models</title>
		<author>
			<persName><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.08689</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A learned representation for artistic style</title>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manjunath</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><surname>Kudlur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.07629</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Taming transformers for high-resolution image synthesis</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Ommer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09841</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>De Fauw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.04933</idno>
		<title level="m">Hierarchical autoregressive image models with auxiliary decoders</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Generating images from caption and vice versa via clip-guided generative latent space search</title>
		<author>
			<persName><forename type="first">Federico</forename><forename type="middle">A</forename><surname>Galatolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C A</forename><surname>Mario</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gigliola</forename><surname>Cimino</surname></persName>
		</author>
		<author>
			<persName><surname>Vaglini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.01645</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning energybased models by diffusion recovery likelihood</title>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.08125</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2661</idno>
		<title level="m">Aaron Courville, and Yoshua Bengio. Generative adversarial networks</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Google</forename><forename type="middle">Cloud</forename><surname>Tpus</surname></persName>
		</author>
		<ptr target="https://cloud.google.com/tpu/" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Variational walkback: Learning a transition operator as a stochastic recurrent net</title>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.02282</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Your classifier is secretly an energy based model and you should treat it like one</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan-Chieh</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jörn-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.03263</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Training products of experts by minimizing contrastive divergence</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1771" to="1800" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11239</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Alexia</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Piché-Taillefer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.05475</idno>
		<title level="m">Rémi Tachet des Combes, and Ioannis Mitliagkas. Adversarial score matching and improved sampling for image generation</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:arXiv:1812.04948</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.04958</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Diffwave: A versatile diffusion model for audio synthesis</title>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaji</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kexin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.09761</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">CIFAR-10 (Canadian Institute for Advanced Research)</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://www.cs.toronto.edu/~kriz/cifar.html" />
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Improved precision and recall metric for assessing generative models</title>
		<author>
			<persName><forename type="first">Tuomas</forename><surname>Kynkäänniemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.06991</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06612</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
				<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015-12">December 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">High-fidelity image generation with fewer labels</title>
		<author>
			<persName><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marvin</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02271</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Knowledge distillation in iterative generative models for improved sampling speed</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Luhman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Troy</forename><surname>Luhman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.02388</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Paulius</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonah</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksii</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganesh</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.03740</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Mixed precision training</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05637</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">cgans with projection discriminator</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05957</idno>
		<title level="m">Spectral normalization for generative adversarial networks</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Charlie</forename><surname>Nash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03841</idno>
		<title level="m">Generating images with sparse representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.09672</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<author>
			<persName><surname>Stylegan2</surname></persName>
		</author>
		<ptr target="https://github.com/NVlabs/stylegan2" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.11222</idno>
		<title level="m">On buggy resizing libraries and surprising subtleties in fid calculation</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01703</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Or</forename><surname>Patashnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongze</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.17249</idno>
		<title level="m">Styleclip: Text-driven manipulation of stylegan imagery</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.07871</idno>
		<title level="m">Harm de Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with a general conditioning layer</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12092</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00446</idno>
		<title level="m">Generating diverse high-fidelity images with VQ-VAE-2</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04597</idno>
		<title level="m">Convolutional networks for biomedical image segmentation</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0575</idno>
		<title level="m">Imagenet large scale visual recognition challenge</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:arXiv:2104.07636</idno>
		<title level="m">Image super-resolution via iterative refinement</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03498</idno>
		<title level="m">Improved techniques for training gans</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Image synthesis with a single (robust) classifier</title>
		<author>
			<persName><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.09453</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">A</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.03585</idno>
		<title level="m">Deep unsupervised learning using nonequilibrium thermodynamics</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Denoising diffusion implicit models</title>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02502</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Improved techniques for training score-based generative models</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09011</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:arXiv:1907.05600</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Score-based generative modeling through stochastic differential equations</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.13456</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.00567</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Nvae: A deep hierarchical variational autoencoder</title>
		<author>
			<persName><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03898</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">Wavenet: A generative model for raw audio</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00937</idno>
		<title level="m">Neural discrete representation learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Bayesian learning via stochastic gradient langevin dynamics</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning (ICML-11)</title>
				<meeting>the 28th international conference on machine learning (ICML-11)</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="681" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><surname>Logan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.00953</idno>
		<title level="m">Latent optimisation for generative adversarial networks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08494</idno>
		<title level="m">Group normalization</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<author>
			<persName><forename type="first">Jianwen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.03264</idno>
		<title level="m">A theory of generative convnet</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<author>
			<persName><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><surname>Lsun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03365</idno>
		<title level="m">Construction of a large-scale image dataset using deep learning with humans in the loop</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03242</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><surname>Thop</surname></persName>
		</author>
		<ptr target="https://github.com/Lyken17/pytorch-OpCounter" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
