<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Block-Skim: Efficient Question Answering for Transformer</title>
				<funder ref="#_23tRryj">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
				<funder ref="#_A6vWNsJ #_2wR79Dv #_8QV3SHJ">
					<orgName type="full">National Natural Science Foundation of China</orgName>
					<orgName type="abbreviated">NSFC</orgName>
				</funder>
				<funder>
					<orgName type="full">Shanghai Pujiang Program</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-05-15">15 May 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yue</forename><surname>Guan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shanghai Qi Zhi Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhengyi</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shanghai Qi Zhi Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
							<email>lin.zhouhan@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuhao</forename><surname>Zhu</surname></persName>
							<email>yzhu@rochester.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Rochester</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jingwen</forename><surname>Leng</surname></persName>
							<email>leng-jw@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shanghai Qi Zhi Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Minyi</forename><surname>Guo</surname></persName>
							<email>guo-my@cs.sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shanghai Qi Zhi Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Block-Skim: Efficient Question Answering for Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-05-15">15 May 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2112.08560v2[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformer models have achieved promising results on natural language processing (NLP) tasks including extractive question answering (QA). Common Transformer encoders used in NLP tasks process the hidden states of all input tokens in the context paragraph throughout all layers. However, different from other tasks such as sequence classification, answering the raised question does not necessarily need all the tokens in the context paragraph. Following this motivation, we propose Block-Skim, which learns to skim unnecessary context in higher hidden layers to improve and accelerate the Transformer performance. The key idea of Block-Skim is to identify the context that must be further processed and those that could be safely discarded early on during inference. Critically, we find that such information could be sufficiently derived from the self-attention weights inside the Transformer model. We further prune the hidden states corresponding to the unnecessary positions early in lower layers, achieving significant inference-time speedup. To our surprise, we observe that models pruned in this way outperform their full-size counterparts. Block-Skim improves QA models' accuracy on different datasets and achieves 3? speedup on BERTbase model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The Transformer model <ref type="bibr" target="#b35">(Vaswani et al. 2017</ref>) has pushed model performance on various NLP applications to a new stage by introducing multi-head attention (MHA) mechanism <ref type="bibr" target="#b24">(Lin et al. 2017)</ref>. Further, the Transformer-based BERT <ref type="bibr" target="#b7">(Devlin et al. 2018</ref>) model advances its performances by introducing self-supervised pre-training and has reached state-of-the-art accuracy on many NLP tasks. This has made it at the core of many state-of-the-art models, especially in recent question answering (QA) models <ref type="bibr" target="#b17">(Huang et al. 2020)</ref>.</p><p>Our key insight for QA is that when human beings are answering a question with a passage as a context, they do not spend the same level of comprehension for each of the sentences equally across the paragraph. Most of the contents are quickly skimmed over with little attention on it, which means that for a specific question most of the contents are semantically redundant. However, in the Transformer architecture, all tokens go through the same amount</p><p>[CLS] Who played quarterback for the Broncos after Peyton Manning was benched ? <ref type="bibr">[SEP]</ref> Following their loss in the divisional round of the previous season 's playoffs , the Denver Broncos underwent numerous coaching changes, including a mutual parting with head coach John Fox ( who had won four divisional championships in his four years as Broncos head coach ), and the hiring of Gary Kubiak as the new head coach. under Kubiak, the Broncos planned to install a run -oriented offense with zone blocking to blend in with quarterback Peyton Manning's shotgun passing skills, but struggled with numerous changes and injuries to the offensive line, as well as manning having his worst statistical season since his rookie year with the Indianapolis Colts in 1998, due to a plantar fasciitis injury in his heel that he had suffered since the summer, and the simple fact that Manning was getting old, as he turned 39 in the 2015 off -season. Although the team had a 7 -0 start , Manning led the NFL in interceptions. In week 10, Manning suffered a partial tear of the plantar fasciitis in his left foot. He set the NFL's all -time record for career passing yards in this game, but was benched after throwing four interceptions in favor of backup quarterback Brock Osweiler , who took over as the starter for most of the remainder of the regular season. Osweiler was injured, however, leading to Manning's return during the week 17 regular season finale, where the Broncos were losing 13 -7 against the 4 -11 San Diego Chargers, resulting in Manning re -claiming the starting quarterback position for the playoffs by leading the team to a key 27 -20 win that enabled the team to clinch the number one overall AFC seed. Under defensive coordinator Wade Phillips, the Broncos'defense ranked number one in total yards allowed, passing yards allowed and sacks, and like the previous three seasons, the team has continued. <ref type="bibr">[SEP]</ref> Figure <ref type="figure">1</ref>: Example of Block-Skim method on a query from the SQuAD dataset. The question and answer tokens are annotated in red. Only question and few evidence blocks are fully processed (annotated by yellow). And other blocks are skimmed for acceleration with the knowledge from attention weights (annotated by grey). Here the block size is 32 tokens.</p><p>of computation, which suggests that we can take advantage of that by discarding many of the tokens early in the lower layers of the Transformer. This semantic level redundancy sheds light on effectively reducing the sequence lengths at higher layers. Since the execution overhead of self-attention increases quadratically w.r.t. sequence length, this semantic level pruning could significantly reduce the computation time for long contexts.</p><p>To excavate the efficiency from this insight, we propose to first chop up the context into blocks, and then learn a classifier to terminate those less relevant ones early in lower layers by looking at the attention weights as shown in Fig. <ref type="figure">1</ref>. Moreover, with the supervision of ground truth answer positions, a model that jointly learns to discard context blocks as well as answering questions exhibits significantly better performance over its full-size counterpart. Unfortunately, this also makes the proposed Block-Skim method dedicated for extractive QA downstream task. However, QA task is significant in real work production scenarios. Moreover, our method lies in the trade-off space between generality, usability, and efficiency. While sacrificing generality on appli-cable tasks, our proposed method is easy for adoption as it works as a plug-in for existing models. Similarly, leveraging the QA-specific attention weight patterns makes Block-Skim achieves better speedup results than other methods.</p><p>In this paper, we provide the first empirical study on attention feature maps to show that an attention map could carry enough information to locate the answer scope. We then propose Block-Skim, a plug-and-play module to the transformer-based models, to accelerate transformer-based models on QA tasks. By handling the attention weight matrices as feature maps, the CNN-based Block-Skim module extracts information from the attention mechanism to make a skim decision. With the predicted block mask, Block-Skim skips irrelevant context blocks, which do not enter subsequent layers' computation. Besides, we devise a new training paradigm that jointly trains the Block-Skim objective with the native QA objective, where extra optimization signals regarding the question position are given to the attention mechanism directly.</p><p>In our evaluation, we show Block-Skim improves the QA accuracy and F1 score on all the datasets and models we evaluated. Specifically, BERT base is accelerated for 3? without any accuracy loss.</p><p>This paper contributes to the following 3 aspects. ? We for the first time show that an attention map is effective for locating the answer position in the input. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Recurrent Models with Skimming. The idea to skip or skim irrelevant sections or tokens of input sequence has been studied in NLP models, especially recurrent neural networks (RNN) <ref type="bibr" target="#b30">(Rumelhart, Hinton, and Williams 1986)</ref> and long short-term memory network (LSTM) <ref type="bibr" target="#b16">(Hochreiter and Schmidhuber 1997)</ref>. LSTM-Jump <ref type="bibr" target="#b40">(Yu, Lee, and Le 2017)</ref> uses the policy-gradient reinforcement learning method to train an LSTM model that decides how many time steps to jump at each state. They also use hyper-parameters to control the tokens before a jump, maximum tokens to jump, and the maximum number of jumping. Skim-RNN <ref type="bibr" target="#b32">(Seo et al. 2018</ref>) dynamically decides the dimensionality and RNN model size to be used at the next time step. In specific, they adopt two "big" and "small" RNN models and select the "small" one for skimming. Structural-Jump-LSTM <ref type="bibr" target="#b15">(Hansen et al. 2018)</ref> uses two agents to decide whether to jump by a small step to the next token or structurally to the next punctuation. Skip-RNN <ref type="bibr" target="#b1">(Campos et al. 2017</ref>) learns to skip state updates and thus results in the reduced computation graph size. The difference of Block-Skim to these works is twofold. First, the previous works make the skimming decision based on the hidden states or embeddings during processing.</p><p>However, we are the first to analyze and utilize the attention mechanism for skimming. Secondly, our work is based on the Transformer model <ref type="bibr" target="#b35">(Vaswani et al. 2017)</ref>, which has outperformed the recurrent type models on most NLP tasks.</p><p>Transformer with Input Reduction. Unlike the sequential processing of the recurrent models, the Transformer model calculates all the input sequence tokens in parallel.</p><p>As such, skimming can be regarded as a reduction in sequence dimension. Power-BERT <ref type="bibr" target="#b10">(Goyal et al. 2020)</ref> extracts input sequence at a token level while processing. During the fine-tuning process for downstream tasks, Goyal et al.</p><p>proposes a soft-extraction layer to train the model jointly.</p><p>Length-Adaptive Transformer (Kim and Cho 2020) further extends Power-BERT by forwarding the rejected tokens to the final linear layer. Funnel-Transformer <ref type="bibr" target="#b4">(Dai et al. 2020)</ref> proposes a novel pyramid architecture with input sequence length dimension reduced gradually regardless of semantic clues. For tasks requiring full sequence length output, such as masked language modeling and extractive question answering, Funnel-Transformer up-samples at the input dimension to recover. DeFormer <ref type="bibr" target="#b2">(Cao et al. 2020)</ref> propose to pre-process and cache the paragraphs at shallow layers and only concatenate with the question parts at deep layers. Universal Transformer <ref type="bibr" target="#b6">(Dehghani et al. 2018)</ref> proposes a dynamic halting mechanism that determines the refinement steps for each token. Different from these works, Block-Skim utilizes attention information between question and token pairs and skims the input sequence at the block granularity accordingly. Moreover, Block-Skim does not modify the vanilla Transformer model, making it more applicable. Efficient Transformer. There are also many efforts for designing efficient Transformers <ref type="bibr" target="#b42">(Zhou et al. 2020;</ref><ref type="bibr" target="#b38">Wu et al. 2019;</ref><ref type="bibr" target="#b33">Tay et al. 2020)</ref>. For example, researchers have applied well-studied compression methods to Transformers, such as pruning <ref type="bibr" target="#b12">(Guo et al. 2020)</ref>, quantization <ref type="bibr" target="#b36">(Wang and Zhang 2020;</ref><ref type="bibr" target="#b13">Guo et al. 2022)</ref>, distillation <ref type="bibr" target="#b31">(Sanh et al. 2019)</ref>, and weight sharing. Other efforts focus on dedicated efficient attention mechanism considering its quadratic complexity of sequence length <ref type="bibr" target="#b21">(Kitaev, Kaiser, and Levskaya 2019;</ref><ref type="bibr" target="#b0">Beltagy, Peters, and Cohan 2020;</ref><ref type="bibr" target="#b41">Zaheer et al. 2020)</ref>. Block-Skim is orthogonal to these techniques on the input dimension reduction. We demonstrate that Block-Skim is compatible with efficient Transformers with experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention-based Block Relevance Prediction</head><p>Token-Level Relevance Analysis</p><p>Transformer. The Transformer model adopts the multi-head self-attention mechanism and calculates hidden states for each position as an attention-based weighted sum of input hidden states. The weight vector is calculated by parameterized linear projection query Q and key K as Equation <ref type="formula" target="#formula_0">1</ref>.</p><p>Given a sequence of input embeddings, the output contextual embedding is composed by the input sequence with different attention at each position, where Q, K are query and key matrix of input embeddings, d k is the length of a query or key vector. As such, the attention weight feature map is often visualized as a heatmap demonstrating the information gathering relationship along tokens <ref type="bibr" target="#b22">(Kovaleva et al. 2019)</ref>. The model exploits multiple parallel groups of such attention weights, a.k.a. attention heads, for attending to information at different positions.</p><formula xml:id="formula_0">Attention(Q, K) = Sof tmax(QK T / d k ),<label>(1)</label></formula><p>Extractive QA is one of the ultimate downstream tasks in the NLP. Given a text document and a question about the context, the answer is a contiguous span of the text. To predict the start and end position of the input context given a question, the embedding of each certain token is processed for all the layers in the Transformer encoder model. In many end-to-end open-domain QA systems, information retrieval is the preceding step at the coarse-grained passage or paragraph level for filtering out irrelevant passages. With the characteristic of the extractive QA problem where answer spans are part of the passage, our question is that whether we can apply a similar filtering technique at fine-grained granularity during the Transformer model inference.</p><p>In this work, we propose to augment the attention mechanism with the ability to predict the relevance of contextual tokens without modifying the original Transformer model. Prior work <ref type="bibr" target="#b10">(Goyal et al. 2020)</ref> shows that attention strength is a good indicator for answer tokens. However, we analyze the attention weight distribution of a trained BERT base model trained with SQuAD <ref type="bibr" target="#b29">(Rajpurkar et al. 2016)</ref> dataset and find that the attention weights of multi-head attention only have noticeably patterns at the late layers.  dataset. At late layers like Layer 9, the attention weights of answer tokens are significantly larger than those of irrelevant tokens. However, at early layers like Layer 4, the attention weight strength is indistinguishable for answer tokens and irrelevant tokens. For a better latency reduction, it is desirable to find irrelevant tokens as early as possible. However, using the attention weight value as the relevance criterion could be problematic at early layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN Based Block Relevance Prediction</head><p>Given the complex pattern of attention weights, we propose to use a CNN-based feature extractor to process the attention heatmaps as input image channels and predict the relevance of each token. To amortize the processing overhead, we split the input sequence X = (x 0 , x 1 , ..., x i ) into i/k exclusive blocks block j = (x j?k , x j?k+1 ..., x j?k+(k-1) ), where k is the block size, i.e. tokens included in the continuous input span. The relevance of a block is defined as whether it contains the exact final answer. As such, our goal is to figure out the blocks' relevance and skim the irrelevant ones during Transformer inference.</p><p>Fig. <ref type="figure" target="#fig_3">4</ref> shows the details of how we extract the attention information from the Transformer and feed them into the CNN model. In the CNN module, we use two 3 ? 3 convolution and one 1 ? 1 convolution, all of which use the ReLU operation <ref type="bibr" target="#b14">(Hahnloser and Seung 2001)</ref> as the activation function. We insert a 2 ? 2 average pooling layer for the first two 3 ? 3 convolutional layers to reduce the feature map size. In addition, we also use two batch normalization layers <ref type="bibr" target="#b18">(Ioffe and Szegedy 2015)</ref> to improve the prediction accuracy. To locate the answer context blocks, we use a linear classification layer to calculate the score for each block. The module outputs a block-level prediction mask that corresponds to the relevance of a block of input tokens to the question.</p><p>This model is trained with all attention heatmap profiled from the same set of heatmap data as described before. The prediction accuracy is shown as Fig. <ref type="figure" target="#fig_2">3</ref>. In general, the model achieves decent accuracy demonstrating that a CNN model is capable to extract the attending behavior information and locate the answer. Intuitively, the CNN models with higher layer attention heatmaps have better performance. It suggests that the backbone model becomes more convinced on question answering when it gets deeper. The total number of blocks and attention heads are K and H. We only show the main operations for simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simplifying CNN Predictor with Diagonal Attention</head><p>The above method of feeding the whole attention feature map to the CNN predictor has a major problem, which is the predictor needs to deal with the variable size of the attention feature map. As such, we simplify the input to the CNN model with only attention from its diagonal region. In specific, we only feed the diagonal heat-map region as the input representation for each input sequence block, as expressed in Fig. <ref type="figure" target="#fig_3">4</ref>. Our hypothesis is that the diagonal region of the attention heat map contains sufficient information to identify the block relevance. Because previous works <ref type="bibr" target="#b3">(Clark et al. 2019;</ref><ref type="bibr" target="#b11">Guan et al. 2020)</ref> show that the attention mechanism has several fixed patterns, that is, diagonal, stride, block, or dense types. And all of these patterns can be easily recognized with only the diagonal region.</p><p>Similarly, we optimize CNN models with reduced heatmap and the result is shown as Fig. <ref type="figure" target="#fig_2">3</ref>. As we can see, the models achieve similar prediction accuracy compared with using a whole attention weight heatmap. The result justifies our hypothesis that it is possible to use the diagonal information from attention heatmaps to predict the answer relevance. By doing so, the computation complexity is also reduced dramatically as the input size is much smaller.</p><p>The above finding confirms our hypothesis that the diagonal attention weight indeed carries information for figuring out answer positions. This motivates us to utilize such attention information to narrow the possible answer position along with the processing of the input sequence. In the next section, we introduce our design that uses a plug-andplay end-to-end learning module to extract useful information from the attention weights for skimming decisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformer with Block-Skim</head><p>The previous section shows the feasibility of using the attention weights to predict the relevance of token blocks. However, naively using the predictor can lead to significant degradation of the QA task accuracy. Because the block relevance predictor is only trained with the answer labels, it could fail in the multi-hop QA task, which requires information beyond the answer labels. To solve this problem, we propose an end-to-end multi-objective joint training paradigm. Then during inference time, the prediction of the Block-Skim model is augmented to filter the input sequence for acceleration. This causes a mismatch between training and inference models. However, skimming blocks during training makes joint training unstable. And our experimental results demonstrate that this mismatch is negligible. We give a detailed demonstration of the proposed joint training paradigm and inference process as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single-Task Multi-Objective Joint Training</head><p>Following the previous experiments, we append the aforementioned CNN models to each layer to predict the blocks' relevance and optimize them together with the backbone Transformer model. As such, there are two types of classifiers in the model augmented with Block-Skim module. The first is the original QA classifier at the last layer and the second is the block-level relevance classifier at each layer. These two classifiers optimize the same downstream task of predicting the answer position with an identical target label. However, they are fed with a different type of loss objectives, that is, the QA objective with Transformer output embeddings and the Block-Skim objective with attention weights. We jointly train these classifiers so that the training objective is to minimize the sum of all classifiers' losses.</p><p>The loss function of each block-level classifier is calculated as the cross-entropy loss against the ground truth label whether a block contains answer tokens or not. Equation <ref type="formula">2</ref>gives the formal definition. The total loss of the block-level classifier L BlockSkim is the sum of all blocks that only contain passage tokens. The reason is that we only want to throw away blocks with irrelevant passage tokens instead of questions. Blocks that have question tokens are not used in the training process.</p><formula xml:id="formula_1">L BlockSkim = mi?{passage blocks}</formula><p>CELoss(m i , y i )</p><formula xml:id="formula_2">y i =</formula><p>1 , block i has answer tokens 0 , block i has no answer tokens</p><p>(2)</p><p>For the calculation of the final total loss L total , we introduce two hyper-parameters in Equation <ref type="formula">3</ref>. We first use a harmony coefficient ? so that different models and settings could adjust the ratio between the QA loss and block-level relevance classifier loss. It is decided by grid search on the development set. We then use the balance factor ? to adjust the loss from positive and negative relevance blocks because there are typically many more blocks that contain no answer tokens (i.e., negative bocks) than the blocks that do contain answer tokens (i.e., positive bocks). This hyper-parameter selection will be explained in detail in experiments setup.</p><formula xml:id="formula_3">L total = L QA + ? i th layer (?L i,y=1</formula><p>BlockSkim + L i,y=0 BlockSkim )</p><p>(3) Our Block-Skim is a convenient plugin module owing to the following two reasons. First, it does not affect the backbone model calculation, because it only regularizes the attention value distribution with extra parameters to the backbone model. In other words, a model trained with Block-Skim can be used with it removed. Second, the introduced Block-Skim objective neither extra training signal nor reduces the QA accuracy. In fact, we will show that the extra gradient signal feeding to the attention improves the original QA accuracy. Multi-hop QA. Our joint training approach can also address the challenge in the multi-hop QA tasks <ref type="bibr" target="#b39">(Yang et al. 2018)</ref>, where deriving answers requires multiple pieces of evidence and reasoning. Although the block relevance prediction only uses the answer label signal, the original QA task ensures that evidence needs to be kept. In other words, the evidence reasoning information is encoded implicitly in the contextual embeddings. To illustrate such a point, we perform an ablation study that incorporates the evidence label the Block-Skim predictor training. The predictor accuracy does not improve with the additional evidence label, which confirms the effectiveness of our single-task multiobjective joint training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inference with Block-Skim</head><p>We now describe how to use the Block-Skim to accelerate the QA task inference. Although we add the block-level relevance classification loss in the joint training process, we do not actually throw away any blocks because it can skip answer blocks and the QA task training becomes unstable. However, we only augment block reduction with the Block-Skim module during the inference for saving computation and avoiding heavy changes to the underlying Transformer. During inference computation, we split the input sequence by the block granularity, which is a hyper-parameter in our model. The model skips a set of blocks according to the skimming module results for the following layers. With those design features, Block-Skim works as an add-on component to the original Transformer model and is compatible with many Transformer variant models as well as model compression methods.</p><p>We provide an analytical model to demonstrate the latency speedup potential of Block-Skim. Suppose that we insert the Block-Skim module to a vanilla model with the total L layers, and a portion of m i blocks remains for the following layers after layer i. The ideal processing complexity of one token for one Transformer layer is noted as T layer . Here we make an approximation that the computation complexity is linear to the sequence length N . This is a conservative approximation because the attention mechanism is O(N 2 ). The performance speedup is formulated by Equation <ref type="formula" target="#formula_4">4</ref>if we ignore the computation overhead of Block-Skim. In fact, the computation of a single Block-Skim module is smaller than Transformer layers for 100 times. For example, when m k ?{passage blocks} m k = 0.9, it means 10% of tokens are skimmed each layer. This skimming decision will result in a total speedup ratio of 1.86?.</p><formula xml:id="formula_4">Speedup = T V anilla T Block-Skim = L ? N ? T layer L i=0 ( i j=0 m j,k ?{layer j } m j,k ? N ) ? T layer = L L i=0 i j=0 m j,k ?{layer j } m j,k<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Experimental Setup</head><p>Dataset. We evaluate our method on 6 extractive QA datasets, including SQuAD 1.1 <ref type="bibr" target="#b29">(Rajpurkar et al. 2016</ref>), Natural Questions <ref type="bibr" target="#b23">(Kwiatkowski et al. 2019)</ref>, Trivi-aQA <ref type="bibr" target="#b19">(Joshi et al. 2017)</ref>, NewsQA <ref type="bibr" target="#b34">(Trischler et al. 2016)</ref>, SearchQA <ref type="bibr" target="#b8">(Dunn et al. 2017)</ref> and HotpotQA <ref type="bibr" target="#b39">(Yang et al. 2018)</ref>. HotpotQA provides questions that require multi-hop reasoning to answer with supporting facts. The diversity of these datasets such as various passage lengths and different document sources lets us evaluate the general applicability of the proposed method.</p><p>Model. We follow the setting of the BERT model to use the structure of the Transformer encoder and a linear classification layer for all the datasets. As previously explained, Block-Skim works as an add-on module to the vanilla Transformer, and therefore is generally applicable to all Transformer-based models, as well as model compression methods. To illustrate this point, we apply the Block-Skim method to two BERT models with different size settings. We evaluate the base setting with 12 heads and 12 layers, as well And for the other datasets, we apply batch size 32 and maximum sequence length 512. We perform all the experiments reported with random seed 42. We train a baseline model and Block-Skim model with the same setting for two epochs and report accuracies from MRQA task benchmark for comparison. We use four V100 GPUs with 32 GB memory for the training experiments.</p><p>The balance factor ? is determined by block sample numbers and reported in Tbl. 1. The harmony factor ? is 0.01 for ALBERT and 0.1 for all the other models we used. It is determined by hyper-parameter grid search from 1e -3 to 10 with a step of ?10.</p><p>We use the inference FLOPs as a general measurement of the model computational complexity on all platforms. We use TorchProfile <ref type="bibr" target="#b26">(Liu 2020)</ref> to calculate the FLOPs for each model and normalize the results as a ratio to BERT base .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joint Training Results</head><p>We first evaluate Block-Skim joint training effect to the QA task by comparing BERT base models and their variants with Block-Skim augmented. In their Block-Skim versions, the Block-Skim modules only participate in the training process and are removed in the inference task. Tbl. 1 shows the result on multiple QA datasets. Block-Skim outperforms the baseline training objective on all datasets evaluated and exceeds with 0.58% F1 score on average. This suggests that the Block-Skim objective is consistent with the QA objective and even improves its accuracy. The results show the wide applicability of our method to different datasets with varying difficulty and complexity.</p><p>We further show the robustness when using the Block-Skim joint training as an add-on module. Tbl. 2 shows the result of multiple runs using the identical optimization setting with different random seeds. By introducing the Block-Skim loss in Training, the QA accuracy of the backbone model is improved for 0.4 on exact match and 0.32 on F1 score.  2 times speedup. With Block-Skim method appended to this model, the methods can be further accelerated with no or minor accuracy loss. Specifically, using Block-Skim with DistilBERT achieves 5? speedup compared to the vanilla BERT model. And even with head-pruning reducing the attention information, Block-Skim is also compatible and achieves over 2? speedup. Even though still compatible, Block-Skim gets less acceleration on ALBERT models. We suggest that sharing parameters of attention mechanism makes it harder to optimize with extra Block-Skim objective. As the proposed Block-Skim method aims to reduce the input sequence dimension semantic redundancy, it is compatible to these model compression methods focusing on model redundancy theoretically. By designing Block-Skim not to modify the backbone model, our method is generally applicable to these algorithms as well as other model pruning methods <ref type="bibr" target="#b12">(Guo et al. 2020;</ref><ref type="bibr" target="#b28">Qiu et al. 2019;</ref><ref type="bibr" target="#b9">Gan et al. 2020)</ref>.</p><p>Block-Skim achieves close speedup with less accuracy degradation compared to Deformer and more speedup with similar accuracy degradation compared to Length-Adaptive Transformer on SQuAD-1.1 dataset. This suggests Block-Skim captures the runtime semantic redundancy better. Although this also makes it only applicable to QA tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>We design a series ablation experiment of components in Block-Skim to study their individual effect. The experiments are performed based on the same setting. We report the detailed results in Tbl. 3, and summarize the key findings as follows. ID-3. Instead of joint training, we perform a two-step training. We first perform the fine-tuning for the QA task. We then perform the Block-Skim training with the baseline QA model frozen. In other words, we only use the Block-Skim objective and only update the weights in the Block-Skim modules. Therefore, the QA accuracy remains the same as the baseline model, which is lower than the joint training (ID-3). Meanwhile, the Block-Skim classifier also has a lower accuracy than the joint training especially at layer 6. ID-4 We skim blocks during the joint Block-Skim QA training process. Because the mis-skimmed blocks may confuse the QA optimization, it leads to a considerable accuracy loss. ID-5-ID-9. We study the impact of different block sizes. Specifically, when the block size is 1, it is equivalent to skim at the token granularity. Our experimental result shows that the accuracy of Block-Skim classifier is better when the block size is larger. On the other hand, a larger block size also leads to less number of blocks and therefore the performance speedup becomes limited. To this end, we choose the block size of 32 as a design sweet spot. ID-11-ID-12. We evaluate the applicability of Block-Skim to multi-hop QA task with HotpotQA dataset. As introduced in , we add supporting facts (i.e., evidence) for each question to the Block-Skim objective in the ID-12 experiment by labelling evidence blocks to 1 in Eq.2 for the skim predictor modules. This leads to a higher QA accuracy. But the average accuracy of skim predictors at all layers is worse, which is 86.08% compared to 92.67%. This ablation experiment shows that our single-task multi-objective joint training is already able to capture the evidence information, rendering explicitly adding it to the training unnecessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this work, we propose a plug-and-play Block-Skim module to Transformer and its variants for efficient QA processing. We empirically demonstrate that the attention mechanism can provide instructive information for locating the answer span. Leveraging this insight, we propose to learn the attention in a supervised manner, which terminates irrelevant blocks at early layers, significantly reducing the computations. Besides, the proposed Block-Skim training objective provides attention mechanism with extra learning signal and improves QA accuracy on all datasets and models we evaluated. With the use of Block-Skim module, such distinction is strengthened in a supervised fashion. This idea may be also applicable to other tasks and architectures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Attention weight value distribution comparison on the answer and irrelevant tokens. The attention heatmaps are profiled on the development set of SQuAD dataset with a BERT base model with 12 layers and 12 attention heads per layer. The full results are shown in appendix.</figDesc><graphic url="image-2.png" coords="3,54.00,159.05,238.50,104.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig. 2 compares the attention weights at Layer 4 and 9 in the trained BERT base model. The tokens are classified to answer tokens or irrelevant tokens with the labels from the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Accuracy of CNN model predicting whether a block contains answer with attention weight as input. The CNN is feed with either an all attention weight heatmap or only the diagonal block region.</figDesc><graphic url="image-3.png" coords="3,319.50,54.00,238.50,97.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: The overall schematic of Block-Skim and the architecture of the CNN model. Here we take block size 32 as example. The total number of blocks and attention heads are K and H. We only show the main operations for simplicity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: FLOPs speedup of different models and model compression methods with Block-Skim on SQuAD and Hot-potQA datasets. The FLOPs are normalized to BERT base result of 48.32G FLOPs result. The results of vanilla models with different size, model compression algorithms and Block-Skim augmented methods are grouped together.</figDesc><graphic url="image-5.png" coords="7,54.00,159.02,238.50,104.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results of multiple runs under same training and hyper-parameter setting with different random seeds.</figDesc><table><row><cell>Seed</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>Avg. Std.</cell></row><row><cell>Vanilla</cell><cell cols="6">EM 80.95 81.08 80.98 81.06 80.96 81.00 0.06 F1 88.32 88.44 88.59 88.44 88.42 88.44 0.10</cell></row><row><cell>Block-Skim</cell><cell cols="6">EM 81.52 81.25 81.24 81.51 81.84 81.47 0.25 F1 88.92 88.48 88.66 88.76 88.99 88.76 0.20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation studies of the Block-Skim components with BERT base backbone model on SQuAD and HotpotQA datasets.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The source code is available at https://github.com/ ChandlerGuan/blockskim.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We use pre-trained language model checkpoints released from https://huggingface.co/models.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was supported by the <rs type="funder">National Key R&amp;D Program of China</rs> under Grant <rs type="grantNumber">2019YFF0302600</rs>, and the <rs type="funder">National Natural Science Foundation of China (NSFC)</rs> grant (<rs type="grantNumber">62072297</rs>, <rs type="grantNumber">62106143</rs>, and <rs type="grantNumber">61832006</rs>). We would like to thank the anonymous reviewers for their thoughtful comments and constructive suggestions. Zhouhan Lin is also supported by <rs type="funder">Shanghai Pujiang Program</rs>. We also thank <rs type="person">Yuxian Qiu</rs> and <rs type="person">Kexin Li</rs> with whom we have inspiring discussions on the evaluation experiment design. Finally, we thank <rs type="person">Zhihui Zhang</rs> for helping the presentation and visualization of experimental results. <rs type="person">Jingwen Leng</rs> and <rs type="person">Zhouhan Lin</rs> are the corresponding authors of this paper. Figure 6: Attention weight value distribution comparison on the answer and irrelevant tokens. The attention heatmaps are profiled on the development set of SQuAD dataset with a BERT base model with 12 layers and 12 attention heads per layer.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_23tRryj">
					<idno type="grant-number">2019YFF0302600</idno>
				</org>
				<org type="funding" xml:id="_A6vWNsJ">
					<idno type="grant-number">62072297</idno>
				</org>
				<org type="funding" xml:id="_2wR79Dv">
					<idno type="grant-number">62106143</idno>
				</org>
				<org type="funding" xml:id="_8QV3SHJ">
					<idno type="grant-number">61832006</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inference Speedup Results</head><p>Results on Various Datasets. The FLOPs speedup result normalized to BERT base model is demonstrated in Tbl. 1. Block-Skim achieves 2.59? speedup on average with a minor accuracy degradation of 0.23 on different datasets evaluated. On the multi-hop QA dataset HotpotQA, our method also achieves 2.28 times speedup. The results show that the proposed Block-Skim method is capable to identify the semantic redundancy with attention information.</p><p>Comparison to Vanilla BERT Baseline. Block-Skim improves the BERT base model inference latency by 3.1? and 2.4? respectively on SQuAD and HotpotQA datasets. When treating the model size settings of vanilla BERT model as a trade-off between accuracy and complexity, Block-Skim improves this trade-off by a margin. As shown in Fig. <ref type="figure">5</ref> our method accelerate BERT large as fast as the vanilla BERT base model but with a much higher accuracy. In specific, the latency of vanilla BERT large model is 3.47? of BERT base , and our method reduces the gap to 1.09? on SQuAD dataset, which translates to the 3.18? speedup.</p><p>Compatibility to Model Compression Methods. We compare the Block-Skim's compatibility to other model compression methods with Fig. <ref type="figure">5</ref>. These model compression methods trade accuracy for computation complexity to different extents. For example, distilling 12-layer BERT base model to 6 layers results in a 2% accuracy decrease and </p><note type="other">ID Description Update Transformer Skim Training Block-Skim Module Block Size QA EM F1 SQuAD</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix Attention Distribution</head><p>We show the full results of attention weight value distribution discussed in Fig. <ref type="figure">2</ref>. Fig. <ref type="figure">6</ref> shows that deeper layers have more distinguishable patterns.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<title level="m">Longformer: The long-document transformer</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gir?-I-Nieto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<idno>CoRR, abs/1708.06834</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deformer: Decomposing pre-trained transformers for faster question answering</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Balasubramanian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00697</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">What Does BERT Look at? An Analysis of BERT&apos;s Attention</title>
		<author>
			<persName><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACL Workshop Black-boxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2019 ACL Workshop Black-boxNLP: Analyzing and Interpreting Neural Networks for NLP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="276" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><surname>Funnel-Transformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03236</idno>
		<title level="m">Filtering out Sequential Redundancy for Efficient Language Processing</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Universal Transformers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<meeting><address><addrLine>Bert</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">U</forename><surname>Guney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cirik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05179</idno>
		<title level="m">Searchqa: A new q&amp;a dataset augmented with context from a search engine</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ptolemy: Architecture Support for Robust Deep Learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<publisher>MICRO</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chakaravarthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Man-Ishraje</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Verma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08950</idno>
		<title level="m">PoWER-BERT: Accelerating BERT inference for Classification Tasks</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.00943</idno>
		<title level="m">How Far Does BERT Look At: Distance-based Clustering and Analysis of BERT s Attention</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Y</forename><surname>Hsueh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.13006</idno>
		<title level="m">Accelerating Sparse DNN Models without Hardware-Support via Tile-Wise Sparsity</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">SQuant: On-the-Fly Data-Free Quantization via Diagonal Hessian Approximation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Permitted and forbidden sets in symmetric threshold-linear networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Hahnloser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="217" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural Speed Reading with Structural-Jump-LSTM</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Alstrup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Simonsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recent Trends in Deep Learning Based Open-Domain Textual Question Answering Systems</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="94341" to="94356" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension</title>
		<author>
			<persName><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1601" to="1611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.07003</idno>
		<title level="m">Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime with Search</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reformer: The Efficient Transformer</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Revealing the dark secrets of BERT</title>
		<author>
			<persName><forename type="first">O</forename><surname>Kovaleva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rumshisky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08593</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Natural questions: a benchmark for question answering research</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="453" to="466" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">ALBERT: A Lite BERT for Selfsupervised Learning of Language Representations</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N D</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03130</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2019. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>A structured self-attentive sentence embedding</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<title level="m">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://github.com/zhijian-liu/torchprofile/" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Are Sixteen Heads Really Better than One?</title>
		<author>
			<persName><forename type="first">P</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="14014" to="14024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adversarial Defense Through Network Profiling Based Path Extraction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">SQuAD: 100, 000+ Questions for Machine Comprehension of Text</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning representations by back-propagating errors. nature</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page" from="533" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01108</idno>
		<title level="m">Dis-tilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Neural Speed Reading via Skim-RNN</title>
		<author>
			<persName><forename type="first">M</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<idno>arXiv-2009</idno>
		<title level="m">Efficient Transformers: A Survey. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">NEWSQA: A MA-CHINE COMPREHENSION DATASET</title>
		<author>
			<persName><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Suleman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Q-BERT: A BERT-based Framework for Computing SPARQL Similarity in Natural Language</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion Proceedings of the Web Conference 2020</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="65" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">HuggingFace&apos;s Transformers: State-of-the-art Natural Language Processing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno>ArXiv, abs/1910.03771</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Lite Transformer with Long-Short Range Attention</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning to Skim Text</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.14062</idno>
		<title level="m">Transformers for longer sequences</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<title level="m">BERT Loses Patience: Fast and Robust Inference with Early Exit. arXiv</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">2006</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
