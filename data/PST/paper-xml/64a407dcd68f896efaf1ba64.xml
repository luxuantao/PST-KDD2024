<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Profiling gem5 Simulator</title>
				<funder ref="#_8QTSnz5">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
				<funder>
					<orgName type="full">DARPA</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Johnson</forename><surname>Umeike</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical Engineering and Computer Science Department</orgName>
								<orgName type="institution">University of Kansas</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Neel</forename><surname>Patel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical Engineering and Computer Science Department</orgName>
								<orgName type="institution">University of Kansas</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alex</forename><surname>Manley</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical Engineering and Computer Science Department</orgName>
								<orgName type="institution">University of Kansas</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Amin</forename><surname>Mamandipoor</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical Engineering and Computer Science Department</orgName>
								<orgName type="institution">University of Kansas</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Heechul</forename><surname>Yun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical Engineering and Computer Science Department</orgName>
								<orgName type="institution">University of Kansas</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohammad</forename><surname>Alian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical Engineering and Computer Science Department</orgName>
								<orgName type="institution">University of Kansas</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Profiling gem5 Simulator</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we set out to find the answers to the following questions: (1) Where are the bottlenecks in a state-of-theart architectural simulator? (2) How much faster can architectural simulations run by tuning system configurations? (3) What are the opportunities in accelerating software simulation using hardware accelerators? We choose gem5 as the representative architectural simulator, run several simulations with various configurations, perform a detailed architectural analysis of the gem5 source code on different server platforms, tune both system and architectural settings for running simulations, and discuss the future opportunities in accelerating gem5 as an important application. Our detailed profiling of gem5 reveals that its performance is extremely sensitive to the size of the L1 cache. Our experimental results show that a RISC-V core with 32KB data and instruction cache improves gem5's simulation speed by 31%?61% compared with a baseline core with 8KB L1 caches. Our paper is the first step toward building specialized hardware and software environments for accelerating software-based simulators.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>We are in the golden age of computer architecture <ref type="bibr" target="#b0">[1]</ref> where the continuation of Moore's law is premised upon the specialization of hardware for different application domains. This simply means that computer architects are going to design many more hardware in the years to come.</p><p>Software-based simulation is the backbone of computer architecture research and development. Since the inception of computer architecture as a field, many software-based architectural simulators <ref type="foot" target="#foot_0">1</ref> and simulation techniques have emerged <ref type="bibr" target="#b1">[2]</ref>. Currently, various architectural simulators are in use by academia and industry for modeling different aspects of future computing platforms. gem5 <ref type="bibr" target="#b2">[3]</ref>, Sniper <ref type="bibr" target="#b3">[4]</ref>, MARSSx86 <ref type="bibr" target="#b4">[5]</ref>, and ZSim <ref type="bibr" target="#b5">[6]</ref> are just a few examples of architectural simulators currently with active communities. Designing hardware requires many hours of simulation and this figure will only increase in the future due to the proliferation of open-source hardware <ref type="bibr" target="#b6">[7]</ref> and the need for domain-specific hardware design.</p><p>Improving simulation performance has been in the spotlight from the early implementations of software-based simulators. Throughout the years, many techniques such as parallelizing simulation on multiple cores <ref type="bibr" target="#b5">[6]</ref> or multiple nodes <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, using hardware virtualization support <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, sampling techniques <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, trading off simulation accuracy for speed <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, and using configurable hardware for modeling flexible systems <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> have been proposed and implemented to improve simulation performance. Previous works often overlook simple software and system optimizations that can significantly improve the simulation speed without introducing complex changes to the simulator. In this work, we set to fill this gap.</p><p>gem5 is one of the most widely used architectural simulators, providing a platform for modeling future computer systems <ref type="bibr" target="#b19">[20]</ref>. gem5 also supports various modes of execution as well as different levels of simulation detail. Due to the ubiquity of gem5 and its large user base, we select gem5 as the representative simulator for this work. We simulate different workloads on gem5 with diverse configurations, profile gem5 code, and perform a detailed architectural analysis of the gem5 execution to find the bottlenecks in the official gem5 release. We compare the simulation time, (measured as host seconds) when running gem5 on two different platforms: Intel Xeon and Apple M1 Chips. We perform a detailed comparison of the architectural statistics between the platforms. We also run gem5 on FireSim <ref type="bibr" target="#b16">[17]</ref>, which is an FPGA-accelerated architectural simulator, to investigate the sensitivity of gem5's speed to some architectural parameters. Finally, we use our profiling insights to perform simple system tuning and propose architectural recommendations to improve gem5 simulation speed.</p><p>This work is the first step towards better understanding the characteristics of a state-of-the-art architectural simulator and developing hardware and software solutions to meet the growing demand for architectural simulation. Our major contributions in this paper hinge upon answering the following questions:</p><p>? Where are the bottlenecks in running gem5 on a Xeon server? Our results show that gem5 is extremely frontend (instruction) bound with large iCache and TLB miss rates. Due to the huge code size, an abundance of virtual functions, and runtime polymorphism in the source code, there is no particular hot function or code block in gem5. As a result, the decoder unit in an out-of-order processor is under extreme pressure to supply ?Ops to the back-end, and there is large misprediction and resteer overhead in the pipeline's front-end. ? How does the performance of gem5 vary in different server platforms when running simulations? When running architectural simulations, the focus is usually on the configuration of the simulated system but the configuration of the host is often ignored. Our results show that the underlying physical hardware notably impacts simulation time. For instance, a MacBook Pro with an M1 chip completes the same simulation 1.7??3.02? faster than a server equipped with Xeon Gold 6242R CPUs and 96GB of DDR4 DRAM. ? Long term solution? Lastly, we discuss some of the solutions moving forward, such as designing specialized accelerators for simulation. The rest of this work is organized as follows. In Sec.II, we discuss the motivation for this work and provide background information on computer architecture simulators. In Sec.III, we describe our methodology for profiling gem5 and collecting experimental results. In Sec.IV, we analyze gem5 source code and microarchitectural statistics and reveal the runtime execution characteristics of gem5 running on different platforms. In Sec.V we discuss the sensitivity of gem5 speed to varying system and architectural configurations of the simulation server. Sec.VI summarizes some of our takeaways and discusses future directions for accelerating simulation speed. Sec.VIII concludes this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. MOTIVATION AND BACKGROUND</head><p>Simulation is extensively used in both academia and industry. Although the bar for the accuracy of simulation in academic research is lower, potentially impactful academic research requires full-system modeling of various hardware and software components <ref type="bibr" target="#b20">[21]</ref>. Ideally, we would have a simulator that is as fast as the real hardware, as flexible as a software implementation, and performant as the target hardware. However, the speed of simulation, and complexity of implementation are influenced by the required simulation detail <ref type="bibr" target="#b1">[2]</ref>.</p><p>We can classify architectural simulation into two categories: functional and timing. Functional simulation (i.e., emulation) models the functionality of future hardware. This is mainly used for validating hardware functionalities and software development and testing before the hardware is built. Some examples are HASE <ref type="bibr" target="#b21">[22]</ref>, simCore <ref type="bibr" target="#b22">[23]</ref>, Barra <ref type="bibr" target="#b23">[24]</ref>, Simics <ref type="bibr" target="#b24">[25]</ref>, AtomicSimple CPU configuration in gem5 <ref type="bibr" target="#b2">[3]</ref>. Timing simulation (i.e., performance simulation) is used to model the timing aspects of hardware while providing the correct functionality. There are timing simulators with different performance-modeling fidelity. Clearly, the complexity of a simulator proportionally increases with its modeling fidelity. Some examples are zSim <ref type="bibr" target="#b5">[6]</ref>, sniper <ref type="bibr" target="#b3">[4]</ref>, HAsim <ref type="bibr" target="#b17">[18]</ref>, gem5 <ref type="bibr" target="#b2">[3]</ref>, Simple Scalar <ref type="bibr" target="#b25">[26]</ref>. Additionally, simulators can operate in user-level or full-system mode. In the user-level mode, the simulator only executes user-level code without modeling the operating system. System calls are bypassed and serviced by the underlying host. This mode is also referred to as system-call emulation mode. On the other hand, a full-system simulator models the entire computing system, including memory, and I/O subsystems while running an unmodified operating system.</p><p>gem5 is a state-of-the-art architectural simulator with an active user and developer community. It is extremely configurable, supports multiple ISAs, and can perform full-system simulations with network and device modeling. This makes gem5 a valuable tool for evaluating future accelerators, processor cores, system-on-chips, hardware/OS/network interactions, and heterogeneous systems <ref type="bibr" target="#b19">[20]</ref>.</p><p>One observation that motivated this work is the drastic difference in simulation speed when running gem5 on different server platforms. Fig. <ref type="figure">1</ref> shows the geometric mean of the simulation time of executing gem5 on a MacBook Pro (M1_Pro) and a Mac Studio (M1_Ultra), normalized to the simulation time on a Dell server equipped with Xeon Gold Scalable CPUs (Intel_Xeon) across all nine (9) PARSEC and SPLASH-2x workloads. Both MacBook and Mac Studio are equipped with Apple M1 chips. More information on the workloads, simulated system configuration, physical hardware configurations, and experimental methodology are provided in Sec.III. We run gem5 in full-system (FS) and system-call emulation (SE) modes. An important parameter in the tests performed is the number of processes simultaneously running on each platform. In the left most and right most sub-graphs of Fig. <ref type="figure">1</ref>, we run a single gem5 process on the host server, while in the middle sub-graphs, we co-run one gem5 process per physical core and one gem5 process per hardware thread. There are 4, and 16 performance cores in M1_Pro and M1_Ultra, and 20 physical cores and 40 hardware threads on Intel_Xeon. Therefore for corunning scenarios, we launch 4 (M1_Pro), 16 (M1_Ultra), 20 (Intel_Xeon for "gem5 processes = # of physical cores" with SMT off configuration), and 40 (Intel_Xeon for "gem5 processes = # of hardware threads" with SMT on configuration) gem5 processes.</p><p>As shown in the figure, regardless of whether SMT is turned on or off for Intel_Xeon (it is worth noting that M1_Pro and M1_Ultra does not support hardware multithreading), simulation mode (full system vs. system-call emulation) or simulation detail (Atomic vs. Timing or In-order vs. Out-of-Order), M1 platforms consistently deliver lower simulation time. This applies to different benchmarks simulated on gem5 as illustrated in Fig. <ref type="figure">1</ref>. The simulation speed of M1 platforms is even higher when co-running multiple gem5 processes. As depicted, running gem5 on an M1_Ultra is up to 4.15? faster compared with execution on a high-end Xeon server. We see on average ?47% performance improvement on Intel_Xeon with SMT disabled. That is, the simulation time of running 20 gem5 processes (with SMT disabled) is ?47% less than running 40 gem5 processes (SMT enabled). As we will discuss in Sec. IV this is expected since gem5 is sensitive to L1 cache size and disabling SMT will reduce cache contention, thus improving the overall simulation speed.</p><p>Motivated by the huge speedup gains by just running gem5 on a different platform, we set out to profile the execution of gem5 on both Intel and M1 platforms to shed light on gem5's execution bottlenecks. Many of our insights from profiling gem5 can be applied to other architectural simulators or even simulators in different fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>In terms of simulation configurations, we change the CPU type, number of CPUs, and memory size. We use the following CPU types: AtomicSimpleCPU (Atomic): CPU type with CPI = 1 where memory accesses are atomic and completed without modeling any contention or queuing delays. TimingSimpleCPU (Timing): CPU type with CPI = 1 where memory accesses are modeled in detail considering the queuing delays and resource contentions in the memory and interconnect.</p><p>In-order CPU (Minor): In-order or Minor CPU models a fixed pipeline with strict in-order instruction execution. Minor CPU uses the detailed timing memory model for accessing memory. Out-of-order CPU (O3): O3 CPU models an out-of-order superscalar loosely based on the Alpha 21264 core. O3 CPU uses the detailed timing memory model for accessing memory.</p><p>Simple CPUs are used for fast-forwarding simulation, warming up caches, or for studies that do not require detailed CPU modeling. In-order and out-of-order CPU models are used for detailed microarchitectural studies. Table <ref type="table" target="#tab_2">I</ref> shows the processor configuration for each CPU type. We used Linaro 7.5.0 toolchain for SPLASH-2x and an Ubuntu 18.04 disk image for PARSEC workloads, respectively. We use Linux kernel 5.4.0 and ARM ISA for full-system simulations <ref type="bibr" target="#b26">[27]</ref>. We simulate the following workloads on gem5:</p><p>? Boot-Exit: Boot Linux in FS mode and immediately exit. Note that M1_Pro and M1_Ultra cannot take readable checkpoints so we use them to recover from checkpoints taken by Intel_Xeon.  We used the Intel VTune profiler <ref type="bibr" target="#b28">[29]</ref> to access the processor performance counters and perform the Top-Down microarchitectural analysis <ref type="bibr" target="#b29">[30]</ref>. To collect performance counters on the Intel_Xeon CPUs, perf was used <ref type="bibr" target="#b30">[31]</ref> . We also profile Apple M1 CPUs by reading performance counters from the privileged level <ref type="bibr" target="#b31">[32]</ref>. We modify the main simulation loop function in gem5 to read the performance counters for each execution.</p><p>Experiments are run on three platforms. Table <ref type="table" target="#tab_4">II</ref> summarises the configuration of these three platforms <ref type="bibr" target="#b32">[33]</ref>. We refer to these platforms using their configuration name (Config Name in Table <ref type="table" target="#tab_4">II</ref>) throughout the paper: Intel_Xeon, M1_Pro, M1_Ultra.</p><p>Since gem5 is a single-threaded application, its microarchitectural behavior can be directly compared to single-threaded CPU benchmarks. For this comparison, we choose a mix of 3 benchmarks from the SPEC 2017 benchmark suite <ref type="bibr" target="#b33">[34]</ref>. The SPEC workloads that we choose are:</p><p>? 525.x264_r has been observed to have the highest IPC of all benchmark in SPEC 2017 suite <ref type="bibr" target="#b34">[35]</ref>. ? 531.deepsjeng_r has a large memory footprint and has been observed to have the highest L3 cache miss rate among other SPEC benchmarks <ref type="bibr" target="#b35">[36]</ref>. ? 505.mcf_r is chosen due to its high front-end and back-end stalls resulting from cache misses, and branch misprediction. 505.mcf_r has the lowest IPC of all benchmarks in SPEC 2017 suite <ref type="bibr" target="#b34">[35]</ref>. Note that we run SPEC benchmarks on bare metal hardware, not on gem5. We only use SPEC benchmarks as a reference to compare with gem5's top-down profile in Sec.IV-A.</p><p>Using FireMarshal <ref type="bibr" target="#b36">[37]</ref>, we run gem5 as a workload on Firesim <ref type="bibr" target="#b16">[17]</ref> for profiling purposes. In our study, we execute gem5 in system-call emulation mode on a chipyard SoC design <ref type="bibr" target="#b37">[38]</ref>. Our base hardware configuration on Firesim is a quad-core Rocket chip, which is a RISC-V open-source CPU with a single- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PROFILING GEM5</head><p>In this section, we profile gem5 simulating various configurations and report microarchitectural statistics to demystify the execution inefficiencies of gem5 as an application. Our goal is to gain some insights into the execution profile of gem5 to set the stage for future targeted hardware-level, system-level, and application-level optimizations that will make gem5 run faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Top-Down Analysis on Intel Xeon Platform</head><p>We use VTune to perform a Top-Down microarchitectural performance analysis <ref type="bibr" target="#b29">[30]</ref> of gem5 when simulating different workloads with varying configurations. Top-Down analysis split the machine cycles into four categories: retiring, front-end bound, bad speculation, and back-end bound. Ideally, we want every cycle to be categorized as retiring, which is the only category in which the CPU performs useful work. A cycle is considered to be front-end bound if the fetch and decode units (i.e., front-end of the out-of-order processor) cannot supply sufficient ?-ops for the back-end. The main culprits for frontend bound cycles are iCache/iTLB misses and inefficiencies in the instruction decoders. A cycle is considered to be back-end bound when the processor is stalled because there are not enough resources in the back-end. This would occur when the load/store queue is full or the functional units are all busy. Lastly, a cycle is considered to be bad speculation when a cycle is lost due to running miss-speculated instructions or recovering from previous bad speculation.</p><p>Fig. <ref type="figure">2</ref> Moreover, the simulated memory size is often small and limited to a few gigabytes, which is not even fully touched by the simulated workload. The small dynamic working set size and temporally slow memory access to this working set results in predictable data cache accesses from gem5 that can be efficiently captured by the hardware prefetchers or overlapped in the out-of-order engine of the modern processors.</p><p>Hyper-scale workloads such as web-search, web-serving, and video processing are known to have large instruction cache footprint and thus are considered to be front-end bound as their front-end bound cycles are 2?3? more than those in typical SPEC benchmarks <ref type="bibr" target="#b38">[39]</ref> (in the 15?30% range across various workloads). Looking at Fig. <ref type="figure">2</ref>(top), the front-end bound cycles for gem5 are in the 30.1%?41.5% range, which is even higher than that of hyper-scale workloads. Next, we present a breakdown of performance events impacting front-end bound stall cycles to find out why such a large number of cycles are spent waiting for the front-end to supply instructions.</p><p>Figure <ref type="figure">3</ref> shows the classification of the front-end bound cycles between front-end bandwidth and latency. The main reasons for bandwidth-and latency-bound cycles are inefficiencies in instruction decoding and iCache/iTLB misses, respectively. As shown in Fig. <ref type="figure">3</ref>(top), simpler CPU models are more skewed toward bandwidth-bound and as the level of CPU detail increases, the front-end becomes more latency-bound.  This can be explained by the fact that as the complexity of the CPU model increases, gem5 touches more simulation object binaries for processing each event. Therefore, the instruction cache footprint increases with the CPU model complexity, and consequently, gem5 becomes more front-end latency-bound. Compared with SPEC, gem5 is more front-end bandwidthbound. Next, we zoom into the front-end latency and bandwidth breakdown to better understand the bottlenecks in the front-end when executing gem5 on Intel_Xeon. Figure <ref type="figure" target="#fig_0">4</ref>(top) plots the breakdown of front-end latencybound cycles. As illustrated in the figure, the O3 and Minor CPUs have up to 11? higher iCache misses compared with Atomic CPU simulations. Interestingly, stalled cycles due to iTLB misses are high across all gem5 executions. On the other hand, SPEC benchmarks are neither iCache nor iTLB bound. Along with iCache and iTLB overheads, we see a huge increase in the branching-related overhead when using O3 and Minor CPUs. The aggregated branching overhead for O3 PARSEC and Minor PARSEC (sum of Mispredict Resteers, Clear Resteers, and Unknown Branches) is 6.0? and 4.7? higher than that of ATOMIC PARSEC. As shown in the figure, by using more detailed CPUs, the percentage of unknown branches significantly increases. The high branch overhead of detailed gem5 simulation occurs because increasing the CPU model's complexity initiates more function calls, parameter checks, and event generation and activation. These, in turn, increase the branch density of the code, contribute to the large branch overhead, and increase the number of hard-to-predict branches. For SPEC benchmarks, the branching category contributes to the majority of the front-end latencybound cycles. Mispredict Resteers and Unknown Branches alone contribute to 43.5%?73.6% of total front-end latencybound cycles in SPEC benchmarks.</p><p>Figure <ref type="figure">5</ref>(top) shows the breakdown of bandwidth-bound cycles. Interestingly, between 92?97% of the front-end bandwidth-bound cycles are limited due to waiting for MITE (Micro-Instruction Translation Engine), and only less than 7% are bounded by the DSB (know as Decoded iCache or ?Op Cache) ?Op supply. ?Op cache is a small memory structure in the decoder unit that holds hot ?Op traces. ?Op cache works for codes with lots of instruction reuse and loops, which  are both rare in gem5. The irregularity in the gem5's code results in a lot of pressure on the instruction decoder to supply enough instructions to the back-end. Compared to gem5, as shown in Fig. <ref type="figure">5</ref>(bottom), when running SPEC, more of the front-end bandwidth-bound cycles are categorized under DSB. This is because the DSB coverage for regular applications is often very high. Fig. <ref type="figure" target="#fig_1">6</ref> compares the DSB coverage of gem5 and SPEC benchmarks running on Intel_Xeon. As shown, the DSB coverage of gem5 is much lower than that of SPEC, regardless of the CPU type or workload. This puts pressure on the decoder and thus, the MITE stall cycles are high for gem5 simulations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Profiling gem5 on M1</head><p>In the previous subsection, we performed a detailed top-down analysis of gem5 running on a Intel_Xeon platform and compared its behavior against conventional SPEC benchmarks. We understand that the front-end of the Intel_Xeon is the bottleneck in running gem5 simulations. There are many iCache and iTLB misses, and the instruction decoder of complex x86 instructions cannot feed enough ?ops to the out-of-order backend. We perform the analysis while running gem5 simulations using three CPU types (Atomic, Timing, and O3), then we execute water_nsquared on gem5.</p><p>Figure <ref type="figure">7</ref>  and M1_Ultra are 2.22? and 2.24? higher than that of Intel_Xeon, respectively. This margin in IPC values across both platforms reflects on the simulation time differences illustrated in Fig. <ref type="figure">1</ref>. Unsurprisingly, the time percentage that Intel_Xeon is stalled is much higher than that of M1_Pro and M1_Ultra. Figure <ref type="figure">8</ref> compares the TLB, L1 cache, and branch prediction performance of Intel_Xeon, M1_Pro, and M1_Ultra when running gem5 simulations. As shown in the figure, the iTLB, dTLB, and L1 cache miss rates of M1 platforms are much lower than that of Intel_Xeon; iTLB and dTLB miss rates of Intel_Xeon are 11.7? and 10.5? higher than that of M1_Ultra, respectively. We believe the main cause of the performance difference between M1 platforms and Intel_Xeon reported in Fig. <ref type="figure">1</ref> are the TLB and L1 caches. Looking at Table <ref type="table" target="#tab_2">I</ref>, the performance cores in both M1_Pro and M1_Ultra have 192KB iCache and 128KB dCache, while Intel_Xeon has 32KB iCache and 32KB dCache. This is 6? and 4? larger iCache and dCache for M1 platforms, respectively.</p><p>Although there is no information on the associativity of the L1 cache of M1 platforms, we can reverse engineer the number of ways assuming that the L1 is implemented as a virtuallyindexed, physically tagged (VIPT) cache. In VIPT caches, the total capacity of a single way cannot exceed the virtual memory page size in order to overlap the TLB access (address translation) with indexing into the cache <ref type="bibr" target="#b39">[40]</ref>. Since M1 uses 16KB virtual memory page sizes, the iCache and dCache associativity should be 12 and 8, respectively. The number of ways for the 32KB icache and dcache of Intel_Xeon is 8 ways. Therefore, the 10.1? ?13.4? reduction in dCache miss rate for M1 platforms shown in Fig. <ref type="figure">8</ref> is mostly due to the reduction in capacity and compulsory misses (4? higher capacity, 2? larger cache line size).</p><p>We also notice that the branch prediction accuracy of M1 platforms is higher than that of Intel_Xeon. As shown in Fig. <ref type="figure">8</ref>, the branch misprediction rate of Intel_Xeon is 0.22% while both M1 platforms have ?0.14% branch misprediction rates. In Sec.V-B we run gem5 on FireSim and study the impact of changing the L1 and L2 cache configurations of the host (simulation server) on gem5's performance. What is clear is that the combination of using larger cache lines (64B vs. 128B), larger virtual memory page size (4KB vs. 16KB), and larger L1 caches<ref type="foot" target="#foot_2">3</ref> (32KB vs. 128KB) dramatically improves L1 and TLB performance in M1 platforms. The performant TLB, L1, and branch predictor results in higher IPC, and in turn, higher simulation speed for M1_Pro and M1_Ultra when compared to Intel_Xeon (Fig. <ref type="figure">7</ref> and Fig. <ref type="figure">1</ref>). Figure <ref type="figure" target="#fig_2">9</ref>(left) shows the LLC occupancy per gem5 process and Fig. <ref type="figure" target="#fig_2">9</ref>(right) shows the DRAM bandwidth utilization of gem5 when running simulations with different CPU models in Full-System (FS) and System-call Emulation (SE) modes on Intel_Xeon. Unfortunately, we were not able to find L2, LLC, or DRAM-related performance counters on M1 platforms. Therefore, we could not include M1-related information in Fig. <ref type="figure" target="#fig_2">9</ref>. As shown in Fig. <ref type="figure" target="#fig_2">9</ref> (right), surprisingly, the DRAM bandwidth utilization of gem5 is negligible regardless of whether it is running in FS or SE mode. Such low DRAM bandwidth utilization suggests that gem5's data set size fits in the last-level cache (LLC). Fig. <ref type="figure" target="#fig_2">9</ref> (left) plots the LLC occupancy of a single gem5 process running on Intel_Xeon. As shown, the LLC occupancy increases with the detail level of simulation, and a gem5 simulation with O3 CPU has the largest instruction and data footprint compared to simulation with Atomic and Timing CPUs. The LLC occupancy of a single gem5 process is between 255KB?3.1MB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. SENSITIVITY ANALYSIS OF SIMULATION SPEED</head><p>In this section, we leverage our insights from Sec. IV to perform a sensitivity analysis for the simulation speed of gem5. We divide the analysis into systems and architecture sensitivity. Under the systems analysis, we study the sensitivity of simulation speed to several systems and compiler parameters that do not require changes to the server hardware or gem5 application. Under architecture analysis, we study the sensitivity of simulation speed to the size and associativity of the L1 and L2 caches of the simulation server. Since such analysis requires changes to the hardware, we run gem5 on FireSim and configure FireSim to simulate a host server with various cache configurations. We run unmodified gem5 on FireSim. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Sensitivity to System Configurations</head><p>As discussed in detail in Sec.IV-A, due to the large instruction footprint of gem5, we observe many stalled cycles due to iTLB misses while running gem5 simulations. A simple solution to the iTLB misses is to use huge pages to back gem5 code/text. We explored two ways to back gem5 code/text regions with huge pages: Transparent Huge Pages (THP) and Explicit Huge Pages (EHP). Linux supports Transparent Huge Pages (THP) <ref type="bibr" target="#b40">[41]</ref> which is a kernel feature that provides dynamic huge page allocations at application runtime. The current Linux THP implementation only works with anonymous memory mappings (i.e., the memory that is not backed by the file system such as implicit memory allocations on the heap and stack) and tmpfs/shmem. We utilize an open-source library by Intel <ref type="bibr" target="#b41">[42]</ref> to back gem5's code segment with transparent/explicit huge pages. By invoking a few API calls at the beginning of gem5 runtime, the library automatically remaps a subset of gem5's code to 2MB huge pages <ref type="bibr" target="#b42">[43]</ref>. We also explored the use of libhugetlbfs library package <ref type="bibr">[44]</ref>, which requires that gem5 be recompiled so the binary is aligned at huge page boundaries. libhugetlbfs automatically backs the code, data, heap, and shared memory segments with explicitly allocated huge pages when invoked with requisite parameters. However, our experiments show an abysmal improvement in simulation time compared with Intel iodlr. We suggest this is a result of a sub-optimal gem5 binary layout.</p><p>As shown in Fig. <ref type="figure" target="#fig_3">10</ref>, using huge pages to back the code of gem5 improves simulation speed by up to 5.9%. The benefits from using huge pages are low for simple CPU models (i.e., Atomic and Timing CPUs), while the benefits for more detailed CPU models are higher. This is expected because the code footprint of simple CPUs is smaller than that of detailed CPU models. This is in line with our discussion in Sec.IV-A and Fig. <ref type="figure">3</ref> which illustrates that the simulation of simpler CPUs is less front-end latency bound compared to detailed CPUs.</p><p>We do not see any specific pattern in the performance of EHP and THP. For some configurations, EHP performs better than THP. Fig. <ref type="figure">11</ref> shows the improvement in iTLB overhead and retiring cycles when backing gem5's code with THP. As shown in Fig. <ref type="figure">11</ref>(top), using THP significantly reduces the iTLB overhead for Minor and O3 simulations. On average, THP reduces the iTLB overhead by 63%. The improvement in iTLB overhead results in 3?7% improvement in the number Next, we study the impact of compiling gem5 using the "-O3" flag passed to the GNU G++ compiler. We modified the scons script to compile gem5 with a higher level of compiler optimization (i.e, "-O3" flag). Though we used gem5.opt, we still notice a reduction in the size of the resulting binary and in the simulation time. Fig. <ref type="figure">12</ref> compares the simulation speed up when using a gem5 binary that is compiled with "-O3" flag compared with baseline gem5 compiled without the optimization flag. On average, this simple change in the build process results in 1.38%, 0.98%, and 0.78% speedup for Intel_Xeon, M1_Pro, and M1_Ultra platforms. "-O3" flag only performs static compile time optimizations and thus there is a possibility for hurting the application speed after applying the optimizations. We see a few instances of such cases in Fig. <ref type="figure">12</ref>.</p><p>Lastly, we study the impact of CPU frequency on gem5 speed. Fig. <ref type="figure" target="#fig_5">13</ref> shows how simulation time changes when running gem5 on Intel_Xeon operating at various frequencies. The simulation times in Fig. <ref type="figure" target="#fig_5">13</ref> are normalized to the run with 3.1GHz frequency. As expected, reducing CPU frequency from 3.1GHz to 1.2GHz increases the simulation time by 2.67?. This shows a linear increase in simulation time with a reduction in CPU frequency. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Sensitivity to Architectural Configurations</head><p>In this subsection, we run gem5 on FireSim and change the L1 and L2 configurations of the O3 core in FireSim to study gem5's sensitivity to the cache configuration of the simulation server. As mentioned earlier, since FireSim is much slower than real hardware, we run a simple C++ application on gem5 and do not run PARSEC.</p><p>Figure <ref type="figure" target="#fig_0">14</ref> compares the simulation time of gem5 running on a server with various L1 and L2 configurations. The cache line size and virtual memory page size in the simulated node in FireSim are 64B and 4KB, respectively. Since the L1 cache is a VIPT cache, to increase the L1 size we only increase the associativity and keep the number of sets fixed at 64 to overlap TLB access and L1 cache indexing. We use the following format to represent different L1 and L2 configurations in Fig. <ref type="figure" target="#fig_0">14:</ref> (iCache size/iCache associativity : dCache size/dCache associativity : L2 size/L2 associativity).</p><p>As illustrated in Fig. <ref type="figure" target="#fig_0">14</ref>, increasing the size of both iCache and dCache are critical in improving the simulation speed. The simulation times are normalized to a baseline configuration with 8KB 2-way set associative iCache/dCache and a 512KB 8-way set associative L2 cache. Increasing iCache and dCache size from 8KB to 16KB reduces Atomic, Timing, and O3 simulation time by 30%, 25%, and 18%. On the other hand, doubling L2 cache size from 1MB to 2MB has almost no impact on the simulation time. The best-performing configuration is the last configuration where we keep L2 the same size as the baseline and configure both iCache and dCache as 64KB 16-way set-associative caches (64KB/16 : 64KB/16 : 512KB/8). This configuration improves simulation speed by 68.7%, 68.2%, and 43.8% for Atomic, Timing, and O3 simulations, respectively. We notice that gem5 simulations with O3 CPU benefit less than simpler CPU models from increasing L1 cache size. We suspect that the TLB bottleneck in detailed simulations limits the benefits of the larger L1 size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. DISCUSSION OF FUTURE WORK</head><p>Our detailed microarchitectural analysis revealed the bottlenecks in gem5 execution. The fact that changing the physical  platform can result in significant simulation speedup motivates us to think about developing specialized computing platforms for running architectural simulations. One potential area to explore is to offload simulation entirely or partially to a hardware accelerator.</p><p>For an application to be qualified for hardware acceleration, the application needs to be (1) widely used and (2) stable with no structural changes over time. gem5 satisfies the first requirement as it is a popular application with a growing user base in both academia and industry. Although gem5 is constantly changing, its core, which is the event queue and event scheduler has been the same for many years, and will probably remain the same in the future. Therefore, gem5 satisfies the second requirement as well.</p><p>Figure <ref type="figure">15</ref> shows the cumulative distribution function (CDF) of the CPU time of the 50 hottest functions executed in gem5 simulating different CPU types. As shown, there is no killer function inside the gem5 source code whose optimization would significantly improve the simulation time. The hottest function in Atomic, Timing, Minor, and O3 CPU types contribute to 10.1%, 8.5%, 2.9%, and 4.2% of the total simulation time, respectively. As we increase the complexity of the CPU, the CDF of individual function execution time gets flatter; meaning that the hotness of individual functions gets lower. This is not surprising since increased simulation complexity causes more simulation objects to get activated in each event to more accurately model the complexity of the hardware. Therefore, more diverse functions get called when simulating with O3 CPU type compared with simpler CPU models. The total number of functions called throughout the simulation for the results shown in Fig. <ref type="figure">15</ref> are 1602, 2557, 3957, and 5209 for Atomic, Timing, Minor, and O3 CPU types, respectively.</p><p>Since there is no killer function, accelerating even several gem5 functions in hardware would not provide a significant performance improvement. Therefore, the results of Fig. <ref type="figure">15</ref> suggest that building an off-chip hardware accelerator for gem5 is probably not an option. Instead, hardware acceleration should be at a finer granularity and tightly coupled with the CPU. The comparison of Intel_Xeon and M1 platforms revealed that even a general-purpose CPU with some finetuning of architectural and system parameters can significantly improve gem5 performance. Detailed basic-block analysis of gem5 source code is required to identify commonly used operations and data structures to map them to specialized, complex instructions. The open-source RISC-V ISA facilitates the development of such a specialized CPU. Designing such a specialized CPU for accelerating event-driven simulation is an interesting future research direction. However, a short-term solution is to utilize the configurability of current servers at the system-and compiler-level optimizations to improve the simulation execution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. RELATED WORK</head><p>Acceleration of Simulation FireSim <ref type="bibr" target="#b16">[17]</ref> is an FPGA Architecture Model Execution (FAME) simulator <ref type="bibr" target="#b43">[45]</ref>. FireSim uses FPGAs to implement the complete RTL of a target system to model the timing of future hardware. The timing model can be decoupled from the real design such that multiple host FPGA clock cycles are used to model one target clock cycle when modeling complex logic. Although FAME simulators significantly speed up the simulation speed, developing new models on them is time-consuming, errorprone, and inflexible. Therefore FAME cannot replace softwarebase simulation. Parallel Discrete Event Simulation (PDES) technique is used to simulate different components in parallel and conservatively or optimistically synchronize them in fixed intervals called quantum <ref type="bibr" target="#b44">[46]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b45">[47]</ref>. Using conservative PDES for parallelizing the simulation of on-chip resources has diminishing returns as the overhead of frequent synchronization offsets the benefits of parallel execution. Therefore, PDES simulators such as SST <ref type="bibr" target="#b45">[47]</ref> only operate at larger component levels since the speed of modeling individual components will become a bottleneck in the overall simulation.</p><p>Sampling techniques and using hardware virtualization support for fast-forwarding simulations are widely used for improving the speed and accuracy of architectural simulation <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. Such techniques are orthogonal to improving the speed of detailed simulation. Top-Down Microarchitectural Analysis. There is a large body of work on profiling applications and performing Top-Down microarchitectural analyses on various applications such as data analytics and cloud applications <ref type="bibr" target="#b46">[48]</ref>, <ref type="bibr" target="#b47">[49]</ref>, hyperscale services <ref type="bibr" target="#b38">[39]</ref>, SPEC benchmark <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b48">[50]</ref>, web search <ref type="bibr" target="#b49">[51]</ref>, network stack <ref type="bibr" target="#b50">[52]</ref>, data-intensive applications <ref type="bibr" target="#b51">[53]</ref>, <ref type="bibr" target="#b52">[54]</ref>, <ref type="bibr" target="#b53">[55]</ref>, video transcoding <ref type="bibr" target="#b54">[56]</ref>, graph applications <ref type="bibr" target="#b55">[57]</ref>, network fuctions <ref type="bibr" target="#b56">[58]</ref>, and many more application domains.</p><p>However, no previous work has performed a detailed Top-Down microarchitectural analysis to profile the execution of a software-based architectural simulator. VIII. CONCLUSION There has been no work characterizing the execution bottlenecks in gem5 even though it is considered one of the most versatile and slowest architectural simulators and has a huge active user community. In this work, we profiled the performance characteristics of gem5 and demystified the inefficiencies of gem5 simulations. Our detailed Top-Down microarchitectural analysis reveals three main bottlenecks in gem5 execution: (1) high iCache and iTLB misses, (2) high branch resteer overheads, and (3) extremely low ?Op cache utilization when running on an Intel Xeon CPU. These bottlenecks are the result of huge code size, cold code execution, extensive use of virtual functions, and polymorphism throughout the gem5 source code. We observe that running gem5 on an Apple M1 MacBook reduces simulation time by up to 3?times compared to a high-end Xeon server. Our profiling results reveal that the larger L1 cache size along with the use of a larger virtual memory page size leads to such performance improvement for gem5. This work is the first step towards better understanding the characteristics of detailed, softwarebased architectural simulation and developing optimized server solutions for accelerating the simulation of future computer systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Front-end latency bound cycles breakdown for gem5 (top) and SPEC (bottom) running on Intel_Xeon.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: DSB (?Op Cache) coverage of gem5 (top) and SPEC (bottom) running on Intel_Xeon.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: LLC occupancy and memory bandwidth utilization of gem5 running with different configurations and operating modes on Intel_Xeon.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 10 :</head><label>10</label><figDesc>Fig. 10: Performance gain from enabling huge pages for gem5 simulations running on Intel_Xeon.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 11 :Fig. 12 :</head><label>1112</label><figDesc>Fig. 11: Improvement in iTLB overhead and retiring cycles when backing gem5 code with transparent huge pages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 13 :</head><label>13</label><figDesc>Fig. 13: Normalized gem5 simulation time when changing CPU frequency and enabling Turbo Boost on Intel_Xeon platform. The simulation times are normalized to the baseline CPU running at 3100MHz without TurboBoost.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 14 :Fig. 15 :</head><label>1415</label><figDesc>Fig. 14: Simulation speedup when running gem5 on FireSim with varying cache configurations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I :</head><label>I</label><figDesc>Base Hardware Configuration on FireSim</figDesc><table><row><cell>Parameters</cell><cell>Value</cell></row><row><cell>Core Frequency</cell><cell>4GHz</cell></row><row><cell>Number of Cores</cell><cell>4 Cores</cell></row><row><cell>Superscalar</cell><cell>8-width wide</cell></row><row><cell>ROB/IQ/LQ/SQ Entries</cell><cell>192/64/32/32</cell></row><row><cell>Int &amp; FP Registers</cell><cell>128 &amp; 192</cell></row><row><cell>Branch Predictor/BTB Entries</cell><cell>TournamentBP/4096</cell></row><row><cell>Cache: L1I/L1D</cell><cell>48KB(I), 32KB(D)</cell></row><row><cell>DRAM</cell><cell>2GB, DDR3-1600-8x8</cell></row><row><cell>Operating System</cell><cell>Linux Linaro (kernel 5.4.0)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Because FireSim is orders of magnitude slower than real hardware (a gem5 simulation that completely executes in 2.34 seconds on a quadcore Intel_Xeon host results in a slowdown of ?118? on Firesim), we run a simple algorithm called Sieve_of_Eratosthenes on the simulated node on FireSim to evaluate the performance of gem5.</figDesc><table /><note><p><p><p><p>? PARSEC: We execute Canneal, Blackscholes, Dedup, and streamcluster within the mainline PARSEC 3.0 benchmark and water_nsquared 2 , water_spatial, ocean_ncp, ocean_cp, and fmm apps within SPLASH-2x</p><ref type="bibr" target="#b27">[28]</ref> </p>benchmark suite. The benchmark input size used for all workloads is simmedium.</p>? C++ program:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II :</head><label>II</label><figDesc>Evaluation platforms</figDesc><table><row><cell>Platform</cell><cell>Dell Precision 7920</cell><cell>Apple Macbook</cell><cell>Apple MacStudio</cell></row><row><cell>Config Name</cell><cell>Intel_Xeon</cell><cell>M1_Pro</cell><cell>M1_Ultra</cell></row><row><cell>SoC</cell><cell>Xeon Gold 6242R</cell><cell>M1</cell><cell>M1 Ultra</cell></row><row><cell>micro-architecture</cell><cell>Cascade Lake</cell><cell cols="2">Firestorm(P) + Icestorm(E)</cell></row><row><cell>Cores</cell><cell>20C/40T</cell><cell>P:4C/4T + E:4C/4T</cell><cell>P:16C/16T + E:4C/4T</cell></row><row><cell>Max Freq</cell><cell>3.1GHz (4.1GHz TB)</cell><cell>3.2GHz(P), 2GHz(E)</cell><cell>3.2GHz(P), 2GHz(E)</cell></row><row><cell>L1 (per-core)</cell><cell>32KB(I) + 32KB(D)</cell><cell cols="2">P:192KB(I) + 128KB(D) E:128KB(I) + 64KB(D)</cell></row><row><cell>L2</cell><cell>20MB</cell><cell>P:12MB + E:4MB</cell><cell>P:48MB + E:8MB</cell></row><row><cell>L3</cell><cell>35.75MB</cell><cell>8MB</cell><cell>96MB</cell></row><row><cell>Cacheline</cell><cell>64B</cell><cell>128B</cell><cell>128B</cell></row><row><cell>Memory</cell><cell>96GB, DDR4-2933</cell><cell cols="2">8GB, LPDDR4X-4266 64GB, LPDDR5-6400</cell></row><row><cell>DRAM BW</cell><cell>141 GB/s</cell><cell>68 GB/s</cell><cell>819.2 GB/s</cell></row><row><cell>Single core BW (unloaded)</cell><cell>45 GB/s</cell><cell>58 GB/s</cell><cell>58 GB/s</cell></row><row><cell>DRAM Latency</cell><cell>96ns</cell><cell>97ns</cell><cell>97ns</cell></row><row><cell>VM page size</cell><cell>4KB</cell><cell>16KB</cell><cell>16KB</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>. To have a reference for comparison, we also run three SPEC 2017 benchmarks with diverse characteristics on the Intel_Xeon platform and show their Top-Down analysis at the bottom of Fig.2. Refer to Sec.III for more information on the choice of SPEC benchmarks. As shown, 43.5?64.7% of cycles retire instructions across different gem5 simulations. This is a relatively high retiring percentage compared to conventional workloads. As shown in the figure, the retiring cycle percentage for SPEC 2017 benchmarks are between 13.2?82.2%. However, gem5's front-end bound cycles are much higher while the back-end bound cycles are lower compared with SPEC. 505.mcf_r, which is a memoryintensive workload. It has 53.7% of back-end bound cycles while gem5 workloads only spent 0.9%?11.3% of their cycles stalling for back-end. This is expected since gem5's dynamic working set increases very slowly while simulating different workloads as gem5 is orders of magnitude slower than real hardware.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Front-End Latency</cell><cell></cell><cell cols="3">Front-End Bandwidth</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">O3_BOOT_EXIT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>O3_PARSEC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">MINOR_BOOT_EXIT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">MINOR_PARSEC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GEM5</cell><cell cols="2">TIMING_BOOT_EXIT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">TIMING_PARSEC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">ATOMIC_BOOT_EXIT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">ATOMIC_PARSEC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>525.X264_R</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SPEC</cell><cell cols="2">531.DEEPSJENG_R</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>505.MCF_R</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>-15%</cell><cell>-10%</cell><cell>-5%</cell><cell>0%</cell><cell>5%</cell><cell>10%</cell><cell>15%</cell><cell>20%</cell><cell>25%</cell><cell>30%</cell><cell>35%</cell><cell>40%</cell><cell>45%</cell></row><row><cell cols="13">Fig. 3: Front-end latency bound cycles breakdown for gem5</cell></row><row><cell cols="11">(top) and SPEC (bottom) running on Intel_Xeon.</cell><cell></cell><cell></cell></row></table><note><p><p>(top)  </p>shows the top-level profiling results categorizing the CPU cycles spent executing gem5 with different CPU types</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Unless mentioned otherwise, throughout the paper, we refer to "softwarebased architectural simulators" as "simulators"</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>In this paper, Top-Down microarchitectural analysis was carried out using water nsquared as a representative workload from PARSEC benchmark suite</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>The larger virtual memory page size enables the implementation of lowassociativity, large VIPT L1 caches in M1 platforms as explained earlier.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENT</head><p>This work was supported in part by grants from <rs type="funder">National Science Foundation</rs> (<rs type="grantNumber">CNS-2213807</rs>) and <rs type="institution">ACE</rs>, one of the seven centers in JUMP 2.0, a <rs type="grantName">Semiconductor Research Corporation</rs> (SRC) program sponsored by <rs type="funder">DARPA</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_8QTSnz5">
					<idno type="grant-number">CNS-2213807</idno>
					<orgName type="grant-name">Semiconductor Research Corporation</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">turing award lecture</title>
		<ptr target="https://www.youtube.com/watch?v=3LVeEjsn8Ts" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A survey of computer architecture simulation techniques and tools</title>
		<author>
			<persName><forename type="first">A</forename><surname>Akram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sawalha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The gem5 simulator</title>
		<author>
			<persName><forename type="first">N</forename><surname>Binkert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Reinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Hower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sardashti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH computer architecture news</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Advanced Computer Architecture and Compilation for High-Performance and Embedded Systems (ACACES-2012). High-Performance and Embedded Architecture and Compilation Network of</title>
		<author>
			<persName><forename type="first">W</forename><surname>Heirman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>Sniper: Scalable and accurate parallel multi-core simulation</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Marss-x86: A qemu-based microarchitectural and systems simulator for x86 multicore processors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Afram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ghose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1st International Qemu Users&apos; Forum</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Zsim: Fast and accurate microarchitectural simulation of thousand-core systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer architecture news</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Instruction sets should be free: The case for risc-v</title>
		<author>
			<persName><forename type="first">K</forename><surname>Asanovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<idno>UCB/EECS-2014-146</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
		<respStmt>
			<orgName>EECS Department, University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Sst simulation</title>
		<author>
			<persName><surname>Sst</surname></persName>
		</author>
		<ptr target="://sst-simulator.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">dist-gem5: Distributed simulation of computer clusters</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Darbaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dozsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Diestelhorst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Performance Analysis of Systems and Software (ISPASS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Full speed ahead: Detailed architectural simulation at near-native speed</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sandberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nikoleris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hagersten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaxiras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Black-Schaffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Symposium on Workload Characterization</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adaptive cache warming for faster simulations</title>
		<author>
			<persName><forename type="first">G</forename><surname>Borgstr?m</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sembrant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Black-Schaffer</surname></persName>
		</author>
		<idno type="DOI">10.1145/3023973.3023974</idno>
		<ptr target="https://doi.org/10.1145/3023973.3023974" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Workshop on Rapid Simulation and Performance Evaluation: Methods and Tools, ser. RAPIDO &apos;17</title>
		<meeting>the 9th Workshop on Rapid Simulation and Performance Evaluation: Methods and Tools, ser. RAPIDO &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Smarts: Accelerating microarchitecture simulation via rigorous statistical sampling</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Wunderlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Hoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th annual international symposium on Computer architecture</title>
		<meeting>the 30th annual international symposium on Computer architecture</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Using simpoint for accurate and efficient simulation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Perelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hamerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Biesbrouck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sherwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGMETRICS Performance Evaluation Review</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Directed statistical warming through time traveling</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nikoleris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hagersten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
		<idno type="DOI">10.1145/3352460.3358264</idno>
		<ptr target="https://doi.org/10.1145/3352460.3358264" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture, ser. MICRO &apos;52</title>
		<meeting>the 52nd Annual IEEE/ACM International Symposium on Microarchitecture, ser. MICRO &apos;52<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Slacksim: A platform for parallel simulations of cmps on cmps</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Annavaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dubois</surname></persName>
		</author>
		<idno type="DOI">10.1145/1577129.1577134</idno>
		<ptr target="https://doi.org/10.1145/1577129.1577134" />
	</analytic>
	<monogr>
		<title level="s">SIGARCH Comput. Archit. News</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<date type="published" when="2009-07">jul 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">pd-gem5: Simulation infrastructure for parallel/distributed computer systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Architecture Letters</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Firesim: Fpgaaccelerated cycle-exact scale-out system simulation in the public cloud</title>
		<author>
			<persName><forename type="first">S</forename><surname>Karandikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Biancolin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Amid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pemberton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Amaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kovacs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nikolic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asanovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hasim: Fpgabased high-detail multicore simulation using time-division multiplexing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pellauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kinsy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Parashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE 17th International Symposium on High Performance Computer Architecture</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ramp: Research accelerator for multiple processors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wawrzynek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Hoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asanovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Lowe-Power</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Akram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Amslinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andreozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Armejach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Asmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bharadwaj</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03152</idno>
		<title level="m">The gem5 simulator: Version 20.0+</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The future of architectural simulation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Hoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sendag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hase: a flexible high performance architecture simulator</title>
		<author>
			<persName><forename type="first">A</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ibbett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1994 Proceedings of the Twenty-Seventh Hawaii International Conference on System Sciences</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">simcore: an event-driven simulation framework for performance evaluation of computer systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chiba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 8th International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems</title>
		<meeting>8th International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>Cat. No.PR00728</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Barra: A parallel functional simulator for gpgpu</title>
		<author>
			<persName><forename type="first">C</forename><surname>Collange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daumas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Defour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Simics: A full system simulation platform</title>
		<author>
			<persName><forename type="first">P</forename><surname>Magnusson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Christensson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eskilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hallberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hogberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moestedt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Werner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Simplescalar: an infrastructure for computer system modeling</title>
		<author>
			<persName><forename type="first">T</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ernst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The arm research starter kit: System modelling using gem5</title>
		<ptr target="https://github.com/arm-university/arm-gem5-rsk" />
		<imprint>
			<date type="published" when="2023-02">Feb. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Parsec3.0: A multicore benchmark suite with network stacks and splash-2x</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bienia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1145/3053277.3053279</idno>
		<ptr target="https://doi.org/10.1145/3053277.3053279" />
	</analytic>
	<monogr>
		<title level="s">SIGARCH Comput. Archit. News</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<date type="published" when="2017-02">feb 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Intel? vtune? profiler</title>
		<ptr target="https://www.intel.com/content/www/us/en/developer/tools/oneapi/vtune-profiler.html#gs.jescps" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A top-down method for performance analysis and counters architecture</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yasin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">T</forename><surname>Gleixner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Molnar</surname></persName>
		</author>
		<ptr target="https://github.com/torvalds/linux" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Reading m1 performance counters</title>
		<ptr target="https://gist.github.com/ibireme" />
		<imprint>
			<date type="published" when="2022-11">1f0d67d12. Nov. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Apple mac m1 microarchitectural features</title>
		<ptr target="https://https://everymac.com/" />
		<imprint>
			<date type="published" when="2022-12">Dec. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Spec 2017 documentation</title>
		<author>
			<persName><surname>Spec</surname></persName>
		</author>
		<ptr target="https://www.spec.org/cpu2017/Docs" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Wait of a decade: Did spec cpu 2017 broaden the performance horizon?</title>
		<author>
			<persName><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A workload characterization of the spec cpu2017 benchmark suite</title>
		<author>
			<persName><forename type="first">A</forename><surname>Limaye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Adegbija</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Firemarshal: Making hw/sw co-design reproducible and reliable</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pemberton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Amid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Chipyard: Integrated design, simulation, and implementation framework for custom socs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Amid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Biancolin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grubb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karandikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Magyar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pemberton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rigge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asanovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nikoli?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Profiling a warehouse-scale computer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kanev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Darago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hazelwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
		<idno type="DOI">10.1145/2749469.2750392</idno>
		<ptr target="https://doi.org/10.1145/2749469.2750392" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual International Symposium on Computer Architecture, ser. ISCA &apos;15</title>
		<meeting>the 42nd Annual International Symposium on Computer Architecture, ser. ISCA &apos;15<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Organization and performance of a two-level virtual-real cache hierarchy</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Baer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Levy</surname></persName>
		</author>
		<idno type="DOI">10.1145/74926.74942</idno>
		<ptr target="https://doi.org/10.1145/74926.74942" />
	</analytic>
	<monogr>
		<title level="s">SIGARCH Comput. Archit. News</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<date type="published" when="1989-04">apr 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Transparent hugepage support</title>
		<author>
			<persName><forename type="first">A</forename><surname>Arcangeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KVM forum</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Intel optimizations for dynamic language runtimes</title>
		<ptr target="https://github.com/intel/iodlr" />
		<imprint>
			<date type="published" when="2022-10">Oct. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Runtime performance optimization blueprint: Intel architecture optimization with large code pages</title>
		<ptr target="https://www.intel.com/content/www/us/en/developer/articles/technical/runtime-performance-optimization-blueprint-intel-architecture-optimization-with-large-code.html" />
		<imprint>
			<date type="published" when="2022-12">Dec. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A case for fame: Fpga architecture model execution</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waterman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asanovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<idno type="DOI">10.1145/1815961.1815999</idno>
		<ptr target="https://doi.org/10.1145/1815961.1815999" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th Annual International Symposium on Computer Architecture, ser. ISCA &apos;10</title>
		<meeting>the 37th Annual International Symposium on Computer Architecture, ser. ISCA &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Adaptive and speculative slack simulations of cmps on cmps</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Dabbiru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Annavaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dubois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 43rd Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Structural simulation toolkit (sst)</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Voskuilen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Hemmert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sandia National Lab.(SNL-NM)</title>
		<meeting><address><addrLine>Albuquerque, NM (United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Performance characterization of in-memory data analytics on a modern cloud server</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Awan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brorsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vlassov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ayguade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Fifth International Conference on Big Data and Cloud Computing</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep-dive analysis of the data analytics workload in cloudsuite</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ben-Asher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mendelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Symposium on Workload Characterization (IISWC)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Fine-grain power breakdown of modern out-of-order cores and its implications on skylake-based systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Haj-Yihia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">B</forename><surname>Asher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mendelson</surname></persName>
		</author>
		<idno type="DOI">10.1145/3018112</idno>
		<ptr target="https://doi.org/10.1145/3018112" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Archit. Code Optim</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<date type="published" when="2016-12">dec 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Memory hierarchy for web search</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ayers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Tas: Tcp acceleration as an os service</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stamler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Anderson</surname></persName>
		</author>
		<idno type="DOI">10.1145/3302424.3303985</idno>
		<ptr target="https://doi.org/10.1145/3302424.3303985" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth EuroSys Conference 2019, ser. EuroSys &apos;19</title>
		<meeting>the Fourteenth EuroSys Conference 2019, ser. EuroSys &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Data motifs: A lens towards fully understanding big data and ai workloads</title>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.1145/3243176.3243190</idno>
		<ptr target="https://doi.org/10.1145/3243176.3243190" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Parallel Architectures and Compilation Techniques, ser. PACT &apos;18</title>
		<meeting>the 27th International Conference on Parallel Architectures and Compilation Techniques, ser. PACT &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Understanding big data analytics workloads on modern processors</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">How data volume affects spark based data analytics on a scale-up server</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Awan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brorsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vlassov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ayguade</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>in BPOE</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Vbench: Benchmarking Video Transcoding in the Cloud</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lottarini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Coburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stodolsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wachsler</surname></persName>
		</author>
		<idno type="DOI">10.1145/3173162.3173207</idno>
		<ptr target="https://doi.org/10.1145/3173162.3173207" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="797" to="809" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Heterogeneous memory subsystem for natural graph analytics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Addisie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kassa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bertacco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Symposium on Workload Characterization (IISWC)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Toward an ideal ndn router on a commercial off-the-shelf computer</title>
		<author>
			<persName><forename type="first">J</forename><surname>Takemasa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Koizumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hasegawa</surname></persName>
		</author>
		<idno type="DOI">10.1145/3125719.3125731</idno>
		<ptr target="https://doi.org/10.1145/3125719.3125731" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th ACM Conference on Information-Centric Networking, ser. ICN &apos;17</title>
		<meeting>the 4th ACM Conference on Information-Centric Networking, ser. ICN &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
