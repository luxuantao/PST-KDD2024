<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attending to Graph Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-02-08">8 Feb 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Luis</forename><forename type="middle">M</forename><surname>?ller</surname></persName>
							<email>luis.mueller@cs.rwth-aachen.de</email>
							<affiliation key="aff0">
								<orgName type="institution">RWTH Aachen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mikhail</forename><surname>Galkin</surname></persName>
							<email>mikhail.galkin@intel.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Intel AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
							<email>morris@cs.rwth-aachen.de</email>
							<affiliation key="aff0">
								<orgName type="institution">RWTH Aachen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ladislav</forename><surname>Ramp??ek</surname></persName>
							<email>ladislav.rampasek@mila.quebec</email>
							<affiliation key="aff2">
								<orgName type="institution">Universit? de Montr?al</orgName>
								<address>
									<settlement>Mila</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Attending to Graph Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-02-08">8 Feb 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2302.04181v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, transformer architectures for graphs emerged as an alternative to established techniques for machine learning with graphs, such as graph neural networks. So far, they have shown promising empirical results, e.g., on molecular prediction datasets, often attributed to their ability to circumvent graph neural networks' shortcomings, such as over-smoothing and over-squashing. Here, we derive a taxonomy of graph transformer architectures, bringing some order to this emerging field. We overview their theoretical properties, survey structural and positional encodings, and discuss extensions for important graph classes, e.g., 3D molecular graphs. Empirically, we probe how well graph transformers can recover various graph properties, how well they can deal with heterophilic graphs, and to what extent they prevent over-squashing. Further, we outline open challenges and research direction to stimulate future work. Our code is available at https://github. com/luis-mueller/probing-graph-transformers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph-structured data are prevalent across application domains ranging from chemo-and bioinformatics <ref type="bibr" target="#b2">[Barabasi and Oltvai, 2004;</ref><ref type="bibr" target="#b56">Reiser et al., 2022]</ref> to image <ref type="bibr" target="#b62">[Simonovsky and Komodakis, 2017]</ref> and social-network analysis <ref type="bibr" target="#b18">[Easley and Kleinberg, 2010]</ref>, clearly indicating the importance of machine learning methods for such data. In recent years, graph neural networks (GNNs) <ref type="bibr" target="#b6">[Chami et al., 2022;</ref><ref type="bibr" target="#b23">Gilmer et al., 2017;</ref><ref type="bibr" target="#b51">Morris et al., 2021]</ref> were the dominant paradigm in machine learning for graphs. However, with the rise of transformer architectures <ref type="bibr" target="#b66">[Vaswani et al., 2017]</ref> in natural language processing <ref type="bibr">[Lin et al., 2021b]</ref> and computer vision <ref type="bibr" target="#b24">[Han et al., 2022]</ref>, recently, a large number of works in the field focused on designing transformer architectures capable of dealing with graphs, so-called graph transformers (GTs).</p><p>Graph transformers have already shown promising performance <ref type="bibr" target="#b73">[Ying et al., 2021]</ref>, e.g., by topping the leaderboard of the OGB Large-Scale Challenge <ref type="bibr" target="#b26">[Hu et al., 2021;</ref><ref type="bibr" target="#b46">Masters et al., 2022]</ref> in the molecular property prediction track. The superiority of GTs over standard GNN architecture is often explained by GNNs' bias towards encoding local structure and being unable to capture global or long-range information, often attributed to phenomena such as over-smoothing <ref type="bibr" target="#b36">[Li et al., 2018]</ref>, under-reaching or over-squashing <ref type="bibr" target="#b0">[Alon and Yahav, 2021]</ref>. Many papers speculate that GTs <ref type="bibr">[Ramp??ek et al., 2022]</ref> do not suffer from such effects as they aggregate information over all nodes in a given graph and hence are not limited to local structure bias. However, to make GTs aware of graph structure, one has to equip them with so-called structural and postional encodings. Here, structural encodings are, e.g., additional node features to make the GT aware of (sub-)graph structure. In contrast, positional encodings make a node aware of its position in the graph concerning the other nodes.</p><p>Present Work. Here, we derive a taxonomy of state-of-theart GT architectures, giving a structured overview of recent developments. Moreover, we survey common positional and structural encodings and clarify how they are related to GTs' theoretical properties, e.g., their expressive power to capture graph structure. Additionally, we investigate these properties empirically by probing how well GTs can recover various graph properties, deal with heterophilic graphs, and to what extent GTs alleviate the over-squashing phenomenon. Further, we outline open challenges and research direction to stimulate future work. Our categorization, theoretical clarification, and experimental study present a useful handbook for the GT and the broader graph machine-learning community. Its insights and principles will help spur novel research results and avenues.</p><p>Related Work. Since GTs emerged recently, only a few surveys exist. Notably, <ref type="bibr">Min et al. [2022a]</ref> provide a high-level overview of some of the recent GT architectures. Different from the present work, they do not discuss GT's theoretical and practical shortcomings and miss out on recent architectural advancements. <ref type="bibr">Chen et al. [2022a]</ref> gives an overview of GTs for computer vision. Finally, <ref type="bibr" target="#b55">Ramp??ek et al. [2022]</ref> provide a general recipe for classifying GT architectures, focusing on devising empirically well-performing architectures rather than giving a detailed, principled overview of the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Background</head><p>A graph G is a pair (V, E) with a finite set of nodes V (G) and a set of edges E(G) ? {{u, v} ? V | u = v}. For ease of notation, we denote an edge {u, v} as (u, v) or (v, u). In the case of directed graphs,   A node-attributed graph G is a triple (V, E, X), where X ? R n?d , for d &gt; 0, is a node feature matrix and X v is the node feature of node v ? V (G). Similarly, we can represent edge features by an edge feature matrix E ? R m?e , for e &gt; 0, where E vw is the edge feature of edge</p><formula xml:id="formula_0">E(G) ? {(u, v) ? V 2 | u = v}</formula><formula xml:id="formula_1">(v, w) ? E(G). The neighborhood of v ? V (G) is N (v) := {u ? V (G) | (v, u) ? E(G)}.</formula><p>We say that two graphs G and H are isomorphic if there exists an edge-preserving bijection ? :</p><formula xml:id="formula_2">V (G) ? V (H), i.e., (u, v) is in E(G) if and only if (?(u), ?(v)) is in E(H) for all u, v ? V (G).</formula><p>We denote a multiset by { {. . .} }.</p><p>Equivariance and Invariance. Operations on graphs need to respect their symmetries, such as being agnostic to the node permutations or other (group) transformations, such as rotation, leading to the definitions of equivariance and invariance. In general <ref type="bibr" target="#b21">[Fuchs et al., 2021]</ref>, given a transformation T, a function f is equivariant if transforming the vector input x is equal to transforming the output of the function f , i.e., f (Tx) = Tf (x). A function g is invariant if transforming the vector input x does not change the output, i.e., g(Tx) = g(x).</p><p>In the 3D Euclidean space, 3D translations, rotations, and reflections form the E(3) group. Translation and rotation form the SE(3) group. Rotations form the SO(3) group, rotations and reflections form the O(3) group. Graph Transformers. A transformer is a stack of alternating blocks of multi-head attention and fully-connected feedforward networks. Let G be a graph with node feature matrix X ? R n?d . 1 In each layer, t &gt; 0, given node feature matrix</p><formula xml:id="formula_3">X (t) ? R n?d , a single attention head computes Attn(X (t) ) := softmax QK T ? d k V,<label>(1)</label></formula><p>where the softmax is applied row-wise, d k denotes the feature dimension of the matrices Q and K, with X (0) := X. Here, the matrices Q, K, and V are the result of projecting X (t) linearly,</p><formula xml:id="formula_4">Q := X (t) W Q , K := X (t) W K , and V := X (t) W V , using three matrices W Q , W K ? R d?d K , and W V ? R d?d ,</formula><p>with optional bias terms omitted for clarity. Now, multi-head attention MultiHead(X (t) ) concatenates multiple (single) attention heads, followed by an output projection to the feature space of X (t) . By combining the above with additional residual 1 For simplicity, we learn attention between a graph's nodes. However, in Section 2.4, we extend this to, e.g., edges or subgraphs. connections and normalization, the transformer layer updates features X (t) via X (t+1) := FFN MultiHead X (t) + X (t) .</p><p>(2)</p><p>As noticed by <ref type="bibr" target="#b47">Mialon et al. [2021]</ref>, we can rewrite Eq. (1) as</p><formula xml:id="formula_5">Attn(X (t) ) v = u?V (G) k exp (X (t) v , X (t) u ) w?V (G) k exp (X (t) v , X<label>(t)</label></formula><p>w )</p><formula xml:id="formula_6">X (t) w W V , for v ? V (G),</formula><p>where</p><formula xml:id="formula_7">k exp (X (t) v , X (t) w ) := exp X (t) v W Q X (t) w W K / ? d K .</formula><p>Hence, we can view GTs as a special GNN, which we define below, operating on a complete graph, where the attention score weights the importance of each node during the sum aggregation.</p><p>Graph Neural Networks. Intuitively, GNNs learn a vector representing each node in a graph by aggregating information from neighboring nodes. Formally, let G be a graph with node feature matrix X ? R n?d . A GNN architecture consists of a stack of neural network layers, i.e., a composition of permutation-equivariant parameterized functions. Each layer aggregates local neighborhood information, i.e., the neighbors' features, around each node and then passes this aggregated information on to the next layer. Following <ref type="bibr" target="#b23">Gilmer et al. [2017]</ref> and <ref type="bibr" target="#b59">Scarselli et al. [2009]</ref>, in each layer, t ? 0, we compute vertex features</p><formula xml:id="formula_8">h (t+1) v := UPD (t) h (t) v , AGG (t) { {h (t) u | u ? N (v)} } ? R d ,</formula><p>where UPD (t) and AGG (t) may be differentiable parameterized functions, e.g., neural networks. For example, GNNs often compute a vector for node v by using sum aggregation <ref type="bibr" target="#b50">[Morris et al., 2019]</ref>, i.e.,</p><formula xml:id="formula_9">h (t+1) v := ? h (t) v W (t) 1 + w?N (v) h (t) w W (t) 2</formula><p>, where ? is a non-linearity applied pointwise, W</p><p>1 and W (t) 2 ? R d?d are parameter matrices, and h</p><formula xml:id="formula_11">(0) v := X v .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Landscape of Graph Transformers</head><p>In the following, we outline our taxonomy of GTs, see also <ref type="bibr">Figure</ref>  <ref type="figure" target="#fig_1">1</ref>, bringing some order to the growing set of GT architectures. We start by discussing the theoretical properties of GTs that heavily rely on structural and positional encodings, which we study subsequently. Further, we discuss different approaches to dealing with essential classes of input node features, e.g., 3D coordinates in the case of molecules. We then study how to tokenize a graph, i.e., partition a graph into atomic entities between which the attention is computed, e.g., nodes. Then, we review how GTs organize message propagation in the graph through global, sparse, or hybrid attention. Finally, we overview representative applications of GTs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Theoretical Properties</head><p>It is crucial to understand that the general GT architecture of Eq. ( <ref type="formula">2</ref>) is less expressive in distinguishing non-isomorphic graphs than standard GNNs. Hence, it is also weaker in approximating permutation-invariant and -equivariant functions over graphs <ref type="bibr" target="#b7">[Chen et al., 2019]</ref>. GTs are weaker since, without sufficiently expressive structural and positional encodings, they cannot capture any graph structure besides the number of nodes and hence equal DeepSets-like architectures <ref type="bibr" target="#b74">[Zaheer et al., 2020]</ref> in expressive power. Thus, for GTs to capture non-trivial graph structure information, they are crucially dependent on such encodings; see below. In fact, by leveraging the results in <ref type="bibr" target="#b7">Chen et al. [2019]</ref>, it is easy to show that GTs can only become maximal expressive, i.e., universal function approximators, if they have access to maximally expressive structural bias, e.g., structural encodings. However, this is equivalent to solving the graph isomorphism problem <ref type="bibr" target="#b7">[Chen et al., 2019]</ref>. Moreover, we stress that GNN architectures equipped with the same encodings will also possess the same expressive power. Hence, in terms of expressive power, GTs do not have an advantage over GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Structural and Positional Encodings</head><p>As outlined in the previous subsection, GTs are crucially dependent on structural and positional encodings to capture graph structure. Although there is no formal definition or distinction between the two, structural encodings make the GT aware of graph structure on a local, relative, or global level. Such encodings can be attached to node-, edge-, or graph-level features. Examples of local structural encodings include annotating node features with node degree <ref type="bibr">[Chen et al., 2022b]</ref>, the diagonal of the m-step random-walk matrix <ref type="bibr" target="#b16">[Dwivedi et al., 2022]</ref>, the timederivative of the heat-kernel diagonal <ref type="bibr" target="#b34">[Kreuzer et al., 2021]</ref>, enumerate or count predefined substructures and the node's role within <ref type="bibr" target="#b5">[Bouritsas et al., 2022]</ref>, or Ricci curvature <ref type="bibr" target="#b65">[Topping et al., 2022]</ref>. Examples of edge-level relative structural encodings include relative shortest-path distances <ref type="bibr">[Chen et al., 2022a]</ref> or Boolean features indicating if two nodes are in the same substructure <ref type="bibr" target="#b4">[Bodnar et al., 2021]</ref>. Examples of graph-level global structural encodings include eigenvalues of the adjacency or Laplacian matrix <ref type="bibr" target="#b34">[Kreuzer et al., 2021]</ref> or graph properties such as diameter, number of connected components, or treewidth.</p><p>On the other hand, positional encodings make, e.g., a node, aware of its relative position to the other nodes in a graph. Hence, two such encodings should be close to each other if the corresponding nodes are close in the graph. Again, we can distinguish between local, global, or relative encodings.</p><p>Examples of node-level local positional encodings include the shortest-path distance of a node to a hub or central node or the sum of each column of the non-diagonal elements of the m-step random walk matrix. An example of edge-level relative positional encodings is pair-wise node distances <ref type="bibr">[Chen et al., 2022a;</ref><ref type="bibr" target="#b3">Beaini et al., 2021;</ref><ref type="bibr" target="#b34">Kreuzer et al., 2021;</ref><ref type="bibr" target="#b47">Mialon et al., 2021;</ref><ref type="bibr" target="#b37">Li et al., 2020]</ref>. Examples of node-level global positional encodings include eigenvalues of the adjacency or Laplacian matrix <ref type="bibr" target="#b34">[Kreuzer et al., 2021;</ref><ref type="bibr" target="#b15">Dwivedi and Bresson, 2020]</ref> or unique identifiers for each connected component of the graph.</p><p>When designing such encodings, one must ensure equivariance or invariance to the nodes' ordering. Such equivariance is trivially satisfied for simple encodings such as node degree but not for more powerful encodings based on eigenvalues of the adjacency or Laplacian matrix <ref type="bibr" target="#b39">[Lim et al., 2022]</ref>. It is an ongoing effort to design equivariant Laplacian-based encodings <ref type="bibr" target="#b39">[Lim et al., 2022;</ref><ref type="bibr" target="#b68">Wang et al., 2022]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Input Features</head><p>Besides characterizing GTs based on their use of structural and positional encodings, we can also characterize them based on their ability to deal with different node and edge features. To this end, we devise two families of input features. First, we consider so-called non-geometric features where nodes and edges have feature vectors in R d , i.e., graphs are described with a tuple (V, E, X, E). Secondly, we consider so-called geometric features where nodes and edges features contain geometric information, e.g., 3D coordinates for nodes X 3D ? R 3 . Therefore, graphs are described with (V, E, X, E, X 3D , E 3D ). We categorize GT architectures as non-geometric and those supporting both features in the following.</p><p>Non-geometric GTs <ref type="bibr">[Chen et al., 2022b;</ref><ref type="bibr" target="#b10">Choromanski et al., 2021;</ref><ref type="bibr" target="#b15">Dwivedi and Bresson, 2020;</ref><ref type="bibr" target="#b25">He et al., 2022;</ref><ref type="bibr" target="#b31">Kim et al., 2022;</ref><ref type="bibr" target="#b34">Kreuzer et al., 2021;</ref><ref type="bibr" target="#b28">Jain et al., 2021;</ref><ref type="bibr" target="#b47">Mialon et al., 2021;</ref><ref type="bibr" target="#b55">Ramp??ek et al., 2022]</ref> are most common and follow equations in Section 1.1. Graphs with non-geometric features do not have explicit geometric inductive bias. Examples of such features include encoded node attributes in citation networks or learnable atom-type embeddings in molecular graphs. Nongeometric features are supposed to be equivariant to node permutations, and transformers provide such equivariance by default. Structural and positional features, see Section 2.2, are often added to non-geometric features to increase the expressive power of GTs.</p><p>3D molecular graphs provide geometric features describing nodes and edges, e.g., 3D coordinates of atoms, angles of bonds, or torsion angles of planes. Building GTs supporting geometric features is more challenging as geometric features have to be invariant or equivariant to certain group transformations, such as rotation, depending on the task. Further, the architectures must be invariant for graph-level molecular property prediction tasks. In contrast, models must be equivariant in node-level tasks such as predicting structural conformers or force fields. SE(3)-Transformer <ref type="bibr" target="#b20">[Fuchs et al., 2020]</ref> was one of the first attempts to incorporate SE(3) equivariance. By using irreducible representations, Clebsch-Gordan coefficients, and spherical harmonics, the authors encode SE(3) equivariance into the attention operation. Equiformer <ref type="bibr" target="#b38">[Liao and Smidt, 2023]</ref> further extends this mechanism to complete E(3) equivariance. In contrast, TorchMD-NET <ref type="bibr" target="#b64">[Th?lke and Fabritiis, 2022]</ref> achieves SO(3) equivariance by incorporating inter-atomic distances into the attention operation via exponential normal radial basis functions (RBF). Graphormer <ref type="bibr" target="#b61">[Shi et al., 2022]</ref>, Transformer-M <ref type="bibr">[Luo et al., 2022a]</ref> and GPS++ <ref type="bibr" target="#b46">[Masters et al., 2022</ref>] take a similar approach, using Gaussian kernels to encode 3D distances between all pairs of atoms. Tailored for graph-level prediction tasks, GPS++ remains SE(3)-invariant, while Graphormer and Transformer-M introduce an additional SE(3)-equivariant prediction head for node-level molecular dynamics tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Graph to Sequence Tokenization</head><p>The nature of graph tokenization, i.e., mapping a graph into a sequence of tokens, directly affects the supported features and computational complexity. Here, we identify three approaches to graph tokenization: (1) nodes as tokens, (2) nodes and edges as tokens, and (3) patches or subgraphs as tokens.</p><p>Using nodes as input tokens is the most common approach followed by many GTs, e.g., <ref type="bibr" target="#b15">[Dwivedi and Bresson, 2020;</ref><ref type="bibr" target="#b20">Fuchs et al., 2020;</ref><ref type="bibr" target="#b34">Kreuzer et al., 2021;</ref><ref type="bibr">Luo et al., 2022b;</ref><ref type="bibr" target="#b55">Ramp??ek et al., 2022;</ref><ref type="bibr" target="#b64">Th?lke and Fabritiis, 2022;</ref><ref type="bibr" target="#b73">Ying et al., 2021]</ref>. Here, we often treat structural and positional features as additional node features. Given a graph with n nodes and the attention procedure of Eq. ( <ref type="formula" target="#formula_3">1</ref>), the complexity of such GTs is in O(n 2 ). We note that more scalable, sparse attention mechanisms are also possible; see Section 2.5. Edge features, e.g., shortest-path distances <ref type="bibr" target="#b73">[Ying et al., 2021]</ref> or relative 3D distances <ref type="bibr">[Luo et al., 2022b;</ref><ref type="bibr" target="#b64">Th?lke and Fabritiis, 2022]</ref>, may be added as an attention bias given the fully computed attention score matrix with n 2 entries. Alternatively, <ref type="bibr" target="#b47">Mialon et al. [2021]</ref>; <ref type="bibr" target="#b28">Jain et al. [2021]</ref>; <ref type="bibr">Chen et al. [2022b]</ref> leverage a GNN to incorporate node and edge features before applying a transformer on the resulting node features. However, the transformer's quadratic complexity remains the bottleneck.</p><p>The second approach uses nodes and edges in the input sequence as employed by EGT <ref type="bibr" target="#b27">[Hussain et al., 2022]</ref> and TokenGT <ref type="bibr" target="#b31">[Kim et al., 2022]</ref>. Turning an input graph into a graph of its edges is often used in molecular GNNs <ref type="bibr" target="#b22">[Gasteiger et al., 2021]</ref> and NLP <ref type="bibr" target="#b71">[Yao et al., 2020]</ref>. In addition to soft modeling the edges, i.e., the node-to-node interactions, the attention operation also possibly models higher-order nodeedge and edge-edge interactions that theoretically result in an expressiveness boost <ref type="bibr" target="#b31">Kim et al. [2022]</ref>. The input sequence can naturally incorporate node features, their positional encodings, and edge features. A pitfall of this approach is its O(n + m) 2 computational complexity. However, since the approach includes edge features in the input sequence, such GTs might benefit from sparse attention mechanisms that do not materialize the full attention matrix.</p><p>The third approach relies on patches or subgraphs as tokens. In visual transformers <ref type="bibr" target="#b14">[Dosovitskiy et al., 2021]</ref>, such patches correspond to k ? k image slices. A generalization of patches to the graph domain often corresponds to graph coarsening or partitioning <ref type="bibr" target="#b1">[Baek et al., 2021;</ref><ref type="bibr" target="#b35">Kuang et al., 2022;</ref><ref type="bibr">He et al., 2022]</ref>. There, tokens are small subgraphs extracted with various strategies. Initial representations of tokens are obtained by passing subgraphs through a GNN using a form of pooling to a single vector. <ref type="bibr" target="#b25">He et al. [2022]</ref> adds token position features to the resulting vectors to distinguish coarsened subgraphs better. Finally, these tokens are passed through a transformer with O(k 2 ) complexity for a graph with k extracted subgraphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Message Propagation</head><p>Most GTs follow the global all-to-all attention of <ref type="bibr" target="#b66">Vaswani et al. [2017]</ref> between all pairs of tokens. In the initial GT <ref type="bibr" target="#b15">[Dwivedi and Bresson, 2020]</ref> and TokenGT <ref type="bibr" target="#b31">[Kim et al., 2022]</ref> this mechanism is unchanged, relying on token representations augmented with graph structural or positional information. Other models alter the global attention mechanism to bias it explicitly, typically based on the input graph's structural properties. Graphormer <ref type="bibr" target="#b73">[Ying et al., 2021]</ref> incorporates shortestpath distances, representation of edges along a shortest path, and node degrees. Transformer-M <ref type="bibr">[Luo et al., 2022a]</ref> follows Graphormer and adds kernelized 3D inter-atomic distances. GRPE <ref type="bibr" target="#b53">[Park et al., 2022]</ref> considers multiplicative interactions of keys and queries with node and edge features instead of Graphormer's additive bias and additionally augments output token values. SAN <ref type="bibr" target="#b34">[Kreuzer et al., 2021]</ref> relies on positional encodings and only adds preferential bias to interactions along input-graph edges over long-distance virtual edges. GraphiT <ref type="bibr" target="#b47">[Mialon et al., 2021]</ref> employs diffusion kernel bias, while SAT <ref type="bibr">[Chen et al., 2022b</ref>] develops a GNN-based structure-aware attention kernel. EGT <ref type="bibr" target="#b27">[Hussain et al., 2022]</ref> includes a mechanism akin to cross-attention to edge tokens to bias inter-node attention and update edge representations.</p><p>As standard global attention incurs quadratic computational complexity, it limits the application of graph transformers to graphs of up to several thousands of nodes. To alleviate this scaling issue, <ref type="bibr" target="#b11">Choromanski et al. [2022]</ref> proposed GKAT based on a kernelized attention mechanism of the Performer <ref type="bibr" target="#b10">[Choromanski et al., 2021]</ref>, scaling linearly with the number of tokens. Another approach to improve GTs' scaling is to consider a reduced attention scope, e.g., based on locality or sparsified instead of dense all-to-all, following expander graph-based propagation <ref type="bibr" target="#b13">[Deac et al., 2022]</ref>.</p><p>Finally, hybrid approaches combine several propagation schemes. For example, GPS and GPS++ <ref type="bibr" target="#b55">[Ramp??ek et al., 2022;</ref><ref type="bibr" target="#b46">Masters et al., 2022]</ref> fuse local GNN-like models with global all-to-all attention in one layer. While GPS employs standard attention and can utilize linear attention mechanisms such as Performer <ref type="bibr" target="#b11">[Choromanski et al., 2022]</ref>, GPS++ follows Transformer-M's attention conditioning. GraphTrans <ref type="bibr" target="#b28">[Jain et al., 2021]</ref> is also a hybrid but applies a stack of GNNs layers first, followed by a stack of global transformer layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Applications of Graph Transformers</head><p>Although GTs only emerged recently, they have already been applied in various application areas, most notably in molecular property prediction. In the following, we give a representative overview of the applications of GTs. <ref type="bibr" target="#b29">Kan et al. [2022]</ref> propose the Brain Network Transformers to predict properties of brain networks, e.g., the presence of diseases, stemming from magnetic resonance imaging. To that, they leverage rows of the adjacency matrix of each node as structural encodings, which showed superior performance over Laplacian-based encodings in previous studies. Moreover, they devise a custom pooling layer leveraging the fact that nodes in the same functional module tend to have similar properties.  <ref type="bibr" target="#b38">Liao and Smidt [2023]</ref>; <ref type="bibr" target="#b64">Th?lke and Fabritiis [2022]</ref> devise an equivariant transformer architecture to predict quantum mechanical properties of molecules. To capture the molecular structure, they encode atom types and the atomic neighborhood into a vectorial representation, followed by a multi-head attention mechanism. To predict scalar atom-wise prediction, they rely on gated equivariant blocks <ref type="bibr" target="#b60">[Sch?tt et al., 2021]</ref>, which are then aggregated into single molecular predictions. <ref type="bibr" target="#b71">Yao et al. [2020]</ref> use transformers to tackle the graph-tosequence problem, i.e., the problem of translating a graph to word sequences. They first translate a graph to its Levi graph, replacing labeled edges with additional nodes to incorporate edge labels. They then split such a graph into multiple subgraphs according to the different edge nodes. Each subgraph uses a standard transformer architecture to learn the vectorial representation for each node. To incorporate graph structure, they mask out non-neighbors of a node, concentrating on the local structure. Finally, they concatenate multiple node representations. Further applications use transformers for rumor detection in microblogs <ref type="bibr" target="#b30">[Khoo et al., 2020]</ref>, predicting properties of crystals <ref type="bibr" target="#b70">[Yan et al., 2022]</ref> or click-through rates <ref type="bibr">[Min et al., 2022b]</ref>, or leverage them for 3D human pose and mesh reconstruction from a single image <ref type="bibr">[Lin et al., 2021a]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Study</head><p>We empirically evaluate two highly discussed aspects of graph transformers: (1) the effectiveness of incorporating graph structural bias into GTs, and (2) their ability to reduce oversmoothing and over-squashing affecting GNNs. Concretely, we aim to answer the following questions. Q1 How well do different strategies for incorporating structural awareness into GTs contribute to recovering fundamental structural properties of graphs? Q2 Does the ability of transformers to reduce over-smoothing lead to improved performance on heterophilic datasets? Q3 Do graph transformers alleviate over-squashing better than GNN models?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Structural Awareness of GTs</head><p>For question Q1, we evaluate the two most prevalent strategies for incorporating graph structure bias into transformers.</p><p>Positional and Structural Encodings (Sec. 2.2). Randomwalk structural encodings (RWSE) and Laplacian positional encodings (LapPE), two popular positional or structural encodings for transformers <ref type="bibr" target="#b55">[Ramp??ek et al., 2022]</ref>.</p><p>Attention Bias (Sec. 2.5). Attention bias based on spatial information such as shortest-path distance between nodes, following the Graphormer architecture <ref type="bibr" target="#b73">[Ying et al., 2021]</ref>.</p><p>We propose a benchmark of three tasks that require increasingly higher structural awareness of non-geometric graphs. We determine the level of structural awareness necessary to solve a task according to the baseline performance of GIN <ref type="bibr" target="#b69">[Xu et al., 2019]</ref>, a 1-WL-equivalent GNN reference model. In addition, we report the performance of a vanilla transformer without any structural bias to understand the relative impact of the positional or structural encodings (PE/SE) and attention biasing.</p><p>We first describe the tasks in our benchmark and their estimated difficulty, then outline task-specific hyper-parameters of evaluated models, and interpret the observed results; see Table <ref type="table" target="#tab_2">2</ref> for quantitative results.</p><p>Detect Edges (Easy). Detecting whether an edge connects two nodes can be considered the fundamental test for structural awareness. We investigate this task using a custom dataset, EDGES, derived from the ZINC <ref type="bibr" target="#b17">[Dwivedi et al., 2023]</ref> dataset. For each graph, we treat the pairs of nodes connected by an edge as positive examples and select an equal number of unconnected nodes as negative examples, resulting in a binary edge detection task with balanced classes. Let P denote the set of pairs selected as either positive or negative examples, and let h (T ) v denote the feature vector of node v after the last layer T of a model. We make predictions as follows. We first compute the cosine similarity between h (T ) v and h (T ) w for each pair (v, w) of nodes in P , resulting in a scalar similarity score. Finally, we apply a linear layer to each similarity score, followed by a sigmoid activation, resulting in binary class probabilities.</p><p>Count Triangles (Medium). Counting triangles only requires information within a node's immediate neighborhood. However, more than 1-WL expressivity is required to solve it <ref type="bibr" target="#b50">[Morris et al., 2019]</ref>. For this task, we evaluate models on the TRIANGLES dataset proposed by <ref type="bibr" target="#b33">Knyazev et al. [2019]</ref>, which poses triangle counting as a 10-way classification problem. Here, graphs have between 1 and 10 triangles, each corresponding to one class. The dataset specifies a fixed train/validation/test split which we adopt in our experiments. Graphs in the train and validation split are roughly the same size. The test set is a mixture of two graph distributions, where 50% are graphs with a similar size to those in the training and validation set (TRIANGLES-SMALL) and 50% are graphs of larger size (TRIANGLES-LARGE). We separately report model performance for TRIANGLES-SMALL and TRIANGLES-LARGE to study the ability of transformers with different structural biases to generalize to larger graphs. We analyzed the datasets' class balance and report that each test set contains 5000 graphs with 500 graphs per class. For more details about the dataset, see <ref type="bibr" target="#b33">Knyazev et al. [2019]</ref>.</p><p>Distinguish Circular Skip Links (CSL) (Hard). A 1-WL limited model cannot distinguish non-isomorphic CSL graphs <ref type="bibr" target="#b52">[Murphy et al., 2019]</ref> as the task requires an understanding of distance <ref type="bibr" target="#b50">[Morris et al., 2019]</ref>. Here, we evaluate models on the CSL dataset <ref type="bibr" target="#b17">[Dwivedi et al., 2023]</ref>, which contains 150 graphs with skip-link lengths ranging from 2 to 16 and poses a 10-way classification problem. We follow <ref type="bibr" target="#b17">Dwivedi et al. [2023]</ref> in training with 5-fold cross-validation.</p><p>Hyper-parameters. To simplify hyper-parameter selection, we hand-designed two general sets of hyper-parameters; see Table <ref type="table" target="#tab_1">1</ref>. For EDGES and TRIANGLES, we fix a parameter budget of around 200k for the transformer models, resulting in six layers for each model with the respective embedding sizes specified in Table <ref type="table" target="#tab_1">1</ref>. Further, we train Graphormer on 1k epochs. Due to the small number of graphs in the CSL dataset, we fix a parameter budget of around 100k for the transformer models, resulting in three layers for each model with the exact embedding sizes as above. Further, we train Graphormer on 2k epochs. We repeat each experiment on five random seeds and report the model accuracy's mean and standard deviation.</p><p>For our 1-WL-equivalent reference model, we chose the GIN-layer <ref type="bibr" target="#b69">[Xu et al., 2019]</ref>. To improve comparability with the transformer models, we use a feed-forward network composed of the same components and using the same hyper-parameters as for transformers. For Graphormer, we use the feed-forward network specified by <ref type="bibr" target="#b73">Ying et al. [2021]</ref>. Further, we train GIN with the SET 1 hyper-parameters. For EDGES and TRIANGLES, this results in around 150k parameters, while for CSL, the GIN model contains approximately 75k parameters.</p><p>Answering Q1. Table <ref type="table" target="#tab_2">2</ref> shows that GTs supplemented with structural bias generally perform well on all three tasks with a few exceptions. First, the GT with Laplacian positional encodings performs sub-par on the TRIANGLES task. However, it is still an improvement over the 1-WL-equivalent GIN. We hypothesize this is due to an expressivity limit of Laplacian encodings regarding triangle counting. Secondly, we observe that all models generalize poorly to larger graphs on the TRI-ANGLES dataset. Lastly, we observe that on CSL, Graphormer cannot surpass 90% accuracy. A deeper analysis revealed that the shortest-path distributions can only distinguish 9 out of the 10 classes correctly, meaning that Graphormer is theoretically limited to at most 90% accuracy on CSL.</p><p>The above failure cases highlight that current graph transformers still suffer from limited expressivity, and no clear expressivity hierarchy exists for the used positional or structural encodings. Moreover, GTs may generalize poorly to larger graphs. At the same time, we demonstrate a general superiority of structurally biased GTs over standard 1-WL-equivalent models such as GIN. Both the transformer with RWSE as well as Graphormer solve EDGES, TRIANGLES-SMALL, and CSL almost perfectly, two of which pose a challenge for GIN, especially on CSL where GIN performs no better than random.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Reduced Over-smoothing in GTs?</head><p>Graph transformers are often ascribed with an ability to circumvent GNNs' over-smoothing problem due to their global attention mechanism. Thus, we set out to benchmark several variants of GCN <ref type="bibr" target="#b32">[Kipf and Welling, 2017]</ref>, hybrid GPS models, and Graphormer on six heterophilic transductive datasets: ACTOR <ref type="bibr" target="#b63">[Tang et al., 2009]</ref>; CORNELL, TEXAS, WISCON-SIN <ref type="bibr">[CMU, 2001]</ref>; <ref type="bibr">CHAMELEON and SQUIRREL [Rozemberczki et al., 2021]</ref>. In these datasets, we expect over-smoothing and under-reaching to be limiting factors.</p><p>We broadly follow the SET 1 hyper-parameters (Table <ref type="table" target="#tab_1">1</ref>). However, we perform a grid search for each model variant to select the embedding size <ref type="bibr">(32, 64, or 96)</ref> and dropout rates while we fix the number of layers to two. We implement the GCN and GT models following GPS with hybrid GCN+Transformer aggregation layers but with the latter or former component disabled, respectively. We train all models in full-batch mode using the entire graph as input. Answering Q2. All transformer-based models outperform a 2-layer GCN, and often the specialized Geom-GCN <ref type="bibr" target="#b54">[Pei et al., 2020]</ref>, which experimental setup we follow. With the exemption of node degree encodings (DEG), other PE/SE had minimal effect on GCN's performance. Adding global attention to the GCN, i.e., following the GPS model, universally improves the performance. Most interestingly, disabling the local GCN in GPS, i.e., becoming the Transformer model, increases the performance even further. Such results indicate that GNN-like models are unfit for these heterophilic datasets, while the global attention of a transformer empirically facilitates considerably more successful information propagation. Graphormer, which utilizes node degree encodings, performs comparably to the Transformer counterparts. Surprisingly, the attention bias of Graphormer had no impact on its performance. The shortestpath distance bias appears uninformative in these datasets, unlike, e.g., in ZINC, where we observed degradation from 0.12 test mean absolute error to 0.54 when disabling the attention bias.</p><p>We conclude that we empirically confirm the expected benefits of global attention, albeit GTs do not achieve overall SOTA performance (e.g., see <ref type="bibr" target="#b43">Luan et al. [2022]</ref>), which is a reminder that specialized architectures can achieve similar or higher performance without global attention still.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Reduced Over-squashing in GTs?</head><p>To answer question Q3, we evaluate a GT on the NEIGHBORS-MATCH problem proposed by <ref type="bibr" target="#b0">Alon and Yahav [2021]</ref>. This synthetic dataset requires long-range interaction between leaf nodes and the root node of a tree graph of depth d. The problem demonstrates GNNs' difficulty in transmitting information through a receptive field growing exponentially with d. We run our experiments with minimal changes to the code of <ref type="bibr" target="#b0">Alon and Yahav [2021]</ref> and train our transformer on depths 2 to 6. Note that GNN models fail to perfectly fit the training set of trees with depth 4. Convergence on NEIGHBORSMATCH can sometimes take up to 100 000 epochs for large depths d. Since the structure of the graphs in NEIGHBORSMATCH is irrelevant to solving the problem, we did not need to augment node features with positional/structural encodings or attention  <ref type="figure">2</ref> shows that the GT performs exceedingly better than the GNN baselines and can perfectly fit the training set even for depth d = 6. However, we note that the NEIGHBORSMATCH problem is idealized and has only limited practical implications. The core issue of over-squashing, which is squashing an exponentially growing amount of information into a fixed-size vector representation, is not resolved by transformers. Nonetheless, our results demonstrate that the ability of transformers to model long-range interactions between nodes can circumvent the problem posed by <ref type="bibr" target="#b0">Alon and Yahav [2021]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Open Challenges and Future Work</head><p>Since the area of GTs is a new, emerging field, there are still many open challenges, both from practical and theoretical points of view. On the theoretical side, although it is often claimed that GTs offer better predictive performance over GNNs, are more capable of capturing long-range dependencies and preventing over-smoothing and over-squashing, a principled explanation still needs to be formed. Moreover, there needs to be a clearer understanding of the properties of structural and positional encodings. For example, it has yet to be understood when certain encodings are helpful and how they compare. A first step could be precisely characterizing different encodings in terms of distinguishing non-isomorphic graphs, similar to the WL hierarchy. Further, understanding GTs' generalization performance on larger graphs has yet to be understood, at least on a similar level to GNNs <ref type="bibr" target="#b72">[Yehudai et al., 2021;</ref><ref type="bibr" target="#b75">Zhou et al., 2022]</ref>. On the practical side, one major downside of GTs is their quadratic running time in the number of nodes, preventing them from scaling to truly large graphs typical in real-world node-level prediction tasks. Moreover, due to the attention mechanism's nature, it still needs to be determined how best to incorporate edge-feature information into GT architectures. Further, our experimental analysis reveals a disadvantage of local GNN-like model when used in conjunction with transformers as in <ref type="bibr" target="#b55">Ramp??ek et al. [2022]</ref> on heterophilic datasets. Dealing with heterophilic data is thus an open challenge also for GTs. Moreover, it is crucial to incorporate expert or domain knowledge, e.g., physical or chemical knowledge for molecular prediction, into the attention matrix, in a principled manner.</p><p>Explaining and interpreting the performance of GTs remains an open research area where we draw parallels with NLP. We posit that studying graph transformers in the graph machine learning community follows a similar path of studying transformer language models in NLP unified under the name of Bertology <ref type="bibr" target="#b57">[Rogers et al., 2021;</ref><ref type="bibr" target="#b67">Vuli? et al., 2020]</ref>. Numerous Bertology studies reported that more than dissecting attention matrices and attention scores in transformer layers is needed for understanding how language models work. Instead, the community converged on designing datasets and tasks tailored to language model features such as co-reference resolution or mathematical reasoning. Therefore, we hypothesize that understanding GTs' performance through attention scores is limited, and the community should focus on designing diverse benchmarks probing particular GTs' properties. Further, studying scaling laws and emergent behavior of GTs and GNNs is still in its infancy, with few examples in chemistry <ref type="bibr" target="#b19">[Frey et al., 2022]</ref> and protein representation learning <ref type="bibr" target="#b42">[Lin et al., 2022]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have provided a taxonomy of graph transformers (GTs). To this end, we overviewed GTs' theoretical properties and their connection to structural and positional encodings. We then thoroughly surveyed how GTs can deal with different input features, e.g., 3D information, and discussed the different ways of mapping a graph to a sequence of tokens serving as GTs' input. Moreover, we thoroughly discussed different ways GTs propagate information and recent real-world applications. Most importantly, we showed empirically that different encodings and architectural choices drastically impact GTs' predictive performance. We verified that GTs can deal with heterophilic graphs and prevent over-squashing to some extent. Finally, we proposed open challenges and outlined future work. We hope our survey presents a helpful handbook of graph transformers' methods, perspectives, and limitations, and that its insights and principles will help spur and shape novel research results in this emerging field.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>. Throughout the paper, we set n := |V (G)| and m := |E(G)|.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Categorization of graph transformers along four main categories with representative architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Hyper-parameter sets for GTs and GNNs with or without PE/SE (SET 1), and for Graphormer models (SET 2).</figDesc><table><row><cell>Hyper-parameter</cell><cell>SET 1</cell><cell>SET 2</cell></row><row><cell>Embed. dim.</cell><cell>64</cell><cell>72</cell></row><row><cell>Self-attn. heads</cell><cell>4</cell><cell>4</cell></row><row><cell>Weight decay</cell><cell>10 -5</cell><cell>10 -2</cell></row><row><cell>Learning rate</cell><cell>10 -3</cell><cell>10 -3</cell></row><row><cell>Gradient clip norm</cell><cell>1.0</cell><cell>5.0</cell></row><row><cell>LR scheduler</cell><cell>cosine, warm-up</cell><cell>constant</cell></row><row><cell>Batch size</cell><cell>96</cell><cell>256</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Average test accuracy of GTs with structural bias (? SD) over five random seeds on the structural awareness tasks. Difficulty level on top derived from GIN performance. We additionally report the performance of a transformer without any structural bias serving as a baseline.</figDesc><table><row><cell></cell><cell>Easy</cell><cell>Medium</cell><cell></cell><cell>Hard</cell></row><row><cell>Model</cell><cell>EDGES</cell><cell cols="2">TRIANGLES-SMALL TRIANGLES-LARGE</cell><cell>CSL</cell></row><row><cell></cell><cell cols="4">2-way Accuracy ? 10-way Accuracy ? 10-way Accuracy ? 10-way Accuracy ?</cell></row><row><cell>GIN</cell><cell>98.11 ?1.78</cell><cell>71.53 ?0.94</cell><cell>33.54 ?0.30</cell><cell>10.00 ?0.00</cell></row><row><cell>Transformer</cell><cell>55.84 ?0.32</cell><cell>12.08 ?0.31</cell><cell>10.01 ?0.04</cell><cell>10.00 ?0.00</cell></row><row><cell>Transformer (LapPE)</cell><cell>98.00 ?1.03</cell><cell>78.29 ?0.25</cell><cell>10.64 ?2.94</cell><cell>100.00 ?0.00</cell></row><row><cell>Transformer (RWSE)</cell><cell>97.11 ?1.73</cell><cell>99.40 ?0.10</cell><cell>54.76 ?7.24</cell><cell>100.00 ?0.00</cell></row><row><cell>Graphormer</cell><cell>97.67 ?0.97</cell><cell>99.09 ?0.31</cell><cell>42.34 ?6.48</cell><cell>90.00 ?0.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Benchmarking of multiple model variants on six heterophilic transductive datasets. Here we report average test accuracy (? SD) over ten random seeds. We follow the dataset protocol of<ref type="bibr" target="#b54">Pei et al. [2020]</ref>; for additional model comparison; see<ref type="bibr" target="#b43">Luan et al. [2022]</ref>.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Figure 2: Average train accuracy over</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ten random seeds of a GT on the NEIGH-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>BORSMATCH dataset, compared to mod-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>els from Alon and Yahav [2021].</cell></row><row><cell>Model (PE/SE type)</cell><cell>ACTOR</cell><cell>CORNELL</cell><cell>TEXAS</cell><cell cols="3">WISCONSIN CHAMELEON SQUIRREL</cell></row><row><cell cols="5">Geom-GCN [Pei et al., 2020] 31.59 ?1.15 60.54 ?3.67 64.51 ?3.66 66.76 ?2.72 GCN (no PE/SE) 33.92 ?0.63 53.78 ?3.07 65.95 ?3.67 66.67 ?2.63</cell><cell>60.00 ?2.81 43.14 ?1.33</cell><cell>38.15 ?0.92 30.70 ?1.17</cell><cell>1.0</cell></row><row><cell>GCN (LapPE) GCN (RWSE)</cell><cell cols="4">34.30 ?1.12 56.22 ?2.65 65.95 ?3.67 66.47 ?1.37 33.69 ?1.07 53.78 ?4.09 62.97 ?3.21 69.41 ?2.66</cell><cell>43.53 ?1.45 43.84 ?1.68</cell><cell>30.80 ?1.38 31.77 ?0.65</cell><cell>0.8</cell></row><row><cell>GCN (DEG) GPS GCN+Transformer (LapPE) GPS GCN+Transformer (RWSE) GPS GCN+Transformer (DEG) Transformer (LapPE) Transformer (RWSE)</cell><cell cols="4">33.99 ?0.91 53.51 ?2.65 66.76 ?2.72 67.26 ?1.53 37.68 ?0.52 66.22 ?3.87 75.41 ?1.46 74.71 ?2.97 36.95 ?0.65 65.14 ?5.73 73.51 ?2.65 78.04 ?2.88 36.91 ?0.56 64.05 ?2.43 73.51 ?3.59 75.49 ?4.23 38.43 ?0.87 69.46 ?1.73 77.84 ?1.08 76.08 ?1.92 38.13 ?0.63 70.81 ?2.02 77.57 ?1.24 80.20 ?2.23</cell><cell>46.36 ?2.07 48.57 ?1.02 47.57 ?0.90 52.59 ?1.81 49.69 ?1.11 49.45 ?1.34</cell><cell>34.50 ?0.87 35.58 ?0.58 34.78 ?1.21 42.24 ?1.09 35.77 ?0.50 35.35 ?0.75</cell><cell>Train Acc.</cell><cell>0.6 0.4 0.2</cell><cell>GCN GGNN Transformer</cell></row><row><cell>Transformer (DEG) Graphormer (DEG only)</cell><cell cols="4">37.39 ?0.50 71.89 ?2.48 77.30 ?1.32 79.80 ?0.90 36.91 ?0.85 68.38 ?1.73 76.76 ?1.79 77.06 ?1.97</cell><cell>56.18 ?0.83 54.08 ?2.35</cell><cell>43.64 ?0.65 43.20 ?0.82</cell><cell>2</cell><cell>3 Problem radius (depth) 4 5</cell><cell>6</cell></row><row><cell cols="5">Graphormer (DEG, attn. bias) 36.69 ?0.70 68.38 ?1.73 76.22 ?2.36 77.65 ?2.00</cell><cell>53.84 ?2.32</cell><cell>43.75 ?0.59</cell></row><row><cell cols="5">bias. Otherwise, we used the same transformer architecture as</cell><cell></cell><cell></cell></row><row><cell>in Section 4.1.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Answering Q3. Figure</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On the bottleneck of graph neural networks and its practical implications</title>
		<author>
			<persName><forename type="first">U</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yahav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Accurate learning of graph representations with graph multiset pooling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Network biology: understanding the cell&apos;s functional organization</title>
		<author>
			<persName><forename type="first">A.-L</forename><surname>Barabasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">N</forename><surname>Oltvai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Genetics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="101" to="113" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Directional graph networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Passaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>L?tourneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="748" to="758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Weisfeiler and Lehman go cellular: CW networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bodnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Otter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Montufar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="2625" to="2640" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving graph neural network expressivity via subgraph isomorphism counting</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bouritsas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Machine learning on graphs: A model and comprehensive taxonomy</title>
		<author>
			<persName><forename type="first">I</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>R?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="page" from="1" to="64" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the equivalence between graph isomorphism testing and function approximation with gnns</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="15868" to="15876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A survey on graph neural networks and graph transformers in computer vision: A task-oriented perspective</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Structure-aware transformer for graph representation learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rethinking attention with performers</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sarl?s</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Q</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Weller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">From block-Toeplitz matrices to differential equations on graphs: towards a general theory for scalable masked transformers</title>
		<author>
			<persName><forename type="first">K</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sehanobish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Parker-Holder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Weller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weingarten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">World Wide Knowledge Base</title>
		<imprint/>
		<respStmt>
			<orgName>CMU</orgName>
		</respStmt>
	</monogr>
	<note>Web-KB</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Expander graph propagation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Deac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lackenby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning on Graphs Conference</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A generalization of transformer networks to graphs</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bresson. Graph neural networks with learnable structural and positional representations</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bresson. Benchmarking graph neural networks</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Easley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<title level="m">Networks, Crowds, and Markets: Reasoning About a Highly Connected World</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural scaling of deep chemical models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Soklaski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Axelrod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Samsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gomez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Coley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gadepally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">chemRxiv</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">SE(3)-Transformers: 3D roto-translation equivariant attention networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Iterative SE(3)-Transformers</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">B</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wagstaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dauparas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Geometric Science of Information</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="585" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gemnet: Universal directional graph neural networks for molecules</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gasteiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="6790" to="6802" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A survey on vision transformer</title>
		<author>
			<persName><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bresson. A generalization of ViT/MLP-mixer to graphs</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">OGB-LSC: A large-scale challenge for machine learning on graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nakata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS: Datasets and Benchmarks Track</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Global selfattention as a replacement for graph convolution</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Zaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Subramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="655" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Representing long-range context for graph neural networks with global attention</title>
		<author>
			<persName><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">X</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain network transformer. ArXiv</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Interpretable rumor detection in microblogs by attending to user interactions</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M S</forename><surname>Khoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Chieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8783" to="8790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Pure transformers are powerful graph learners</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Understanding attention and generalization in graph neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Knyazev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Amer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="4202" to="4212" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Rethinking graph transformers with spectral attention</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kreuzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>L?tourneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tossou</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Coarformer: Transformer for large graph via graph coarsening</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ding</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>OpenReview</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3538" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Distance encoding: Design provably more powerful neural networks for graph representation learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Equiformer: Equivariant graph attention transformer for 3D atomistic graphs</title>
		<author>
			<persName><forename type="first">Y.-L</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Smidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Sign and basis invariant networks for spectral graph representation learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Smidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Mesh graphormer</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12919" to="12928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A survey of transformers</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Language models of protein sequences at the scale of evolution enable accurate structure prediction</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Akin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Smetanin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dos Santos Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fazel-Zarandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Candido</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Revisiting heterophily for graph neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<title level="m">One transformer can understand both 2D &amp; 3D molecular data</title>
		<imprint>
			<publisher>ArXiv</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Your transformer may not be as powerful as you expect</title>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">GPS++: an optimised hybrid MPNN/transformer for molecular property prediction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Masters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maddrell-Mander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sanders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Helal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Beker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ramp??ek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Beaini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">GraphiT: Encoding graph structure in transformers</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mialon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Selosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Transformer for graphs: An overview from architecture perspective</title>
		<author>
			<persName><forename type="first">E</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ananiadou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Neighbour interaction based click-through rate prediction via graph-masked transformer</title>
		<author>
			<persName><forename type="first">E</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ananiadou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="353" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Weisfeiler and Leman go machine learning: The story so far</title>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rieck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Relational pooling for graph representations</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4663" to="4673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">GRPE: Relative positional encoding for graph transformer</title>
		<author>
			<persName><forename type="first">W</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Geom-gcn: Geometric graph convolutional networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Recipe for a General, Powerful, Scalable Graph Transformer</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ramp??ek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Galkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Beaini</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Graph neural networks for materials science and chemistry</title>
		<author>
			<persName><forename type="first">P</forename><surname>Reiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Eberhard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Metni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Van Hoesel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schopmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications Materials</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">93</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A primer in bertology: What we know about how bert works</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kovaleva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rumshisky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="842" to="866" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Multi-scale attributed node embedding</title>
		<author>
			<persName><forename type="first">B</forename><surname>Rozemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Complex Networks</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Equivariant message passing for the prediction of tensorial properties and molecular spectra</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sch?tt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">T</forename><surname>Unke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gastegger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9377" to="9388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Benchmarking graphormer on largescale molecular modeling datasets</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Dynamic edge-conditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Social influence analysis in large-scale networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Equivariant transformers for neural network based molecular potentials</title>
		<author>
			<persName><forename type="first">P</forename><surname>Th?lke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Fabritiis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Understanding over-squashing and bottlenecks on graphs via curvature</title>
		<author>
			<persName><forename type="first">J</forename><surname>Topping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Giovanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Probing pretrained language models for lexical semantics</title>
		<author>
			<persName><forename type="first">I</forename><surname>Vuli?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Ponti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Litschko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Glava?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7222" to="7240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Equivariant and stable positional encoding for more powerful graph neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Periodic graph transformers for crystal material property prediction</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Heterogeneous graph transformer for graph-to-sequence learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<publisher>Online</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7145" to="7154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">From local structures to size generalization in graph neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Yehudai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Meirom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="11975" to="11986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Do transformers really perform badly for graph representation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Big Bird: Transformers for longer sequences</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Onta??n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">OOD link prediction generalization capabilities of message-passing GNNs in larger test graphs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kutyniok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ribeiro</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
