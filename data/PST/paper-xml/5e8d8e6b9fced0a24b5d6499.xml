<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Zero-Shot Entity Linking by Reading Entity Descriptions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-06-18">18 Jun 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<country>‡ Google Research</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
							<email>mingweichang@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<country>‡ Google Research</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
							<email>kentonl@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<country>‡ Google Research</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<country>‡ Google Research</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
							<email>jacobdevlin@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<country>‡ Google Research</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
							<email>honglak@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<country>‡ Google Research</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Zero-Shot Entity Linking by Reading Entity Descriptions</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-06-18">18 Jun 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1906.07348v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present the zero-shot entity linking task, where mentions must be linked to unseen entities without in-domain labeled data. The goal is to enable robust transfer to highly specialized domains, and so no metadata or alias tables are assumed. In this setting, entities are only identified by text descriptions, and models must rely strictly on language understanding to resolve the new entities. First, we show that strong reading comprehension models pre-trained on large unlabeled data can be used to generalize to unseen entities. Second, we propose a simple and effective adaptive pre-training strategy, which we term domainadaptive pre-training (DAP), to address the domain shift problem associated with linking unseen entities in a new domain. We present experiments on a new dataset that we construct for this task and show that DAP improves over strong pre-training baselines, including BERT. The data and code are available at https: //github.com/lajanugen/zeshel. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Entity linking systems have achieved high performance in settings where a large set of disambiguated mentions of entities in a target entity dictionary is available for training. Such systems typically use powerful resources such as a high-coverage alias table, structured data, and linking frequency statistics. For example, <ref type="bibr" target="#b15">Milne and Witten (2008)</ref> show that by only using the prior probability gathered from hyperlink statistics on Wikipedia training articles, one can achieve 90% accuracy on the task of predicting links in Wikipedia test articles.</p><p>While most prior works focus on linking to general entity databases, it is often desirable to link to specialized entity dictionaries such as legal cases, company project descriptions, the set of characters in a novel, or a terminology glossary. Unfortunately, labeled data are not readily available and are often expensive to obtain for these specialized entity dictionaries. Therefore, we need to develop entity linking systems that can generalize to unseen specialized entities. Without frequency statistics and meta-data, the task becomes substantially more challenging. Some prior works have pointed out the importance of building entity linking systems that can generalize to unseen entity sets <ref type="bibr" target="#b21">(Sil et al., 2012;</ref><ref type="bibr" target="#b23">Wang et al., 2015)</ref>, but adopt an additional set of assumptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Review: Entity linking</head><p>Entity linking (EL) is the task of grounding entity mentions by linking them to entries in a given database or dictionary of entities. Formally, given a mention m and its context, an entity linking system links m to the corresponding entity in an entity set E = {e i } i=1,...,K , where K is the number of entities. The standard definition of EL <ref type="bibr" target="#b0">(Bunescu and Pasca, 2006;</ref><ref type="bibr" target="#b19">Roth et al., 2014;</ref><ref type="bibr" target="#b20">Sil et al., 2018)</ref> assumes that mention boundaries are provided by users or a mention detection system. The entity set E can contain tens of thousands or even millions of entities, making this a challenging task. In practice, many entity linking systems rely on the following resources or assumptions:</p><p>Single entity set This assumes that there is a single comprehensive set of entities E shared between training and test examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Alias table</head><p>An alias table contains entity candidates for a given mention string and limits the possibilities to a relatively small set. Such tables are often compiled from a labeled training set and domain-specific heuristics.</p><p>Frequency statistics Many systems use frequency statistics obtained from a large labeled corpus to estimate entity popularity and the probability of a mention string linking to an entity. These statistics are very powerful when available.</p><p>Structured data Some systems assume access to structured data such as relationship tuples (e.g., (Barack Obama, Spouse, Michelle Obama)) or a type hierarchy to aid disambiguation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Task Definition</head><p>The main motivation for this task is to expand the scope of entity linking systems and make them generalizable to unseen entity sets for which none of the powerful resources listed above are readily available. Therefore, we drop the above assumptions and make one weak assumption: the existence of an entity dictionary E = {(e i , d i )} i=1,..,K , where d i is a text description of entity e i .</p><p>Our goal is to build entity linking systems that can generalize to new domains and entity dictionaries, which we term worlds. We define a world as W = (M W , U W , E W ), where M W and U W are distributions over mentions and documents from the world, respectively, and E W is an entity dictionary associated with W.  <ref type="bibr" target="#b21">(Sil et al., 2012)</ref> Zero-Shot EL Table <ref type="table">1</ref>: Assumptions and resources for entity linking task definitions. We classify task definitions based on whether (i) the system is tested on mentions from the training domain (In-Domain), (ii) linked mentions from the target entity set are seen during training (Seen Entity Set), (iii) a small high-coverage candidate set can be derived using alias tables or strict token overlap constraints (Small Candidate Set) and the availability of (iv) Frequency statistics, (v) Structured Data, and (vi) textual descriptions (Entity dictionary).</p><p>are defined as mention spans in documents from U W . We assume the availability of labelled men- We additionally assume that samples from the document distribution U Wtgt and the entity descriptions E Wtgt are available for training. These samples can be used for unsupervised adaptation to the target world. During training, mention boundaries for mentions in W tgt are not available. At test time, mention boundaries are provided as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Relationship to other EL tasks</head><p>We summarize the relationship between the newly introduced zero-shot entity linking task and prior EL task definitions in Table <ref type="table">1</ref>.</p><p>Standard EL While there are numerous differences between EL datasets <ref type="bibr" target="#b0">(Bunescu and Pasca, 2006;</ref><ref type="bibr" target="#b14">Ling et al., 2015)</ref>, most focus on a standard setting where mentions from a comprehensive test entity dictionary (often Wikipedia) are seen during training, and rich statistics and meta-data can be utilized <ref type="bibr" target="#b19">(Roth et al., 2014)</ref>. Labeled in-domain documents with mentions are also assumed to be available.</p><p>Cross-Domain EL Recent work has also generalized to a cross-domain setting, linking entity mentions in different types of text, such as blogposts and news articles to the Wikipedia KB, while only using labeled mentions in Wikipedia for training (e.g., <ref type="bibr" target="#b10">Gupta et al. (2017)</ref>; <ref type="bibr" target="#b13">Le and Titov (2018)</ref>, inter alia).</p><p>Linking to Any <ref type="bibr">DB Sil et al. (2012)</ref> proposed a task setup very similar to ours, and later work <ref type="bibr" target="#b23">(Wang et al., 2015)</ref> has followed a similar setting. The main difference between zero-shot EL and these works is that they assumed either a highcoverage alias table or high-precision token overlap heuristics to reduce the size of the entity candidate set (i.e., to less than four in <ref type="bibr" target="#b21">Sil et al. (2012)</ref>) and relied on structured data to help disambiguation. By compiling and releasing a multi-world dataset focused on learning from textual information, we hope to help drive progress in linking entities for a broader set of applications.</p><p>Work on word sense disambiguation based on dictionary definitions of words is related as well <ref type="bibr" target="#b1">(Chaplot and Salakhutdinov, 2018)</ref>, but this task exhibits lower ambiguity and existing formulations have not focused on domain generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset Construction</head><p>We construct a new dataset to study the zeroshot entity linking problem using documents from Wikia. <ref type="foot" target="#foot_1">3</ref> Wikias are community-written encyclopedias, each specializing in a particular subject or theme such as a fictional universe from a book or film series. Wikias have many interesting properties suitable for our task. Labeled mentions can be automatically extracted based on hyperlinks. Mentions and entities have rich document context that can be exploited by reading comprehension approaches. Each Wikia has a large number of unique entities relevant to a specific theme, making it a useful benchmark for evaluating domain generalization of entity linking systems.</p><p>We use data from 16 Wikias, and use 8 of them for training and 4 each for validation and testing. To construct data for training and evaluation, we first extract a large number of mentions from the Wikias. Many of these mentions can be easily linked by string matching between mention string and the title of entity documents. These mentions are downsampled during dataset construction, and occupy a small percentage (5%) of the final dataset. While not completely representative of the natural distribution of mentions, this data construction method follows recent work that focuses on evaluating performance on the challenging aspects of the entity linking problem (e.g., <ref type="bibr" target="#b10">Gupta et al. (2017)</ref> selected mentions with multiple possible entity candidates for assessing indomain unseen entity performance). Each Wikia document corresponds to an entity, represented by the title and contents of the document. These entities, paired with their text descriptions, comprise the entity dictionary. Since the task is already quite challenging, we assume that the target entity exists in the entity dictionary and leave NIL recognition or clustering (NIL mentions/entities refer to entities nonexistent in the knowledge-base) to future editions of the task and dataset.</p><p>We categorize the mentions based on token overlap between mentions and the corresponding entity title as follows. High Overlap: title is identical to mention text, Multiple Categories: title is mention text followed by a disambiguation phrase (e.g., mention string: 'Batman', title: 'Batman (Lego)'), Ambiguous substring: mention is a substring of title (e.g., mention string: 'Agent', title: 'The Agent'). All other mentions are categorized  as Low Overlap. These mentions respectively constitute approximately 5%, 28%, 8% and 59% of the mentions in the dataset.</p><p>Table <ref type="table" target="#tab_2">2</ref> shows some statistics of the dataset. Each domain has a large number of entities ranging from 10,000 to 100,000. The training set has 49,275 labeled mentions. To examine the indomain generalization performance, we construct heldout sets seen and unseen of 5,000 mentions each, composed of mentions that link to only entities that were seen or unseen during training, respectively. The validation and test sets have 10,000 mentions each (all of which are unseen).</p><p>Table <ref type="table" target="#tab_4">3</ref> shows examples of mentions and entities in the dataset. The vocabulary and language used in mentions and entity descriptions differs drastically between the different domains. In addition to acquiring domain specific knowledge, understanding entity descriptions and performing reasoning is required in order to resolve mentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Models for Entity Linking</head><p>We adopt a two-stage pipeline consisting of a fast candidate generation stage, followed by a more expensive but powerful candidate ranking stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Candidate generation</head><p>Without alias tables for standard entity linking, a natural substitute is to use an IR approach for candidate generation. We use BM25, a variant of TF-IDF to measure similarity between mention string and candidate documents. <ref type="foot" target="#foot_2">4</ref> Top-k entities retrieved by BM25 scoring with Lucene<ref type="foot" target="#foot_3">5</ref> are used for training and evaluation. In our experiments k is set to 64. The coverage of the top-64 candidates is less than 77% on average, indicating the difficulty of the task and leaving substantial room for improvement in the candidate generation phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Candidate ranking</head><p>Since comparing two texts-a mention in context and a candidate entity description-is a task similar to reading comprehension and natural language inference tasks, we use an architecture based on a deep Transformer <ref type="bibr" target="#b22">(Vaswani et al., 2017)</ref> which has achieved state-of-the-art performance on such tasks <ref type="bibr" target="#b18">(Radford et al., 2018;</ref><ref type="bibr" target="#b4">Devlin et al., 2019)</ref>.</p><p>As in BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>, the mention in context m and candidate entity description e, each represented by 128 word-piece tokens, are concatenated and input to the model as a sequence pair together with special start and separator tokens:</p><formula xml:id="formula_0">([CLS] m [SEP] e [SEP]</formula><p>). Mention words are signaled by a special embedding vector that is added to the mention word embeddings. The Transformer encoder produces a vector representation h m,e of the input pair, which is the output of the last hidden layer at the special pooling token <ref type="bibr">[CLS]</ref>. Entities in a given candidate set are scored as w h m,e where w is a learned parameter vector, and the model is trained using a softmax loss. An architecture with 12 layers, hidden dimension size 768 and 12 attention heads was used in our experiments. We refer to this model as Full-Transformer. By jointly encoding the entity description and the mention in context with a Transformer, they can attend to each other at every layer.</p><p>Note that prior neural approaches for entity linking have not explored such architectures with deep cross-attention. To assess the value of this departure from prior work, we implement the following two variants: (i) Pool-Transformer: a siamese-like network which uses two deep Transformers to separately derive single-vector repre-sentations of the mention in context, h m , and the candidate entity, h e ; they take as input the mention in context and entity description respectively, together with special tokens indicating the boundaries of the texts: ([CLS] m [SEP]) and ([CLS] e [SEP]), and output the last hidden layer encoding at the special start token. The scoring function is h m h e . Single vector representations for the two components have been used in many prior works, e.g., <ref type="bibr" target="#b10">Gupta et al. (2017)</ref>. (ii) Cand-Pool-Transformer: a variant which uses single vector entity representations but can attend to individual tokens of the mention and its context as in <ref type="bibr" target="#b6">Ganea and Hofmann (2017)</ref>. This architecture also uses two Transformer encoders, but introduces an additional attention module which allows h e to attend to individual token representations of the mention in context.</p><p>In the experiments section, we also compare to re-implementations of <ref type="bibr" target="#b10">Gupta et al. (2017)</ref> and <ref type="bibr" target="#b6">Ganea and Hofmann (2017)</ref>, which are similar to Pool-Transformer and Cand-Pool-Transformer respectively but with different neural architectures for encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Adapting to the Target World</head><p>We focus on using unsupervised pre-training to ensure that downstream models are robust to target domain data. There exist two general strategies for pre-training: (1) task-adaptive pre-training, and</p><p>(2) open-corpus pre-training. We describe these below, and also propose a new strategy: domainadaptive pre-training (DAP), which is complementary to the two existing approaches.</p><p>Task-adaptive pre-training <ref type="bibr" target="#b8">Glorot et al. (2011)</ref>; <ref type="bibr" target="#b2">Chen et al. (2012)</ref>; <ref type="bibr" target="#b24">Yang and Eisenstein (2015)</ref>, inter alia, pre-trained on the source and target domain unlabeled data jointly with the goal of discovering features that generalize across domains. After pre-training, the model is fine-tuned on the source-domain labeled data. <ref type="foot" target="#foot_4">6</ref>Open-corpus pre-training Instead of explicitly adapting to a target domain, this approach simply applies unsupervised pre-training to large corpora before fine-tuning on the source-domain labeled data. Examples of this approach include ELMo <ref type="bibr" target="#b17">(Peters et al., 2018)</ref>, OpenAI GPT <ref type="bibr" target="#b18">(Radford et al., 2018)</ref>, and BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>.</p><p>Intuitively, the target-domain distribution is likely to be partially captured by pre-training if the open corpus is sufficiently large and diverse. Indeed, open-corpus pre-training has been shown to benefit out-of-domain performance far more than indomain performance <ref type="bibr" target="#b11">(He et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain-adaptive pre-training</head><p>In addition to pre-training stages from other approaches, we propose to insert a penultimate domain adaptive pre-training (DAP) stage, where the model is pre-trained only on the target-domain data. As usual, DAP is followed by a final fine-tuning stage on the source-domain labeled data. The intuition for DAP is that representational capacity is limited, so models should prioritize the quality of target domain representations above all else.</p><p>We introduce notation to describe various ways in which pre-training stages can be composed.</p><p>• U src denotes text segments from the union of source world document distributions U W 1 src . . . U W n src . • U tgt denotes text segments from the document distribution of a target world W tgt .</p><p>• U src+tgt denotes randomly interleaved text segments from both U src and U tgt .</p><p>• U WB denotes text segments from open corpora, which in our experiments are Wikipedia and the BookCorpus datasets used in BERT.</p><p>We can chain together a series of pre-training stages. For example, U WB → U src+tgt → U tgt indicates that the model is first pre-trained on the open corpus, then pre-trained on the combined source and target domains, then pre-trained on only the target domain, and finally fine-tuned on the source-domain labeled data. <ref type="foot" target="#foot_5">7</ref> We show that chaining together different pre-training strategies provides additive gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>Pre-training We use the BERT-Base model architecture in all our experiments. The Masked LM objective <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref> is used for unsupervised pre-training. For fine-tuning language models (in the case of multi-stage pre-training) and fine-tuning on the Entity-Linking task, we use a small learning rate of 2e-5, following the recommendations from <ref type="bibr" target="#b4">Devlin et al. (2019)</ref>. For models trained from scratch we use a learning rate of 1e-4.</p><p>Evaluation We define the normalized entitylinking performance as the performance evaluated on the subset of test instances for which the gold entity is among the top-k candidates retrieved during candidate generation. The unnormalized performance is computed on the entire test set. Our IR-based candidate generation has a top-64 recall of 76% and 68% on the validation and test sets, respectively. The unnormalized performance is thus upper-bounded by these numbers. Strengthening the candidate generation stage improves the unnormalized performance, but this is outside the scope of our work. Average performance across a set of worlds is computed by macro-averaging.</p><p>Performance is defined as the accuracy of the single-best identified entity (top-1 accuracy).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Baselines</head><p>We first examine some baselines for zero-shot entity linking in Table <ref type="table" target="#tab_5">4</ref>. We include naive baselines such as Levenshtein edit-distance and TF-IDF, which compare the mention string against candidate entity title and full document description, respectively, to rank candidate entities. We re-implemented recent neural models designed for entity linking <ref type="bibr" target="#b6">(Ganea and Hofmann, 2017;</ref><ref type="bibr" target="#b10">Gupta et al., 2017)</ref>, but did not expect them to perform well since the original systems were designed for settings where labeled mentions or meta-data for the target entities were available.   The poor performance of these models validates the necessity of using strong reading comprehension models for zero-shot entity linking.</p><p>When using the Full-Transformer model, pretraining is necessary to achieve reasonable performance. We present results for models pre-trained on different subsets of our task corpus (U src , U tgt , U src+tgt ) as well as pre-training on an external large corpus (U WB ). We observe that the choice of data used for pre-training is important.</p><p>In Table <ref type="table" target="#tab_5">4</ref> we also compare the Pool-Transformer, Candidate-Pool-Transformer and Full-Transformer. The significant gap between Full-Transformer and the other variants shows the importance of allowing fine-grained comparisons between the two inputs via the cross attention mechanism embedded in the Transformer. We hypothesize that prior entity linking systems did not need such powerful reading comprehension models due to the availability of strong additional meta information. The remaining experiments in the paper use the Full-Transformer model, unless mentioned otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Generalization to Unseen Entities and New Worlds</head><p>To analyze the impact of unseen entities and domain shift in zero-shot entity linking, we evaluate performance on a more standard in-domain entity linking setting by making predictions on held out mentions from the training worlds. 5-point drop in performance. Entities from new worlds (which are by definition unseen and are mentioned in out-of-domain text) prove to be the most difficult. Due to the shift in both the language distribution and entity sets, we observe a 11-point drop in performance. This large generalization gap demonstrates the importance of adaptation to new worlds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Impact of Domain Adaptive Pre-training</head><p>Our experiments demonstrate that DAP improves on three state-of-the-art pre-training strategies:</p><p>• U src+tgt : task-adaptive pre-training, which combines source and target data for pretraining <ref type="bibr" target="#b8">(Glorot et al., 2011)</ref>.<ref type="foot" target="#foot_6">9</ref> </p><p>• U WB : open-corpus pre-training, which uses Wikipedia and the BookCorpus for pre-training (We use a pre-trained BERT model <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>).</p><p>• U WB → U src+tgt : the previous two strategies chained together. While no prior work has applied this approach to domain adaptation, a similar approach for task adaptation was proposed by <ref type="bibr" target="#b12">Howard and Ruder (2018)</ref>. The results are in Figure <ref type="figure" target="#fig_2">2</ref>(a). DAP improves all pre-training strategies with an additional pretraining stage on only target-domain data. The best setting, U WB → U src+tgt → U tgt , chains together all existing strategies. DAP improves the performance over a strong pre-trained model <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref> by 2%.</p><p>To further analyze the results of DAP, we plot the relationships between the accuracy of Masked LM (MLM accuracy) on target unlabeled data and the final target normalized accuracy (after finetuning on the source labeled data) in Figure <ref type="figure" target="#fig_2">2(b)</ref>. Adding an additional pre-training stage on the target unlabeled data unsurprisingly improves the MLM accuracy. More interestingly, we find improvements in MLM accuracy are consistently followed by improvements in entity linking accuracy. It is intuitive that performance on unsupervised objectives reflect the quality of learned representations and correlate well with downstream performance. We show empirically that this trend holds for a variety of pre-training strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Test results and performance analysis</head><p>Table <ref type="table">6</ref> shows the normalized and unnormalized Entity Linking performance on test worlds. Our best model that chains together all pretraining strategies achieves normalized accuracy of 77.05% and unnormalized accuracy of 56.58%. Note that the unnormalized accuracy corresponds to identifying the correct entity from tens of thousands of candidate entities.</p><p>To analyze the mistakes made by the model, we compare EL accuracy across different mention categories in Table <ref type="table">7</ref>. Candidate generation (Recall@64) is poor in the Low Overlap category. However, the ranking model performs in par with other hard categories for these mentions. Overall EL accuracy can thus be improved significantly by strengthening candidate generation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>We discussed prior entity linking task definitions and compared them to our task in section 2. Here, we briefly overview related entity linking models and unsupervised domain adaptation methods.</p><p>Entity linking models Entity linking given mention boundaries as input can be broken into the tasks of candidate generation and candidate ranking. When frequency information or alias tables are unavailable, prior work has used measures of similarity of the mention string to entity names for candidate generation <ref type="bibr" target="#b21">(Sil et al., 2012;</ref><ref type="bibr" target="#b16">Murty et al., 2018)</ref>. For candidate ranking, recent work employed distributed representations of mentions in context and entity candidates and neural models to score their compatibility. Mentions in context have been represented using e.g., CNN <ref type="bibr" target="#b16">(Murty et al., 2018)</ref>, LSTM <ref type="bibr" target="#b10">(Gupta et al., 2017)</ref>, or bag-of-word embeddings (Ganea and Hofmann, 2017). Entity descriptions have been represented using similar architectures. To the best of our knowledge, while some models allow for crossattention between single-vector entity embeddings and mention-in-context token representations, no prior works have used full cross-attention between mention+context and entity descriptions. Prior work on entity linking tasks most similar to ours used a linear model comparing a mention in context to an entity description and associated structured data <ref type="bibr" target="#b21">(Sil et al., 2012)</ref>. <ref type="bibr" target="#b21">Sil et al. (2012)</ref> also proposed a distant supervision approach which could use first-pass predictions for mentions in the target domain as noisy supervision for re-training an in-domain model. We believe this approach is complementary to unsupervised representation learning and could bring additional benefits. In another task similar to ours, <ref type="bibr" target="#b23">Wang et al. (2015)</ref> used collective inference and target database relations to obtain good performance without (domain, target database)-specific labeled training data. Collective inference is another promising direction, but could have limited success when no metadata is available.</p><p>Unsupervised domain adaptation There is a large body of work on methods for unsupervised domain adaptation, where a labeled training set is available for a source domain and unlabeled data is available for the target domain. The majority of work in this direction assume that training and test examples consist of (x, y) pairs, where y is in a fixed shared label set Y. This assumption holds for classification and sequence labeling, but not for zero-shot entity linking, since the source and target domains have disjoint labels.</p><p>Most state-of-the-art methods learn non-linear shared representations of source and target domain instances, through denoising training objectives <ref type="bibr" target="#b5">(Eisenstein, 2018)</ref>. In Section 5, we overviewed such work and proposed an improved domain adaptive pre-training method.</p><p>Adversarial training methods <ref type="bibr" target="#b7">(Ganin et al., 2016)</ref>, which have also been applied to tasks where the space Y is not shared between source and target domains <ref type="bibr" target="#b3">(Cohen et al., 2018)</ref>, and multisource domain adaptation methods <ref type="bibr" target="#b25">(Zhao et al., 2018;</ref><ref type="bibr" target="#b9">Guo et al., 2018)</ref> are complementary to our work and can contribute to higher performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We introduce a new task for zero-shot entity linking, and construct a multi-world dataset for it. The dataset can be used as a shared benchmark for entity linking research focused on specialized domains where labeled mentions are not available, and entities are defined through descriptions alone. A strong baseline is proposed by combining powerful neural reading comprehension with domainadaptive pre-training.</p><p>Future variations of the task could incorporate NIL recognition and mention detection (instead of mention boundaries being provided). The candidate generation phase leaves significant room for improvement. We also expect models that jointly resolve mentions in a document would perform better than resolving them in isolation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Zero-shot entity linking. Multiple training and test domains (worlds) are shown. The task has two key properties: (1) It is zero-shot, as no mentions have been observed for any of the test world entities during training. (2) Only textual (non-structured) information is available.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Left: (a) Impact of using Domain Adaptive Pre-training. We fine-tune all the models on the source labeled data after pretraining. Right: (b) Relationship between MLM (Masked LM) accuracy of pre-trained model and Entity-Linking performance of the fine-tuned model, evaluated on target domains. Adding domain adaptive pre-training improves both MLM accuracy as well as the entity linking performance. Note: src represents the union of all 8 training worlds and we adapt to one tgt world at a time. The target worlds are W 1 tgt : Coronation street, W 2 tgt : Muppets, W 3 tgt : Ice hockey, W 4 tgt : Elder scrolls. † We refer to Glorot et al. (2011) for the idea of training a denoising autoencoder on source and target data together rather than the actual implementation. See text for more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Mentions m from M W</figDesc><table><row><cell>Task</cell><cell>In-Domain</cell><cell>Seen Entity Set Candidate Set Small</cell><cell>Statistics</cell><cell>Structured Data</cell><cell>Entity dictionary</cell></row><row><cell>Standard EL</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Cross-Domain EL</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Linking to Any DB</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>tion, entity pairs from one or more source worlds W 1 src . . . W n src for training. At test time we need to be able to label mentions in a new world W tgt . Note that the entity sets E W 1 src , . . . , E W n src , E Wtgt are disjoint. See Figure 1 for an illustration of several training and test worlds.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Zero-shot entity linking dataset based on Wikia.</figDesc><table><row><cell>World</cell><cell>Entities</cell><cell></cell><cell>Mentions</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Train</cell><cell cols="2">Evaluation</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Seen Unseen</cell></row><row><cell></cell><cell>Training</cell><cell></cell><cell></cell><cell></cell></row><row><cell>American Football</cell><cell>31929</cell><cell>3898</cell><cell>410</cell><cell>333</cell></row><row><cell>Doctor Who</cell><cell>40281</cell><cell>8334</cell><cell>819</cell><cell>702</cell></row><row><cell>Fallout</cell><cell>16992</cell><cell>3286</cell><cell>337</cell><cell>256</cell></row><row><cell>Final Fantasy</cell><cell>14044</cell><cell>6041</cell><cell>629</cell><cell>527</cell></row><row><cell>Military</cell><cell cols="3">104520 13063 1356</cell><cell>1408</cell></row><row><cell>Pro Wrestling</cell><cell>10133</cell><cell>1392</cell><cell>151</cell><cell>111</cell></row><row><cell>StarWars</cell><cell cols="3">87056 11824 1143</cell><cell>1563</cell></row><row><cell>World of Warcraft</cell><cell>27677</cell><cell>1437</cell><cell>155</cell><cell>100</cell></row><row><cell></cell><cell cols="2">Validation</cell><cell></cell><cell></cell></row><row><cell>Coronation Street</cell><cell>17809</cell><cell>0</cell><cell>0</cell><cell>1464</cell></row><row><cell>Muppets</cell><cell>21344</cell><cell>0</cell><cell>0</cell><cell>2028</cell></row><row><cell>Ice Hockey</cell><cell>28684</cell><cell>0</cell><cell>0</cell><cell>2233</cell></row><row><cell>Elder Scrolls</cell><cell>21712</cell><cell>0</cell><cell>0</cell><cell>4275</cell></row><row><cell></cell><cell>Test</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Forgotten Realms</cell><cell>15603</cell><cell>0</cell><cell>0</cell><cell>1200</cell></row><row><cell>Lego</cell><cell>10076</cell><cell>0</cell><cell>0</cell><cell>1199</cell></row><row><cell>Star Trek</cell><cell>34430</cell><cell>0</cell><cell>0</cell><cell>4227</cell></row><row><cell>YuGiOh</cell><cell>10031</cell><cell>0</cell><cell>0</cell><cell>3374</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Mention She told ray that Dickie and Audrey had met up again and tried to give their marriage another go . . . I don't want to see her face again . . . "</figDesc><table><row><cell cols="2">Coronation Street</cell></row><row><cell>Dickie</cell><cell>Richard "Dickie" Fleming lived</cell></row><row><cell>Fleming</cell><cell>in coronation street with his wife</cell></row><row><cell></cell><cell>Audrey from 1968 to 1970.</cell></row><row><cell>Audrey</cell><cell>Audrey Fleming (neé bright) was</cell></row><row><cell>Fleming</cell><cell>a resident of 3 coronation street</cell></row><row><cell></cell><cell>from 1968 to 1970 . Audrey mar-</cell></row><row><cell></cell><cell>ried Dickie Fleming . . .</cell></row><row><cell>Zeedan</cell><cell>Zeedan Nazir is the son of the</cell></row><row><cell>Nazir</cell><cell>Late Kal and Jamila Nazir . . .</cell></row><row><cell></cell><cell>Star Wars</cell></row><row><cell cols="2">Mention The droid acted as Moff Kilran's representative</cell></row><row><cell cols="2">on board the Black Talon, an Imperial trans-</cell></row><row><cell>port ship.</cell><cell></cell></row><row><cell>Gage-</cell><cell>The Gage-class transport was a</cell></row><row><cell>class</cell><cell>transport design used by the re-</cell></row><row><cell>transport</cell><cell>constituted Sith Empire of the</cell></row><row><cell></cell><cell>Great Galactic War.</cell></row><row><cell>Imperial</cell><cell>The Kuat Drive Yards Imperial</cell></row><row><cell>Armored</cell><cell>Armored Transport was fifty me-</cell></row><row><cell>Transport</cell><cell>ters long and carried ten crewmen</cell></row><row><cell></cell><cell>and twenty soldiers.</cell></row><row><cell>M-class</cell><cell>The M-class Imperial Attack</cell></row><row><cell>Imperial</cell><cell>Transport was a type of starship</cell></row><row><cell>Attack</cell><cell>which saw service in the Imperial</cell></row><row><cell>Transport</cell><cell>Military during the Galactic War.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Example mention and entity candidates from Coronation Street and Star Wars. Note that the language usage is very different across different Worlds.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Baseline results for Zero-shot Entity Linking. Averaged normalized Entity-Linking accuracy on all validation domains. U src+tgt refers to masked language model pre-training on unlabeled data from training and validation worlds.</figDesc><table><row><cell>Model</cell><cell cols="2">Resources Avg Acc</cell></row><row><cell>Edit-distance</cell><cell>∅</cell><cell>16.49</cell></row><row><cell>TF-IDF 8</cell><cell>∅</cell><cell>26.06</cell></row><row><cell>Ganea and Hofmann (2017)</cell><cell>GloVe</cell><cell>26.96</cell></row><row><cell>Gupta et al. (2017)</cell><cell>GloVe</cell><cell>27.03</cell></row><row><cell>Full-Transformer</cell><cell>∅</cell><cell>19.17</cell></row><row><cell>Full-Transformer (Pre-trained)</cell><cell>Usrc</cell><cell>66.55</cell></row><row><cell>Full-Transformer (Pre-trained)</cell><cell>Utgt</cell><cell>67.87</cell></row><row><cell>Full-Transformer (Pre-trained)</cell><cell>Usrc+tgt</cell><cell>67.91</cell></row><row><cell>Pool-Transformer (Pre-trained)</cell><cell>UWB</cell><cell>57.61</cell></row><row><cell>Cand-Pool-Trans. (Pre-trained)</cell><cell>UWB</cell><cell>52.62</cell></row><row><cell>Full-Transformer (Pre-trained)</cell><cell>UWB</cell><cell>76.06</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Table 5 compares entity linking performance for different entity splits. Seen entities from the training worlds are unsurprisingly the easiest to link to. For unseen entities from the training world, we observe a Performance of the Full-Transformer (U WB ) model evaluated on seen and unseen entities from the training and validation worlds.</figDesc><table><row><cell>Evaluation</cell><cell>Accuracy</cell></row><row><cell>Training worlds, seen</cell><cell>87.74</cell></row><row><cell>Training worlds, unseen</cell><cell>82.96</cell></row><row><cell>Validation worlds, unseen</cell><cell>76.06</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">Existing datasets are either unsuitable or would have to be artificially partitioned to construct a dataset for this task.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">https://www.wikia.com.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">We also experimented with using the mention+context text but this variant performs substantially worse.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3">http://lucene.apache.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4">In many works, the learned representations are kept fixed and only higher layers are updated.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5">We use the notation Ux interchangeably to mean both the unsupervised data x and the strategy to pre-train on x.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_6">We use Masked LM and Transformer encoder, which are more powerful than the instantiation in<ref type="bibr" target="#b8">(Glorot et al., 2011)</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Zero-shot Entity Linking</head><p>We first review standard entity linking task definitions and discuss assumptions made by prior systems. We then define the zero-shot entity linking task and discuss its relationship to prior work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Rahul Gupta and William Cohen for providing detailed helpful feedback on an earlier draft of this paper. We thank the Google AI Language Team for valuable suggestions and feedback.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Examining model errors and predictions</head><p>In tables 8, 9, 10, 11 we show some example mentions and model predictions. For each instance, the examples show the correct gold entity and the top-5 predictions from the model. Examples show 32 token contexts centered around mentions and the first 32 tokens of candidate entity documents.   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Using encyclopedic knowledge for named entity disambiguation</title>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marius</forename><surname>Pasca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th Conference of the European Chapter</title>
				<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Knowledge-based word sense disambiguation using topic models</title>
		<author>
			<persName><forename type="first">Devendra</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaplot</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</title>
				<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Marginalized denoising autoencoders for domain adaptation</title>
		<author>
			<persName><forename type="first">Minmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhixiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning</title>
				<meeting>the 29th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cross domain regularization for neural ranking models using adversarial learning</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katja</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 41st International ACM SIGIR Conference on Research</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Development in Information Retrieval</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
				<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<title level="m">Natural Language Processing</title>
				<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep joint entity disambiguation with local neural attention</title>
		<author>
			<persName><forename type="first">Octavian-</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Eugen</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francois</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning</title>
				<meeting>the 28th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-source domain adaptation with mixture of experts</title>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darsh</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Entity linking via joint encoding of types, descriptions, and context</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Jointly predicting predicates and arguments in neural semantic role labeling</title>
		<author>
			<persName><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04787</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving entity linking by modeling latent relations between mentions</title>
		<author>
			<persName><forename type="first">Phong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Design challenges for entity linking</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Transactions of the Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to link with wikipedia</title>
		<author>
			<persName><forename type="first">David</forename><surname>Milne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<idno type="DOI">10.1145/1458082.1458150</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM Conference on Information and Knowledge Management</title>
				<meeting>the 17th ACM Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hierarchical losses and new resources for fine-grained entity typing and linking</title>
		<author>
			<persName><forename type="first">Shikhar</forename><surname>Murty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irena</forename><surname>Radovanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter</title>
				<meeting>the 2018 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Improving language understanding with unsupervised learning</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>OpenAI</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Wikification and beyond: The challenges of entity and concept grounding</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Cassidy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Tutorials</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-lingual entity discovery and linking</title>
		<author>
			<persName><forename type="first">Avi</forename><surname>Sil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silviu-Petru</forename><surname>Cucerzan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Tutorial Abstracts</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Linking named entities to any database</title>
		<author>
			<persName><forename type="first">Avirup</forename><surname>Sil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ernest</forename><surname>Cronin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Penghai</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana-Maria</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
				<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Language and domain independent entity linking with quantified collective validation</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><forename type="middle">Guang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised multi-domain adaptation with feature embeddings</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adversarial multiple source domain adaptation</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanhang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joao</forename><forename type="middle">P</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Costeira</surname></persName>
		</author>
		<author>
			<persName><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
