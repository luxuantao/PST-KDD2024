<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Analysis of EDF Schedulability on a Multiprocessor</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2005-06-22">22 June 2005</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Theodore</forename><forename type="middle">P</forename><surname>Baker</surname></persName>
							<email>baker@cs.fsu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Florida State University</orgName>
								<address>
									<addrLine>207A Love Building, Palmetto Drive</addrLine>
									<postCode>32306-4530</postCode>
									<settlement>Tallahassee</settlement>
									<region>FL</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Analysis of EDF Schedulability on a Multiprocessor</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2005-06-22">22 June 2005</date>
						</imprint>
					</monogr>
					<idno type="MD5">7B3564DDDF4B60BB1E635409E1160111</idno>
					<note type="submission">received 23 Dec. 2003; revised 13 June 2004; accepted 30 Oct. 2004;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multiprocessor systems</term>
					<term>real-time scheduling</term>
					<term>deadline scheduling</term>
					<term>earliest deadline first</term>
					<term>utilization</term>
					<term>feasibility</term>
					<term>multiprocessor scheduling</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A new schedulability test is derived for preemptive deadline scheduling of periodic or sporadic real-time tasks on a singlequeue m-server system. The new test allows the task deadline to be more or less than the task period, and is based on a new analysis concept, called a -busy interval. This generalizes a result of Goossens et al. [11]  that a system of periodic tasks with maximum individual task utilization u max is EDF-schedulable on m processors if the total utilization does not exceed mð1 À u max Þ þ u max . The new test allows the analysis of hybrid EDF-US [x] scheduling, and the conclusion that EDF-US[1/2] is optimal, with a guaranteed worstcase schedulable utilization of ðm þ 1Þ=2.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>I T has long been known that preemptive earliest-deadline- first (EDF) scheduling is optimal for single-processor batch-oriented systems, in the sense that a set of independent jobs with deadlines is feasible if and only if it is EDFschedulable. Liu and Layland <ref type="bibr" target="#b15">[16]</ref> extended this result to systems of independent periodic tasks for which the relative deadline of each task is equal to its period, proving that so long as the total processing demand does not exceed 100 percent of the system capacity EDF will not permit any missed deadlines. Besides showing that that EDF scheduling is optimal for such task systems, this utilization bound provides a simple and effective schedulability test.</p><p>It also well-known that the optimality of EDF scheduling breaks down on multiprocessor systems <ref type="bibr" target="#b16">[17]</ref>. For example, consider a batch of jobs J 1 ; . . . ; J mþ1 with deadlines</p><formula xml:id="formula_0">d 1 ¼ Á Á Á ¼ d m ¼ mx, d mþ1 ¼ mx þ 1, and computation times c 1 ¼ Á Á Á ¼ c m ¼ 1, c mþ1 ¼ mx þ 1,</formula><p>where x is an arbitrary constant, all released at time zero. If job J mþ1 is allowed to monopolize one processor, all the tasks can be completed by their deadlines using m processors. However, an EDF scheduler for m processors would schedule jobs J 1 . . . J m ahead of J mþ1 , causing J mþ1 to miss its deadline.</p><p>Dhall and Liu <ref type="bibr" target="#b8">[9]</ref> showed that the effect illustrated above means one cannot guarantee EDF schedulability on m processors for a utilization any higher than can be guaranteed on a single processor. It is tempting to conclude from Dhall and Liu's example that there is no useful utilization bound test for EDF scheduling, or that EDF is not a good real-time scheduling policy for multiprocessor systems. However, neither conclusion is actually justified.</p><p>Dhall and Liu's result depends on two extreme kinds of tasks: "heavy" ones, with a high ratio of computation time to deadline, and "light" ones, with a low ratio of computation time to deadline. It is the mixing of those two kinds of tasks that causes a problem for EDF. A modified EDF policy that segregates the heavy tasks from the light tasks, on disjoint sets of CPUs, would have no problem with this example. Examination of further examples leads one to conjecture that such a segregated EDF scheduling policy would not miss any deadlines until a very high level of CPU utilization is achieved, and may even permit the use of simple utilization-based schedulability tests.</p><p>Perhaps motivated by reasoning similar to that above, Srinivasan and Baruah <ref type="bibr" target="#b18">[19]</ref> examined the deadline-based scheduling of periodic tasks on multiprocessors, and showed that any system of independent periodic tasks for which the utilization of every individual task is at most m=ð2m À 1Þ can be scheduled successfully on m processors if the total utilization is at most m 2 =ð2m À 1Þ. They then proposed a hybrid scheduling algorithm, called EDF-US[m=ð2m À 1Þ], that gives higher priority to tasks with utilizations above m=ð2m À 1Þ, and showed that EDF-US[m=ð2m À 1Þ] is able to successfully schedule any set of independent periodic tasks with total utilization up to m 2 =ð2m À 1Þ. These results were generalized in <ref type="bibr" target="#b10">[11]</ref>, where it is shown that a system of independent periodic tasks can be scheduled successfully on m processors by EDF scheduling if the total utilization is at most mð1 À u max Þ þ u max , where u max is the maximum utilization of any individual task.</p><p>The above cited results were derived indirectly, using a theorem relating EDF schedulability on sets of processors of different speeds by Phillips et al. <ref type="bibr" target="#b17">[18]</ref>. We have approached the problem more directly, and obtained a different and more general schedulability condition, of which the above cited results turn out to be special cases.</p><p>The rest of this paper presents the derivation of this more general multiprocessor EDF schedulability condition. Section 2 defines the problem formally, and outlines the overall approach. Section 3 derives a lower bound on the workload contributions of competing tasks in an interval where a task misses a deadline. Section 4 defines the notion of -busy interval and derives an upper bound on the workload contributions of competing tasks in any such interval. Section 5 combines the upper and lower bounds on workload to obtain schedulability tests. Section 6 reviews in more detail the prior work mentioned above, shows how it relates to the new schedulability tests, and derives a new result regarding the optimality of EDF-US[1/2] scheduling. Section 7 summarizes and concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">DEFINITION OF THE PROBLEM</head><p>Suppose one is given a set of N simple independent periodic tasks 1 ; . . . ; N , where each task i has minimum interrelease time (called period for short) T i , worst-case computation time c i , and relative deadline d i , where c i d i and c i T i . Each task generates a sequence of jobs, each of whose release time is separated from that of its predecessor by at least T i . (No special assumptions are made about the first release time of each task.) Time is represented by rational numbers. Square brackets and parentheses are used to distinguish whether time intervals include their endpoints. For example, the time interval ½t 1 ; t 2 Þ contains the time values greater than or equal to t 1 and less than t 2 . All of the intervals ½t 1 ; t 2 , ½t 1 ; t 2 Þ, ðt 1 ; t 2 , and ðt 1 ; t 2 Þ are said to be of length t 2 À t 1 .</p><p>Note that what we call a periodic task here is sometimes called a sporadic task. In this regard, we follow Liu <ref type="bibr" target="#b16">[17]</ref>, who observed that defining periodic tasks to have interrelease times exactly equal to the period "has led to the common misconception that scheduling and validation algorithms based on the periodic task model are applicable only when every periodic task is truly periodic...in fact, most existing results remain correct as long as interrelease times of jobs in each task are bounded from below by the period of the task."</p><p>In this paper, we assume that the jobs of a set of periodic tasks are scheduled preemptively according to an EDF policy on m processors, with dynamic processor assignment. That is, whenever there are m or fewer jobs ready they will all be executing, and whenever there are more than m jobs ready there will be m jobs executing, all with deadlines earlier than or equal to the deadlines of the jobs that are not executing.</p><p>Our objective is to formulate a simple test for schedulability, expressed in terms of the periods, deadlines, and worst-case computation times of the tasks, such that if the test is passed no deadlines will be missed. We approach the problem by analyzing the minimum processor demand that is needed over an interval of time to cause a missed deadline.</p><p>Definition 1 (demand). The demand of a time interval is the total amount of computation that would need to be completed within the interval for all the deadlines within the interval to be met.</p><p>Definition 2 (load). The load of an interval is W =Á, where W is the demand of the interval and Á is the length of the interval.</p><p>Definition 3 (first missed deadline). Consider any sequence of job release times and computation times that are consistent with the interrelease and worst-case computation time constraints and produce a schedule with a missed deadline. The first point in such a schedule at which a deadline is missed is a first missed deadline. Since T i is only a lower bound on interrelease times and c i is only an upper bound on execution times, and since the first start time of each task is also variable, there are infinitely many possible schedules. Each first missed deadline is understood to occur within the context of one of these schedules.</p><p>If we can find a lower bound on the processor load over an interval leading up to every first missed deadline, and we can guarantee that a given set of tasks could not possibly generate so much load in such an interval, that would be sufficient to serve as a proof of schedulability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LOWER BOUND ON LOAD</head><p>One can establish a lower bound on the load of the interval between a first missed deadline and the release time of the corresponding job by observing that, since the job does not complete by the end of the interval, the lengths of all the subintervals in which the job that misses its deadline does not execute must exceed its slack time. This fact is well known, and is used by Phillips et al. in <ref type="bibr" target="#b17">[18]</ref>. It is illustrated Fig. <ref type="figure" target="#fig_1">1</ref>, for the case where m ¼ 3 and d k T k . The diagonally shaded rectangles indicate blocks of time during which k executes. The dotted rectangles indicate times during which all m processors must be busy executing other jobs that contribute to the demand for this interval. It is easy to see that the total demand of the interval ½t À d k ; tÞ must be at least x þ mðd k À xÞ, where x c k is the amount of time that the the job of k that misses its deadline executes in the interval.</p><p>To allow for the possibility that d k ! T k , we need to extend the reasoning above to intervals that may include more than one job of the task that misses a deadline. Fig. <ref type="figure" target="#fig_2">2</ref> shows the release times and deadlines of two such jobs k;1 and k;2 . The first job of k is delayed by two blocks of higher priority interference. This interference is not enough to cause this job to miss its deadline, but (because jobs of the same task must be executed sequentially) it delays the start of the next job of k enough to cause that job to miss its deadline.</p><p>Definition 4 (backlogged). A job is backlogged at a time t if the task has a job that is released before time t and has nonzero execution time remaining at t. Definition 5 ( k -busy interval). For any task k , a time interval ½t 0 ; tÞ is k -busy if there are backlogged jobs of k continually throughout the interval ðt 0 ; tÞ.</p><p>Lemma 1. If a task k is backlogged at time t then there is a unique maximal k -busy interval ½t À Á; tÞ, and there are no backlogged jobs of k at time t À Á.</p><p>Proof. Let t 0 be the release time of a job of k that is backlogged at time t. The interval ½t 0 ; tÞ is k -busy. If there is another job of k that is backlogged at time t 0 , the k -busy interval can be extended downward. However, there is a limit to such extensions, since the system has some start time, before which no task is released. The set of all k -busy intervals with endpoint t is totally ordered by length. Therefore, it has a unique maximal element, ½t À Á; tÞ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>t u</head><p>Note that if d k T k the maximal k -busy interval cannot be longer than d k , but if d k &gt; T k the interval may be arbitrarily longer than d k . For example, consider a system of m þ 1 tasks, such that</p><formula xml:id="formula_1">T 1 ¼ Á Á Á ¼ T mþ1 ¼ d 1 ¼ Á Á Á ¼ d m ¼ 1, d mþ1 ¼ 2, c 1 ¼ Á Á Á ¼ c m ¼ , and c mþ1 ¼ 1.</formula><p>The first m tasks create a block of interference of duration for every release of mþ1 , so that each successsive job of mþ1 completes later by . The first job of task mþ1 to miss its deadline will be the jth job, where j is the least integer greater than 1  . It will miss its deadline at time</p><formula xml:id="formula_2">jT mþ1 þ d mþ1 ¼ j þ 1, since j &gt; d mþ1 À T mþ1 ¼ 1.</formula><p>We can make j arbitrarily large by choosing small enough.</p><p>Lemma 2 (lower bound on load). If W is the demand of a maximal k -busy interval ½t À Á; tÞ, then</p><formula xml:id="formula_3">W Á &gt; m À ðm À 1Þ c k minfd k ; T k g :</formula><p>Proof. Let x be the amount of time k executes in the interval ½t À Á; tÞ. Since there are no backlogged jobs of k at the start of the interval, the demand of k in the interval consists only of jobs that are released in the interval. Let j ! 1 be the number of such jobs that have deadlines on or before t. The demand of k over the interval is jc k . Since k is backlogged continually over the interval, we know that x &lt; jc k . Since the j jobs are all released on or after t À Á, have deadlines on or before t À d k , and have release times separated by at least T k , we know that</p><formula xml:id="formula_4">ðj À 1ÞT k þ d k Á j Á þ T k À d k T k x &lt; jc k c k T k ðÁ þ T k À d k Þ:</formula><p>Since a processor is never idle while a job is waiting to execute, during every subinterval in which k is not executing all m processors are busy executing jobs of other tasks. Let y be the sum of the lengths of all the subintervals for which all m processors are executing jobs of tasks other than k . The entire demand of the interval ½t À Á; tÞ must be at least x þ my. It follows that</p><formula xml:id="formula_5">W ! x þ my ¼ x þ mðÁ À xÞ ¼ mÁ À xðm À 1Þ &gt; mÁ À ðm À 1Þ c k T k ðÁ þ T k À d k Þ:</formula><p>Dividing both sides of the above inequality by Á, we obtain</p><formula xml:id="formula_6">W Á &gt; m À ðm À 1Þ c k T k 1 þ T k À d k Á : If d k T k , we can use the fact that Á &gt; d k and obtain W Á &gt; m À ðm À 1Þ c k d k :</formula><p>Otherwise, if d k &gt; T k , we can take the limit as Á ! 1 and obtain</p><formula xml:id="formula_7">W Á &gt; m À ðm À 1Þ c k T k : u t</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">UPPER BOUND ON LOAD</head><p>We now derive an upper bound on the load W Á of an interval leading up to a first missed deadline. If we can find such an upper bound , it will follow that schedulability of a task system can be guaranteed by checking that is less than the minimum load needed to cause a missed deadline. The upper bound will be defined as the sum of individual upper bounds k ðiÞ on the load contribution W i =Á for each individual task in the interval. The contribution W i of each task i to the demand of such an interval consists of two parts:</p><p>1. The computation time of jobs of i that are released within the interval. 2. The computation time of jobs of i that are released before the start of the interval, which we call the carried-in contribution of i . To bound the size of the carried-in contribution of i , we extend the notion of k -busy interval as follows:</p><p>Definition 6 (-busy interval). A time interval is -busy if its total load is at least . A downward extension of a -busy interval is the interval itself or a -busy interval that has an earlier starting point and shares the same endpoint. A proper downward extension of a -busy interval is a downward extension that has an earlier starting point. A maximal -busy downward extension of a -busy interval is a downward extension that has no further proper downward extensions.</p><p>Lemma 3. Any k -busy interval ½t À Á; tÞ has a unique maximal -busy downward extension ½t À Á Á; tÞ for any m À ðm À 1Þ</p><formula xml:id="formula_8">c k minfd k ;T k g . Proof.</formula><p>The proof is analogous to that of Lemma 1. Let ½t À Á; tÞ be any k -busy interval. By Lemma 2, this interval is -busy, so the set of -busy downward extensions of ½t À Á; tÞ is nonempty. The system has some start time, before which no task is released, so the length of all -busy downward extensions of any such interval is bounded. Any set of intervals with a common endpoint is totally ordered by length. Therefore, the set of -busy downward extensions of ½t À Á; tÞ has a unique maximal element.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>t u</head><p>Definition 7 (maximal -busy interval). For any first missed deadline t of a system, if k is a task that misses a deadline at time t and m À ðm À 1Þ ck minfdk;Tkg , then Lemma 1 guarantees that there is a unique maximal k -busy interval ½t À Á; tÞ, and Lemma 3 guarantees that it has a unique maximal -busy downward extension ½t À Á Á; tÞ, such that Á Á ! Á ! d k . We call the interval ½t À Á Á; tÞ the maximal -busy interval of the first missed deadline t.</p><p>We next look for an upper bound on the contribution W i of each task i to the demand W of a maximal -busy interval. The analysis is divided into cases, according to whether i makes any carried-in contribution.</p><p>Case 1. If there is no carried-in contribution, the analysis is simple. No jobs released prior to t À Á Á can contribute to W i . Therefore, an upper bound on W i can be obtained by multiplying the worst-case computation time of i by the maximum number of jobs of i that may be released in an interval of length Á Á À d i .</p><formula xml:id="formula_9">W i max 0; c i Á Á À d i T i &amp; ' ( ) max 0; c i T i ð Á Á þ T i À d i Þ &amp; ' : Since Á Á ! d k , it follows that W i Á Á &lt; c i T i 1 þ T i À d i d k if d i T i ; and<label>ð1Þ</label></formula><formula xml:id="formula_10">W i Á Á &lt; c i T i if d i &gt; T i :<label>ð2Þ</label></formula><p>It now only remains to analyze the case where there is a carried-in contribution. Therefore, from this point on, we assume there is at least one job of i backlogged at time t À Á Á.</p><p>Definition 8 (preamble). By Lemma 1, if ½t À Á Á; tÞ is a maximal -busy interval and there is a job of i backlogged at time t À Á Á, then there is a unique maximal i -busy interval ½t À Á Á À ; t À Á ÁÞ, for some &gt; 0. We call this interval the preamble with respect to i of the maximal -busy interval ½t À Á Á; tÞ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>It follows that:</head><p>1. There are no backlogged jobs of i at time t À Á Á À . 2. A job of i is released at time t À Á Á À . 3. At every point in the interval ðt À Á Á À ; t À Á ÁÞ, there is at least one backlogged job of i .</p><p>Lemma 4 (upper bound on carry-in). If ½t À Á Á À ; t À Á ÁÞ is the preamble of ½t À Á Á; tÞ with respect to task i , then the amount x of computation that the task i completes in the preamble is greater than , where ¼ mÀ mÀ1 . Proof. The reasoning is like that of Lemma 2. Let x be the total amount of time spent executing jobs of i in the preamble and let y be the sum of the lengths of all the subintervals within the preamble where all m processors are simultaneously executing jobs that preempt i . Since i can execute when and only when there are less than m processors executing jobs that preempt i , we know that y ¼ À x. It follows that the total amount of work executed in the preamble on i must be at least my þ x. Putting this together with the fact that the interval ½t À Á Á; tÞ is -busy, one can conclude that the load of the combined interval ½t À Á Á À ; tÞ is at least Á Á þ my þ x. Since ½t À Á Á; tÞ has no proper -busy downward extension, the demand of the interval ½t À Á Á À ; tÞ is less than ð þ Á ÁÞ. It follows that</p><formula xml:id="formula_11">Á Á þ my þ x &lt; ð þ Á ÁÞ my þ x &lt; mð À xÞ þ x &lt; ðm À ðm À 1ÞÞ m À xðm À 1Þ &lt; m À ðm À 1Þ xðm À 1Þ &gt; ðm À<label>1Þ</label></formula><p>x &gt; : u t</p><p>Let j be the number of jobs of i released in the preamble. Since, by Lemma 4, i executes for at least time in the preamble, the carried-in contribution of i to W i is less than jc i À .</p><p>The noncarried-in portion of W i comes from jobs released in the interval ½t À Á Á À þ jT i ; t À d i . This contribution is bounded by c i d Á ÁþÀjT i Àd i Ti e. (We are assuming i makes a nonzero carried-in contribution, so this quantity must be positive.) Combining the upper bounds on the carried-in and noncarried-in contributions to W i , we have</p><formula xml:id="formula_12">W i &lt; jc i À þ c i Á Á þ À jT i À d i T i &amp; ' ¼ c i Á Á þ À d i T i &amp; ' À c i Á Á þ À d i T i þ 1 ! À :</formula><p>That is,</p><formula xml:id="formula_13">W i &lt; c i T i ð Á Á þ T i À d i Þ þ c i T i À :<label>ð3Þ</label></formula><p>We will consider separately the case where ci Ti and the case where ci Ti &gt; . Case 2.1.</p><formula xml:id="formula_14">If c i T i</formula><p>, the value of the expression on the right of ( <ref type="formula" target="#formula_13">3</ref>) is nonincreasing with respect to , and since ! 0, the global maximum is achieved when ¼ 0. It follows that the upper bounds ( <ref type="formula" target="#formula_9">1</ref>) and ( <ref type="formula" target="#formula_10">2</ref>) for W i Á Á derived in Case 1 also apply in this case.</p><p>Case 2.2. If c i T i &gt; , the value of the expression on the right of ( <ref type="formula" target="#formula_13">3</ref>) is increasing with respect to . We will consider separately the case where d i T i and the case where d i &gt; T i .</p><p>Case 2.2.1. If d i T i , each job of i must complete by the release time of the next job. It follows that there can be no i -busy interval of length longer than d i , and so &lt; d i .</p><p>Using this fact and that the expression on the right of ( <ref type="formula" target="#formula_13">3</ref>) is increasing with respect to , we have</p><formula xml:id="formula_15">W i &lt; c i T i ð Á Á þ T i À d i Þ þ d i c i T i À : Since c i À T i &gt; 0 and Á Á ! d k , it follows that W i Á Á &lt; c i T i 1 þ T i d k À d i d k :<label>ð4Þ</label></formula><p>Observe that the bound above is greater than or equal to the corresponding bound (1) derived for Case 1.</p><p>Case 2.2.2. If d i &gt; T i , then may be arbitrarily large, and so inequality (3) is not useful; we need to find another way to bound W i . Let be the offset from the start of the maximal i -busy interval ½t À Á Á; tÞ of the release time of the first carried-in job of i . That is, t À Á Á À is the first time before t À Á Á where a job of i is released that is still backlogged at time t À Á Á. All the jobs of i that contribute to the demand of ½t À Á Á; tÞ must be released within the interval ½t À Á Á À ; t À d i . The maximum number of jobs of i that can be released in an interval of this length an have a deadline in the interval is</p><formula xml:id="formula_16">Á Á þ À d i T i &amp; ' :<label>ð5Þ</label></formula><p>The job of i released at time t À Á Á À is still backlogged at time t À Á Á, but since the first missed deadline comes later, at time t, this job must complete by its deadline, which is</p><formula xml:id="formula_17">t À Á Á À þ d i . It follows that &lt; d i .</formula><p>Putting this upper bound on together with the upper bound (5) on the number of jobs that contribute to W i , and multiplying by the worst-case computation time of i , we obtain</p><formula xml:id="formula_18">W i c i Á Á À d i þ T i &amp; ' &lt; c i Á Á T i &amp; ' c i T i ð Á Á þ T i Þ: Since Á Á &gt; d k , it follows that W i Á Á &lt; c i T i 1 þ T i d k :<label>ð6Þ</label></formula><p>Observe that the bound above is greater than or equal to the corresponding bound (2) derived for Case 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 5 (upper bounds on load). If ½tÀ Á</head><p>Á; tÞ is a maximal -busy interval for task k , then the contribution Wi Á Á of each task i to the demand of the interval is strictly bounded above by the function k ðiÞ defined by Table <ref type="table">1</ref>, where ¼ mÀ mÀ1 . Proof. We showed in the analysis for Cases 1 and 2.1 above that if ci Ti and d i T i , then</p><formula xml:id="formula_19">W i Á Á &lt; c i T i 1 þ T i À d i d k ;</formula><p>and if c i T i and d i &gt; T i , then</p><formula xml:id="formula_20">W i Á Á &lt; c i T i :</formula><p>We showed in the analysis for Cases 1 and 2.2 above that if c i T i &gt; and d i T i , then</p><formula xml:id="formula_21">W i Á Á &lt; c i T i 1 þ T i d k À d i d k ;</formula><p>and if ci Ti &gt; and d i &gt; T i , then</p><formula xml:id="formula_22">W i Á Á &lt; c i T i 1 þ T i d k : u t</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">SCHEDULABILITY CONDITION</head><p>Using the above analysis, we can now prove the following theorem, which provides a sufficient condition for EDF schedulability.</p><p>Theorem 1 (EDF schedulability test). A set of periodic tasks 1 ; . . . ; N is schedulable on m processors using preemptive EDF scheduling if, for every task k , there exists a positive value m À ðm À 1Þ</p><formula xml:id="formula_23">c k minfd k ;T k g such that X N i¼1 k ðiÞ ;<label>ð7Þ</label></formula><p>where ¼ mÀ mÀ1 and k ðiÞ is as defined in Table <ref type="table">1</ref>.</p><p>Proof. The proof is by contradiction. Suppose some task misses a deadline. Let k be any first task to miss a deadline. (If more than one misses a deadline together at the same time, choose any one of them.) Let ½t À Á Á; tÞ be the maximal -busy interval guaranteed by Lemma 1 and Lemma 3. Since ðt À Á; tÞ is -busy, we have</p><formula xml:id="formula_24">W Á Á &gt; :</formula><p>By Lemma 5, Wi Á Á &lt; k ðiÞ, for i ¼ 1; . . . ; N, and so</p><formula xml:id="formula_25">X N i¼1 k ðiÞ &gt; W Á Á &gt; :</formula><p>The above is a contradiction of <ref type="bibr" target="#b6">(7)</ref>.</p><formula xml:id="formula_26">t u</formula><p>To get the most accuracy from the above condition as a schedulability test, it might seem necessary to consider all possible values of for each k. However, the only values of that need to be considered are the upper bound and the points at which k ðiÞ is discontinuous with respect to the implicit parameter . That is, at the points</p><formula xml:id="formula_27">i ¼ m À c i T i ðm À<label>1Þ</label></formula><p>for i ¼ 1; . . . ; k, and</p><formula xml:id="formula_28">max ¼ m À c k minfd k ; t k g ðm À 1Þ:</formula><p>The schedulability test above must be checked individually for each task k . If we are willing to sacrifice some precision, there is a simpler test that only needs to be checked once for the entire system of tasks.</p><p>Corollary 1 (simplified test). A set of periodic tasks 1 ; . . . ; N is schedulable on m processors using preemptive EDF scheduling if</p><formula xml:id="formula_29">X N i¼1 c i T i 1 þ maxf0; T i À d i g d min m À ðm À 1Þ;<label>ð8Þ</label></formula><formula xml:id="formula_30">where ¼ max c i minfd i ; T i g j i ¼ 1; . . . ; N &amp; ' and d min ¼ minfd k j i ¼ 1; . . . ; Ng.</formula><p>Proof. Corollary 1 is proved by repeating the proof of Theorem 1, adapted to fit the definition of . Let k be a first task to miss a deadline. Let 0 ¼ mÀ ðmÀ1Þ. Since ! ck minfd k ;T k g , it follows that 0 mÀðmÀ1Þ ck minfd k ;T k g . Therefore, there is a maximal 0 -busy interval ½t À Á Á; tÞ whose existence is guaranteed by Lemma 3. Since ½t À Á Á; tÞ is 0 -busy, we have</p><formula xml:id="formula_31">W Á Á &gt; m À ðm À 1Þ: By Lemma 5, Wi Á Á k ðiÞ, for i ¼ 1; . . . ; N. Since ! c i minfd i ;T i g ! c i</formula><p>T i , only the first column of the definition of k ðiÞ in Table <ref type="table">1</ref> applies, i.e.,</p><formula xml:id="formula_32">k ðiÞ ¼ c i T i 1 þ maxf0; T i À d i g d k :</formula><p>It follows that</p><formula xml:id="formula_33">X N i¼1 c i T i 1 þ maxf0; T i À d i g d min ! X N i¼1 k ðiÞ ! W Á Á &gt; m À ðm À 1Þ:</formula><p>The above contradicts <ref type="bibr" target="#b7">(8)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>t u</head><p>If we replace d i by T i in Corollary 1, we immediately obtain an independent proof of the following utilization bound test, which was first reported in <ref type="bibr" target="#b10">[11]</ref>.</p><p>Theorem 2 <ref type="bibr">(Goossens, Funk, and Baruah)</ref>. A set of periodic tasks 1 ; . . . ; N , all with deadline equal to period, and with maximum individual task utilization u max is guaranteed to be schedulable on m processors using preemptive EDF scheduling if the total system utilization U does not exceed m À ðm À 1Þu max .</p><p>The results above are useful as schedulability tests. They can be applied directly to prove that a task set will meet deadlines with EDF scheduling, either before runtime for a fixed set of tasks or during runtime as an admission test for a system with a dynamic set of tasks. With the simpler forms, one checks the schedulability condition once for the entire task set. This computation can be performed in OðnÞ time. With the more general form, one checks the schedulability condition for each task and each of n possible values . This computation can be performed in Oðn 3 Þ time. In the latter case the specific value(s) of k for which the test fails provide some indication of where the problem lies.</p><p>The schedulability test of Theorem 1 allows preperiod deadlines (as well as postperiod deadlines), but it is more complicated than the utilization bound test. One may naturally wonder whether this extra complexity gains anything over the well-known technique for handling preperiod deadlines by "padding" computation times and then using the utilization bound test. By padding the computation times, we mean that if a task k has computation time c k and deadline d k &lt; T k , we replace it (for the purpose of schedulability testing only) by 0 k , where</p><formula xml:id="formula_34">c 0 k ¼ c k þ T k À d k and d 0 k ¼ T 0 k ¼ T k .</formula><p>If 0 k will always complete within its period, then k will always complete T k À d k time units before its period. That is, with EDF scheduling, the original task k can be scheduled to meet its deadline if the following condition holds for 0 k .</p><formula xml:id="formula_35">T k À d k T k þ U mð1 À u 0 max Þ þ u 0 max ;<label>ð9Þ</label></formula><p>where</p><formula xml:id="formula_36">u 0 max ¼ max c i T i j i ¼ 1; . . . ; N &amp; ' [ c 0 i T i &amp; ' and U ¼ P N i¼1 c i T i .</formula><p>To compare the accuracy of these two tests, consider the case where just task k has a preperiod deadline, d k &lt; T k . For simplicity, consider just the value ¼ m À ðm À 1Þ, where is as in the proof of Corollary 1. For task k , k ðkÞ ¼ ck Tk þ TkÀdk Tk ck dk . For all the other tasks i , k ðiÞ ¼ ci Ti . Therefore, the test of Theorem 1 for the case of k reduces to</p><formula xml:id="formula_37">T k À d k T k c k d k þ U m 1 À c k d k þ c k d k ;<label>ð10Þ</label></formula><p>and if we assume u 0 max ¼ c k d k ! u max , the padded utilization test (9) reduces to</p><formula xml:id="formula_38">T k À d k T k þ U m 1 À c k d k þ c k d k :<label>ð11Þ</label></formula><p>It is easy to see that ( <ref type="formula" target="#formula_37">10</ref>) is more accurate than (11</p><formula xml:id="formula_39">) if c k &lt; d k .</formula><p>For example, suppose we have three processors and six tasks, with periods</p><formula xml:id="formula_40">T 1 ¼ Á Á Á ¼ T 6 ¼ 1, computation times c 1 ¼ Á Á Á ¼ c 6 ¼ 1=3, and deadlines d 1 ¼ Á Á Á ¼ d 5 ¼ 1 and d 6 ¼ 2=3. The system utilization is U ¼ 2.</formula><p>If we apply the padded utilization test to 3 , the padded computation time is c 0</p><formula xml:id="formula_41">6 ¼ 1 3 þ 1 À 2 3 À Á ¼ 2 3 , u 0 max ¼ 2 3</formula><p>, and (9) fails:</p><formula xml:id="formula_42">1 À 2 3 1 þ U &gt; 2:33 &gt; 1:667 &gt; 3 1 À 2 3 þ 2 3 :</formula><p>On the other hand, ( <ref type="formula" target="#formula_37">10</ref>) is satisfied:</p><formula xml:id="formula_43">1 À 2 3 1 Á 1 3 2 3 þ U &lt; 2:167 &lt; 2:33 &lt; 3 1 À 1 3 þ<label>1 3 :</label></formula><p>Of course, these schedulability tests are only sufficient conditions for schedulability. They are based on very conservative, worst-case assumptions about task periods and task phasing. However, the Liu and Layland nð2 1=n À 1Þ utilization bound for rate monotonic scheduling is proof that a similarly conservative test can still be very useful.</p><p>None of the schedulability tests presented here should be used when N m. In this case a system will always be schedulable up to a total utilization of m, but the schedulability tests will not recognize that. However, as soon as N ¼ m þ 1 the worst-case utilization bound becomes 1, as shown by Dhall and Liu, and the tests given here become useful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATION TO PRIOR WORK</head><p>Srinivasan and Baruah <ref type="bibr" target="#b18">[19]</ref> defined a periodic task set f 1 ; 2 ; . . . N g to be a light system on m processors if it satisfies the following properties:</p><formula xml:id="formula_44">1. P N i¼1 c i T i m 2 2mÀ1 . 2. c i T i m 2mÀ1 , for 1 i N.</formula><p>They then proved that any periodic task system that is light on m processors is scheduled to meet all deadlines on m processors by EDF.</p><p>The above analysis was refined in <ref type="bibr" target="#b10">[11]</ref>, to obtain the mð1 À u max Þ þ u max utilization bound. In the same paper this utilization bound was shown to be tight, i.e., there is no</p><formula xml:id="formula_45">utilization bound Û U &gt; mð1 À u max Þ þ u max þ</formula><p>, where &gt; 0 and u max ¼ maxfc i =T i j i ¼ 1; . . . ; Ng, for which U Û U guarantees EDF schedulability.</p><p>We have gone beyond the above cited result by covering the cases where the deadline of task is more or less than its period. Preperiod deadlines are useful for modeling tasks with bounded jitter requirements. They can also be useful for tasks that have precedence constraints, such as a first part that initiates input, and a second part that processes the resulting input. Postperiod deadlines are useful for modeling tasks that use buffers to cope with bursty inputs.</p><p>We also have provided a more direct proof of the mð1 À u max Þ þ u max utilization bound. The proof in <ref type="bibr" target="#b10">[11]</ref> is derived from a theorem in <ref type="bibr" target="#b9">[10]</ref>, on scheduling for uniform multiprocessors, which in turn is based on a multiprocessor speed-up result by Phillips et al. <ref type="bibr" target="#b17">[18]</ref>. The same utilization bound follows directly from Corollary 1.</p><p>Srinivasan and Baruah <ref type="bibr" target="#b18">[19]</ref>, proposed adapting their utilization-bound schedulability condition to situations where there are a few high-utilization tasks as follows:</p><p>Algorithm 1 ðEDF -US½m=ð2m À 1ÞÞ: (heavy task rule) If c i =T i &gt; m=ð2m À 1Þ, then schedule i 's jobs at maximum priority (i.e., as if they had a deadline long in the past). (light task rule) If c i =T i m=ð2m À 1Þ then schedule i 's jobs according to their normal deadlines.</p><p>They then proved that Algorithm EDF-US½m=ð2m À 1Þ correctly schedules on m processors any periodic task system whose utilization is at most m 2 =ð2m À 1Þ. Their proof is based on the observation that their upper bound on total utilization guarantees that the number of heavy tasks cannot exceed m. The essence of the argument is that Algorithm EDF-US½m=ð2m À 1Þ can do no worse than scheduling each of the heavy tasks on its own processor, and then scheduling the remainder (which must must be light on the remaining processors) using EDF. The EDF-US½m=ð2m À 1Þ idea was further studied by <ref type="bibr" target="#b0">[1]</ref>, in the context of a variable task set, where the number of heavy tasks may vary over time. The mð1 À u max Þ þ u max utilization bound naturally suggests the following generalization of the EDF-US idea, for an arbitrary single-task utilization cut-off .</p><p>Algorithm 2 ðEDF -US½Þ : (heavy task rule) If c i =T i &gt; , then schedule i 's jobs at maximum priority. (light task rule) If c i =T i , then schedule i 's jobs according to their normal deadlines. Theorem 3. Given a set of N periodic tasks, let k be the larger of m À 1 or the number of tasks with utilization higher than , i.e., k ¼ maxfm À 1; jfi j u i &gt; gjg.</p><p>Algorithm EDF-US½ correctly schedules the task set on m processors if the combined utilization of the N À k lightest tasks is at most ðm</p><formula xml:id="formula_46">À kÞð1 À Þ þ .</formula><p>Proof. As argued by Srinivasan and Baruah, if k is the number of heavy tasks (and k m À 1), the performance of this algorithm cannot be worse than an algorithm that dedicates one processor to each of the k heavy tasks, and schedules the remaining N À k tasks on the m À k remaining processors. Theorem 2 guarantees the remaining N À k tasks can be scheduled on the remaining m À k processors since their combined utilization is at most ðm</p><formula xml:id="formula_47">À kÞð1 À Þ þ .</formula><p>The remaining case is where there are more than k heavy tasks (and k ¼ m À 1). In this case, the performance of this algorithm cannot be worst than an algorithm that dedicates one processor to each of the k heaviest tasks and schedules the remaining N À k tasks on the remaining single processor. Since the combined utilization of the N À k lightest tasks is at most ðm À kÞð1 À Þ þ ¼ 1, they can all be scheduled on one processor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>t u</head><p>Observe that if a system is not guaranteed schedulable by Theorem 3, the total utilization must be greater than k þ ðm À kÞ À ððm À kÞ À 1Þ. Minimizing this expression with respect to k and maximizing with respect to , we can see that the maximum value is achieved for ¼ 1=2. That is, Algorithm EDF-US½ has the maximum guaranteed total schedulable utilization for ¼ 1=2.</p><p>Corollary 2. Any task system is schedulable correctly by EDF-US[1/2] on m processors if the total utilization does not exceed ðm þ 1Þ=2.</p><p>Proof. Suppose the total utilization is U, U ðm þ 1Þ=2, and k is as in Theorem 3. We know the system is schedulable if the total utilization does not exceed</p><formula xml:id="formula_48">k=2 þ ðm À kÞ À ððm À kÞÞ À 1Þ=2 ¼ ðk þ 2m À 2k À m þ k þ 1Þ=2 ¼ ðm þ 1Þ=2: u t</formula><p>The above bound is tight. This can be shown using the same argument used in <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b1">[2]</ref> to show that "for all studied static-priority scheduling approaches (partitioning, global, and pfair global)" one cannot do better than utilization of m=2 on m processors. First, observe that even with deadline scheduling the priority for each job is fixed once the job is released. Consider a set of identical tasks 1 ; . . . ; mþ1 with c i ¼ 1=2 þ 1=x and d i ¼ T i ¼ 1. All the tasks have the same period, so if they are released together at the same time one job of each task will be competing with one job of the rest and all will have the same deadline. Any scheduling scheme that is based on assigning a fixed priority to each job will run m of the jobs until they have completed, and then run the remaining job. It will not have enough time to complete within its deadline. By choosing x large enough, one can make</p><formula xml:id="formula_49">U ¼ ðm þ 1Þð1=2 þ 1=xÞ arbi- trarily close to ðm þ 1Þ=2.</formula><p>If there is a need to support preperiod and/or postperiod deadlines, the idea of singling out a few "ugly" tasks for higher priority treatment, which is at the core of the EDF-US algorithms, can be adapted further. Suppose one has m processors and a set of N tasks for which the schedulability test (that of Theorem 1 or, alternatively, that of Corollary 1) fails. One can try partitioning the N tasks into two sets: a set of k (k &lt; m) ugly tasks to receive maximum priority, and schedule the remaining N À k tasks using EDF scheduling. If the EDF schedulability test succeeds for the N À k nice tasks on m À k processors, then the entire task set is schedulable. A logical sequence of ugly task sets to try would be k tasks with the largest values of ci di , for k ¼ 1; 2; . . . m À 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We have demonstrated an efficiently computable schedulability test for EDF scheduling on a homogeneous multiprocessor system, which allows preperiod and postperiod deadlines. It can be applied statically, or applied dynamically as an admission test. Besides extending and generalizing previously known utilization-based tests for EDF multiprocessor schedulability by supporting preperiod and postperiod deadlines, we have provided a distinct and independent proof using a new technique. We have also shown that EDF-US[1/2] is an optimal multiprocessor scheduling technique, with respect to maximizing the processor utilization at which deadlines can be missed. The notion of -busy interval, used to derive an EDF schedulability condition here, has broader applications. In <ref type="bibr" target="#b5">[6]</ref> and <ref type="bibr" target="#b4">[5]</ref>, the same approach is applied to the analysis of deadline monotonic scheduling for multiprocessor systems. The technique may also be applicable to more general multiprocessor systems, such as those studied in <ref type="bibr" target="#b9">[10]</ref>, or to probabilistic analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The author is thankful to the anonymous reviewers for their constructive comments, which improved the quality of this paper. He is especially thankful to the reviewer who noticed a false statement in one of the proofs of the original paper, and the one reviewer who suggested extending the analysis to include postperiod deadlines. Some of the results reported here also appeared in a paper presented at the 2003 IEEE Real-Time Systems Symposium <ref type="bibr" target="#b4">[5]</ref>. This paper differs from that paper by focusing specifically on deadline scheduling, including new and more detailed proofs, extending the analysis to include tasks with postperiod deadlines, and showing that EDF-US[1/2] is optimal.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. All processors must be busy whenever k is not executing.</figDesc><graphic coords="2,306.60,69.17,216.46,130.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. More than one job of k may execute in a k -busy interval if d k ! T k .</figDesc><graphic coords="3,130.39,69.17,305.89,114.86" type="bitmap" /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Theodore P. Baker received the PhD degree in computer science from Cornell University in 1974, for research on relative computability and computational complexity. He is a professor in the Department of Computer Science at Florida State University (FSU), and served four years as chair of that department. Starting in 1979, Professor Baker became involved with the development of the Ada programming language. The group he organized at FSU produced one of the first validated Ada cross-compilers for embedded systems. Since then, he has done research, development, and consulting related to real-time embedded computing, from basic research on scheduling and concurrency control through development of kernels and runtime system support for real-time programming languages. He has also been active in IEEE (POSIX) and ISO standards work related to real-time systems. He was a member of the SEI Rate Monotonic Analysis group, served as real-time area expert for the Ada 9X language mapping and revision team, and was a member of the 1997 National Research Council panel on Software Policies for the Department of Defense. He directed the FSU teams that developed several software artifacts, including the FSU POSIX threads library, the Florist implementation of the POSIX.5 API, a validation suite for the same, and the multitasking runtime system for the Gnu Ada (GNAT) compiler. He directed the porting of the latter to several environments, including the Java Virtual Machine and RTLinux. Professor Baker's current research focii are multiprocessor scheduling theory and experimentation, and real-time device driver architecture. He is a senior member of the IEEE Computer Society.</p><p>. For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/publications/dlib.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Static-Priority Scheduling on Multiprocessors</title>
		<author>
			<persName><forename type="first">B</forename><surname>Andersson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<pubPlace>Sweden</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Dept. of Computer Eng., Chalmers Univ. of Technology, Go ¨teborg</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Static-Priority Scheduling on Multiprocessors</title>
		<author>
			<persName><forename type="first">B</forename><surname>Andersson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baruah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jonsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 21st IEEE Real-Time Systems Symp</title>
		<meeting>21st IEEE Real-Time Systems Symp</meeting>
		<imprint>
			<date type="published" when="2001-12">Dec. 2001</date>
			<biblScope unit="page" from="193" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Early-Release Fair Scheduling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th Euromicro Conf. Real-Time Systems</title>
		<meeting>12th Euromicro Conf. Real-Time Systems</meeting>
		<imprint>
			<date type="published" when="2001-06">June 2001</date>
			<biblScope unit="page" from="34" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stack-Based Scheduling of Real-Time Processes</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Real-Time Systems J</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67" to="100" />
			<date type="published" when="1991-03">Mar. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multiprocessor EDF and Deadline Monotonic Schedulability Analysis</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 23rd IEEE Real-Time Systems Symp</title>
		<meeting>23rd IEEE Real-Time Systems Symp</meeting>
		<imprint>
			<date type="published" when="2003-12">Dec. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An Analysis of Deadline-Monotonic Scheduling on a Multiprocessor</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Florida State Univ., Dept. of Computer Science, technical report</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Proportionate Progress: A Notion of Fairness in Resource Allocation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Baruah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Plaxton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Varvel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithmica</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="600" to="625" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast Scheduling of Periodic Tasks on Multiple Resources</title>
		<author>
			<persName><forename type="first">S</forename><surname>Baruah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gherke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Plaxton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Ninth Int&apos;l Parallel Processing Symp</title>
		<meeting>Ninth Int&apos;l Parallel essing Symp</meeting>
		<imprint>
			<date type="published" when="1995-04">Apr. 1995</date>
			<biblScope unit="page" from="280" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On a Real-Time Scheduling Problem</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="127" to="140" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On-Line Scheduling on Uniform Multiprocessors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Funk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Goossens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baruah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 21st IEEE Real-Time Systems Symp</title>
		<meeting>21st IEEE Real-Time Systems Symp</meeting>
		<imprint>
			<date type="published" when="2001-12">Dec. 2001</date>
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Priority-Driven Scheduling of Periodic Task Systems on Multiprocessors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Goossens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Funk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baruah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Real Time Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2/3</biblScope>
			<biblScope unit="page" from="187" to="205" />
			<date type="published" when="2003-11">Sept./Nov. 2003</date>
			<publisher>Kluwer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Aperiodic Servers in a Deadline Scheduling Environment</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Ghazalie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Real-Time Systems J</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="68" />
			<date type="published" when="1995-07">July 1995</date>
			<publisher>Kluwer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Validating Timing Constraints in Multiprocessor and Distributed Systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ha</surname></persName>
		</author>
		<idno>UIUCDCS-R- 95-1907</idno>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
		<respStmt>
			<orgName>Dept. of Computer Science, Univ. of Illinois at Urbana-Champaign</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Validating Timing Constraints in Multiprocessor and Distributed Real-Time Systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W S</forename><surname>Liu</surname></persName>
		</author>
		<idno>UIUCDCS-R-93-1833</idno>
		<imprint>
			<date type="published" when="1993-10">Oct. 1993</date>
		</imprint>
		<respStmt>
			<orgName>Dept. of Computer Science, Univ. of Illinois at Urbana-Champaign</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Validating Timing Constraints in Multiprocessor and Distributed Real-Time Systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th IEEE Int&apos;l Conf. Distributed Computing Systems</title>
		<meeting>14th IEEE Int&apos;l Conf. Distributed Computing Systems</meeting>
		<imprint>
			<date type="published" when="1994-06">June 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scheduling Algorithms for Multiprogramming in a Hard-Real-Time Environment</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Layland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="46" to="61" />
			<date type="published" when="1973-01">Jan. 1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Real-Time Systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W S</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Prentice-Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Optimal Time-Critical Scheduling via Resource Augmentation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Torng</surname></persName>
		</author>
		<author>
			<persName><surname>Wein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 29th Ann. ACM Symp. Theory of Computing</title>
		<meeting>29th Ann. ACM Symp. Theory of Computing</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="140" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deadline-Based Scheduling of Periodic Task Systems on Multiprocessors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baruah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing Letters</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="93" to="98" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
