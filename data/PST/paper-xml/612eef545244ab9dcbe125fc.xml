<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adaptive Label Smoothing To Regularize Large-Scale Graph Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-08-30">30 Aug 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kaixiong</forename><surname>Zhou</surname></persName>
							<email>kaixiong.zhou@rice.edu</email>
						</author>
						<author>
							<persName><forename type="first">Ninghao</forename><surname>Liu</surname></persName>
							<email>ninghao.liu@uga.edu</email>
						</author>
						<author>
							<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
							<email>fyang@rice.edu</email>
						</author>
						<author>
							<persName><forename type="first">Zirui</forename><surname>Liu</surname></persName>
							<email>zirui.liu@rice.edu</email>
						</author>
						<author>
							<persName><forename type="first">Rui</forename><surname>Chen</surname></persName>
							<email>rui.chen1@samsung.com</email>
						</author>
						<author>
							<persName><forename type="first">Li</forename><surname>Li</surname></persName>
							<email>li.li1@samsung.com</email>
						</author>
						<author>
							<persName><forename type="first">Soo-Hyun</forename><surname>Choi</surname></persName>
							<email>soohyunc@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
							<email>xia.hu@rice.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Rice University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Georgia</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Rice University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Rice University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Samsung Research America</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Samsung Research America</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Rice University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Adaptive Label Smoothing To Regularize Large-Scale Graph Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-08-30">30 Aug 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2108.13555v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural networks (GNNs), which learn the node representations by recursively aggregating information from its neighbors, have become a predominant computational tool in many domains. To handle large-scale graphs, most of the existing methods partition the input graph into multiple sub-graphs (e.g., through node clustering) and apply batch training to save memory cost. However, such batch training will lead to label bias within each batch, and then result in over-confidence in model predictions. Since the connected nodes with positively related labels tend to be assigned together, the traditional cross-entropy minimization process will attend on the predictions of biased classes in the batch, and may intensify the overfitting issue. To overcome the label bias problem, we propose the adaptive label smoothing (ALS) method to replace the one-hot hard labels with smoothed ones, which learns to allocate label confidences from the biased classes to the others. Specifically, ALS propagates node labels to aggregate the neighborhood label distribution in a pre-processing step, and then updates the optimal smoothed labels online to adapt to specific graph structure. Experiments on the real-world datasets demonstrate that ALS can be generally applied to the main scalable learning frameworks to calibrate the biased labels and improve generalization performances.</p><p>Recently, several scalable algorithms of GNNs have been proposed to handle the large-scale graphs, among which sub-graph sampling methods are dominant in literature <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b51">52]</ref>. Specifically, instead of training on the full graph, the sampling methods sample subsets of nodes and edges to Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large-scale graphs, which are characterized by massive nodes and edges, are ubiquitous in real-world applications, such as social networks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b32">33]</ref> and knowledge graphs <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b42">43]</ref>. Although graph neural networks (GNNs) have shown effectiveness in many fields <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b46">47]</ref>, most of them rely on propagating messages over the whole graph dataset, and are mainly developed for relatively small graphs. Such message passing paradigms lead to prohibitive computation and memory requirements. formulate a sub-graph at each step, which is treated as an independent mini-batch. For example, Cluster-GCN <ref type="bibr" target="#b9">[10]</ref> first clusters the input graph into sub-graph groups, and then formulates each batch with a fixed number of groups (referred as batch size) during model training. LGCN <ref type="bibr" target="#b14">[15]</ref> samples sub-graphs via breadth first search, as motivated by small patch cropping on a large image.</p><p>Nevertheless, the label bias existing in the sampled sub-graphs could make GNN models become over-confident about their predictions, which leads to overfitting and lowers the generalization accuracy <ref type="bibr" target="#b16">[17]</ref>. Note that in the real-world assortative graphs <ref type="bibr" target="#b38">[39]</ref>, the closely connected nodes are potential to share the same label or positively related labels. The sub-graph sampling methods usually assign these related nodes into the same sub-graph and lead to label bias in a batch, whose node label distribution is significantly different from the other batches. Taking Cluster-GCN as an example, where the community with the same node labels is clustered as a sub-graph, the label distribution variance among batches is dramatic as shown in Figure <ref type="figure" target="#fig_0">1</ref>. Comparing with the traditional deep neural networks trained by uniform batch <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b47">48]</ref>, the cross-entropy minimization in the biased batch will severely make GNN model to attend only on the correctness of biased ground-truth category by producing an extremely large prediction probability. Such over-confident prediction will overfit on training set (e.g., the decreasing training loss of Cluster-GCN in Figure <ref type="figure" target="#fig_0">1</ref>), but generalizes poorly on the testing set (e.g., the increasing testing loss). To overcome the over-prediction and overfitting, label smoothing has been proposed to soften the one-hot class label by mixing it with a uniform distribution <ref type="bibr" target="#b37">[38]</ref>. Through penalizing the over-confident prediction towards the ground-truth label, the label smoothing (LS) has been used to improve the generalization performance across a range of tasks, including image classification <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b50">51]</ref>, semantic parsing <ref type="bibr" target="#b16">[17]</ref>, and neural machine translation <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>However, it is non-trivial to apply the label smoothing to regularize and adapt to the large-scale graph training from two structural levels: local node and global graph. First, different from generic machine learning problems associated with independent data instances, in graph data, it is generally assumed that class labels of connected nodes are positively related in many real-world applications <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b21">22]</ref>. In other words, for a specific local node, its label prediction highly depends on the label distribution of its neighbors. The traditional label smoothing, after mixing one-hot hard target with a uniform distribution, could wrongly regularize nodes in graph data. Second, considering the global graph, the relevance between different pairs of labels could vary. For example, in academic networks, civil engineering researchers tend to collaborate more with ecology researchers than with physicists <ref type="bibr" target="#b29">[30]</ref>. The optimal label smoothing should be conditioned on the the relevance between the ground-truth label and the related labels. The fixed label smoothing paradigm, by mixing uniform distribution, will fail to model such global label relevance specifically to the downstream tasks.</p><p>To bridge the gap, in this paper, we develop a simple yet effective adaptive label smoothing (ALS) method to regularize the representation learning on large-scale graphs. We aim to answer two research questions. First, given a local node, how can we estimate its smoothed label that is aware of the neighborhood structure? Second, how can we learn the global label relevance for a specific task? Through exploring these questions, we make three significant contributions as follows.</p><p>• We are the first to analyze the label bias problem in the sub-graph sampling methods for the largescale graph training. The biased batch training could make GNN model produce over-confident prediction and overfit on the training set. • We present an adaptive label smoothing methods decoupled into the following stages: a label propagation preprocessing step to aggregate the local neighborhood label distribution; a label refinement step mapping the preprocessed neighborhood labels to learn the global smoothed label adpatively. Our method is very simple and memory efficient, and could be scaled to the large-scale graph with negligible step to map the desired smoothed label. • We propose a label smoothing pacing function to allocate different smoothing strengths along the training process, in order to avoid the overly regularization at the beginning. • The empirical results show that our adaptive label smoothing could relieve the overfitting issue and yield better node classification accuracies based upon most scalable learning frameworks.  We use A i,j to index the (i, j)-th element in matrix A, and use y i,j to represent the j-th entity of vector y i . In this work, we focus on node classification tasks, and propose using label smoothing to address the over-confident prediction and overfitting issue in large-scale graph analysis. A graph is represented by G = (A, X), where A ∈ R N ×N denotes the adjacency matrix, X ∈ R N ×d denotes the feature matrix, and N is the number of nodes. Each node i ∈ V is associated with a feature vector x i ∈ R d (indexed by the i-th row in X) and a one-hot class label y i ∈ R C , where C is the number of class labels. Given a training set V l with labels, the goal is to classify the nodes in the unlabeled set V u = V \ V l via learning effective node representations. Let f θ denote the GNN model, where θ denotes model parameters. The prediction for a node is ŷi = f θ ((A, X)) ∈ R C . Recalling the batch training in a large-scale graph, the plain cross-entropy loss in a batch B is given by:</p><formula xml:id="formula_0">L Plain (θ) = 1 |B| i∈B H(y i , ŷi ) = − 1 |B| i∈B C c=1 y i,c log ŷi,c ,<label>(1)</label></formula><p>where H is the cross-entropy function. Sub-graphs are sampled to build each batch, and B contains the nodes in the sampled sub-graphs for training. |B| denotes the number of training nodes in B.</p><p>Model analysis. We empirically study the label bias phenomenon, over-prediction and overfitting issues of Cluster-GCN models trained on the ogbn-products dataset. Other sub-graph sampling methods with the similar issue are shown in Appendix. First, to evaluate the label bias, we define the probability of nodes with class c in a batch as:</p><formula xml:id="formula_1">p c = i∈B 1y i,c =1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>|B|</head><p>, where 1 yi,c=1 indicates whether node i belongs to class c. The mean and standard variance of p c among batches are shown in the right part of Figure <ref type="figure" target="#fig_0">1</ref>. We could observe that the standard variance is significantly larger when batch size is small, while the mean value is relatively stable. It means that nodes within a small batch tend to belong to certain classes, instead of evenly distribute across all classes. Such label bias is inherent in sub-graph sampling methods, such as clustering <ref type="bibr" target="#b9">[10]</ref> and random walk sampling <ref type="bibr" target="#b51">[52]</ref>, since positively related nodes are more likely to be selected together in a sampled sub-graph.</p><p>We further analyze the training and testing losses of Cluster-GCN in Figure <ref type="figure" target="#fig_0">1</ref>. Different from using uniform batch labels in traditional machine learning, the label bias in the batches further leads the GNN model to over-confidently attend (i.e., produce large prediction probabilities) on the groundtruth classes by minimizing the vanilla cross-entropy loss. The over-confident prediction overfits the training set and accelerates the decrease of training loss, but poorly generalizes to the testing set as shown in the increased testing loss. Comparing with the batch size of 2, a larger batch size of 32 reduces the variance of p c and relieves the label bias to some extent, which brings a smaller testing loss and better generalization performance. However, the big batch size would improve computation and memory costs, which is not inline with the purpose of sub-graph batch training on large graphs.</p><p>Label smoothing. To overcome over-confidence and overfitting, label smoothing has been proposed to mix the one-hot hard labels with uniform distribution in the image classification <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b50">51]</ref> and natural language processing <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b27">28]</ref>. To be specific, considering a training node i, its smoothed label is given by:</p><formula xml:id="formula_2">y LS i = (1 − α)y i + α1/C, where 1/C = [1/C, . . . , 1/C] ∈ R C is a uniform</formula><p>distribution and α is the regularization strength. Then the cross-entropy is given by:</p><formula xml:id="formula_3">L LS (θ) = 1 |B| i∈B H(y LS i , ŷi ) = 1 |B| i∈B (1 − α)H(y i , ŷi ) + αH(1/C, ŷi ).<label>(2)</label></formula><p>By minimizing L LS (θ), we are able to prevent the model from being over-confident on the groundtruth y i , via a penalty that enforces non-nigligible prediction probabilities on the other classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Adaptive Label Smoothing</head><p>The above label smoothing fails to adapt to graph data by ignoring two informative attributes: the label distribution within local neighborhood and the global label relevance. First, the nodes' labels are not independently distributed, and correlate positively to their local neighbors <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b60">61]</ref>.</p><p>The vanilla label smoothing assumes that node labels are independent and identically distributed, and applies the uniform distribution to regularize representation learning. It could mislead the model prediction to attend on the negatively related labels. Second, in the overall graph, the relevance between each pair of labels is different from each other. For example, the pair-wise collaboration strengths among engineering, ecology, and physical researchers are unbalanced in the academic networks <ref type="bibr" target="#b29">[30]</ref>. In the hierarchical GNNs, it is commonly assumed that the connections between labeled communities should be sparse <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b58">59]</ref>, instead of the uniform and full connection. The vanilla label smoothing with fixed uniform distribution cannot properly learn the latent global label relevance for the downstream task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Proposed Techniques</head><p>In this work, we propose ALS to calibrate the label bias and regularize the sub-graph batch training for the large-scale graph. ALS consists of three parts: (1) a pre-processing step of label propagation to obtain the prior knowledge of neighborhood label distribution; (2) a label refinement step to correct the prior knowledge and learn the global label relevance in the training phase; and (3) a smooth pacing function to gradually schedule the smooth strength and avoid the overly label regularization.</p><p>Label propagation. Based on the expectation that the two connected nodes are likely to have the same label according to graph homophily, label propagation passes labels iteratively to learn the label predictions <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2]</ref>. However, most of them involve parameters and are expensive to be trained.</p><p>To scale to large-scale graphs, we simplify the label propagation by removing the trainable weights, and conduct it only as a pre-processing step. Specifically, let</p><formula xml:id="formula_4">Y (k) = [y (k) 1 , • • • , y<label>(k)</label></formula><p>N ] ∈ R N ×C denote the propagated label matrix obtained from the k-th iteration of label propagation. At the k + 1-th iteration, we update the propagated label matrix as follows:</p><formula xml:id="formula_5">Y (k+1) = (1 − β)D −1 AY (k) + βY (0) .<label>(3)</label></formula><p>For the initial label matrix</p><formula xml:id="formula_6">Y (0) = [y (0) 1 , • • • , y<label>(0)</label></formula><p>N ] , it consists of one-hot hard labels y i for training nodes and zero vectors otherwise. β is residual strength to preserve the initial training labels, and D is the diagonal degree matrix of A. The label propagation in Eq. ( <ref type="formula" target="#formula_5">3</ref>) is similar in spirit to <ref type="bibr" target="#b59">[60]</ref>, but we preserve Y (0) to avoid the overwhelming of training labels. After K iterations of label propagation, we obtain the prior knowledge of neighborhood label distribution Y (K) up to K hops away. Such prior knowledge provides enough neighborhood information to be refined. Our label propagation has a good trade-off between the efficiency and effectiveness.</p><p>Label refinement. In this step, we aim to refine propagated label matrix Y (K) . Specifically, given the propagated label y</p><formula xml:id="formula_7">(K) i ∈ R C of</formula><p>training node i (i.e., indexed from the i-th row of Y (K) ), we correct it by:</p><formula xml:id="formula_8">y soft i = Softmax(W y (K) i</formula><p>). W ∈ R C×C is a trainable matrix. The smoothed label used to regularize model training is then given by:</p><formula xml:id="formula_9">y ALS i = (1 − α)y i + αy soft i .<label>(4)</label></formula><p>Notably, element W c,j indicates the latent relevance between classes c and j, and is shared globally by all nodes over the graph. Considering node i, the real relevance to class c is corrected to be proportional to j W c,j y (K) i,j . To well learn the global label relevance in W , we jointly train with classification task and compute the batch loss as follows:</p><formula xml:id="formula_10">L ALS (θ, W ) = 1 |B| i∈B H(y ALS i , ŷi ) + γKL(y soft i , 1/C) = 1 |B| i∈B (1 − α)H(y i , ŷi ) + αH(y soft i , ŷi ) + γKL(y soft i , 1/C),<label>(5)</label></formula><p>where KL denotes the KL distance of two probability distribution vectors, and γ is a positive hyperparameter. Compared with the traditional label smoothing in Eq. ( <ref type="formula" target="#formula_3">2</ref>), ALS relaxes the uniform distribution to learn the optimal soft label y soft i and adapt to the downstream task. On one hand, parameter W is updated to learn the global label relevance and produce a reasonable y soft i . On the other hand, the KL distance constraint is exploited to avoid y soft i collapsing into the one-hot hard target y i and guarantee the divergence.</p><p>Smooth pacing. Considering the batch training in large-scale graph, the constant smoothing strength α may overly regularize model at the initial training phase. Given the randomly initialized parameter W , the soft label y soft i will mislead model prediction ŷi to attend on the unrelated classes. Motivated from the batch pacing in curriculum learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24]</ref>, we propose a smooth pacing function to gradually schedule the appropriate smoothing strength α t at the t-th epoch. At the early phase, since the over-confident prediction has not appeared, we use a small α t to let model learn the correct prediction. With the ongoing of training, we gradually improve α t to regularize model. Specially, we consider the following two categories of pacing function: (1) a linear pacing function of α t = min(r • t, α max ), where r is pacing rate and α max is the maximum smoothing strength; (2) an exponential pacing function of α t = min(b • exp(r • t), α max ), where b is the initial smoothing strength at epoch t = 0. In our ALS, we replace the constant smoothing strength α in Eq. ( <ref type="formula" target="#formula_10">5</ref>) with α t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Analysis</head><p>Scalability analysis. Based on the sparse matrix multiplication, the time complexity of label propagation is O(K||A|| 0 C), where ||A|| 0 is number of nonzeros in A. Since the label propagation is conducted in pre-processing step, it could scale to the large-scale graph on CPU platforms with large memory. We thus ignore its memory complexity. The computation of label refinement mainly lines in the matrix multiplication with W . Considering any a backbone network, the extra time complexity is only O(|B|C 2 ), and the extra memory complexity is only O(C 2 ). Therefore, our ALS can augment any scalable algorithms to handle the large-scale graph.</p><p>Generalization analysis. We apply ALS to augment Cluster-GCN and train on dataset ogbnproducts. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, comparing with the plain Cluster-GCN, our ALS has larger training losses but is accompanied with smaller testing losses. In other word, our model has better generalization performance on testing test by avoiding the overfitting on training set. Specially, the label bias problem is much severe in the small batch size of 2. In this case, the testing loss of plain Cluster-GCN increases significantly due to the extremely over-confident prediction and the overfitting on training set. In contrast, our model could still avoid the over-confident prediction by even increasing the training loss at the end of training. The label smoothing regularization brings and maintains a lower testing loss.</p><p>Comparison to previous work. Although the label smoothing has been applied in computer vision and natural language processing <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b25">26]</ref>, it has not been studied to regularize GNN models for the graph data analytics. The previous GNNs are mainly developed to process small graph. In this paper, we observe the label bias problem resulted from the sub-graph batch training in the large-scale graph, and analyze the over-confident prediction and overfitting issue. Compared with the traditional label smoothing with uniform distribution, we propose ALS to adapt to the graph data. We are aware that recently there have been some label smoothing works to learn the soft label <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b12">13]</ref>, which is similar to our label refinement module. However, they are not targeted for graph data, missing to incorporate the graph structure. In the experiments, we empirically demonstrate that all the three modules in ALS are crucial to regularize the large-scale graph training.</p><p>Another similar line of work is label propagation. Most of previous methods involve trainable weights and cannot scale to the large-scale graph <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2]</ref>. For those simple and scalable methods, they either directly use the propagated labels to predict the testing nodes <ref type="bibr" target="#b59">[60]</ref>, or concatenate them to node features as the nodes' inputs <ref type="bibr" target="#b36">[37]</ref>. In this work, we exploit the pre-processed soft label to regularize the model training. In the experiments, we empirically show that the label smoothing is a better way to exploit this prior label knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we empirically evaluate the effectiveness of ALS on several real-world datasets. Overall, we aim to answer four research questions as follows. Q1: Can ALS effectively regularize the model to obtain better generalization performance, comparing with the plain model and label smoothing with uniform distribution? Q2: How does each module of ALS affect its performance? Q3: How does ALS preform comparing with the other exploitation ways of prior label knowledge? Q4: How do the hyperparameters influence the performance of ALS?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Setup</head><p>Datasets. We evaluate our proposed models on 4 graphs with different scales using node classification tasks, following the previous large-scale graph representation learning efforts. These benchmark datasets include Flickr <ref type="bibr" target="#b51">[52]</ref>, Reddit <ref type="bibr" target="#b19">[20]</ref>, ogbn-products and ogbn-mag <ref type="bibr" target="#b20">[21]</ref>, whose node numbers are 89K, 233K, 2449K and 1940K, respectively. Their data statistics are provided in Appendix.</p><p>Backbone frameworks. We mainly evaluate ALS on the scalable backbone frameworks based upon sub-graph sampling. Since the precomputing methods are another important lines of scalable graph representation learning, we conduct our method on them to demonstrate the general effectiveness. For the sub-graph sampling based methods, we adopt the popular backbone frameworks of GraphSAGE <ref type="bibr" target="#b19">[20]</ref>, Cluster-GCN <ref type="bibr" target="#b9">[10]</ref> and GraphSAINT <ref type="bibr" target="#b51">[52]</ref>. For the pre-computing based methods, we choose the backbone frameworks of MLP and SIGN <ref type="bibr" target="#b33">[34]</ref>. The detailed descriptions of these five frameworks are provided in Appendix. Note that we aim to demonstrate the general effectiveness of ALS in improving model generalization for the diverse scalable learning frameworks, instead of achieving the state-of-the-art performance on each classification task. Therefore, for each experiment on benchmark datasets, we conduct and compare three implementations: the plain scalable model trained with cross-entropy loss in Eq. ( <ref type="formula" target="#formula_0">1</ref>), the model augmented with conventional label smoothing (LS) as shown in Eq. ( <ref type="formula" target="#formula_3">2</ref>), and the model augmented with ALS as shown in Eq. <ref type="bibr" target="#b4">(5)</ref>.</p><p>Implementation. We directly use the implementations of the backbone networks either from the their official repositories or based on the official examples of PyTorch Geometric. We further implement LS and ALS over each backbone model. For LS with uniform distribution, we set the constant smoothing strength α as 0.1, which is widely applied in regularizing image classification. For our ALS, we choose the appropriate hyperperameters of residual strength β and step K in the label propagation, and also determine the KL distance constraint γ as well as the smooth pacing rate r. While the linear smooth pacing is adopted in the sub-graph sampling methods, the exponential pacing function is used in the precomputing methods. The detailed choices on four datasets are shown in Appendix. We study the influences of these hyperparameters in the experiments, and show that our model is not sensitive to them within a wide value range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiment Results</head><p>Generalization improvement by label smoothing. To provide answers for the research question Q1, Table <ref type="table" target="#tab_1">1</ref> summarizes the comprehensive comparisons among the plain model without any label smoothing, the regularized model with LS, and the regularized model with ALS over each combination of backbone framework and dataset. It is observed that our ALS can achieve superior performances in 18 cases out the 20 in total. Specifically, compared with the plain frameworks based on sub-graph sampling, both LS and ALS can generally improve test accuracy. The sampling methods assign connected nodes possibly with the same label into a sub-graph, which will lead to label bias within a batch. The label bias will make model over-confidently attend on the prediction of the ground-truth class, and may mislead model to fall into local minimums and decrease its generalization ability. While LS uses the uniform distribution to regularize model's prediction on other classes, our proposed ALS calibrates model more accurately by considering the local neighbors and using the global label refinement to correct the smoothed label. Compared with the plain precomputing model, our ALS can still generally improve the test accuracy, although LS tends to deteriorate model performance. Trained on GeForce RTX 2080 Ti GPU, the official implementations of MLP and SIGN use a full batch of training nodes. Since the label bias is not a big concern in such full batch training scenarios, the crude LS over-regularizes models and further hinders the accurate predictions on the ground-truth class. Notably, ALS exploits the informatic neighborhood label distribution to calibrate the model prediction, considering that the connected nodes should be close in the label space. Furthermore, the label refinement module learns to correct the smoothed label and jointly trains with the cross-entropy classification loss, which could adapt to the desired classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Base frameworks Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Flickr</head><p>Ablation studies. To demonstrate how each module of ALS affects the generalization performance and answer the research question Q2, we perform ablation studies over two sub-graph sampling based backbone networks, i.e., Cluster-GCN and GraphSAINT. In particular, to study the contribution of label propagation in ALS, we ablate it and replace soft label y (K) i with one-hot hard label y i to compute the smoothed label used for model training. The smoothed label is then obtained by:</p><formula xml:id="formula_11">y ALS i = (1 − α t )y i + α t Softmax(W y i ).</formula><p>To ablate the label refinement, we use the soft label y (K) i obtained from the label propagation to compute the smoothed label, i.e., y ALS i = (1−α t )y i +α t y (K) i . At the same time, we remove the KL distance constraint in Eq. (5). To ablate smooth pacing module, we use a constant smoothing strength α with 0.1 to generate smoothed label, i.e., y ALS i = 0.9 * y i + 0.1 * Softmax(W y</p><formula xml:id="formula_12">(K) i ).</formula><p>We summarize the ablation studies on the three modules of ALS in Table <ref type="table" target="#tab_2">2</ref>. It is observed that the ablation of any module will decrease the test accuracy, which empirically demonstrate their importances to adapt label smoothing in regularizing the graph representation learning. Comparing with the ablation of label propagation, we observe that the removing of label refinement and smooth pacing extremely damages the performance of ALS. Even with inaccurate prior knowledge of neighborhood label distribution, the label refinement module could be supervised to refine the smoothed label correctly to regularize model prediction. The smooth pacing allocates a smaller smoothing strength α t at the initial training phase, since the smoothed label is far from being well refined, and then gradually improves α t to regularize model from being overfitting.</p><p>Comparison of prior label knowledge. Besides the label smoothing, to scale to the large-scale graph, there are two other lines of work to exploit the prior knowledge of neighborhood label distribution y (K) i . First, the label propagation method uses y . Specifically, we implement label smoothing and concatenate y (K) i over the GraphSAINT backbone, and directly adopt the proposed label propagation module.</p><p>Table <ref type="table" target="#tab_3">3</ref> summarizes the test accuracies on the four benchmark datasets, where ALS generally achieves the superior performances. The label propagation fails to learn the informatic label prediction on ogbn-mag dataset, since it cannot combine the neighbor labels effectively without modeling the diverse node/edge types in heterogeneous graphs. The label propagation cannot adapt the prior label knowledge to classification tasks without any learnable parameters. Compared with the input concatenation, our label smoothing approach directly regularizes the model prediction to avoid the over-confident prediction and thus obtains better generalization performance. Recently, scalable learning method of C&amp;S is proposed to refine model prediction in the post-processing step <ref type="bibr" target="#b21">[22]</ref>, and shows promising performance on ogbn-products. By training a simple MLP to obtain the initial label predictions, C&amp;S propagates prediction errors and labels to obtain smoothed prediction results. To demonstrate that ALS is general to any scalable learning framework, we use LS and ALS to regularize the training of MLP module in C&amp;S, and compare test accuracies in Table <ref type="table" target="#tab_4">4</ref>. By regularizing MLP to obtain better initial label predictions, our proposed ALS can further improve the test accuracy up to 84.28, which is the state-of-the-art performance in the leader board.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Hyperparameter studies. To answer the research question Q4, we conduct experiments with different values of pacing rate r, loss hyperparameter γ, residual strength β, and label propagation step K. Figure <ref type="figure">2</ref> illustrates the hyperparameter studies of GraphSAINT+ALS on ogbn-products.</p><p>In general, within appropriate value ranges, most of these hyperparameter settings can achieve test accuracies larger than 79.3%, which is much superior than the baseline GraphSAINT. Specifically, the over-small (or large) pacing rate r damages the test performance due to the insufficient (or excessive) label smoothing regularization. The loss hyperparameter γ should be large enough (e.g., γ ≥ 10 −3 ), so as to avoid the learned smoothed label collapsing into one-hot hard target and to guarantee its regularization effect. The superior performance brought by β ≤ 0.5 demonstrates the importance of neighborhood label distribution in learning the structure-aware smoothed label. Based on Eq. ( <ref type="formula" target="#formula_5">3</ref>), with a smaller β, we tend to aggregate neighborhood labels during the label propagation. Similar to the common GNN settings, a small value of K is sufficient to aggregate the positively related neighbors to model the smoothed label correctly.</p><p>Global label relevance visualization. We visualize Softmax transformation of the global label relevance matrix along each row, i.e., Softmax(W ). Note that W is learned on backbone framework Graph neural networks. GNNs have shown superiority in processing graphs, i.e., data with rich relational structures. GNNs could be categorized into spectral domain and spatial domain models. The spectral models <ref type="bibr" target="#b4">[5]</ref> extends convolution on images to graph data by modeling on the spectrum of graph Laplacian. Models designed from the spatial perspective simplify the spectral models. Spatial models such as ChebNet <ref type="bibr" target="#b11">[12]</ref>, GCN <ref type="bibr" target="#b24">[25]</ref>, GAT <ref type="bibr" target="#b39">[40]</ref> and GIN <ref type="bibr" target="#b46">[47]</ref>, could also be understood from the message passing perspective. GNNs are playing increasingly crucial roles in various applications such as recommender systems <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b54">55]</ref>, social network analysis <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57]</ref>, and biochemical module analysis <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b58">59]</ref>.</p><p>Scalable graph representation learning. Two types of methods have been proposed to tackle scalability issue of GNNs, including sub-graph sampling methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b51">52]</ref> and precomputing methods <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b3">4]</ref>. To reduce computation and memory cost, the sub-graph sampling methods feed GNNs only with a small batch of sub-graphs, which consist of subsets of nodes and edges. Specifically, ClusterGCN <ref type="bibr" target="#b9">[10]</ref> conducts training on sampled sub-graphs in each batch, but the sub-graphs are obtained through clustering algorithms. GraphSAINT <ref type="bibr" target="#b51">[52]</ref> samples sub-graphs that are appropriately connected for information propagation, where a normalization technique is also proposed to eliminate bias. The major limitation for sub-graph based methods is that distant nodes in the original graph are unlikely to be fed into the GNNs in the same batch, thus leading to label bias in the trained models. The precomputing methods of SIGN <ref type="bibr" target="#b33">[34]</ref> and SGC <ref type="bibr" target="#b44">[45]</ref> remove trainable weights, and propagate node features over the graph in advance to store the smoothed features.</p><p>Label propagation. Label propagation distributes the observed node labels over the graph following the connection between nodes <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b61">62]</ref>. It has been used for semi-supervised training on graph data <ref type="bibr" target="#b52">[53]</ref>, where node labels are partially observed. The assumption behind is that labels and features change smoothly over the edges of the graph. It has also been proposed to combine feature propagation with label propagation towards a unified message passing scheme <ref type="bibr" target="#b35">[36]</ref>. Some recent work connects GNNs with label propagation <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b22">23]</ref> by studying how labels/features spread over a graph and how the initial feature/label of one node influences the prediction of another node.</p><p>Label smoothing. Label smoothing improves the generalization <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b31">32]</ref> and robustness <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b34">35]</ref> of a deep neural network. Label smoothing replaces one-hot labels with smoothed labels. It has been shown that label smoothing has similar effect as randomly replacing some of the ground-truth labels with incorrect values at each mini-batch <ref type="bibr" target="#b45">[46]</ref>. <ref type="bibr" target="#b30">[31]</ref> proposes reverse cross entropy for gradient smoothing. It encourages a model to better distinguish adversarial examples from normal ones in representation space. <ref type="bibr" target="#b43">[44]</ref> proposes the graduated label smoothing method, where high-confidence predictions are assigned with higher smoothing penalty than low-confidence ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we point out the inherent label bias within the sampled sub-graphs for the mini-batch training of large-scale graph. By minimizing vanilla cross-entropy loss, we empirically analyze that such label bias will make GNN model over-confidently predict the ground-truth class and lead to overfitting issue. To overcome the label bias and the resulted over-confident prediction, we propose an adaptive label smoothing to replace the one-hot hard target with smoothed label, which allocates prediction confidence to other classes to avoid overfitting. Specially, we learn the smoothed label with the prior knowledge of local neighborhood label distribution and the global label refinement to adapt to graph data on hand. The experiments show that our algorithm could generally improve the test performance by relieving the overfitting on biased labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix A.1 Datasets</head><p>The dataset statistics of Flickr <ref type="bibr" target="#b51">[52]</ref>, Reddit <ref type="bibr" target="#b19">[20]</ref>, obgn-products <ref type="bibr" target="#b20">[21]</ref>, and ogbn-mag <ref type="bibr" target="#b20">[21]</ref> are listed in Table <ref type="table" target="#tab_5">5</ref>. Flickr is a social network, where the nodes represent images and the edges denote the shared properties between two images. The node classification task in Flickr is to categorize the types of images. Reddit is a social netowork, where the nodes represent posts in Reddit forum and the edges indicate the same user comments between two posts. The node classification task in Reddit is to predict the communities of online posts based on user comments. ogbn-products is an Amazon product co-purchasing network, where the nodes represent products sold in Amazon and the edges indicate the co-purchasing relationships between two products. The node classification task in ogbn-products is to predict the category of a product. ogbn-mag is a heterogeneous network extracted from the Microsoft Academic Graph. It contains four types of entities: papers, authors, institutions, and fields of study. The directed edges are categorized into four types-an author is "affiliated with" an institution, an author "writes" a paper, a paper "cites" a paper, and a paper "has a topic of" a field of study. The node classification task in ogbn-mag is to predict the venue (conference or journal) of each entity of paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Backbone Frameworks</head><p>We evaluate our ALS on two main categories of scalable graph representation learning frameworks: one is based on the sub-graph sampling and the other one is based on precomputing. Although we aim at solving the label bias and over-confident prediction in the sub-graph sampling methods, we show that our method is general to both of these two scalable backbone frameworks. Specifically, we adopt the following five representative backbone frameworks:</p><p>• GraphSAGE <ref type="bibr" target="#b19">[20]</ref> (sub-graph sampling based). It is a node-wise sampling method to uniformly sample a batch of training nodes and their neighbors of different orders. The sampled nodes and neighbors construct several sub-graphs to formulate a batch. • Cluster-GCN <ref type="bibr" target="#b9">[10]</ref> (sub-graph sampling based). It first conducts node clustering algorithm to partition the input graph into a series of sub-graphs. During the training phase, each batch is directly formulated by a random subset of preprocessed sub-graphs. • GraphSAINT <ref type="bibr" target="#b51">[52]</ref> (sub-graph sampling based). Starting from a subset of training nodes, we choose the random walk sampler (i.e., the most powerful one as reported in <ref type="bibr" target="#b51">[52]</ref>) to sample their neighbors for constructing sub-graphs in one batch. • MLP (precomputing based). MLP is widely used to classify nodes based on the precomputed node features. Herein, MLP directly uses the original node features, which has been shown to achieve good classification performance in the graph data. In the precomputing methods, each node can be regarded as an independent sample, and does not connect to its neighbors. The batch is thus directly represented by an independent subset of training nodes. • SIGN <ref type="bibr" target="#b33">[34]</ref> (precomputing based). In the preprocessing step, SIGN conducts message-passing strategy and precomputes node features as: Âl X for l ∈ {1, • • • , L}. Â is a normalized adjacency matrix used in GCN <ref type="bibr" target="#b24">[25]</ref>. The precomputed node features of different orders are concatenated together to augment the original node features. In the training phase, the batch is constructed by a random subset of training nodes, and taken as input to the downstream classification model of MLP.</p><p>We implement the above scalable backbone frameworks according to the official examples of Pytorch Geometric </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Implementation Details</head><p>We further implement LS and ALS over each backbone model. For LS with uniform distribution, we set the constant smoothing strength α as 0.1, which is widely applied in regularizing image classification. For our ALS, we use the linear pacing and exponential pacing functions for the sub-graph sampling methods and precomputing methods, respectively. To have a fair comparison with LS, we set α max = 0.1 in our ALS. For each combination of backbone framework and dataset, we choose the appropriate hyperperameters of residual strength β and step K in the label propagation, and also determine the KL distance constraint γ as well as the smooth pacing rate r. The detailed hyperparameters involved in ALS are shown in Table <ref type="table">6</ref>. Notably, comparing with the sub-graph sampling methods, we use the negative pacing rate r in the precomputing methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Running Environment</head><p>All the experiments are implemented with PyTorch, and tested on a machine with 24 Intel(R) Xeon(R) CPU E5-2650 v4 @ 2.20GB processors, 128GB CPU memory size, and one GPU of GeForce RTX 3090 with 24 GB memory size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Label Bias, Over-confident Prediction and Overfitting Observations</head><p>As shown in Figure <ref type="figure" target="#fig_0">1</ref>, the sub-graph sampling method of Cluster-GCN brings label bias, and leads to the over-confident prediction and overfitting in training set. These problems will damage the model's generalization performance in testing set. Our ALS learns to replace the one-hot hard target y i with the smoothed label y ALS i , which could relieve these three problems to improve the generalization ability. In this section, we report the training losses, the testing losses, and the label biases of all the sub-graph sampling methods on ogbn-products, ogbn-mag, and Flickr. We show the experimental results in Figures <ref type="figure" target="#fig_13">4-11</ref>. Note that the batch sizes of GraphSAGE, Cluster-GCN, and GraphSAINT are defined by the corresponding sub-graph sampling functions in Pytorch Geometric<ref type="foot" target="#foot_1">2</ref> , i.e., NeighborSampler, ClusterLoader, and GraphSAINTRandomWalkSampler. While the batch sizes of GraphSAGE and GraphSAINT specify how many training samples per batch to load, the batch size of Cluster-GCN determines how many clustered sub-graphs to sample. We make the following empirical observations:</p><p>• All the sub-graph sampling methods bring label bias within a batch. It is shown that the standard deviance of p c is extremely large, which is compatible with the mean value of p c .</p><p>In other word, the nodes within a small batch tend to belong to certain classes, and the label distributions vary dramatically between batches. In general, the smaller the batch size is, the larger the standard deviance of p c will be. • Comparing with the plain backbone frameworks, our ALS has larger training losses. That is because ALS replaces the one-hot hard target y i with the smoothed label y ALS i , which distributes label confidences to both ground-truth class and the other classes. By minimizing the regularized loss in Eq. ( <ref type="formula" target="#formula_10">5</ref>), ALS reduces the model's prediction probability on the groundtruth class, and thus increases the training loss. The regularized prediction probability on the ground-truth class will help the model avoid the over-confident prediction and the overfitting on the training set.</p><p>• Comparing with the plain backbone frameworks, our ALS generally has smaller testing losses and better generalization ability. Since the model is regularized to avoid the over-confident prediction, the smooth prediction probability in ALS is more easier to be generalized to the unseen testing set.            </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Left, Middle: The training and testing losses on ogbn-products. While Plain-a means the original Cluster-GCN trained with batch size of a, ALS-a denotes the Cluster-GCN equipped with ALS. Right: The mean probability p c of nodes with specific class c within a batch, and the standard variance of probability p c among batches. Herein we show c = 0 on ogbn-products for an example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Hyperparameter studies of r, γ, β, and K of GraphSAINT+ALS on ogbn-products.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 . 6 :</head><label>16</label><figDesc>The basic model hyperparameters are defined in the examples or determined according to their public literature, including batch size, learning rate, weight decay, training epochs, Hyperparameter choices in ALS for each combination study of backbone and dataset. units, dropout rate, etc. All of the sub-graph sampling methods apply a three-layer GNN model, while the precomputing methods use a three-layer MLP. Following the official examples, we use full batch training in the precomputing methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Instead of using mini-batch training, the official examples of MLP and SIGN applies the full batch training. That means the trainable parametere W and soft label y soft i could be well updated at the initial training phase. Therefore, we use the decreasing smoothing strength in the precomputing methods, where the models are regularized strictly by the difficult smoothed label from the beginning. At the end of training, the precomputing methods are relaxed to learn the easy one-hot hard target to improve the test performance. The initial smoothing strengths b in the exponential pacing function for MLP are: 0.05 in Flickr, 0.2 in Reddit, and 0.08 in ogbn-products &amp; ogbn-mag. The values of b for SIGN are: 0.1 in Flickr, 0.2 in Reddit, 0.15 in ogbn-products, and 0.08 in ogbn-mag.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Left, Middle: The training and testing losses upon backbone GraphSAGE and dataset ogbn-products. While Plain-a means the original backbone trained with batch size of a, ALS-a denotes one equipped with ALS. Right: The mean probability p c of nodes with specific class c within a batch (i.e., blue square), and the standard variance of probability p c among batches (i.e., upper and lower bar). Herein we show c = 0 on ogbn-products for an example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Left, Middle: The training and testing losses upon backbone GraphSAINT and dataset ogbn-products. While Plain-a means the original backbone trained with batch size of a, ALS-a denotes one equipped with ALS. Right: The mean probability p c of nodes with specific class c within a batch (i.e., blue square), and the standard variance of probability p c among batches (i.e., upper and lower bar). Herein we show c = 0 on ogbn-products for an example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Left, Middle: The training and testing losses upon backbone GraphSAGE and dataset ogbn-mag. While Plain-a means the original backbone trained with batch size of a, ALS-a denotes one equipped with ALS. Right: The mean probability p c of nodes with specific class c within a batch (i.e., blue square), and the standard variance of probability p c among batches (i.e., upper and lower bar). Herein we show c = 0 on ogbn-mag for an example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Left, Middle: The training and testing losses upon backbone Cluster-GCN and dataset ogbn-mag. While Plain-a means the original backbone trained with batch size of a, ALS-a denotes one equipped with ALS. Right: The mean probability p c of nodes with specific class c within a batch (i.e., blue square), and the standard variance of probability p c among batches (i.e., upper and lower bar). Herein we show c = 0 on ogbn-mag for an example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Left, Middle: The training and testing losses upon backbone GraphSAINT and dataset ogbn-mag. While Plain-a means the original backbone trained with batch size of a, ALS-a denotes one equipped with ALS. Right: The mean probability p c of nodes with specific class c within a batch (i.e., blue square), and the standard variance of probability p c among batches (i.e., upper and lower bar). Herein we show c = 0 on ogbn-mag for an example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Left, Middle: The training and testing losses upon backbone GraphSAGE and dataset Flickr. While Plain-a means the original backbone trained with batch size of a, ALS-a denotes one equipped with ALS. Right: The mean probability p c of nodes with specific class c within a batch (i.e., blue square), and the standard variance of probability p c among batches (i.e., upper and lower bar). Herein we show c = 0 on Flickr for an example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Left, Middle: The training and testing losses upon backbone Cluster-GCN and dataset Flickr. While Plain-a means the original backbone trained with batch size of a, ALS-a denotes one equipped with ALS. Right: The mean probability p c of nodes with specific class c within a batch (i.e., blue square), and the standard variance of probability p c among batches (i.e., upper and lower bar). Herein we show c = 0 on Flickr for an example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Left, Middle: The training and testing losses upon backbone GraphSAINT and dataset Flickr. While Plain-a means the original backbone trained with batch size of a, ALS-a denotes one equipped with ALS. Right: The mean probability p c of nodes with specific class c within a batch (i.e., blue square), and the standard variance of probability p c among batches (i.e., upper and lower bar). Herein we show c = 0 on Flickr for an example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Test accuracies in percent of the plain model, the model with LS, and the model augmented with ALS. The best performance in each study is in bold.</figDesc><table><row><cell>Reddit</cell><cell>ogbn-products ogbn-mag</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Test accuracies in percent of ALS and its three variants obtained by ablating specific modules.</figDesc><table><row><cell>(K) i</cell><cell>to predict test nodes without any</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Test accuracies in percent of different label exploitation methods.</figDesc><table><row><cell></cell><cell>Flickr</cell><cell>Reddit</cell><cell cols="2">ogbn-products</cell><cell>ogbn-mag</cell></row><row><cell>Label propogation</cell><cell>50.33</cell><cell>92.33</cell><cell>73.45</cell><cell></cell><cell>-</cell></row><row><cell cols="6">GraphSAINT+Label Input 51.75 ± 0.22 95.10 ± 0.09 76.90 ± 0.32 47.20 ± 0.29</cell></row><row><cell>GraphSAINT+ALS</cell><cell cols="2">51.74±0.13 95.24±0.11</cell><cell cols="2">79.48±0.44</cell><cell>47.94±0.25</cell></row><row><cell></cell><cell></cell><cell cols="4">Backbone Methods obgn-products</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Plain</cell><cell>84.18±0.07</cell></row><row><cell></cell><cell></cell><cell></cell><cell>C&amp;S</cell><cell>LS</cell><cell>84.22±0.09</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ALS</cell><cell>84.28±0.06</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Test accuracy in percent.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Data statistics.</figDesc><table><row><cell></cell><cell># Nodes</cell><cell cols="4"># Edges # Classes # Features # Train/Validation/Test</cell></row><row><cell>Flickr</cell><cell>89,250</cell><cell>899,756</cell><cell>7</cell><cell>500</cell><cell>0.50 / 0.25 / 0.25</cell></row><row><cell>Reddit</cell><cell cols="2">232,965 11,606,919</cell><cell>41</cell><cell>602</cell><cell>0.66 / 0.10 / 0.24</cell></row><row><cell cols="3">ogbn-products 2,449,029 61,859,140</cell><cell>47</cell><cell>100</cell><cell>0.08 / 0.02 / 0.90</cell></row><row><cell cols="3">ogbn-mag 1,939,743 21,111,007</cell><cell>349</cell><cell>128</cell><cell>0.85 / 0.09 / 0.06</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/rusty1s/pytorch_geometric</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning on graph with laplacian regularization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Ando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Manifold regularization: A geometric framework for learning from labeled and unlabeled examples</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international conference on machine learning</title>
				<meeting>the 26th annual international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scaling graph neural networks with approximate pagerank</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rózemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2464" to="2473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Fastgcn: fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10247</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Stochastic training of graph convolutional networks with variance reduction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10568</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Revisiting graph based collaborative filtering: A linear residual graph convolutional network approach</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1725" to="1735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName><forename type="first">W.-L</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">One trillion edges: Graph processing at facebook-scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ching</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kabiljo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Logothetis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
				<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1804" to="1815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.09375</idno>
		<title level="m">Convolutional neural networks on graphs with fast localized spectral filtering</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Q</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-T</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05474</idno>
		<title level="m">Adaptive regularization of labels</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Graph neural networks for social recommendation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="417" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Large-scale learnable graph convolutional networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1416" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards a better understanding of label smoothing in neural machine translation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Herold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing</title>
				<meeting>the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="212" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning better structured representations using low-rank adaptive label smoothing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mehdad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
				<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Goibert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dohmatob</surname></persName>
		</author>
		<title level="m">Adversarial robustness via adversarial label-smoothing</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02216</idno>
		<title level="m">Inductive representation learning on large graphs</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Combining label propagation and simple models out-performs graph neural networks</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-N</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Benson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.13993</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Residual correlation in graph neural network regression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Benson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="588" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Self-paced curriculum learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Regularization via structural label smoothing</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dasarathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Berisha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1453" to="1463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">From label smoothing to label relaxation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lienen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hüllermeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th AAAI Conference on Artificial Intelligence, AAAI, Online</title>
				<meeting>the 35th AAAI Conference on Artificial Intelligence, AAAI, Online</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2021">February 2-9, 2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Generalized entropy regularization or: There&apos;s nothing special about label smoothing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Salesky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cotterell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00820</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02629</idno>
		<title level="m">When does label smoothing help?</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Coauthorship networks and patterns of scientific collaboration</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
				<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="5200" to="5205" />
		</imprint>
	</monogr>
	<note>suppl</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Towards robust detection of adversarial examples</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00633</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06548</idno>
		<title level="m">Regularizing neural networks by penalizing confident output distributions</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11198</idno>
		<title level="m">Sign: Scalable inception graph neural networks</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Defending against adversarial attacks by suppressing the largest eigenvalue of fisher information matrix</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.06137</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Masked label prediction: Unified message passing model for semi-supervised classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.03509</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Scalable and adaptive graph neural networks with self-label-enhanced training</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.09376</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Exploiting homophily effect for trust prediction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixth ACM international conference on Web search and data mining</title>
				<meeting>the sixth ACM international conference on Web search and data mining</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="53" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Unifying graph convolutional neural networks and label propagation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06755</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Billion-scale commodity embedding for e-commerce recommendation in alibaba</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="839" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Acekg: A large-scale knowledge graph for academic data mining</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM international conference on information and knowledge management</title>
				<meeting>the 27th ACM international conference on information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1487" to="1490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">On the inference calibration of neural machine translation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00963</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Disturblabel: Regularizing cnn on the loss layer</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4753" to="4762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks?</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11653</idno>
		<title level="m">Towards understanding label smoothing</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08804</idno>
		<title level="m">Hierarchical graph representation learning with differentiable pooling</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Revisiting knowledge distillation via label smoothing regularization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3903" to="3911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Graphsaint: Graph sampling based inductive learning method</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Prasanna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04931</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Hyperparameter learning for graph based semi-supervised learning algorithms</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">101</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Lal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Temporal augmented graph neural networks for session-based recommendations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06972</idno>
		<title level="m">Towards deeper graph neural networks with differentiable group normalization</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Dirichlet energy constrained learning for deep graph neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.02392</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03184</idno>
		<title level="m">Auto-gnn: Neural architecture search of graph neural networks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.08306</idno>
		<title level="m">Multi-channel graph neural networks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Learning from labeled and unlabeled data with label propagation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International conference on Machine learning (ICML-03)</title>
				<meeting>the 20th International conference on Machine learning (ICML-03)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Semi-supervised learning literature survey</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
