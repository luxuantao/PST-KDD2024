<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Investigating Transfer Learning in Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nishai</forename><surname>Kooverjee</surname></persName>
							<email>nishai.kooverjee@gmail.com</email>
							<idno type="ORCID">0000-0001-5471-9465</idno>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Applied Mathematics</orgName>
								<orgName type="institution">University of the Witwatersrand</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Steven</forename><surname>James</surname></persName>
							<email>steven.james@wits.ac.za</email>
							<idno type="ORCID">0000-0003-4366-4125</idno>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Applied Mathematics</orgName>
								<orgName type="institution">University of the Witwatersrand</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Terence</forename><surname>Van Zyl</surname></persName>
							<email>tvanzyl@uj.ac.za</email>
							<idno type="ORCID">0000-0003-4281-630X</idno>
							<affiliation key="aff1">
								<orgName type="department">Institute for Intelligent Systems</orgName>
								<orgName type="institution">University of Johannesburg</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Investigating Transfer Learning in Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Received: Accepted:</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>graph neural networks</term>
					<term>machine learning</term>
					<term>transfer learning</term>
					<term>multi-task learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural networks (GNNs) build on the success of deep learning models by extending them for use in graph spaces. Transfer learning has proven extremely successful for traditional deep learning problems: resulting in faster training and improved performance. Despite the increasing interest in GNNs and their use cases, there is little research on their transferability. This research demonstrates that transfer learning is effective with GNNs, and describes how source tasks and the choice of GNN impact the ability to learn generalisable knowledge. We perform experiments using real-world and synthetic data within the contexts of node classification and graph classification. To this end, we also provide a general methodology for transfer learning experimentation and present a novel algorithm for generating synthetic graph classification tasks. We compare the performance of GCN, GraphSAGE and GIN across both the synthetic and real-world datasets. Our results demonstrate empirically that GNNs with inductive operations yield statistically significantly improved transfer. Further we show that similarity in community structure between source and target tasks support statistically significant improvements in transfer over and above the use of only the node attributes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep learning has achieved success in a wide variety of problems, ranging from timeseries data to images and video <ref type="bibr" target="#b0">[1]</ref>. Data from these tasks are referred to as Euclidean <ref type="bibr" target="#b1">[2]</ref> and specialised models such as recurrent and convolutional neural networks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> have been designed to leverage the properties of such data. Despite these successes, not all problems are Euclidean. One particular class of such problems involve graphs, which naturally model complex real-world settings involving objects and their relationships. Recently, deep learning approaches have been extended to graph-based domains using graph neural networks (GNNs) <ref type="bibr" target="#b4">[5]</ref>, which leverage certain topological structures and properties specific to graphs <ref type="bibr" target="#b1">[2]</ref>. Since graphs comprise entities and the relationships between them, GNNs are said to learn relational information and may have the ability for relational reasoning <ref type="bibr" target="#b5">[6]</ref>.</p><p>One reason for the success of deep learning models is their ability to transfer to new tasks. In image classification, this transfer leads to more robust models and faster training <ref type="bibr" target="#b6">[7]</ref>. Despite the importance of transfer in deep learning, there has been little insight into the nature of transferring relational knowledge -that is, the representations learnt by graph neural networks. There is also no comparison of the generalisability of different GNNs when evaluated on downstream task performance. This lack of insight is in part due to the lack of a model-agnostic and task-agnostic framework and standard benchmark datasets and tasks for carrying out transfer learning experiments with GNNs.</p><p>We conduct an empirical study within the contexts of node classification and graph classification to determine whether transfer in GNNs occur and, if so, what factors influence success. In particular, we make the following contributions: First, we provide a methodology and additional metrics for evaluating GNN transfer learning empirically. Second, we arXiv:2202.00740v1 [cs.LG] 1 Feb 2022 provide a novel method for creating synthetic graph classification tasks with community structure. Finally, we evaluate the transferability of several popular GNNs on both real and synthetic datasets. Our results demonstrate that we can achieve positive transfer using graph neural networks; and that certain models exploit strong community structure properties present in the source task to yield effective transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Background</head><p>In this work, we consider problems for which the data can be modelled as a graph. A graph G = {V, E} consists of a set of N vertices, V, and a set of edges E. Let v i ∈ V denote a vertex, and e ij = (v i , v j ) ∈ E denote a directed edge from v i to v j . The adjacency matrix, A ∈ R N×N , has a ij = 1 where e ij ∈ E, and 0 otherwise. A graph may have node attributes, where X ∈ R N×C is the node feature matrix, and x i ∈ R C is the attribute vector for node v i . Similarly, a graph may have an edge attribute matrix X e <ref type="bibr" target="#b4">[5]</ref>. An important measure of a node's connectivity is its degree, which is the number of edges the node is connected to <ref type="bibr" target="#b7">[8]</ref>. We denote the degree of the i th node as d i . The degree matrix D = diag(d 1 , . . . , d N ) is the diagonal matrix containing the degrees of all vertices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.1.">Graph Neural Networks</head><p>The success of CNNs for computer vision problems motivate the need to formulate a counterpart to the convolution operator for graphs <ref type="bibr" target="#b1">[2]</ref>. Similarly to that of Euclidean signals, the problem of convolution for graphs can be tackled from either the spatial domain or spectral domain. One popular model is Graph Convolution Networks (GCNs) <ref type="bibr" target="#b8">[9]</ref>, which make use of a spectral graph convolution operation stacked layer-wise. We also consider two other models, GraphSAGE <ref type="bibr" target="#b9">[10]</ref> and Graph Isomorphism Networks (GINs) <ref type="bibr" target="#b10">[11]</ref>, which utilise spatial graph convolutions to aggregate information from a node's neighbourhood. GINs have previously outperformed both GCN and GraphSAGE in experimental performance on social media and biological datasets <ref type="bibr" target="#b10">[11]</ref>.</p><p>These three considered GNNs fall under the category of message-passing neural networks (MPNNs) <ref type="bibr" target="#b11">[12]</ref>. To provide comparison of the three GNNs' operations, we rewrite them as MPNN updates:</p><formula xml:id="formula_0">Model Update Rule GCN h (k) v ← σ W • ∑ u∈N (v)∪{v} 1 √ du dv h (k−1) u GraphSAGE h (k) v ← σ W • 1 din v ∑ u∈N (v)∪{v} h (k−1) u GIN h (k) v ← σ W • (1 + (k) ) • h (k−1) v + ∑ u∈N (v) h (k−1) u where h (k)</formula><p>v and W (k) are the GNN embedding for node v and the weight matrix at the layer k respectively, σ(•) is a non-linear activation function, N (v) is the neighbourhood of node v, dv and din v are the renormalised degree and in-degree of node v (see Kipf and Welling <ref type="bibr" target="#b8">[9]</ref>), and (k) is GIN's central node weighting parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.2.">Transfer Learning</head><p>Deep neural network and machine learning models are usually trained for a specific task from a random initialisation of their parameters. If the task or nature of the input data changes, the network must be retrained from scratch. This retraining differs from humans, who reuse past knowledge and apply it to new contexts and tasks. The ability to reuse knowledge beyond the context in which it was learnt is known as transfer learning <ref type="bibr" target="#b12">[13]</ref>. Transfer with neural networks can be achieved by simply reusing a previously-trained network's weights. The transferred weights can either be used as a starting point for training a network (fine-tuned transfer), or used as fixed features on the target task (frozen-weight transfer). To formalise transfer learning, Pan and Yang <ref type="bibr" target="#b13">[14]</ref> define the notion of domains and tasks as used in this paper.</p><p>Taylor and Stone <ref type="bibr" target="#b12">[13]</ref> present various metrics for the evaluation of transfer learning, including Transfer Ratio, Jumpstart and Asymptotic Performance. The Transfer Ratio is the ratio of the total cumulative performance of the transfer learner to the base learner, while Jumpstart is the initial performance improvement by the transfer over the base learner. Asymptotic Performance refers to the improvement made in the final learnt performance in the target task. The figure below describes these metrics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training time</head><note type="other">Performance</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.3.">Related Work</head><p>Bengio <ref type="bibr" target="#b14">[15]</ref> notes that deep learning algorithms seem well suited to transfer learning through learning 'abstract' representations. Knowledge is represented at multiple levels, with higher level representations learnt as compositions of lower level features <ref type="bibr" target="#b15">[16]</ref>. Kornblith et al. <ref type="bibr" target="#b16">[17]</ref> and Huh et al. <ref type="bibr" target="#b17">[18]</ref> investigate transfer learning with CNNs pretrained on ImageNet <ref type="bibr" target="#b18">[19]</ref> and find that networks train faster, and achieve improved accuracy as a result.</p><p>Partly due to recency, and partly due to the multitude of approaches, not much research exists which investigates transfer learning for GNNs. Hamilton <ref type="bibr" target="#b19">[20]</ref> note that little success has been achieved by pretraining GNNs. This may be due to the fact that randomly initialised GNNs extract features that are just as useful as a trained network's <ref type="bibr" target="#b20">[21]</ref>.</p><p>However, Lee et al. <ref type="bibr" target="#b21">[22]</ref> do propose a framework to transfer spectral information between source and target tasks for node classification. Experiments on real-world datasets show transfer is most effective when the source and target graphs are similar. Hu et al. <ref type="bibr" target="#b22">[23]</ref> develop a framework to effectively pretrain GNNs for downstream tasks by pretraining a GNN at the level of individual nodes and the entire graphs. Thus, they describe two techniques to exploit node-level knowledge: context prediction and attribute masking; as well as two approaches for graph-level pretraining. More recently, Dai et al. <ref type="bibr" target="#b23">[24]</ref> present AdaGCN: a framework for transfer learning based on adversarial domain adaption with GCNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Materials and Methods</head><p>Errica et al. <ref type="bibr" target="#b24">[25]</ref> note that the experimental settings from GNN research papers are often ambiguous, and the results are not reproducible. Reproducibility is, therefore, a core objective, and as such, we provide the code for our experiments. We utilise the PyTorch-Geometric <ref type="bibr" target="#b25">[26]</ref> library for efficient GPU-optimised implementations of the three selected GNNs <ref type="bibr" target="#b26">[27]</ref>. We track our experiments using Comet.ml, and make them publicly available for transparency.</p><p>Our experiments pretrain GNNs on various source tasks and fine-tune them on a target task. All experiments are conducted in a fully supervised setting. The experiments track the performance of pretrained models as well as the randomly-initialised models across training. This methodology allows us to compare transferability in terms of the transfer learning metrics described earlier. We make use of synthetic and real-world data in our experiments. To avoid the problems of a lack of meaningful data-splits <ref type="bibr" target="#b24">[25]</ref> and the instability of training on small graph datasets <ref type="bibr" target="#b27">[28]</ref>, we make use of the Open Graph Benchmark (OGB) <ref type="bibr" target="#b28">[29]</ref> datasets for our real-world datasets. To ascertain the statistical significance of our results, we employ a pairwise two-sided t-test for identical means of independent sample statistics with the alternative hypothesis 'greater than' throughout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Node Classification Experimental Design</head><p>Node classification involves assigning a class label to an unlabelled node in a graph. Solving this problem involves structural properties of graphs (e.g. node degree), the node's attributes, or some relationship between them. Practical applications include real-world contexts, such as social networks <ref type="bibr" target="#b29">[30]</ref> or knowledge graphs <ref type="bibr" target="#b31">[31]</ref>. Below we describe the datasets and experimental methodology used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Synthetic Data</head><p>For meaningful classifications, instances within a single class should be similar. A graph where nodes in the same class are densely connected is said to contain strong community structure.</p><p>The Modularity, defined as</p><formula xml:id="formula_1">M = 1 2|E| ∑ ij a ij − d i d j 2|E| δ(i, j),</formula><p>is one measure of a graph's community structure with respect to the structure of the network <ref type="bibr" target="#b32">[32]</ref>, where δ(i, j) = 1 if v i and v j belong to the same class, and 0 otherwise. The Within Inertia ratio is another measure of the community structure that takes into account node attribute values <ref type="bibr" target="#b33">[33]</ref>. Given a partition of the graph's vertices P, the Within Inertia is given by:</p><formula xml:id="formula_2">I = ∑ C∈P ∑ v∈C dist(x v , g C ) 2 ∑ v∈V dist(x v , g) 2 ,</formula><p>where dist(x v i , x v j ) is the Euclidean distance between node attribute vectors, g C is the centre of gravity of the vertices in C, and g the global centre of gravity of all vertices.</p><p>To generate synthetic datasets, we make use of DANCer: a generator for dynamic attributed networks with community structure <ref type="bibr" target="#b33">[33]</ref>. The generator produces graphs with communities using micro-operations (local operations such as removing a node) and macrooperations (community-level operations such as splitting a community). The generator is designed with both structural and attribute homophily <ref type="bibr" target="#b34">[34]</ref> in mind. It also models phenomena such as preferential attachment, where nodes are more likely to connect with nearby or highly connected nodes <ref type="bibr" target="#b35">[35]</ref>. These properties make DANCer an ideal generator for our purposes.</p><p>The authors provide four benchmark configurations: one for each combination of strong and weak structural and attribute community structure. We fix a single target task with these datasets (Configuration 4), and pretrain on the other three configurations as described in Table <ref type="table">1</ref>. The parentheses indicate the strength of Modularity and Within Inertia respectively -e.g. M ↑ I ↓ indicates strong Modularity and weak Within Inertia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modularity Within Inertia</head><p>Configuration 1</p><formula xml:id="formula_3">(M ↑ I ↑ ) Strong 0.64 Strong 0.37 Configuration 2 (M ↑ I ↓ ) Strong 0.64 Weak 0.47 Configuration 3 (M ↓ I ↑ ) Weak 0.32 Strong 0.39 Configuration 4 (M ↓ I ↓ ) Weak 0.28 Weak 0.99 Table 1:</formula><p>The four configurations of the synthetic node classification datasets. The average modularity and Within Inertia ratios are computed on the generated datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">Real-World Data</head><p>A common node classification domain is citation networks, where each node in the graph represents a publication, and edges indicates citations. We perform several transfer learning experiments using OGB real-world citation networks, and complement the experiments with synthetic data. In particular, we select Arxiv and MAG-both are directed citation networks, where each node has an attribute vector containing a 128-dimensional word embedding of the paper. In addition, a paper's year of publication is also associated with its node in the network.</p><p>Arxiv contains 169,343 Computer Science papers; and the task is to predict which of the 40 subject areas a paper belongs to. MAG is taken from a subset of the Microsoft-Academic-Graph <ref type="bibr" target="#b36">[36]</ref>, and contains four types of node entities: papers, authors, institution and field of study. For consistency we will only make use of the papers, which consist of 736,389 nodes, making it a much larger and more complex network than Arxiv. The task here is to predict which of 349 venues (conferences or journals) each paper belongs to. OGB also provides model evaluators, which use the standard accuracy score.</p><p>To evaluate how MAG transfers to itself, we split it into a source and a target graph. Papers from 2010-2014 are placed in the source split, and those from 2015-2019 belong to the target split. Any edges between nodes in separate splits are removed. Table <ref type="table" target="#tab_0">2</ref> lists the statistics of the above datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3.">Experimental Methodology</head><p>Table <ref type="table">3</ref> presents the source and target tasks for our experiments using both real-world and synthetic datasets. We evaluate transfer from both Arxiv and the MAG source split to the target MAG split. Lastly, to investigate how important attributes are for our node classification tasks, we damage the node attributes for both Arxiv and the MAG source graph.</p><p>To damage the attributes, we replace the attributes with Gaussian distributed random noise with a mean of 0 and a standard deviation of 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>#</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source Task Target Task</head><p>Real-world</p><formula xml:id="formula_4">1 Base [Random seed] MAG (Target Split) 2 Arxiv 3 Arxiv [Damaged features] 4 MAG (Source split) [Old layer] 5 MAG (Source split) 6 MAG (Source split) [Damaged features] Synthetic 1 Base [Random seed] Configuration 4 (M ↓ I ↓ ) 2 Configuration 1 (M ↑ I ↑ ) 3 Configuration 2 (M ↑ I ↓ ) 4 Configuration 3 (M ↓ I ↑ )</formula><p>Table <ref type="table">3</ref>: Experiments conducted for node classification using both real-world and synthetic datasets.</p><p>We perform 10 runs of each of the six sets of experiments for each of the three GNNs. We use the same network architecture used by OGB <ref type="bibr" target="#b28">[29]</ref> for their experiments on Arxiv and MAG, which allows us to compare the performance for the base models as a sanity check. The network comprises of three GNN layers: with an input dimensionality of 128, an output dimensionality of 349 (for MAG), and a hidden dimensionality of 256. We train the networks on the target task for 2000 epochs using the Adam optimiser <ref type="bibr" target="#b37">[37]</ref>. The best performing learning rate for each GNN is selected and fixed across our 6 experiment sets. GCN, GraphSAGE and GIN are all trained with a learning rate of 0.001 in this case.</p><p>For synthetic data experiments, we generate 10 unique graphs for each of Configurations 1, 2, and 3. Throughout, we use a single instance of Configuration 4 so that the target task is fixed. The task is 5-class node classification, and we train the models for 2000 epochs using the Adam optimiser with a learning rate of 0.01 for GCN and GraphSAGE, and 0.001 for GIN</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Graph Classification Experimental Design</head><p>Graph classification is the problem of categorising whole graphs. To investigate GNN transfer for graph classification, we conduct experiments involving both real-world and synthetic problems. We follow the same experimental procedure as for node classification. We want to evaluate whether a dataset contains graphs with community structure at a graph-level. With this aim, we present an extension of the concept of community structure from node to graph level for both structural and attribute properties. The Within Inertia is a general measure of community structure since it does not depend on graph-theoretic properties, but only on the Euclidean distance between data points. Given a dataset D, and a partition of classes P, a general form of the Within Inertia can be written as:</p><formula xml:id="formula_5">∑ C∈P ∑ i∈C dist(ρ i , g C ) 2 ∑ i∈D dist(ρ i , g) 2 , (<label>1</label></formula><formula xml:id="formula_6">)</formula><p>where ρ i is a graph property we are interested in measuring for graph i in class C, g C is the centre of gravity of the graphs in C, and g is the global center of gravity of all the graphs. The Euclidean distance dist(•) may be replaced by other distance metrics if the generation process is unknown <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b39">39]</ref>. We replace ρ i in the Eq. 1 with any property we want to measure community structure for. For attribute community structure we substitute the mean of each graph's attribute matrix, X, for ρ i :</p><formula xml:id="formula_7">I A = ∑ C∈P ∑ i∈C dist Xi , g C 2 ∑ i∈D dist Xi , g 2 .</formula><p>There is no consensus on how to approach structural community structure. There are numerous ways of measuring graph similarity <ref type="bibr" target="#b40">[40]</ref>, the most common being graph-edit distance <ref type="bibr" target="#b41">[41]</ref>. Another approach is to compare graph spectra. However, these methods are slow to compute and are thus not useful for measuring entire datasets with multiple graphs.</p><p>Modularity takes node degrees as a valuable property for determining community structure for nodes. Since we want to measure community structure at the graph level, we use the average node degree for a graph. We substitute the average node degree, di , for ρ i in Eq. 1. This serves as the structural community structure measure for our datasets:</p><formula xml:id="formula_8">I S = ∑ C∈P ∑ i∈C dist di , g C 2 ∑ i∈D dist di , g 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Synthetic Data</head><p>We present a novel process for generating synthetic datasets for the task of graph classification. To create a meaningful graph classification task, we parameterise a generator to control the community structure for both structure and attributes. To do this, we generate the dataset using the following four steps:</p><p>1.</p><p>Create a random n-class classification problem with a sample X and labels y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>For each label y i in y, generate several graphs and set their node attributes to the relevant example from X. Label these graphs with y i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>Optionally, swap the labels assigned to some of the graphs to weaken community structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>Optionally, replace node attributes with noise to weaken attribute community structure.</p><p>Steps 1 and 2 create community structure for attributes and structure respectively, and the final two steps weaken the community structure if selected to do so. Figure <ref type="figure" target="#fig_1">2</ref> illustrates this process, where the blue blocks create community structure and the pink blocks weaken it. The generator takes the following parameters as input:  The num_classes, n_per_class and n_features parameters allow for dataset level properties to be varied, while percent_swap and percent_damage influence the structural and attribute community structure respectively. We fix the size of the graphs at 30 nodes. More details regarding this generation process are provided in the appendix.</p><p>Similarly to the synthetic node classification datasets, we generate four configurations for each strong and weak community structure combination (see Table <ref type="table" target="#tab_3">4</ref>). We again indicate the strength of the Structural and Attribute Within Inertia using arrows. For example,  </p><formula xml:id="formula_9">I S ↑ I A ↓ indicates</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Real-World Data</head><p>Many real-world problems within bioinformatics and chemistry present themselves as graph classification problems. We select datasets from OGB where the task is molecular property prediction: BBBP and HIV. BBBP (Blood-Brain Barrier Penetration) is a physiological dataset where the task is to predict whether a given compound penetrates the blood-brain barrier or not <ref type="bibr" target="#b42">[42]</ref>. HIV is a biophysics dataset where the task is to predict whether a compound has anti-HIV activity or not.</p><p>Both datasets are pre-processed in the same manner and are both binary classification tasks. Nodes are atoms and chemical bonds are edges, while node attributes are 9-dimensional and contain information about atomic properties. BBBP is a much smaller dataset than HIV so we split HIV into a source and target split similar to the real-world node classification experiments: half the HIV is randomly sampled for each split, and this sample is kept fixed. A summary of the datasets is given in Table <ref type="table" target="#tab_5">5</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No. Graphs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Average</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3.">Experimental Methodology</head><p>We conduct experiments similar to those for real-world node classification. The target task, HIV target split is fixed, and we pretrain our GNNs on BBBP and the HIV source split and then evaluate them on the target task. We also evaluate the transfer performance where the models are pretrained on the source datasets with damaged node attributes: i.e. where the node attributes are replaced by Gaussian distributed random noise with a mean of 0 and a standard deviation of 1. The experiments are described in Table <ref type="table" target="#tab_6">6</ref> In Figure <ref type="figure">3</ref> we see the training curves for the experiments in Table <ref type="table">3</ref>. We note for GCN, all the pretrained model curves rise above the base model, indicating we have positive   <ref type="table" target="#tab_7">7</ref> we note that only GIN and GCN have significant transfer from the completely new Arxiv dataset. Interestingly for GCN and GIN we note that Arxiv with damaged attributes, in absolute terms, performs somewhat better than just Arxiv, indicating that graph characteristics beyond attribute values are being used for transfer. Although some of the GraphSAGE Transfer Ratios are positive, these are not statistically better than the control. Turning to Table <ref type="table" target="#tab_8">8</ref> we note that GIN statistically always outperforms the other GNNs in this metric, indicating that GIN benefits the most from sharing knowledge from the source domains.</p><p>The MAG (Source split) [Old layer] tasks, in Table <ref type="table" target="#tab_7">7</ref>, have a greater Jumpstart than the rest, since the ouptut layer does not need to be retrained. We note that both GCN and GraphSAGE show significant Jumpstart with the completely new task Arxiv. In fact GraphSAGE exploits graph characteristics beyond attribute values as evidenced by the Arxiv [Damaged] results. In Table <ref type="table" target="#tab_8">8</ref> we note that GCN is either on par or significantly better than the other GNNs across all datasets on this metric; however, the result is not compelling.</p><p>All GNNs achieve significant positive asymptotic performance above the control on the new Arxiv task. GraphSAGE is inconsistent and does not always show transfer at the end of training. Despite the huge Jumpstart with MAG (Source split) [Old layer], only GIN retains a large absolute improvement in Asymptotic Performance. Both GCN and GIN once again exploit structural characteristics beyond the attribute values as evidenced by the Arxiv [Damaged] results. In Table <ref type="table" target="#tab_8">8</ref> we see a mixture of best performing GNNs with both GCN and GraphSAGE performing well on the completely new Arxiv task.</p><p>Takeaway 1: We have statistical evidence that transfer to a new task for node classification does occur across all metrics and GNNs. We demonstrate that GCN and GIN exploit structural rather than attribute information for achieving a positive Transfer Ratio and Asymptotic Performance; as does GraphSAGE for Jumpstart.</p><p>Next we consider our synthetic data to interrogate exactly which characteristics of graphs are being transferred by the above GNNs.  <ref type="table" target="#tab_9">9</ref>: Transfer metrics for synthetic node classification (10 runs). Bold results are not statistically greater than the best at p = 0.1. We evaluate significance for each model/metric combination.</p><p>In Table <ref type="table" target="#tab_9">9</ref> we note that the performance of GIN is as a result of the strong Modularity in Configurations 1 (M ↑ I ↑ ) and 2 (M ↑ I ↓ ). For GraphSAGE, this distinction between Modularity and Within Inertia is less clear as the results for Configurations 2 (M ↑ I ↓ ) and 3 (M ↓ I ↑ ) are statistically equivalent. The absolute performance of Configuration 3 (M ↓ I ↑ ) for GraphSAGE does suggest that Within Inertia is being exploited, but further investigation is required.</p><p>For Jumpstart, we are unable to see any significant difference in Table <ref type="table" target="#tab_9">9</ref> across all configurations for the GCN and GIN, making it unclear as to which of Modularity or Within Inertia is responsible for the positive transfer. GraphSAGE is significantly better in Configuration 1 (M ↑ I ↑ ) indicating both the Strong Modularity and Within Inertia are being exploited, supporting our real-world assertion that it exploits graph characteristics beyond attribute values. We do note in Table <ref type="table" target="#tab_9">9</ref> that all methods show positive Jumpstart on Configuration 1 (M ↑ I ↑ ) and 2 (M ↑ I ↓ ) suggesting that it is might be the Strong Modularity that all the GNNs are exploiting.</p><p>With Asymptotic Performance, our results do not allow us to say anything about which of structure or attributes GraphSAGE is exploiting. However, GIN, as evidenced in Figure <ref type="figure">5</ref> shows that GCN and GraphSAGE have more stable training curves than GIN, which is poorer in its performance. We observe clear transfer with GCN and GraphSAGE for both undamaged source tasks. In addition, we observe negative transfer for both damaged source tasks for GCN and GraphSAGE. GIN achieves positive self-transfer with HIV (Source split), and negative transfer with the remaining pretrainings. GIN's training curves also show a decay over training with BBBP pretrainings. The training curves indicate GraphSAGE and GIN suffer from worse negative transfer than GCN.</p><p>From Table <ref type="table" target="#tab_11">10</ref>, we see that across the transfer metrics, GCN and GraphSAGE achieved significant positive Transfer Ratios for HIV (Source split) and BBBP when compared to the control. This result indicates that they are able to transfer to a completely new task. GIN shows transfer from the similar task HIV (Source split) but not from the new task BBBP for any of the metrics. The biggest jumpstart is seen with HIV (Source split), which is understandable since it is a self-transfer task. None of the GNNs for any of the tasks show any significant  In Table <ref type="table" target="#tab_13">11</ref> we note that GraphSAGE is significantly better than all other GNNs across all metrics when transferring from the completely new BBBP task. For the transfer from the more similar HIV (Source split) results are mixed with GCN requiring further experimentation to determine outperformance.</p><p>Takeaway 3: We have significant statistical evidence to support that transfer happens for graph classification for both GCN and GraphSAGE across all metrics considered. We can also reject the hypothesis that the transfer is as a result of graph structure beyond the node attributes alone.</p><p>In the following section, we consider synthetic data to further investigate which structural characteristics of graphs are being transferred by the GNNs. is not achieved at the start of training, and that the pretrained model performs similarly to the base models initially. For graph classification on the synthetic data, we note from Table <ref type="table" target="#tab_12">12</ref> that none of the considered methods are able to achieve positive Jumpstart. These values are not discussed further here. The values for the asymptotic performance show similar results to the Transfer Ratios. Since there is minimal Jumpstart, any positive or negative transfer appears to be achieved by the GNNs towards the end of training. GraphSAGE and GIN both achieve positive Asymptotic Improvements as seen Table <ref type="table" target="#tab_12">12</ref>; however for GraphSAGE these values are not significantly different from negative transfer. We note for both GraphSAGE and GIN that the best transfer occurs for Configuration 5 (I S ↓ I A ↓ ) and 7 (I S ↓ I A ↑ ). In absolute terms, we might infer that due to the higher values in Configuration 7 (I S ↓ I A ↑ ), transfer results from the strong Attribute Within Inertia. However, these differences are not statistically significant and require further experimentation to confirm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Synthetic Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>When considering the graph classification Transfer Ratios in</head><p>The fact that no Jumpstart but improved Asymptotic Performance is observed is interesting: indicating that the transferred knowledge is not immediately useful but rather leads to better performance on downstream training. This observation would be supported by an argument that the transferred knowledge is not in the linear output layers but instead available in the deeper non-linear feature layers and exposed later in training.</p><p>Takeaway 4: There is significant evidence that GraphSAGE and GIN exploit Strong Attribute Within Inertia in order to achieve transfer. These results support our realworld findings with respect to GraphSAGE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion</head><p>Our research presented several contributions for understanding transfer learning using graph neural networks. We proposed a framework for evaluating transfer with GNNs by testing various source tasks on a fixed target task. We employed this framework, along with transfer learning metrics and notions of community structure, to evaluate the transferability of three useful GNNs: GCN, GraphSAGE and GIN. We tested and compared these models using real-world and synthetic graph data for node classification and graph classification contexts. In addition, we presented a novel procedure for generating synthetic datasets for graph classification.</p><p>All three of the GNNs we selected can transfer knowledge across training on the target task. GCN and GIN can exploit Strong Modularity in the source task, while GraphSAGE can leverage both structural and attribute information for node classification. For graph  classification, it is less clear that any model exploits either attribute or structural community structure, and they appear to leverage a combination of the two to achieve transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>This research begins to standardise the procedure and metrics for evaluating transfer learning using GNNs. Research on deep learning with graphs is expanding rapidly, and understanding how we can achieve effective transfer is therefore of great benefit. There remains large scope for future research. Our results considered node and graph classification; but our experiments for transfer learning may be extended to other common graph domains such as link prediction and edge classification. Another avenue for future research is to repeat our experiments with other GNNs such as Graph Attention Network <ref type="bibr" target="#b43">[43]</ref>, and other Graph Network types described by Battaglia et al. <ref type="bibr" target="#b5">[6]</ref>.</p><p>Author Contributions: Conceptualization, N.K., S.J. and T.v.Z.; methodology, N.K.; software, N.K.; validation, N.K.; formal analysis, N.K.; investigation, N.K., S.J. and T.v.Z.; resources, S.J. and T.v.Z.; data curation, N.K..; writing-original draft preparation, N.K., S.J. and T.v.Z.; writing-review and editing, N.K., S.J. and T.v.Z.; visualization, N.K.; supervision, S.J. and T.v.Z.; project administration, S.J. and T.v.Z.; funding acquisition, N.K., S.J. and T.v.Z. All authors have read and agreed to the published version of the manuscript.</p><p>Funding: This research was funded in part by the National Research Foundation of South Africa grant number 122158.</p><p>Data Availability Statement: Publicly available datasets were analyzed in this study. This data can be found here: https://ogb.stanford.edu.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflicts of Interest:</head><p>The authors declare no conflict of interest. swapping more graphs, the classes have less distinct average node degrees compared to one another, resulting in weaker community structure. This is demonstrated in Figure <ref type="figure" target="#fig_8">A2</ref>. The final step weakens attribute community structure by replacing node attribute vectors with random noise. This step is also optional, and is controlled by the percent_ damage parameter. This parameter also has a range of [0, 1], with a default value of 0, and defines the percentage of graphs which will have their node attributes damaged (replaced with random values). The higher the percentage is, the less distinct the attributes from different classes are from one another, and thus a weaker community structure. This is demonstrated in Figure <ref type="figure" target="#fig_9">A3</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. An illustration of the jumpstart, asymptotic performance, and transfer ratio metrics. The transfer ratio is computed using the area under the curve (AUC).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Generation process for synthetic graph classification datasets</figDesc><graphic url="image-31.png" coords="8,221.42,232.59,157.99,143.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>split) [No new layer] MAG (Source split) MAG (Source split) [Damaged]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .Figure 4 .</head><label>34</label><figDesc>Figure 3. Real-world node classification training curves</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1 -</head><label>1</label><figDesc>M ↑ I ↑ -0.203 ± 0.031 0.031 ± 0.044 -0.103 ± 0.026 C.2 -M ↑ I ↓ -0.108 ± 0.065 0.012 ± 0.039 -0.036 ± 0.038 C.3 -M ↓ I ↑ -0.176 ± 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>Figure 5. Real-world graph classification training curves</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>± 0.016 -0.023 ± 0.017 0.004 ± 0.016 C.7 -I S ↓ I A ↑ 0.027 ± 0.017 -0.021 ± 0.026 0.027 ± 0.013</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure A2 .</head><label>A2</label><figDesc>Figure A2. The effect of varying the percent_swap parameter on w.i. struct . The shaded region is the 1σ interval variance over 10 runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure A3 .</head><label>A3</label><figDesc>Figure A3. The effect of varying the percent_damage parameter on w.i. attr . The shaded region is the 1σ variance over 10 runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Statistics of real-world node classification datasets used in our experiments.</figDesc><table><row><cell></cell><cell>Nodes</cell><cell cols="5">Edges Features Modularity Within Inertia Classes Metric</cell></row><row><cell>Arxiv</cell><cell cols="2">169,343 1,166,243</cell><cell>128</cell><cell>0.495</cell><cell>0.890</cell><cell>40 Accuracy</cell></row><row><cell>MAG (Source)</cell><cell cols="2">402,598 1,615,644</cell><cell>128</cell><cell>0.299</cell><cell>0.813</cell><cell>349 Accuracy</cell></row><row><cell>MAG (Target)</cell><cell cols="2">333,791 1,390,589</cell><cell>128</cell><cell>0.286</cell><cell>0.806</cell><cell>349 Accuracy</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>strong Structural Within Inertia and weak Attribute Within Inertia. We select Configuration 8 (I S ↑ I A ↑ ) as our target task, and use the remaining three as source tasks.</figDesc><table><row><cell></cell><cell cols="4">w.i. struct w.i. attr percent_swap percent_damage</cell></row><row><cell>Configuration 5 (I S ↓ I A ↓ )</cell><cell>Weak</cell><cell>Weak</cell><cell>0.95</cell><cell>0.95</cell></row><row><cell cols="2">Configuration 6 (I S ↑ I A ↓ ) Strong</cell><cell>Weak</cell><cell>0.92</cell><cell>0.95</cell></row><row><cell>Configuration 7 (I S ↓ I A ↑ )</cell><cell>Weak</cell><cell>Strong</cell><cell>0.95</cell><cell>0.92</cell></row><row><cell cols="3">Configuration 8 (I S ↑ I A ↑ ) Strong Strong</cell><cell>0.92</cell><cell>0.92</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The four configurations of synthetic graph classification datasets.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Statistics of real-world graph classification datasets used in our experiments.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>. Experiments conducted for graph classification using both real-world and synthetic datasets.</figDesc><table><row><cell></cell><cell>#</cell><cell>Source Task</cell><cell>Target Task</cell></row><row><cell>Real-world</cell><cell>1 2 3 4 5</cell><cell>Base [Random seed] BBBP BBBP [Damaged features] HIV (Source split) HIV (Source split) [Damaged features]</cell><cell>HIV (Target Split)</cell></row><row><cell>Synthetic</cell><cell>1 2 3 4</cell><cell>Base [Random seed] Configuration 5 (I S ↓ I A ↓ ) Configuration 6 (I S ↑ I A ↓ ) Configuration 7 (I S ↓ I A ↑ )</cell><cell>Configuration 8 (I S ↑ I A ↑ )</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table><row><cell>Source Task → MAG-Target (Spectral Dist.)</cell><cell>Model</cell><cell>Transfer Ratio</cell><cell>Jumpstart</cell><cell>Asymptotic Performance</cell></row><row><cell>MAG-Source [Old layer]</cell><cell cols="2">GCN G'SAGE 0.044 ± 0.041 0.042 ± 0.007 GIN 0.183 ± 0.008</cell><cell cols="2">0.228 ± 0.023 0.003 ± 0.003 0.239 ± 0.018 0.001 ± 0.010 0.226 ± 0.004 0.024 ± 0.002</cell></row><row><cell></cell><cell>GCN</cell><cell>0.032 ± 0.011</cell><cell cols="2">0.001 ± 0.002 0.002 ± 0.003</cell></row><row><cell>MAG-Source</cell><cell cols="2">G'SAGE 0.046 ± 0.041</cell><cell cols="2">0.000 ± 0.001 0.003 ± 0.011</cell></row><row><cell></cell><cell>GIN</cell><cell cols="3">0.089 ± 0.007 -0.001 ± 0.004 0.009 ± 0.001</cell></row><row><cell></cell><cell>GCN</cell><cell>0.021 ± 0.007</cell><cell cols="2">0.001 ± 0.001 0.008 ± 0.002</cell></row><row><cell>Arxiv</cell><cell cols="2">G'SAGE 0.028 ± 0.040</cell><cell cols="2">0.001 ± 0.001 0.007 ± 0.010</cell></row><row><cell></cell><cell>GIN</cell><cell cols="3">0.048 ± 0.009 -0.001 ± 0.004 0.005 ± 0.001</cell></row></table><note>Transfer metrics for real-world node classification experiments (10 runs). Bold results are positive and statistically greater than the control at p = 0.1. We evaluate significance for each model/metric combination.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Transfer metrics for real-world node classification experiments (10 runs).</figDesc><table /><note>Bold results are not statistically greater than the best at p = 0.1. We evaluate significance for each source-task/metric combination.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 ,</head><label>9</label><figDesc>is able to build on the Modularity from Configuration 1 (M ↑ I ↑ ) as the Weak Inertia of Configuration 2 (M ↑ I ↓ ) is not significantly better.</figDesc><table><row><cell>Takeaway 2:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>In general, GIN predominately exploits Strong Modularity for transfer- while GraphSAGE can exploit both Modularity and Within Inertia for Jumpstart-in support of our real-world data findings.</head><label></label><figDesc></figDesc><table><row><cell cols="2">3.2. Graph Classification Results</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">3.2.1. Real-World Data</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>Source Task → HIV-Target</cell><cell>Transfer Ratio</cell><cell>Jumpstart</cell><cell>Asymptotic performance</cell></row><row><cell>Control</cell><cell>HIV-Source [Damaged]</cell><cell>-0.002 ± 0.015</cell><cell>0.002 ± 0.017</cell><cell>0.014 ± 0.012</cell></row><row><cell></cell><cell>HIV-Source</cell><cell>0.065 ± 0.010</cell><cell>0.148 ± 0.009</cell><cell>0.031 ± 0.017</cell></row><row><cell>GCN</cell><cell>BBBP</cell><cell>0.036 ± 0.012</cell><cell>0.047 ± 0.016</cell><cell>0.026 ± 0.011</cell></row><row><cell></cell><cell>BBBP [Damaged]</cell><cell cols="2">-0.007 ± 0.013 -0.008 ± 0.021</cell><cell>0.000 ± 0.011</cell></row><row><cell>Control</cell><cell>HIV-Source [Damaged]</cell><cell cols="3">-0.069 ± 0.023 -0.029 ± 0.049 -0.038 ± 0.021</cell></row><row><cell></cell><cell>HIV-Source</cell><cell>0.030 ± 0.006</cell><cell>0.160 ± 0.016</cell><cell>0.011 ± 0.014</cell></row><row><cell>G'SAGE</cell><cell>BBBP</cell><cell>0.048 ± 0.008</cell><cell>0.072 ± 0.011</cell><cell>0.035 ± 0.009</cell></row><row><cell></cell><cell>BBBP [Damaged]</cell><cell cols="3">-0.064 ± 0.058 -0.067 ± 0.052 -0.042 ± 0.064</cell></row><row><cell>Control</cell><cell>HIV-Source [Damaged]</cell><cell>-0.197 ± 0.037</cell><cell cols="2">0.039 ± 0.048 -0.130 ± 0.051</cell></row><row><cell></cell><cell>HIV-Source</cell><cell>0.033 ± 0.016</cell><cell>0.186 ± 0.045</cell><cell>0.029 ± 0.040</cell></row><row><cell>GIN</cell><cell>BBBP</cell><cell>-0.059 ± 0.033</cell><cell cols="2">0.026 ± 0.046 -0.081 ± 0.075</cell></row><row><cell></cell><cell>BBBP [Damaged]</cell><cell cols="3">-0.157 ± 0.038 -0.013 ± 0.017 -0.136 ± 0.049</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Transfer metrics for real-world graph classification experiments (10 runs). Bold results are statistically greater than the control at p = 0.1. We evaluate significance for each model/metric combination.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 ,</head><label>12</label><figDesc>only GraphSAGE and GIN showe any positive transfer. Interestingly, for both models, the result is significant for Configuration 7 (I S ↓ I A ↑ ) indicating that it is the strong Attribute Within Inertia that is being transferred. The amount of Jumpstart achieved by all GNNs is low, indicating that transfer</figDesc><table><row><cell>Source Task → HIV-Target</cell><cell>Model</cell><cell>Transfer Ratio</cell><cell>Jumpstart</cell><cell>Asymptotic performance</cell></row><row><cell></cell><cell>GCN</cell><cell cols="2">0.065 ± 0.010 0.148 ± 0.009</cell><cell>0.031 ± 0.017</cell></row><row><cell>HIV-Source</cell><cell>G'SAGE</cell><cell cols="2">0.030 ± 0.006 0.160 ± 0.016</cell><cell>0.011 ± 0.014</cell></row><row><cell></cell><cell>GIN</cell><cell cols="2">0.033 ± 0.016 0.186 ± 0.045</cell><cell>0.029 ± 0.040</cell></row><row><cell></cell><cell>GCN</cell><cell cols="2">0.036 ± 0.012 0.047 ± 0.016</cell><cell>0.026 ± 0.011</cell></row><row><cell>BBBP</cell><cell>G'SAGE</cell><cell cols="2">0.048 ± 0.008 0.072 ± 0.011</cell><cell>0.035 ± 0.009</cell></row><row><cell></cell><cell>GIN</cell><cell cols="3">-0.059 ± 0.033 0.026 ± 0.046 -0.081 ± 0.075</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc>Transfer metrics for real-world graph classification experiments (10 runs). Bold results are not statistically greater than the best at p = 0.1. We evaluate significance for each source-task/metric combination.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12 :</head><label>12</label><figDesc>Transfer metrics for synthetic graph classification (10 runs). Bold results are not statistically greater than the best at p = 0.1. We evaluate significance for each model/metric combination.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Data Generation for Graph Classification</head><p>As described in the main article, we present a novel approach for synthetic graph classification datasets. We want to be able to control the level of community structure for both structure and attributes. To this end we generate the datasets in the following four steps:</p><p>Step 1: Create an attribute-level task The first step in the generation process is to create a attribute-level classification task, where vectors of length n_features are generated belonging to num_classes classes. The total number of these vectors is num_classes × n_per_class × 30, so that each node in each graph for each class has an attribute vector that can be assigned to it. We follow Morris et al. <ref type="bibr" target="#b44">[44]</ref> in generating this attribute level task using the scikit-learn library. 1 This tool generates the classification task using a modified algorithm from Guyon <ref type="bibr" target="#b45">[45]</ref>. After this step the attributes have a high level of community structure.</p><p>Step 2: Generate graphs and assign attributes to the graphs Now that we have attribute vectors that are labelled, we want to generate graphs to assign them to. We want the graphs in each class to have different average degrees, so that strong community structure in terms of w.i. struct exists. A common and useful graph generation algorithm is the Barabási-Albert (BA) model <ref type="bibr" target="#b46">[46]</ref>. The BA algorithm models preferential attachment, and takes in two parameters: the number of nodes n, and m the number of edges to attach from a new node to existing nodes while growing the graph. Varying the m parameter changes the connectivity of a generated graph, and thus its average node degree. This can be seen in the figure above. By assigning graphs generated with different values of m to different classes, we ensure structural community structure with respect to average node degrees.</p><p>Once n_per_class graphs are generated for num_classes with different m values, the corresponding attribute vectors from the previous step are assigned to graphs with the same label. At the end of this step, we have a labelled dataset with strong community structure for both nodes and attributes.</p><p>Step 3: Swap graphs This step weakens the structural community structure of the dataset by swapping graphs. This is an optional step, and the extent to which the community structure is weakened is controlled by the percent_swap parameter. This parameter may be in the range [0, 1], and the default value is 0 (no graphs are swapped). A random sample of pairs of graphs to swap is selected, corresponding to the specified percentage of the dataset. By</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A survey on deep learning: Algorithms, techniques, and applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pouyanfar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sadiq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Shyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Iyengar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ACM Computing Surveys (CSUR</publisher>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="1" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond Euclidean data</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Convolutional networks for images, speech, and time series. The Handbook of Brain Theory and Neural Networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Others</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<title level="m">others. Relational inductive biases, deep learning, and graph networks</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Using pre-training can improve model robustness and uncertainty</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Barabási</surname></persName>
		</author>
		<author>
			<persName><surname>Others</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network Science</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<title level="m">How Powerful are Graph Neural Networks? International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Transfer learning for reinforcement learning domains: A survey</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep learning of representations for unsupervised and transfer learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Unsupervised and Transfer Learning</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<title level="m">How transferable are features in deep neural networks? Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<title level="m">Do better ImageNet models transfer better? IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">What makes ImageNet good for transfer learning? NIPS Large Scale Computer Vision Systems Workshop (Oral) 2016</title>
		<author>
			<persName><forename type="first">M</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Graph representation learning</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Artifical Intelligence and Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<title level="m">Deep Graph Infomax. International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Transfer learning for deep learning on graph-structured data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<title level="m">Strategies for Pre-training Graph Neural Networks. International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Network Transfer Learning via Adversarial Domain Adaptation with Graph Convolution</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A fair comparison of graph neural networks for graph classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Errica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Podda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bacciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Micheli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fast Graph Representation Learning with PyTorch Geometric</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><surname>Others</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<title level="m">Bresson, X. Benchmarking graph neural networks</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Open Graph Benchmark: Datasets for Machine Learning on Graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Node classification in social networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bhagat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Social Network Data Analytics</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Towards a Definition of Knowledge Graphs</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ehrlinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wöß</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SEMANTiCS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Finding community structure in very large networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Clauset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review E</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">DANCer: dynamic attributed networks with community structure generation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Largeron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Mougel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Benyahia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">R</forename><surname>Zaïane</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Birds of a feather: Homophily in social networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mcpherson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Smith-Lovin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Sociology</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Microscopic evolution of social networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Backstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tomkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Microsoft academic graph: When experts are not enough</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kanakia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quantitative Science Studies</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<title level="m">A method for stochastic optimization. International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On information and sufficiency</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kullback</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Leibler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematical Statistics</title>
		<imprint>
			<date type="published" when="1951">1951</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The Kolmogorov-Smirnov test for goodness of fit</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Massey</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<date type="published" when="1951">1951</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Algorithms for graph similarity and subgraph matching</title>
		<author>
			<persName><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramdas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An exact graph edit distance algorithm for solving pattern recognition problems</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Abu-Aisheh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raveaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ramel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Martineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition Applications and Methods</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A Bayesian approach to in silico blood-brain barrier penetration modeling</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">F</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Teixeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O</forename><surname>Falcao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Modeling</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Graph attention networks. International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Faster kernels for graphs with continuous attributes via hashing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Data Mining</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Design of experiments of the NIPS 2003 variable selection benchmark</title>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Feature Extraction and Feature Selection</title>
				<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Emergence of scaling in random networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Barabási</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Albert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
