<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EasyNet: 100 Gbps Network for HLS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhenhao</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Systems Group</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dario</forename><surname>Korolija</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Systems Group</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gustavo</forename><surname>Alonso</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Systems Group</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EasyNet: 100 Gbps Network for HLS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/FPL53798.2021.00040</idno>
					<note type="submission">Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 06:13:54 UTC from IEEE Xplore. Restrictions apply.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The massive deployment of FPGAs in data centers is opening up new opportunities for accelerating distributed applications. However, developing a distributed FPGA application remains difficult for two reasons. First, commonly available development frameworks (e.g., Xilinx Vitis) lack explicit support for networking. Developers are, thus, forced to build their own infrastructure to handle the data movement between the host, the FPGA, and the network. Second, distributed applications are made even more complex by using low level interfaces to access the network and process packets. Ideally, one needs to combine high performance with a simple interface for both point-to-point and collective operations. To overcome these inefficiencies and enable further research in networking and distributed application on FPGAs, we first show how to integrate an open-source 100 Gbps TCP/IP stack into a state-of-the-art FPGA development framework (Xilinx Vitis) without degrading its performance. Further, we provide a set of MPI-like communication primitives for both point-to-point and collective operations as a High Level Synthesis (HLS) library. Our point-to-point primitives saturate a 100 Gbps link and our collective primitives achieve low latency. With our approach, developers can write hardware kernels in high level languages with the network abstracted away behind standard interfaces. To evaluate the ease of use and performance in a real application, we distribute a K-Means algorithm with the new stack and achieve a 1.9X and 3.5X throughput increase with 2 FPGAs and 4 FPGAs respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The network plays a fundamental role in data centers. Accordingly, cloud deployments have started to treat FPGAs as first-class processing components with direct network access such as in the Microsoft's Catapult <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> or Amazon's AQUA <ref type="bibr" target="#b2">[3]</ref>. As Microsoft's BrainWave <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> and KV-Direct <ref type="bibr" target="#b5">[6]</ref> projects indicate, support for data center networking is a key step to go beyond the FPGA as a co-processor.</p><p>FPGAs are increasing their programmability by providing high-level synthesis (HLS) languages to enable software developers trained in conventional programming languages such as C++ to also use FPGAs. The most common vendor development frameworks, Xilinx Vitis <ref type="bibr" target="#b6">[7]</ref> and Intel Quartus <ref type="bibr" target="#b7">[8]</ref>, support HLS and follow a clear trend towards higher abstractions. For instance, they all now have a far simpler way to manage data movement between host, FPGA memory, and application through the use of standard host APIs and hiding the underlying software-hardware interaction. When these two trends are put together, one area emerges where there are glaring gaps in terms of support: networking. While there is a growing number of research efforts targeting data center networking protocols for FPGAs <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b11">[12]</ref>, tool support for data center networking is still lacking. Current development frameworks do provide basic networking functionality through low-level link and physical layer protocols <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b14">[15]</ref>. Such support is not sufficient in a data center where reliability, automatic connection, and a large number of connections are needed. Only recently, support for UDP in Vitis <ref type="bibr" target="#b15">[16]</ref> has become available. Proprietary commercial implementations of network stacks <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b18">[19]</ref> exist but they are often not available to researchers, are limited in their functionality or performance (only 10 or 40 Gbps), and are not integrated with HLS. Moreover, the presence of a data center network stack is often not enough. On the CPU side, almost all distributed application development relies on high-level APIs such as OpenMP <ref type="bibr" target="#b19">[20]</ref> or MPI <ref type="bibr" target="#b20">[21]</ref> for communication and even more complex systems for distributed coordination, such as ZooKeeper <ref type="bibr" target="#b21">[22]</ref>. All these platforms are widely available and open-source, greatly facilitating the creation of new distributed applications. For FPGAs to become competitive, they must provide a similar infrastructure and be integrated with such systems. Recent work has tried to provide high-level abstractions for different communication patterns <ref type="bibr" target="#b22">[23]</ref>- <ref type="bibr" target="#b24">[25]</ref>. However, they either target a proprietary protocol for fixed typologies or provide a minimal subset of operations, thus lacking both generality and tool support.</p><p>To address these issues, we propose EasyNet, a system aiming to reduce the programming effort for distributed FPGA applications. As a first step, we integrate a 100 Gbps opensource TCP/IP stack <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b25">[26]</ref> into Vitis <ref type="bibr" target="#b6">[7]</ref>, to enable HLS network programming. This integration faces two major challenges: <ref type="bibr" target="#b0">(1)</ref> The integration should be seamlessly compatible with the original design flow of the Vitis application, meaning that the instantiation of the network stack has to be hidden from the application developer; (2) The performance of the 100 Gbps network stack should not be affected although Vitis is optimized for bulk data transfer with aligned memory address <ref type="bibr" target="#b6">[7]</ref>, while network communication often doesn't have this memory access pattern. Besides, to raise the level of abstraction further, we develop a rich set of MPI-like communication primitives for point-to-point and collective operations. The primitives hide the interaction and control management within the network layer and they can be easily invoked from an HLS C library. We show that our primitives running on FPGA clusters can easily saturate a 100 Gbps network and achieve lower latency than software MPI running on a CPU cluster. To evaluate EasyNet, we present a case study based on distributing K-Means on an FPGA cluster. Compared to a single node implementation, we show a negligible overhead from communication and achieve 1.9X and 3.5X by using 2 and 4 FPGAs, respectively. EasyNet is open-source 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. FPGA Development Framework</head><p>Vitis <ref type="bibr" target="#b6">[7]</ref> is one of the state-of-the-art FPGA development frameworks. In Vitis, the FPGA is divided into two regions. The static region (shell), which contains common infrastructure, such as the DMA engine. The dynamic region contains the customized logic for user-defined kernels. Vitis abstracts data movement by automatically adding the necessary interconnects for these kernel interfaces to communicate with the rest of the platform. Combined with HLS, Vitis is a huge boost in productivity compared to older platforms such as Vivado <ref type="bibr" target="#b26">[27]</ref> or SDAccel <ref type="bibr">[28]</ref>. However, Vitis imposes constraints on kernel interfaces, and interconnects to the network are not supported, limiting its use in distributed applications. Thus, current development of distributed applications requires to build a customized infrastructure to handle data movement between host, FPGA memory and network using low level interfaces, which lacks support of HLS and portability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Distributed Communication Primitives</head><p>Point-to-point and collective communication operations, such as broadcast and reduce, are essential in distributed applications. Distributed application development in software relies on platforms that provide such primitives behind standard interfaces such that developers handle communication only through high-level interfaces. MPI <ref type="bibr" target="#b20">[21]</ref> is an example of such a platform providing both point-to-point and collective primitives, automatically maps the communication to the best network protocol, and, in some cases, even changes the algorithm used depending on the system size and workload. This is important as, for instance, reduce can be implemented either as a client-server for medium-size systems and messages, through a broadcast tree <ref type="bibr" target="#b28">[29]</ref> for larger systems, or with a tile-based algorithm <ref type="bibr" target="#b29">[30]</ref> to handle large message sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RELATED WORK</head><p>In the past, FPGAs were typically connected through pointto-point serial links with fixed topologies and with lightweight or proprietary protocols <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b30">[31]</ref>- <ref type="bibr" target="#b33">[34]</ref>. Many of these implementations build on top of low-level networking IP cores provided by Xilinx and Intel, such as the Ethernet Media Access Controller (MAC) <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b34">[35]</ref>.</p><p>The situation has changed since FPGAs are deployed in the data center, where they are directly connected with data center infrastructures (e.g., high bandwidth links and network switches) <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b35">[36]</ref>. Therefore, there is now a trend to build infrastructure for high-performance FPGA-based NICs <ref type="bibr" target="#b36">[37]</ref>- <ref type="bibr" target="#b38">[39]</ref> and to develop complete network stacks on FPGAs for protocols commonly used in data centers, such as UDP <ref type="bibr" target="#b15">[16]</ref>, TCP <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b39">[40]</ref> or RDMA <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b10">[11]</ref>. As pointed out above, networking itself is not enough. Accordingly, there is also a growing interest in implementing higher-level abstractions for communication. For instance, IBM's CloudFPGA <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref> proposes to convert software executable MPI code to hardware synthesizable code that runs over hardware UDP stack. Eskandari et al. <ref type="bibr" target="#b22">[23]</ref> proposed a system integrated with SDAccel [28], an older generation of the framework, that maps HLS kernels to different FPGAs by incorporating an open-source 10 Gbps TCP/IP stack <ref type="bibr" target="#b9">[10]</ref> and providing MPI-like point-to-point primitives. There are also efforts aiming at providing MPI-like communication abstractions targeting physical/link layer protocols and fixed topologies <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b42">[43]</ref>. Recently VNx <ref type="bibr" target="#b15">[16]</ref> has become available, which extends Vitis with UDP, allowing the developer to interact with the UDP kernel using HLS streaming interfaces. Our goal in this paper is to seamlessly integrate TCP/IP in Vitis, a reliable protocol far more complex than UDP, and provide a rich set of communication primitives containing both point-to-point and collective operations.</p><p>A wide range of use cases can benefit from EasyNet. For instance, Zhang et al. <ref type="bibr" target="#b43">[44]</ref> propose to use a pipelined ring-based FPGA cluster to accelerate convolutional neural networks. Owaida et al. <ref type="bibr" target="#b44">[45]</ref> partition inference over decision tree ensembles on an FPGA cluster. In both cases, these efforts target a pre-defined cluster topology with point-to-point serial links for lack of better infrastructure. More recent work has explored DNN inference using FPGA clusters <ref type="bibr" target="#b45">[46]</ref>- <ref type="bibr" target="#b47">[48]</ref>, offloading an SDN stack to an FPGA-based smart NIC <ref type="bibr" target="#b48">[49]</ref>, managing the scatter and gather problem in parallel data processing with FPGAs <ref type="bibr" target="#b49">[50]</ref>, or implemented key-value stores <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b50">[51]</ref>. Through EasyNet, such applications will become easier to develop by providing access from HLS to a 100 Gbs TCP/IP stack that is usable through a set of MPI-like interfaces without loss of performance or flexibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DESIGN OVERVIEW</head><p>EasyNet comprises two parts. First, it incorporates a 100 Gbps TCP/IP stack into a state-of-the-art FPGA development framework: Vitis (Section V). The seamless integration of the network stack into Vitis is challenging because Vitis limits how user kernels interface and interact with the rest of the platform. For instance, network pin assignment is not supported by Vitis. Further, memory access in Vitis is highly optimized for bulk data transfer while packet buffering in a network stack could have irregular memory access since packets with varying sizes don't come in order. Regardless of these challenges, the goal is for EasyNet to meet the following three constraints regarding the network stack. C1: The network infrastructure should be generic to a variety of applications rather than optimized for a concrete one. C2: The network infrastructure should be abstracted away from the application developer and usable from HLS. C3: The integration into Vitis should not reduce the performance of the network stack.</p><p>Second, EasyNet provides various communication primitives that hide the network layer and can be easily instantiated in an HLS design (Section VI). The goal is for such EasyNet primitives to satisfy the following constraints. C4: Each communication primitive should keep a low resource footprint while achieving high throughput or maintain low latency. C5: The primitives should be encapsulated in an HLS library callable as a function. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. NETWORK INFRASTRUCTURE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overall Infrastructure Architecture</head><p>The design of EasyNet aims to provide network functionality as a common infrastructure to different applications (C1). Therefore, we adopt a modular design principle, where different functionalities are separated into different kernels. The overall design is divided into three kernels (Figure <ref type="figure" target="#fig_0">1</ref>). CMAC Kernel. The CMAC kernel contains an IP block for the 100G Ethernet Subsystem, which needs to be configured for each board. A separate CMAC kernel increases the portability among different FPGA boards. This kernel bridges the whole infrastructure to the GT pins, which are the I/O pins towards the QSFP network interfaces. It also exposes two 512-bit AXI4-Stream interfaces to the network kernel for transmitting (Tx) and receiving (Rx) network packets. Network Kernel. The network kernel contains IP blocks of an open-source 100 Gbps TCP/IP stack <ref type="bibr" target="#b51">[52]</ref> supporting thousands of TCP/IP connections, window scaling and outof-order packet processing. It is clocked at 250 MHz to saturate the network bandwidth. The kernel contains two 512-bit memory-mapped streaming interfaces to two memory banks, which serve as temporary buffers for re-transmission of Tx data and buffering of Rx data respectively. User Kernel. The user kernel contains streaming interfaces to the network kernel and other interfaces that can be customized for each application. The user kernel can be written in various languages: RTL, C/C++, and OpenCL. The interconnects to the memory and the network are hidden from the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. User-Network Streaming Interfaces</head><p>Interacting with the network kernel involves manipulating several streaming interfaces. Figure <ref type="figure">2</ref> shows the interfaces on the Tx path. The user kernel can open active connections through the openConReq interface providing the destination's IP address and port. Through the openConRsp, the user kernel will be notified whether the connection has been established and receive the session ID of the new connection. Data transmission through an established connection requires a control handshake before the transmission of the payload. The user kernel has to first provide the session ID and the length of the data to the txDataReq interface and then the TCP module will return a response on the txDataRsp indicating potential errors and the remaining buffer space for that connection. If the txDataRsp doesn't return any error, the user kernel can send the payload to the txData interface. Therefore, to saturate the Fig. <ref type="figure">2</ref>: Streaming interfaces between user and network kernel. network rate, the control handshake of the next packet needs to be overlapped with the data transmission of the current packet, imposing more difficulties on the application developer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Integration into Vitis</head><p>Network Instantiation Automation. To satisfy C2, the major challenge is to make the connection from the CMAC kernel to the network pins without the involvement of the application developer. With Vitis, each kernel is synthesized to a Xilinx object and the interconnects for each kernel interface are automated according to a configuration file. However, connecting a kernel interface to the network pins is not allowed. Thus, we have to explicitly guide the back-end routing and placement tool <ref type="bibr" target="#b26">[27]</ref> to make the routing connection from the CMAC kernel to the GT pins<ref type="foot" target="#foot_1">2</ref> . To avoid user intervention, we add an extra step in the Vitis synthesis and compilation workflow by providing a post-synthesis TCL file that automatically makes the routing connection from the CMAC kernel to the GT pins. Maintaining Performance. To keep the performance of the original 100 Gbps implementation (C3), the major hurdle is the interaction between the network kernel and the memory banks. In a TCP/IP stack, the payloads are temporarily stored in a memory bank for re-transmission or buffering purposes. This requires a memory bandwidth of at least 100 Gbps if the goal is to saturate the 100 Gbps network link. However, Vitis is optimized for 64-byte aligned, sequential memory access. Unaligned memory access significantly decreases the memory bandwidth because it will trigger several aligned memory accesses. For each TCP/IP connection, an initial memory address is assigned and upcoming packets are stored with an offset from the initial memory address. First, the initial memory address is determined by the initial sequence number of the connection, which is a random number, making it most likely not aligned to 64 bytes. Second, in a default TCP/IP setting, the maximum segmentation size (MSS) is 1460 bytes, which is not multiple of 64 bytes, and network devices tend to pack small messages to match MSS to maximize network utilization. This leads to a throughput drop due to the unaligned access even if packets arrive in order and sequentially access the memory. To overcome these two inefficiencies, we first use a relative initial sequence number that results in a 64-byte aligned initial memory address and then tune the MSS of the hardware TCP/IP stack to 1408 bytes, which is the maximum multiple of 64 bytes lower than the default.</p><p>With these changes and adaptations, EasyNet encapsulates the 100 Gbs stack behind a relatively simple interface that is usable from HLS and without the developer having to deal with the details of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. COMMUNICATION PRIMITIVES</head><p>The interaction with the TCP/IP stack is still complex due to low-level streaming interfaces and control handshakes, as previously explained in subsection V-B. To provide an even higher level of abstraction, EasyNet also includes various communication primitives that can be called as a function from an HLS C library (C5). These primitives have an MPI-like interface and semantics with the underlying implementation being optimized to minimize resource utilization (C4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Connection Establishment</head><p>The open connection primitive takes a list of destination IP and port pairs as input arguments and issues a large amount of open connection request to the TCP/IP stack in a pipelined fashion. At the same time, it processes open connection status from the TCP/IP stack. This is useful in distributed applications to quickly connect to several nodes. Once a connection is established, the function writes the session ID, used to distinguish among different connections, into the session table. A boolean value of true is returned once all the connections are established. Similarly, the listen port primitive open ports according to a list of port numbers provided by the caller and returns true once all ports are successfully opened. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Point-to-point Primitives</head><p>The send and receive primitives are point-to-point operations that involve one connection between two processes. These primitives send/receive data with connection specified via session ID and support operating with either a data stream or a memory pointer. They support variable data widths while to saturate network rate requires a data width of 512 bits. One important feature is that these primitives can be used with other data processing functions in a data flow region, such that the network communication and the computation can be pipelined and overlapped for higher efficiency. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Collective Primitives</head><p>Implementing collective primitives requires manipulation of several connections on each node at the same time and timesharing of the TCP/IP stack. EasyNet implements several collective primitives: scatter, broadcast, gather, reduce and allreduce. Due to space limitations, we will focus on broadcast, reduce, and all-reduce. In EasyNet, we focus on optimizing latency for medium size messages. Therefore, we adopt an all-to-one (client-to-server) implementation for reduce and a one-to-all (server-to-client) implementation for broadcast. Our implementation can be used as a basic building block for other collective algorithms, such as a tree-based algorithm <ref type="bibr" target="#b29">[30]</ref>.</p><p>To set up connections for collective operations, the server node actively establishes all the connections to the clients using the open connection primitive. Once all connections are established, both the server and the client nodes can perform duplex data transmission.</p><p>1) Broadcast: One way to broadcast data is to use the send primitive multiple times over the target data in sequence to each client node. However, this will starve the later connections until sending through the previous connection completes.</p><p>To avoid this, we provide a simple yet effective broadcast primitive by interleaving the sending of small amounts of data to each client. A list of session IDs and the total number of bytes to be sent are provided when invoking the broadcast primitive. Internally, the primitive repeatedly reads a small portion of the data and stores it in a small yet fast on-chip temporary buffer. It iterates over the temporary buffer and sends data to different connections in a round-robin fashion. The primitive then reads the next data chunk once the previous chunk of data is sent to all connections. On the client-side, the application simply uses the receive primitive. 2) Reduce: In our implementation, all the client nodes send data to one server node for reduction. Figure <ref type="figure">3</ref> shows the architecture of the reduce primitive on the server node. It mainly contains two parts: (1) A gather module that collects data chunks from several connections in a round-robin manner.</p><p>(2) A reducer that performs parallel reduction at line rate.</p><p>One challenge of gathering data is to preserve connection order for receiving and storing data. We deploy a finite state machine (FSM) that processes packet notifications from the TCP/IP stack and keeps track of the available data buffered in the TCP/IP stack for each connection. Then the FSM issues read request of a fixed chunk of data (1 KB) to the TCP/IP stack for each connection in a round-robin fashion. In this way, we preserve the order of the received data at the application level in the granularity of the chunk size. If the connection in turn doesn't have enough data for a read, the FSM will wait until new data for that connection arrives and we rely on the TCP/IP stack to perform flow control of the unbalanced sending rate between connections. The gathered data is multiplexed to temporary FIFOs (4KB) according to port number. The total number of FIFOs (MAX SS in the listing) is a trade-off between the maximum amount of connections and the total resource utilization.</p><p>The reducer operates on data stored in the FIFOs in a roundrobin fashion. It is equipped with arithmetic units that can perform parallel reduction at network line rate on specified data granularity (WIDTH in the listing). However, if the FIFO  3) All-Reduce: For the all-reduce implementation, we adopt a classical approach where an all-to-one reduce is followed by a one-to-all broadcast. In this setting, we could use previous primitives to build the all-reduce function. On the server node, the all-reduce implementation can be realized with a reduce and a broadcast primitive, while on the clientside, the operation can be realized with a send primitive followed by a receive primitive, as shown in Figure <ref type="figure">4</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Setup</head><p>We run the experiments on a cluster containing two nodes. Each nodes has a CPU with 4 Intel Xeon Gold 6234 processors and 376 GB of memory, and 2 U280 FPGAs. All the CPUs and FPGAs are connected through a 100 Gbps Cisco Nexus 9336C-FX2 network switch. As a comparison, we also run equivalent primitives with OpenMPI 3.1.4 on a cluster containing Intel Xeon E5-2609 2.40 GHz processors with 128 GB RAM and a Mellanox QDR HCA 100 Gbs NIC. OpenMPI uses TCP/IP as the underlying communication protocol. We run each experiment 5 times and report the average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Micro Benchmarks 1) Latency:</head><p>We measure the latency to establish a varying number of connections between two FPGAs using the open connection primitive, as shown in Figure <ref type="figure" target="#fig_7">5a</ref>. We compare with the latency between an FPGA client and a CPU server, where the server processes are mapped to different cores. The latency  2) Throughput: We compare the throughput between 2 FPGAs and between an FPGA and a CPU using the send and receive primitives. In the latter case, we implement a receiver application on the CPU using TCP sockets, which is mapped to a single core since point-to-point operations involve single connection. As shown in Figure <ref type="figure" target="#fig_7">5b</ref>, the throughput between 2 FPGAs approaches 100 Gbps with a small data size(1 MB), showing that both our primitives work at network rate. In contrast, with a CPU receiver, it can only achieve half of the bandwidth and it takes a larger amount of data to reach peak performance due to the packet processing overhead in the software network stack. To saturate line rate on the CPU, it requires opening more connections and larger payload size.</p><p>3) Collective Operations: We benchmark the broadcast and all-reduce primitives using 4 FPGAs and we compare them to the corresponding communication primitives in MPI, as shown in Figure <ref type="figure" target="#fig_8">6</ref>. The evaluation is done with 32-bit fixedpoint data and with SUM as the reduce operation. We observe that EasyNet achieves lower latency than its MPI counterparts running on CPUs. Further, when the data size is small, the latency is dominated by the network time. As we increase the data size, we see an almost linear increase for both primitives although the increase is more marked on the CPU case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Application: K-Means</head><p>K-Means is a popular machine learning algorithm and is a common use case for hardware acceleration <ref type="bibr" target="#b52">[53]</ref>- <ref type="bibr" target="#b61">[62]</ref>. We examine the ease of use of EasyNet by distributing the computation of K-Means across multiple FPGAs. The algorithm mainly contains two steps in each iteration: sample assignment and center update. First, each sample is assigned to its closest center by calculating the squared Euclidean distance. For each cluster, a sum vector is used to accumulate the sample assigned to it and an assignment counter is used to count the number of assignments. Second, new centers of each cluster are calculated as the mean of the samples assigned by performing a division of the sum vector over the assignment counter.  Listing 1: Code snippet of EasyNet-KM server node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Single Node Baseline:</head><p>We employ an existing highlypipelined FPGA K-Means implementation: Flex-KM <ref type="bibr" target="#b62">[63]</ref>. For the sample assignment, the design contains 16 parallel pipelines, each containing a systolic array of 16 distance processors that can process one dimension of a sample per cycle. For the center update, it contains a collector that aggregates partial sum vectors and assignment counters from each pipeline and a divider for division. For a comparison of the programming effort between HDL and HLS, we reimplement the original Flex-KM HDL design with Vitis HLS.</p><p>2) Distributed Implementation: Distributing K-Means with EasyNet (EasyNet-KM) is simple. Each compute node completes the sample assignment step on its local partition of samples, producing a partial result of sum vectors and assignment counters. An extra aggregation step is needed to aggregate all partial sum vectors and assignment counters from all nodes. Listing 1 shows the code snippet of the server node of the EasyNet-KM. Notice that only two extra functions are required (line 3 to open connections and line 6 to perform all-reduce over the network) compared to a single node implementation.</p><p>3) Experiments: We use the Forest data set <ref type="bibr" target="#b63">[64]</ref> (581014 samples, 54 dimensions) with the number of clusters set to 7. The data set is partitioned equally across different nodes. Figure <ref type="figure" target="#fig_10">7a</ref> and figure <ref type="figure" target="#fig_10">7b</ref> show the runtime per iteration and the aggregated throughput of our EasyNet-KM implementation running with 1, 2 and 4 nodes. First, we observe that the network communication time is negligible compared to the computation time, proving the advantages of the low latency all-reduce primitive. Additionally, the single node throughput of our EasyNet-KM design is on a par with Flex-KM and we could only achieve a sub-linear increase of overall throughput Table <ref type="table" target="#tab_1">I</ref> shows lines of code for the Flex-KM in HDL and the EasyNet-KM in HLS. The advantage of programming with Vitis HLS over HDL is obvious and by adding few lines of code, we easily turned a single node application into a distributed version. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Resource Consumption</head><p>Table <ref type="table" target="#tab_2">II</ref> shows the resource consumption of the design. The CMAC and the network kernel occupy only less than 10% LUT and BRAM. The communication primitives consume a minimal amount of resources. The K-Means kernel has low DSP usage since the Flex-KM was designed to run on an old platform (HARP2 <ref type="bibr" target="#b64">[65]</ref>) and we follow the same configuration for an apple-to-apple comparison of the programming effort. The single-node performance of EasyNet-KM could be further optimized with more resources but we decided to focus instead on evaluating the ease of use of EasyNet. VIII. CONCLUSION Aiming to provide tool support and facilitate the development of distributed applications with FPGAs, EasyNet integrates a 100 Gbps network stack into a state-of-the-art FPGA development framework and provides a rich set of highperformance communication primitives using HLS. We show that with EasyNet, an application can be easily partitioned across an FPGA cluster by changing a few lines of code and achieving a performance boost with minimal communication overhead.</p><p>ACKNOWLEDGMENT We would like to thank Xilinx for their generous donation of the XACC FPGA cluster at ETH Zurich on which the experiments were conducted.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Overall infrastructure architecture.</figDesc><graphic url="image-1.png" coords="3,64.02,69.81,216.71,92.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 bool</head><label>1</label><figDesc>openConnection(int numCon, uint32 t * IP, int * port , ssStruct &amp;session, opnConStruct&amp;opnCon); 2 bool listenPort ( int numCon, int * port, ssStruct &amp;session, lstnPortStruct &amp;lstnPort ) ;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 void</head><label>1</label><figDesc>send(type * data , uint64 t byte , ssStruct session , TcpTxStruct&amp;TcpTx); 2 void recv(type * data , uint64 t byte , ssStruct session , TcpRxStruct&amp;TcpRx);</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1 void</head><label>1</label><figDesc>broadcast (type * data , uint64 t byte , ssStructs sessionID , TcpTxStruct&amp;TcpTx);</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 :Fig. 4 :</head><label>34</label><figDesc>Fig. 3: Hardware architecture of reduce primitive</figDesc><graphic url="image-4.png" coords="5,63.27,165.81,216.71,82.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 template&lt;int</head><label>1</label><figDesc>MAX SS, int WIDTH&gt; 2 void reduce sum(type * data, uint64 t byte , ssStruct session , TcpRxStruct&amp;TcpRx);</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>1 template&lt;int</head><label>1</label><figDesc>MAX SS, int WIDTH&gt; 2 void all reduce sum(type * data , uint64 t byte , ssStruct session , TcpRxStruct&amp;TcpRx, TcpTxStruct&amp;TcpTx); VII. EVALUATION</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Latency and throughput measurement.</figDesc><graphic url="image-8.png" coords="5,422.33,171.75,107.33,86.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 :</head><label>6</label><figDesc>Fig.6: Broadcast and all-reduce primitive comparison between EasyNet and MPI running with various data size between two FPGAs is more than one order of magnitude lower in all cases, showing the advantage of a hardware network stack and the efficiency of the primitives. Besides, the round trip time(RTT) measured as a ping-pong benchmark of 64-byte data between two FPGAs is 4.3 us while the RTT between two CPUs is 56 us.2) Throughput: We compare the throughput between 2 FPGAs and between an FPGA and a CPU using the send and receive primitives. In the latter case, we implement a receiver application on the CPU using TCP sockets, which is mapped to a single core since point-to-point operations involve single connection. As shown in Figure5b, the throughput between 2 FPGAs approaches 100 Gbps with a small data size(1 MB), showing that both our primitives work at network rate. In contrast, with a CPU receiver, it can only achieve half of the bandwidth and it takes a larger amount of data to reach peak performance due to the packet processing overhead in the software network stack. To saturate line rate on the CPU, it requires opening more connections and larger payload size.3) Collective Operations: We benchmark the broadcast and all-reduce primitives using 4 FPGAs and we compare them to the corresponding communication primitives in MPI, as shown in Figure6. The evaluation is done with 32-bit fixedpoint data and with SUM as the reduce operation. We observe that EasyNet achieves lower latency than its MPI counterparts running on CPUs. Further, when the data size is small, the latency is dominated by the network time. As we increase the data size, we see an almost linear increase for both primitives although the increase is more marked on the CPU case.</figDesc><graphic url="image-7.png" coords="5,312.51,171.75,107.33,86.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>1 static</head><label>1</label><figDesc>ap uint&lt;32&gt; center [DIM MAX] [CNTR MAX]; //On−chip memory for center 2 static ap uint&lt;64&gt; sum n cnt [DIM MAX * CNTR MAX + CNTR MAX]; // On−chip memory for sum and assignment counters 3 openConnection( numCon, IP, port , &amp;session, opnCon); 4 static ap uint&lt;32&gt; byte = (DIM MAX * CNTR MAX + CNTR MAX) * 8; //Set all−reduce data amount 5 read sample n assign dataflow(sample, center , sum n cnt, numSample, numCluster, numDim); //Data flow region to read sample and assign sample to cluster</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Runtime per iteration and overall throughput of EasyNet-KM with Forest data set due to the serial part of the computation, such as collecting partial results from each pipeline and center update.TableIshows lines of code for the Flex-KM in HDL and the EasyNet-KM in HLS. The advantage of programming with Vitis HLS over HDL is obvious and by adding few lines of code, we easily turned a single node application into a distributed version.TABLE I: Lines of code.</figDesc><graphic url="image-9.png" coords="6,312.66,74.27,107.11,90.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Lines of code.</figDesc><table><row><cell>Flex-KM(HDL)</cell><cell cols="2">EasyNet-KM(HLS)</cell><cell></cell></row><row><cell>Total</cell><cell cols="3">Computation Network Total</cell></row><row><cell>4463</cell><cell>396</cell><cell>48</cell><cell>444</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Resource consumption</figDesc><table><row><cell>Resources</cell><cell>LUT</cell><cell>BRAM</cell><cell>DSPs</cell></row><row><cell>CMAC</cell><cell>15,717 (1.21%)</cell><cell>18 (2.24%)</cell><cell>0 (0%)</cell></row><row><cell>Network</cell><cell>114,699 (8.80%)</cell><cell>417 (7.09%)</cell><cell>0 (0%)</cell></row><row><cell>Send</cell><cell>2,326 (0.22%)</cell><cell>4 (0.29%)</cell><cell>0 (0%)</cell></row><row><cell>All-reduce</cell><cell>8,312 (0.81%)</cell><cell>83 (6.22%)</cell><cell>9 (0.10%)</cell></row><row><cell>K-Means</cell><cell cols="3">166,821 (14.03%) 486 (28.44%) 329 (3.65%)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 06:13:54 UTC from IEEE Xplore. Restrictions apply.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">We choose Vitis shell XDMA 201920.3 since only the latest shell exposes the GT pins to the dynamic region</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="200" xml:id="foot_2">Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 06:13:54 UTC from IEEE Xplore. Restrictions apply.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="202" xml:id="foot_3">Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 06:13:54 UTC from IEEE Xplore. Restrictions apply.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A reconfigurable fabric for accelerating large-scale datacenter services</title>
		<author>
			<persName><forename type="first">A</forename><surname>Putnam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Caulfield</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in ISCA&apos;14</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A cloud-scale acceleration architecture</title>
		<author>
			<persName><forename type="first">A</forename><surname>Caulfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chung</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in MICRO&apos;16</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Aqua (advanced query accelerator) for amazon redshift</title>
		<ptr target="https://aws.amazon.com/redshift/features/aqua/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Serving dnns in real time at datacenter scale with project brainwave</title>
		<author>
			<persName><forename type="first">E</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fowers</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in IEEE MICRO&apos;18</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A configurable cloud-scale dnn processor for real-time ai</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fowers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ovtcharov</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in ISCA&apos;18</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Kv-direct: High-performance in-memory keyvalue store with programmable nic</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ruan</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in SOSP&apos;17</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Vitis application acceleration development flow documentation</title>
		<ptr target="https://www.xilinx.com/htmldocs/xilinx20202/vitisdoc/kme1569523964461.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Intel quartus prime standard edition user guide: Getting started</title>
		<ptr target="https://www.intel.com/content/www/us/en/programmable/documentation/yoq1529444104707.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Hardware tcp offload engine based on 10-gbps ethernet for low-latency network communication</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Kang</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in FPT&apos;16</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Scalable 10gbps tcp/ip stack architecture for reconfigurable hardware</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Alonso</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in FCCM&apos;15</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Strom: Smart remote memory</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in EuroSys &apos;20</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">40gbps multi-connection tcp/ip offload engine</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in WCSP&apos;11</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<ptr target="https://www.intel.com/content/dam/www/programmable/us/en/pdfs/literature/ug/ug20085.pdf" />
		<title level="m">Low latency 100g ethernet intel fpga ip core user guide</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Ultrascale+ devices integrated 100g ethernet subsystem v3.1</title>
		<ptr target="https://www.xilinx.com/support/documentation/ipdocumentation/cmacusplus/v31/pg203-cmac-usplus.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Aurora 64b/66b v11</title>
		<ptr target="https://www.xilinx.com/support/documentation/ipdocumentation/aurora64b66b/v112/pg074-aurora-64b66b.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Xup vitis network example (vnx)</title>
		<ptr target="https://github.com/Xilinx/xupvitisnetworkexample" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Enyx premieres 25g tcp and udp offload engines with xilinx virtex ultrascale+ 16nm fpga on bittwares xupp3r pcie board</title>
		<ptr target="https://www.enyx.com/blog" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Chevin technologies&apos;s tcp/ip</title>
		<ptr target="https://chevintechnology.com/ethernet-ip-2/ct1008-xgtcp/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Dini group&apos;s tcp/ip</title>
		<ptr target="https://www.synopsys.com/verification/prototyping/dini-products.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Openmp: an industry standard api for sharedmemory programming</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dagum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Menon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computational Science and Engineering</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Using mpi-2: Advanced features of the message passing interface</title>
		<author>
			<persName><forename type="first">W</forename><surname>Gropp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lusk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Consensus in a box: Inexpensive coordination in hardware</title>
		<author>
			<persName><forename type="first">Z</forename><surname>István</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sidler</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in NSDI&apos;16</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A modular heterogeneous stack for deploying fpgas and cpus in the data center</title>
		<author>
			<persName><forename type="first">N</forename><surname>Eskandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tarafdar</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>ser. FPGA&apos;19</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Streaming message interface: High-performance distributed memory programming on reconfigurable hardware</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">De</forename><surname>Matteis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>De Fine Licht</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in SC &apos;19</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Tmd-mpi: An mpi implementation for multiple processors across multiple fpgas</title>
		<author>
			<persName><forename type="first">M</forename><surname>Saldana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chow</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in FPL&apos;06</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Limago: An fpga-based open-source 100 gbe TCP/IP stack</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sidler</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in FPL&apos;19</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Vivado design suite hlx editions</title>
		<ptr target="https://www.xilinx.com/support/documentation/backgrounders/vivado-hlx.pdf[28" />
	</analytic>
	<monogr>
		<title level="m">Sdaccel environment user guide</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName><surname>Available</surname></persName>
		</author>
		<ptr target="https://www.xilinx.com/htmldocs/xilinx20191/sdacceldoc/itd1534452174535.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Optimization of collective reduction operations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rabenseifner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of High Performance Computing Applications</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Two-tree algorithms for full bandwidth broadcast, reduction and scan</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sanders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Speck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Träff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Elsevier Science Publishers B. V</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A transport-layer network for distributed fpga platforms</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in FPL&apos;15</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Latency-optimized networks for clustering fpgas</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bunker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Swanson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FCCM&apos;13</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Cube: A 512-fpga cluster</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mencer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Tsoi</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in SPL&apos;09</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Maxwell -a 64 fpga supercomputer</title>
		<author>
			<persName><forename type="first">R</forename><surname>Baxter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Booth</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in AHS&apos;07</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Enabling flexible network FPGA clusters in a heterogeneous cloud data center</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tarafdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in FPGA&apos;17</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Network-attached fpgas for data center applications</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weerasinghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Polig</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in FPT&apos;16</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">PANIC: A high-performance programmable NIC for multi-tenant networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Patel</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in OSDI&apos;20</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Corundum: An open-source 100-gbps nic</title>
		<author>
			<persName><forename type="first">A</forename><surname>Forencich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Snoeren</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Zilberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Audzevich</surname></persName>
		</author>
		<title level="m">Netfpga sume: Toward 100 gbps as research commodity</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Low-latency TCP/IP stack for data center applications</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>István</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Alonso</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in FPL&apos;16</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Programming reconfigurable heterogeneous computing clusters using mpi with transpilation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ringlein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Abel</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in H2RC&apos;20</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Zrlmpi: A unified programming model for reconfigurable heterogeneous computing clusters</title>
		<imprint/>
	</monogr>
	<note>in FCCM&apos;20</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A scalable fpga-based multiprocessor</title>
		<author>
			<persName><forename type="first">A</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Madill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FCCM&apos;06</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Energy-efficient CNN implementation on a deeply pipelined FPGA cluster</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in ISLPED &apos;16</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Application partitioning on FPGA clusters: Inference over decision tree ensembles</title>
		<author>
			<persName><forename type="first">M</forename><surname>Owaida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Alonso</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in FPL&apos;18</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">A configurable cloud-scale dnn processor for real-time ai</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fowers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ovtcharov</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in ISCA&apos;18</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Distributed recommendation inference on fpga clusters</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<idno>FPL&apos;21</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Fleetrec: Large-scale recommendation inference on hybrid gpu-fpga clusters</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Azure accelerated networking: Smartnics in the public cloud</title>
		<author>
			<persName><forename type="first">D</forename><surname>Firestone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Putnam</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Specializing the network for scatter-gather workloads</title>
		<author>
			<persName><forename type="first">C</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in SOCC&apos;20</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Caribou: Intelligent distributed storage</title>
		<author>
			<persName><forename type="first">Z</forename><surname>István</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Alonso</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in VLDB&apos;17</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Scalable network stack supporting tcp/ip, rocev2, udp/ip at 10-100gbit/s</title>
		<ptr target="https://github.com/fpgasystems/fpga-network-stack" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">An FPGA Implementation of K-means Clustering for Color Images Based on Kd-tree</title>
		<author>
			<persName><forename type="first">T</forename><surname>Saegusa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Maruyama</surname></persName>
		</author>
		<editor>FPL</editor>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">K-Means Clustering for Multispectral Images Using Floating-point Divide</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Leeser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FCCM</title>
				<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Experience with a Hybrid Processor: K-Means Clustering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gokhale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Frigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Supercomputing</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Algorithmic Transformations in the Implementation of K-means Clustering on Reconfigurable Hardware</title>
		<author>
			<persName><forename type="first">M</forename><surname>Estlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Leeser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FPGA</title>
				<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Novel Dynamic Partial Reconfiguration Implementation of K-means Clustering on FPGAs: Comparative Results with GPPs and GPUs</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Benkrid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Reconfig. Comput</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Melia: A MapReduce Framework on OpenCL-based FPGAs</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPDS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">K-means Implementation on FPGA for High-dimensional Data Using Triangle Inequality</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chow</surname></persName>
		</author>
		<editor>FPL</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Acceleration of K-means Algorithm Using Altera SDK for OpenCL</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Khalid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TRETS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Map-reduce Processing of K-Means Algorithm with FPGA-accelerated Computer Cluster</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K H</forename><surname>So</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ASAP</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Bis-km: Enabling any-precision kmeans on fpgas</title>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Alonso</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in FPGA&apos;20</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">A Flexible K-Means Operator for Hybrid Databases</title>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Istvan</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in FPL&apos;18</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Comparative Accuracies of Artificial Neural Networks and Discriminant Analysis in Predicting Forest Cover Types From Cartographic Variables</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Blackard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computers and Electronics in Agriculture</title>
				<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">A Reconfigurable Computing System Based on a Cache-Coherent Fabric</title>
		<author>
			<persName><forename type="first">N</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in ReConFig&apos;11</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
