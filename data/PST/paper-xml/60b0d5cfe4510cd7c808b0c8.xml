<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Bi-hemisphere Domain Adversarial Neural Network Model for EEG Emotion Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Wenming</forename><surname>Zheng</surname></persName>
							<email>zheng@seu.edu.cn</email>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Yuan</forename><surname>Zong</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Zhen</forename><surname>Cui</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiaoyan</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Child Development and Learning Science (Ministry of Education)</orgName>
								<orgName type="department" key="dep2">School of Information Science and Engineering</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<postCode>210096</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Development and Learning Science (Ministry of Education)</orgName>
								<orgName type="department" key="dep2">School of Biological Sciences and Medical Engineering</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<postCode>210096</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">Key Laboratory of Intelligent Perception and Systems for High-Dimensional Information (Ministry of Education)</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<postCode>210094</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Nanjing University of Information Science and Engineering Technology</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Bi-hemisphere Domain Adversarial Neural Network Model for EEG Emotion Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">23C6373497882D1AF2C6A7CED5AB2585</idno>
					<idno type="DOI">10.1109/TAFFC.2018.2885474</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2018.2885474, IEEE Transactions on Affective Computing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>EEG emotion recognition</term>
					<term>long short term memory (LSTM)</term>
					<term>cerebral hemisphere asymmetry</term>
					<term>adversarial network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a novel neural network model, called bi-hemisphere domain adversarial neural network (BiDANN) model, for electroencephalograph (EEG) emotion recognition. The BiDANN model is inspired by the neuroscience findings that the left and right hemispheres of human's brain are asymmetric to the emotional response. It contains a global and two local domain discriminators that work adversarially with a classifier to learn discriminative emotional features for each hemisphere. At the same time, it tries to reduce the possible domain differences in each hemisphere between source and target domains so as to improve the generality of the recognition model. In addition, we also propose an improved version of BiDANN, denoted by BiDANN-S, for subject-independent EEG emotion recognition problem by lowering the influences of the personal information of subjects to the EEG emotion recognition. Extensive experiments on the SEED database are conducted to evaluate the performance of both BiDANN and BiDANN-S. The experimental results have shown that the proposed BiDANN and BiDANN models achieve state-of-the-art performance in the EEG emotion recognition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>E MOTION, as a common mental phenomenon, is closely related to human's cognition and behavior <ref type="bibr" target="#b0">[1]</ref>. Although emotion can be easily captured by human beings, it is still hard to be understood by machines. As one of the most active research topics of affective computing <ref type="bibr" target="#b1">[2]</ref>, emotion recognition had received substantial attentions from computer vision and pattern recognition research communities. Basically, the responses of emotion can be roughly divided into the external and the internal responses. The typical external responses include facial expression, gesture or speech of human beings, and the typical internal responses include skin conductance response, heart rate, blood pressure, respiration rate, electroencephalograph (EEG), magnetoencephalogram (MEG) <ref type="bibr" target="#b2">[3]</ref>.</p><p>From the neuroscience view of point <ref type="bibr" target="#b3">[4]</ref>, there are some major brain cortex regions, e.g., the orbital frontal cortex, ventral medial prefrontal cortex, and amygdala, are closely related with emotions <ref type="bibr" target="#b4">[5]</ref>[6] <ref type="bibr" target="#b6">[7]</ref>, which provides us a potential way to decode emotion by recording human's brain signals over these brain regions. For example, by placing the EEG electrodes on the scalp, we can record the neural activities of the brain, which can be used to recognize human's emotions.</p><p>Traditional EEG emotion recognition system usually consists of two major parts, i.e., feature extraction part and classifier design part. EEG features can be extracted either from time domain, frequency domain, or from time-frequency domain <ref type="bibr" target="#b7">[8]</ref>. Jenke et al. <ref type="bibr" target="#b7">[8]</ref> provided a comprehensive survey on the EEG feature extraction approaches. For dealing with the classification problem, many EEG emotion recognition models and methods were proposed over the past several years <ref type="bibr" target="#b8">[9]</ref> <ref type="bibr" target="#b9">[10]</ref>. Zheng et al. <ref type="bibr" target="#b10">[11]</ref> proposed a novel group sparse canonical correlation analysis (GSCCA) method for simultaneous EEG channel selection and emotion recognition. Li et al. <ref type="bibr" target="#b11">[12]</ref> proposed a graph regularized sparse linear regression (GRSLR) method to deal with EEG emotion recognition problem. Recently, using deep learning methods for EEG emotion recognition had been widely adopted and had demonstrated better performance than traditional methods. In <ref type="bibr" target="#b12">[13]</ref>, Zheng et al. proposed to use Deep Belief Network (DBN) for EEG emotion classification. In <ref type="bibr" target="#b13">[14]</ref>, Song et al. used a graph to model the multichannel EEG features and then perform EEG emotion classification based on it.</p><p>Although many algorithms or models have been proposed for the EEG emotion recognition problem, most of them focused on the scenarios where both training and testing data come from the same domain, in which we usually assume that the feature distributions of training and testing data are similar. Under this assumption, the label information of the target data is predicted directly based on the classifier learned with the source domain data samples. For cross-domain EEG emotion recognition problems, however, many of the aforementioned EEG emotion recognition methods would fail because of the mismatch problem of the EEG feature distributions between the source domain and the target one. A typical example is the cross-subject EEG emotion recognition problem, in which the training and testing EEG data are from different subjects.</p><p>To deal with the challenging cross-subject EEG emotion recognition problem, Pandey et al. <ref type="bibr" target="#b14">[15]</ref> proposed a subject independent approach for EEG emotion recognition. Li et al. <ref type="bibr" target="#b15">[16]</ref> proposed another method for cross-subject EEG emotion recognition. Recently, Ganin et al. proposed a domain adversarial neural networks (DANN) to deal with the domain adaptation problems <ref type="bibr" target="#b16">[17]</ref>. It uses a domain discriminator as well as a feature generator to alleviate the differences of the feature distribution between the source domain and the target domain and at the same time generate domain-invariable data features without any class information from testing data. In our preliminary work of <ref type="bibr" target="#b17">[18]</ref>, we had proposed a novel neural network model that fits the asymmetry of emotional brain into the DANN structure meanwhile utilizing time information for EEG emotion recognition task.</p><p>On the other hand, from the neuroscience point of view, it is notable that the left and right hemispheres of human's brain are not entirely symmetrical <ref type="bibr">[19][20]</ref>[21] <ref type="bibr" target="#b21">[22]</ref>. In <ref type="bibr" target="#b22">[23]</ref>, Dimond et al. firstly found that there exists different 'emotional vision' between the right and the left hemispheres of the human brain, where the right hemisphere demonstrates to be more powerful in perceiving unpleasant emotion than the left one. In <ref type="bibr" target="#b23">[24]</ref>[25] <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr">Davidson et al.</ref> showed that the EEG signals of the left frontal cortex are closely related to positive emotions while the ones in the right frontal cortex are related to negative emotions. In <ref type="bibr" target="#b26">[27]</ref>, Costanzo et al. focused on the research of mood induction and language activation based on functional magnetic resonance imaging (fMRI) and found that sadness and happiness are processed by each hemisphere. Moreover, some researchers had also proposed methods to measure the differences between the two hemispheres, which were used to the depression detection and EEG monitoring <ref type="bibr" target="#b27">[28]</ref>[29] <ref type="bibr" target="#b29">[30]</ref>. Nevertheless, it is still an interesting topic of how to utilize the asymmetry property of brain to improve the emotion recognition performance.</p><p>Motivated by the recent success of the DANN method and the findings of neuroscience about the asymmetry property of human brain, in this paper we propose a novel deep neural network model, called bi-hemispheres domain adversarial neural network (BiDANN) model, to deal with EEG emotion recognition problem, which can be seen as an extension of our preliminary work of <ref type="bibr" target="#b17">[18]</ref>. In the BiDANN model, we design a global and two local domain discriminators that work adversarially with a classifier to learn discriminative emotional features of each hemisphere and at the same time to alleviate the domain differences between source and target domains on each hemisphere. Specifically, the BiDANN model consists of three major modules, i.e., the feature extractor module, the classifier module, and the domain discriminator module. The major goal of feature extractor module is to extract discriminative deep features based on the original EEG features extracted from each cerebral hemisphere, the classifier module aims to predict the emotion class information based on the EEG deep features, and the target of the domain discriminator module is to alleviate the domain differences between the source and target domains. Moreover, considering that the subject's information, e.g., the personal identity information, may influence the EEG emotion recognition performance, we also propose an improved version of BiDANN, denoted by BiDANN-S, to deal with subject-independent EEG emotion recognition. The basic idea is to ensure that the extracted EEG features are robust to the variation of subjects so as to improve the generality of the recognition model.</p><p>The remainder of this paper is organized as follows: In section II, we give a brief overview of preliminary work. In sections III and IV, we propose the BiDANN model and the BiDANN-S model, respectively, for EEG emotion recognition. The experiments will be presented in section V, and finally, in section VI, we will conclude the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PRELIMINARIES</head><p>In this section, we will briefly review the preliminary work of long Short term memory (LSTM) method <ref type="bibr" target="#b30">[31]</ref> and the domain adversarial neural networks (DANN) method <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. LSTM</head><p>In the traditional neural networks methods, the inputs are independent with each other and hence they neglect sequence information. In contrast to the traditional neural network methods, the recurrent neural networks (RNN) method <ref type="bibr" target="#b32">[33]</ref> aims to map the input sequence into a series of hidden states through a complex dynamics transform with a chain of repeating modules of neural network and remember and forget the information in the whole sequence adaptively <ref type="bibr" target="#b33">[34]</ref>. The LSTM network <ref type="bibr" target="#b30">[31]</ref> is a special type of RNN, which replaces the repeating module of RNN with a more complex structure consisting of three gates and a cell state to deal with long-term dependence problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. DANN</head><p>The DANN model was proposed by Ganin et al. <ref type="bibr" target="#b16">[17]</ref> to deal with domain adaptation problem under the assumption that the desired features should not be able to discriminate the data samples between the source and target domains. To this end, DANN borrows the idea of generative adversarial networks (GAN) <ref type="bibr" target="#b31">[32]</ref> whose goal is to design an adversarial process that simultaneously learns a generative model and a discriminative model, in which the discriminative model estimates the probability of a sample coming from real data set or a fake generative data set whereas the generative model aims to estimate the distribution of real training data. Based on the GAN method, the DANN method aims to generate domaininvariant data features that are discriminative for the classification task whereas in-discriminative for the shift between the source and target domains, in which a gradient reversal layer (GRL) is introduced for this purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. BIDANN FOR EEG EMOTION RECOGNITION</head><p>In this section, we will firstly address the BiDANN model and then extend it to deal with subject-independent EEG emotion recognition problem. Before addressing the detailed information of the model, we will firstly provide a list of the major notations and the corresponding definitions in Table <ref type="table" target="#tab_0">I</ref>. In addition, we use the subscript notations of f , c, d to denote the feature extractor, the classifier and the discriminator, respectively. Similarly, we use the superscript notations of l, r to denote left and right hemisphere, respectively, and use the subscript notations of S, T to denote subscript of source and target domain, respectively. For example, we use X l and X S to denote the EEG feature matrix associated with the left hemisphere and the EEG feature matrix associated with the source domain, respectively, and use X l S to denote the EEG feature matrix corresponding to the left hemisphere of the source domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. BiDANN</head><p>The target of BiDANN is to alleviate the possible feature distribution difference between source and target domains, either in each hemisphere or the whole brain cortex area. To achieve this goal, we borrow the basic idea of the DANN method by leverage the adversarial operation between the source and the target domains into the discriminative EEG feature learning module, and also make use of the neuroscience findings that the left and right hemispheres of human's brain are asymmetric to the emotional response to further enhance the discriminative ability of the EEG features. Fig. <ref type="figure" target="#fig_0">1</ref> illustrates the framework of the BiDANN model, which consists of the following major parts: (1) two feature extractors (f l and f r );</p><p>(2) a global discriminator (D g ) and two local discriminators (D l and D r ); (3) three GRL, and a classifier (C). In Fig. <ref type="figure" target="#fig_0">1</ref>, the black lines and arrows refer to source domain path that the EEG data matrix X S is from the source domain, while the red lines and arrows refer to target domain path that the EEG data matrix X T is from the target domain. In BiDANN model, the two feature extractors f l and f r aim to capture the dynamic features of two hemispheric EEG signal separately. The three discriminators are trained on a binary domain label set D = {0, 1}, in which the domain labels of source samples are set to 0 while the domain labels of target samples are set to 1. The global discriminator aims to narrow the features' distribution gap between source and target domains, while the two local discriminators complementally eliminate the left and right hemispheric features' distribution difference between source and target domains. The use of GRL is to maximize the loss of discriminators by leaving the input data unchanged during forward propagation and reversing the gradient by multiplying it with a negative scalar during backpropagation <ref type="bibr" target="#b16">[17]</ref>. The classifier maps the emotionrelated and domain-invariant features into class label space to predict the class labels. In what follows, we will specify the BiDANN model with respect to feature extractor, local discriminator, global discriminator and classifier:</p><p>1) Feature extractor: The goal of feature extractor is to extract more discriminative features from handcraft EEG features so as to improve the EEG classification performance. The whole feature extraction procedure is summarized in Fig. <ref type="figure" target="#fig_1">2</ref>, which consists of the handcraft EEG feature extraction part and the discriminative deep feature learning part.</p><p>To extract the handcraft EEG features, we firstly decompose the EEG signals into five frequency bands, i.e., δ band (1-3 Hz), θ band (4-7 Hz), α band (8-13 Hz), β band (14-30 Hz), and γ band (31-50Hz). For each EEG frequency band, we segment the EEG signals of each trial into some clips using a non-overlapped Hanning window <ref type="bibr" target="#b12">[13]</ref>, in which the EEG signals of each clip is used to extract 62 differential entropy (DE) features, resulting in a 310-dimensional feature vector. To explore the temporal dependence among the EEG feature vectors, every 9 neighboring EEG feature vectors determined by a slicing window with the size of 9 EEG clips are chosen to form an EEG feature matrix, denoted by X ∈ IR dx×tx , and this matrix is used to serve as one sample. By slicing the window with an interval of one EEG clip, we finally obtain a set of EEG feature matrices as the input EEG data.</p><p>Suppose that</p><formula xml:id="formula_0">X l = [x l 1 , • • • , x l tx ] ∈ IR d l x ×tx and X r = [x r 1 , • • • , x r tx ] ∈ IR d r</formula><p>x ×tx denote two handcraft EEG feature matrices of an EEG clip sequence associated with the left and right hemisphere, respectively, where d l x and d r x denote the dimensions of the EEG feature vector, and t x is the number of EEG clips. Let</p><formula xml:id="formula_1">X = [X l , X r ] = [x l 1 , • • • , x l tx , x r 1 , • • • , x r tx ].</formula><p>To capture the temporal dependence among the EEG feature sequence, we adopt the LSTM model to extract more discriminative EEG features, resulting in the following LSTM feature matrices Y l and Y r :</p><formula xml:id="formula_2">Y l = [y l 1 , • • • , y l tx ] = [f (x l 1 ), • • • , f (x l tx )],<label>(1)</label></formula><formula xml:id="formula_3">Y r = [y r 1 , • • • , y r tx ] = [f (x r 1 ), • • • , f (x r tx )],<label>(2)</label></formula><p>where f (•) denote the LSTM feature learning operation,</p><formula xml:id="formula_4">y l i = f (x l i ) and y r i = f (x r i ) (i = 1, • • • , t x ). Let G = [G ik</formula><p>] tx×K be a t x × K coefficient matrix and σ(•) be a nonlinear mapping. Then, under the transformation of G and the operation of σ(•), we obtain the following deep feature matrices H l and H r : </p><formula xml:id="formula_5">H l = σ(Y l G + B) = σ(f (X l )G + B),<label>(3)</label></formula><formula xml:id="formula_6">H r = σ(Y r G + B) = σ(f (X r )G + B), (<label>4</label></formula><formula xml:id="formula_7">)</formula><p>where B is a bias matrix. Now suppose that X l S (X r S ) and X l T (X r T ) are the input EEG feature matrices of the left (right) hemisphere associated with the source domain and the target domain, respectively. Let H l S (H r S ) and H l T (H r T ) be the corresponding deep feature matrices obtained using (3) and (4). Then, by combining H l S (H r S ) with H l T (H r T ), we obtain the following new matrices:</p><formula xml:id="formula_8">H l S,T = [H l S , H l T ] = E f (X l S , X l T ),<label>(5)</label></formula><formula xml:id="formula_9">H r S,T = [H r S , H r T ] = E f (X r S , X r T ),<label>(6)</label></formula><p>where E f (•) denotes the feature extractor function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Local, global discriminators and classifier:</head><p>The use of local discriminator aims to alleviate the feature distribution difference between source and target domains, in either left hemisphere or right one. In contrast to the local discriminators, the target of global discriminator is also to eliminate the feature distribution difference between source and target domains, but across the whole brain cortex area instead of only the hemisphere area.</p><p>Different from both local and global discriminators, the purpose of the classifier part is to learn the discriminative features across source and target domains, which is implemented by applying the softmax function to the transformed hidden states to predict the class label, where the loss function of class label prediction is expressed as:</p><formula xml:id="formula_10">L c (X S ; θ l f , θ r f , θ c ) = ∑ c -τ (y, c) × logP (c|X S ), (<label>7</label></formula><formula xml:id="formula_11">)</formula><p>where c is the label of c-th emotion class, y is the ground-truth label associated with the data sample X S , and</p><formula xml:id="formula_12">τ (y, c) = { 1, if y = c, 0, otherwise. (<label>8</label></formula><formula xml:id="formula_13">)</formula><p>The optimal parameters of feature extractors are determined by minimizing the loss of classifier and maximizing the loss of both local and global domain discriminators. This leads to an adversarial learning between classifier and domain discriminators for emotion-related and domain-invariant EEG features.</p><p>According to the above analysis, we obtain that the BiDAN-N problem can be formulated as the optimization problem of minimizing the overall loss function L(θ l f , θ r f , θ c , θ l d , θ r d , θ g d ) defined as the following form:</p><formula xml:id="formula_14">min L(θ l f , θ r f , θ c , θ l d , θ r d , θ g d ) = L c (X S ; θ l f , θ r f , θ c ) -L l d (X l S,T ; θ l f , θ l d ) -L r d (X r S,T ; θ r f , θ r d ) -L g d (X S,T ; θ l f , θ r f , θ g d ),<label>(9)</label></formula><p>where </p><formula xml:id="formula_15">L c (•), L l d (•), L r d (•)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Optimization of BiDANN</head><p>We can iteratively train the classifier and three discriminators and update the parameters with the similar approach of standard deep learning methods by chain rule. Specifically, to solve the optimal solution of BiDANN, we adopt the stochastic gradient descent (SGD) algorithm <ref type="bibr" target="#b34">[35]</ref> to find optimal model parameters (θ l f , θ r f , θ c ) in <ref type="bibr" target="#b8">(9)</ref>. Moreover, we also introduce three gradient reversal layers (GRL) for the three domain discriminators (see Fig. <ref type="figure" target="#fig_0">1</ref>), which keep the gradient sign but reverse it while performing the back-propagation operation.</p><p>By iteratively minimizing the loss function of L defined in <ref type="bibr" target="#b8">(9)</ref>, we can finally obtain the optimal parameters. More specifically, suppose that the optimal parameters of θg </p><formula xml:id="formula_16">θ l f ,θ r f ,θc L(X S,T ; (θ l f , θ r f , θ c ), θg d , θl d , θr d ).</formula><p>On the other hand, after obtaining the optimal parameters of θl f , θr f , θc , the optimal parameters of θg d , θl d , and θr d can be updated via solving the following optimization problems:</p><formula xml:id="formula_17">θg d = arg max θ g d L(X S,T ; θl f , θr f , θc , θ g d , θl d , θr d ),<label>(10)</label></formula><formula xml:id="formula_18">θl d = arg max θ l d L(X l S,T ; θl f , θr f , θc , θg d , θ l d , θr d ),<label>(11)</label></formula><formula xml:id="formula_19">θr d = arg max θ r d L(X r S,T ; θl f , θr f , θc , θg d , θl d , θ r d ). (<label>12</label></formula><formula xml:id="formula_20">)</formula><p>Here it should be noted that the parameters before the GRL module (i.e., the parameters of feature extractors) will take the reverse direction of the gradients during back-propagation operation so as to generate data representations that minimize the loss of classifier while maximize the loss of discriminators. The optimization procedure of BiDANN is summarized in Algorithm 1. Update the parameters of the classifier:</p><formula xml:id="formula_21">θ c ← θ c -α ∂Lc ∂θc , θ l f ← θ l f -α ∂Lc ∂θ l f</formula><p>, and</p><formula xml:id="formula_22">θ r f ← θ r f -α ∂Lc ∂θ r f ; 4:</formula><p>Update the parameters of the global discriminator:</p><formula xml:id="formula_23">θ g d ← θ g d -α ∂L g d ∂θ g d , θ l f ← θ l f + α ∂L g d ∂θ l f ,<label>and</label></formula><formula xml:id="formula_24">θ r f ← θ r f + α ∂L g d ∂θ r f ; 5:</formula><p>Update the parameters of the local discriminator corresponding to the left hemisphere:</p><formula xml:id="formula_25">θ l d ← θ l d -α ∂L l d ∂θ l d , θ l f ← θ l f + α ∂L l d ∂θ l f ; 6:</formula><p>Update the parameters of the local discriminator corresponding to the right hemisphere:</p><formula xml:id="formula_26">θ r d ← θ r d -α ∂L r d ∂θ r d , θ r f ← θ r f + α ∂L r d ∂θ r f ; 7:</formula><p>If the model has been trained for 100 epochs, then α ← 0.9 × α and goto step 3; 8: until The iterations satisfies the predefined condition, i.e., the loss function satisfies:</p><formula xml:id="formula_27">L(θ l f , θ r f , θ c , θ l d , θ r d , θ g d ) &lt; 10 -3 . Output: 9: Parameters: θl f , θr f , θc , θl d , θr d , θg d .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. BIDANN-S FOR SUBJECT-INDEPENDENT EEG EMOTION RECOGNITION</head><p>The aforementioned BiDANN model provides a useful way to deal with the mismatch feature distribution problem between source domain and the target one. Nevertheless, it is notable that we may still suffer from the difficulties of handling the cross-subject EEG emotion recognition problem, in which the EEG feature distributions of different subjects would be different even if these subjects are from the same source domain. Consequently, it is very desirable to design an effective subject-independent model for the cross-subject EEG emotion recognition problem.</p><p>To handle such a subject-independent EEG emotion recognition problem, in this section we propose the BiDANN-S method based on the aforementioned BiDANN model. The basic idea of BiDANN-S is to lower the influences of the personal information of subjects to the EEG emotion recognition as more as possible. This goal is achieved by imposing an additional subject discriminator (denoted by D s ) onto the BiDANN model, in which the subject discriminator tries to lower the discrepancy for the recognition of different subjects. In designing the subject discriminator, we use the subject identity of the labeled source EEG data to serve as another class label set, denoted by D s = {1, 2, • • • , N }, where N is the number of subject in training dataset. In this case, the subject discriminator will work adversarially with the emotion classifier in the BiDANN-S model to result in the subjectindependent discriminative EEG features. Fig. <ref type="figure" target="#fig_4">3</ref> illustrates the subject-independent framework of BiDANN-S. As a result, the loss function of the BiDANN-S model contains an additional part, denoted by L s d (X S,T ; θ l f , θ r f , θ s d ), compared with the BiDANN method. To achieve the subjectindependent features, we should maximize the loss function L s d (X S,T ; θ l f , θ r f , θ s d ), where θ s d is the parameter of subject discriminator. In this case, the entire loss function of BiDANN-S can be expressed as following form:</p><formula xml:id="formula_28">L(θ l f , θ r f , θ c , θ l d , θ r d , θ g d , θ s d ) = L c (X S ; θ l f , θ r f , θ c ) -L l d (X l S,T ; θ l f , θ l d ) -L r d (X r S,T ; θ r f , θ r d ) -L g d (X S,T ; θ l f , θ r f , θ g d ) -L s d (X S,T ; θ l f , θ r f , θ s d ).<label>(13)</label></formula><p>By minimizing the loss function <ref type="formula" target="#formula_14">9</ref>), we can iteratively solve the optimal model parameters</p><formula xml:id="formula_29">L(θ l f , θ r f , θ c , θ l d , θ r d , θ g d , θ s d ) in (</formula><formula xml:id="formula_30">θ l f , θ r f , θ c , θ l d , θ r d , θ g d , θ s d of BiDANN-S.</formula><p>The solution approach is similar with that shown in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>In this section, we will conduct extensive experiments on the SJTU Emotion EEG Dataset (SEED) <ref type="bibr" target="#b12">[13]</ref> to evaluate the proposed BiDANN and BiDANN-S methods in EEG emotion recognition. The SEED database contains 15 subjects' EEG signals recorded from 62 electrode channels using ESI NeuroScan with a sampling rate of 1000 Hz. The EEG data are collected when the participants are watching three kinds of emotional film clips, i.e., positive emotion, neutral emotion, negative emotion. To evaluate the proposed BiDANN method, we adopt both subject-dependent and subject-independent strategy to design experiments. In the subject-dependent experiment, we follow the same experimental protocol as that of <ref type="bibr" target="#b12">[13]</ref> by choosing the EEG signals associated with two of the three sessions for each subject to serve as the experimental data, in which each session contains 15 trials of EEG signals.</p><p>In the subject-independent experiment, on the other hand, we also adopt the same experimental protocol as that of <ref type="bibr" target="#b35">[36]</ref> by choosing one of the EEG signals associated with one of the three sessions to serve as the experimental data.</p><p>Moreover, to evaluate the contributions of different parts of BiDANN to the EEG emotion recognition, we also propose two simplified versions of BiDANN, denoted by BiDANN-R1 and BiDANN-R2, respectively, and compare the recognition performance among them under the same experimental settings. In the BiDANN-R1 model, we use two feature extractors to capture the emotion information from source and target data without considering the hemispheric differences. In contrast to the BiDANN-R1 model, the BiDANN-R2 model uses two feature extractors to capture each brain hemispheric emotion information from source and target data separately. The frameworks of BiDANN-R1 and BiDANN-R2 are shown in Fig. <ref type="figure" target="#fig_5">4</ref> and Fig. <ref type="figure" target="#fig_6">5</ref>, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Subject-dependent EEG Emotion Recognition Experiments</head><p>In this experiment, we adopt the experimental protocol of Zheng et al. <ref type="bibr" target="#b12">[13]</ref> by using 9 trails of EEG data per session of each subject as source (training) domain data whereas using the remaining 6 trials per session as target (testing) domain data. In this way, we can obtain 1938 samples in total as the training data and 1336 samples in total as the testing data. For comparison purpose, we use the linear support vector machine (SVM) <ref type="bibr" target="#b36">[37]</ref>, canonical correlation analysis (CCA) <ref type="bibr" target="#b38">[39]</ref>, group sparse canonical correlation analysis (GSCCA) <ref type="bibr" target="#b10">[11]</ref>, deep believe network (DBN) <ref type="bibr" target="#b12">[13]</ref>, graph regularization sparse linear regression (GRSLR) <ref type="bibr" target="#b11">[12]</ref>, graph convolutional neural network (GCNN) <ref type="bibr" target="#b39">[40]</ref> and dynamical graph convolutional neural network (DGCNN) <ref type="bibr" target="#b13">[14]</ref> to conduct the same experiment, respectively. Moreover, in this experiment we also conduct additional experiments for the BiDANN-R1 method by using hemispheric EEG data to investigate which hemisphere contributes more to the emotion recognition. Table II summarizes the experimental results of the various methods. In addition, we also show the confusion matrices of the BiDANN method in Fig. <ref type="figure" target="#fig_7">6</ref> to see the confusions of BiDANN in recognizing the three emotions.</p><p>From Table <ref type="table" target="#tab_2">II</ref>, we can see that the proposed BiDANN method achieves the best recognition result among the various methods. Especially, compared with the other deep learning based methods such as DBN, GCNN and DGCNN, the proposed BiDANN method increases the classification accuracy of 6.30%, 4.98% and 1.98%, respectively. In addition, from Table II, we can see that the methods with domain discriminator, such as DANN and BiDANN-R1, achieve higher recognition accuracies than the other methods. On the other hand, from Fig. <ref type="figure" target="#fig_7">6</ref>, we can see that the average recognition accuracies of BiDANN in recognizing the three types of emotions are 86.15% (negative), 93.61% (neutral), and 96.89% (positive), which means that positive emotion would be much easier to be recognized than both negative and neutral emotions.</p><p>Moreover, we also adopt the BiDANN-R1 method to investigate the differences between left and right hemispheres in the perception of the three types of emotions. The experimental results are shown in Fig. <ref type="figure" target="#fig_8">7</ref>, from which we can see that, the right hemisphere shows more powerful perception ability than the left hemisphere in perceiving the negative emotion, whereas the left hemisphere demonstrates better perception ability than the right hemisphere in perceiving the positive emotion. The experimental results are consistent with the neuroscience finding about the emotion perception of the two hemispheres <ref type="bibr" target="#b22">[23]</ref>.   It is notable that the previous work of Zheng et al. <ref type="bibr" target="#b10">[11]</ref> showed that different frequency bands of the EEG signals may play different roles in the emotion recognition task. In this experiment, we also conduct experiments to verify this point. Table <ref type="table" target="#tab_3">III</ref> summarizes the experimental results of the various emotion recognition methods across the five EEG frequency bands. From Table <ref type="table" target="#tab_3">III</ref>, we can see that the experimental results are consistent with the findings of Zheng et al. in <ref type="bibr" target="#b10">[11]</ref>, i.e., the higher frequency bands (β and γ) would achieve better recognition performance than lower frequency bands (δ, θ and α). From Table <ref type="table" target="#tab_3">III</ref>, we can also see that the BiDANN method achieves better performance than the other methods for each frequency band. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Subject-independent EEG Emotion Recognition Experiments</head><p>In this experiment, we will focus our attention on the subject-independent EEG emotion recognition problem. For this purpose, we adopt the leave-one-subject-out (LOSO) cross-validation strategy to evaluate the performance of the proposed BiDANN-S method, in which we use the EEG signals of one subject as testing data and the rest of ones of 14 subjects as training data to conduct the experiment. This procedure is repeated such that the EEG signals of each subject have been used once as testing data. For comparison purpose, we also conduct the same experiments using various methods, including the methods of Kullback-Leibler importance estimation procedure (KLIEP) <ref type="bibr" target="#b40">[41]</ref>, unconstrained least-squares importance fitting (ULSIF) <ref type="bibr" target="#b41">[42]</ref>, selective transfer machine (STM) <ref type="bibr" target="#b42">[43]</ref>, linear SVM <ref type="bibr" target="#b36">[37]</ref>, kernel principal component analysis (KPCA) <ref type="bibr" target="#b43">[44]</ref>, transfer component analysis   (TCA) <ref type="bibr" target="#b44">[45]</ref>, transfer kernel learning (TKL) <ref type="bibr" target="#b45">[46]</ref>, subspace alignment (SA) <ref type="bibr" target="#b46">[47]</ref>, geodesic flow kernel (GFK) [48], transductive SVM (T-SVM) <ref type="bibr" target="#b48">[49]</ref>, transductive parameter transfer (TPT) <ref type="bibr" target="#b49">[50]</ref>, and DGCNN <ref type="bibr" target="#b13">[14]</ref>. The experimental results of the various methods are shown in Table <ref type="table" target="#tab_5">IV</ref>. <ref type="foot" target="#foot_0">1</ref>From Table <ref type="table" target="#tab_5">IV</ref>, we can see that the proposed BiDANN-S method achieves the best recognition result among the various methods. The better recognition performance of BiDANN-S is very likely due to the fact that the subject discriminator of BiDANN-S can effectively reduce the subjects' personal identity information such that it is more powerful for the subject-independent emotion recognition problem. Moreover, we can also see that the BiDANN-R2 method achieves better recognition result than the BiDANN-R1 method, which indicates the importance of considering the discrepancy information between left and right cerebral hemispheric data in the EEG emotion recognition. Furthermore, we can also see that the BiDANN method with local discriminators also achieves better performance than BiDANN-R2, which indicates that the local discriminators are useful to improve the EEG emotion recognition performance.</p><p>In addition, we also depict the confusion matrices corresponding to the experimental results of BiDANN and BiDANN-S to see the confusions of the three emotions. Fig. <ref type="figure" target="#fig_9">8</ref> shows the confusion matrices of the experimental results of BiDANN and BiDANN-S, respectively. From Fig. <ref type="figure" target="#fig_9">8</ref>, we can see that both BiDANN and BiDANN-S perform well in recognizing all the three types of emotion. Compared with BiDANN, it is interesting to see that BiDANN-S perform much better in recognizing both negative and neutral emotions than BiDANN, indicating the effectiveness of the subject discriminator in the BiDANN-S model.</p><p>Finally, to investigate the different contributions of the different frequency bands to the emotion recognition, we also conduct the experiments on each of the five frequency bands using the LOSO experimental strategy. Table V summarizes the experimental results of the various methods with respect to the five frequency bands. Again, from Table <ref type="table" target="#tab_6">V</ref>, we can see that the higher frequency bands achieve better recognition performance than lower frequency bands. Moreover, from Table <ref type="table" target="#tab_6">V</ref> we can also see that the proposed BiDANN and BiDANN-S methods achieve better recognition performance than the other methods for the five frequency bands.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Discussions</head><p>In the aforementioned experiments, we had demonstrated that the BiDANN-S method can further improve the recognition performance in contrast to the BiDANN method. This mainly attributes to the fact that the subject discriminator used in BiDANN-S can largely remove the influences of the subject information from the EEG features and hence is able to improve the discriminative ability of the EEG features for emotion recognition.</p><p>To further verify this point, we conduct an additional experiment on the two-class HR-EEG4EMO database <ref type="bibr" target="#b50">[51]</ref>, which consists of the EEG signals of 27 subjects. In this experiment, we firstly adopt the same band power (BP) EEG feature as that used in <ref type="bibr" target="#b50">[51]</ref> to extract 64 handcraft features associated with 64 EEG electrodes from each subject. Then, we visualize the feature distributions associated with five subjects using the t-SNE embeddings method <ref type="bibr" target="#b51">[52]</ref>. Fig. <ref type="figure" target="#fig_10">9</ref> shows the visualized results of the raw EEG handcraft features and the discriminative features learnt by BiDANN and BiDANN-S respectively, where the feature distribution corresponding to each subject is obtained based on BiDANN or BiDANN-S using the EEG data of the other 26 subjects. From Fig. <ref type="figure" target="#fig_10">9</ref>, we can see that the raw EEG feature points associated with positive emotion are relatively hard to be recognized compared with the feature points associated with negative emotion. In contrast to the raw features, we can see that the separability between the positive emotion and negative emotion becomes much easier when BiDANN or BiDANN-S is used. On the other hand, compared with the BiDANN method, the BiDANN-S method achieves better separability for most cases. The better separability of BiDANN-S than BiDANN mainly attributes to the reduction of the subject personal information in the EEG features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>In this paper, we had proposed a novel BiDANN method to deal with EEG emotion recognition, which is inspired by the neuroscience findings about the asymmetry property of human brain to the emotion response. The extensive experiments on the SEED EEG database demonstrated that the proposed BiDANN method achieves better EEG emotion recognition performance than the state-of-the-art methods. The better recognition performance of BiDANN mostly attributes to the fact that the BiDANN method can not only make use of the advantages of adversarial discriminative domain adaptation to improve the EEG recognition performance, but also is able to make use of the asymmetry of the left and right hemispheres in the perception of different emotions. Based on the BiDANN method, we also proposed the BiDANN-S method to further improve the BiDANN method to deal with the cross subject</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: The framework of BiDANN. The black lines and arrows refer to source domain path, while the red lines and arrows refer to target domain path.</figDesc><graphic coords="3,48.96,511.12,250.99,136.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: The feature extraction procedures in the BiDANN model, which consists of the handcraft EEG feature extraction part and the discriminative deep feature extraction part.</figDesc><graphic coords="4,120.10,53.57,371.80,212.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>and L g d (•) denote the loss function of classifier, the loss function of local discriminator associated with the left and right hemisphere, and the loss function of global discriminator, respectively, and the parameters θ c , θ l d , θ r d , θ g d , θ l f and θ r f denote the corresponding model parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1 :</head><label>1</label><figDesc>Procedures of training BiDANN model. Input: Source data set {X S } and Target data set {X T }; Ground-truth label set L S = {y} of source data set; Source domain label set D S =[D l S , D r S ]={0} and target domain label set D T =[D l T , D r T ]={1}; 1: Initialize model parameters and learning rate α; 2: repeat 3:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: The framework of the proposed BiDANN-S. Compared with BiDANN, BiDANN-S has an additional subject discriminator to decrease the effect of identity information to the discriminative emotion features.</figDesc><graphic coords="5,311.98,366.49,250.99,135.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: The framework of BiDANN-R1, which uses two feature extractors to capture the emotion information from source and data.</figDesc><graphic coords="6,99.17,504.73,150.61,85.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: The framework of BiDANN-R2, which uses two feature extractors to capture each brain hemispheric emotion information from source and target data separately.</figDesc><graphic coords="6,311.98,53.53,250.99,150.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 :</head><label>6</label><figDesc>Fig.6: The confusion matrices of the subject-dependent EEG emotion recognition results using the BiDANN method on the SEED database.</figDesc><graphic coords="7,96.18,171.96,150.61,112.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: The experimental results of using the BiDANN-R1 method to recognizing the three emotions based on the left and the right hemispheres EEG signals, respectively.</figDesc><graphic coords="7,55.24,339.50,238.52,188.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: The confusion matrices of the subject-independent EEG emotion recognition results using BiDANN and BiDANN-S method on the SEED database. (a) Confusion matrix of BiDANN; (b) Confusion matrix of BiDANN-S.</figDesc><graphic coords="8,185.72,190.94,112.91,84.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: The t-SNE visualization before and after domain adaptation. (a) -(e) are raw data. (f) -(j) are feature distributions obtained by BiDANN. (k) -(o) are feature distributions obtained by BiDANN-S. The red color denotes the negative emotion and the blue color denotes the positive emotion.</figDesc><graphic coords="9,161.97,254.16,92.49,58.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>The major notations and the corresponding definitions used in BiDANN.</figDesc><table><row><cell>Notation</cell><cell>Definition</cell></row><row><cell>X</cell><cell>EEG raw feature matrix</cell></row><row><cell>Y</cell><cell>LSTM feature matrix</cell></row><row><cell>H</cell><cell>Deep feature matrix</cell></row><row><cell>θ</cell><cell>Learnable parameters</cell></row><row><cell>E f (•)</cell><cell>Feature extractor function</cell></row><row><cell>L(•)</cell><cell>Loss function</cell></row><row><cell>D</cell><cell>Domain label set</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>The mean accuracies (ACC) and standard deviations (STD) on SEED database for conventional EEG emotion recognition experiment. /08.47 92.38/07.04 * denotes the experiment results obtained are based on our own implementation.</figDesc><table><row><cell cols="3">Method RF  ACC/STD(%) SVM [37] 83.99/09.72</cell><cell>77.63/13.21</cell><cell>82.96/09.95</cell><cell>86.08/08.34</cell><cell>87.39/08.64</cell><cell>87.40/09.20</cell></row><row><cell>Method</cell><cell>DGCNN [14]</cell><cell cols="2">DANN  *  [17] BiDANN-R1</cell><cell>BiDANN-R2</cell><cell>BiDANN</cell></row><row><cell>ACC/STD(%)</cell><cell>90.40/08.49</cell><cell>91.36/08.30</cell><cell>90.29/08.02</cell><cell>91.60</cell><cell></cell></row></table><note><p><p><p><p><p><p><p><p><p><p><p><p>*</p><ref type="bibr" target="#b37">[38]</ref> </p>CCA *</p><ref type="bibr" target="#b38">[39]</ref> </p>GSCCA *</p><ref type="bibr" target="#b10">[11]</ref> </p>DBN</p><ref type="bibr" target="#b12">[13]</ref> </p>GRSLR *</p><ref type="bibr" target="#b11">[12]</ref> </p>GCNN</p><ref type="bibr" target="#b39">[40]</ref> </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>The mean accuracies (and standard deviations) using different frequency bands for subject-dependent EEG emotion recognition experiment on SEED database. * denotes the experiment results obtained are based on our own implementation.</figDesc><table><row><cell>Methods</cell><cell>δ</cell><cell>θ</cell><cell cols="3">Frequency bands α</cell><cell>β</cell><cell>γ</cell></row><row><cell>SVM [37]</cell><cell>60.50 (14.14)</cell><cell cols="2">60.95 (10.20)</cell><cell>66.64 (14.41)</cell><cell cols="2">80.76 (11.56)</cell><cell>79.56 (11.38)</cell></row><row><cell>RF  *  [38]</cell><cell>64.56 (08.32)</cell><cell cols="2">65.27 (11.64)</cell><cell>65.67 (13.94)</cell><cell cols="2">73.35 (14.35)</cell><cell>74.48 (12.80)</cell></row><row><cell>CCA  *  [39]</cell><cell>55.30 (12.02)</cell><cell cols="2">55.75 (10.99)</cell><cell>64.96 (12.05)</cell><cell cols="2">69.16 (11.45)</cell><cell>70.67 (14.06)</cell></row><row><cell>GSCCA  *  [11]</cell><cell>63.92 (11.16)</cell><cell cols="2">64.64 (10.33)</cell><cell>70.10 (14.76)</cell><cell cols="2">76.93 (11.00)</cell><cell>77.98 (10.72)</cell></row><row><cell>DBN [13]</cell><cell>64.32 (12.45)</cell><cell cols="2">60.77 (10.42)</cell><cell>64.01 (15.97)</cell><cell cols="2">78.92 (12.48)</cell><cell>79.19 (14.58)</cell></row><row><cell>GRSLR  *  [12]</cell><cell>63.90 (11.83)</cell><cell cols="2">62.61 (10.73)</cell><cell>71.11 (09.04)</cell><cell cols="2">81.18 (10.74)</cell><cell>81.91 (10.36)</cell></row><row><cell>GCNN [40]</cell><cell>72.75 (10.85)</cell><cell cols="2">74.40 (08.23)</cell><cell>73.46 (12.17)</cell><cell cols="2">83.24 (09.93)</cell><cell>83.36 (09.43)</cell></row><row><cell>DGCNN [14]</cell><cell>74.25 (11.42)</cell><cell cols="2">71.52 (05.99)</cell><cell>74.43 (12.16)</cell><cell cols="2">83.65 (10.17)</cell><cell>85.73 (10.64)</cell></row><row><cell>DANN  *  [17]</cell><cell>72.13 (11.22)</cell><cell cols="2">68.75 (07.40)</cell><cell>70.27 (10.84)</cell><cell cols="2">83.35 (11.46)</cell><cell>87.89 (11.35)</cell></row><row><cell>BiDANN-R1</cell><cell>71.63 (10.69)</cell><cell cols="2">66.79 (08.96)</cell><cell>67.71 (12.49)</cell><cell cols="2">85.68 (10.13)</cell><cell>86.93 (09.56)</cell></row><row><cell>BiDANN-R2</cell><cell>75.26 (10.95)</cell><cell cols="2">71.20 (07.73)</cell><cell>75.40 (14.26)</cell><cell cols="2">87.73 (10.58)</cell><cell>86.80 (10.78)</cell></row><row><cell>BiDANN</cell><cell>76.97 (10.95)</cell><cell cols="2">75.56 (07.88)</cell><cell>81.03 (11.74)</cell><cell cols="2">89.65 (09.59)</cell><cell>88.64 (09.46)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>1949-3045 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2018.2885474, IEEE Transactions on Affective Computing</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV :</head><label>IV</label><figDesc>The mean accuracies (ACC) and standard deviations (STD) on SEED database for subject-indepent EEG emotion recognition experiment. * denotes the experiment results obtained are based on our own implementation.</figDesc><table><row><cell>Method</cell><cell>KLIEP  *  [41]</cell><cell>ULSIF  *  [42]</cell><cell>STM  *  [43]</cell><cell>SVM [37]</cell><cell>KPCA [44]</cell><cell>TCA [45]</cell></row><row><cell>ACC/STD(%)</cell><cell>45.71/17.76</cell><cell>51.18/13.57</cell><cell>51.23/14.82</cell><cell>56.73/16.29</cell><cell>61.28/14.62</cell><cell>63.64/14.88</cell></row><row><cell>Method</cell><cell>TKL  *  [46]</cell><cell>SA  *  [47]</cell><cell>GFK  *  [48]</cell><cell>T-SVM [49]</cell><cell>TPT [50]</cell><cell>DGCNN [14]</cell></row><row><cell>ACC/STD(%)</cell><cell>63.54/15.47</cell><cell>69.00/10.89</cell><cell>71.31/14.09</cell><cell>72.53/14.00</cell><cell>76.31/15.89</cell><cell>79.95/09.02</cell></row><row><cell>Method</cell><cell>DANN  *  [17]</cell><cell>BiDANN-R1</cell><cell>BiDANN-R2</cell><cell>BiDANN</cell><cell>BiDANN-S</cell><cell></cell></row><row><cell>ACC/STD(%)</cell><cell>75.08/11.18 1</cell><cell>76.97/11.08</cell><cell>82.22/07.61</cell><cell>83.28/09.60</cell><cell>84.14/06.87</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V :</head><label>V</label><figDesc>The mean accuracies (and standard deviations) using different frequency bands for subject-independent EEG emotion recognition on SEED database.</figDesc><table><row><cell>Methods</cell><cell>δ</cell><cell>θ</cell><cell cols="3">Frequency bands α</cell><cell>β</cell><cell>γ</cell></row><row><cell>KLIEP  *  [41]</cell><cell>39.22 (11.31)</cell><cell cols="2">35.98 (07.50)</cell><cell>33.31 (06.60)</cell><cell cols="2">44.47 (12.89)</cell><cell>42.05 (12.65)</cell></row><row><cell>ULSIF  *  [42]</cell><cell>41.32 (11.30)</cell><cell cols="2">36.27 (06.84)</cell><cell>38.94 (08.30)</cell><cell cols="2">41.87 (13.64)</cell><cell>41.02 (11.65)</cell></row><row><cell>STM  *  [43]</cell><cell>44.16 (09.60)</cell><cell cols="2">40.89 (08.22)</cell><cell>40.37 (09.82)</cell><cell cols="2">42.09 (13.34)</cell><cell>47.97 (12.43)</cell></row><row><cell>SVM  *  [37]</cell><cell>43.06 (08.27)</cell><cell cols="2">40.07 (06.50)</cell><cell>43.97 (10.89)</cell><cell cols="2">48.63 (10.29)</cell><cell>51.59 (11.83)</cell></row><row><cell>TCA  *  [45]</cell><cell>44.10 (08.22)</cell><cell cols="2">41.26 (09.21)</cell><cell>42.93 (14.33)</cell><cell cols="2">43.93 (10.06)</cell><cell>48.43 (09.73)</cell></row><row><cell>TKL  *  [46]</cell><cell>48.36 (10.31)</cell><cell cols="2">52.60 (11.84)</cell><cell>52.89 (11.07)</cell><cell cols="2">55.47 (09.80)</cell><cell>59.81 (12.41)</cell></row><row><cell>SA  *  [47]</cell><cell>53.23 (07.47)</cell><cell cols="2">50.60 (08.31)</cell><cell>55.06 (10.60)</cell><cell cols="2">56.72 (10.78)</cell><cell>64.47 (14.96)</cell></row><row><cell>GFK  *  [48]</cell><cell>52.73 (11.90)</cell><cell cols="2">54.07 (06.78)</cell><cell>54.98 (11.49)</cell><cell cols="2">59.29 (10.75)</cell><cell>66.92 (10.97)</cell></row><row><cell>DGCNN [14]</cell><cell>49.79 (10.94)</cell><cell cols="2">46.36 (12.06)</cell><cell>48.29 (12.28)</cell><cell cols="2">56.15 (14.01)</cell><cell>54.87 (17.53)</cell></row><row><cell>DANN  *  [17]</cell><cell>56.66 (06.48)</cell><cell cols="2">54.95 (10.45)</cell><cell>59.37 (10.57)</cell><cell cols="2">67.14 (07.10)</cell><cell>71.30 (10.84)</cell></row><row><cell>BiDANN-R1</cell><cell>57.33 (06.76)</cell><cell cols="2">57.00 (08.92)</cell><cell>58.20 (13.50)</cell><cell cols="2">64.76 (13.79)</cell><cell>65.15 (14.14)</cell></row><row><cell>BiDANN-R2</cell><cell>59.67 (10.48)</cell><cell cols="2">60.70 (07.42)</cell><cell>61.08 (10.77)</cell><cell cols="2">74.09 (11.54)</cell><cell>72.77 (11.51)</cell></row><row><cell>BiDANN</cell><cell>62.04 (06.64)</cell><cell cols="2">62.13 (07.37)</cell><cell>63.31 (11.46)</cell><cell cols="2">73.55 (08.83)</cell><cell>73.25 (09.21)</cell></row><row><cell>BiDANN-S</cell><cell>63.01 (07.49)</cell><cell cols="2">63.22 (07.52)</cell><cell>63.50 (09.50)</cell><cell cols="2">73.59 (09.12)</cell><cell>73.72 (08.67)</cell></row><row><cell cols="7">*  denotes the experiment results obtained are based on our own</cell></row><row><cell>implementation.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Noting that the subspace based methods, such as KLIEP, ULSIF, STM, TCA, TKL, SA, GFK and KPCA, are problematic to handle larger size of EEG data due to the of computer memory limitation and computational problem, we only randomly select 5000 EEG feature samples from the training data set to train these methods.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Emotion, cognition, and behavior</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">298</biblScope>
			<biblScope unit="issue">5596</biblScope>
			<biblScope unit="page" from="1191" to="1194" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Affective computing</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Picard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>MIT press Cambridge</publisher>
			<biblScope unit="volume">252</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A survey of affective brain computer interfaces: principles, state-of-the-art, and challenges</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mühl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Allison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nijholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain-Computer Interfaces</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="66" to="84" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Practical emotional neural networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lotfi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-R</forename><surname>Akbarzadeh-T</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="61" to="72" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural correlates of social and nonsocial emotions: An fmri study</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Britton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Welsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Berridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Liberzon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="397" to="409" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Emotional processing in anterior cingulate and medial prefrontal cortex</title>
		<author>
			<persName><forename type="first">A</forename><surname>Etkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Egner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kalisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="85" to="93" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A functional architecture of the human brain: emerging insights from the science of emotion</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Lindquist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Barrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="533" to="540" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Feature extraction and selection for emotion recognition from eeg</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jenke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Peer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Buss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="327" to="339" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Feature extraction from eegs associated with emotions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Musha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Terasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Ivamitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Life and Robotics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="19" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A review on the computational methods for emotional state estimation from the human eeg</title>
		<author>
			<persName><forename type="first">M.-K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-P</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational and Mathematical Methods in Medicine</title>
		<imprint>
			<biblScope unit="volume">2013</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multichannel eeg-based emotion recognition via group sparse canonical correlation analysis</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cognitive and Developmental Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="281" to="290" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Eeg emotion recognition based on graph regularized sparse linear regression</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Processing Letters</title>
		<imprint>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks</title>
		<author>
			<persName><forename type="first">W.-L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-L</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Autonomous Mental Development</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="162" to="175" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Eeg emotion recognition using dynamical graph convolutional neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Emotional state recognition with eeg signals using subject independent approach</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Seeja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Science and Big Data Analytics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="117" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Exploring eeg features in cross-subject emotion recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neuroscience</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">162</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">59</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A novel neural network model based on cerebral hemispheric asymmetry for eeg emotion recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">27th International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Lateral brain function, emotion, and conceptualization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cerebral lateralization: Biological mechanisms, associations, and pathology: I. a hypothesis and a program for research</title>
		<author>
			<persName><forename type="first">N</forename><surname>Geschwind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Galaburda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Archives of Neurology</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="428" to="459" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Functional localization and lateralization of human olfactory cortex</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Zatorre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones-Gotman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Meyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">360</biblScope>
			<biblScope unit="issue">6402</biblScope>
			<biblScope unit="page" from="339" to="340" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A surface-based analysis of language lateralization and cortical asymmetry</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Greve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Haegen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stufflebeam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fischl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brysbaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cognitive Neuroscience</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1477" to="1492" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Differing emotional response from right and left hemispheres</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Dimond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Farrington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">261</biblScope>
			<biblScope unit="issue">5562</biblScope>
			<biblScope unit="page" from="690" to="692" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Approach-withdrawal and cerebral asymmetry: Emotional expression and brain physiology: I</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Saron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Senulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">330</biblScope>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Cerebral asymmetry, emotion and affective style, brain asymmetry</title>
		<author>
			<persName><forename type="first">R</forename><surname>Davison</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Depression: perspectives from affective neuroscience</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pizzagalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Nitschke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Putnam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Psychology</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="545" to="574" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hemispheric specialization in affective responses, cerebral dominance for language, and handedness: lateralization of emotion, language, and dexterity</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Y</forename><surname>Costanzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Villarreal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Drucaroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ortiz-Villafañe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goldschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Wainsztein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Ladrón-De Guevara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">I</forename><surname>Brusco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioural Brain Research</title>
		<imprint>
			<biblScope unit="volume">288</biblScope>
			<biblScope unit="page" from="11" to="19" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The revised brain symmetry index</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Van Putten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinical Neurophysiology</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2362" to="2367" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Electroencephalographic spectral asymmetry index for detection of depression</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hinrikus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Suhhova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bachmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Aadamsoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ü</forename><surname>Võhma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tuulik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical &amp; Biological Engineering &amp; Computing</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">1291</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Behavioral measures and eeg monitoring using the brain symmetry index during the wada test in children</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tomas-Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Van Putten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Loddenkemper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Epilepsy &amp; Behavior</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="247" to="253" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for driver activity anticipation via sensory-fusion architecture</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3118" to="3125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Recurrent face aging</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2378" to="2386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COMPSTAT&apos;2010</title>
		<meeting>COMPSTAT&apos;2010</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Personalizing eeg-based affective models with transfer learning</title>
		<author>
			<persName><forename type="first">W.-L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-L</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the Twenty-Fifth International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2732" to="2738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Least squares support vector machine classifiers</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Processing Letters</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="293" to="300" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Canonical correlation analysis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of Statistics in Behavioral Science</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Direct importance estimation with model selection and its application to covariate shift adaptation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nakajima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Buenau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kawanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1433" to="1440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A least-squares approach to direct importance estimation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kanamori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1391" to="1445" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Selective transfer machine for personalized facial expression analysis</title>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="529" to="545" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Nonlinear component analysis as a kernel eigenvalue problem</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1299" to="1319" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Domain adaptation via transfer component analysis</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="199" to="210" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Domain invariant transfer kernel learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1519" to="1532" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unsupervised visual domain adaptation using subspace alignment</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sebban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2960" to="2967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2066" to="2073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Large scale transductive svms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1687" to="1712" />
			<date type="published" when="2006-08">Aug. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">We are not all equal: Personalizing models for facial expression analysis with transductive parameter transfer</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia (MM)</title>
		<meeting>the 22nd ACM international conference on Multimedia (MM)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="357" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Emotion recognition based on high-resolution eeg recordings and reconstructed brain sources</title>
		<author>
			<persName><forename type="first">H</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fleureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Guillotel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wendling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Merlet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Albera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">His researches focus on Pattern Recognition and Machine Learning. Wenming Zheng (M&apos;08) Wenming Zheng received the B.S. degree in computer science from Fuzhou University</title>
		<title level="s">Li received the B.S. degree in electronic information and science technology from School of Physics and Electronics</title>
		<meeting><address><addrLine>China; China; China; Fuzhou, China; Quanzhou, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">2012. 2015. 1997</date>
		</imprint>
		<respStmt>
			<orgName>Shandong Normal University ; School of Electronic Engineering, Xidian University ; M.S. degree in computer science from Huaqiao University</orgName>
		</respStmt>
	</monogr>
	<note>Currently, he is pursuing the Ph.D. degree in information and communication engineering in Southeast University. in 2001, and the Ph.D. degree in</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
