<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised Prototype Representation Learning for Event-Based Corporate Profiling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zixuan</forename><surname>Yuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
							<email>liuhao30@baidu.com</email>
							<affiliation key="aff1">
								<orgName type="department">Business Intelligence Lab</orgName>
								<address>
									<settlement>Baidu Research</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Renjun</forename><surname>Hu</surname></persName>
							<email>renjun.hrj@alibaba-inc.com</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Denghui</forename><surname>Zhang</surname></persName>
							<email>denghui.zhang@rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hui</forename><surname>Xiong</surname></persName>
							<email>hxiong@rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Supervised Prototype Representation Learning for Event-Based Corporate Profiling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Event-based corporate profiling aims to assess the evolving operational status of the corresponding corporate from its event sequence. Existing studies on corporate profiling have partially addressed the problem via (i) case-by-case empirical analysis by leveraging traditional financial methods, or (ii) the automatic profile inference by reformulating the problem into a supervised learning task. However, both approaches heavily rely on domain knowledge and are laborintensive. More importantly, the task-specific nature of both approaches prevents the obtained corporate profiles from being applied to diversified downstream applications. To this end, in this paper, we propose a Self-Supervised Prototype Representation Learning (SePaL) framework for dynamic corporate profiling. By exploiting the topological information of an event graph and exploring self-supervised learning techniques, SePaL can obtain unified corporate representations that are robust to event noises and can be easily fine-tuned to benefit various down-stream applications with only a few annotated data. Specifically, we first infer the initial cluster distribution of noise-resistant event prototypes based on latent representations of events. Then, we construct four permutation-invariant self-supervision signals to guide the representation learning of the event prototype. In terms of applications, we exploit the learned time-evolving corporate representations for both stock price spike prediction and corporate default risk evaluation. Experimental results on two real-world corporate event datasets demonstrate the effectiveness of SePaL for these two applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Event-based corporate profiling aims to assess the evolving operational status of the concerned corporate from its event sequence. Instead of static corporate overview, it incorporates dynamic event information to capture the evolving business circumstances, which enables investors to exploit the price inefficiencies around events. Thus, event-based corporate profiling could be beneficial to a wide range of financial services, such as corporate budget planning, corporate bond credit ratings, and event-driven asset investing.</p><p>Prior studies on event-based corporate profiling can be roughly categorized into two classes: (i) Traditional finan- cial methods, which implement case-by-case empirical analysis on substantial events <ref type="bibr" target="#b18">(Richardson, Taylor, and Wright 2014)</ref>, and (ii) Learning-based methods, which achieve automatic profile inference by formulating the problem into various supervised learning tasks such as stock price prediction <ref type="bibr" target="#b4">(Ding et al. 2016</ref>) and corporate default risk evaluation <ref type="bibr" target="#b24">(Yeh, Wang, and Tsai 2015)</ref>. However, both types of approaches rely to a large extent on domain knowledge and are inevitably labor-intensive. More particularly, their taskspecific nature prevents the obtained corporate profile from being applied to various application scenarios.</p><p>Indeed, the recent emergence of prototype learning and self-supervised learning provides great potentials for us to improve existing literature, in terms of both dependence of domain knowledge and generalization of profile. We detail our research insights from the following two perspectives.</p><p>First, real-life event sequences are usually characterized by their high degree of stochasticity and obscurity. Inspired by event trend analysis <ref type="bibr" target="#b17">(Raj and Musgrave 2009)</ref>, we find that the representative event subsequences can explicitly reflect the corporate status, but do not always strictly appear; instead, they may be delivered in various permutations and accompanied by noise events. For instance, in Figure <ref type="figure" target="#fig_0">1</ref>, an event set {1, 2, 3} is exactly repeated in different permutations of subsequences to express negative expectations on the operational status for corporates A and B, while corporate C breaks such permutation-invariant property with noise event 4. Moreover, the support of these useful subsequence patterns decreases drastically with the growing cardinality of event subsequences. A promising solution to the above issues is to utilize the concept of prototype learning <ref type="bibr" target="#b19">(Schmidt et al. 2001)</ref>, which essentially obtains a refined representation of the original sequence by clustering single sequence items into abstract, interpretable prototypes. It allows us to learn a unified representation of event sequence with extracted event sets (i.e., prototypes) that are delivered in the form of meaningful subsequences, and highlight the evolving operational status for corporate profiling. While recent studies have successfully applied this concept into sequence modeling <ref type="bibr" target="#b12">(Liu et al. 2016;</ref><ref type="bibr" target="#b28">Zhang et al. 2020b)</ref>, most of them adopted simple regularization schemes to quantify the event correlations, which reduces the consistency of prototype selections from long event sequences.</p><p>Second, we notice that in lack of domain-expert guidance, the majority of non-landmark corporate events only reveal immaterial, neutral information w.r.t. corporate situation. According to the large-scale data analysis through Wind Data Service 1 , the landmark events only account for less than 8% of all event data, which results in the absence of annotated data for effective supervised training. We therefore require a self-supervised learning paradigm to extract high-quality training signals from event sequences for dynamic corporate profiling.</p><p>Along these lines, in this paper, we propose a Self-Supervised Prototype Representation Learning (SePaL) framework for event-based corporate profiling. Our major contributions are summarized as follows:</p><p>• We first introduce the temporal skeletonization technique to embed event co-occurrence patterns into a graph structure. To reduce the stochasticity and obscurity of event sequences, we develop a graph-based co-occurrence smoothing operation to learn the event representations, which are further used to infer the initial cluster distribution of a set of noise-resistant event prototypes.</p><p>• We further construct four permutation-invariant selfsupervision signals to guide the event prototype representation learning, in order to maintain the consistency of prototype selections.</p><p>• We finally utilize the learned prototypes to generate the time-evolving corporate representations and exploit them for various applications, including stock price spike prediction and corporate default risk evaluation. The corresponding experimental results demonstrate the effectiveness of proposed SePaL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Event-based Corporate Profiling. Unlike traditional profiling tasks such as user profiling <ref type="bibr" target="#b22">(Wang et al. 2019;</ref><ref type="bibr" target="#b5">Gu et al. 2020)</ref>, event-based corporate profiling receives little attention from prior studies in the machine learning field. Moreover, the majority of them put emphasis on profiling corporates for specific downstream tasks <ref type="bibr" target="#b4">(Ding et al. 2016;</ref><ref type="bibr" target="#b10">Li et al. 2020b;</ref><ref type="bibr" target="#b23">Xu et al. 2020;</ref><ref type="bibr" target="#b11">Lin et al. 2017)</ref>. For instance, <ref type="bibr" target="#b10">Li et al. (2020b)</ref> proposed an event-driven sequential neural network to aggregate multi-source information of market fundamentals and news articles for stock price prediction. However, we argue that the tight coupling with con-1 https://www.wind.com.cn/en/data.html crete tasks narrows down the applicable values of the learned event representations in dynamic corporate profiling. Prototype Learning. The intuition of prototype learning is to cluster single items into abstract, interpretable prototypes for better pattern recognition <ref type="bibr" target="#b12">(Liu et al. 2016;</ref><ref type="bibr" target="#b28">Zhang et al. 2020b)</ref>. As an unsupervised learning exemplar, temporal skeletonization <ref type="bibr" target="#b12">(Liu et al. 2016)</ref> proposes to form a series of abstract prototypes that cluster and represent the sequence items with similar co-occurrence patterns via an undirected sequential graph. Its major advantages include proactively reducing the sequence cardinality with extracted prototypes, and uncovering the significant high-hierarchy structures inside sequences. However, due to its low complexity of graph learning, such method has difficulty in fast convergence when dealing with long, noisy event sequences.</p><p>Self-Supervised Representation Learning. As an important subarea of unsupervised learning, self-supervised representation learning intends to learn the useful feature embeddings through self-supervisory signals that are directly constructed from input data. Such learning mechanism has been successfully applied in many areas, such as sequence modeling <ref type="bibr" target="#b12">(Liu et al. 2016)</ref>, translation embedding methods <ref type="bibr" target="#b25">(Zhang et al. 2017)</ref>, job title benchmarking <ref type="bibr" target="#b26">(Zhang et al. 2019</ref>) and language modeling <ref type="bibr" target="#b27">(Zhang et al. 2020a</ref>). To the best of our knowledge, this is the first study that adopts the self-supervised representation learning for event-based corporate profiling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preliminaries</head><p>Suppose there are M event sequences</p><formula xml:id="formula_0">S = {S 1 , • • • , S M } that entail a collection of N distinct corporate events E [1:N ] = {e 1 , • • • , e N }.</formula><p>We denote the m-th event sequence as</p><formula xml:id="formula_1">S m = {e 1 m , • • • , e tm m }</formula><p>, where t m is the length of the m-th event sequence, and m ∈ {1, • • • , M }.</p><p>Definition 1 Event Prototype (EP) is defined as an event set that clusters individual events with similar co-occurrence. The purpose of EP is to form the meaningful event subsequences in different permutations, which can represent the evolving operational states of corporates, e.g., the event set {1, 2, 3} in Figure <ref type="figure" target="#fig_0">1</ref> is considered as an EP exemplar that appears in different permutations of subsequences, delivering negative expectation on the corporate operational status.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem Statement</head><p>This paper studies the problem of profiling companies with corporate event data. Specifically, we formulate the problem as a task of learning dynamic representation of corporate profile from event sequences. Since EP can encapsulate the hierarchical patterns of event sequences that reflect the corporates' real status, this problem is therefore decomposed to (i) group individual events into multiple useful EPs, and (ii) preserve the EP patterns in building meaningful event subsequences for dynamic corporate profiling.</p><p>Note that, throughout the paper, we use boldface capital and lowercase letters (e.g., W and e) to denote matrices and vectors, and all W s and b s are regarded as the learnable weight matrices and bias terms, respectively.  In the first part, we propose the Event prototype initialization module, in which the Event graph construction block first embeds raw event sequences into an event graph through a temporal skeletonization technique, and the Event representation learning applies a graph-based smoothing operation to initialize the low-dimensional representations of each event. Then, the Prototype initialization block incorporates a clustering approach to derive the initial cluster distribution of EPs based on event representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Framework Overview</head><p>In the second part, we develop the Self-supervised prototype representation learning module, where an N -round Iterative prototype assignment block is introduced to validate the cluster distribution of EPs. In each round, the Permutation-aware self-supervision block applies four types of neural units, i.e., Intra-cluster unit, Inter-cluster unit, Contrastive unit, and Unassigned unit, to produce selfsupervised permutation-invariant signals for current EPs. Representations of EPs are finalized in the Prototype representation learning block, and we use three separate selfsupervision losses to regularize the end-to-end training.</p><p>In the third part, we incorporate the learned EPs to refine the original event sequence for dynamic corporate profiling, which can be fine-tuned using the existing temporal model for various applications, such as stock price spike prediction and corporate default risk evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Event Prototype Initialization</head><p>We first present the event prototype initialization module to derive the initial distribution of EPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Event Graph Construction</head><p>Inspired by the temporal skeletonization <ref type="bibr" target="#b12">(Liu et al. 2016)</ref>, we first create an encoding scheme of event that corresponds to EP label: ep = F(e) ∈ {1, • • • , K} such that the sequen-tial variations of events is minimized for all given sequences:</p><formula xml:id="formula_2">min ep∈{1,•••, K} 1 M M m=1 1≤i,j≤tm |i−j|≤δ (F(e i m ) − F(e j m )) 2 . (<label>1</label></formula><formula xml:id="formula_3">)</formula><p>Here δ denotes a pre-defined range of local sequence variations. The cardinality K of generated EPs is typically much smaller than that of any original sequence. Furthermore, we assume that given an event sequence S m ∈ S, a pair of events s i m and s j m that are within a close interval are usually correlated with each other, which also applies to their encoded form F(s i m ) and F(s j m ). Finding the optimal F(•) w.r.t. objective (1) belongs to an integer programming problem, which has shown to be NP-hard <ref type="bibr" target="#b12">(Liu et al. 2016)</ref>. We then construct an event graph to relax the integer constraint to real numbers, and encode the closeness of events e i , e j ∈ E [1:N ] into graph structure:</p><formula xml:id="formula_4">A ij = 1 M 1≤m≤M e i ,e j ∈Sm Φ λ (|l(e i , S m ) − l(e j , S m )|),<label>(2)</label></formula><p>where l(e, S) represents the exact location of event e in the sequence S. The range |l(e i , S m ) − l(e j , S m )| of local sequence is smaller than the pre-defined δ. Here, we use Φ λ (x) = exp(−λx), a non-increasing function, to quantify the averaged closeness of event co-occurrences reflected by each element A ij in the graph adjacency matrix A ∈ R N ×N . However, this simple exponential cooccurrence function still suffers from the scarcity problem of event co-occurrences, such that noise events can hardly be distinguished from useful ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Event Representation Learning</head><p>To further reduce the stochasticity and obscurity of event sequences, we develop a graph-based co-occurrence smoothing operation to substantiate the event correlations with its neighboring features. Specifically, we apply a multi-scale graph neural network (e.g., GAT <ref type="bibr" target="#b21">(Veličković et al. 2017</ref>)) to convolve over the event graph for event encoding:</p><formula xml:id="formula_5">               êi = Softmax(φ L (• • • φ 2 (φ 1 (e 1 i ))), e l+1 i = φ l (e l i ), l ∈ [1, L], φ l (e l i ) = W l 1 H || h=1 ReLU( ej ∈Ni α h ij W l 2 e l j ) + b l 1 , α h ij = exp(ReLU(W[e l i ||e l j ])) eq ∈N i exp(ReLU(W[e l i ||e l q ])) ,<label>(3)</label></formula><p>in which || is the concatenate operator. H is the number of attention heads in GAT. e q ∈ N i represents the 1-hop neighbours of node event e</p><formula xml:id="formula_6">i . W l 1 ∈ R d×Hd , W l 2 ∈ R d×d , W ∈ R 2d , e l i , êi ∈ R d×1</formula><p>, where d is the dimension of event embeddings, and l ∈ [1, L]. e 1 i , êi denote the randomlyinitialized and the learned graph representations of event e i , respectively. Note that e l i represents the l-th scale representation of event e i . The time complexity of multi-scale GAT is O(LHN d 2 + LHN 2 d). GR Loss. To ensure that the learned event embeddings can capture the original event co-occurrence patterns stored in event graph adjacency matrix, we introduce the graph reconstruction loss to regularize the representations of events:</p><formula xml:id="formula_7">L GR = − 1 N 2 N i=1 N j=1 (ê i êj − A ij ). (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prototype Initialization</head><p>The concept of prototype learning has provided the inherent interpretability to highlight prototype patterns from long sequences <ref type="bibr" target="#b28">(Zhang et al. 2020b</ref>). We therefore leverage the pre-learned event representations to derive the EPs, with the purpose of reflecting the corresponding corporate status. Specifically, we form the EPs ep [1:N ] via the mean shift algorithm M S(•) <ref type="bibr" target="#b3">(Cheng 1995)</ref> on events E [1:N ] , parametrized by window radius γ and kennel function κ:</p><formula xml:id="formula_8">ep [1:N ] ∼ M S(E [1:N ] ; γ, κ),<label>(5)</label></formula><p>in which ep i = F(e i ) denotes the EP label which the event e i is assigned to. Until now, we have established the initial cluster distribution of EP p γ,κ (ep</p><formula xml:id="formula_9">[1:N ] ; E [1:N ]</formula><p>) based on Eq.</p><p>(5), as shown in Figure <ref type="figure" target="#fig_1">2</ref>(a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Supervised Prototype Representation Learning</head><p>Then, we develop the self-supervised prototype representation learning module to learn the EP embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Iterative Prototype Assignment</head><p>During the early stage of training, the above event representations barely reflect the actual eventive semantics, ending up with the biased prototype distribution p γ,κ . Motivated by <ref type="bibr" target="#b16">(Pakman et al. 2018)</ref>, we develop an iterative prototype assignment process to maintain the consistency of prototype selection, via examining the permutation-invariant property possessed by desired EPs. Specifically, we first apply chain rule to reformulate p γ,κ in the log conditional probability of EP assignments onto events</p><formula xml:id="formula_10">e n ∈ E [1:N ] ,</formula><p>where n ∈ {1, • • • , N }:</p><formula xml:id="formula_11">p γ,κ (ep [1:N ] ; E [1:N ] ) = N n=2 log p(ep n ; e n , ep [1:n−1] ). (6)</formula><p>Note that the first event e 1 naturally creates the first EP ep 1 , and each of the factor in Eq. ( <ref type="formula">6</ref>) can be represented by a generic factor:</p><formula xml:id="formula_12">p(ep n ; e n , ep [1:n−1] ) = p(E [1:n] , ep [1:n] ) K+1 ep n =1 p(E [1:n] , ep [1:n] ) ,<label>(7)</label></formula><p>where K denotes the quantity of existing distinct EPs for events E [1:n−1] . The values of Eq. ( <ref type="formula" target="#formula_12">7</ref>) decide on the event e n of joining any of the K EPs, or forming a new EP cluster. Since the above EP configuration Eq. ( <ref type="formula">6</ref>) is difficult to compute directly, we propose a neural representation of Eq. ( <ref type="formula" target="#formula_12">7</ref>) to estimate these factors.</p><p>Permutation-Aware Self-Supervision</p><p>Critically, the neural representations of Eq. ( <ref type="formula" target="#formula_12">7</ref>) should respect the symmetric structures inside the joint distribution of these EP assignments p(E [1:n] , ep [1:n] ). That is, we aim to identify the permutation-invariant properties of generated EPs from: (i) the within-EP events, (ii) the between-EP clusters, (iii) the within-EP events in consecutive assignment rounds, and (iv) the unassigned events. To construct such permutation-equivariant representations ep [1:n] of EPs, we design four neural units to produce self-supervision signals:</p><p>• Intra-cluster unit is proposed to maintain the invariant permutations of Eq. ( <ref type="formula" target="#formula_12">7</ref>) under the events E [1:n] inside a specific EP. For the k-th EP sampled from K existing ones, we define the intra-cluster unit as:</p><formula xml:id="formula_13">F (k) n = i:epi=k f (ê i ).<label>(8)</label></formula><p>• Inter-cluster unit is proposed to maintain the invariant permutations of Eq. ( <ref type="formula" target="#formula_12">7</ref>) under different EPs in the same round. For each of the between-EP invariant H</p><p>n , we design the inter-cluster unit as:</p><formula xml:id="formula_15">H (k) n = K k=1 h(F (k) n ).<label>(9)</label></formula><p>• Contrastive unit is proposed to maintain the invariant permutations of Eq. ( <ref type="formula" target="#formula_12">7</ref>) under the same EP in the consecutive assignment rounds. In terms of the contrastive invariant</p><formula xml:id="formula_16">C (k)</formula><p>n in the n-th assignment round, we design the contrastive unit as:</p><formula xml:id="formula_17">C (k) n = K k=1 c(F (k) n ).<label>(10)</label></formula><p>• Unassigned unit is proposed to maintain the invariant permutations of Eq. ( <ref type="formula" target="#formula_12">7</ref>) under N − n unassigned events. In terms of the unassigned invariants G n , we design the unassigned unit as:</p><formula xml:id="formula_18">G n = N i=n+1 g(ê i ).<label>(11)</label></formula><p>Note that f (•), h(•), c(•) and g(•) provide fixed-dimensional, permutation-invariant representations of the assigned and non-assigned events, respectively. Self-supervised signals generated by these forms yield arbitrarily accurate approximations for (partially) symmetric functions, which demonstrates at least two benefits: (i) the end-to-end selfsupervision protocol allows assignment decision to adaptively improve the representation learning of event, and (ii)</p><p>the permutation-invariant units justify the validity of cluster distribution p γ,κ , such that the generated EPs will become more resistant to long-sequence stochastic noise. Note that the time complexity of all neural nets is O(4d<ref type="foot" target="#foot_0">2</ref> ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prototype Representation Learning</head><p>Once the cluster distribution of EPs is anchored, we finalize the representation vectors for all K EPs, as illustrated in Figure <ref type="figure" target="#fig_1">2</ref>(b). Here, to downscale the parameter space, for the k-th EP representation ep (k) , where k ∈ {1, • • • , K}, we simply average the embeddings of its included events as:</p><formula xml:id="formula_19">ep (k) = 1 |ep (k) | i:epi=k êi . (<label>12</label></formula><formula xml:id="formula_20">)</formula><p>KL Loss. After assigning n − 1 corporate events to K existing EPs, where K &lt; K, the possibility of allocating the n-th event e n to each of K + 1 possible EPs in Eq. ( <ref type="formula" target="#formula_12">7</ref>) is approximated by:</p><formula xml:id="formula_21">q θ (ep n = k; e n , ep [1:n−1] ) = exp(u(R (k) n )) K+1 k =1 exp(u(R (k) n ))</formula><p>.</p><p>In above, R</p><formula xml:id="formula_23">(k) n = H (k) n ||C (k) n ||G n , where || is the concate- nate operator, k ∈ {1, • • • , K + 1}. The single-layer linear mapping u(•) projects each pair of (H (k) n , C (k) n , G n ) into a real-number logit. Note that a new EP is formed with F (k) n = f (ê n</formula><p>) when the possibility of EP candidate to be assigned in Eq. ( <ref type="formula" target="#formula_22">13</ref>) peaks at k = K + 1. To minimize the distribution divergence between the initial distribution p γ,κ and approximated one q θ of EPs, we define the expected KL divergence loss as:</p><formula xml:id="formula_24">L KL = −E pγ,κ [ N n=2 log q θ (ep n ; e n , ep [1:n−1] )]. (<label>14</label></formula><formula xml:id="formula_25">)</formula><p>PC Loss. We then introduce the probabilistic contrastive loss <ref type="bibr" target="#b15">(Oord, Li, and Vinyals 2018)</ref> to maximize a lower bound on mutual information shared by the same EP in consecutive EP-assignment rounds:</p><formula xml:id="formula_26">L P C = −E pγ,κ [ K k=1 log I(C (k) n−1 , F (k) n ) K k1=1 K k2=1 I(C (k1) n−1 , F<label>(k2) n )</label></formula><p>], <ref type="bibr">(15)</ref> in which mutual information between two EPs is calculated as I(x, y) = exp(xW 1 y+b 1 ). As can be seen, the minimal value for L P C is negatively proportional to I(C</p><formula xml:id="formula_27">(k1) n−1 , F<label>(k2)</label></formula><p>n ), such that we can maximize the mutual information between same EPs in the consecutive assignment rounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Training</head><p>Overall, we define the end-to-end loss function as the weighted aggregation of the above self-supervised losses (i.e., L GR , L KL , and L P C ) with hyper-parameters α, β: </p><formula xml:id="formula_28">L = L P C + αL KL + βL GR . (<label>16</label></formula><formula xml:id="formula_29">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Applications</head><p>In order to evaluate the effectiveness of the learned EPs, we re-encode the original event sequence by replacing each event with its assigned EP label, and obtain the dynamic corporate representations for diverse applications.</p><p>Definition 2 Stock price spike prediction. Given an EP sequence in an openly traded company, the objective is to predict the existence of its day-frequent stock price spike in the next trading day:</p><formula xml:id="formula_30">f P S (cp t ) → {0, 1} t , (<label>17</label></formula><formula xml:id="formula_31">)</formula><p>where the label is set to be 1 if the percentage difference of daily stock price in two consecutive days is greater than 10% (|p t−1 − p t | /p t−1 ≥ 0.1) and 0 otherwise.</p><p>Definition 3 Corporate default risk evaluation. Given an EP sequence in a publicly listed firm, the objective is to forecast the existence of its next corporate bond default event:</p><formula xml:id="formula_32">f CD (cp t ) → {0, 1} t ,<label>(18)</label></formula><p>in which the label is set to be 1 if the corporate bond default event happens on the t-th day and 0 otherwise.</p><p>As shown in Figure <ref type="figure" target="#fig_1">2</ref>(c), the corporate representation cp t on the t-th day is represented by the most recent ν EPs. Furthermore, we introduce two separate temporal models (e.g., GRU <ref type="bibr" target="#b4">(Chung et al. 2014</ref>)) as predictive functions f P S (•), f CD (•) to fine-tune cp t for the above applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Experimental Setup</head><p>Data Description. We collected three real-world datasets for corporate events, stock price spike events, and corporate bond default events from Wind-Financial Terminal 2 . They all correspond to 549 publicly traded companies in Shanghai and Shenzhen Stock Exchanges. The first dataset is ranged from May 1, 2013, to June 1, 2020, while the rest are both ranged from May 1, 2017, to June 1, 2020. Since the number of positive cases for real corporate bond default event is very limited (≤ 170), according to <ref type="bibr" target="#b0">(Altman 1998)</ref>, we expand the original bond default event dataset with the monthly corporate default evaluation (i.e., 'improved', 'unchanged', 'deteriorated') provided by credit rating agency<ref type="foot" target="#foot_1">3</ref> , where we treat the 'improved' and 'unchanged' as negative cases, and the 'deteriorated' as positive ones. We leverage LDA <ref type="bibr" target="#b2">(Blei, Ng, and Jordan 2003)</ref> to extract event-based keywords, and categorize raw events into 176 distinct types of events as</p><formula xml:id="formula_33">E [1:N ] ,</formula><p>where N = 176. We chronologically order the groundtruth data of stock price spike and corporate bond default events for performance evaluation, where we take the first 60% as the training set, the following 20% for validation, and the rest as the test set. The statistics of the datasets are summarized in Table <ref type="table" target="#tab_0">1</ref>. Implementation Details. Our model 4 and all ten baselines are implemented with Pytorch, and optimized by the Adam algorithm <ref type="bibr" target="#b8">(Kingma and Ba 2014)</ref>. All models are trained by a NVIDIA GeForce RTX 2080 Ti. We set the learning rate to 0.0001, the batch size to 32, the dimension d of all representation vectors to 600, the decay weight λ of non-increasing function to 0.9, the scale L of graph convolutional operation to 5, the length of time window ν in EP sequence to 10, the number H of attention heads in GAT to 4, and hyper-parameters α, β to 0.5, 0.3. The kernel function κ and window radius γ of mean shift function M S(•) are set to be a Gaussian kernel and 2, respectively. f (•), h(•), c(•) and g(•) are four separate 3-layer MLPs with Parametric ReLUs, and the number of neurons in each layer is <ref type="bibr">[600,</ref><ref type="bibr">400,</ref><ref type="bibr">200]</ref>, <ref type="bibr">[200,</ref><ref type="bibr">200,</ref><ref type="bibr">200]</ref>, <ref type="bibr">[200,</ref><ref type="bibr">200,</ref><ref type="bibr">200]</ref>, <ref type="bibr">[600,</ref><ref type="bibr">400,</ref><ref type="bibr">200]</ref>, respectively. The range of local sequence variations δ is set to 23, i.e., the average number of events between two consecutive landmarks. Each temporal model consists of a single layer of GRU cell with tanh activation function. For fair comparison, we fine-tune the model parameters and set the number of training epochs to be 400 for all baselines. Evaluation Metrics. We adopt F 1 score, Precision, and Recall, three widely used metrics for evaluation.</p><p>Baselines. We compare our full approach with two prototype learning methods, three unsupervised representation learning methods, three rare event prediction methods, and two variants of SePaL:</p><p>• TSM <ref type="bibr" target="#b12">(Liu et al. 2016</ref>) exploits the sequential structures of original sequences by encoding event graph topology into prototypes.</p><p>• TapNet <ref type="bibr" target="#b28">(Zhang et al. 2020b</ref>) proposes a random dimension permutation method to form the attentional prototypes. In the experiments, we use the event sequence as input, and train separate models for two different tasks.</p><p>• Sqn2vec <ref type="bibr" target="#b14">(Nguyen et al. 2018</ref>) encodes the sequential patterns into low-dimensional vectors, and applies a gap constraint to obtain discriminative patterns. In the experiments, corporate event sequence is treated as model input.</p><p>• Event2vec (Hong et al. 2017) develops a sample generator to conduct probabilistic walks on event-based graphs for event representations. In the experiments, we adopt the identical operations in Sqn2vec.</p><p>• FHVAE <ref type="bibr" target="#b7">(Hsu, Zhang, and Glass 2017)</ref> presents an unsupervised variational autoencoder that learns disentangled and interpretable representations from sequences. In the experiments, we use the event graph as input and apply the same representations for both applications.</p><p>4 Source code: https://github.com/yuanzx33033/SePaL • AEDM <ref type="bibr" target="#b1">(Arora, Sun, and Wang 2019</ref>) introduces an embedding-based method to perform classification with data imbalanced over different classes using triple header hinge loss. In the experiments, we adopt the identical operations in FHVAE.</p><p>• SSRDVis <ref type="bibr" target="#b9">(Li et al. 2020a</ref>) proposes a hybrid sequential pattern representation learning paradigm to summarize the common sequential patterns and detect the abnormal behaviors. In the experiments, we feed the original corporate event sequence as input, and leverage the sequence location of detected outliers for two applications.</p><p>• VAE-sim <ref type="bibr" target="#b6">(Hamaguchi, Sakurada, and Nakamura 2019)</ref> learns disentangled representations from only low-cost negative samples using different variant and invariant factors. In the experiments, we generate the event sequences as input for both positive and negative labels in two application tasks.</p><p>• SePaL-G is a variant of SePaL that directly applies the eigenvectors of event graph as event embeddings, without using graph-based co-occurrence smoothing operation.</p><p>• SePaL-P is a variant of SePaL without applying selfsupervised prototype representation learning module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overall Performance</head><p>Table <ref type="table" target="#tab_1">2</ref> presents the overall performance comparisons of our model SePaL and all baselines. As can be seen, SePaL, together with its variants, outperforms all other baselines on both datasets using all metrics. Specifically, SePaL achieves (8.60%, 7.57%, 9.81%) and (8.92%, 5.07%, 13.10%) improvements beyond the optimal prototype baseline, TSM on both datasets in terms of three metrics respectively. In addition, both prototype learning methods perform relatively better than unsupervised representation learning models and rare event prediction methods, which indicates that EPs may achieve better understandings in discovering event-based sequential patterns. Finally, our full approach SePaL outperforms all the baselines by (14.24%, 12.72%, 15.45%, 15.08%, 13.42%, 16.53%) in average on two datasets. Indeed, the introduction of graph-based cooccurrence smoothing and self-supervised prototype representation learning does generate meaningful EPs, imposing positive influences on dynamic corporate profiling.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robustness Study</head><p>Figure <ref type="figure" target="#fig_3">3</ref> demonstrates the performances of SePaL and baselines on the firms that own different event sequence length.</p><p>It is notable that fewer event sequence length leads to better performances for all models, possibly because the sequential pattern increases with the dwindling cardinality. Compared with all baselines, SePaL is more stable and achieves excellent performance with longer event sequences. More concretely, with the decrease in event sequence length, the F 1 performances of SePaL can be improved from 0.6652 to 0.7034 in stock price spike prediction, and from 0.7123 to 0.7637 in corporate default risk evaluation, which indicates its effectiveness in extracting meaningful EPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative study</head><p>Visualization of EP embeddings. In Figure <ref type="figure" target="#fig_4">4</ref>, we use the t-SNE algorithm <ref type="bibr" target="#b13">(Maaten and Hinton 2008)</ref> to visualize the EP embeddings learned by SePaL and TSM in the form of a two-dimensional map, respectively. Compared to TSM, SePaL can clearly demonstrate the distinctive characteristics possessed by all events, instead of aggregating the majority of them into one gigantic cluster. We also elaborate on the dominant semantic descriptions for each detected EP, as shown in Table <ref type="table" target="#tab_2">3</ref>. Note that the semantic information here is only used to summarize each EP to better interpret our results, but not for the purpose of grouping the events. We also find some interesting observations on the extracted EPs: (i) The nearby EPs tend to be conceptually relevant, e.g., ep (1) and ep (5) , where the dividend policy of a corporate is closely related to its default risk event <ref type="bibr" target="#b20">(Sun, Wang, and Zhang 2018)</ref>; (ii) EPs with similar keywords may not be clustered in the same group. For instance, ep (8) , (11) , ep 12) are all marked with "Amendment of Corporate Articles", but they fall into three separate ESs. This could be explained by the fact that these three EPs are respectively distributed close to ep (6) , ep (1) , and ep (9) , implying that they are characterized by different levels of warnings on corporate performance. Nevertheless, they are still distributed close to each other, mainly because of the same event con- Disclosure of Performance Shareholder Meeting 20 ep (5)  Dividends Seasoned Issuance 20 ep (6)  Operating Event Regulatory Compliance 9 ep (7)  Fiscal Policy Red Warning 16 ep (8)  Disclosure of Performance Amendment of Corporate Articles 10 ep (9)  Red Warning Financial Affair 12 ep (10)  Shareholder Meeting Operating Activity 9 ep (11)  Asset Trading Amendment of Corporate Articles 12 ep (12)  tent, i.e., "Amendment of Corporate Articles". That is to say, EPs exploit more fine-grained sequential correlations via SePaL, while still could be partially consistent with contentbased clusters as well.</p><p>Analysis of EP Paths. After transforming the original event sequences into EP sequences via SePaL, we obtain corresponding EP paths to uncover the operational status of corporate. For better interpretation, we focus on frequentlyappeared EP paths in the corporates with relatively more landmark events (i.e., stock price spike events). Specifically, given a landmark event with its most recent ν EPs, we merge all consecutive and identical EPs to generate the EP path. Some interesting findings could be observed from the frequently-appeared EP paths in Table <ref type="table" target="#tab_3">4</ref>. For instance, the EP path P 1 passes through three EPs, as ep (4) → ep (10) → ep (8) . These corporates may release the latest status of corporate performance, then hold the shareholder meeting for detailed clarification and future planning. This may lead to the occurrence of stock price spike, which is triggered by investors' prompt reactions towards significant variations in enterprise status. All these paths can be grouped into the 'Yes' class, and used to discover the potential risks of the corporates. Moreover, the remaining three paths, P 4 , P 5 , P 6 , are labeled as 'No' class, indicating that these EP sequences do not provide salient patterns for stock spikes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we proposed a self-supervised prototype representation learning (SePaL) framework for event-based corporate profiling. Specifically, we first generated the cluster distribution of event prototypes based on an event graph. Then, we proposed four permutation-invariant supervision signals to learn the noise-resistance prototype embeddings.</p><p>After that, three self-supervision losses are introduced to guide the entire network update. Later, we incorporated the re-encoded prototype sequence as dynamic corporate profile, which can be fine-tuned with the temporal model for downstream applications. Finally, extensive experiments demonstrated that our SePaL can identify the evolving corporate status to benefit various financial applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustrative example of corporate event sequences.</figDesc><graphic url="image-1.png" coords="1,320.31,216.00,236.87,120.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Framework of SePaL.</figDesc><graphic url="image-2.png" coords="3,65.36,29.09,478.80,158.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2</head><label>2</label><figDesc>Figure 2 presents an overview of SePaL and its applications for financial services. It includes three major parts: (i) initializing EPs based on event graph learning, (ii) optimizing EP representation via self-supervised learning, and (iii) validating the learned time-evolving corporate representations in two representative financial applications.In the first part, we propose the Event prototype initialization module, in which the Event graph construction block first embeds raw event sequences into an event graph through a temporal skeletonization technique, and the Event representation learning applies a graph-based smoothing operation to initialize the low-dimensional representations of each event. Then, the Prototype initialization block incorporates a clustering approach to derive the initial cluster distribution of EPs based on event representations.In the second part, we develop the Self-supervised prototype representation learning module, where an N -round Iterative prototype assignment block is introduced to validate the cluster distribution of EPs. In each round, the Permutation-aware self-supervision block applies four types of neural units, i.e., Intra-cluster unit, Inter-cluster unit, Contrastive unit, and Unassigned unit, to produce selfsupervised permutation-invariant signals for current EPs. Representations of EPs are finalized in the Prototype representation learning block, and we use three separate selfsupervision losses to regularize the end-to-end training.In the third part, we incorporate the learned EPs to refine the original event sequence for dynamic corporate profiling, which can be fine-tuned using the existing temporal model for various applications, such as stock price spike prediction and corporate default risk evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Robustness check.</figDesc><graphic url="image-4.png" coords="7,61.12,142.48,221.76,111.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Visualizations of event prototypes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of datasets.</figDesc><table><row><cell>Description (corporate events)</cell><cell>Shanghai</cell><cell>Shenzhen</cell></row><row><cell># of events</cell><cell>104,072</cell><cell>161,246</cell></row><row><cell># of corporates</cell><cell>262</cell><cell>287</cell></row><row><cell>Description (downstream applications)</cell><cell cols="2">Stock price Corporate bond spike events default events</cell></row><row><cell># of positive cases</cell><cell>6,060</cell><cell>1,465</cell></row><row><cell># of total cases</cell><cell>153,480</cell><cell>18,415</cell></row><row><cell># of corporates</cell><cell>549</cell><cell>549</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Overall performance.</figDesc><table><row><cell></cell><cell cols="3">Stock price spike</cell><cell cols="3">Corporate default</cell></row><row><cell>Algorithm</cell><cell></cell><cell>prediction</cell><cell></cell><cell></cell><cell>risk evaluation</cell></row><row><cell></cell><cell>F 1</cell><cell>P.</cell><cell>R.</cell><cell>F 1</cell><cell>P.</cell><cell>R.</cell></row><row><cell>TSM</cell><cell cols="6">0.6034 0.5778 0.6314 0.6473 0.6519 0.6428</cell></row><row><cell>TapNet</cell><cell cols="6">0.5745 0.5249 0.6345 0.6126 0.5899 0.6371</cell></row><row><cell>Sqn2vec</cell><cell cols="6">0.5954 0.5771 0.6149 0.5472 0.5606 0.5344</cell></row><row><cell cols="7">Event2vec 0.4603 0.5065 0.4218 0.5017 0.4438 0.5770</cell></row><row><cell>FHVAE</cell><cell cols="6">0.5262 0.4938 0.5632 0.5745 0.5118 0.6547</cell></row><row><cell>AEDM</cell><cell cols="6">0.4037 0.4406 0.3725 0.4473 0.4920 0.4100</cell></row><row><cell>SSRDVis</cell><cell cols="6">0.5026 0.4533 0.5639 0.5527 0.5309 0.5764</cell></row><row><cell>VAE-sim</cell><cell cols="6">0.5341 0.4966 0.5777 0.5941 0.5562 0.6375</cell></row><row><cell>SePaL-G</cell><cell cols="6">0.6460 0.6125 0.6834 0.7051 0.6837 0.7279</cell></row><row><cell>SePaL-P</cell><cell cols="6">0.6242 0.5802 0.6864 0.6749 0.6629 0.6873</cell></row><row><cell>SePaL</cell><cell cols="6">0.6894 0.6535 0.7295 0.7365 0.7026 0.7738</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The semantic annotation of EPs.</figDesc><table><row><cell>EP</cell><cell>Keywords@top-1</cell><cell cols="2">Corporate Event</cell><cell>Keywords@top-2</cell><cell>Size</cell></row><row><cell>ep (1)</cell><cell cols="2">Default Risk Event</cell><cell cols="2">Operational Abnormality</cell><cell>31</cell></row><row><cell>ep (2)</cell><cell>Equity Incentive</cell><cell></cell><cell cols="2">Disclosure of Performance</cell><cell>16</cell></row><row><cell>ep (3)</cell><cell>Key Project</cell><cell></cell><cell cols="2">Credibility Evaluation</cell><cell>16</cell></row><row><cell>ep (4)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Amendment of Corporate Articles Equity Incentive 5 Sequential EP paths in stock price spike prediction. → ep (10) → ep (8) Yes 424 / 96 P2 ep (1) → ep (4) → ep (7) → ep (8) 267 / 65 P3 ep (8) → ep (9) 222 / 58 P4 ep (5) → ep (7) → ep (2) No 2153 / 39 P5 ep (1) → ep (8) → ep (12) → ep (1) → ep (8) 3870 / 45 P6 ep (1) → ep (9) → ep (4) 1318 / 15</figDesc><table><row><cell>Path ID</cell><cell>EP Path</cell><cell>Stock Price Spike Exists? (Positive / Negative) Size</cell></row><row><cell>P1</cell><cell>ep (4)</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">https://www.wind.com.cn/en/wft.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">https://www.chinaratings.com.cn/CreditRating</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was partially supported by the National Natural Science Foundation through award 91746301. Also, we are grateful to Dr. Ding Chen for insightful suggestions and assistance with financial event data.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The Importance and Subtlety of Credit Rating Migration</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">I</forename><surname>Altman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Banking &amp; Finance</title>
		<imprint>
			<biblScope unit="page" from="1231" to="1247" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep Embeddings for Rare Audio Event Detection with Imbalanced Data</title>
		<author>
			<persName><forename type="first">V</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3297" to="3301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent Dirichlet Allocation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Mean Shift, Mode Seeking, and Clustering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="790" to="799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Knowledge-Driven Event Embedding for Stock Prediction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of coling 2016, the 26th international conference on computational linguistics: Technical papers</title>
				<meeting>coling 2016, the 26th international conference on computational linguistics: Technical papers</meeting>
		<imprint>
			<date type="published" when="2014">2014. 2016</date>
			<biblScope unit="page" from="2133" to="2142" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hierarchical User Profiling for E-commerce Recommender Systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Web Search and Data Mining</title>
				<meeting>the 13th International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="223" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Event2vec: Learning Representations of Events on Temporal Sequences</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sakurada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName><surname>Ieee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asia-Pacific Web (APWeb) and Web-Age Information Management (WAIM) Joint Conference on Web and Big Data</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2019. 2017</date>
			<biblScope unit="page" from="33" to="47" />
		</imprint>
	</monogr>
	<note>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised Learning of Disentangled and Interpretable Representations from Sequential Data</title>
		<author>
			<persName><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1878" to="1889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">SSRDVis: Interactive Visualization for Event Sequences Summarization and Rare Detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visualization</title>
		<imprint>
			<biblScope unit="page" from="171" to="184" />
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A Multimodal Event-driven LSTM Model for Stock Prediction using Online News</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Knowledge and Data Engineering</title>
				<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02987</idno>
		<title level="m">Collaborative Company Profiling: Insights from an Employee&apos;s Perspective</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Temporal Skeletonization on Sequential Data: Patterns, Categorization, and Visualization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="page" from="211" to="223" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visualizing Data Using t-SNE</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sqn2Vec: Learning Sequence Representation via Sequential Patterns with a Gap Constraint</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Phung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="569" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation Learning with Contrastive Predictive Coding</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Pakman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mitelut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Paninski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00409</idno>
		<title level="m">Discrete Neural Processes</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Musgrave</surname></persName>
		</author>
		<title level="m">Event Management and Sustainability</title>
				<imprint>
			<publisher>Cabi</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Corporate Profiling of Tax-Malfeasance: A Theoretical and Empirical Assessment of Tax-Audited Australian Firms</title>
		<author>
			<persName><forename type="first">G</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">eJournal of Tax Research</title>
		<imprint>
			<biblScope unit="volume">359</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cased-based Reasoning for Medical Knowledge-based Systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Montani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bellazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Portinale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gierl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Medical Informatics</title>
		<imprint>
			<biblScope unit="page" from="355" to="367" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Corporate Payout Policy and Credit Risk: Evidence from CDS Markets</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Finance Association (AsianFA) 2018 Conference</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Graph Attention Networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adversarial Substructured Representation Learning for Mobile User Profiling</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="130" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An Adaptive Master-Slave Regularized Model for Unexpected Revenue Prediction Enhanced with Alternative Data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="601" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep Belief Networks for Predicting Corporate Defaults</title>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-F</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 24th Wireless and Optical Communication Conference (WOCC)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="159" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient Parallel Translating Embedding for Knowledge Graphs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Web Intelligence</title>
				<meeting>the International Conference on Web Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="460" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Job2Vec: Job Title Benchmarking with Collective Multi-View Representation Learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management</title>
				<meeting>the 28th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2763" to="2771" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.02835</idno>
		<title level="m">E-BERT: A Phrase and Product Knowledge Enhanced Language Model for E-commerce</title>
				<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Tap-Net: Multivariate Time Series Classification with Attentional Prototypical Network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-T</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020b</date>
			<biblScope unit="page" from="6845" to="6852" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
