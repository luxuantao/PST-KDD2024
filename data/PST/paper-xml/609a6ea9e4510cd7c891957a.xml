<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Duplex Restricted Network With Guided Upsampling for the Semantic Segmentation of Remotely Sensed Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiaoyu</forename><surname>Wan</surname></persName>
							<idno type="ORCID">0000-0002-0213-5342</idno>
						</author>
						<author>
							<persName><forename type="first">Longxue</forename><surname>Lia</surname></persName>
							<idno type="ORCID">0000-0002-3938-7359</idno>
						</author>
						<author>
							<persName><forename type="first">Haowen</forename><surname>Yan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiaosuo</forename><surname>Wu</surname></persName>
							<idno type="ORCID">0000-0002-2707-2042</idno>
						</author>
						<author>
							<persName><forename type="first">Wanzhen</forename><surname>Lu</surname></persName>
							<idno type="ORCID">0000-0003-3473-4670</idno>
						</author>
						<author>
							<persName><forename type="first">Jiali</forename><surname>Cai</surname></persName>
							<idno type="ORCID">0000-0003-2632-5084</idno>
						</author>
						<author>
							<affiliation>
								<orgName>1 School of Electronic and Information Engineering, Lanzhou Jiaotong University, </orgName>
								<address><addrLine>Lanzhou 730070, China Gansu Academy of Science, Lanzhou 730070, China Lanzhou 730070, China</addrLine></address>
							</affiliation>
						</author>
						<author>
							<affiliation>
								<orgName> 2 Institute of Sensor Technology, </orgName>
							</affiliation>
						</author>
						<author>
							<affiliation>
								<orgName> 3 Key Laboratory of Opt-Technology and Intelligent Control, Ministry of Education, Lanzhou Jiaotong University,</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Duplex Restricted Network With Guided Upsampling for the Semantic Segmentation of Remotely Sensed Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/ACCESS.2021.3065695</idno>
					<note type="submission">Received March 2, 2021, accepted March 5, 2021, date of publication March 12, 2021, date of current version March 23, 2021.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>INDEX TERMS Information distinction</term>
					<term>remote sensing</term>
					<term>semantic segmentation</term>
					<term>convolutional networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep convolutional networks are of great significance for the automatic semantic annotation of remotely sensed images. Object position and semantic labeling are equally important in semantic segmentation tasks. However, the convolution and pooling operations of the convolutional network will affect the image resolution when extracting semantic information, which makes acquiring semantics and capturing positions contradictory. We design a duplex restricted network with guided upsampling. The detachable enhancement structure to separate opposing features on the same level. In this way, the network can adaptively choose how to trade-off classification and localization tasks. To optimize the detailed information obtained by encoding, a concentration-aware guided upsampling module is further introduced to replace the traditional upsampling operation for resolution restoration. We also add a content capture normalization module to enhance the features extracted in the encoding stage. Our approach uses fewer parameters and significantly outperforms previous results on two very high resolution (VHR) datasets: 84.81% (vs 82.42%) on the Potsdam dataset and 86.76% (vs 82.74%) on the Jiage dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The semantic segmentation task is assigning semantic labels to each pixel of an image, which is fundamental work in the field of computer vision. With the boom of computer hardware and remote sensing instruments such as hyperspectral and synthetic aperture radar <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, increasingly more computer vision methods have achieved remarkable performance on remotely sensed images. Furthermore, the semantic segmentation of very high resolution (VHR) remotely sensed images has been applied in various domains, e.g., the analysis of road traffic situations, urban and rural construction, and the environmental monitoring of forest resources <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b5">[6]</ref>.</p><p>The associate editor coordinating the review of this manuscript and approving it for publication was Fahmi Khalifa .</p><p>However, accurately extracting spatial and semantic information on the Earth's environment remains particularly challenging. First, the spectral information of VHR data will be affected by uncertain factors (e.g., illumination and noise). Second, the compatibility of classifications that needs to be considered is due to great differences in the size of earth objects.</p><p>In the processing of remotely sensed images, the nature of the interclass fuzziness and intraclass discrepancies of objects can lead to inaccurate classification results. For example, artificially constructed impervious surfaces (buildings and roads) have similar spectral information but different classifications. In addition, the scale difference between objects is too large to utilize only the spectral information of an image. Deeper context relations, which require a higher degree of nonlinearity of the network to fit complex features, must also be mined. Deep learning algorithms, particularly convolutional neural networks (CNNs) <ref type="bibr" target="#b6">[7]</ref>, have surpassed traditional methods and achieved excellent results in many subjects, e.g., image classification, object detection and target tracking <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref>. Recently, the fully convolutional network (FCN) <ref type="bibr" target="#b10">[11]</ref>, an end-to-end neural network, was proposed; and the network pioneered the pixel-wise image classification and guided many outstanding semantic segmentation methods. FCN-based methods, such as SegNet <ref type="bibr" target="#b11">[12]</ref>, U-net <ref type="bibr" target="#b12">[13]</ref> and ResUNet-a <ref type="bibr" target="#b13">[14]</ref>, have achieved state-of-theart performance on benchmark dataset. The deep residual network <ref type="bibr" target="#b14">[15]</ref> solves exploding and vanishing gradient problems. The deep residual network also avoids the problem of overfitting, which allows the network depth and parameters to be continuously increased to learn more abstract features. Although the focus of researchers has shifted from extracting as much image information as possible to using the extracted information effectively, both convolution and pooling operations could affect the resolution, which inevitably makes object semantic assignment and object position marking contradictory.</p><p>There are three main solutions to ensure the accuracy of location information while extracting context information: (i) Multiscale inference <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b17">[18]</ref> inputs images of different resolutions into the network, and then the results are obtained by multiscale fusion. Although it does not change the size of the network's receptive field, it allows the network to share diverse information by adjusting the scale of the input image, which improves the network's ability to process multiscale elements. These algorithms need to recalculate the same picture multiple times, which increases the computational cost exponentially. (ii) The encoder-decoder paradigm <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref> utilizes skip connections to join the detailed information from the shallow network in the decoding stage. The loss of location information caused by downsampling is compensated by information transmission. (iii) A dilation structure <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b20">[21]</ref> increases the receptive field using dilated convolutions and enhances the understanding of semantic information with different receptive fields via multiscale fusion. Furthermore, a part of the downsampling operation is removed to reduce the precision loss caused by the reduced resolution. As the depth of the network increases, the receptive field of the latter two structures is sufficient to cover the target object. However, these methods only consider the information exchange between the features of different receptive fields while ignoring that the feature maps of the same receptive field also include detailed information and contextual information (the odd and even rows in Figure <ref type="figure" target="#fig_0">1</ref>) and merging these features together without distinction.</p><p>In this article, we propose a novel duplex path element extraction network, termed the duplex restricted network (DRN) with guided upsampling, to effectively utilize the information encoded by the backbone. Of course, we also adopt the encoder-decoder paradigm. The main contributions are as follows: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>In recent years, we have witnessed significant progress in image semantic segmentation in remote sensing, biomedicine and other disciplines. Compared with image classification and target detection, the result of semantic segmentation is accurate to the pixel level, which can provide complete category information about an image. This related work will be divided into two subsections. First, the traditional segmentation algorithms are briefly reviewed. Second, we focus on the modern algorithms based on convolutional networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. TRADITIONAL SEGMENTATION ALGORITHM</head><p>In early semantic segmentation research, researchers used classic methods to solve semantic segmentation problems.</p><p>Otsu <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, a method based on threshold selection, considers that pixels with gray levels in a certain range belong to a category. The region extraction method <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref> assumes that the pixel values in the same region are similar, and the key of this method is the selection of similarity criteria. Lakshmi and Sankaranarayanan <ref type="bibr" target="#b25">[26]</ref> summarized the edge detection method, the basic idea of which is that the pixel value will change drastically at the class boundary. The method based on probability graph models <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b28">[29]</ref> uses high-order conditional random field modeling, which can integrate more refined terminology to simulate the contextual relationships between object classes. Super-pixel algorithms <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b31">[32]</ref> adopt the similarity of the features between pixels to group pixels. They use a small number of super-pixels instead of a large number of pixels to express image features, which greatly reduces the complexity of image postprocessing.</p><p>Traditional algorithms usually use handcrafted methods to extract features instead of learning from data, so it is impossible to accurately explain all the features of an image, which results in a semantic chasm in the semantic concepts of these features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. MODERN ALGORITHMS BASED ON CONVOLUTIONAL NETWORKS</head><p>The learning network introduced by LeCun et al. <ref type="bibr" target="#b32">[33]</ref> has inspired researchers to design convolutional neural networks that surpass traditional methods in various computer vision tasks. Starting from end-to-end learning, when first introduced, the FCN achieved state-of-the-art results by a significant margin (20% relative improvement to the 62.2% mean of the class-wise Intersection over Union) on the PASCAL VOC. The authors replaced all fully connected layers with convolutional layers and restored the image resolution using the interpolation upsampling method. Ronneberger et al. <ref type="bibr" target="#b12">[13]</ref> proposed the U-net architecture and designed a symmetrical encoder-decoder paradigm to be widely used in the medical field. Chen et al. <ref type="bibr" target="#b19">[20]</ref> proposed DeepLab, showcasing the effectiveness of the atrous convolution for the first time. Atrous spatial pyramid pooling (ASPP) probes convolutional features at different sampling rates without adding additional calculations, making it possible to capture the semantics of objects at multiple scales. The research of the GCN <ref type="bibr" target="#b33">[34]</ref> proves that the stacked small filters cannot completely replace a large kernel (and an effective receptive field) when performing dense prediction tasks. Their asymmetric convolution not only increased the receptive field with a relatively small amount of calculation but also realized the dense connection inside the convolution kernel. In order to strengthen the representational power of CNNs, Yu et al. <ref type="bibr" target="#b34">[35]</ref> and Hu et al. <ref type="bibr" target="#b35">[36]</ref> added an attention mechanism to enhance the relationship between features in channels and in space. Currently, the self-attention mechanism is recognized as a powerful tool that can model the correlation of different dimensions of features. A Nonlocal network designed by Wang et al. <ref type="bibr" target="#b36">[37]</ref> realizes long-range dependencies, which helps to focus on what networks want to model. Fu et al. <ref type="bibr" target="#b37">[38]</ref> proposed the DANet based on a self-attention mechanism that integrates the correlations between all features in position and in channel, regardless of distance. The GPSNet <ref type="bibr" target="#b38">[39]</ref> proposed by Geng et al. inspires us to design the DES. The gated path selection module in the GPSNet intensively combines features from different receptive fields, allowing the network to dynamically select the desired semantic context.</p><p>The CNN analyzes images by constructing a multilayer convolutional network and using multilayer transformations to imitate the mechanism of the human brain. This allows the computer to abstract hierarchical features from large-scale data, thereby establishing a mapping from images to semantic categories. Researchers have proposed the above method to make full use of the features extracted by a convolutional network. In this work, we are inspired by ASPP and the GPSNet mentioned above to propose the CCNM and DES, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DUPLEX RESTRICTED NETWORK WITH GUIDED UPSAMPLING</head><p>In this section, we describe the overall framework of the network (section 3.1) and elaborate the implementation details of the three aspects we proposed: (i) The content capture normalization module based on atrous convolution (section 3.2). (ii) The detachable enhancement structure for obtaining different characteristics from the same receptive field (section 3.3). (iii) A novel upsampling strategy (CAGU) to achieve higher performance (section 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. OVERALL FRAMEWORK</head><p>In this article, we propose a duplex restricted network to address the contradiction between acquiring semantics and capturing positions caused by convolution and pooling operations. Our network consists of three components: a content capture normalization module, a detachable enhancement structure and concentration-aware guided upsampling. The overall segmentation framework is illustrated in Figure <ref type="figure" target="#fig_1">2</ref>. We first utilize VGG-16 <ref type="bibr" target="#b39">[40]</ref> as the backbone network to extract sophisticated features and adopt the encoder-decoder paradigm to restore the resolution. For the backbone network, there are five stages where we discard the fully connected layer to facilitate pixel-by-pixel prediction. Next, we add the CCNM after the output of each stage. The CCNM is composed of atrous convolutions with different sampling rates for learning multiscale contextual information. Then, a detachable enhancement structure (DES) separates the multiscale features extracted by the CCNM into detailed and contextual features. This allows the output of each stage to form streams of details and context. We concatenate these  two streams in channels so that we can selectively recover the related features in the decoding stage. Finally, we propose a novel guided upsampling module to replace all upsampling operations. This module generates a soft-mask, which guides the upsampling process. Because it undergoes concentrated encoding, we call it Concentrate-Aware Guided Upsampling (CAGU). CAGU can be inserted into any network structure that requires upsampling without adding too much calculation. We not only applied it in the decoding stage but also plugged it into the DES.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. CONTENT CAPTURE NORMALIZATION MODULE</head><p>The GCN <ref type="bibr" target="#b33">[34]</ref> proves that an effective receptive field is very important. Although the receptive field of the backbone network stacked by multiple small kernel convolutional layers is large, it is not enough to capture complex changes in the scale and shape of objects. DeepLab V3 <ref type="bibr" target="#b20">[21]</ref> embeds stacked atrous convolution layers and an ASPP structure, which demonstrate the effectiveness of the atrous convolution in extracting features at different scales. Encouraged by this work, we introduce a content capture normalization module, which realizes multiscale feature extraction by stacking atrous convolutions with different sampling rates.</p><p>The atrous convolution utilizes a filter with 'holes' to convolve an image and introduces zero values between the sampling points of the normal convolution. This allows multiscale information to be obtained without reducing the image resolution and increasing the number of parameters. In the scenario of inputting a two-dimensional signal x (i, j), after an atrous convolution with a convolution kernel size k, the output y (i, j) is defined as:</p><formula xml:id="formula_0">y (i, j) = k l=0 k m=0 x(i + rl, j + rm) • w l,m + b (1)</formula><p>where w l,m is the parameter of the filter, r is the dilation rate, and the standard convolution is a special case when r=1.</p><p>Our CCNM consists of stacked layers of three atrous convolutions. The dilation rates are odd numbers which are {3, 5, 7}; and the number of kernels is 128. We use the dilated convolutional layers with 3 dilation rates (i.e., 3, 5 and 7) instead of convolutional filters with kernel size 7 * 7, 11 * 11 and 15 * 15.The purpose of designing this sampling rate is to achieve global awareness in stage 4 and stage 5. After calculation, the points of the last layer of our CCNM can be spread to 31 positions of the input feature map of the module. If the input image of the network is 512 × 512, the outputs of the last two stages of the encoder part are 32 × 32 and 16 × 16 respectively, covering their main area. We carefully consider the number of filters to ensure the information extraction of each stage. After the feature map passes through the CCNM, the resolution is maintained and the depth uniformly becomes C, so that we can easily split the feature in the later stage. Applying the CCNM to each stage of the backbone network generates multilevel information L = {l i , i = 1, . . . , 5}. Given that the feature size of Level 1 is W×H×C, then</p><formula xml:id="formula_1">l i ∈ R W 2 i−1 × H 2 i−1 ×C</formula><p>Although each of our CCNM uses the same sampling rate, the size of the feature maps extracted by each stage of the backbone network are different, so we can still obtain rich information of different scales at each level. These features obtained through different sampling rates will be sensitive to multiscale objects. Moreover, the experimental results from Table <ref type="table" target="#tab_0">1</ref> and Table <ref type="table" target="#tab_1">2</ref> verify the effectiveness of the CCNM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. DETACHABLE ENHANCEMENT STRUCTURE</head><p>In the semantic segmentation task, it is equally important to mark the semantic category of the object and restore the spatial position. Since both convolution and pooling affect the resolution when extracting semantics, these two elements are naturally contradictory. Most of the current studies mix these two kinds of information together in their discussions and cannot achieve a compromise between them. Yu et al. <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref> utilized semantic branch and detail branch to extract two kinds of information. The different information contained in the same receptive field feature map is shown in Figure <ref type="figure" target="#fig_0">1</ref>. Although deep features contain information that cannot be intuitively understood, the relative position is still within the scope of shallow features. That is, after the resolution of the deep feature map is restored, the position of its information is still in the shallow feature map. Through this mechanism, contextual information and detailed information from the same receptive field can be distinguished. Therefore, we propose the detachable enhancement structure. This structure can split the backbone into a detail flow and a context flow. In this way, we achieve a good trade-off between contradictory information.</p><p>The structure of the DES is shown in Figure <ref type="figure" target="#fig_1">2</ref>. We design a detachable enhancer (DE) to separate a certain level of features. DE uses top-down and bottom-up restriction methods to weight features. Considering that there are different resolutions for multilevel features when restricting information, downsampling and upsampling operations are added (the details of our guided upsampling operation will be explained in section 3.4). The detachable enhancer has two inputs: one is the multilevel information L obtained through CCNM, and the other is the restriction function R n,i±1 generated by the previous DE. The detachable enhancer process is as follows:</p><formula xml:id="formula_2">A 1,i = Conv(l i ; θ 1,i ) ⊗ Down(R 1,i−1 ), R 1,0 = 1 (2) A 2,i = Conv(l i ; θ 2,i ) ⊗ Up(R 2,i+1 ), R 2,5 = 1 (<label>3</label></formula><formula xml:id="formula_3">)</formula><p>where Conv( * ; θ) is a 3 × 3 convolutional layer with parameter θ, followed by batch normalization and the exponential linear unit (ELU). The number of filters is the same as the number of feature channels output by the CCNM. ⊗ is the elementwise product operation. Down and Up respectively represent the max pooling and upsampling (CAGU) operations (size = 2 × 2) that adapt the latter feature. In order to ensure the accuracy of information transmission, we map A n,i through F to obtain W n,i , as show in Equation <ref type="formula" target="#formula_4">4</ref>. Our F is the boundary refinement module of Peng et al. <ref type="bibr" target="#b33">[34]</ref>.</p><formula xml:id="formula_4">F : A n,i → W n,i , i ∈ (1, . . . 5)<label>(4)</label></formula><p>We apply sigmoid activation and ELU to W n,i to obtain the restricted function R n,i and the detachable feature F n,i ,respectively. F 1,i , dominated by shallow information, is our detail flow. The corresponding F 2,i is our context flow.</p><p>We extract the detail flow from the shallowest feature map l 1 containing intense detailed information and F 1,5 still captures the fuzzy details in the deepest layer that contains intense semantics. The context flow is dominated by l 5 , which contains intense semantics. At the shallowest level F 2,1 , we still extract valuable semantic categories. Figure <ref type="figure" target="#fig_0">1</ref> shows that we separate the features of each level into contextual information and detailed information, which proves that our method is effective. We concatenate the feature maps on the same level from detail flow and context flow, so that the network can automatically choose between the two types of information in subsequent channel compression. The DES distinguishes different information of features by restricting feature expression, which clarifies the division of labor for feature extraction in deep neural networks. The deep network provides rich semantic categories, while the shallow network focuses on the restoration of boundaries. This greatly facilitates the feature extraction process and resolution recovery in the decoding stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. CONCENTRATION-AWARE GUIDED UPSAMPLING</head><p>The upsampling of feature maps is essential for end-to-end convolutional networks. Widely used upsampling operations include nearest and bilinear interpolations, which calculate the spatial distance between pixels to enlarge feature maps. SegNet <ref type="bibr" target="#b11">[12]</ref> adopts the index in the pooling layer to guide the upsampling process. These methods only consider the neighboring pixels in an image, so the results obtained are suboptimal. Deconvolution is a learnable upsampler, but it adds too many padding operations and requires a large kernel to capture context. Inspired by CARAFE <ref type="bibr" target="#b42">[43]</ref>, we designed concentration-aware guided upsampling. Our CAGU is an upsampler that automatically learns the interpolation method. It is divided into two parts: concentration-aware and nearest interpolation, as shown in Figure <ref type="figure" target="#fig_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) NEAREST INTERPOLATION</head><p>A feature map X with size W×H×C undergoes nearest interpolation to obtain an image X magnified by a factor of σ , which has severe mosaics. Nearest interpolation is the simplest image scaling algorithm, so it is easier to generalize and integrate into convolutional networks than methods such as bilinear interpolation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) CONCENTRATION-AWARE</head><p>Channel X is compressed by a 1 × 1 convolution to half of its original size, which forms a bottleneck structure for eliminating redundant information. Then, we perform concentrated encoding on the compressed features P with size W×H×C/2, where the encoding uses a 3 × 3 kernel and the number of kernels is the square of σ . The reason why we use a 3 × 3 kernel is that the edges of the object are mainly collected during upsampling, so there are not too many semantic features. Concentrated encoding encodes the compressed content to generate reassembly kernels. Finally, we reshape and activate the reassembly kernels to produce the final soft mask. Since our reshape operation restores the pixels at the same position in each layer, the depth features of the reorganized kernel will correct the mosaic generated by interpolation one by one. This soft mask learned an upsampling method, which performs elementwise products on X to guide the elimination of the mosaic in X . The implementation details are shown in Equation <ref type="formula" target="#formula_5">5</ref>.</p><formula xml:id="formula_5">Y = Retanh(conv(P; θ)) ⊗ X (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>where Retanh is the reshape operation and tanh activation function. The reshape operation is shown in Figure <ref type="figure" target="#fig_2">3</ref>. This operation restores the pixel from the same position in each channel to the adjacent position of the pixel, which forms a soft mask. θ is the parameter of a 3 × 3 × σ 2 convolution kernel.</p><p>In general, the CAGU uses the soft mask generated by concentration-aware to standardize the results of nearest interpolation, so as to realize the refinement of the object boundary. Our CAGU can adaptively learn an interpolation method to refine the upsampling. It can be applied in any end-to-end neural network. We not only apply CAGU to resolution restoration but also use it to replace the upsampling operation of the DES, as in Equation <ref type="formula" target="#formula_2">3</ref>. Its optimization effect on upsampling is discussed in section 4.3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we first introduce two datasets and their preprocessing in section 4.1. Next, we report the evaluation criteria in section 4.2. In section 4.3 we investigate the effectiveness of our method through a series of ablation and comparative experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. DATASETS AND PREPROCESSING</head><p>To fully verify the proposed method, we conduct comprehensive experiments on the Potsdam and Jiage datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) POTSDAM DATASET</head><p>The dataset includes digital orthophoto maps and digital surface models captured over the capital of Brandenburg in Germany with a ground sample distance of 5 cm. The true orthophotos are stored as TIFF files in different channel compositions, where each channel has a spectral resolution of 8 bits: IRRG with 3 channels (IR-R-G), RGB with 3 channels (R-G-B) and, RGBIR with 4 channels (R-G-B-IR). In our research, only sixteen R-G-B patches are considered for comparison purposes. As shown in Figure <ref type="figure" target="#fig_3">4</ref>, the task in our experiments is to classify all pixels in the images into six categories. Considering that the resolution of each patch is too high, this consumes too many computing resources. We crop these 16 patches into 2100 images that are the same size (512 × 512). We randomly selected 1500 images as the training set, 300 images as the validation set, and 300 images as the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) JIAGE DATASET</head><p>The dataset comes from the CCF Big Data &amp; Computing Intelligence Contest (CCF BDCI) and was collected by an unmanned aerial vehicle over a certain area of southern China. These images have 5 classes of high-quality pixellevel labels (as shown in some samples in Figure <ref type="figure" target="#fig_4">5</ref>): water, vegetation, road, building, and background. Since the dataset is small, we employ random rotation and scaling to augment the dataset during the training process. We cropped 3173 patches from six medium-resolution images with different sizes. We perform similar operations with the Potsdam dataset and randomly select 2390 images for training, 383 images for validation, and 400 images for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. EVALUATION CRITERIA</head><p>For each patch of the test set, we construct the confusion matrix to count the classification results of the segmentation model. To quantitatively evaluate the performance of the compared models, we report the Intersection over Union (IoU) of each category. Furthermore, the Pixel Accuracy (PA), F1 score (F1), and mean Intersection over Union (mIoU) are calculated as the average metrics to assess the overall performance. These metrics are calculated as:</p><formula xml:id="formula_7">Precision = TP TP + FP (6) Recall = TP TP + FN (7) F1 = 2 × Precision × Recall Precision + Recall (8) m IoU = TP TP + FP + FN (9)</formula><p>where TP, TN, FP and FN respectively represent the true positives, true negatives, false positives, and false negatives in the confusion matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. EVALUATION RESULTS</head><p>All implementations are deployed on multiple Nvidia GeForce GTX 1080 Ti (11 GB) GPUs with CUDA 10.2 and CUDNN 7.6.5 (note that we use GTX 1660Ti during the evaluation phase). For training, we adopt minibatch gradient descent <ref type="bibr" target="#b41">[42]</ref> with a batch size of 4. The Adam optimizer is selected to optimize the network, and the initial learning rate is set as 0.001. Following DANet <ref type="bibr" target="#b37">[38]</ref>, we employ the ''poly'' learning rate policy where the power is 0.9.</p><p>The parameter initialization of each model comes from ''Kaiming normal'' <ref type="bibr" target="#b40">[41]</ref>. Any regularization or dropout is not used. Without weight transplantation, our training takes 4 days. After 500 epochs, we choose the highest accuracy as the final model. We use the same settings for the hyperparameters of all experiments without any training tricks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) ABLATIVE EVALUATION</head><p>This subsection examines the effectiveness of the individual components in duplex restricted network step by step. In the following experiment, we use VGG-16 as the backbone and evaluate our method on the test sets of Potsdam and Jiage datasets. In addition, we supply ''standard deviations'' for each module to evaluate the relevance of the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>a: BASELINE</head><p>Our baseline discarded the last three fully connected layers of VGG-16 and gradually performed an add operation from the upsampling of the last block to connect the previous blocks until stage 1, which formed the baseline of the encoder-decoder paradigm. We quantitatively evaluate the performance of our baseline on two datasets. Table <ref type="table" target="#tab_0">1</ref> and Table <ref type="table" target="#tab_1">2</ref> show the Intersection over Union (IoU) of each category and the average metrics of the baseline network, respectively on the Potsdam and Jiage datasets. There are six intuitive segmentation samples of each dataset for the qualitative observation, as shown in Figure <ref type="figure" target="#fig_3">4</ref> and Figure <ref type="figure" target="#fig_4">5</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>b: ABLATION FOR THE CCNM</head><p>We design CCNM (details in Section 3.2) to provide a sufficient receptive field, where the CCNM is plugged into the output of each stage. Increasing the receptive field will improve the network's ability to distinguish multiscale objects. The building and car in Table <ref type="table" target="#tab_0">1</ref> show that the network is considerably more sensitive in these two objects after adding the CCNM. Compared with the baseline, the IoU is increased by 2.31% and 16.29% respectively. Likewise, the performance (IoU) of the two categories on the Jiage dataset has been improved, as shown in Table <ref type="table" target="#tab_1">2</ref>, where background (3.28%) and road (8.70%). When comparing the average metrics of the CCNM and the baseline, we can find that the mIoU and F1 on the two datasets have improved. We can also intuitively see this result from the third and fourth columns in Figure <ref type="figure" target="#fig_3">4</ref> and Figure <ref type="figure" target="#fig_4">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>c: ABLATION FOR THE DES</head><p>We also investigate the contribution of the DES to the segmentation effect. The DES (details in Section 3.3) is used to distinguish the conflicting information contained in features from the same level. Figure <ref type="figure" target="#fig_3">4</ref> and Figure <ref type="figure" target="#fig_4">5</ref> intuitively show the considerable gain achieved by the network in the segmentation effect. In the fifth column of Figure <ref type="figure" target="#fig_3">4</ref>, we can clearly see that both large objects (background and building) and small objects (car) can be distinguished well by the network. However, objects with unclear boundaries, such as trees, still need to be improved. The DES improves the segmentation mIoU from 73.26% to 83.19% on the Potsdam dataset, as illustrated in Table <ref type="table" target="#tab_0">1</ref>. In Table <ref type="table" target="#tab_1">2</ref>, the mIoU of the DES (84.38%) outperforms that of the CCNM (75.25%) and the baseline (70.58%). In Table <ref type="table" target="#tab_0">1</ref>, we can find that the network's ability to distinguish the background is very weak when DES is not added. With the observation in Figure <ref type="figure" target="#fig_3">4</ref>, this situation is caused by the network's confusion of the background and the building. Furthermore, major breakthroughs have been made in the IoU for both large and small objects, especially for categories that are difficult to distinguish, such as trees. This shows that the network's ability to understand contextual and detailed information has been further improved. The addition of the DES provides a huge improvement in the average segmentation metrics, which proves our point: a typical segmentation network mixes contradictory information in the same field such that the result obtained is suboptimal. The neural network needs to distinguish the information of the same receptive field. But the elementwise product operations make the testing time of CCNM+DES (0.24s) is longer than that of CCNM (0.12s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>d: ABLATION FOR CAGU</head><p>CAGU can learn an interpolation method that is data-related. We expect that CAGU can optimize resolution restoration during upsampling, especially for the edges of classified objects. Table <ref type="table" target="#tab_0">1</ref> and Table <ref type="table" target="#tab_1">2</ref> show that the IoUs of all categories have been improved to a certain extent with the addition of CAGU. The mIoU is 0.57% and 1.49% higher than before on the Potsdam and Jiage datasets, respectively, while the other average metrics have improved. The last columns in Figure <ref type="figure" target="#fig_3">4</ref> and Figure <ref type="figure" target="#fig_4">5</ref> clearly show that the edge of the segmented object is closer to the ground truth. This is sufficient to show that the interpolation method obtained through CAGU learning is superior to the traditional method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>e: ABLATION FOR BACKBONE</head><p>Different backbones (EfficientNet <ref type="bibr" target="#b44">[45]</ref>, ResNet, DenseNet <ref type="bibr" target="#b46">[47]</ref>) are used in our proposed method to guarantee the generalization ability of our method. Table <ref type="table" target="#tab_3">3</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) PERFORMANCE COMPARISON WITH THE STATE-OF-THE-ART METHODS</head><p>We compare the proposed method with 9 state-of-the-art models with different backbones, i.e., SegNet, PSPNet, DeepLab V3, GCN, DANet, etc., on two datasets. For a fair comparison, these models are trained and evaluated with inputs with a 512 × 512 resolution. The results are shown in Table <ref type="table" target="#tab_5">5</ref> and Table <ref type="table" target="#tab_6">6</ref>. We report the mIoU, parameters and time (average time per picture) of the models. The parameters indicate the number of operations to process the images of this resolution, which reflect the speed of model processing.</p><p>As illustrated in Table <ref type="table" target="#tab_5">5</ref> and Table <ref type="table" target="#tab_6">6</ref>, our method is substantially more accurate than the methods mentioned above and we have relatively fewer parameters. The duplex restricted network with guided upsampling achieves an 83.76% mIoU on the Potsdam dataset, which is better than the 82.42% of DANet (2019). However, our method consumes more time than a network with the same parameters. As shown in Table <ref type="table" target="#tab_5">5</ref>, when the parameters are about 24M, the test time of GCN, ScAttNet and RCANet is 0.1s, 0.08s and 0.1s less than our method, respectively. This is mainly caused by a large number of elementwise product operations of the DES module.   Moreover, Figure <ref type="figure" target="#fig_5">6</ref> and Figure <ref type="figure" target="#fig_6">7</ref> show the intersection over union of each category on two datasets. Our method performs well in several main categories of the IoU on the Potsdam dataset, and each category has the highest IoU on the Jiage dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this article, we present duplex restricted network to solve the conflict between understanding semantics and recovering detailed information. A content capture normalization module is proposed to provide a variety of fields that ensure maximum information extraction. A detachable enhancement structure is further introduced to distinguish contradictory information (detailed and contextual information) in features from the same level. Additionally, all upsampling operations in the model are replaced by concentration-aware guided upsampling. This data-related learnable upsampling optimizes the resolution recovery process. Experimental results on two datasets (i.e., Potsdam and Jiage) demonstrate that our proposed approach outperforms 9 state-of-the-art methods under the same experimental conditions.</p><p>The experimental results also show that the proposed method still has some limitations. The accuracy of the road on the Jiage dataset and the accuracy of the background on the Potsdam dataset are significantly lower than other categories. We will further study the accuracy in the low feature contrasts environment. In addition, reducing the impact of DES on the testing time will also be the focus on our future research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIGURE 1 .</head><label>1</label><figDesc>FIGURE 1. Visualization results of the detailed features and contextual features separated from each level feature map. The red area is the focus of this layer. F1,i and F2,i respectively represent the detail flow and the context flow. The larger i is, the deeper the network depth.</figDesc><graphic url="image-3.png" coords="2,298.68,66.06,238.00,167.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIGURE 2 .</head><label>2</label><figDesc>FIGURE2. An overview of the duplex restricted network with guided upsampling. Our model exploits VGG-16<ref type="bibr" target="#b39">[40]</ref> as the backbone (left) to extract multiscale information. The detachable enhancement structure includes a detachable downsampling enhancer and a detachable upsampling enhancer, which respectively generate the detail flow and context flow.</figDesc><graphic url="image-6.png" coords="4,37.77,187.31,238.00,161.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FIGURE 3 .</head><label>3</label><figDesc>FIGURE 3. The details of concentration-aware guided upsampling. The concentration-aware portion (top) generates a soft mask to guide the upsampling process.</figDesc><graphic url="image-9.png" coords="6,37.77,66.06,238.00,120.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FIGURE 4 .</head><label>4</label><figDesc>FIGURE 4. Visual improvement between the baseline and the three proposed methods on the Potsdam test set.</figDesc><graphic url="image-11.png" coords="7,37.77,344.86,238.00,237.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>FIGURE 5 .</head><label>5</label><figDesc>FIGURE 5. Visual improvement between the baseline and the three proposed methods on the Jiage test set.</figDesc><graphic url="image-12.png" coords="7,298.68,215.77,238.00,236.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>FIGURE 6 .</head><label>6</label><figDesc>FIGURE 6. Results of the different state-of-the-art networks for each class on the Potsdam test set.</figDesc><graphic url="image-16.png" coords="9,51.57,462.19,210.41,180.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>FIGURE 7 .</head><label>7</label><figDesc>FIGURE 7. Results of different state-of-the-art networks for each class on the Jiage test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 .</head><label>1</label><figDesc>Results of the ablation study on the Potsdam test set considering the IoU for each class, the mIoU, precision and the F1. The Highest score is marked with bold.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2 .</head><label>2</label><figDesc>Results of the ablation study on the Jiage test set considering the IoU for each class, the mIoU, the precision and the F1. The Highest score is marked with bold.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>and Table4show the mIoU, parameters, and testing time (the average time to test a 512 × 512 picture) of our method in different backbone networks. Our method achieves the best results on EfficientNet: 84.81% on the Potsdam dataset and 86.</figDesc><table /><note>76% on the Jiage dataset. The results on other backbones have also been improved, which shows that our method has a certain generalization ability. Considering the influence of the parameters on training efficiency and testing time, we recommend using VGG-16 or EfficientNet as the backbone.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3 .</head><label>3</label><figDesc>The results of the mIoU and computational cost of our method under different backbones on the Potsdam test set.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 4 .</head><label>4</label><figDesc>The results of the mIoU and computational cost of our method under different backbones on the Jiage test set.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 5 .</head><label>5</label><figDesc>Quantitative comparison with the 9 state-of-the-art methods on the Potsdam test set.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 6 .</head><label>6</label><figDesc>Quantitative comparison with the 9 state-of-the-art methods on the Jiage test set.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ VOLUME 9, 2021</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1">VOLUME 9, 2021   </note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Key Research and Development Program under Contract 2017YFB0504203, in part by the Gansu Science and Technology Program (Key Research and Development Program) under Contract 20YF8GA035, in part by the Planned Project of Gansu Science and Technology Department under Contract 18JR3RA123, in part by the Department of Education of Gansu Province under Contract 2018B-029, in part by the Department of Housing and Urban Rural Development of Gansu Province under Contract JK2017-24, in part by the Youth Science Fund Project of Lanzhou Jiaotong University under Contract 2016003, in part by the Science and Technology Bureau of Chengguan District, Lanzhou City, under Grant 2018-4-5, and in part by the Innovation Fund of Gansu Academy of Science and Technology under Contract 2018QN-05.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hyperspectral and multispectral data fusion: A comparative review of the recent literature</title>
		<author>
			<persName><forename type="first">N</forename><surname>Yokoya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Grohnfeldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chanussot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">IEEE Geosci. Remote Sens. Mag.</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="29" to="56" />
			<date type="published" when="2017-06">Jun. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A tutorial on synthetic aperture radar</title>
		<author>
			<persName><forename type="first">A</forename><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prats-Iraola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Younis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Krieger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hajnsek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Papathanassiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">IEEE Geosci. Remote Sens. Mag.</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="6" to="43" />
			<date type="published" when="2013-03">Mar. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation for semantic segmentation of remote sensing images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Remote Sens.</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">701</biblScope>
			<date type="published" when="2020-02">Feb. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hierarchical weakly supervised learning for residential area semantic segmentation in remote sensing images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="117" to="121" />
			<date type="published" when="2020-01">Jan. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">CSE-HRNet: A context and semantic enhanced high-resolution network for semantic segmentation of aerial imagery</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="182475" to="182489" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Traffic scene semantic segmentation using self-attention mechanism and bi-directional GRU to correlate context</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">386</biblScope>
			<biblScope unit="page" from="293" to="304" />
			<date type="published" when="2020-04">Apr. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2017-05">May 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for image classification: A comprehensive review</title>
		<author>
			<persName><forename type="first">W</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2352" to="2449" />
			<date type="published" when="2017-09">Sep. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning multi-domain convolutional neural networks for visual tracking</title>
		<author>
			<persName><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="4293" to="4302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient segmentation pyramid network</title>
		<author>
			<persName><forename type="first">T</forename><surname>Singha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-S</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dunstan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Neural Inf. Process</title>
				<meeting>Int. Conf. Neural Inf. ess<address><addrLine>Cham, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="386" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015-06">Jun. 2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">SegNet: A deep convolutional encoder-decoder architecture for robust semantic pixel-wise labelling</title>
		<author>
			<persName><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.07293</idno>
		<ptr target="http://arxiv.org/abs/1505.07293" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Med</title>
				<meeting>Int. Conf. Med<address><addrLine>Cham, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ResUNet-A: A deep learning framework for semantic segmentation of remotely sensed data</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">I</forename><surname>Diakogiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Waldner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Caccetta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">ISPRS J. Photogramm. Remote Sens.</title>
		<imprint>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="94" to="114" />
			<date type="published" when="2020-04">Apr. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="3640" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="3194" to="3203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Hierarchical multi-scale attention for semantic segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10821</idno>
		<ptr target="http://arxiv.org/abs/2005.10821" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Context prior for scene segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
				<meeting>IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06">Jun. 2020</date>
			<biblScope unit="page" from="12416" to="12425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018-04">Apr. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<ptr target="http://arxiv.org/abs/1706.05587" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A survey of thresholding techniques</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Sahoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soltani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K C</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis., Graph., Image Process</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="233" to="260" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A threshold selection method from gray-level histograms</title>
		<author>
			<persName><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="66" />
			<date type="published" when="1979-01">Jan. 1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Seeded region growing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="641" to="647" />
			<date type="published" when="1994-06">Jun. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Watersheds in digital spaces: An efficient algorithm based on immersion simulations</title>
		<author>
			<persName><forename type="first">L</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Soille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="583" to="598" />
			<date type="published" when="1991-06">Jun. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A study of edge detection techniques for segmentation computing approaches,&apos;&apos; in Proc</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lakshmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Sankaranarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Control Automat. Special Issue Comput. Aided Soft Comput. Techn. Imag. Biomed. Appl. (CASCT)</title>
		<imprint>
			<biblScope unit="page" from="35" to="40" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Associative hierarchical CRFs for object class image segmentation</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">U</forename><surname>Ladický</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 12th Int. Conf. Comput. Vis</title>
				<meeting>IEEE 12th Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2009-10">Sep./Oct. 2009</date>
			<biblScope unit="page" from="739" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected CRFs with Gaussian edge potentials,&apos;&apos; in Proc</title>
		<author>
			<persName><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="109" to="117" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multiscale conditional random fields for image labeling</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Carreira-Perpinan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
				<meeting>IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004-06">Jun. 2004</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning a classification model for segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2003.1238308</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th IEEE Int. Conf. Comput. Vis</title>
				<meeting>9th IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2003-10">Oct. 2003</date>
			<biblScope unit="page" from="10" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Seeds: Superpixels extracted via energy-driven sampling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Van Den Bergh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Boix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Roig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>De Capitani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
				<meeting>Eur. Conf. Comput. Vis<address><addrLine>Cham, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="13" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">SLIC superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Süsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
			<date type="published" when="2012-11">Nov. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989-12">Dec. 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Large kernel matters-Improve semantic segmentation by global convolutional network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07">Jul. 2017</date>
			<biblScope unit="page" from="4353" to="4361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning a discriminative feature network for semantic segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE/CVF Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018-06">Jun. 2018</date>
			<biblScope unit="page" from="1857" to="1866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE/CVF Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018-06">Jun. 2018</date>
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018-06">Jun. 2018</date>
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
				<meeting>IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06">Jun. 2019</date>
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Gated path selection network for semantic segmentation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.06819</idno>
		<ptr target="http://arxiv.org/abs/2001.06819" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<ptr target="http://arxiv.org/abs/1409.1556" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">BiSeNet v2: Bilateral network with guided aggregation for real-time semantic segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02147</idno>
		<ptr target="http://arxiv.org/abs/2004.02147" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis. (ECCV)</title>
				<meeting>Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="325" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">CARAFE: Content-aware reassembly of features</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV)</title>
				<meeting>IEEE/CVF Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10">Oct. 2019</date>
			<biblScope unit="page" from="3007" to="3016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07">Jul. 2017</date>
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">'</forename><surname>Efficientnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<ptr target="http://arxiv.org/abs/1905.11946" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Relation matters: Relational contextaware fully convolutional network for semantic segmentation of highresolution aerial images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="7557" to="7569" />
			<date type="published" when="2020-11">Nov. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">DenseU-net-based semantic segmentation of small objects in urban remote sensing images</title>
		<author>
			<persName><forename type="first">R</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="65347" to="65356" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">His research interests include computer graphics, computer vision, machine learning, and image processing. LONGXUE received the B</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tao</surname></persName>
		</author>
		<idno type="DOI">10.1109/LGRS.2020.2988294</idno>
	</analytic>
	<monogr>
		<title level="m">He is currently pursuing the M.S. degree with Lanzhou Jiaotong University</title>
		<title level="s">IEEE Geosci. Remote Sens. Lett., early access</title>
		<meeting><address><addrLine>Hebei, China; Lanzhou, China; Tianjin, China; Lanzhou, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1988">Apr. 29, 2020. 2019. 1988</date>
		</imprint>
		<respStmt>
			<orgName>XIAOYU WANG received the B.Eng. degree from Hebei University ; Eng. degree from Tianjin University ; Associate Professor with Lanzhou Jiaotong University</orgName>
		</respStmt>
	</monogr>
	<note>His research interests include electronic technology applications. integrated circuit technology, and special semiconductor devices</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">He is currently pursuing the M.S. degree with Lanzhou Jiaotong University. His research interests include microelectronics, solid-state electronics, computer vision, machine learning, and image processing. WANZHEN LU received the B</title>
	</analytic>
	<monogr>
		<title level="m">he was a Sub-Decanal with the School of Mathematics and Software Engineering</title>
		<title level="s">Lanzhou Jiaotong University. XIAOSUO WU received the B.Sc. and M.S. degrees from Lanzhou Jiaotong University</title>
		<meeting><address><addrLine>Hubei, China; Hubei; Gansu, China; Lanzhou, China; Lanzhou; Hainan, China; Lanzhou, China; Lanzhou, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002. 2005 to 2012. 2004. 2011. 2017. 2019</date>
		</imprint>
		<respStmt>
			<orgName>HAOWEN YAN received the B.Eng. degree from the Wuhan Technical University of Surveying and Mapping ; Lanzhou Jiaotong University ; D. degree from Lanzhou University ; Eng. degree from Hainan University ; Jiaotong University ; CAI received the B.Eng. degree from Lanzhou Jiaotong University</orgName>
		</respStmt>
	</monogr>
	<note>His research interests include computer graphics, computer vision, machine learning, and image processing. where she is currently pursuing the M.S. degree. Her research interests include computer graphics, computer vision, machine learning, and image processing</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
