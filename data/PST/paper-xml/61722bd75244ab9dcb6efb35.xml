<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A DATA-CENTRIC OPTIMIZATION FRAMEWORK FOR MACHINE LEARNING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Oliver</forename><surname>Rausch</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tal</forename><surname>Ben-Nun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nikoli</forename><surname>Dryden</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrei</forename><surname>Ivanov</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shigang</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Torsten</forename><surname>Hoefler</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A DATA-CENTRIC OPTIMIZATION FRAMEWORK FOR MACHINE LEARNING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">DaCeML supports loading models from the Open Neural Network eXchange (ONNX) format (ONNX, 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Rapid progress in deep learning is leading to a diverse set of quickly changing models, with a dramatically growing demand for compute. However, as frameworks specialize optimization to patterns in popular networks, they implicitly constrain novel and diverse models that drive progress in research. We empower deep learning researchers by defining a flexible and user-customizable pipeline for optimizing training of arbitrary deep neural networks, based on data movement minimization. The pipeline begins with standard networks in PyTorch or ONNX and transforms computation through progressive lowering. We define four levels of general-purpose transformations, from local intra-operator optimizations to global data movement reduction. These operate on a data-centric graph intermediate representation that expresses computation and data movement at all levels of abstraction, including expanding basic operators such as convolutions to their underlying computations. Central to the design is the interactive and introspectable nature of the pipeline. Every part is extensible through a Python API, and can be tuned interactively using a GUI. We demonstrate competitive performance or speedups on ten different networks, with interactive optimizations discovering new opportunities in EfficientNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The modern development of deep learning is spearheaded by the conflux of algorithms <ref type="bibr" target="#b6">(Bottou et al., 2018)</ref>, data <ref type="bibr" target="#b17">(Halevy et al., 2009;</ref><ref type="bibr" target="#b52">Sun et al., 2017)</ref>, hardware <ref type="bibr" target="#b45">(Raina et al., 2009;</ref><ref type="bibr" target="#b27">Jouppi et al., 2017)</ref>, and systems <ref type="bibr" target="#b0">(Abadi et al., 2015;</ref><ref type="bibr" target="#b42">Paszke et al., 2019)</ref>. Today, machine learning systems have become critical in light of the increasing complexity of models and the breadth of emerging hardware. Training performance in particular is key to both researcher productivity and reducing the environmental impact of machine learning <ref type="bibr" target="#b51">(Strubell et al., 2019;</ref><ref type="bibr" target="#b43">Patterson et al., 2021)</ref>. To this end, systems and compilers such as XLA <ref type="bibr" target="#b17">(Google, 2021)</ref>, ONNX Runtime <ref type="bibr" target="#b35">(Microsoft, 2021)</ref>, and TorchScript (PyTorch <ref type="bibr" target="#b44">Team, 2020)</ref> are widely used to automatically optimize training.</p><p>Most such compilers tend to focus on operator-centric optimization, that is, defining specific rule-sets based on a set of predefined building blocks. Thus, these frameworks tend to perform very well on popular neural networks, but often have limited performance improvements on new, unseen models -which many researchers would like to explore, but may not be able to train given the high compute costs. Indeed, this effect has led to a situation where the models that perform well on existing hardware and systems are more likely to succeed than others <ref type="bibr" target="#b19">(Hooker, 2020)</ref>.</p><p>Preprint.</p><p>In this work, we propose to shift the paradigm from operators to their memory access patterns by performing datacentric DNN optimization. It is well-established that the performance of modern DNNs such as Transformers hinges on data movement minimization <ref type="bibr">(Ivanov et al., 2021;</ref><ref type="bibr" target="#b35">Microsoft, 2020)</ref>, and that a certain FLOP reduction does not guarantee matching speedup <ref type="bibr" target="#b53">(Tan &amp; Le, 2019)</ref>. Up until now such optimizations have been performed by specialized engineering teams for specific architectures, remaining out of reach for most research groups <ref type="bibr" target="#b3">(Barham &amp; Isard, 2019)</ref>.</p><p>We present DaCeML<ref type="foot" target="#foot_0">1</ref> , the Data-Centric Machine Learning framework, which provides a simple, flexible, and customizable Python-based pipeline for optimizing training and empowering deep learning research. DaCeML seamlessly integrates with PyTorch <ref type="bibr" target="#b42">(Paszke et al., 2019)</ref> and ONNX <ref type="bibr" target="#b40">(ONNX, 2021)</ref> to enable accelerating and tuning existing models, both in evaluation and backpropagation. The input models are then optimized with a general-purpose transformation pipeline, which reduces data movement at fine and coarse grain, regardless of the internal computation. Lastly, DaCeML provides interfaces to programmatically and interactively guide the optimization process further.</p><p>Internally, the framework uses Stateful Dataflow Multigraphs (SDFGs) <ref type="bibr" target="#b4">(Ben-Nun et al., 2019)</ref> as a data-centric, hierarchical graph-based intermediate representation, which enables it to work with operators and data movement at all levels, from registers to distributed memory. The DaCeML optimization pipeline begins with a standard PyTorch or ONNX model and transforms computation through progressive lowering, using four levels of general-purpose transformations: coarse-grained; local data movement reduction; global data layout optimization; and hardware specialization. Data-centric optimizations manifest in different ways, especially when training is considered. One of the several unique controls DaCeML provides, for example, is choosing whether to recompute or retain data for backpropagation. With the included automatic optimizations, DaCeML often already matches or outperforms modern frameworks.</p><p>The central focus of DaCeML is the ability to then go further -it unpacks the DNN compiler-black box and puts the machine learning practitioner in the driver's seat. This starts with the visual, introspectable intermediate representation that makes finding and understanding performance issues, such as excessive data movement, intuitive. These are subsequently addressed by manipulating data movement or the data layouts of the parameters and intermediate storage. The SDFG IR allows automated or manual searches to be performed on multiple granularities simultaneously, rather than in the traditional compiler-based fixed set of passes. These can be performed either using a Python API or interactively using the Visual Studio Code <ref type="bibr" target="#b35">(Microsoft, 2021)</ref> IDE.</p><p>We provide an overview of DaCeML's design, its approach to progressive lowering, and its transformations, including novel optimizations not supported by other frameworks. In particular, we demonstrate up to 3.43× speedups over prior best on non-mainstream activations; state-of-the-art performance with automatic optimization of a wide range of DNNs from various domains <ref type="bibr" target="#b5">(Bochkovskiy et al., 2020;</ref><ref type="bibr" target="#b13">Devlin et al., 2019;</ref><ref type="bibr" target="#b18">He et al., 2015;</ref><ref type="bibr" target="#b33">Long et al., 2015;</ref><ref type="bibr" target="#b38">Naumov et al., 2019;</ref><ref type="bibr" target="#b56">van den Oord et al., 2016;</ref><ref type="bibr" target="#b48">Sandler et al., 2019;</ref><ref type="bibr" target="#b53">Tan &amp; Le, 2019;</ref><ref type="bibr" target="#b54">Tolstikhin et al., 2021;</ref><ref type="bibr" target="#b62">Zagoruyko &amp; Komodakis, 2017)</ref>, compared with PyTorch, TensorFlow + XLA, and JAX; and two guided optimization case studies on BERT <ref type="bibr" target="#b13">(Devlin et al., 2019)</ref> and EfficientNet <ref type="bibr" target="#b53">(Tan &amp; Le, 2019)</ref>, the former highlighting the importance of data layout and the latter showing that nonlocal data movement minimization can yield 1.33× speedup over cuDNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND RELATED WORK</head><p>Most closely related to our work is that of <ref type="bibr">Ivanov et al. (2021)</ref>, which also takes a data-centric view of optimizing deep learning training and finds that transformers <ref type="bibr" target="#b58">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b13">Devlin et al., 2019)</ref> are memory bound, illustrating the importance of data-centric analysis. However, their work focuses exclusively on BERT <ref type="bibr" target="#b13">(Devlin et al., 2019)</ref>, and is entirely manual. Similarly, DeepSpeed <ref type="bibr" target="#b35">(Microsoft, 2020</ref>) also provides manually-optimized primitives for transformers. Numerous other works have focused on specific optimizations <ref type="bibr" target="#b16">(Frostig et al., 2018;</ref><ref type="bibr" target="#b26">Jia et al., 2019b;</ref><ref type="bibr" target="#b49">Sivathanu et al., 2019;</ref><ref type="bibr" target="#b2">Baghdadi et al., 2019;</ref><ref type="bibr" target="#b57">Vasilache et al., 2018;</ref><ref type="bibr" target="#b30">Lethin, 2017;</ref><ref type="bibr" target="#b60">Wei et al., 2018;</ref><ref type="bibr" target="#b55">Truong et al., 2016;</ref><ref type="bibr" target="#b59">Venkat et al., 2019;</ref><ref type="bibr" target="#b14">Dong et al., 2019;</ref><ref type="bibr" target="#b15">Elango et al., 2018;</ref><ref type="bibr" target="#b20">Hu et al., 2020;</ref><ref type="bibr" target="#b41">Oyama et al., 2018;</ref><ref type="bibr" target="#b31">Li et al., 2016;</ref><ref type="bibr" target="#b63">Zheng et al., 2020;</ref><ref type="bibr" target="#b32">Li et al., 2020;</ref><ref type="bibr" target="#b50">Steiner et al., 2020;</ref><ref type="bibr" target="#b61">Yang et al., 2021)</ref> that can be implemented as DaCeML transformations.</p><p>Training vs. inference compilation Inference optimization elides several concepts required in training. Most importantly, no automatic differentiation is needed. Secondly, as opposed to training, intermediate activations need not be stored for backpropagation, which drastically changes the optimization search space. Thirdly, several operators (e.g., batch normalization, dropout, convolution) behave differently during training. For example, batch normalization stores running statistics throughout the process, and convolutions that use basis transformations (FFT, Winograd) can pre-transform the weights once, since they will not change during inference. Lastly, apart from backpropagation, gradient updates and stochastic optimizer rules must be applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>State-of-the-art DNN compilers</head><p>Researchers have often used custom kernels, linear algebra primitives (e.g., BLAS), and vendor-optimized libraries (e.g., cuDNN, <ref type="bibr" target="#b10">Chetlur et al., 2014</ref><ref type="bibr">, oneDNN, Intel, 2021a)</ref>. Recently, with the modularization and diversity of DNNs, the focus is shifting towards using general compiler infrastructure in DNN optimization, which uses Just-in-Time (JIT) compilation to both optimize individual operators and fuse them.</p><p>The methods by which DNN compilers optimize (transform) code can be classified into three categories: graph rewriting rules, expansions, and global passes. Graph rewriting represents DNNs as DAGs and pattern-matches certain subgraphs to replace them with others. Expansions convert a known operation (e.g., batch normalization) directly into an explicit, pre-optimized version. Finally, global passes operate on the entire code (e.g., memory scheduling). Ultimately, the first two categories, and sometimes the third, are implemented on an operator-centric basis. Below, we discuss state-of-the-art DNN compilers and their inner workings.</p><p>XLA Advanced compiler infrastructure (Google, 2021) used by TensorFlow <ref type="bibr" target="#b0">(Abadi et al., 2015)</ref> and JAX <ref type="bibr" target="#b7">(Bradbury et al., 2018)</ref>, which contains multiple intermediate representations (e.g., HLO, LLO) and various domain-specific expansion transformations and global passes. Apart from those, general purpose optimizations (such as dead-code elimination) are performed by the LLVM <ref type="bibr" target="#b28">(Lattner &amp; Adve, 2004)</ref> and MLIR <ref type="bibr" target="#b29">(Lattner et al., 2020)</ref> (experimental) infrastructure. Prior to XLA, graph rewriting rules in TensorFlow were provided by a component called Grappler.</p><p>TorchScript (torch.jit) The main static optimization effort in PyTorch <ref type="bibr" target="#b47">(Rotem et al., 2018;</ref><ref type="bibr" target="#b44">PyTorch Team, 2020;</ref><ref type="bibr" target="#b42">Paszke et al., 2019)</ref>. Through either tracing or Python introspection, TorchScript can convert PyTorch modules into Others Frameworks that maintain a graph structure also provide optimizing transformations. ONNX Runtime (Microsoft, 2021) provides graph rewrite rules and global passes, mostly focused on cleaning up artifacts resulting from the ONNX (ONNX, 2021) format (e.g., constant folding) and algebraic fusion involving chained ONNX operators (e.g., MatMulAddFusion). OpenVINO (Intel, 2021b) uses nGraph <ref type="bibr" target="#b11">(Cyphers et al., 2018)</ref> with graph rewrite rules and single-node matching transformations to optimize inference. For interactivity, it allows users to deselect certain optimization passes on given node names through its command-line optimizer. TVM <ref type="bibr" target="#b9">(Chen et al., 2018)</ref> provides passes on its two IRs (Relay, TIR) based on statement visitors/mutators, similar to AST manipulation. TASO <ref type="bibr" target="#b25">(Jia et al., 2019a</ref>) also provides subgraph pattern rewrite rules, focused on linear algebra. Ten-sorRT (NVIDIA, 2021) and cuDNN <ref type="bibr" target="#b10">(Chetlur et al., 2014)</ref> provide functionality to fuse elementwise operations to DNN primitives, the former for the purpose of accelerating inference and the latter for training as well.</p><p>Limitations Each of the aforementioned DNN compilers is affected by at least one of three limiting factors. First, most of the transformations rely on specific operator types, which change from network to network, and over time. Second, the transformations are written in lower-level languages (typically C++) and sometimes require compiler analysis knowledge to develop. Lastly, the transformations are applied in passes, usually in a greedy fashion, inhibiting opportunities for further tuning and analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SYSTEM OVERVIEW</head><p>We propose to tackle the problem of DNN optimization through a holistic and user-centered approach. Data-Centric Machine Learning (DaCeML) is a semi-automated, userextensible pipeline for compiling and optimizing deep learning workloads. It takes PyTorch models and ONNX files, optimizes the program through data-centric means, and generates code with state-of-the-art performance for multiple platforms. The goal of DaCeML is threefold: (1) Usability: keeping the simplicity and expressiveness of PyTorch and its powerful model definition; (2) Generality: using generalpurpose transformations that generalize well to new models, from coarse-grained optimizations to low-level code generation; and (3) Interactivity: translating the simplicity of defining models to optimizing them by enabling easy extensibility and interactive reasoning.</p><p>To achieve this goal, DaCeML defines a progressive lowering pipeline, depicted in Figure <ref type="figure" target="#fig_0">1</ref>. At the core of DaCeML is the data-centric Stateful DataFlow multiGraph (SDFG) representation <ref type="bibr" target="#b4">(Ben-Nun et al., 2019)</ref>, a parametric graph IR designed for high-performance computing. SDFGs promote separation of concerns between domain scientists and performance engineers by optimizing the data movement of a program separately from the algorithmic part of the input code. The DaCe framework is written in Python and has recently powered multiple applications in several domains, including quantum chemistry <ref type="bibr" target="#b64">(Ziogas et al., 2019)</ref> and numerical weather prediction <ref type="bibr" target="#b12">(de Fine Licht et al., 2020)</ref>, achieving state-of-the-art performance on CPUs, GPUs, and FPGAs.</p><p>Once within that representation, DaCeML can automatically optimize inference, forward, or backward passes of a model to performance that is on par with (or faster than) state-ofthe-art DNN compilers. Furthermore, DaCeML's API is geared towards extensibility -from defining new operator implementations, through local and global data movement planning, to low-level work partitioning in code generation -allowing users to assume direct control and guide the optimization process towards faster performance. The process can be done interactively, through APIs and IDE support, or through combinatorial searches over the optimization space. ONNX library nodes The predefined semantics of library nodes allows them to be expanded to lower-level implementations, either "native" SDFG elements (tasklets, memlets, access nodes, and maps) or fast library calls (e.g., cuBLAS and cuDNN). As we shall show, native expansions can sometimes be optimized further than such libraries. In DaCeML, each ONNX operator is represented by a library node. Native implementations for many ONNX operators use the NumPy DaCe frontend to generate SDFGs. For example:</p><formula xml:id="formula_0">@python_pure_op_implementation def Softplus(X, Y): Y[:] = numpy.log(1 + numpy.exp(X))</formula><p>As not all operators are natively implemented, and the ONNX standard is expected to grow, we also support a fall- back for any ONNX operator, using ONNX Runtime (Microsoft, 2021) as a backend. Specifically, the expansions generate code that eagerly calls the operators within the compiled module.</p><p>Progressive lowering of library nodes Several library node expansions lower to other ONNX nodes. As an example, the MatMul node is lowered first to an Einstein sum (Einsum), which enables some tensor-contraction algebraic optimizations. Another is the im2col implementation of Conv, in which a Gemm node is included in the lowering output. This multi-level, progressive lowering reduces implementation redundancies and facilitates operator kernel authoring due to reuse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Symbolic Automatic Differentiation</head><p>To compile and optimize for training, DaCeML uses sourcetransformation-based reverse-mode Automatic Differentiation (AD) to compute Vector-Jacobian Products (VJP) of the operators. Guided by the principle of generality, all lower level optimization on the backward pass is done using the same optimizing transformations as the forward pass. To enable this, the DaCeML AD engine transforms a forward pass SDFG into the corresponding SDFG that computes the required gradients. Critically, the differentiation is performed on the lowered, native SDFG, removing the need for manually specified VJPs.</p><p>Differentiating SDFGs is challenging, since nodes not only represent computation, but memory accesses and parametric replication. It is important to differentiate the computation symbolically, even if all sizes are known, since the access pattern (memlets) within each operator is still symbolic.</p><p>Given an SDFG, the engine first determines the subgraph to differentiate, based on the output access nodes and the access nodes of inputs that require gradients (part of the metadata obtained from PyTorch). The subgraph is then traversed in reverse topological order and reversed as follows. Access nodes are reversed by "inverting" the access of the node, and replacing the data that is written/read with the adjoint of that data. Maps can be reversed by simply converting map entry nodes to map exit nodes and vice versa. The reverse of a code node --whether tasklet, library node, or nested SDFG --is a node that computes the VJP of the forward node. The inner tasklets of most machine learning operators work with scalar values. To automatically differentiate these scalar expressions, DaCeML uses the SymPy <ref type="bibr" target="#b34">(Meurer et al., 2017)</ref> symbolic differentiation engine (see example in Figure <ref type="figure" target="#fig_1">3</ref>). Nested SDFGs are reversed recursively, and in the case of library nodes, they are lowered to their native SDFG implementation for further differentiation, with the possibility to provide manual backward expansions.</p><p>The VJPs of code nodes often require values that were inputs to the corresponding forward node. These values are either forwarded by storing the value in the forward pass and reading it in the backward pass, or recomputed in the backward pass.</p><p>Manual backward expansions DaCeML's AD engine is able to automatically differentiate almost all operator implementations, producing backward passes comparable to the hand-written implementations. However, it is often beneficial to specify manual backward operators (e.g., Einsum can be reversed directly for performance or Softmax/LogSoftmax for numerical stability). DaCeML provides this capability by manual expansions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">OPTIMIZATION PIPELINE</head><p>DaCeML follows DaCe's white-box optimization approach, in which optimization can be performed automatically as a starting point, and then is interactive and guided by performance engineers. We further simplify the interface and extend its capabilities, in order to allow ML practitioners to productively write transformations of their own.  where users provide the subgraph to match (pattern), and replacement instructions (replacement), which include returning a new node or nested SDFG and instructions on how to reconnect to the pattern graph or add new arrays (via the mapping in the return value). For more complex behavior, any method (exact matching condition, application, reconnection) can be overridden.</p><p>The transformations provided in DaCeML's standard set can be categorized into four distinct levels, ordered by their application as the graph is lowered: (1) Coarse-grained transformations on the ONNX-SDFG representation; (2) local data movement reduction, e.g., operator fusion and replication; (3) global (network-wide) data movement optimization; and (4) hardware specialization and workload partitioning. From those categories we sub-select transformations to apply globally on every network in our experiments, forming a "recipe" for DNN optimization, and show how guided application of the rest of them can yield even higher performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Coarse-Grained Transformations</head><p>The first step in transforming the SDFG is leveraging the semantics of ONNX operators. We can further categorize this into transformations that clean up the graph from frameworkand ONNX-related clutter, and transformations that find subgraph combinations to improve the performance or numerical stability of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cleanup</head><p>We provide a general ONNX transformation called ConstantFolding, which evaluates nodes whose inputs are not part of the weights or model inputs, and replaces the subsequent paths with the evaluated value. Following expansion to native SDFG, another transformation (InputToConstant) complements this by inlining constant inputs into their respective tasklets. This results in a chain of optimizations, for example, a 3.0 exponent in a Pow operator is first considered as a constant array; after cleanup, the code generator will subsequently replace the operation with much faster multiplications. Overall, this eliminates many unnecessary copy and computational operations (see Appendix A), and enables more complex transformations, such as algebraic fusion.</p><p>Algebraic fusion Due to the progressive lowering approach taken in DaCeML, many linear algebra operations (e.g., matrix-vector multiplication, transpose, batched tensor contraction) are lowered into Einsums. We provide a set of transformations, such as horizontal and vertical fusions, which can fuse together a transposition into a single Einsum (e.g., ij-&gt;ji and ik,jk-&gt;ij to ki,jk-&gt;ij, see Appendix C.1 for visualization). This information potentially feeds into combinatorial searches of optimal data layouts -the shape of a weight can be modified through DaCeML if the Einsum does not generate an optimized BLAS call due to layouts (for which we provide a check). The ability to modify the shape of intermediates and weights is necessary to maximize performance in today's DNN architectures <ref type="bibr">(Ivanov et al., 2021)</ref> and, to the best of our knowledge, unique to DaCeML.</p><p>Lifting and omitting operations Other transformations relate to removing nodes that do not perform computations through symbolic size analysis, such as removing an Adaptive Pooling operator if the input/output sizes are the same, or fusing together padding and convolution operations. Known subgraphs generated by frameworks can be also be "lifted" to new nodes (e.g., with custom implementations or backward expansions), as is the case for Layer Normalization <ref type="bibr" target="#b1">(Ba et al., 2016)</ref> in Figure <ref type="figure" target="#fig_2">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Local Data Movement Reduction</head><p>While powerful, domain-specific transformations on the ONNX representation do not suffice, as they only capture operator-centric behavior and will not work with unseen operators or unexpected combinations thereof. We thus focus on transformations on the native SDFG (after ONNX library node expansion) that apply directly to ML workloads.</p><p>DaCe contains a library of standard transformations that use the structure of the graph to manipulate data movement. Of particular importance are MapTiling and LocalStorage, where the former partitions the workload of a map into tiles by introducing another nested map, and the latter adds an access node between two such maps in order to create storage only accessible by the current tile (e.g., placing weights in shared memory in Section 6.3). For DaCeML, we develop additional transformations to handle data movement bottlenecks in deep learning workloads.</p><p>Transformations Given an arbitrary subgraph, the SubgraphFusion transformation tries to find a way to combine multiple map scopes into one scope. This reduces write/read roundtrips to global memory and, on GPUs, kernel launch overhead. In elementwise operations where iteration spaces are equal, the operation is trivial. However, in many cases the spaces are permuted (when different data layouts are involved) or do not share the entire space (e.g., in the Softmax operator, see Appendix B.2). For this reason, we extend subgraph fusion to find permutations, offsets (e.g., start:end vs. 0:end-start), parallel regions in reduction, and the greatest common subset of map iteration domains to fuse over. The transformation extracts them out first and then fuses the requested outer maps, greatly extending the realm of fusion possibilities.</p><p>Fusion space exploration To automate the process of optimizing data movement in graphs, we develop automatic optimization heuristics that enumerate the space of fusible subgraphs. Currently, DaCeML supports greedy enumeration with pruning (based on path constraints for fusion), and ranking according to scoring functions. For evaluation, we rank by largest neighboring regions to minimize data movement and GPU kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Backpropagation and Data Movement</head><p>When optimizing the forward and backward passes at the same time, data-centric optimizations can control aspects of auto-differentiation. As AD requires intermediate value forwarding, the memory footprint can become infeasible and movement too demanding <ref type="bibr" target="#b8">(Chen et al., 2016;</ref><ref type="bibr" target="#b24">Jain et al., 2020)</ref>. However, fusing maps on the forward pass means omitting intermediate values, and replicating them means recomputation on the backward pass. This process can create a tunable "knob" to optimize backpropagation.</p><p>For this purpose, we develop two transformations: TaskletFusion, which fuses two computations into one symbolic tasklet; and MapReplication, a transformation that detects access nodes being read more than once, and replicates the immediate map leading to it. Example uses of the two on the Mish activation <ref type="bibr" target="#b37">(Misra, 2020)</ref> and statistical normalization are shown in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Global Data Layout Optimizations</head><p>After tuning local data movement, one can look beyond the traditional "peephole optimizations" and schedule data assignment and movement on the entire graph with datacentric transformations. Briefly, such transformations do not have a pattern to match, but view the entire graph and generate multiple options. One such optimization is TransientReuse, which detects arrays that have the same volume but are used in non-overlapping segments of time (using DAG level analysis). References to these arrays are replaced by a reference to a single memory region, and unused arrays are removed. Another potential example is finding an optimal set of data layouts for weights and intermediate values. Since DaCeML generates native SDFGs, it can generate optimized code for each layout, constrain the search space by ensuring layouts match, and prune it based on domain-specific constraints, e.g., by only considering BLAS-optimized layouts for Einsums.</p><p>Allocation and deallocation of memory can create significant overheads when happening within critical code. To avoid them, the SDFG provides fine-grained control over their lifetimes. For example, when storage is annotated as Persistent, it will outlive multiple SDFG invocations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Hardware Specialization</head><p>Lastly, we need to consider the underlying system we are compiling for. This may mean the accelerator architecture(s), or whether we are running on one or more nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPU specialization DaCe transformations such as</head><p>GPUTransformSDFG and FPGATransformSDFG can offload code to different platforms by introducing copies and kernel code <ref type="bibr" target="#b4">(Ben-Nun et al., 2019)</ref>. In DaCeML, we extend the transformation capabilities to partition workload efficiently on GPUs. WarpTiling is a variant of MapTiling that takes a GPU kernel map and divides its work across a warp. It detects reductions inside the map and inserts efficient warp-collective reductions as necessary. Vectorization allows using target-specific vector instructions. We extend the system to support reducedprecision vector types, provide fast implementations of vector-to-scalar reductions, and "fill in gaps" in math library functions by calling sequences of scalar operations.</p><p>Distributed computing DaCeML supports dataparallelism for distributed training. DistDataParallel adds a distributed allreduce library node after each weight gradient access node in the backward-pass SDFG. Since the code generator traverses SDFGs in topological order, communication is performed as soon as the data are ready, promoting pipelining. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EVALUATION</head><p>Experimental setup We run each experiment at least 100 times and report the median value with a 95% nonparametric confidence interval. For measurement, we use a server with an Intel Xeon Gold 6140 CPU (2.30GHz), 768 GiB RAM, and an NVIDIA Tesla V100 GPU (16 GiB RAM). We use Python 3.8.8, DaCe 0.10.8, CUDA 11.4, CUDNN 8.2.0.54. PyTorch 1.8.1, ONNXRuntime 1.7.0, TensorFlow 2.5.0, TensorRT 8.0.1.6, TVM 0.8.0dev0 (commit dbfbebe), and JAX 0.2.13. The torch-based frameworks were all run from the same model source code, unmodified except for the addition of a decorator for torch.jit and DaCeML.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Neural Network Operators</head><p>We demonstrate DaCeML's ability to optimize single operators, and then examine how these perform when used as part of a larger model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer Normalization</head><p>We first examine layer normalization <ref type="bibr" target="#b1">(Ba et al., 2016)</ref>, a primitive widely used in transformer models; see Figure <ref type="figure">5</ref> (left). The generated code outperforms all frameworks on the forward+backward pass, where JAX closely matches in backpropagation but misses several fusion opportunities. On the forward part, PyTorch is slightly faster due to DaCeML storing more information for backpropagation, which ends up being faster overall. Without this storage, DaCeML takes 49 µs (Figure <ref type="figure" target="#fig_2">4</ref>). DaCeML's native SDFG is also fusible with neighboring operators, which delivers improved overall performance when used in larger models. In Appendix B.2, we show that the same transformations, which partition the work across GPU warps, apply to other classes of statistical normalization, e.g., softmax.</p><p>Mish activation YOLOv4 <ref type="bibr" target="#b5">(Bochkovskiy et al., 2020</ref>) is a widely-used network for object detection. Among other features, it relies on the Mish (Misra, 2020) activation, mish(x) = x tanh(softplus(x)), which is not natively supported by PyTorch. We evaluate Mish in isolation, and overall inference in YOLOv4 with Mish optimized.</p><p>Figure <ref type="figure">5</ref> (right) shows our Mish results. For forward evaluation, PyTorch produces three kernels: tanh, softplus, and multiplication. torch.jit and JAX are able to fuse two out of the three operations. DaCeML, however, is able to fuse the entire activation function, resulting in 3.43× improvements over PyTorch and 3× over torch.jit. In backpropagation, similar results hold, and we achieve 2.67× improvements over PyTorch and 1.06× over TF+XLA.</p><p>YOLOv4 Figure <ref type="figure" target="#fig_3">6</ref> lists inference results for YOLOv4 with the automated recipe ( § 5.6), which optimizes Mish in the context of the full network. The figure shows reduction in kernel counts, as well as 1.22-1.3× speedup over training frameworks. This nearly reaches the optimization levels of inference-only frameworks, such as TensorRT and TVM. Upon deeper inspection, the two frameworks make use of inference-specific optimizations, such as custom implementations for convolutions and pre-transforming weights. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Automatic Optimization Recipe</head><p>While DaCeML's major strength lies in its manual tuning capabilities, the automated transformation recipe from Section 5.6 already yields performance improvements over state-of-the-art frameworks. We demonstrate networks from different domains, with varying data movement patterns, as well as utilizing different hardware units (e.g., tensor cores).</p><p>We run the following models with the recipe alone, and list the results in  workflow, where the results are summarized in Table <ref type="table" target="#tab_4">1</ref> (top).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Guided Optimization Case Study: EfficientNet</head><p>In this case study, we consider EfficientNet-B0 <ref type="bibr" target="#b53">(Tan &amp; Le, 2019)</ref>. Since EfficientNets use repeated blocks, we can optimize one such block and reuse the techniques for the rest of the network. We focus on the first MBConv block, where performance results are reported in Figure <ref type="figure" target="#fig_4">7</ref>, as well as the progressive improvements gained by various optimizations, using the automatic recipe as a base that was further improved. For this case study, we perform the guided optimizations using the Visual Studio Code plugin (see Figure <ref type="figure" target="#fig_5">8</ref>), and report the number of clicks performed in the UI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Map fission (6 clicks)</head><p>We use two fission transformations to split a map in the backward pass. This allows us to avoid atomic operations by swapping iteration order and accumulating into thread-local memory. Full fusion and vectorization (2 clicks) The map fission performed enables further fusion in the backward pass. We also tune the backpropagation to recompute intermediate values rather than loading and storing them. This is done by applying TaskletFusion before the AD engine runs (in 27 lines of code). We fully fuse, flatten and vectorize the maps where possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Guided Optimization Case Study: BERT</head><p>Our second case study is the widely-used BERT <ref type="bibr" target="#b13">(Devlin et al., 2019)</ref> transformer. We optimize the BERT LARGE architecture with a batch size of 8 and sequence length 512. We run in mixed precision: here the advantages of the datacentric optimization are more pronounced as data movement becomes important. For example, we already see in Figure <ref type="figure" target="#fig_6">9</ref> that TF+XLA exhibits slower performance due to suboptimal data layouts that yield slower tensor contractions.</p><p>All transformations are performed using the Python API starting from the automatic recipe; we report lines of code.</p><p>Reshape-relayout Fusion (25 lines) After applying the algebraic fusion heuristic and forced BLAS operation generation, we observed extra relayout (transposition) calls with data reshapes. We write a transformation that detects this pattern and fuses the output write into the prior (or subse- quent) computation, if elementwise. This applies six times (thrice forward, thrice backward) in the graph.</p><p>Softmax Fusion (39 lines) Instead of calling a preoptimized library for softmax, we decide to "break out of the library jail" and expand it to the native SDFG, followed by applying the recipe in Appendix B.2.1. This results in fusing scaling into softmax, at a 27 µs overhead instead of 223 µs in the forward pass, and 1.1 ms gain in total.</p><p>Layer-normalization Fusion (47 lines) Here we use the lifted layer normalization scheme to nest the preceding linear layer bias into normalization. We extend and adapt the lifting transformation to include the prior/subsequent operations. For the backpropagation, since bias is reduced into 1,024 elements, we use DaCe's warp-based reduction schedule and combine it with the layer normalization weight gradient kernel, which has the same iteration space.</p><p>In both case studies, the resulting code outperforms all compiler infrastructures. This demonstrates the strength of guided data-centric optimization -inspecting complex DNN models from a bird's eye view for data movement bottlenecks, and mitigating them via transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We explore the concept of data-centric optimization for deep learning with DaCeML. The framework enables generalpurpose data layout and movement transformations to be applied on arbitrary networks written in PyTorch, matching and outperforming state-of-the-art compilers. The two key insights of the data-centric view are focusing on data movement minimization based on memory access patterns rather than operator types, and allowing practitioners to further tune global and local movement. The former can perform a superset of the optimizations applied by DNN compilers; and the latter can turn massive engineering effort into a click of a button in the analysis and transformation UI. Either automatic or human-in-the-loop, DaCeML helps practitioners speed up training without sacrificing productivity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A SYMBOLIC GRAPH ANALYSIS</head><p>In Figure <ref type="figure" target="#fig_7">10</ref>, we can see an MBConv block of EfficientNet-B0, before and after cleanup transformations. As Figure <ref type="figure" target="#fig_7">10</ref> shows, the PyTorch ONNX exporter generates many extraneous computations, casting, and Unsqueezes that precede certain shape computations. While this is necessary to compute the block sizes, it generates many calls that can be hoisted out of the computation. Only after applying this transformation, a new transformation is exposed, PadConvFusion, which can fuse the convolution padding dimensions into the operator itself. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B DACEML TRANSFORMATION RECIPE B.1 Mish Activation</head><p>We optimize the Mish operator <ref type="bibr" target="#b37">(Misra, 2020)</ref>, a novel activation function that, among other uses, has been applied successfully in object detection <ref type="bibr" target="#b5">(Bochkovskiy et al., 2020)</ref>. Due to it being recent, it has not yet been implemented as a built-in activation in PyTorch, ONNX, or ONNXRuntime. We demonstrate how DaCeML can be used to optimize this operator.</p><p>We begin with code for the PyTorch Module, and import it into DaCeML by annotating it with the @dace_module decorator.</p><p>import torch from torch import nn from torch.nn import functional as F from daceml.pytorch import dace_module @dace_module(backward=True, auto_optimize=False) class DaCeMish(nn.Module): def __init__(self): super().__init__()</p><formula xml:id="formula_1">def forward(self, x): x = x * (torch.tanh(F.softplus(x))) return x</formula><p>The module works immediately with DaCeML for the forward and backward pass, and the unoptimized graphs (due to the auto_optimize=False flag) can be seen in ONNXRuntime_Sum <ref type="bibr">[i0=0:8, i1=0:32, i2=0:224, i3=0:224] [i0=0:8, i1=0:32, i2=0:224, i3=0:224] [i0=0:8, i1=0:32, i2=0:224, i3=0:224] [i0=0:8, i1=0:32, i2=0:224, i3=0:224] [i0=0:8, i1=0:32, i2=0:224, i3=0:224]</ref> Figure <ref type="figure" target="#fig_0">11</ref>. Unoptimized Mish activation forward-pass (left) and backward-pass (right) SDFGs.</p><p>Notice the bold outline on the intermediate arrays. This means that they are global, and are thus all retained for the backward pass. This is standard behavior for DNN compilers that cannot control both the forward and backward passes. This is an optimization opportunity for DaCeML: the AD engine is "forwarding" intermediate values to perform the differentiation, and we can avoid that by fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.1 Optimization</head><p>We can now run the full recipe, as described in Section 5.</p><p>In particular, to demonstrate the API we will show how to programmatically call only a part of that recipe to optimize the operator: The resulting graphs generated are shown in Figure <ref type="figure" target="#fig_9">12</ref>. All expressions in the backward tasklet were automatically generated by SymPy. Also note the change in map range indicating vectorization. The graphs in Section 6 are identical to those in the figure. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Statistical Normalization</head><p>Statistical normalization operations (e.g., Batch Normalization, Layer Normalization, Group Normalization) are commonplace in modern DNNs. For example, BERT uses layer normalization twice in each encoder/decoder block. Despite the fact that all normalization operations are similar, their performance varies. All the aforementioned operations perform the general computation γ</p><formula xml:id="formula_2">• x−E[x] √ Var(x)+</formula><p>+ β, but on different dimensions or subsets thereof. We find, for example, that PyTorch has two separate implementations for layer and batch normalization. The implementation of batch normalization can use cuDNN, while layer normalization uses a manually-optimized function. Thus, despite batch normalization being more expensive than layer normalization (due to saving running mean/variance statistics to memory), the former is faster than the latter. Moreover, when exporting models from PyTorch to ONNX, since layer normalization is not an ONNX-native operator, it is split at arbitrary positions and a certain order of operations (mean computation followed by variance computation from the result of the mean) is enforced.</p><p>Here we show how our transformation recipe from Section 5 operates on layer normalization, and also applies to a more general form of statistical normalization -the Softmax operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.1 Softmax</head><p>We start with a detailed account of automatically applying the recipe on the Softmax operator. First, DaCeML provides a numpy implementation of the operator itself (see Figure <ref type="figure" target="#fig_10">13</ref>). This naive implementation, which corresponds to the more numerically-stable version of softmax, lowers the ONNX library node to four separate operations: two reductions (computing the maximum value to subtract; and summarizing the contributions for the denominator), and two elementwise operations (exponentiation, division). In the current mode, three extraneous tensors sized as the input would be generated. We can thus apply the second step of our recipe and locally reduce this data movement.</p><p>Fusion of those operators is not trivial, as the dimensions of the maps and reductions differ. The greedy_fuse method in DaCe automatically expands reductions (via ReduceExpansion) and finds common dimensions to extract out of the maps in the subgraph, which in turn allows to fuse them all to one map. This stores all memory locally, within the map. The end result is shown on Figure <ref type="figure" target="#fig_2">14</ref>, on the top-left SDFG.</p><p>As the global data movement optimization step does not apply in this case, the recipe proceeds to specialize the code to the GPU. The code gradually lowers the representation (expanding the two reductions to nested SD-FGs), transforms the map to be executed on a GPU (via GPUTransformSDFG), and calls WarpTiling to partition the work in the internal maps across warps. Note that  this automatically generates warp-level reductions, and by default replicates all computations that have multiple results dependent on reductions (the latter is configurable). This is shown in the top-right SDFG in Figure <ref type="figure" target="#fig_2">14</ref>. Lastly, the SDFG is cleaned up via another pass of transformations -HoistState to move initialization up, another batch of map fusion, and vectorization (shown in the bottom graph of the figure).</p><p>The resulting performance matches the performance of stateof-the-art hand-written implementations, such as the Py-Torch built-in softmax implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.2 Layer Normalization</head><p>The same exact recipe that was applied in Softmax can be applied to the Layer Normalization operator. In Figure <ref type="figure" target="#fig_0">15</ref> we can see the naive expansion of the PyTorch/ONNX op-</p><p>The exported PyTorch implementation contains a large subgraph consisting of shape-based computation of the padding size for a Pad operator (see Figure <ref type="figure" target="#fig_7">10</ref>). Using the information from the symbolic shape inference and the constant folding transformation, we are able to eliminate this subgraph to statically know the padding shape. Following constant folding, we are able to apply a high-level Pad-Conv fusion transformation that fuses the Pad operator into Conv by using the statically known padding sizes, and updating the pads attribute on the convolution operator (transformation implementation listed in Figure <ref type="figure" target="#fig_4">17</ref>).</p><p>Following the automated transformation procedure, we perform interactive optimization using Visual Studio Code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C TRANSFORMATION EXAMPLES C.1 Algebraic Fusion through Einsums</head><p>The transformation VerticalEinsumFusion, described in Section 5.1, is shown in Figure <ref type="figure" target="#fig_11">16</ref>. Its detection pattern is a chain of two Einsum nodes, and if the dimensions match, the replacement is a single, fused Einsum node. This pattern appears in several workloads where tensors with over two dimensions are used, transformers for example. It may result from code that uses .permute() or .transpose() operators, or may be generated automatically in some cases of the dot product operator.</p><p>In the transformation, since the pattern subgraph is connected, the tensor dimensions must match, but the index letters do not. Thus, we first parse the two expressions and make the test, keeping a mapping dictionary between one and the other. The replacement simply takes the output of the first Einsum and maps the characters to the one of the second Einsum, removing the access node and the original Einsum. Currently the only possible transformations are ones with one input (i.e., no contraction), and that do not change the dimensionality of the tensor, i.e., that the number of characters before and after the -&gt; matches. However, it is trivially possible to extend the matching condition to expansions, e.g., ij-&gt;ijj. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Replacement Transformations</head><p>In Figures <ref type="figure" target="#fig_5">17-18</ref> The matching condition is extended beyond subgraph matching (lines 8-25), checking first that the subgraph matches using the superclass (lines 10-11), and then that the two values are foldable, i.e., not computed by the DNN itself (lines 14-21). Then, it checks the validity of the padding values themselves: whether the fill value is zero (in lines 22-23), and whether the padding is performed on the spatial dimensions of the sample, which is the only case supported by fast libraries (lines 24-32). The replacement part (lines 34-46) is straightforward as well: the pads values of the Conv node are modified according to the Pad node's values <ref type="bibr">(lines 35-42)</ref>. Lastly, to reconnect the graph (line 46), only the Conv node is returned, and the input (X) is reconnected to it directly, using the memlet that was connected to the Pad node's data input. Figure <ref type="figure" target="#fig_5">18</ref> lists the code for a transformation that fuses matrix multiplications with a scalar multiplication that follows. In optimized BLAS libraries, this capability is directly supported in the GEneral Matrix-Matrix (GEMM) multiplication operator. Since this pattern occurs in multi-head attention in transformers, we can simply add a general-purpose transformation that would apply there as well. The pattern in this case is a MatMul operator followed by a Div (or Mul) operator (line 4). If the scalar can be folded (line 19), the matrix multiplication node is replaced with a different Gemm ONNX operator node, which contains those scaling values. We set the value of α to the new scale (lines 23-30) and replace the node, reconnecting the inputs of the MatMul node into A,B and the outputs of the scaling to Y (lines 33-36). The resulting code is short, and more such use cases can be added with ease. 1 class MatmulScal(ReplacementTransformation):   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. DaCeML system overview.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Elementwise logarithm SDFG (left) and symbolic auto-differentiated version (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Coarse-grained transformation example ( § 5.1): lifting Layer Normalization from PyTorch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Runtime results for inference with YOLOv4. Brackets [•] indicate number of kernels launched. Due to profiler issues, we could not measure kernel counts for TensorRT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Runtime results (left) and transformation breakdown (right) of the EfficientNet-B0 guided optimization case study.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Guided EfficientNet optimization in Visual Studio Code.</figDesc><graphic url="image-2.png" coords="10,65.90,67.06,210.59,177.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Runtime results (left) and transformation breakdown (right) of the BERTLARGE guided optimization case study.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. EfficientNet-B0 MBConv block before (left) and after (right) constant folding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Mish SDFGs after optimization (forward: left, backward: right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. Softmax implementation in DaCeML.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 16 .</head><label>16</label><figDesc>Figure16. Vertical (dependent) Einsum fusion in action. On the left is the SDFG before transformation and on the right is the graph after transformation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>, we see two additional examples of patternmatching replacement transformations, along with their full source code. As mentioned in Appendix B.3, PadConvFusion takes two consecutive Pad and Conv nodes, and modifies the Conv operator to include padding. In the implementation (Figure 17), we can see that the subgraph pattern definition (line 5) is straightforward -a path graph between the two nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>Figure 17. Pad-Convolution Fusion, as used in the EfficientNet case study.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 18 .Figure 19 .</head><label>1819</label><figDesc>Figure 18. Matrix Multiplication Scaling transformation, as used in transformers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>AsDaCeML is designed to optimize training workloads, such techniques are outside the scope of this work.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">PyTorch</cell><cell cols="2">torch.jit</cell><cell>JAX</cell><cell></cell><cell cols="2">TF+XLA</cell><cell>DaCeML</cell></row><row><cell></cell><cell></cell><cell>→</cell><cell></cell><cell>→</cell><cell></cell><cell>→</cell><cell></cell><cell>→</cell><cell></cell><cell>→</cell></row><row><cell>Guided</cell><cell>EfficientNet ( ) BERT LARGE (mixed) ( )</cell><cell>2.05 2.94</cell><cell>6.90 8.18</cell><cell>2.04 2.92</cell><cell>6.94 8.20</cell><cell cols="2">2.39 7.40 3.19 8.11</cell><cell cols="2">1.54 6.37 3.80 10.76</cell><cell>1.40 2.74</cell><cell>5.97 7.62</cell></row><row><cell></cell><cell>ResNet-50 ( )</cell><cell cols="2">14.55 32.04</cell><cell cols="7">9.98 31.94 14.17 33.93 12.33 35.57 10.03 32.45</cell></row><row><cell></cell><cell cols="3">Wide ResNet-50-2 ( ) 22.50 70.94</cell><cell cols="7">22.45 70.83 40.49 98.13 32.79 99.06 20.62 67.99</cell></row><row><cell></cell><cell>MobileNet V2 ( )</cell><cell cols="2">9.98 18.45</cell><cell cols="2">6.22 15.53</cell><cell>-</cell><cell>-</cell><cell cols="2">7.42 20.29</cell><cell>4.74 14.77</cell></row><row><cell>Automatic</cell><cell>EfficientNet ( ) MLP Mixer ( ) FCN8s ( ) WaveNet ( )</cell><cell cols="4">2.05 1.63 46.85 158.42 46.82 158.40 6.90 2.04 6.94 3.65 1.36 3.66 23.21 46.39 18.67 41.49</cell><cell cols="2">2.39 7.40 1.77 4.01 ----</cell><cell cols="2">1.54 6.37 ------</cell><cell>1.57 15.00 1.48 4.25 45.97 166.30 26.16 41.07</cell></row><row><cell></cell><cell>BERT LARGE (single) ( )</cell><cell cols="2">11.05 31.76</cell><cell cols="7">11.05 31.82 10.93 29.94 11.14 38.73 11.44 32.98</cell></row><row><cell></cell><cell>BERT LARGE (mixed) ( )</cell><cell>2.94</cell><cell>8.18</cell><cell>2.92</cell><cell>8.20</cell><cell cols="2">3.19 8.11</cell><cell cols="2">3.80 10.76</cell><cell>3.34</cell><cell>9.25</cell></row><row><cell></cell><cell>DLRM ( )</cell><cell cols="4">118.07 126.55 117.38 126.83</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>117.69 126.42</cell></row></table><note>Table1. Median runtime for the forward (→) and forward + backward ( ) passes for convolutional vision ( ), non-convolutional vision ( ), audio ( ), image segmentation ( ), transformer ( ) and recommendation system ( ) models. BERTLARGE reports for a single encoder layer; EfficientNet for the MBConv 1 layer. A -indicates we could not find an implementation for the model.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell>10</cell><cell></cell><cell cols="3">EfficientNet-B0 (MBConv) Forward+Backward</cell><cell>Forward</cell><cell></cell><cell>18</cell><cell></cell><cell cols="3">EfficientNet-B0 (MBConv) Forward+Backward</cell><cell>Forward</cell></row><row><cell></cell><cell>8</cell><cell cols="5">[ 18 / 54 ] [ 15 / 51 ][ 39 / 141 ][ 26 / 93 ] [ 13 / 56 ] Batch: 8, input: 3x224x224 EfficientNet-B0 MBConv 1</cell><cell></cell><cell>14 16</cell><cell>15.00 [ 13 / 60 ]</cell><cell>[ 13 / 62 ]</cell><cell>[ 13 / 61 ]</cell><cell>[ 13 / 56 ]</cell></row><row><cell>Time (ms)</cell><cell>6 4</cell><cell>6.90</cell><cell>6.94</cell><cell>7.40</cell><cell>6.37</cell><cell>5.97</cell><cell>Time (ms)</cell><cell>12 8 10 6</cell><cell></cell><cell>7.96</cell><cell>6.92</cell><cell>5.97</cell></row><row><cell>(bottom): ResNet-50 (He et al., 2015), Wide ResNet-50-2 (Zagoruyko &amp; Komodakis, 2017),</cell><cell>0 2</cell><cell cols="2">PyTorch torch.jit 2.05 2.04</cell><cell cols="3">JAX TF+XLA DaCeML 2.39 1.54 1.40</cell><cell></cell><cell>4 2 0</cell><cell>Auto 1.57</cell><cell>Map fission 1.57</cell><cell>Fused convolution 1.48</cell><cell>&amp; vectorization Full fusion 1.40</cell></row><row><cell>MobileNet V2 (Sandler et al., 2019), EfficientNet-B0's MB-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Conv block (Tan &amp; Le, 2019), MLP Mixer (Tolstikhin et al.,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2021), Fully Convolutional Network (Long et al., 2015),</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>WaveNet (van den Oord et al., 2016), BERT (Devlin et al.,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2019) encoder block in single (32-bit) and mixed (16-bit)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>precision, and the DLRM (Naumov et al., 2019) recommen-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>dation system. An interested ML practitioner could then use</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DaCeML to further optimize performance as necessary.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>The table shows that no single framework operates best</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>across all DNNs. Additionally, with the automatic optimiza-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>tions alone, we see that DaCeML roughly matches and in</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>multiple cases outperforms the other compiler frameworks.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>This is especially interesting in variants of popular networks,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>such as Wide ResNets. DaCeML is roughly 2× slower on</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Wide ResNet-50-2 than on ResNet-50, as expected for per-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>forming twice the computations; yet other frameworks are</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>up to 2.89× slower. This potentially indicates specialization</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>for certain operators and sizes, which does not occur with</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>the data-centric transformations in DaCeML.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>We now proceed with two case studies that highlight the possibilities enabled by DaCeML's user-guided optimization</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/spcl/daceml arXiv:2110.10802v1 [cs.LG]</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="20" xml:id="foot_1"> Oct 2021   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2">https://github.com/lukemelas/ EfficientNet-PyTorch</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This project received funding from the European Research Council (ERC) under the European Union's Horizon 2020 programme (grant agreements DAPP, No. 678880; EPiGRAM-HS, No. 801039; MAELSTROM, No. 955513; and DEEP-SEA, No. 955606). T.B.N. is supported by the Swiss National Science Foundation (Ambizione Project #185778). N.D. is supported by the ETH Postdoctoral Fellowship. The authors wish to acknowledge the support from the PASC program (Platform for Advanced Scientific Computing), as well as the Swiss National Supercomputing Center (CSCS) for providing computing infrastructure.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>erator on the left-hand side, and an optimized graph on the right, which acts as the default expansion of the lifted LayerNormalization operator (from Figure <ref type="figure">4</ref>).</p><p>In the figure, the transformed version is similar to the softmax code, with one important algorithmic distinction: the formula used for variance computation is E</p><p>which can be computed in parallel, as the graph shows. This is a direct result of domain knowledge in lowering. Also important with such an expansion is mixed-precision operation: since the input/output data types are known at lowering, we can control the intermediate data types, e.g., using 32-bit accumulators when 16-bit precision is involved. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 EfficientNet MBConv Block</head><p>To optimize the building block of the EfficientNet architecture, we use the popular PyTorch implementation from the efficientnet_pytorch PyPI package 2 . To import the block, we make no code modifications, and simply wrap the module using the DaceModule class (equivalent to the decorator).  To optimize this model, we perform the same recipe as in the Mish case (map fusion, tasklet fusion and vectorization).</p><p>In particular, DaCeML fuses and vectorizes the Sigmoid and Mul operators, which form the Swish activation, since the Swish operator does not yet exist in ONNX.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Largescale machine learning on heterogeneous systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/.Softwareavailablefromtensorflow.org" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">E. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">TIRAMISU: A polyhedral compiler for dense and sparse deep learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Baghdadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Debbagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Abdous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">F</forename><surname>Zohra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Renda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Systems for ML at NeurIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Machine learning systems are stuck in a rut</title>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Hot Topics in Operating Systems</title>
				<meeting>the Workshop on Hot Topics in Operating Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Stateful dataflow multigraphs: A datacentric model for performance portability on heterogeneous architectures</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>De Fine Licht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Ziogas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hoefler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</title>
				<meeting>the International Conference for High Performance Computing, Networking, Storage and Analysis</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Optimal speed and accuracy of object detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><forename type="middle">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><surname>Yolov4</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Optimization methods for large-scale machine learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E</forename><surname>Curtis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Siam Review</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">JAX: composable transformations of Python+NumPy programs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Necula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wanderman-Milne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="http://github.com/google/jax" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Training deep nets with sublinear memory cost</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.06174</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">TVM: An end-to-end optimization stack for deep learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th USENIX Symposium on Operating Systems Design and Implementation (OSDI)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Chetlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Woolley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandermersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.0759</idno>
		<title level="m">Efficient primitives for deep learning</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Intel nGraph: An intermediate representation, compiler, and executor for deep learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cyphers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhiwandiwalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bobba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brookhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Constable</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Convey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kanawi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.08058</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">StencilFlow: Mapping large stencil programs to distributed spatial computing systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>De Fine Licht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>De Matteis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hoefler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Acorns: A framework for accelerating deep neural networks with input sparsity</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 28th International Conference on Parallel Architectures and Compilation Techniques (PACT)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Diesel: DSL for linear algebra and neural net computations on GPUs</title>
		<author>
			<persName><forename type="first">V</forename><surname>Elango</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ravishankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sandanagobalane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Grover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages</title>
				<meeting>the 2nd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Compiling machine learning programs via high-level tracing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Systems for Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Optimizing compiler for machine learning</title>
		<author>
			<persName><forename type="first">Google</forename><surname>Xla ; Halevy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Norvig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename></persName>
		</author>
		<ptr target="https://www.tensorflow.org/xla" />
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009">2021. 2009</date>
		</imprint>
	</monogr>
	<note>The unreasonable effectiveness of data</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Hooker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06489</idno>
		<title level="m">The hardware lottery</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">FeatGraph: A flexible and efficient backend for graph neural network systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</title>
				<meeting>the International Conference for High Performance Computing, Networking, Storage and Analysis</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<ptr target="https://01.org/oneDNN" />
		<title level="m">Intel. oneAPI deep neural network library</title>
				<imprint>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Open visual inference and neural network optimization toolkit</title>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<ptr target="https://01.org/openvinotoolkit" />
		<imprint>
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Data movement is all you need: A case study on optimizing transformers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dryden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hoefler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Conference on Machine Learning and Systems (MLSys)</title>
				<meeting>the Fourth Conference on Machine Learning and Systems (MLSys)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Checkmate: Breaking the memory wall with optimal tensor rematerialization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nrusimha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Learning and Systems (MLSys)</title>
				<meeting>the Third Conference on Machine Learning and Systems (MLSys)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">TASO: optimizing deep learning computation with automatic generation of graph substitutions</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Padon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Warszawski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM Symposium on Operating Systems Principles (SOSP)</title>
				<meeting>the 27th ACM Symposium on Operating Systems Principles (SOSP)</meeting>
		<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Optimizing DNN computation with relaxed graph substitutions</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Warszawski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Conference on Systems and Machine Learning (SysML)</title>
				<meeting>the 2nd Conference on Systems and Machine Learning (SysML)</meeting>
		<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">In-datacenter performance analysis of a tensor processing unit</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Borchers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th annual international symposium on computer architecture</title>
				<meeting>the 44th annual international symposium on computer architecture</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">LLVM: A compilation framework for lifelong program analysis &amp; transformation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lattner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Adve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Code Generation and Optimization</title>
				<imprint>
			<date type="published" when="2004">2004. 2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Lattner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Bondhugula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pienaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Riddle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shpeisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasilache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Zinenko</surname></persName>
		</author>
		<author>
			<persName><surname>Mlir</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.11054</idno>
		<title level="m">A compiler infrastructure for the end of moore&apos;s law</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Polyhedral optimization of tensorflow computation graphs</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lethin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth Workshop on Extreme-scale Programming Tools (ESPT)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Optimizing memory efficiency for deep convolutional neural networks on gpus</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chakradhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC&apos;16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="633" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Taming unbalanced training workloads in deep learning with partial collective operations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Girolamo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Alistarh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hoefler</surname></persName>
		</author>
		<idno type="DOI">10.1145/3332466.3374528</idno>
		<ptr target="https://doi.org/10.1145/3332466.3374528" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPoPP &apos;20</title>
				<meeting>the 25th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPoPP &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="45" to="61" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery. ISBN 9781450368186</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015-06">June 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">SymPy: symbolic computing in python</title>
		<author>
			<persName><forename type="first">A</forename><surname>Meurer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paprocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Čertík</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Kirpichev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rocklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rathnayake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bonazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Curry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Terrel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Roučka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saboo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kulal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cimrman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Scopatz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PeerJ Computer Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">e103</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title/>
		<author>
			<persName><surname>Microsoft</surname></persName>
		</author>
		<author>
			<persName><surname>Deepspeed</surname></persName>
		</author>
		<ptr target="https://www.onnxruntime.ai" />
	</analytic>
	<monogr>
		<title level="s">URL deepspeed.ai. Microsoft. ONNX Runtime</title>
		<imprint>
			<date type="published" when="2020">2020. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Visual Studio Code -Code editing</title>
		<ptr target="https://code.visualstudio.com/" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Microsoft</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A self regularized non-monotonic activation function</title>
		<author>
			<persName><forename type="first">D</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><surname>Mish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st British Machine Vision Conference (BMVC)</title>
				<meeting>the 31st British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deep learning recommendation model for personalization and recommendation systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Naumov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><forename type="middle">M</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sundaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Azzolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dzhulgakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mallevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Cherniavskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Krishnamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kondratenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Nvidia</forename><surname>Tensorrt</surname></persName>
		</author>
		<ptr target="https://developer.nvidia.com/tensorrt" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><surname>Onnx</surname></persName>
		</author>
		<author>
			<persName><surname>Onnx</surname></persName>
		</author>
		<ptr target="https://onnx.ai/" />
		<title level="m">Open neural network exchange</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Accelerating deep learning frameworks with micro-batches</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Oyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Matsuoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Cluster Computing (CLUSTER)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-M</forename><surname>Munguia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rothchild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Texier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10350</idno>
		<title level="m">Carbon emissions and large neural network training</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Pytorch</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><surname>Torchscript</surname></persName>
		</author>
		<ptr target="https://pytorch.org/docs/stable/jit.html" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Large-scale deep unsupervised learning using graphics processors</title>
		<author>
			<persName><forename type="first">R</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international conference on machine learning</title>
				<meeting>the 26th annual international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">L</forename><surname>Roeder</surname></persName>
		</author>
		<author>
			<persName><surname>Netron</surname></persName>
		</author>
		<ptr target="https://netron.app/" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Rotem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Abdulrasool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Catron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dzhabarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hegeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Levenstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00907</idno>
		<title level="m">Graph lowering compiler techniques for neural networks</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Exploiting predictability to optimize deep learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sivathanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Singapuram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><surname>Astra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
				<meeting>the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Value function based performance optimization of deep learning workloads</title>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Leather</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Energy and policy considerations for deep learning in NLP</title>
		<author>
			<persName><forename type="first">E</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision (ICCV)</title>
				<meeting>the IEEE international conference on computer vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">EfficientNet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<title level="m">Mlp-mixer: An all-mlp architecture for vision</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Latte: A language, compiler, and runtime for elegant and efficient deep neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Totoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Markley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shpeisman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)</title>
				<meeting>the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><surname>Wavenet</surname></persName>
		</author>
		<title level="m">A generative model for raw audio</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Vasilache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Zinenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Theodoridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>De-Vito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Moses</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Verdoolaege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04730</idno>
		<title level="m">Tensor comprehensions: Framework-agnostic high-performance machine learning abstractions</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">SWIRL: High-performance many-core CPU code generation for deep neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Venkat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rusira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Truong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of High Performance Computing Applications</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">DLVM: A modern compiler infrastructure for deep learning systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Adve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Conference on Learning Representations -Workshop Track (ICLR)</title>
				<meeting>the Sixth International Conference on Learning Representations -Workshop Track (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Equality saturation for tensor graph superoptimization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Phothilimtha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Willsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pienaar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Wide residual networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Ansor: Generating high-performance tensor programs for deep learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Haj-Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th USENIX Symposium on Operating Systems Design and Implementation</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Optimizing the data movement in quantum transport simulations via data-centric parallel programming</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Ziogas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">I</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Luisier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hoefler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</title>
				<meeting>the International Conference for High Performance Computing, Networking, Storage and Analysis</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
