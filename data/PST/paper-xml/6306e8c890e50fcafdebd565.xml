<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ADMoE: Anomaly Detection with Mixture-of-Experts from Noisy Labels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-08-24">24 Aug 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yue</forename><surname>Zhao</surname></persName>
							<email>zhaoy@cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guoqing</forename><surname>Zheng</surname></persName>
							<email>guoqing.zheng@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Subhabrata</forename><surname>Mukherjee</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Robert</forename><surname>Mccann</surname></persName>
							<email>robert.mccann@microsoft.com</email>
							<affiliation key="aff2">
								<address>
									<settlement>Microsoft</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ahmed</forename><surname>Awadallah</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ADMoE: Anomaly Detection with Mixture-of-Experts from Noisy Labels</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-08-24">24 Aug 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2208.11290v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing works on anomaly detection (AD) rely on clean labels from human annotators that are expensive to acquire in practice. In this work, we propose a method to leverage weak/noisy labels (e.g., risk scores generated by machine rules for detecting malware) that are cheaper to obtain for anomaly detection. Specifically, we propose ADMoE, the first framework for anomaly detection algorithms to learn from noisy labels. In a nutshell, ADMoE leverages Mixtureof-experts (MoE) architecture to encourage specialized and scalable learning from multiple noisy sources. It captures the similarities among noisy labels by sharing most model parameters, while encouraging specialization by building "expert" sub-networks. To further juice out the signals from noisy labels, ADMoE uses them as input features to facilitate expert learning. Extensive results on eight datasets (including a proprietary enterprise security dataset) demonstrate the effectiveness of ADMoE, where it brings up to 34% performance improvement over not using it. Also, it outperforms a total of 13 leading baselines with equivalent network parameters and FLOPS. Notably, ADMoE is model-agnostic to enable any neural network-based detection methods to handle noisy labels, where we showcase its results on both multiplelayer perceptron (MLP) and leading AD method DeepSAD.</p><p>* The project is primarily done at Microsoft Research. 1 We use the terms noisy and weak interchangeably.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Anomaly detection (AD), also known as outlier detection, is a crucial learning task with many real-world applications, including malware detection <ref type="bibr" target="#b18">(Nguyen et al. 2019)</ref>, anti-money laundering <ref type="bibr" target="#b17">(Lee et al. 2020)</ref>, rare-disease detection <ref type="bibr" target="#b17">(Li et al. 2018</ref>) and so on. Although there are numerous detection algorithms <ref type="bibr" target="#b0">(Aggarwal 2013;</ref><ref type="bibr" target="#b19">Pang et al. 2021;</ref><ref type="bibr" target="#b38">Zhao, Rossi, and Akoglu 2021;</ref><ref type="bibr" target="#b13">Han et al. 2022)</ref>, existing AD methods assume the availability of (partial) labels that are clean (i.e. without noise), and cannot learn from weak/noisy labels 1 .</p><p>Simply treating noisy labels as (pseudo) clean labels leads to biased and degraded models <ref type="bibr" target="#b29">(Song et al. 2022)</ref>. Over the years, researchers have developed algorithms for classification and regression tasks to learn from noisy sources (Rodrigues and Pereira 2018; <ref type="bibr" target="#b10">Guan et al. 2018;</ref><ref type="bibr" target="#b32">Wei et al. 2022</ref>), which has shown great success. However, these methods are not tailored for AD with extreme data imbalance, and existing AD methods cannot learn from (multiple) noisy sources. Why is it important to leverage noisy labels in AD applications? Taking malware detection as an example, it is impossible to get a large number of clean labels due to the data sensitivity and the cost of annotation. However, often there exists a large number of weak/noisy historical security rules designed for detecting malware from different perspectives, e.g., unauthorized network access and suspicious file movement, which have not been used in AD yet. Though not as perfect as human annotations, they are valuable as they encode prior knowledge from past detection experiences. Also, although each noisy source may be insufficient for difficult AD tasks, learning them jointly may build competitive models as they tend to complement each other.</p><p>In this work, we propose ADMoE, (to our knowledge) the first weakly-supervised approach for enabling anomaly detection algorithms to learn from multiple sets of noisy labels. In a nutshell, ADMoE enhances existing neuralnetwork-based AD algorithms by Mixture-of-experts (MoE) network(s) <ref type="bibr" target="#b14">(Jacobs et al. 1991;</ref><ref type="bibr" target="#b28">Shazeer et al. 2017)</ref>, which has a learnable gating function to activate different subnetworks (experts) based on the incoming samples and their noisy labels. In this way, the proposed ADMoE can jointly learn from multiple sets of noisy labels with the majority of parameters shared, while providing specialization and scalability via experts. Unlike existing noisy label learning approaches, ADMoE does not require explicit mapping from noisy labels to network parameters, providing better scalability and flexibility. To encourage ADMoE to develop specialization based on the noisy sources, we use noisy labels as (part of the) input features with learnable embeddings to make the gating function aware of them.</p><p>Key Results. Fig. <ref type="figure" target="#fig_0">1</ref> shows that a multiple layer perception (MLP) <ref type="bibr" target="#b24">(Rosenblatt 1958)</ref> enhanced by ADMoE can largely outperform both (1a) leading AD algorithms as well as (1b) noisy-label learning methods for classification. Note AD-MoE is not strictly another detection algorithm, but a general framework to empower any neural-based AD methods to leverage multiple sets of weak labels. ?4 shows extensive results on more datasets, and the improvement in enhancing more complex DeepSAD <ref type="bibr" target="#b26">(Ruff et al. 2019)</ref> with ADMoE.</p><p>In summary, the key contributions of this work include: ? Problem formulation, baselines, and datasets. We formally define the crucial problem of using multiple sets of noisy labels for AD (MNLAD), and release the first batch of baselines and datasets for future research 2 . ? The first AD framework for learning from multiple noisy sources. The proposed ADMoE is a novel method with Mixture-of-experts (MoE) architecture to achieve specialized and scalable learning for MNLAD. ? Model-agnostic design. ADMoE enhances any neuralnetwork-based AD methods, and we show its effectiveness on MLP and state-of-the-art (SOTA) DeepSAD. ? Effectiveness and real-world deployment. We demonstrate ADMoE's SOTA performance on seven benchmark datasets and a proprietary enterprise security application, in comparison with two groups of leading baselines (13 in total). It brings on average 14% and up to 34% improvement over not using it, with the equivalent number of learnable parameters and FLOPs as baselines.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Weakly-supervised Anomaly Detection</head><p>There exists some literature on weakly-supervised AD, and most of them fall under the "incomplete supervision" category. These semi-supervised methods assume access to a small set of clean labels and it is unclear how to extend them for multi-set noisy labels. Representative work includes XG-BOD <ref type="bibr" target="#b35">(Zhao and Hryniewicki 2018)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Learning from Single Set of Noisy Labels</head><p>There has been rich literature on learning from a single set of noisy labels, including learning a label corruption/transition matrix <ref type="bibr" target="#b21">(Patrini et al. 2017)</ref>, correcting labels via metalearning <ref type="bibr" target="#b39">(Zheng, Awadallah, and Dumais 2021)</ref>, and building robust training mechanisms like co-teaching <ref type="bibr" target="#b12">(Han et al. 2018)</ref>, co-teaching+ <ref type="bibr" target="#b34">(Yu et al. 2019)</ref>, and JoCoR <ref type="bibr" target="#b31">(Wei et al. 2020)</ref>. More details can be found in a recent survey (Han   2 Anonymous repo: https://tinyurl.com/admoe22 et al. 2020). These algorithms are primarily for a single set of noisy labels, and are not designed for AD tasks. MNLAD falls under weakly supervised ML <ref type="bibr" target="#b41">(Zhou 2018)</ref>, where it deals with multiple sets of inaccurate/noisy labels. Naturally, one can aggregate noisy labels to generate a "corrected" label set <ref type="bibr" target="#b39">(Zheng, Awadallah, and Dumais 2021)</ref>, while it may be challenging for AD with extreme data imbalance. Other than label correction, one may take ensembling to train multiple independent AD models for combination, e.g., one model per set of noisy labels, which however faces scalability issues while dealing with many sets of labels. What is worse, independently trained models fail to explore the interaction among noisy labels. Differently, endto-end noisy label learning methods, including Crowd Layer <ref type="bibr" target="#b23">(Rodrigues and Pereira 2018)</ref>, DoctorNet <ref type="bibr" target="#b10">(Guan et al. 2018)</ref>, and UnionNet <ref type="bibr" target="#b32">(Wei et al. 2022)</ref>, can directly learn from multiple sets of noisy labels and map each set of noisy labels to part of the network (e.g., transition matrix), encouraging the model to learn knowledge from all noisy labels collectively. Although they yield great performance in crowd-sourcing scenarios with a small number of annotators, they do not scale in MNLAD with many sets of "cheap" noisy labels due to this explicit one-to-one mapping. Also, each set of noisy AD labels may be only good at certain anomalies as a biased annotator. Consequently, explicit one-to-one mapping (e.g., transition matrix) from a single set of labels to a network in existing classification works is not ideal for MNLAD. AD-MoE lifts this constraint to allow many-to-many mapping, improving model scalability and robustness for MNLAD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Learning from Multiple Noisy Sources</head><p>We summarize the methods for learning from multiple sources of noisy labels in Table <ref type="table" target="#tab_1">1 categorized as:</ref> (1) Sin-gleNoisy trains an AD model using only one set of weak labels, which sets the lower bound of all baselines. (2) La-belVote trains an AD model based on the consensus of weak labels via majority vote. (3) HyperEnsemble <ref type="bibr" target="#b33">(Wenzel et al. 2020</ref>) trains an individual model for each set of noisy labels (i.e., k models for k sets of labels), and combine their anomaly scores by averaging (i.e., HE A) and maximizing (i.e., HE M). ( <ref type="formula">4</ref>) CrowdLayer <ref type="bibr" target="#b23">(Rodrigues and Pereira 2018)</ref> tries to reconstruct the input weak labels during training. (5) UnionNet <ref type="bibr" target="#b32">(Wei et al. 2022</ref>) learns a transition matrix for all weak labels together. Note that they are primarily for classification and not tailored for anomaly detection. Nonetheless, we adapt the methods in Table <ref type="table" target="#tab_1">1</ref> as baselines. In ?4.3, we show ADMoE outperforms all these methods. In ?3.1, we formally present the problem of AD with multiple sets of noisy labels, followed by the discussion on why multiple noisy sources help AD in ?3.2. Motivated by above, we describe the proposed ADMoE framework in ?3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Statement</head><p>We consider the problem of anomaly detection with multiple sets of weak/noisy labels. We refer to this problem as MN-LAD, an acronym for using multiple sets of noisy labels for anomaly detection. We present the problem definition here. Problem 1 (MNLAD) Given an anomaly detection task with input feature X ? R n?d (e.g., n samples and d features) and t sets of noisy/weak labels Y w = {y w,1 , . . . , y w,t } (each in R n ), build a detection model M to leverage all information to achieve the best performance.</p><p>Existing AD methods <ref type="bibr" target="#b0">(Aggarwal 2013;</ref><ref type="bibr" target="#b19">Pang et al. 2021;</ref><ref type="bibr" target="#b13">Han et al. 2022</ref>) can (at best) treat one set of noisy labels from Y w as (pseudo) clean labels to train a model. None of them leverages multiple sets of noisy labels Y w collectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Why and How Do Multiple Sets of Weak</head><p>Labels Help in Anomaly Detection?</p><p>Benefits of joint learning in MNLAD. AD benefits from model combination and ensemble learning from diverse base models <ref type="bibr" target="#b1">(Aggarwal and Sathe 2017;</ref><ref type="bibr">Zhao et al. 2019;</ref><ref type="bibr" target="#b8">Ding, Zhao, and Akoglu 2022)</ref>, and the improvement is expected when base models make complementary errors <ref type="bibr" target="#b42">(Zimek, Campello, and Sander 2014;</ref><ref type="bibr" target="#b1">Aggarwal and Sathe 2017)</ref>. Multiple sets of noisy labels are natural sources for ensembling with built-in diversity, as they reflect distinct detection aspects and historical knowledge. Appx. A Fig. <ref type="figure" target="#fig_0">A1</ref> shows that averaging the outputs from multiple AD models (each trained on one set of noisy labels) leads to better results than training each model independently; this observation holds true for both deep (neural) AD models (e.g., PreNet in Appx. Fig. <ref type="figure" target="#fig_0">A1a</ref>) and shallow models (e.g., XG-BOD in Appx. Fig. <ref type="figure" target="#fig_0">A1b</ref>). This example justifies the benefit of learning from multiple sets of noisy labels; even simple averaging in MNLAD can already "touch" the performance upper bound (i.e., training a model using all clean labels).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">ADMoE: Specialized and Scalable Anomaly Detection with Multiple Sets of Noisy Labels</head><p>Motivation. After reviewing gaps and opportunities in existing works, we argue the ideal design for MNLAD should fulfill the following requirements: (i) encourage specialization from different noisy sources but also explore their similarity (ii) scalability to handle an increasing number of noisy sources and (iii) generality to apply to various AD methods. Overview of ADMoE. In this work, (for the first time) we adapt Mixture-of-experts (MoE) architecture/layer <ref type="bibr" target="#b14">(Jacobs et al. 1991;</ref><ref type="bibr" target="#b28">Shazeer et al. 2017</ref>) (see preliminary in ?3.3.1) to AD algorithm design for MNLAD. Specifically, we propose model-agnostic ADMoE<ref type="foot" target="#foot_0">3</ref> to enhance any neural networkbased AD method via: (i) mixture-of-experts (MoE) layers to do specialized and scalable learning from noisy labels and (ii) noisy-label aware expert activation by using noisy labels as (part of the) input with learnable embedding to facilitate specialization. Refer to Fig. <ref type="figure">2c</ref> for an illustration of applying ADMoE to a simple MLP, where we add an ADMoE layer between the dense layers and output layer for MNLAD, and use noisy labels directly as input with learnable embedding to help ADMoE to better specialize. Other neural-networkbased AD methods can follow the same procedure to be enhanced by ADMoE (i.e., inserting ADMoE layers before the output layer and using noisy labels as input). In the following subsections, we give a short background of MoE and then present the design of ADMoE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Preliminary on</head><p>Mixture-of-experts (MoE) Architecture. The original MoE <ref type="bibr" target="#b14">(Jacobs et al. 1991</ref>) is designed as a dynamic learning paradigm to allow different parts (i.e., experts) of a network to specialize for different samples.</p><p>More recent (sparsely-gated) MoE <ref type="bibr" target="#b28">(Shazeer et al. 2017</ref>) has been shown to improve model scalability for natural language processing (NLP) tasks, where models can have billions of parameters <ref type="bibr" target="#b9">(Du et al. 2022)</ref>. The key difference between the original MoE and the sparse MoE is the latter builds more experts (sub-networks) for a large model but only activates a few of them for scalability and efficiency.</p><p>Basically, MoE splits specific layer(s) of a (large) neural network into m small "experts" (i.e., sub-networks). It uses top-k gating to activate k experts (k &lt; m or k m for sparse MoE) for each input sample computation as opposed to using the entire network as standard dense models. More specifically, MoE uses a differentiable gating function G(?) to calculate the activation weights of each expert. It then aggregates the weighted outputs of the top-k experts with the highest activation weights as the MoE layer output. For example, the expert weights ? j ? R m of the j-th sample is a function of input data by the gating G(?), i.e., ? j = G(X j ).</p><p>Till now, MoE has been widely used in multi-task learning <ref type="bibr" target="#b40">(Zheng et al. 2019)</ref>, video captioning <ref type="bibr" target="#b30">(Wang et al. 2019)</ref>, and multilingual neural machine translation (NMT) <ref type="bibr" target="#b7">(Dai et al. 2022)</ref> tasks. Taking NMT as an example, prior work <ref type="bibr" target="#b7">(Dai et al. 2022)</ref> shows that MoE can help learn a model from diverse sources (e.g., different languages), where they share the most common parameters but also specialize for individual source via "experts" (sub-networks). We combine the strengths of original and sparse MoE for our ADMoE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Capitalizing</head><p>Mixture-of-experts (MoE) Architecture for MNLAD. It is easy to see the connection between MoE's applications (e.g., multilingual machine translation) and MNLAD-in both cases MoE can help learn from diverse sources (e.g., multiple sets of noisy labels in MNLAD) to capture the similarity via parameter sharing while encouraging specialization via expert learning.</p><p>By recognizing this connection, we (for the first time), introduce MoE architecture for weakly supervised AD with noisy labels (i.e., MNLAD), called ADMoE. Similar to other works that use MoE, proposed ADMoE keeps (does not alter) an (AD) algorithm's original layers before the output layer (e.g., dense layers in an MLP) to explore the agreement among noisy sources via shared parameters, while in- serting an MoE layer before the output layer to learn from each noisy source with specialization (via the experts/subnetworks). In an ideal world, each expert is good at handling samples from different noisy sources and updated only with more accurate sets of noisy labels. As the toy example shown in Fig. <ref type="figure">2c</ref> to apply ADMoE on an MLP for AD, we insert an ADMoE layer between the dense layers and the output layer: where the ADMoE layer contains four experts, and the gating activates only the top two for each input example. For the j-th sample, the MoE layer's output O j is shown in Eq. ( <ref type="formula">1</ref>) as a weighted sum of all activated experts' outputs, where E i (?) denotes the i-th expert network, and h j is the output from the dense layer (as the input to ADMoE). ? j i is the weight of the i-th expert assigned by the gating function G(?), and we describe its calculation in Eq. ( <ref type="formula" target="#formula_1">2</ref>). Although Eq. ( <ref type="formula">1</ref>) enumerates all m experts for aggregation, only the top-k experts with non-negative weights ? j i &gt; 0 are used.</p><formula xml:id="formula_0">O j = m i=1 ? j i E i (h j ) (1)</formula><p>Improving ADMoE with Noisy-Label Aware Expert Activation. The original MoE calculates the activation weights with only the raw input feature X (see ?3.3.1), which can be improved in MNLAD with the presence of noisy labels. To such end, we explicitly make the gating function G(?) aware of noisy labels Y w while calculating the weights for (activating) experts. Intuitively, t sets of noisy labels can be expressed as t-element binary vectors (0 means normalcy and 1 means abnormality). However, it is hard for neural networks to directly learn binary inputs <ref type="bibr" target="#b4">(Buckman et al. 2018)</ref>. Thus, we propose to learn a R t?e continuous embedding Emb(?) for noisy labels (e is embedding dimension), and use both the raw input features X in d dims and the average of the embedding in e dims as the input of the gating. As such, the expert weights for the j-th sample ? j can be calculated with Eq. ( <ref type="formula" target="#formula_1">2</ref>), and plugged back into Eq. (1) for ADMoE output.</p><formula xml:id="formula_1">? j = G X j , Emb(Y j w )<label>(2)</label></formula><p>Loss Function. ADMoE is strictly an enhancement framework for MNLAD other than a new detection algorithm, and thus we do not need to design a new loss function but just enrich the original loss L o of the underlying AD algorithm (e.g., cross-entropy for a simple MLP). While calculating the loss in each batch of data, we randomly sample one set of noisy labels treated as the (pseudo) clean labels or combine the loss of all t sets of noisy labels by treating them as (pseudo) clean labels. To encourage all experts to be evenly activated and not collapse on a few of them <ref type="bibr" target="#b22">(Riquelme et al. 2021)</ref>, we include an additional loss term on gating (i.e, L g ).</p><p>Putting these together, we show the loss for the j-th training sample in Eq. ( <ref type="formula">3</ref>), where the first term encourages equal activation of experts with ? as the load balancing factor, and the second part is the loss of the j-th sample, which depends on the MoE layer output O j in Eq. ( <ref type="formula">1</ref>) and the corresponding noisy labels Y j w (either one set or all in loss calculation).</p><formula xml:id="formula_2">L j = ?L g + L o O j , Y j w</formula><p>(3) Remark on the Number of ADMoE Layers and Sparsity. Similar to the usage in NLP, one may use multiple ADMoE layers in an AD model (e.g., every other layer), especially for the AD models with complex structures like transformers (Li, Liu, and Jiao 2022). In this work, we only show inserting the ADMoE layer before the output layer (see the illustration in Fig. <ref type="figure">2c</ref>) of MLP, while future work may explore more extensive use of ADMoE layers in complex AD models. As AD models are way smaller <ref type="bibr" target="#b19">(Pang et al. 2021)</ref>, we do not enforce gating sparsity as in NLP models <ref type="bibr" target="#b28">(Shazeer et al. 2017)</ref>. See ablation studies on the total number of experts and the number of activated experts in ?4.4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Advantages and Properties of ADMoE</head><p>Implicit Mapping of Noisy Labels to Experts with Better Scalability. The proposed ADMoE (Fig. <ref type="figure">2c</ref>) is more scalable than existing noisy-label learning methods for classification, e.g., CrowdLayer (Fig. <ref type="figure">2a</ref>) and UnionNet (Fig. <ref type="figure">2b</ref>). As discussed in ?2.3, these methods explicitly map each set of noisy labels to part of the network. With more sets of noisy labels, the network parameters designated for (recovering or mapping) noisy labels increase proportionately. In contrast, ADMoE does not enforce explicit one-to-one mapping from noisy labels to experts. Thus, its many-to-many mapping becomes more scalable with many noisy sources. Extension with Clean Labels. Our design also allows for easy integration of clean labels. In many AD tasks, a small set of clean labels are feasible, where ADMoE can easily use them. No update is needed for the network design, but simply treating the clean label as "another set of noisy labels" with higher weights. See ?4.4.4 for experiment results.</p><p>We design experiments to answer the following questions: 1. Benchmark Datasets. As shown in Table <ref type="table" target="#tab_2">2</ref>, we evaluate ADMoE on seven public datasets adapted from AD repositories <ref type="bibr" target="#b5">(Campos et al. 2016;</ref><ref type="bibr" target="#b13">Han et al. 2022</ref>) and a proprietary enterprise-security dataset (with t = 3 sets of noisy labels). Note that these public datasets do not have existing noisy labels for AD, so we simulate t = 4 sets of noisy labels per dataset via two methods: 1. Label Flipping <ref type="bibr" target="#b39">(Zheng, Awadallah, and Dumais 2021)</ref> generates noisy labels by uniformly swapping anomaly and normal classes at a designated noise rate. 2. Inaccurate Output uses varying percentages of ground truth labels to train t diverse classifiers, and considers their (inaccurate) predictions as noisy labels. With more ground truth labels to train a classifier, its prediction (e.g., noisy labels) will be more accurate-we control the noise levels by the availability of ground truth labels.</p><p>In this work, we use the noisy labels simulated by Inaccurate Output since that is more realistic and closer to realworld applications (e.g., noise is not random), while we also release the datasets by Label Flipping for broader usage. We provide a detailed dataset description in Appx. B.1. Two Groups of Baselines are described below: 1. Leading AD methods that can only handle a single set of labels to show ADMoE's benefit of leveraging multiple sets of noisy labels. These include SOTA AD methods:</p><p>(  <ref type="bibr" target="#b32">(Wei et al. 2022</ref>). Backbone AD Algorithms, Model Capacity, and Hyperparameters. We show the generality of ADMoE to enhance (i) simple MLP and (ii) SOTA DeepSAD <ref type="bibr" target="#b26">(Ruff et al. 2019</ref>). To ensure a fair comparison, we ensure all methods have the equivalent number of trainable parameters and FLOPs. See Appx. C.2 and code for additional settings (e.g., hyperparameters) in this study. All experiments are run on an Intel i7-9700 @3.00 GH, 64GB RAM, 8-core workstation with an NVIDIA Tesla V100 GPU. Evaluation. For methods with built-in randomness, we run four independent trials and take the average, with a fixed dataset split (70% train, 25% for test, 5% for validation). Following AD research tradition <ref type="bibr" target="#b1">(Aggarwal and Sathe 2017;</ref><ref type="bibr">Zhao et al. 2019;</ref><ref type="bibr" target="#b16">Lai et al. 2021;</ref><ref type="bibr" target="#b13">Han et al. 2022)</ref>, we report ROC-AUC as the primary metric, while also showing additional results of average precision (AP) in Appx. C.3.   </p><formula xml:id="formula_3">1RLV\ODEHOTXDOLW\KLJKHUFOHDQHUWRWKHULJKW 52&amp;$8&amp;KLJKHUWKHEHWWHU ;*%2' 3UH1HW 'HY1HW 'HHS6$' /*% 0/3 $'0R('HHS6$' $'0R(0/3 (a) agnews 1RLV\ODEHOTXDOLW\KLJKHUFOHDQHUWRWKHULJKW 52&amp;$8&amp;KLJKHUWKHEHWWHU (b) aloi 1RLV\ODEHOTXDOLW\KLJKHUFOHDQHUWRWKHULJKW 52&amp;$8&amp;KLJKHUWKHEHWWHU (c) imdb 1RLV\ODEHOTXDOLW\KLJKHUFOHDQHUWRWKHULJKW 52&amp;$8&amp;KLJKHUWKHEHWWHU (d) mnist 1RLV\ODEHOTXDOLW\KLJKHUFOHDQHUWRWKHULJKW 52&amp;$8&amp;KLJKHUWKHEHWWHU (e) spamspace 1RLV\ODEHOTXDOLW\KLJKHUFOHDQHUWRWKHULJKW 52&amp;$8&amp;KLJKHUWKHEHWWHU (f) svhn</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison Between ADMoE Methods and</head><p>Leading AD Methods (Q1)</p><p>Fig. <ref type="figure" target="#fig_0">1a</ref> and Fig. <ref type="figure" target="#fig_2">3</ref> show that ADMoE enables MLP and DeepSAD to use multiple sets of noisy labels, which outperform leading AD methods that can use only one set of labels, at varying noisy label qualities (x-axis). We further use Table <ref type="table" target="#tab_5">3</ref> to compare them at noisy label quality 0.2 to understand the specific gain of ADMoE using multiple sets of noisy labels. The third block of the table shows that AD-MoE brings on average 13.83%, and up to 33.96% (aloi; 3-rd row) improvements to a simple MLP. Additionally, the fourth block of the table further demonstrates that ADMoE enhances DeepSAD for using multiple noisy labels, with on average 14.44%, and up to 32.79% (imdb; 4-th row) gains.</p><p>These results demonstrate the benefit of using ADMoE to enable AD algorithms to learn from multiple noisy sources. ADMoE shows larger improvement when labels are more noisy (to the left of x-axis of Fig. <ref type="figure" target="#fig_2">3</ref>). We observe that ADMoE-DeepSAD brings up to 60% of ROC-AUC improvement over the best-performing AD algorithm at noisy label quality 0.01 (see Fig. <ref type="figure" target="#fig_2">3c</ref> for imdb). This observation is meaningful as we mostly need an approach to improve detection quality when the labels are extremely noisy, where ADMoE yields more improvement. Fig. <ref type="figure" target="#fig_2">3</ref> also demonstrates when the labels are less noisy (to the right of the x-axis), the performance gaps among methods are smaller since the diversity among noisy sources is also reduced-using any set of noisy labels is sufficient to train a good AD model. Also, note that ADMoE-DeepSAD shows better performance than ADMoE-MLP with more noisy labels. This results from the additional robustness of the underlying semi-supervised method DeepSAD with access to unlabeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison Between ADMoE and Noisy</head><p>Label Learning Methods (Q2)</p><p>ADMoE also shows an edge over SOTA classification methods that learn from multiple sets of noisy labels 4 , as shown in Fig. <ref type="figure" target="#fig_0">1b</ref> and<ref type="figure" target="#fig_3">4</ref>. We also analyze the comparison at noisy label quality 0.05 in Table <ref type="table" target="#tab_6">4</ref>. where ADMoE ranks the best in 7 out of 8 datasets. In addition to bet- 4 We adapt these classification methods (Table <ref type="table" target="#tab_1">1</ref>) for MNLAD.   We highlight the best model per row in bold. The specialized expert performs the best in their assigned subsamples by gating, i.e., the diagonal is all in bold.</p><formula xml:id="formula_4">1RLV\ODEHOTXDOLW\KLJKHUFOHDQHUWRWKHULJKW 52&amp;$8&amp;KLJKHUWKHEHWWHU 6LQJOH1RLV\ 0DMRU9RWH +(B$ +(B0 &amp;URZG/D\HU 8QLRQ1HW $'0R(0/3 (a) imdb 1RLV\ODEHOTXDOLW\KLJKHUFOHDQHUWRWKHULJKW 52&amp;$8&amp;KLJKHUWKHEHWWHU 6LQJOH1RLV\ 0DMRU9RWH +(B$ +(B0 &amp;URZG/D\HU 8QLRQ1HW $'0R(0/3 (b) svhn</formula><p>ing to contextualize how each expert responds to specific groups of samples due to the complexity of NLP models <ref type="bibr" target="#b28">(Shazeer et al. 2017;</ref><ref type="bibr" target="#b40">Zheng et al. 2019;</ref><ref type="bibr" target="#b43">Zuo et al. 2021)</ref>.</p><p>Given that AD models are much smaller and we use only one MoE layer before the output layer, we present an interesting case study (MLP with 4 experts where top 1 gets activated) in Table <ref type="table" target="#tab_7">5</ref>. We find each expert achieves the highest ROC-AUC on the subsamples where it gets activated by gating (see the diagonal), and they are significantly better than training an individual model with the same architecture (see the last column). Thus, each expert does develop a specialization in the subsamples they are "responsible" for.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Ablation on</head><p>Using MoE and Noisy Labels as Inputs in ADMoE. Additionally, we analyze the effect of using (i) ADMoE layer and (ii) noisy labels as input features in Fig. <ref type="figure" target="#fig_4">5</ref> and Appx. Fig. <ref type="figure">C2</ref>. First, ADMoE performs the best while using these two techniques jointly in most cases, and significantly better than not using them ( ). Second, AD-MoE helps the most when labels are noisier (to the left of the x-axis) with an avg. of 2% improvement over only using noisy labels as input ( ). As expected, its impact is reduced with less noisy labels (i.e., closer to the ground truth): in that case, noisy labels are more similar to each other and specialization with ADMoE is therefore limited.  We vary the number of activated (x-axis) and total (y-axis) experts in ADMoE, and compare their detection accuracy in Fig. <ref type="figure" target="#fig_5">6</ref> and Appx. Fig. <ref type="figure" target="#fig_12">C3</ref>. The results suggest that the best choice is data-dependent, and we use m = 4 and k = 2 for all the datasets in the experiments.  As introduced in ?3.3.3, one merit of ADMoE is the easy integration of clean labels when available. Fig. <ref type="figure" target="#fig_7">7</ref> and Appx. Fig. <ref type="figure" target="#fig_13">C4</ref> show that ADMoE can leverage the available clean labels to achieve higher performance. Specifically, ADMoE with only 8% clean labels can achieve similar or better results than that of using all available clean labels on spamspace and svhn. 3HUFFOHDQODEHODYDLODEOHDWWUDLQLQJ 52&amp;$8&amp;KLJKHUWKHEHWWHU</p><p>)XOOFOHDQODEHOV $'0R(SDUWFOHDQ $'0R(  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We propose ADMoE, a model-agnostic learning framework, to enable anomaly detection algorithms to learn from multiple sets of noisy labels. Leveraging Mixture-of-experts (MoE) architecture from the NLP domain, ADMoE is scalable in building specialization based on the diversity among noisy sources. Extensive experiments show that ADMoE outperforms a wide range of leading baselines, bringing on average 14% improvement over not using it. Future work can explore its usage with complex detection algorithms. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Experiment Setting and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Dataset Description</head><p>As discussed in ?4.1, we use the results based on the Classification noise since that is more realistic and closer to realworld applications (e.g., noise is not at random), while we also release the datasets by Label flipping for broader usages. See our repo to access both versions of datasets.</p><p>Process of Inaccurate Output. We use varying percentages of ground truth labels to train t diverse classifiers (called noisy label generators), and consider their (inaccurate) predictions as noisy labels. Naturally, with more available ground truth labels to train a classifier, its prediction (e.g., noisy labels) will be more accurate-we therefore control the noise levels by the availability of ground truth labels.</p><p>Following this approach, we generate the noisy labels for the benchmark datasets at varying percentages of ground truth labels (namely, {0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5}) to train classifiers (a simple feed-forward MLP <ref type="bibr" target="#b24">(Rosenblatt 1958)</ref>, decision tree <ref type="bibr" target="#b3">(Breiman et al. 2017)</ref>, and ensemble methods Random Forest <ref type="bibr" target="#b2">(Breiman 2001)</ref> and LightGBM <ref type="bibr" target="#b15">(Ke et al. 2017)</ref>). Please see our code for more details.</p><p>Table <ref type="table" target="#tab_1">B1</ref> summarizes avg. ROC-AUC of the generated noisy labels while using varying percentages of ground truth labels to train noisy label generators. Of course, with more ground truth labels, the generated noisy labels are also more accurate. Note that security * comes with actual noisy labels, and thus ROC-AUC does not vary. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 AD Algorithms and Binary Classifiers</head><p>We provide a brief description of AD algorithms below (see more details to recent literature <ref type="bibr" target="#b25">(Ruff et al. 2021;</ref><ref type="bibr" target="#b13">Han et al. 2022)</ref>). Since it lacks specialized fully supervised AD methods, we discuss some SOTA binary classifiers below.</p><p>1. Extreme Gradient Boosting Outlier Detection (XG-BOD) <ref type="bibr" target="#b35">(Zhao and Hryniewicki 2018)</ref>. XGBOD uses unsupervised outlier detectors to extract representations for the underlying dataset and concatenates the newly generated features to the original feature feature for augmentation. An XGBoost classifier is then applied to the augmented feature space.  <ref type="bibr" target="#b26">(Ruff et al. 2019)</ref>. DeepSAD is considered as the SOTA semi-supervised AD method. It improves the early version of unsupervised DeepSVDD <ref type="bibr" target="#b27">(Ruff et al. 2018</ref>) by including the supervision to penalize the inverse of the distances of anomaly representation. In this way, anomalies are forced to be in the space which is away from the hypersphere center. 5. Multi-layer Perceptron (MLP) <ref type="bibr" target="#b24">(Rosenblatt 1958)</ref>.</p><p>MLP is a simple feedforward version of the neural network, which uses the binary cross entropy loss to update network parameters. 6. Highly Efficient Gradient Boosting Decision Tree (LightGBM) <ref type="bibr" target="#b15">(Ke et al. 2017</ref>) is a gradient boosting framework that uses tree-based learning algorithms with faster training speed, higher efficiency, lower memory usage, and better accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C More Experiment Settings and Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Code and Reproducibility</head><p>See our datasets and code at anonymous repository https: //tinyurl.com/admoe22. Upon acceptance, we will release it publicly. Part of the code is based on PyOD <ref type="bibr">(Zhao, Nasrullah, and Li 2019)</ref> and ADBench <ref type="bibr" target="#b13">(Han et al. 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Hyperparameter Setting of ADMoE and Baselines</head><p>For all baselines used in this study, we use the same set of key hyperparameters for a fair comparison. We also use algorithms default hyperparameter (HP) settings in the original paper for unique hyperparameters. More specifically, we use the same: (i) learning rate=0.001; (ii) batch size= 256; and (iii) model size (number of trainable parameters ? 18, 000). We use the small validation set (5%) to choose the epoch with the highest validation performance. See our code at https://tinyurl.com/admoe22 for more details.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Performance (ROC-AUC) comparison on Yelp (see results on all datasets in ?4.2 and 4.3), where ADMoE outperforms two groups of baselines: (a) SOTA AD methods; (b) leading classification methods for learning from multiple noisy sources. ADMoE enhanced DeepSAD and MLP are denoted as AD and AM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure2:A toy example of a 3-layer MLP for AD; 3 sets of noisy labels y w = {y w,1 , y w,2 , y w,3 } are assumed. Existing methods (a and b) learn to recover noisy labels explicitly, while ADMoE ( ?3.3.2) (c) uses an MoE architecture with noisylabel-aware expert activation to learn specialization from noisy sources without explicit label mapping. In the example, we add an ADMoE layer ( ) between the dense layers and the output layer; only the top two experts ( ) are activated for input samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: ROC-AUC comparison at different noisy label quality of ADMoE enhanced DeepSAD and MLP (denoted as AD and AM) with leading AD methods that can only leverage one set of noisy labels. We already show Yelp's result in Fig. 1a. Notably, ADMoE enabled AD and AM has significant performance improvement especially when the label quality is very low (to the left of the x-axis).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: ROC-AUC comparison at different noisy label quality of ADMoE enhanced MLP (AM) with leading multiset noisy-label learning methods. We already show Yelp's result in Fig. 1b. ADMoE outperforms most baselines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Ablation studies on (i) the use of ADMoE layer and (ii) the noisy labels y w as input. ADMoE (A) using both techniques shows the best results at (nearly) all settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Ablation studies on key hyperparameters in AD-MoE: (x-axis) the number of experts and (y-axis) top-k experts to activate. We show the results at noisy level 0.05, and find the best setting is data-dependent. See Appx. Fig. C3. 4.4.4 Performance on Varying Number of Clean Labels.As introduced in ?3.3.3, one merit of ADMoE is the easy integration of clean labels when available. Fig.7and Appx. Fig.C4show that ADMoE can leverage the available clean labels to achieve higher performance. Specifically, ADMoE with only 8% clean labels can achieve similar or better results than that of using all available clean labels on spamspace and svhn.</figDesc><graphic url="image-8.png" coords="7,325.78,170.07,82.41,82.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Analysis of integrating varying percentages (from 1% to 10%) of additional clean labels in ADMoE. The results show that ADMoE efficiently leverages the clean labels with increasing performance. See more in Appx. Fig. C4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>2. Deviation Networks (DevNet) (Pang, Shen, and van den Hengel 2019) uses a prior probability to enforce a statistical deviation score of input instances. 3. Pairwise Relation prediction-based ordinal regression Network (PReNet) (Pang et al. 2019) is a neural network-based model that defines a two-stream ordinal regression to learn the relation of instance pairs. 4. Deep Semi-supervised Anomaly Detection (Deep-SAD)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig</head><label></label><figDesc>Fig. C3 provides additional results for ?4.4.3 with consistent observations.C.6 Additional Results for Performance onVarying Percentage of Clean labels</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig.</head><label></label><figDesc>Fig. C4 provides additional results for ?4.4.4 with consistent observations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure C3 :</head><label>C3</label><figDesc>Figure C3: Additional ablation results on key hyperparameters in ADMoE: (x-axis) the number of experts and (y-axis) top-k experts to activate. We show the results at noisy level 0.05, and find the best setting is data-dependent.</figDesc><graphic url="image-14.png" coords="11,445.03,241.71,84.11,84.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure C4 :</head><label>C4</label><figDesc>Figure C4: Additional analysis of integrating varying percentages (from 1% to 10%) of additional clean labels in ADMoE. The results show that ADMoE efficiently leverages the clean labels with increasing performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Baselines and ADMoE for comparison with categorization by (first row) whether it uses multiple sets of weak labels, (second row) whether it only trains a single model, (third row) whether the training process is end-to-end and (the last row) whether it is scalable with regard to many sets of weak labels. ADMoE is an end-to-end, scalable paradigm.</figDesc><table><row><cell>Category</cell><cell cols="6">SingleNoisy LabelVote HE A HE M CrowdLayer UnionNet ADMoE</cell></row><row><cell>Multi-source</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Single-model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>End-to-end</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Scalability</cell><cell>High</cell><cell>High</cell><cell>Low Low</cell><cell>Med</cell><cell>Med</cell><cell>High</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>How do ADMoE-enhanced methods compare to SOTA AD methods that learn from a single set of labels? ( ?4.2) 2. How does ADMoE compare to leading (multiple sets of) noisy label learning methods for classification? ( ?4.3) 3. How does ADMoE perform under different settings, e.g., varying num. of experts and clean label ratios? ( ?4.4) Data description of the eight datasets used in this study: the top seven datasets are adapted from AD repo., e.g., DAMI<ref type="bibr" target="#b5">(Campos et al. 2016)</ref> and ADBench<ref type="bibr" target="#b13">(Han et al. 2022)</ref>, and security</figDesc><table><row><cell cols="3">4.1 Experiment Setting</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Data</cell><cell cols="5"># Samples # Features # Anomaly % Anomaly Category</cell></row><row><cell>ag news</cell><cell>10000</cell><cell>768</cell><cell>500</cell><cell>5.00</cell><cell>NLP</cell></row><row><cell>aloi</cell><cell>49534</cell><cell>27</cell><cell>1508</cell><cell>3.04</cell><cell>Image</cell></row><row><cell>mnist</cell><cell>7603</cell><cell>100</cell><cell>700</cell><cell>9.21</cell><cell>Image</cell></row><row><cell cols="2">spambase 4207</cell><cell>57</cell><cell>1679</cell><cell>39.91</cell><cell>Doc</cell></row><row><cell>svhn</cell><cell>5208</cell><cell>512</cell><cell>260</cell><cell>5.00</cell><cell>Image</cell></row><row><cell>Imdb</cell><cell>10000</cell><cell>768</cell><cell>500</cell><cell>5.00</cell><cell>NLP</cell></row><row><cell>Yelp</cell><cell>10000</cell><cell>768</cell><cell>500</cell><cell>5.00</cell><cell>NLP</cell></row><row><cell>security  *</cell><cell>5525</cell><cell>21</cell><cell>378</cell><cell>6.84</cell><cell>Security</cell></row></table><note><p>* is a proprietary enterprise-security dataset.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison between ADMoE-enhanced AD methods and leading AD methods (that can only use one set of labels) at noisy level 0.2. The best performance is highlighted in bold per dataset (row). ADMoE-based methods (ADMoE-MLP and ADMoE-DeepSAD denote ADMoE-enhanced MLP and DeepSAD) outperform all baselines. ADMoE brings on average 13.83% and up to 33.96% improvement over the original MLP, and on average 14.44% and up to 32.79% improvement over DeepSAD. Note that all the neural-network models use the equivalent numbers of parameters and training FLOPs.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Performance comparison between ADMoE and leading noisy-label learning methods (in Table1) on an MLP at noisy label quality 0.05. The best performance is highlighted in bold per dataset (row). ADMoE mostly outperforms baselines, with on avg. 9.4% and up to 19% improvement over SingleNoisy which trains w/ a set of noisy labels.</figDesc><table><row><cell>Dataset</cell><cell>Single Noisy</cell><cell>Major Vote</cell><cell>HE A</cell><cell>HE M</cell><cell>Crowd Layer</cell><cell>Union Net</cell><cell>ADMoE</cell></row><row><cell>agnews</cell><cell cols="7">0.7185 0.5729 0.7824 0.8101 0.8543 0.7412 0.8549</cell></row><row><cell>aloi</cell><cell>0.531</cell><cell>0.479</cell><cell cols="5">0.5195 0.5579 0.5495 0.5773 0.5842</cell></row><row><cell>imdb</cell><cell cols="7">0.5967 0.5521 0.6587 0.6599 0.6219 0.5918 0.6577</cell></row><row><cell>mnist</cell><cell cols="7">0.9482 0.9467 0.9598 0.9563 0.9644 0.6767 0.9655</cell></row><row><cell cols="8">spamspace 0.9626 0.9655 0.9584 0.9586 0.7654 0.9567 0.9655</cell></row><row><cell>svhn</cell><cell cols="7">0.7201 0.6222 0.7971 0.7998 0.8161 0.6608 0.8451</cell></row><row><cell>yelp</cell><cell cols="7">0.6777 0.5708 0.7298 0.7358 0.7418 0.7133 0.7533</cell></row><row><cell>security  *</cell><cell cols="7">0.7363 0.7653 0.7526 0.7484 0.7374 0.7435 0.7767</cell></row><row><cell>Average</cell><cell cols="7">0.7363 0.6843 0.7722 0.7783 0.7563 0.7014 0.8003</cell></row><row><cell cols="8">ter detection accuracy, ADMoE only builds a single model,</cell></row><row><cell cols="8">and is thus faster than HE E and HE M which requires</cell></row><row><cell cols="8">building k independent models to aggregate predictions. We</cell></row><row><cell cols="8">credit ADMoE's performance over SOTA algorithms includ-</cell></row><row><cell cols="8">ing CrowdLayer and UnionNet to its implicit mapping of</cell></row><row><cell cols="4">noisy labels to experts ( ?3.3.3).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">4.4 Ablation Studies and Other Analysis (Q3)</cell></row></table><note><p>4.4.1. Case Study: How does ADMoE Help? Although MoE is effective in various NLP tasks, it is often challeng-</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Perf. breakdown of each expert and a comparison model (w/ the same capacity as each expert but trained independently; last col.) on subsamples activated for each expert by MoE on Yelp.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>Throughout the paper, we slightly abuse the term ADMoE to refer to both our overall framework and the proposed MoE layer.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material of ADMoE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Results by Average Precision (AP) C.4 More Ablation Results on the Use of MoE and Weak Inputs</head><p>In addition to the ablation studies in ?4.4.2, we provide additional results in Fig. <ref type="figure">C2</ref> for the effects of using ADMoE and noisy labels as inputs. Similar statements can be made. First, ADMoE performs the best while using these two techniques jointly in most cases, and significantly better than not using them ( ). Second, ADMoE helps the most when labels are noisier (to the left of the x-axis) with an avg. of 2% improvement over only using noisy labels as input ( ). As expected, its impact is reduced with less noisy labels (i.e., closer to the ground truth): in that case, noisy labels are more similar to each other and specialization with ADMoE is less useful.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Outlier Analysis</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Outlier ensembles: An introduction</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sathe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Olshen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<title level="m">Classification and regression trees</title>
		<imprint>
			<publisher>Routledge</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Thermometer encoding: One hot way to resist adversarial examples</title>
		<author>
			<persName><forename type="first">J</forename><surname>Buckman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">O</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">? ? R</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Micenkov ? ?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Assent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Springer US</publisher>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Xgboost: A scalable tree boosting system</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd acm sigkdd international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">StableMoE: Stable Routing Strategy for Mixture of Experts</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="7085" to="7095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Akoglu</surname></persName>
		</author>
		<idno>abs/2206.07647</idno>
		<title level="m">Hyperparameter Sensitivity in Deep Outlier Detection: Analysis and a Scalable Hyper-Ensemble Solution</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Glam: Efficient scaling of language models with mixture-ofexperts</title>
		<author>
			<persName><forename type="first">N</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5547" to="5569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Who said what: Modeling individual labelers improves classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gulshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A survey of label-noise representation learning: Past, present and future</title>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<idno>ArXiv preprint, abs/2011.04406</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">ADBench: Anomaly Detection Benchmark</title>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<idno>abs/2206.09426</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adaptive mixtures of local experts</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Nowlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="79" to="87" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Lightgbm: A highly efficient gradient boosting decision tree</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Tods: An automated time series outlier detection system</title>
		<author>
			<persName><forename type="first">K.-H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zumkhawaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the aaai conference on artificial intelligence</title>
		<meeting>the aaai conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="16060" to="16062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Self-training multisequence learning with Transformer for weakly supervised video anomaly detection</title>
		<author>
			<persName><forename type="first">M.-C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><surname>Ieee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Big Data (Big Data)</title>
		<imprint>
			<date type="published" when="2018">2020. 2022. 2018</date>
			<biblScope unit="page" from="950" to="956" />
		</imprint>
	</monogr>
	<note>Semi-supervised Rare Disease Detection Using Generative Adversarial Network. In NeurIPS Workshop on Machine Learning for Health (ML4H</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">D ?oT: A federated self-learning anomaly detection system for IoT</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Marchal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Miettinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fereidooni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Asokan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-R</forename><surname>Sadeghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="756" to="767" />
			<date type="published" when="2019">2019. 2019</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep learning for anomaly detection: A review</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep anomaly detection with deviation networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<idno>abs/1910.13601</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="353" to="362" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
	<note>Deep weakly-supervised anomaly detection</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName><forename type="first">G</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishna Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1944" to="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scaling vision with sparse mixture of experts</title>
		<author>
			<persName><forename type="first">C</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Susano Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8583" to="8595" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep learning from crowds</title>
		<author>
			<persName><forename type="first">F</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The perceptron: a probabilistic model for information storage and organization in the brain</title>
		<author>
			<persName><forename type="first">F</forename><surname>Rosenblatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">386</biblScope>
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A unifying review of deep and shallow anomaly detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Kauffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>M?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep Semi-Supervised Anomaly Detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>G?rnitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep One-Class Classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>G?rnitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="4393" to="4402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning from noisy labels with deep neural networks: A survey</title>
		<author>
			<persName><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-G</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to compose topic-aware mixture of experts for zero-shot video captioning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8965" to="8972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Combating noisy labels by agreement: A joint training method with coregularization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="13726" to="13735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep Learning From Multiple Noisy Annotators as A Union</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hyperparameter ensembles for robustness and uncertainty quantification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wenzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6514" to="6527" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">How does disagreement help generalization against label corruption</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7164" to="7173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">XGBOD: Improving Supervised Outlier Detection with Unsupervised Representation Learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Hryniewicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">LSCP: Locally Selective Combination in Parallel Outlier Ensembles</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Nasrullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Hryniewicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 SIAM International Conference on Data Mining</title>
		<meeting>the 2019 SIAM International Conference on Data Mining<address><addrLine>Calgary, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="585" to="593" />
		</imprint>
	</monogr>
	<note>SDM 2019</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">PyOD: A Python Toolbox for Scalable Outlier Detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Nasrullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research (JMLR)</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">96</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Automatic unsupervised outlier model selection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Akoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="4489" to="4502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Meta label correction for noisy label learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Awadallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="11053" to="11061" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Self-supervised mixture-of-experts by uncertainty estimation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5933" to="5940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A brief introduction to weakly supervised learning</title>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">National science review</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="44" to="53" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Ensembles for unsupervised outlier detection: challenges and research questions a position paper</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Sigkdd Explorations Newsletter</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="11" to="22" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Taming Sparsely Activated Transformer with Stochastic Experts</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>International Conference on Learning Representations</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
