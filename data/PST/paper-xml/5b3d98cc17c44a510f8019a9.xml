<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Subspace Clustering by Block Diagonal Representation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Canyi</forename><surname>Lu</surname></persName>
							<email>canyilu@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
							<email>zlin@pku.edu.cn</email>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">T</forename><surname>Mei</surname></persName>
							<email>tmei@microsoft.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (MOE)</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Cooperative Medianet Innovation Center</orgName>
								<orgName type="institution">Shanghai Jiaotong University</orgName>
								<address>
									<postCode>200240</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Subspace Clustering by Block Diagonal Representation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AEA212995815EAAFF6FA31BBE554D934</idno>
					<idno type="DOI">10.1109/TPAMI.2018.2794348</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2018.2794348, IEEE Transactions on Pattern Analysis and Machine Intelligence IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2018.2794348, IEEE Transactions on Pattern Analysis and Machine Intelligence IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 2</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Subspace clustering</term>
					<term>spectral clustering</term>
					<term>block diagonal regularizer</term>
					<term>block diagonal representation</term>
					<term>nonconvex optimization</term>
					<term>convergence analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper studies the subspace clustering problem. Given some data points approximately drawn from a union of subspaces, the goal is to group these data points into their underlying subspaces. Many subspace clustering methods have been proposed and among which sparse subspace clustering and low-rank representation are two representative ones. Despite the different motivations, we observe that many existing methods own the common block diagonal property, which possibly leads to correct clustering, yet with their proofs given case by case. In this work, we consider a general formulation and provide a unified theoretical guarantee of the block diagonal property. The block diagonal property of many existing methods falls into our special case. Second, we observe that many existing methods approximate the block diagonal representation matrix by using different structure priors, e.g., sparsity and low-rankness, which are indirect. We propose the first block diagonal matrix induced regularizer for directly pursuing the block diagonal matrix. With this regularizer, we solve the subspace clustering problem by Block Diagonal Representation (BDR), which uses the block diagonal structure prior. The BDR model is nonconvex and we propose an alternating minimization solver and prove its convergence. Experiments on real datasets demonstrate the effectiveness of BDR.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>As we embark on the big data era -in which the amount of the generated and collected data increases quickly, the data processing and understanding become impossible in the raw form. Looking for the compact representation of data by exploiting the structure of data is crucial in understanding the data with minimal storage. It is now widely known that many high dimensional data can be modeled as samples drawn from the union of multiple low-dimensional linear subspaces. For example, motion trajectories in a video <ref type="bibr" target="#b7">[8]</ref>, face images <ref type="bibr" target="#b1">[2]</ref>, hand-written digits <ref type="bibr" target="#b12">[13]</ref> and movie ratings <ref type="bibr" target="#b43">[44]</ref> can be approximately represented by subspaces, with each subspace corresponding to a class or category. Such a subspace structure has been very widely used for the data processing and understanding in supervised learning, semi-supervised learning and many other tasks <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b44">[45]</ref>. In this work, we are interested in the task of subspace clustering, whose goal is to group (or cluster) the data points which approximately lie in linear subspaces into clusters with each cluster corresponding to a subspace. Subspace clustering has many applications in computer vision <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b29">[30]</ref>, e.g., motion segmentation, face clustering and image segmentation, hybrid system identification in control <ref type="bibr" target="#b0">[1]</ref>, community clustering in social networks <ref type="bibr" target="#b14">[15]</ref>, to name a few. Note that subspace clustering is a data clustering task but with the additional assumption that the sampled data have the approximately linear subspace structure. Such data points are not necessarily locally distributed. The traditional clustering methods, e.g., spectral clustering <ref type="bibr" target="#b32">[33]</ref>, which use the spatial proximity of the data in each cluster are not applicable to subspace clustering. We need some more advanced methods for subspace clustering by utilizing the subspace structure as a prior.</p><p>Notations. We denote matrices by boldface capital letters, e.g., A, vectors by boldface lowercase letters, e.g., a, and scalars by lowercase letters, e.g., a. We denote a ij or A ij as the (i, j)-th entry of A. The matrix columns and rows are denoted by using <ref type="bibr">[•]</ref> with subscripts, e.g., [A] i,: is the i-th row, and [A] :,j is the j-th column. The absolute matrix of A, denoted by |A|, is the absolute value of the elements of A. We denote diag(A) as a vector with its i-th element being the i-th diagonal element of A ∈ R n×n , and Diag(a) as a diagonal matrix with its i-th element on the diagonal being a i . The all one vector is denoted as 1. The identity matrix is denoted as I. If A is positive semi-definite, we denote A 0. For symmetric matrices A, B ∈ R n×n , we denote A B or B A if B -A 0. If all the elements of A are nonnegative, we denote A ≥ 0. The trace of a square matrix A is denoted as Tr(A). We define [A] + = max(0, A) which gives the nonnegative part of the matrix. Some norms will be used, e.g., 0 -norm A 0 (number of nonzero elements), 1 -norm</p><formula xml:id="formula_0">A 1 = ij |a ij |, Frobenius norm (or 2 -norm of a vector) A = ij a 2 ij , 2,1 -norm A 2,1 = j [A] :,j , 1,2 -norm A 1,2 = i [A] i,:</formula><p>, spectral norm A 2 (largest singular value), ∞ -norm A ∞ = max ij |a ij | and nuclear norm A * (sum of all singular values).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head><p>Due to the numerous applications in computer vision and image processing, during the past two decades, subspace clustering has been extensively studied and many algorithms have been proposed to tackle this problem. According to their mechanisms of representing the subspaces, existing works can be roughly divided into four main categories: mixture of Gaussian, matrix factorization, algebraic, and spectral-type methods. The mixture of Gaussian based methods model the data points as independent samples drawn from a mixture of Gaussian distributions. So subspace clustering is converted to the model estimation problem and the estimation can be performed by using the Expectation Maximization (EM) algorithm. Representative methods are K-plane <ref type="bibr" target="#b2">[3]</ref> and Q-flat <ref type="bibr" target="#b37">[38]</ref>. The limitations are that they are sensitive to errors and the initialization due to the optimization mechanism. The matrix factorization based methods, e.g., <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b11">[12]</ref>, tend to reveal the data segmentation based on the factorization of the given data matrix. They are sensitive to data noise and outliers. Generalized Principal Component Analysis (GPCA) <ref type="bibr" target="#b38">[39]</ref> is a representative algebraic method for subspace clustering. It fits the data points with a polynomial. However, this is generally difficult due to the data noise and its cost is high especially for highdimensional data. Due to the simplicity and outstanding performance, the spectral-type methods attract more attention in recent years. We give a more detailed review of this type of methods as follows.</p><p>The spectral-type methods use the spectral clustering algorithm <ref type="bibr" target="#b32">[33]</ref> as the framework. They first learn an affinity matrix to find the low-dimensional embedding of data and then k-means is applied to achieve the final clustering result. The main difference among different spectraltype methods lies in the different ways of affinity matrix construction. The entries of the affinity matrix (or graph) measure the similarities of the data point pairs. Ideally, if the affinity matrix is block diagonal, i.e., the betweencluster affinities are all zeros, one may achieve perfect data clustering by using spectral clustering. The way of affinity matrix construction by using the typical Gaussian kernel, or other local information based methods, e.g., Local Subspace Affinity (LSA) <ref type="bibr" target="#b41">[42]</ref>, may not be a good choice for subspace clustering since the data points in a union of subspaces may be distributed arbitrarily but not necessarily locally. Instead, a large body of affinity matrix construction methods for subspace clustering by using global information have been proposed in recent years, e.g., <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b40">[41]</ref>. The main difference among them lies in the used regularization for learning the representation coefficient matrix.</p><p>Assume that we are given the data matrix X ∈ R D×n , where each column of X belongs to a union of k subspaces {S} k i=1 . Each subspace i contains n i data samples with</p><formula xml:id="formula_1">k i=1 n i = n. Let X i ∈ R D×ni denote the submatrix in X that belongs to S i . Without loss of generality, let X = [X 1 , X 2 , • • • , X k ]</formula><p>be ordered according to their subspace membership. We discuss the case that the sampled data are noise free. By taking advantage of the subspace structure, the sampled data points obey the so called selfexpressiveness property, i.e., each data point in a union of subspaces can be well represented by a linear combination of other points in the dataset. This can be formulated as</p><formula xml:id="formula_2">X = XZ,<label>(1)</label></formula><p>where Z ∈ R n×n is the representation coefficient matrix. The choice of Z is usually not unique and the goal is to find certain Z such that it is discriminative for subspace clustering. In the ideal case, we are looking for a linear representation Z such that each sample is represented as a linear combination of samples belonging to the same subspace, i.e., X i = X i Z i , where Z i is expected not to be an identity matrix. In this case, Z in (1) has the k-block diagonal structure 1 , i.e.,</p><formula xml:id="formula_3">Z =      Z 1 0 • • • 0 0 Z 2 • • • 0 . . . . . . . . . . . . 0 0 • • • Z k      , Z i ∈ R ni×ni .<label>(2)</label></formula><p>So the above Z reveals the true membership of data X. If we apply spectral clustering on the affinity matrix defined as (|Z| + |Z |)/2, then we may get correct clustering. So the block diagonal matrix plays a central role in the analysis of subspace clustering, though there has no "ground-truth" Z (or it is not necessary). We formally give the following definition. <ref type="bibr" target="#b1">(2)</ref>, where the nonzero entries Z i correspond to only X i .</p><formula xml:id="formula_4">Definition 1. (Block Diagonal Property (BDP)) Given the data matrix X = [X 1 , X 2 , • • • , X k ] drawn from a union of k subspaces {S i } k i=1 , we say that Z obeys the Block Diagonal Property if Z is k-block diagonal as in</formula><p>Note that the concepts of the k-block diagonal matrix and block diagonal property have some connections and differences. The block diagonal property is specific for subspace clustering problem but k-block diagonal matrix is not. A matrix obeying the block diagonal property is k-block diagonal, but not vice versa. The block diagonal property further requires that each block corresponds one-to-one with each subject of data.</p><p>Problem (1) may have many feasible solutions and thus the regularization is necessary to produce the block diagonal solution. Motivated by the observation that the block diagonal solution in (2) is sparse, the Sparse Subspace Clustering (SSC) <ref type="bibr" target="#b9">[10]</ref> finds a sparse Z by 0 -norm minimizing. However, this leads to an NP-hard problem and the 1 -norm is used as the convex surrogate of 0 -norm. This leads to the following convex program</p><formula xml:id="formula_5">min Z Z 1 , s.t. X = XZ, diag(Z) = 0.<label>(3)</label></formula><p>It is proved that the optimal solution Z by SSC satisfies the block diagonal property when the subspaces are independent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2. (Independent subspaces)</head><formula xml:id="formula_6">A collection of sub- spaces {S i } k i=1 is said to be independent if dim(⊕ n i=1 S i ) = n i=1 dim(S i ),</formula><p>where ⊕ denotes the direct sum operator.</p><p>1. In this work, we say that a matrix is k-block diagonal if it has at least k connected components (blocks). The block diagonalty is up to a permutation, i.e., if Z is k-block diagonal, then P ZP is still kblock diagonal for any permutation matrix P. See also the discussions in Section 3.1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods f (Z, X)</head><p>Ω SSC <ref type="bibr" target="#b9">[10]</ref> Z 1 {Z|diag(Z) = 0} LRR <ref type="bibr" target="#b20">[21]</ref> Z * -MSR <ref type="bibr" target="#b28">[29]</ref> Z</p><formula xml:id="formula_7">1 + λ Z * {Z|diag(Z) = 0} SSQP [41] Z Z 1 {Z|diag(Z) = 0, Z ≥ 0} LSR [28]</formula><p>Z<ref type="foot" target="#foot_0">2</ref> -CASS <ref type="bibr" target="#b23">[24]</ref> j XDiag([Z]:,j) * {Z|diag(Z) = 0}</p><p>1 Ω is not specified if there has no restriction on Z.</p><p>Another important spectral-type method is Low-Rank Representation (LRR) <ref type="bibr" target="#b20">[21]</ref>. It seeks a low-rank coefficient matrix by nuclear norm minimization</p><formula xml:id="formula_8">min Z Z * , s.t. X = XZ.<label>(4)</label></formula><p>The above problem has a unique closed form solution Z = VV , where V is from the skinny SVD of X = USV . This matrix, termed Shape Interaction Matrix (SIM) <ref type="bibr" target="#b7">[8]</ref>, has been widely used for subspace segmentation. It also enjoys the block diagonal property when the subspaces are independent <ref type="bibr" target="#b20">[21]</ref>.</p><p>Beyond SSC and LRR, many other subspace clustering methods, e.g., <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b40">[41]</ref>, have been proposed and they all fall into the following formulation</p><formula xml:id="formula_9">min f (Z, X), s.t. X = XZ, Z ∈ Ω,<label>(5)</label></formula><p>where Ω is some matrix set. The main difference lies in the choice of the regularizer or objective. For example, the Multi-Subspace Representation (MSR) <ref type="bibr" target="#b28">[29]</ref> combines the idea of SSC and LRR, while the Least Squares Regression (LSR) <ref type="bibr" target="#b27">[28]</ref> simply uses Z 2 and it is efficient due to a closed form solution. See Table <ref type="table" target="#tab_0">1</ref> for a summary of existing spectral-type methods. An important common property for the methods in Table <ref type="table" target="#tab_0">1</ref> is that their solutions all obey the block diagonal property under certain subspace assumption (all require independent subspaces assumption except SSQP <ref type="bibr" target="#b40">[41]</ref> that requires orthogonal subspaces assumption). Their proofs use specific properties of their objectives.</p><p>Beyond the independent subspaces assumption, some other subspaces assumptions are proposed to analyze the block diagonal property in different settings <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b35">[36]</ref>. However, the block diagonal property of Z does not guarantee the correct clustering, since each block may not be fully connected. For example, the work <ref type="bibr" target="#b9">[10]</ref> shows that the block diagonal property holds for SSC when the subspaces are disjoint and the angles between subspace pairs are large enough. Such an assumption is weaker than the independent subspaces assumption, but the price is that SSC suffers from the so-called "graph connectivity" issue <ref type="bibr" target="#b31">[32]</ref>. This issue is also related to the correlation of the columns of the data matrix <ref type="bibr" target="#b27">[28]</ref>. As will be seen in Theorem 3 given later, the 1 -minimization in SSC makes not only the between-cluster connections sparse, but also the innercluster connections sparse. In this case, the clustering results obtained by spectral clustering may not be correct. Nevertheless, the block diagonal property is the condition that verifies the design intuition of the spectral-type methods. If the obtained coefficient matrix Z obeys the block diagonal property and each block is fully connected (Z is not "too sparse"), then we immediately get the correct clustering.</p><p>The block diagonal property of the solutions by different methods in Table <ref type="table" target="#tab_0">1</ref> is common under certain subspace assumptions. However, in real applications, due to the data noise or corruptions, the required assumptions usually do not hold and thus the block diagonal property is violated. By taking advantage of the k-block diagonal structure as a prior, the work <ref type="bibr" target="#b10">[11]</ref> considers SSC and LRR with an additional hard Laplacian constraint, which enforces Z to be k-block diagonal with exact k connected blocks. Though such a k-block diagonal solution may not obey the block diagonal property without additional subspace assumption, it is verified to be effective in improving the clustering performance of SSC and LRR in some applications. Due to the nonconvexity, this model suffers from some issues: the used stochastic sub-gradient descent solver may not be stable; and the theoretical convergence guarantee is relatively weak due to the required assumptions on the data matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Contributions</head><p>In this work, we focus on the most recent spectral-type subspace clustering methods due to their simplicity and effectiveness. From the above review, it can be seen that the key difference between different spectral-type subspace clustering methods (as given in Table <ref type="table" target="#tab_0">1</ref>) is the used regularizer on the representation matrix Z. Their motivations for the design intuition may be quite different, but all have the common property that their solutions obey the block diagonal property under certain subspace assumption. However, their proofs of such a property are given case by case by using specific properties of the models. Moreover, existing methods in Table <ref type="table" target="#tab_0">1</ref> are indirect as their regularizers are not induced by the block diagonal matrix structure. The method in <ref type="bibr" target="#b10">[11]</ref> that enforces the solution to be k-block diagonal with exact k connected blocks by a hard constraint is a direct method. But such a constraint may be too restrictive since the k-block diagonal matrix is not necessary for correct clustering when using spectral clustering. A soft regularizer instead of the hard constraint may be more flexible. Motivated by these observations, we raise several interesting questions: 1. Consider the general model <ref type="bibr" target="#b4">(5)</ref>, what kind of objective f guarantees that the solutions obey the block diagonal property? 2. Is it possible to give a unified proof of the block diagonal property by using common properties of the objective f ? 3. How to design a soft block diagonal regularizer which encourages a matrix to be or close to be k-block diagonal? When applying it to subspace clustering, how to solve the block diagonal regularized problem efficiently with the convergence guarantee?</p><p>We aim to address the above questions and in particular we make the following contributions 2 : 1. We propose the Enforced Block Diagonal (EBD) conditions and prove in a unified manner that if the objective function in <ref type="bibr" target="#b4">(5)</ref> satisfies the EBD conditions, the solutions to (5) obey the block diagonal property when the subspaces are independent. We show that the EBD conditions are not restrictive and a large family of norms and their combinations satisfy these conditions. The block diagonal property of existing methods in Table <ref type="table" target="#tab_0">1</ref> falls into our special case. 2. We propose a k-block diagonal regularizer which encourages a nonnegative symmetric matrix to be k-block diagonal. Beyond the sparsity and low-rankness, we would like to emphasize that the block diagonal matrix is another interesting structure and our proposed block diagonal regularizer is the first soft regularizer for pursuing such a structure. The regularizer plays a similar role as the 0or 1 -norm for pursuing sparsity and the rank function or nuclear norm for pursuing low-rankness. See Figure <ref type="figure" target="#fig_0">1</ref> for intuitive illustrations of the three structured matrices. 3. We propose the Block Diagonal Representation (BDR) method for subspace clustering by using the block diagonal regularizer. Compared with the regularizers used in existing methods, BDR is more direct as it uses the block diagonal structure prior. A disadvantage of the BDR model is that it is nonconvex due to the block diagonal regularizer. We solve it by an alternating minimization method and prove the convergence without restrictive assumptions. Experimental analysis on several real datasets demonstrates the effectiveness of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">THEORY OF BLOCK DIAGONAL PROPERTY</head><p>In this section, considering problem (5), we develop the unified theory for pursuing solutions which obey the block diagonal property. We first give an important property of the feasible solution to <ref type="bibr" target="#b4">(5)</ref>. This will lead to our EBD conditions.</p><p>Theorem 1. Consider a collection of data points drawn from k independent subspaces</p><formula xml:id="formula_10">{S i } k i=1 of dimensions {d i } k i=1 . Let X = [X 1 , • • • , X k ] ∈ R D×n , where X i ∈ R D×ni denotes the data point drawn from S i , rank(X i ) = d i and k i=1 n i = n.</formula><p>For any feasible solution Z * ∈ R n×n to the following system</p><formula xml:id="formula_11">X = XZ,<label>(6)</label></formula><p>decompose it into two parts, i.e., Z * = Z B + Z C , where</p><formula xml:id="formula_12">Z B =       Z * 1 0 • • • 0 0 Z * 2 • • • 0 . . . . . . . . . . . . 0 0 • • • Z * k       , Z C =       0 * • • • * * 0 • • • * . . . . . . . . . . . . * * • • • 0       ,<label>(7)</label></formula><p>with</p><formula xml:id="formula_13">Z * i ∈ R ni×ni corresponding to X i . Then, we have XZ B = X, or equivalently X i Z * i = X i , i = 1, • • • , k, and XZ C = 0.</formula><p>Proof. For any feasible solution Z * to problem (6), we assume that [X] :,j = [XZ * ] :,j ∈ S l for some l. Then</p><formula xml:id="formula_14">[XZ B ] :,j = [X 1 Z 1 , • • • , X k Z k ] :,j ∈ S l and [XZ C ] :,j ∈ ⊕ i =l S i . On the other hand, [XZ C ] :,j = [XZ * ] :,j - [XZ B ] :,j ∈ S l . This implies that [XZ C ] :,j ∈ S l ∩ ⊕ i =l S i .</formula><p>By the assumption that the subspaces are independent, we have S l ∩ ⊕ i =l S i = {0}. Thus, [XZ C ] :,j = 0. Consider the above procedure for all j = 1, • • • , n, we have XZ C = 0 and thus XZ B = X -XZ C = X. The proof is completed. Theorem 1 gives the property of the representation matrix Z * under the independent subspaces assumption. The result shows that, to represent a data point [X] :,j in S l , only the data points X l from the same subspace S l have the real contributions, i.e., X = XZ B , while the total contribution of all the data points from other subspaces ⊕ i =l S i is zero, i.e., XZ C = 0. So Theorem 1 characterizes the underlying representation contributions of all data points. However, such contributions are not explicitly reflected by the representation matrix Z * since the decomposition Z * = Z B + Z C is unknown when Z C = 0. In this case, the solution Z * to (6) does not necessarily obey the block diagonal property, and thus it does not imply the true clustering membership of data. To address this issue, it is natural to consider some regularization on the feasible solution set of ( <ref type="formula" target="#formula_11">6</ref>) to make sure that Z C = 0. Then Z * = Z B obeys the block diagonal property. Previous works show that many regularizers, e.g., the 1 -norm and many others shown in Table <ref type="table" target="#tab_0">1</ref>, can achieve this end. Now the questions is, what kind of functions leads to a similar effect? Motivated by Theorem 1, we give a family of such functions as below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 3. (Enforced Block Diagonal (EBD) conditions)</head><p>Given any function f (Z, X) defined on (Ω, ∆), where Ω is a set consisting of some square matrices and ∆ is a set consisting of matrices with nonzero columns. For any</p><formula xml:id="formula_15">Z = Z 1 Z 3 Z 4 Z 2 ∈ Ω, Z = 0, Z 1 , Z 2 ∈ Ω, and X = [X 1 , X 2 ], where X 1 and X 2 correspond to Z 1 and Z 2 , respectively. Let Z B = Z 1 0 0 Z 2 ∈ Ω.</formula><p>Assume that all the matrices are of compatible dimensions. The EBD conditions for f are (1) f (Z, X) = f (P ZP, XP), for any permutation matrix</p><formula xml:id="formula_16">P, P ZP ∈ Ω. (2) f (Z, X) ≥ f (Z B , X)</formula><p>, where the equality holds if and only if Z = Z B (or</p><formula xml:id="formula_17">Z 3 = Z 4 = 0). (3) f (Z B , X) = f (Z 1 , X 1 ) + f (Z 2 , X 2 ).</formula><p>We have the following remarks for the EBD conditions: 1. The EBD condition (1) is a basic requirement for subspace clustering. It guarantees that the clustering result is invariant to any permutation of the columns of the input data matrix X. Though we assume that</p><formula xml:id="formula_18">X = [X 1 , X 2 , • • • , X k ]</formula><p>is ordered according to the true membership for the simplicity of discussion, the input matrix in problem ( <ref type="formula" target="#formula_9">5</ref>) can be X = XP, where P can be any permutation matrix which reorders the columns of X. Let Z be feasible to X = XZ. Then Ẑ = P ZP is feasible to X = X Ẑ. The EBD condition (1) guarantees that f (Z, X) = f ( Ẑ, X). Thus, Ẑ is equivalent to Z up to any reordering of the input data matrix X. This is necessary for data clustering. 2. The EBD condition (2) is the key which enforces the solutions to (5) to be block diagonal under certain subspace assumption. From Theorem 1, we have X = XZ = XZ B . So the EBD condition (2) guarantees that Z = Z B when minimizing the objective. This will be more clear from the proof of Theorem 3. 3. The EBD condition (3) is actually not necessary to enforce the solutions to (5) to be block diagonal. But through the lens of this condition, we will see the connection between the structure of each block of the block diagonal solutions and the used objective f . Also, we find that many objectives in existing methods satisfy this condition.</p><p>The EBD conditions are not restrictive. Before giving the examples, we provide some useful properties discussing different types of functions that satisfy the EBD conditions. Proposition 1. If f satisfies the EBD conditions ( <ref type="formula" target="#formula_2">1</ref>)-( <ref type="formula" target="#formula_5">3</ref>) on (Ω, ∆), then it does on (Ω 1 , ∆), where</p><formula xml:id="formula_19">Ω 1 ⊂ Ω and Ω 1 = ∅. Proposition 2. Assume that f (Z, X) = ij g ij (z ij )</formula><p>, where g ij is a function defined on Ω ij , and it satisfies that</p><formula xml:id="formula_20">g ij (z ij ) ≥ 0, g ij (z ij ) = 0 if and only if z ij = 0. Then f satisfies the EBD conditions (1)-(3) on (Ω, R D×n ), where Ω = {Z|z ij ∈ Ω ij }. Proposition 3. Assume that f (Z, X) = j g j ([Z] :,j , X), where g j is a function defined on (Ω j , ∆). Assume that X = [X 1 , X 2 ], w = [w 1 ; w 2 ] ∈ Ω j , w B = [w 1 ; 0] ∈ Ω j ,</formula><p>and their dimensions are compatible. If g j satisfies the following conditions:</p><p>(1) g j (w, X) = g j (P w, XP), for any permutation matrix P, P w ∈ Ω j , (2) g j (w, X) ≥ g j (w B , X), where the equality holds if and only if w = w B , (3) g j (w B , X) = g j (w 1 , X 1 ), then f satisfies the EBD conditions (1)-( <ref type="formula" target="#formula_5">3</ref>) on (Ω, ∆), where</p><formula xml:id="formula_21">Ω = {Z|[Z] :,j ∈ Ω j }. Proposition 4. Assume that f (Z, X) = i g i ([Z] i,: , X), where g i is a function defined on (Ω i , ∆). Assume that X = [X 1 , X 2 ], w = [w 1 ; w 2 ] ∈ Ω i , (w B ) = [w 1 , 0] ∈ Ω i ,</formula><p>and their dimensions are compatible. If g i satisfies the following conditions:</p><p>(1) g i (w , X) = g i (w P, XP), for any permutation matrix P, w P ∈ Ω i , (2) g i (w , X) ≥ g i ((w B ) , X), where the equality holds if and</p><formula xml:id="formula_22">only if w = w B , (3) g i ((w B ) , X) = g i (w 1 , X 1 ), then f satisfies the EBD conditions (1)-(3) on (Ω, ∆), where Ω = {Z|[Z] i,: ∈ Ω i }. Proposition 5. If f i satisfies the EBD conditions (1)-(3) on (Ω i , ∆), i = 1, • • • , m, then m i λ i f i (for positive λ i ) also satisfies the EBD conditions (1)-(3) on (Ω, ∆) when Ω = ∩ m i=1 Ω i and Ω = ∅. Proposition 6. Assume that f 1 satisfies the EBD conditions (1)- (3) on (Ω 1 , ∆), f 2 satisfies the EBD conditions (1)(3) on (Ω 2 , ∆) and f 2 (Z, X) ≥ f 2 (Z B , X)</formula><p>, where Z, Z B and X are the same as those in Definition 3. Then, f 1 + f 2 satisfies the EBD conditions ( <ref type="formula" target="#formula_2">1</ref>)-( <ref type="formula" target="#formula_5">3</ref>) on (Ω, ∆) when Ω = Ω 1 ∩ Ω 2 and Ω = ∅. Theorem 2. Some functions of interest which satisfy the EBD conditions (1)-( <ref type="formula" target="#formula_5">3</ref>) are:</p><formula xml:id="formula_23">Function f (Z, X) (Ω, ∆) 0-and 1-norm Z 0 and Z 1 - square of Z 2 - Frobenius norm elastic net Z 1 + λ Z 2 - 2,1-norm Z 2,1 - 1,2-norm Z 1,2 - - Z Z 1 Ω = {Z|Z ≥ 0} 1+nuclear norm Z 1 + λ Z * - trace Lasso j XDiag([Z]:,j) * ∆ = {X|∀j, [X]:,j = 0} others ij λij|zij| p ij -</formula><p>Theorem 2 gives some functions of interest which satisfy the EBD conditions. They can be verified by using Propositions 2-6. An intuitive verification is discussed as follows and the detailed proofs can be found in the supplementary material. </p><formula xml:id="formula_24">Z 0 + λ 1 Z 1 + λ 2 Z 2 +λ 3 Z 2,1 +λ 4 Z 1,2 +λ 5 Z Z 1 +λ 6 Z * + λ 7 i XDiag([Z] :,i ) * ,</formula><p>where λ i &gt; 0. So Proposition 5 enlarges the family of such type of functions and shows that the EBD conditions are not restrictive. 5. Proposition 6 shows that f 1 + f 2 satisfies the EBD conditions ( <ref type="formula" target="#formula_2">1</ref>)-( <ref type="formula" target="#formula_5">3</ref>) when f 1 satisfies the EBD conditions (1)-( <ref type="formula" target="#formula_5">3</ref>) and f 2 satisfies the EBD conditions (1)(3) and the first part of EBD condition (2). An example is</p><formula xml:id="formula_25">Z 1 + λ Z * .</formula><p>See more discussions about Z * below. There are also some interesting norms which do not satisfy the EBD conditions. For example, considering the infinity norm Z ∞ , the EBD condition (1) holds while the other two do not. The nuclear norm Z * satisfies the EBD condition (1)(3). But for the EBD condition (2), we only have (see <ref type="bibr">Lemma 7.4</ref> in <ref type="bibr" target="#b21">[22]</ref>)</p><formula xml:id="formula_26">Z 1 Z 3 Z 4 Z 2 * ≥ Z 1 0 0 Z 2 * = Z 1 * + Z 2 * .</formula><p>But the equality may hold when Z 3 = 0 and Z 4 = 0. A counterexample is that, when both Z and Z B are positive semidefinite,</p><formula xml:id="formula_27">Z * = i λ i (Z) = Tr(Z) = Tr(Z B ) = i λ i (Z B ) = Z B *</formula><p>, where λ i (Z)'s denote the eigenvalues of Z. As will be seen in the proof of Theorem 3, this issue makes the proof of the block diagonal property of LRR which uses the nuclear norm different from others. We instead use the uniqueness of the LRR solution to (4) to fix this issue. Now, based on the EBD conditions, below we will show that the solution to problem (5) satisfies the block diagonal property. This provides a new perspective to understand the common property of the block diagonal solution guarantee. Theorem 3. Consider a collection of data points drawn from</p><formula xml:id="formula_28">k independent subspaces {S i } k i=1 of dimensions {d i } k i=1 . Let X i ∈ R D×ni denote the data points in S i , rank(X i ) = d i and k i=1 n i = n. Let X = [X 1 , • • • , X k ] ∈ ∆,</formula><p>where ∆ is a set consisting of matrices with nonzero columns. Considering problem <ref type="bibr" target="#b4">(5)</ref>, assume that {Z|X = XZ} ∩ Ω is nonempty and let Z * be any optimal solution. If one of the following cases holds, Case I: f satisfies the EBD condition ( <ref type="formula" target="#formula_2">1</ref>)-( <ref type="formula" target="#formula_3">2</ref>) on (Ω, ∆), Case II: f satisfies the EBD condition (1) on (Ω, ∆) and Z * is the unique solution, then Z * satisfies the block diagonal property, i.e.,</p><formula xml:id="formula_29">Z * =       Z * 1 0 • • • 0 0 Z * 2 • • • 0 . . . . . . . . . . . . 0 0 • • • Z * k       ,<label>(8)</label></formula><p>with <ref type="formula" target="#formula_29">8</ref>) is optimal to the following problem:</p><formula xml:id="formula_30">Z * i ∈ R ni×ni corresponding to X i . Furthermore, if f satisfies the EBD conditions (1)-(3), then each block Z * i in (</formula><formula xml:id="formula_31">min W f (W, X i ) s.t. X i = X i W, W ∈ Ω.<label>(9)</label></formula><p>Proof. First, by the EBD condition (1), f (Z, X) = f (P ZP, XP) holds for any permutation P. This guarantees that the learned Z * based on X by solving ( <ref type="formula" target="#formula_9">5</ref>) is equivalent to P Z * P based on XP. So we only need to discuss the structure of Z * based on the ordered input data matrix</p><formula xml:id="formula_32">X = [X 1 , • • • , X k ].</formula><p>For any optimal solution Z * ∈ Ω to problem (5), we decompose it into two parts Z * = Z B + Z C , where Z B and Z C are of the forms in <ref type="bibr" target="#b6">(7)</ref>. Then, by Theorem 1, we have XZ B = X and XZ C = 0. This combines the EBD conditions, which implies that Z B is feasible to <ref type="bibr" target="#b4">(5)</ref>. By the EBD conditions (2), we have f (Z * , X) ≥ f (Z B , X). On the other hand, Z * is optimal to (5), thus we have f (Z * , X) ≤ f (Z B , X). Therefore, f (Z * , X) = f (Z B , X). In Case I, by the EBD condition (2), we have Z * = Z B . The same result holds in Case II. Hence, Z * = Z B satisfies the block diagonal property in both cases.</p><p>If the EBD condition (3) is further satisfied, we have</p><formula xml:id="formula_33">f (Z * , X) = k i=1 f (Z * i , X i ), which is separable. By the block diagonal structure of Z * , X = XZ * is equivalent to X i = X i Z * i , i = 1, • • • , k.</formula><p>Hence, both the objectives and constraints of ( <ref type="formula" target="#formula_9">5</ref>) are separable and thus problem ( <ref type="formula" target="#formula_9">5</ref>) is equivalent to problem <ref type="bibr" target="#b8">(9)</ref> for all i = 1, • • • , k. This guarantees the same solutions of ( <ref type="formula" target="#formula_9">5</ref>) and <ref type="bibr" target="#b8">(9)</ref>.</p><p>We have the following remarks for Theorem 3:</p><p>1. Theorem 3 gives a general guarantee of the block diagonal property for the solutions to (5) based on the EBD conditions. By Theorem 2, the block diagonal properties of existing methods (except LRR) in Table <ref type="table" target="#tab_0">1</ref> are special cases of Theorem 3 (Case I). Note that some existing models, e.g., SSC, have a constraint diag(Z) = 0. This does not affect the EBD conditions due to Proposition 1.</p><p>Actually, additional proper constraints can be introduced in ( <ref type="formula" target="#formula_9">5</ref>) if necessary and the block diagonal property still holds. 2. The nuclear norm used in LRR does not satisfy the EBD condition <ref type="bibr" target="#b1">(2)</ref>. Fortunately, the LRR model ( <ref type="formula" target="#formula_8">4</ref>) has a unique solution <ref type="bibr" target="#b21">[22]</ref>. Thus the block diagonal property of LRR is another special case of Theorem 3 (Case II). If we choose Ω = {Z|X = XZ}, then the nuclear norm satisfies the EBD conditions (1)(2) on (Ω, R d×n ) due to the uniqueness of LRR. So, in some cases, the Case II can be regarded as a special case of Case I in Theorem 3. 3. The SSQP method <ref type="bibr" target="#b40">[41]</ref> achieves the solution obeying the block diagonal property under the orthogonal subspace assumption. However, the EBD conditions and Theorem 3 show that the weaker independent subspace assumption is enough. Actually, if the subspaces are orthogonal, X X already obeys the block diagonal property. 4. Theorem 3 not only provides the block diagonal property guarantee of Z * (there are no connections betweensubspaces), but also shows what property each block has (the property of the connections within-subspace). Let us take the SSC model as an example. The i-th block Z * i of Z * , which is optimal to (3), is the minimizer to</p><formula xml:id="formula_34">Z * i = arg min W W 1 s.t. X i = X i W, diag(W) = 0.</formula><p>So SSC not only finds a sparse representation betweensubspaces but also within-subspace. Hence, each Z * i may be too sparse (not fully connected) especially when the columns of X i are highly correlated. This perspective provides an intuitive interpretation of the graph connectivity issue in SSC. 5. Theorem 3 not only provides a good summary of existing methods, but also provides the general motivation for designing new subspace clustering methods as the EBD conditions are easy to verify by using Proposition 1-6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SUBSPACE CLUSTERING BY BLOCK DIAGONAL REPRESENTATION</head><p>Theorem 3 shows that it is not difficult to find a solution obeying the block diagonal property under the independent subspaces assumption as the EBD conditions are not restrictive. Usually, the solution is far from being k-block diagonal since the independent subspaces assumption does not hold due to data noise. The more direct method <ref type="bibr" target="#b10">[11]</ref> enforces the representation coefficient matrix to be k-block diagonal with exact k connected blocks. However, in practice, the k-block diagonal affinity matrix is not necessary for correct clustering when using spectral clustering. Similar phenomenons are observed in the pursuits of sparsity and low-rankness. The sparsity (or low-rankness) is widely used as a prior in many applications, but the exact sparsity (or rank) is not (necessarily) known. So the 1 -norm (or nuclear norm) is very widely used as a regularizer to encourage the solution to be sparse (or low-rank). Now, considering the k-block diagonal matrix, which is another interesting structure, what is the corresponding regularizer?</p><p>In this section, we will propose a simple block diagonal regularizer for pursuing such an interesting structure. By using this regularizer, we then propose a direct subspace clustering subspace method, termed Block Diagonal Representation (BDR). We will also propose an efficient solver and provide the convergence guarantee.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Block Diagonal Regularizer</head><p>In this work, we say that a matrix is k-block diagonal if it has at least k connected components (blocks). Such a concept is somewhat ambiguous. For example, consider the following matrix</p><formula xml:id="formula_35">B =    B 0 0 0 0 B 0 0 0 0 B 0    , where B 0 = 1 0 -1 1<label>(10)</label></formula><p>is fully connected. We can say that B is 3-block diagonal (this is what we expect intuitively). But by the definition, we can also say that it is 1-or 2-block diagonal. Thus, we need a more precise way to characterize the number of connected components.</p><p>Assume that B is an affinity matrix, i.e., B ≥ 0 and B = B , the corresponding Laplacian matrix, denoted as L B , is defined as</p><formula xml:id="formula_36">L B = Diag(B1) -B.</formula><p>The number of connected components of B is related to the spectral property of the Laplacian matrix. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>For any affinity matrix</head><formula xml:id="formula_37">B ∈ R n×n , let λ i (L B ), i = 1, • • • , n, be the eigenvalues of L B in the decreasing order, i.e., λ 1 (L B ) ≥ λ 2 (L B ) ≥ • • • ≥ λ n (L B ).</formula><p>It is known that L B 0 and thus λ i (L B ) ≥ 0 for all i. Then, by Theorem 4, B has k connected components if and only if</p><formula xml:id="formula_38">λ i (L B ) &gt; 0, i = 1, • • • , n -k, = 0, i = n -k + 1, • • • , n.<label>(11)</label></formula><p>Motivated by such a property, we define the k-block diagonal regularizer as follows.</p><p>Definition 4. (k-block diagonal regularizer) For any affinity matrix B ∈ R n×n , the k-block diagonal regularizer is defined as the sum of the k smallest eigenvalues of L B , i.e.,</p><formula xml:id="formula_39">B k = n i=n-k+1 λ i (L B ).<label>(12)</label></formula><p>It can be seen that B k = 0 is equivalent to the fact that the affinity matrix B is k-block diagonal. So B k can be regarded as the block diagonal matrix structure induced regularizer.</p><p>It is worth mentioning that ( <ref type="formula" target="#formula_38">11</ref>) is equivalent to rank(L B ) = n -k. One may consider using rank(L B ) as the k-block diagonal regularizer. However, this is not a good choice. The reason is that the number of data points n is usually much larger than the number of clusters k and thus L B is of high rank. It is generally unreasonable to find a high rank matrix by minimizing rank(L B ). More importantly, it is not able to control the targeted number of blocks, which is important in subspace clustering. Another choice is the convex relaxation L B * , but it suffers from the same issues.</p><p>It is interesting that the sparse minimization in the SSC model ( <ref type="formula" target="#formula_5">3</ref>) is equivalent to minimizing L B * . Indeed,</p><formula xml:id="formula_40">L B * = Tr(L B ) =Tr(Diag(B1) -B) = B 1 -diag(B) 1 ,</formula><p>where we use the facts that B = B , B ≥ 0 and L B 0. So, the SSC model ( <ref type="formula" target="#formula_5">3</ref>) is equivalent to</p><formula xml:id="formula_41">min Z L B * s.t. X = XZ, diag(Z) = 0, B = (|Z| + |Z |)/2.</formula><p>This perspective shows that the approximation of the block diagonal matrix by using sparse prior in SSC is loose. In contrast, our proposed k-block diagonal regularizer ( <ref type="formula" target="#formula_39">12</ref>) not only directly encourages the matrix to be block diagonal, but is also able to control the number of blocks, which is important for subspace clustering. A disadvantage is that the k-block diagonal regularizer is nonconvex.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Block Diagonal Representation</head><p>With the proposed k-block diagonal regularizer at hand, we now propose the Block Diagonal Representation (BDR) method for subspace clustering. When considering the noise free case, we may use the following model directly</p><formula xml:id="formula_42">min B B k , s.t. X = XB, B ≥ 0, B = B .</formula><p>It is interesting that whether the obtained solution satisfies the block diagonal property. This can be verified by using the EBD conditions in Definition 3.</p><formula xml:id="formula_43">Theorem 5. Let Ω = {B|B ≥ 0, B = B , B k + 1 &gt; 0}. Then B k satisfies the EBD conditions (1)(2) on Ω.</formula><p>As a corollary of Theorem 3, Theorem 5 implies that the above BDR model owns the block diagonal property when the subspaces are independent. Note that in Theorem 5, there has an additional assumption B k + 1 &gt; 0 which requires B to have at most k connected components (blocks). This is because B k = 0 does not exactly control the number of connected components and the magnitudes of entries of B. If we use B k + λ B 2 , where λ &gt; 0, then the EBD conditions (1)(2) hold without the additional assumption B k + 1 &gt; 0. This is because B 2 takes the magnitudes of entries into account. For the reason details, please refer to the proof of Theorem 5 in the supplementary material.</p><p>To handle the problem with noises, we consider the following BDR model where γ &gt; 0 and we simply require the representation matrix B to be nonnegative and symmetric, which are necessary properties for defining the block diagonal regularizer on B. But the restrictions on B will limit its representation capability. We alleviate this issue by introducing an intermediate term</p><formula xml:id="formula_44">min B 1 2 X -XB 2 + γ B k , s.t. diag(B) = 0, B ≥ 0, B = B ,</formula><formula xml:id="formula_45">min Z,B 1 2 X -XZ 2 + λ 2 Z -B 2 + γ B k , s.t. diag(B) = 0, B ≥ 0, B = B .<label>(13)</label></formula><p>The above two models are equivalent when λ &gt; 0 is sufficiently large. As will be seen in Section 3.3, another benefit of the term λ 2 Z -B 2 is that it makes the subproblems involved in updating Z and B strongly convex and thus the solutions are unique and stable. This also makes the convergence analysis easy.</p><p>Example 1. We give an intuitive example to illustrate the effectiveness of BDR. We generate a data matrix X = [X 1 , X 2 , • • • , X k ] with its columns sampled from k subspaces without noise. We generate k = 5 subspaces {S i } k i=1 whose bases {U i } k i=1 are computed by U i+1 = TU i , 1 ≤ i ≤ k, where T is a random rotation matrix and U 1 ∈ R D×r is a random orthogonal matrix. We set D = 30 and r = 5. For each subpace, we sample n i = 50 data vectors by X i = U i Q i , 1 ≤ i ≤ k, with Q i being an r × n i i.i.d. N (0, 1) matrix. So we have X ∈ R D×n , where n = 250. Each column of X is normalized to have a unit length. We then solve <ref type="bibr" target="#b12">(13)</ref> to achieve Z and B (we set λ = 10 and γ = 3). Note that the generated data matrix X is noise free. So we also compute the shape interaction matrix VV (here V is from the skinny SVD of X = USV ), which is the solution to the LRR model ( <ref type="formula" target="#formula_8">4</ref>), for comparison. We plot VV , Z and B and their binarized versions in Figure <ref type="figure" target="#fig_2">2</ref>. The binarization Ẑ of a matrix Z is defined as</p><formula xml:id="formula_46">Ẑij = 0, if |Z ij | &lt;= τ, 1, otherwise,</formula><p>where we use τ = 10 -3 . From Figure <ref type="figure" target="#fig_2">2</ref>, it can be seen that both VV and its binarized version are very dense and neither of them obeys the block diagonal property. This implies that the generated subspaces are not independent, though the sampled data points are noise free. In contrast, the obtained B by our BDR and its binarized version are not only k-block diagonal but they also obey the block diagonal property (this observation does not depend on the choice of the binarization parameter τ ). This experiment clearly shows the effectiveness of the proposed k-block diagonal regularizer for pursuing a solution obeying the block diagonal property in the case that the independent subspaces assumption is violated. Moreover, we observe that Z is close to but denser than B. From the binarized version, we see that Z is not a k-block diagonal matrix. However, when applying the spectral clustering algorithm on Z and B, we find that both lead to correct clustering while VV does not. This shows the robustness of spectral clustering to the affinity matrix which is not but "close to" k-block diagonal. When γ is relatively smaller, we observe that B may not be k-block diagonal, but it still leads to correct clustering. This shows that, for the subspace clustering problem, the soft block diagonal regularizer is more flexible than the hard constraint in <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Optimization of BDR</head><p>We show how to solve the nonconvex problem <ref type="bibr" target="#b12">(13)</ref>. The key challenge lies in the nonconvex term B k . We introduce an interesting property about the sum of eigenvalues by Ky Fan to reformulate B k .</p><p>Theorem 6. [9, p. 515] Let L ∈ R n×n and L 0.</p><formula xml:id="formula_47">Then n i=n-k+1 λ i (L) = min W L, W , s.t. 0 W I, Tr(W) = k.</formula><p>Then, we can reformulate B k as a convex program</p><formula xml:id="formula_48">B k = min W L B , W , s.t. 0 W I, Tr(W) = k. So (13) is equivalent to min Z,B,W 1 2 X -XZ 2 + λ 2 Z -B 2 + γ Diag(B1) -B, W s.t. diag(B) = 0, B ≥ 0, B = B , (<label>14</label></formula><formula xml:id="formula_49">) 0 W I, Tr(W) = k.</formula><p>There are 3 blocks of variables in problem <ref type="bibr" target="#b13">(14)</ref>. We observe that W is independent from Z, thus we can group them Algorithm 1 Solve ( <ref type="formula" target="#formula_48">14</ref>) by Alternating Minimization Input: X ∈ R d×n , λ &gt; 0, γ &gt; 0, &gt; 0. Initialization: t = 0, W t = 0, Z t = 0, B t = 0. while not converged do 1) Compute W t+1 by solving (15); 2) Compute Z t+1 by solving (16); 3) Compute B t+1 by solving (17); 4)</p><formula xml:id="formula_50">If max{ Z t+1 -Z t ∞ , B t+1 -B t ∞ } ≤ , break; 5) t = t + 1. end while Output: Z, B ∈ R n×n .</formula><p>as a super block {W, Z} and treat {B} as the other block. Then ( <ref type="formula" target="#formula_48">14</ref>) can be solved by alternating updating {W, Z} and {B}.</p><p>First, fix B = B t , and update {W t+1 , Z t+1 } by</p><formula xml:id="formula_51">{W t+1 , Z t+1 } = arg min W,Z 1 2 X -XZ 2 + λ 2 Z -B 2 + γ Diag(B1) -B, W s.t. 0 W I, Tr(W) = k.</formula><p>This is equivalent to updating W t+1 and Z t+1 separably by</p><formula xml:id="formula_52">W t+1 = arg min W Diag(B1) -B, W , s.t. 0 W I, Tr(W) = k,<label>(15)</label></formula><p>and</p><formula xml:id="formula_53">Z t+1 = argmin Z 1 2 X -XZ 2 + λ 2 Z -B 2 . (<label>16</label></formula><formula xml:id="formula_54">)</formula><p>Second, fix W = W t+1 and Z = Z t+1 , and update B by</p><formula xml:id="formula_55">B t+1 = arg min B λ 2 Z -B 2 + γ Diag(B1) -B, W s.t. diag(B) = 0, B ≥ 0, B = B . (<label>17</label></formula><formula xml:id="formula_56">)</formula><p>Note that the above three subproblems are convex and have closed form solutions. For <ref type="bibr" target="#b14">(15)</ref>, W t+1 = UU , where U ∈ R n×k consist of k eigenvectors associated with the k smallest eigenvalues of Diag(B1) -B. For ( <ref type="formula" target="#formula_53">16</ref>), it is obvious that</p><formula xml:id="formula_57">Z t+1 = (X X + λI) -1 (X X + λB).<label>(18)</label></formula><p>For <ref type="bibr" target="#b16">(17)</ref>, it is equivalent to</p><formula xml:id="formula_58">B t+1 = arg min B 1 2 B -Z + γ λ (diag(W)1 -W) 2 s.t. diag(B) = 0, B ≥ 0, B = B . (<label>19</label></formula><formula xml:id="formula_59">)</formula><p>This problem has a closed form solution given as follows.</p><p>Proposition 7. Let A ∈ R n×n . Define Â = A -Diag(diag(A)). Then the solution to the following problem</p><formula xml:id="formula_60">min B 1 2 B -A 2 , s.t. diag(B) = 0, B ≥ 0, B = B , (<label>20</label></formula><formula xml:id="formula_61">)</formula><p>is given by</p><formula xml:id="formula_62">B * = ( Â + Â )/2 + .</formula><p>The whole procedure of the alternating minimization solver for ( <ref type="formula" target="#formula_48">14</ref>) is given in Algorithm 1. We denote the objective of ( <ref type="formula" target="#formula_48">14</ref>) as f (Z, B, W). Let S 1 = {B|diag(B) = 0, B ≥ 0, B = B } and S 2 = {W|0 W I, Tr(W) = k}. Denote the indicator functions of S 1 and S 2 as ι S1 (B) and ι S2 (W), respectively. We give the convergence guarantee of Algorithm 1 for nonconvex BDR problem. Proposition 8. The sequence {W t , Z t , B t } generated by Algorithm 1 has the following properties:</p><p>(1) The objective f (Z t , B t , W t ) + ι S1 (B t ) + ι S2 (W t ) is monotonically decreasing. Indeed,</p><formula xml:id="formula_63">f (Z t+1 , B t+1 , W t+1 ) + ι S1 (B t+1 ) + ι S2 (W t+1 ) ≤f (Z t , B t , W t ) + ι S1 (B t ) + ι S2 (W t ) - λ 2 Z t+1 -Z t 2 - λ 2 B t+1 -B t 2 ;</formula><p>(2) Z t+1 -Z t → 0, B t+1 -B t → 0 and W t+1 -W t → 0;</p><p>(3) The sequences {Z t }, {B t } and {W t } are bounded.</p><p>Theorem 7. The sequence {W t , Z t , B t } generated by Algorithm 1 has at least one limit point and any limit point <ref type="bibr" target="#b13">(14)</ref>.</p><formula xml:id="formula_64">(Z * , B * , W * ) of {Z t , B t , W t } is a stationary point of</formula><p>Please refer to the supplementary material for the proof of the above theorem. Generally, our proposed solver in Algorithm 1 for the nonconvex BDR model is simple. The convergence guarantee in Theorem 7 for Algorithm 1 is practical as there have no unverifiable assumptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Subspace Clustering Algorithm</head><p>We give the procedure of BDR for subspace clustering as previous works <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b27">[28]</ref>. Given the data matrix X, we obtain the representation matrix Z and B by solving the proposed BDR problem (13) by Algorithm 1. Both of them can be used to infer the data clustering. The affinity matrix can be defined as W = (|Z| + |Z |)/2 or W = (|B| + |B |)/2, and then the traditional spectral clustering <ref type="bibr" target="#b32">[33]</ref> is applied on W to group the data points into k groups. As will be seen in the experiments, the clustering performance on Z and B is comparable.</p><p>It is worth mentioning that our BDR requires to know the number of subspaces k when computing the affinity matrix and using the spectral clustering to achieve the final result. Such a requirement is necessary for all the spectral-type subspace clustering methods, e.g., <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b27">[28]</ref>, though it is only used in the spectral clustering step. If the number of subspaces is not known, some other techniques can be used for the estimation, e.g., <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b20">[21]</ref>. This work only focuses on the case that the number of subspaces is known.</p><p>We would like to emphasize some differences between our BDR and <ref type="bibr" target="#b10">[11]</ref>. The proposed subspace clustering method in <ref type="bibr" target="#b10">[11]</ref> uses a hard constraint to enforce the solution to be exactly k-block diagonal. However, for correct clustering, the exact k-block diagonal solution is not necessary. The best clustering results may be obtain by balancing the representation loss and the number of blocks of the solution. Our proposed soft block diagonal regularizer is more flexible to control the balance between the representation loss and the number of blocks of the solution by tuning the parameters k and γ in <ref type="bibr" target="#b12">(13)</ref>. Furthermore, the proposed stochastic subgradient method for nonconvex optimization in <ref type="bibr" target="#b10">[11]</ref> is generally very slow and its convergence guarantee requires restrictive assumptions on the input data. This makes their method not practical. In contrast, our solver for BDR owns stronger convergence guarantee without any restrictive assumptions. It will be seen from the experimental results that our method is very efficient in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we conduct several experiments on real datasets to demonstrate the effectiveness of our BDR. The compared methods include SCC <ref type="bibr" target="#b4">[5]</ref>, SSC <ref type="bibr" target="#b9">[10]</ref>, LRR <ref type="bibr" target="#b20">[21]</ref>, LSR <ref type="bibr" target="#b27">[28]</ref>, S 3 C [20], BDR-B (our BDR model by using B) and BDR-Z (our BDR model by using Z). For the existing methods, we use the codes released by the authors. We test on three datasets: Hopkins 155 database <ref type="bibr" target="#b36">[37]</ref> for motion segmentation, Extended Yale B <ref type="bibr" target="#b18">[19]</ref> for face clustering and MNIST <ref type="bibr" target="#b12">[13]</ref> for handwritten digit clustering. For all the compared methods, we tune the parameters (for some methods, we use the parameters which are given in their codes for some datasets) and use the ones which achieve the best results in most cases for each dataset. Note that BDR-B and BDR-Z use the same parameters 3 . In Algorithm 1, we set = 10 -3 . For the performance evaluation, we use the usual clustering error defined as follows</p><formula xml:id="formula_65">clustering error = 1 - 1 n n i=1 δ(p i , map(q i )),<label>(21)</label></formula><p>where p i and q i represent the output label and the ground truth one of the i-th point respectively, δ(x, y) = 1 if x = y, and δ(x, y) = 0 otherwise, and map(q i ) is the best mapping function that permutes clustering labels to match the ground truth labels. All experiments are conducted on a PC with an 3. We will release the codes of our BDR and the used datasets soon.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Motion Segmentation</head><p>We consider the application of subspace clustering for motion segmentation. It refers to the problem of segmenting a video sequence with multiple rigidly moving objects into multiple spatiotemporal regions that correspond to the different motions in the scene. The coordinates of the points in trajectories of one moving object form a low dimensional subspace. Thus, the motion segmentation problem can be solved via performing subspace clustering on the trajectory spatial coordinates. We test on the widely used Hopkins 155 database <ref type="bibr" target="#b36">[37]</ref>. It consists of 155 video sequences, where 120 of the videos have two motions and 35 of the videos have three motions. The feature trajectories of each video can be well modeled as data points that approximately lie in a union of linear subspaces of dimension at most 4 <ref type="bibr" target="#b9">[10]</ref>. Each sequence is a sole dataset (i.e., data matrix X) and so there are in total 155 subspace clustering tasks. We consider two settings to construct the data matrix X for each sequence: (1) use the original 2F -dimensional feature trajectories, where F is the number of frames of the video sequence; (2) project the data matrix into 4kdimensional subspace, where k is the number of subspaces, by using PCA. Most of the compared methods are spectraltype methods, except SCC. For spectral-type methods, they used different post-processing on the learned affinity matrices when using spectral clustering. We first consider the same setting as <ref type="bibr" target="#b9">[10]</ref> which defines the affinity matrix by W = (|Z| + |Z |)/2, where Z is the learned representation coefficient matrix, and no additional complex postprocessing is performed. In the Hopkins 155 database, there are 120 videos of two motions and 35 videos of three motions. So we report the mean and median of the clustering errors of these videos. Table <ref type="table" target="#tab_2">2</ref> and Table <ref type="table">3</ref> report the clustering errors of applying the compared methods on the dataset when we use the original 2F -dimensional feature trajectories and when we project the data into a 4k-dimensional subspace  using PCA, respectively. Figure <ref type="figure" target="#fig_3">3</ref> gives the percentage of sequences for which the clustering error is less than or equal to a given percentage of misclassification. Furthermore, consider that many subspace clustering methods achieve state-of-the-art performance on the Hopkins 155 database by using different techniques for pre-processing and postprocessing. So we give a direct performance comparison of the subspace clustering methods with their reported settings on all 155 sequences in Table <ref type="table" target="#tab_3">4</ref>. Based on these results, we have the following observations:</p><p>• From Table <ref type="table" target="#tab_2">2</ref> and Table <ref type="table">3</ref>, it can be seen that our BDR-B and BDR-Z achieve close performance and both outperform the existing methods in both settings, though many existing methods already perform very well. Considering that the reported results are the means of the clustering errors of many sequences, the improvements (from the existing best result 2.18% to our 0.93% in Table <ref type="table" target="#tab_2">2</ref> and from the existing best result 2.41% to our 1.08% in Table <ref type="table">3</ref>) by our BDR-B and BDR-Z are significant. • From Figure <ref type="figure" target="#fig_3">3</ref>, it can be seen that there are many more sequences which are almost correctly segmented by our BDR-B abd BDR-Z than existing methods. This demonstrates that the improvements over existing methods by our methods are achieved on most of the sequences. • For most methods, the clustering performance using the 2F -dimensional feature trajectories in Table <ref type="table" target="#tab_2">2</ref> is slightly better than using the 4k-dimensional PCA projections in Table <ref type="table">3</ref>. This implies that the feature trajectories of k motions in a video almost perfectly lie in a 4k-dimensional linear subspace of the 2F -dimensional ambient space. • From Table <ref type="table" target="#tab_3">4</ref>, it can be seen that our BDR-Z performed on the 2F -dimensional data points still outperforms many existing state-of-the-art methods which use various post-processing techniques. LatLRR <ref type="bibr" target="#b22">[23]</ref> is slightly better than our method. But it requires much more complex pre-processing and post-processing, and much higher computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Face Clustering</head><p>We consider the face clustering problem, where the goal is to group the face images into clusters according to their subjects. It is known that, under the Lambertian assumption, the face images of a subject with a fixed pose and varying illumination approximately lie in a linear subspace of dimension 9 <ref type="bibr" target="#b1">[2]</ref>. So, a collection of face images of k subjects approximately lie in a union of 9-dimensional subspaces. Therefore the face clustering problem can be solved by using subspace clustering methods. We test on the Extended Yale B database <ref type="bibr" target="#b18">[19]</ref>. This dataset consists of 2,414 frontal face images of 38 subjects under 9 poses and 64 illumination conditions. For each subject, there are 64 images. Each cropped face image consists of 192×168 pixels. To reduce the computation and memory cost, we downsample each image to 32 × 32 pixels and vectorize it to a 1,024 vector as a data point. Each data point is normalized to have a unit length. We then construct the data matrix X from subsets which consist of different numbers of subjects k ∈ {2, 3, 5, 8, 10} from the Extended Yale B database. For each k, we randomly sample k subjects face images from all 38 subjects to construct the data matrix X ∈ R D×n , where D = 1024 and n = 64k. Then the subspace clustering methods can be performed on X and the clustering error is recorded. We run 20 trials and the mean, median, and standard variance of clustering errors are reported.  The clustering errors by different subspace clustering methods on the Extended Yale B database are shown in Table <ref type="table" target="#tab_4">5</ref>. It can be seen that our BDR-B and BDR-Z achieve similar performance and both outperform other methods in most cases. Generally, when the number of subjects (or subspaces) increases, the clustering problem is more challenging. We find that the improvements by our methods are more significant when the number of subjects increases. This experiment clearly demonstrates the effectiveness of our BDR for the challenging face clustering task on the Extended Yale B database. S 3 C [20] is an improved SSC method and it also performs well in some cases. However, it needs to tune more parameters in order to achieve comparable performance and it is time consuming. Figure <ref type="figure" target="#fig_1">4</ref> provides the average computational time of each method as a function of the number of subjects. It can be seen that S 3 C has the most highest computational time, while LSR, which has a closed form solution, is the most efficient method. Our BDR-B (BDR-Z has as similar running time and thus it is not reported) is faster than most methods except LSR (LSR is much faster than BDR). So our BDR is a good choice when considering the trade-off between the performance and computational cost. Furthermore, we consider the influence of the parameters λ and γ on the clustering performance. On It can be seen that the clustering error increases when λ and γ are relatively too small or too large. In the "too small" case, the performance degeneration is due to the relatively weak regularization effect. On the other hand, if λ and γ are relatively large, Z and B in the early iterations are not discriminative due to relatively large reconstruction loss. This issue may accumulate till the algorithm converges due to the nonconvexity of the problem and the non-optimal solution guarantee issue of our solver.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Handwritten Digit Clustering</head><p>We consider the application of subspace clustering for clustering images of handwritten digits which also have the subspace structure of dimension 12 <ref type="bibr" target="#b12">[13]</ref>. We test on the MNIST database <ref type="bibr" target="#b17">[18]</ref>, which contains grey scale images of handwritten digits 0 ∼ 9. There are 10 subjects of digits. We consider the clustering problems with the number of subjects k varying from 2 to 10. For each k, we run the experiments for 20 trials and report the mean clustering error. For each trial and each k, we consider random k subjects of digits from 0 ∼ 9, and each subject has 100 randomly sampled images. Each grey image is of size 28×28 and is vectorized as a vector of length 784. Each data point is normalized to have a unit length. So for each k, we have the data matrix of size 784 × 100k. Figure <ref type="figure" target="#fig_6">6</ref> (a) plots the clustering errors as a function of the number of subjects on the MNIST database. It can be seen that our BDR-B and BDR-Z achieve the smallest clustering errors in most cases, though the improvements over the best compared method are different on different numbers of subjects. Figure <ref type="figure" target="#fig_6">6</ref> (b) gives a comparison on the average running time and it can be seen that our BDR-B (similar to BDR-Z) is much more efficient than most methods except LSR. The clustering performance of SSC and S 3 C is close to our BDR-B in some cases, but their computational cost is much higher than ours. So this experiment demonstrates the effectiveness and high-efficiency of our BDR. Figure <ref type="figure" target="#fig_6">6</ref> (c) plots an example of the affinity matrix B obtained by BDR on a 4 subjects clustering task. By direct computation, we observe that B k = 0, though B does not satisfy the block diagonal property. It still leads to a good performance as it is close to k-block diagonal. Figure <ref type="figure" target="#fig_7">7</ref> plots the clustering errors as a function of the input parameter k in model ( <ref type="formula" target="#formula_45">13</ref>) on the subproblems with 2, 4, 6 and 8 subjects from the MNIST database. It can be seen that our BDR model achieves the best performance when k is set to the ground truth of the subject number in most cases. The clustering errors are relatively larger when the difference of k and the ground truth number of subjects is larger. If k is relatively large, the errors increase more significantly since B is k-block diagonal and thus it tends to be "too sparse". Furthermore, to verify our theoretical convergence results, we plot the objective function value of <ref type="bibr" target="#b13">(14)</ref> in each iteration obtained in Algorithm 1 for all iterations on a 5 subjects subset of the MNIST database in Figure <ref type="figure" target="#fig_8">8</ref>. It can be seen that the objective function value is monotonically decreasing and this phenomenon is consistent with our convergence analysis in Proposition 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION AND FUTURE WORKS</head><p>This paper studies the subspace clustering problem which aims to group the data points approximately drawn from a union of k subspaces into k clusters corresponding to their underlying subspaces. We observe that many existing spectral-type subspace clustering methods own the same block diagonal property under certain subspace assumption. We consider a general problem and show that if the objective satisfies the proposed Enforced Block Diagonal (EBD) conditions or its solution is unique, then the solution(s) obey the block diagonal property. This unified view provides insights into the relationship among the block diagonal property of the solution and the used objectives, as well as to facilitate the design of new algorithms. Inspired by the block diagonal property, we propose the first k-block diagonal regularizer which is useful for encouraging the matrix to be k-block diagonal. This leads to the Block Diagonal Representation (BDR) method for subspace clustering. A disadvantage of the BDR model is that it is nonconvex due to the k-block diagonal regularizer. We propose to solve the BDR model by a simple and generally efficient method and more importantly we provide the convergence guarantee without restrictive assumptions. Numerical experiments well demonstrate the effectiveness of our BDR.</p><p>There are many potential interesting future works: 1. The problem of the affinity matrix construction is not limited to the subspace clustering (or spectral clustering), but is everywhere and appears in many applications, e.g., <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b34">[35]</ref>. The proposed k-block diagonal regularizer provides a new learning way and it is natural to consider the extension to related applications. 2. Beyond the sparse vector and low-rank matrix, the block diagonal matrix is another interesting structure of structured sparsity. The sparsity of the sparse vector is defined on the entries while the sparsity of the low-rank matrix is defined on the singular values. For the block diagonal matrix, its sparsity can be defined on the eigenvalues of the Laplacian matrix. So we can say that a block diagonal affinity matrix is spectral sparse if there have many connected blocks. This perspective motivates us to consider the statistical recovery guarantee of the block diagonal matrix regularized or constrained problems as that in compressive sensing. 3. The proposed k-block diagonal regularizer is nonconvex and this makes the optimization of the problem with such a regularizer challenging. Our proposed solver and convergence guarantee are specific for the nonconstrained BDR problem. How to solve more complicated problems (e.g., using the 1 -norm to control the reconstruction error to model the outliers) and provide the convergence guarantee is interesting. The general Alternating Direction Method of Multipliers <ref type="bibr" target="#b24">[25]</ref> is a potential solver.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Illustrations of three interesting structures of matrix: sparse, low-rank and block diagonal matrices. The first two are extensively studied before. This work focuses on the pursuit of block diagonal matrix.</figDesc><graphic coords="4,472.21,46.53,73.36,73.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Theorem 4 .</head><label>4</label><figDesc><ref type="bibr" target="#b39">[40,</ref> Proposition 4]  For any B ≥ 0, B = B , the multiplicity k of the eigenvalue 0 of the corresponding Laplacian matrix L B equals the number of connected components (blocks) in B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Plots of the shape interaction matrix VV , Z and B from BDR and their binarized versions respectively for Example 1.</figDesc><graphic coords="8,175.82,284.31,102.04,102.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Percentage of sequences for which the clustering error is less than or equal to a given percentage of misclassification. Left: 2F -dimensional data. Right: 4n-dimensional data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Average computational time (sec.) of the algorithms on the Extended Yale B database as a function of the number of subjects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Clustering error (%) of BDR-Z as a function of λ when fixing γ = 1 (left) and γ when fixing λ = 50 (right) for the 10 subjects problems from the Extended Yale B database.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Results on the MNIST database. (a) Plots of clustering errors v.s. the number of subjects; (b) Plots of average computational time (sec.) v.s. the number of subjects; (c) An example of the affinity matrix B obtained by our BDR model.</figDesc><graphic coords="12,219.62,47.70,64.81,64.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Plots of clustering errors v.s. the parameter k in model (13) on the subsets with 2, 4, 6 and 8 subjects from the MNIST database.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Plots of the objective function value of (14) v.s. iterations on a 5 subjects subset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 :</head><label>1</label><figDesc>A summary of existing spectral-type subspace clustering methods based on different choices of f and Ω.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2 :</head><label>2</label><figDesc>Clustering errors (%) of different algorithms on the Hopkins 155 database with the 2F -dimensional data points.</figDesc><table><row><cell>method</cell><cell>SCC</cell><cell cols="6">SSC LRR LSR S 3 C BDR-B BDR-Z</cell></row><row><cell>2 motions</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mean</cell><cell>2.46</cell><cell>1.52</cell><cell>3.65</cell><cell>3.24</cell><cell>1.73</cell><cell>1.00</cell><cell>0.95</cell></row><row><cell>median</cell><cell>0.00</cell><cell>0.00</cell><cell>0.22</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>3 motions</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mean</cell><cell cols="2">11.00 4.40</cell><cell>9.40</cell><cell>5.94</cell><cell>4.76</cell><cell>1.95</cell><cell>0.85</cell></row><row><cell>median</cell><cell>1.63</cell><cell>1.63</cell><cell>3.99</cell><cell>2.05</cell><cell>0.93</cell><cell>0.21</cell><cell>0.21</cell></row><row><cell>All</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mean</cell><cell>4.39</cell><cell>2.18</cell><cell>4.95</cell><cell>3.85</cell><cell>2.41</cell><cell>1.22</cell><cell>0.93</cell></row><row><cell>median</cell><cell>0.00</cell><cell>0.00</cell><cell>0.53</cell><cell>0.45</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell cols="8">TABLE 3: Clustering errors (%) of different algorithms on the</cell></row><row><cell cols="8">Hopkins 155 database with the 4k-dimensional data points by</cell></row><row><cell cols="2">applying PCA.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>method</cell><cell>SCC</cell><cell cols="6">SSC LRR LSR S 3 C BDR-B BDR-Z</cell></row><row><cell>2 motions</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mean</cell><cell>3.58</cell><cell>1.83</cell><cell>4.22</cell><cell>3.35</cell><cell>1.81</cell><cell>1.26</cell><cell>1.04</cell></row><row><cell>median</cell><cell>0.00</cell><cell>0.00</cell><cell>0.29</cell><cell>0.29</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>3 motions</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mean</cell><cell>7.11</cell><cell>4.40</cell><cell>9.43</cell><cell>6.13</cell><cell>5.01</cell><cell>1.22</cell><cell>1.22</cell></row><row><cell>median</cell><cell>0.47</cell><cell>0.56</cell><cell>3.70</cell><cell>2.05</cell><cell>1.06</cell><cell>0.21</cell><cell>0.20</cell></row><row><cell>All</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mean</cell><cell>4.37</cell><cell>2.41</cell><cell>5.40</cell><cell>3.97</cell><cell>2.53</cell><cell>1.25</cell><cell>1.08</cell></row><row><cell>median</cell><cell cols="2">00.00 0.00</cell><cell>0.53</cell><cell>0.53</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 4 :</head><label>4</label><figDesc>The mean clustering errors (%) of 155 sequences on Hopkins 155 dataset by state-of-the-art methods.</figDesc><table><row><cell>LSA [42]</cell><cell>SSC [10]</cell><cell>LRR [21]</cell><cell>LatLRR [23]</cell><cell>LSR [28]</cell></row><row><cell>4.52</cell><cell>2.18</cell><cell>1.59</cell><cell>0.85</cell><cell>1.71</cell></row><row><cell cols="4">CASS [24] SMR [14] BD-SSC [11] BD-LRR [11]</cell><cell>BDR-Z</cell></row><row><cell>1.47</cell><cell>1.13</cell><cell>1.68</cell><cell>0.97</cell><cell>0.93</cell></row><row><cell cols="5">Intel(R) Xeon(R) CPU E5640 at 2.67GHz and 2.66 GHz, 24G</cell></row><row><cell cols="4">memory, running Windows 7 and Matlab 2016a.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 5 :</head><label>5</label><figDesc>Clustering error (%) of different algorithms on the Extended Yale B database.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">2 subjects</cell><cell></cell><cell cols="2">3 subjects</cell><cell></cell><cell></cell><cell>5 subjects</cell><cell></cell><cell></cell><cell>8 subjects</cell><cell></cell><cell></cell><cell>10 subjects</cell></row><row><cell cols="5">method mean median</cell><cell>std</cell><cell cols="2">mean median</cell><cell>std</cell><cell cols="2">mean median</cell><cell>std</cell><cell cols="2">mean median</cell><cell>std</cell><cell cols="2">mean median</cell><cell>std</cell></row><row><cell>SCC</cell><cell cols="2">24.02</cell><cell></cell><cell>19.92</cell><cell>17.82</cell><cell>42.19</cell><cell>41.93</cell><cell>8.93</cell><cell>61.36</cell><cell>62.34</cell><cell>6.10</cell><cell>71.87</cell><cell>72.27</cell><cell>4.72</cell><cell>72.48</cell><cell>73.28</cell><cell>6.14</cell></row><row><cell>SSC</cell><cell></cell><cell>1.64</cell><cell></cell><cell>0.78</cell><cell>2.91</cell><cell>3.26</cell><cell>0.52</cell><cell>7.69</cell><cell>6.30</cell><cell>4.22</cell><cell>5.43</cell><cell>8.94</cell><cell>9.67</cell><cell>6.18</cell><cell>10.09</cell><cell>11.33</cell><cell>4.59</cell></row><row><cell>LRR</cell><cell></cell><cell>5.39</cell><cell></cell><cell>0.39</cell><cell>14.50</cell><cell>6.04</cell><cell>1.04</cell><cell>12.34</cell><cell>8.13</cell><cell>2.34</cell><cell>9.61</cell><cell>6.79</cell><cell>3.42</cell><cell>6.50</cell><cell>9.49</cell><cell>12.58</cell><cell>5.38</cell></row><row><cell>LSR</cell><cell></cell><cell>3.16</cell><cell></cell><cell>0.78</cell><cell>10.18</cell><cell>3.96</cell><cell>1.56</cell><cell>8.72</cell><cell>7.85</cell><cell>6.72</cell><cell>8.72</cell><cell>28.14</cell><cell>31.05</cell><cell>12.32</cell><cell>33.27</cell><cell>33.12</cell><cell>4.57</cell></row><row><cell>S 3 C</cell><cell></cell><cell>1.29</cell><cell></cell><cell>0.00</cell><cell>2.69</cell><cell>2.79</cell><cell>0.52</cell><cell>7.38</cell><cell>4.66</cell><cell>1.88</cell><cell>5.15</cell><cell>6.37</cell><cell>6.35</cell><cell>5.32</cell><cell>6.87</cell><cell>6.17</cell><cell>3.67</cell></row><row><cell>BDR-B</cell><cell></cell><cell>3.28</cell><cell></cell><cell>0.78</cell><cell>10.15</cell><cell>3.02</cell><cell>1.30</cell><cell>7.78</cell><cell>4.45</cell><cell>2.19</cell><cell>6.29</cell><cell>3.08</cell><cell>2.93</cell><cell>1.18</cell><cell>2.95</cell><cell>2.81</cell><cell>1.09</cell></row><row><cell>BDR-Z</cell><cell></cell><cell>2.97</cell><cell></cell><cell>0.00</cell><cell>10.23</cell><cell>1.15</cell><cell>1.04</cell><cell>0.95</cell><cell>3.00</cell><cell>2.66</cell><cell>2.25</cell><cell>4.46</cell><cell>4.20</cell><cell>2.39</cell><cell>4.04</cell><cell>3.52</cell><cell>1.52</cell></row><row><cell></cell><cell>Computational time (sec.)</cell><cell>0 500 1000 1500</cell><cell>2</cell><cell>3 SCC SSC LRR LSR S 3 C BDR-B</cell><cell>5</cell><cell>8</cell><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Number of subjects</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Part of this work is extended from our conference paper<ref type="bibr" target="#b27">[28]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>Ω (resp. ∆) is not specified if there has no restriction on Z (resp. X).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>For the parameters, λ &gt; 0, λ ij &gt; 0, p ij ≥ 0.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Identification of switched linear systems via sparse optimization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bako</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="668" to="677" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Lambertian reflectance and linear subspaces</title>
		<author>
			<persName><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Recognition and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="218" to="233" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">k-plane clustering</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">L</forename><surname>Mangasarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Global Optimization</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="32" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Object segmentation by long term analysis of point trajectories</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision</title>
		<meeting>European Conf. Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="282" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Spectral curvature clustering (SCC)</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l J. Computer Vision</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="317" to="330" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Oracle based active set algorithm for scalable elastic net subspace clustering</title>
		<author>
			<persName><forename type="first">Chong</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun-Guang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robinson</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">P</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">René</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3928" to="3937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scalable sparse subspace clustering by orthogonal matching pursuit</title>
		<author>
			<persName><forename type="first">Chong</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robinson</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">René</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3918" to="3927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A multibody factorization method for independently moving objects</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Costeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l J. Computer Vision</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="159" to="179" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Convex optimization &amp; euclidean distance geometry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dattorro</surname></persName>
		</author>
		<ptr target="http://meboo.convexoptimization.com/Meboo.html" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sparse subspace clustering: Algorithm, theory, and applications</title>
		<author>
			<persName><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Recognition and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2765" to="2781" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robust subspace segmentation with block-diagonal prior</title>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3818" to="3825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multibody grouping from motion images</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Gear</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l J. Computer Vision</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="133" to="150" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Metrics and models for handwritten character recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y</forename><surname>Simard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<imprint>
			<biblScope unit="page" from="54" to="65" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Smooth representation clustering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3834" to="3841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Clustering partially observed graphs via convex optimization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jalali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sanghavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. Machine Learning</title>
		<meeting>Int&apos;l Conf. Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1001" to="1008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Motion segmentation by subspace separation and model selection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kanatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Computer Vision</title>
		<meeting>IEEE Int&apos;l Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="586" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient k-support matrix pursuit</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision</title>
		<meeting>European Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Acquiring linear subspaces for face recognition under variable lighting</title>
		<author>
			<persName><forename type="first">K.-C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Recognition and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="684" to="698" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Structured sparse subspace clustering: a unified optimization framework</title>
		<author>
			<persName><forename type="first">C.-G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="277" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robust recovery of subspace structures by low-rank representation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Recognition and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="171" to="184" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Robust subspace segmentation by lowrank representation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. Machine Learning</title>
		<meeting>Int&apos;l Conf. Machine Learning</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="663" to="670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Latent low-rank representation for subspace segmentation and feature extraction</title>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Computer Vision</title>
		<meeting>IEEE Int&apos;l Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1615" to="1622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Correlation adaptive subspace segmentation by trace Lasso</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Computer Vision</title>
		<meeting>IEEE Int&apos;l Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1345" to="1352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A unified alternating direction method of multipliers by majorization minimization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Recognition and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page">99</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>PP</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Correntropy induced L2 graph for robust subspace clustering</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Computer Vision</title>
		<meeting>IEEE Int&apos;l Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Nonconvex nonsmooth low rank minimization via iteratively reweighted nuclear norm</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="829" to="839" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Robust and efficient subspace segmentation via least squares regression</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision</title>
		<meeting>European Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-subspace representation and discovery</title>
		<author>
			<persName><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conf. Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">6912</biblScope>
			<biblScope unit="page" from="405" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Estimation of subspace arrangements with applications in modeling and segmenting mixed data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Derksen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fossum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Rev</title>
		<imprint>
			<biblScope unit="page">50</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Optimization with first-order surrogate functions</title>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. Machine Learning</title>
		<meeting>Int&apos;l Conf. Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="783" to="791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Graph connectivity in sparse subspace clustering</title>
		<author>
			<persName><forename type="first">B</forename><surname>Nasihatkon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2137" to="2144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Robust low-rank subspace segmentation with semidefinite guarantees</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-F</forename><surname>Cheong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Conf. Data Mining Workshops</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1179" to="1188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Recognition and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A geometric analysis of subspace clustering with outliers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Soltanolkotabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="page" from="2195" to="2238" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A benchmark for the comparison of 3-D motion segmentation algorithms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Nearest q-flat to m points</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Optimization Theory and Applications</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="249" to="252" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Generalized principal component analysis (GPCA)</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Recognition and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1945" to="1959" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName><forename type="first">U</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Von</forename><surname>Luxburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Efficient subspace segmentation via quadratic programming</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Artificial Intelligence</title>
		<meeting>AAAI Conf. Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="519" to="524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A general framework for motion segmentation: Independent, articulated, rigid, non-rigid, degenerate and non-degenerate</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision</title>
		<meeting>European Conf. Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="94" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Graph embedding and extensions: a general framework for dimensionality reduction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Recognition and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="40" to="51" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fawaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Montanari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1208.1544</idno>
		<title level="m">Guess who rated this movie: Identifying users through subspace clustering</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Introduction to semi-supervised learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Artificial Intelligence and Machine Learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="130" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
