<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AutoML-GPT: Automatic Machine Learning with GPT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-05-04">4 May 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shujian</forename><surname>Zhang</surname></persName>
							<email>szhang19@utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chengyue</forename><surname>Gong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lemeng</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xingchao</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mingyuan</forename><surname>Zhou</surname></persName>
							<email>mzhou@utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">AutoML-GPT: Automatic Machine Learning with GPT</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-05-04">4 May 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2305.02499v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>AI tasks encompass a wide range of domains and fields. While numerous AI models have been designed</p><p>for specific tasks and applications, they often require considerable human efforts in finding the right model architecture, optimization algorithm, and hyperparameters. Recent advances in large language models (LLMs) like ChatGPT show remarkable capabilities in various aspects of reasoning, comprehension, and interaction. Consequently, we propose developing task-oriented prompts and automatically utilizing LLMs to automate the training pipeline. To implement this concept, we present the AutoML-GPT, which employs GPT as the bridge to diverse AI models and dynamically trains models with optimized hyperparameters. AutoML-GPT dynamically takes user requests from the model and data cards and composes the corresponding prompt paragraph. Ultimately, with this prompt paragraph, AutoML-GPT will automatically conduct the experiments from data processing to model architecture, hyperparameter tuning, and predicted training log. By leveraging AutoML-GPT's robust language capabilities and the available AI models, AutoML-GPT can tackle numerous intricate AI tasks across various tasks and datasets. This approach achieves remarkable results in computer vision, natural language processing, and other challenging areas. Extensive experiments and ablation studies demonstrate that our method can be general, effective, and beneficial for many AI tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Artificial intelligence (AI) has experienced significant advancements recently. Among these developments, ChatGPT <ref type="bibr" target="#b14">[OpenAI, 2023]</ref> has particularly stood out due to its ability to reason, comprehend, and interact <ref type="bibr" target="#b28">[Wu et al., 2023]</ref>. The ability to execute new tasks based on instructions is a crucial step towards achieving artificial general intelligence, and the remarkable capabilities of large language models (LLMs) have spurred numerous emerging research topics, such as in-context learning <ref type="bibr" target="#b20">[Ram et al., 2023;</ref><ref type="bibr" target="#b30">Xie et al., 2021]</ref>, chain-ofthought prompting <ref type="bibr" target="#b16">[Pilault et al., 2023;</ref><ref type="bibr">Wei et al., 2022b]</ref>, retrieve and read <ref type="bibr" target="#b8">[Izacard and Grave, 2020;</ref><ref type="bibr" target="#b32">Zhang et al., 2021</ref><ref type="bibr" target="#b33">Zhang et al., , 2022]]</ref>, and GPT-based intelligent systems <ref type="bibr" target="#b34">[Zheng et al., 2023]</ref>. These areas aim to explore the vast potential of LLMs and present boundless opportunities for constructing sophisticated AI systems.</p><p>LLMs, such as GPT-4 <ref type="bibr" target="#b0">[Brown et al., 2020;</ref><ref type="bibr" target="#b14">OpenAI, 2023]</ref>, LLaMA <ref type="bibr">[Touvron et al., 2023]</ref>, Flan-T5 <ref type="bibr" target="#b1">[Chung et al., 2022]</ref>, and PaLM <ref type="bibr" target="#b1">[Chowdhery et al., 2022]</ref>, have demonstrated a deep comprehension of natural language and the capacity to produce coherent, contextually appropriate responses. This progress has opened up new potential applications for challenging tasks involving different domain data, such as image and text processing, as well as the incorporation of domain-specific knowledge. In this context, LLMs play a crucial role, as their capacity to comprehend and produce natural language allows AI to better understand and tackle a wide range of challenges.</p><p>In this paper, we aim to develop an Automatic Machine Learning (AutoML) system called AutoML-GPT, which utilizes LLMs to automatically train the models on datasets with user inputs and descriptions. The LLMs are employed as an automatic training system to establish connections with versatile models and process the inputs. We suggest using language as a universal interface and prompt for LLMs to interact with users. By incorporating both data and model descriptions into prompts, LLMs can manage AI models for data processing, model architecture design, and hyperparameter tuning. They can invoke these models as needed to tackle AI tasks and return the predicted training log. However, incorporating multiple AI models into LLMs demands a substantial number of high-quality model descriptions. To overcome this challenge, we recommend tapping into both model card <ref type="bibr" target="#b13">[Mitchell et al., 2019]</ref> that provides well-defined model descriptions and data card <ref type="bibr" target="#b7">[Gebru et al., 2021]</ref> for specific AI tasks. This approach would enable us to connect diverse models through a language-based interface, thus facilitating the solution of complex AI tasks. It can also enhance the transferability among models and datasets by capturing their similarity.</p><p>AutoML-GPT connects versatile machine learning models, training pipelines, and datasets to solve numerous complex AI tasks. More specifically, for each AI task we aim to solve, using its corresponding description (such as model card and data card ), we fuse the paragraph as the prompt into a pretrained LLMs (such as ChatGPT) to establish the AutoML pipeline. Afterward, in our system, LLMs perform the automatic training to return the predicted training logs for the input questions of users. Based on these training logs, we can further interact with the LLM to solve requests (such as hyperparameter tuning) shown in Figure <ref type="figure" target="#fig_0">1</ref>. Thus, the whole process of AutoML-GPT can be divided into four stages: 1) data processing, 2) model architecture design, 3) hyper-parameter tuning with the predicted training log, 4) human feedback on experimental data.</p><p>Benefiting from such a design, AutoML-GPT in Figure <ref type="figure" target="#fig_0">1</ref> is able to use external models and thus can handle multiple tasks on well-known benchmarks, and transfer the knowledge to unknown private dataset when only given metadata (data card). Furthermore, this pipeline also allows AutoML-GPT to continue absorbing the powers from task-specific experts, enabling growable and scalable AI capabilities. In summary, our contributions are as follows:</p><p>? To complement the advantages of large language models and expert models, we propose AutoML-GPT, which acts as the system for data processing and model architecture design and automatically conducts the experiments for each specific task.</p><p>? By integrating the model card with model descriptions and the data card with data descriptions, we provide a fixed-format prompt paragraph and build a training pipeline to tackle general AI tasks.  2 AutoML-GPT AutoML-GPT is a collaborative system that relies on the data and model information to format the prompt input paragraph. The LLM serves as the controller, while numerous expert models as collaborative executors.</p><p>The workflow of AutoML-GPT consists of four stages: data processing, model architecture design, hyperparameter tuning, and training log generation. Specifically, we suggest a general recipe for AutoML-GPT:</p><p>1) generate a fixed-format prompt paragraph with both the model card and data card, 2) build the training pipeline and process the user request on the selected dataset and model architectures, 3) generate the performance training log and tune the hyperparameters, and 4) tune the model with the auto-suggested hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Input Decomposition</head><p>In the first stage of AutoML-GPT, an LLM takes the input from the users. To boost the performance of the LLM and generate an effective prompt, we employ specific instructions for the input prompt. The instructions contain three parts described below.</p><p>Data Card To clarify the intended use cases of datasets and minimize their usage in contexts for which they are not well suited, we utilize the data card that provides comprehensive documentation for this dataset.</p><p>As shown in Figure <ref type="figure">2</ref>, the key components of the data card are comprised of the dataset name, input dataset type (e.g., image data or text data), label space (e.g., the class types or resolution), and default evaluation metrics.</p><p>Figure <ref type="figure">2:</ref> The Data Card includes the data name, input data type, label space, and evaluation metric. Within the data card, the same color denotes information originating from a single dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Card</head><p>The model cards in Figure <ref type="figure" target="#fig_2">3</ref>, complementary to the "Data Card" discussed earlier, serve as one of the proposed paradigms that report details of the model used to train and test the datasets. The model card consists of the model name, model structure (e.g., Swin transformer <ref type="bibr" target="#b12">[Liu et al., 2021]</ref> with a UperNet <ref type="bibr" target="#b29">[Xiao et al., 2018]</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metrics and Additional Requests</head><p>In addition to the model cards and data cards, users can have the option to request more evaluation benchmarks, metrics, or any constraints. Except for the default evaluation metrics, we can add specific metrics or constraints according to the user's request when selecting the model architecture. For example, given a constraint "the inference time smaller than 10 FPS," we then process the user requests under the evaluation metrics and constraints. Benefiting from this instruction and human feedback of these evaluation metrics and additional requests, the LLM can follow instructions better.</p><p>AutoML-GPT provides these task specifications to the LLM as high-level instructions for analyzing the user's requests accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data Processing</head><p>Data processing is an integral step in machine learning as the quality of data and the derived useful information directly affect the ability of our model to learn. It is thus crucial that we process the data before feeding it into our model. For example, in computer vision, data processing refers to the set of techniques and methods used to prepare raw image data for analysis or machine learning algorithms. This can include image resizing, normalization, augmentation, and filtering. Similarly, in Natural Language Processing (NLP) projects, data processing refers to transforming raw text data into a structured and clean format that machine learning algorithms can easily understand and process. Techniques such as tokenization, stopword removal, lowercasing, and removal of special characters and numbers are commonly used. Based on the provided data card and data descriptions, AutoML-GPT provides specific process techniques depending on the project's requirements and the data's nature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Model Architecture</head><p>Upon processing the list of tasks, AutoML-GPT needs to match each task with a corresponding model, essentially selecting the suitable model for every task in the list. To achieve this, we first acquire model cards and descriptions of the models from the user inputs. Following that, we dynamically assign models to tasks using the in-context task-model assignment mechanism. This approach enables incremental model access and offers greater openness and flexibility by combining the providing model descriptions and a better understanding of the user requests.</p><p>Model architectures refer to detailed explanations of a machine learning model's design, structure, and components. These descriptions typically include the following elements: input and output layers, hidden layers, activation functions, loss functions, and model-specific components (such as attention mechanisms, convolutional layers, or recurrent layers).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Hyperparameter Tuning with Predicted Training Log</head><p>To find the optimal set of hyperparameters that yield the best performance for a given model on a specific dataset, hyperparameter tuning is a crucial step in machine learning. Hyperparameters are configuration settings that are not learned during the training process but are predefined and control various aspects of the model's learning behavior. Examples of common hyperparameters include the learning rate, batch size, number of hidden layers, and number of neurons per layer.</p><p>In order to tune hyper-parameters without training on real machines, we predict the performance by generating a training log for a given hyper-parameter setting for the provided data card and model card. Unseen Datasets The hyperparameter tuning for unseen private datasets could be even more challenging.</p><p>Given the metadata of an unseen dataset, AutoML-GPT can recommend a hyperparameter configuration that is likely to be effective for that dataset. We rely on the data card to leverage the necessary text descriptions and identify the correlation between the unseen dataset and the existing ones. Based on the correlation, we transfer the hyper-parameter settings from the existing datasets to the new unseen dataset.</p><p>To calculate the correlation, we use a text encoder to encode the data card. Specifically, in the data card, it contains information such as class type, resolution, image size, and other relevant metadata. We take the dataset scale, task description, label space, and input/output data type as the input to a text encoder (e.g., CLIP <ref type="bibr" target="#b17">[Radford et al., 2021]</ref>) and describe the correlation between this unseen dataset and the existing datasets using the similarity score of the encoded latent representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We assess the performance of our AutoML-GPT and implement it using ChatGPT (OpenAI's "GPT-4" version)<ref type="foot" target="#foot_0">1</ref> . Various case studies are carried out to showcase the efficacy of our approach from multiple angles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Unseen Dataset</head><p>In Figure <ref type="figure" target="#fig_3">4</ref>, we present the results of training on an unseen dataset using AutoML-GPT. To verify the performance in real cases, we construct a set of performance and hyper-parameters on already trained datasets, and some coming untrained datasets. We will predict hyperparameter configurations for these untrained datasets. We make our test environment based on the classification setting described in <ref type="bibr" target="#b24">Vinyals et al. [2016]</ref>. We also follow the MiniImageNet <ref type="bibr" target="#b24">[Vinyals et al., 2016]</ref> to subsample and split the training dataset <ref type="bibr" target="#b3">[Deng et al., 2009]</ref> into 80% and 20% portions. From the 80% data, we construct the data cards and corresponding model cards (containing model best hyperparameters). We randomly select fifteen classes to create various subset datasets (e.g., dataset A, B, etc.), grid search the hyper-parameters, finetune the ViT base model <ref type="bibr" target="#b4">[Dosovitskiy et al., 2020]</ref> and log the best performance on these subset datasets. We then create a new dataset called "New" with ten image classes from the remaining 20% data. Here is the python script we use for data preprocessing for swin transformer-based object dection on COCO: For object detection on COCO dataset using a slided window approach with transformer-based models, DETR has a transformer encoder and decoder, and it is designed to directly output a set of object queries and their corresponding bounding box predictions in a single forward pass. This eliminates the need for anchor-based or anchor-free detection methods and achieves stateof-the-art performance on COCO object detection ?</p><p>We use your suggested model card and data card. The hyperparameter of DETR model on COCO dataset for object detection is  To demonstrate the capabilities of our approach on unseen datasets, we utilize AutoML-GPT to recommend the best training configuration for the "New" dataset based on the provided data card and model card.</p><p>In our data card, we log the label space, i.e., text descriptions for each class. In practice, we incorporate a similarity score between two data cards by passing the text in the data card through a text encoder, e.g., the CLIP text encoder, and calculating the similarity. Specifically, in Figure <ref type="figure" target="#fig_3">4</ref>, we state that the "New" dataset has a 60% label space similarity to dataset A and a 40% label space similarity to dataset B. Using this information and the hyper-parameter settings in the data cards for dataset A and B, AutoML-GPT can recommend the appropriate hyperparameter settings for training on the "New" dataset. In our experiments, we achieve 98% accuracy for the Top 1 prediction, compared to 80% Top 1 accuracy with average random-selected hyperparameters. Moreover, we also initialize the model using the suggested hyperparameter settings from AutoML-GPT without giving any additional datasets With this configuration, we achieve 82% Top 1 accuracy, which is better than the average randomly-selected hyperparameters but not as good as our recommended setting. It also suggests that ChatGPT can give good hyperparameter settings for a specific task (e.g., image classification). This demonstrates the effectiveness of our proposed auto-training approach in addressing machine learning problems, even with unseen or new datasets. These findings highlight the potential of our auto-training method to enhance machine learning by providing accurate hyperparameter recommendations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Object Detection</head><p>Figure <ref type="figure" target="#fig_4">5</ref> presents our results on the COCO dataset <ref type="bibr" target="#b11">[Lin et al., 2014]</ref> for object detection. The top block displays the data card for the COCO dataset and the model card for ImageNet, based on user input. The middle block demonstrates the AutoML-GPT Prompt Paragraph derived from the input decomposition. The information from the data card and model card is automatically incorporated into our prompt format. We report the results for data processing, model architecture design, hyperparameter tuning, and training log generation. In data processing, AutoML-GPT generates a script for handling the input dataset. We also provide a Python script example in Figure <ref type="figure" target="#fig_4">5</ref>. For model architecture design, our pipeline generates a model composition for subsequent training. Once both the data and model are prepared, the detailed configurations are provided in the hyperparameter-tuning stage (e.g., learning rate: 10 -4 , weight decay: 10 -4 ) and are further tuned with predicted training logs. These results further validate that our method can serve as  an effective pipeline for flexibly adapting LLMs to downstream tasks. Our approach, which employs data and model cards to derive the AutoML-GPT prompt paragraph, can also be considered as a complementary module for works focused on enhancing LLM prompt components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Question Answering</head><p>We present the experimental results on the Natural Questions Open dataset <ref type="bibr" target="#b10">[Kwiatkowski et al., 2019]</ref> in Figure <ref type="figure" target="#fig_5">6</ref>. We utilize Dense Passage Retrieval (DPR) <ref type="bibr" target="#b9">[Karpukhin et al., 2020]</ref>. x For the data card, users input the data name, input data type, label space, and evaluation metrics. y For the model card, it includes model name, model structure, model descriptions, and architecture hyperparameters. z With the generated AutoML-GPT prompt paragraph, AutoML-GPT carries out data processing, model architecture creation, hyperparameter tuning, and generates a predicted training log. As seen in the "Hyperparameter Tuning," the hyperparameters generated by AutoML-GPT and those provided by DPR align closely, e.g., the learning rate is 10 -5 and max epochs is 40. { Once the predicted training log is available, we showcase a scenario where the user can ask AutoML-GPT for different evaluation metrics or model architectures based on their requirements, as illustrated in Figure <ref type="figure" target="#fig_5">6</ref> "Additional requests: fast inference time for DPR retriever." As seen in the returned response in Figure <ref type="figure" target="#fig_5">6</ref>, AutoML-GPT also offers hints such as "without sacrificing too much performance." AutoML-GPT further tunes the hyper-parameters based on these requests and predicted logs. Our method demonstrates the powerful ability to automatically conduct experiments and perform interactive hyperparameter tuning. It further confirms that our approach works well for various datasets and can generalize across different input types and domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Classification</head><p>We also evaluate AutoML-GPT on the UCI Adult dataset <ref type="bibr" target="#b5">[Dua and Graff, 2017]</ref> using XGBoost. As before, we supply the data card and model card to generate the input prompt paragraph. The same training pipeline is applied here, as shown in Figure <ref type="figure">7</ref>. We also adhere to the hyperparameter settings suggested by AutoML-GPT and train the XGBoost model. This training results in a final validation loss of 0.277 with 85.92% accuracy.</p><p>Despite the different inputs and tasks, our proposed AutoML-GPT consistently delivers strong performance in classification. This further demonstrates that AutoML-GPT can be employed for a wide range of machine 2023] is built with the HuggingFace transformers library and utilizes the GPT as the interaction agent.</p><p>VisualGPT <ref type="bibr" target="#b28">[Wu et al., 2023]</ref> incorporates different Visual Foundation Models to enable the user to interact with ChatGPT. OpenAGI <ref type="bibr" target="#b6">[Ge et al., 2023]</ref>, an open-source AGI research platform, is designed to offer complex, multi-step tasks and accompany by task-specific datasets. Similarly, we also integrate the GPT into our AutoML pipeline. There is also another GPT based system that can incorporate extra information from search engines, e.g., AutoGPT<ref type="foot" target="#foot_1">2</ref> . AutoML-GPT rethinks the impact of ChatGPT from the auto training perspective. We focus on building the training pipeline and establishing an AutoML system from the start to end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Our work demonstrates the benefits of building AutoML systems upon GPT. The proposed method can automatically conduct machine learning experiments. This automatic learning dramatically improves training efficiency and enhances the model's performance. We demonstrate use cases across computer vision, natural questions answering, and classification benchmarks. We further conduct a detailed use case with the unseen datasets and additional interactions between the user and AutoML-GPT. To summarize, the proposed AutoML-GPT is effective and general, with the potential to create a natural language interface for tuning machine learning models for various tasks. In the future, we will 1) automatically generate the model and data cards for well-known benchmarks and make them a part of our system, and 2) extract task-aware sub-networks from large pretrained models with the help of ChatGPT.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of AutoML-GPT. Some notations are labeled along with corresponding components. 'Eval Metrics &amp; Add' refers to the evaluation metrics and additional requests.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>head), model descriptions, and architecture hyperparameter. By providing this information, model cards inform the LLM about the machine learning systems used and the degree of flexibility the user would like to have on the model architecture. It would further create more inclusive outcomes with the LLM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The Model Card comprises model name, model structure, model descriptions, and architecture hyperparameters. In the model card, the same color represents information from a single model card.</figDesc><graphic url="image-2.png" coords="3,127.56,607.31,340.17,107.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Overview of AutoML-GPT for the unseen dataset: the top block showcases data card and model information. We first log the training information for several datasets. The data cards for these datasets are processed through a text encoder to obtain similarity scores, which are then combined with model parameters of corresponding trained models to form the AutoML-GPT prompt paragraph. The bottom block presents the predicted training log based on the recommended hyperparameter settings for the unseen dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Overview of AutoML-GPT for object detection: The top block displays the data card and model card. The middle block showcases the AutoML-GPT prompt paragraph, derived from the data card and model card. The bottom block outlines the four steps: data processing, model architecture, hyperparameter tuning, and predicted training log. We use the predicted training log to tune the hyperparameters before feedbacking the hyperparameters to the users.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Overview of AutoML-GPT for question answering: The top block presents data card and model information, while the middle block highlights the AutoML-GPT prompt paragraph, derived from both data card and model card. The bottom block details the four steps: data processing, model architecture, hyperparameter tuning, and predicted training log.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Data Processing Model Architecture Hyperparameter Tuning Predicted Training Log Data Card Model Card Eval Metric &amp; Add Input Paragraph</head><label></label><figDesc></figDesc><table /><note><p>? Extensive evaluations on multiple AI tasks across language, vision, and continual learning demonstrate the capability of AutoML-GPT in auto training. It further demonstrates the effectiveness of providing the hyperparameter tuning for an unseen or new dataset.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Label Space COCO Data: large scale object ? Image Input Data Type Data Name Shifted window with a 2-layer MLP ? Window size: 7 ? Model Des</head><label></label><figDesc>Assume we have a {Data Card: COCO dataset with 328K images and evaluation metric ?}, we adopt the {Model Card: slided window swin transformer ?} as the model. We use {A: Data Processing} and {B: Model Architecture} script for processing COCO dataset. We use {C: Hyperparameter} for the hyperparameters on {Resource: 4 Nvidia a5000 GPU}. Then we would have the training log {D: Predicted Training Log}.</figDesc><table><row><cell>Data Card</cell><cell></cell><cell>Model Card</cell></row><row><cell></cell><cell>Model Name</cell><cell>Model Structure</cell></row><row><cell></cell><cell>Swin Transformer</cell><cell>Multi-head self attention with ?</cell></row><row><cell></cell><cell>Eval</cell><cell>Architecture Hyperparameter</cell></row><row><cell>Object Categories: person ?</cell><cell>Box AP</cell></row><row><cell></cell><cell>AutoML-GPT Prompt Paragraph</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Label Space Natural Questions: open domain ? Text Input Data Type Data Name Dense retriever is bi-encoder ?. Encoder sequence length: 350 ? Model Des Architecture Hyperparameter Wikipedia that may or may not ? Exact match? Eval DPR Question encoder ? Model Name Model Structure Data Card Model Card AutoML-GPT Prompt Paragraph Assume</head><label></label><figDesc>we have a {Data Card: natural question is open ?}, we adopt the {Model Card: DPR with the dense retriever which is biencoder?} as the model. We use {A: Data Processing} and {B: Model Architecture} script for processing natural questions answering dataset. We use {C: Hyperparameter} for the hyperparameters on {Resource: 8 Nvidia v100 GPU}. Then we would have the training log {D: Predicted Training Log}. We also have an {Additional Request</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>: the faster inference time for DPR retriever}.</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>Data Processing</cell><cell></cell><cell>Model Architecture</cell></row><row><cell cols="3">Here's a Python script for data preprocessing of DPR on NQ dataset for open domain question</cell><cell>The model architecture for DPR (Dense Passage Retrieval) on the Natural Questions (NQ)</cell></row><row><cell>answering:</cell><cell></cell><cell></cell><cell>dataset for open domain question answering involves two components: Retriever ?</cell></row><row><cell>import json import random</cell><cell></cell><cell></cell><cell>Predicted Training Log</cell></row><row><cell>def load_data(file_path):</cell><cell></cell><cell></cell><cell>Here is an training log for a DPR model with the retriever trained on NQ dataset using the</cell></row><row><cell>with open(file_path, 'r') as f: data = json.load(f)</cell><cell></cell><cell></cell><cell>suggested hyperparameters:</cell></row><row><cell>return data</cell><cell></cell><cell></cell><cell>Training Log:</cell></row><row><cell>def prepare_data(data): processed_data = []?</cell><cell>Hyperparameter Tuning</cell><cell>AutoML-GPT</cell><cell>Epoch 1: Iteration 100 -Loss: 6.7532 Iteration 200 -Loss: 4.4215</cell></row><row><cell cols="2">We use your suggested model card and data card. The hyperparameter of DPR model on the Natural Questions (NQ) dataset is: ? max_epochs: 40 ? learning_rate: 1e-5 ? batch_size: 128 Retriever Training Hyperparameters:</cell><cell></cell><cell>Iteration 300 -Loss: 3.6890 ? learning_rate: 1e-5 batch_size: 128 adjusting certain ? Here's a suggested configuration for training the model: Additional Requests (Yes): To achieve faster inference time without sacrificing too much performance, you can consider faster inference time for DPR retriever</cell></row><row><cell>? warmup_steps: 1000</cell><cell></cell><cell></cell><cell>max_epochs: 40 # Additional hyperparameters for faster inference</cell></row><row><cell>? drop_out: 0.1 ?</cell><cell></cell><cell></cell><cell>model_dimension: 256 # Reduce model dimensionality (default is 768 for BERT-based DPR) max_sequence_length: 128 # Limit the input sequence length (default is 512) ?</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://platform.openai.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://github.com/Significant-Gravitas/Auto-GPT</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AutoML-GPT Prompt Paragraph</head><p>Assume we have a {Data Card: UCI adut income dataset is?}, we adopt the {Model Card: XGBoost is an ensemble learning?} as the model. We use {A: Data Processing} and {B: Model Architecture} script for processing UCI ault income dataset. We use {C: Hyperparameter} for the hyper-parameters on {Resource:</p><p>AMD EPYC 7702P 64-Core Processor}. Then we would have the training log {D: Predicted Training Log}.</p><p>Here's a Python script for data preprocessing of XGBoost on UCI adult dataset: XGBoost (eXtreme Gradient Boosting) is an ensemble method that utilizes decision trees as base learners. The model architecture of XGBoost consists of multiple decision trees, where each tree is added iteratively to correct the errors of the previous trees in the ensemble. The final prediction is made based on the sum of the predictions of all trees in the ensemble ?  <ref type="bibr" target="#b1">[Chowdhery et al., 2022]</ref> with 540 billion parameters. The scaling of LLM has unlocked new emergent abilities previously unobserved under smaller models <ref type="bibr">[Wei et al., 2022a]</ref>. These LLMs have demonstrated the superiority of LLMs for zero-shot learning. Among existing LLMs, ChatGPT has unique characteristics. It has the ability to interact with users in a conversation-like manner, while retaining its accumulated knowledge and generalization ability gained from pre-training.</p><p>Going a step further, we explore the zero-shot learning capability of ChatGPT on different tasks beyond dialogue in this work.</p><p>Chain of Thought Chain-of-thought (CoT) prompting induces LLMs to generate intermediate reasoning steps before answering <ref type="bibr">[Wei et al., 2022b]</ref>. There are two lines of research focusing on the current CoT prompting. One line is exploring the manually designed CoT. In the manually designed CoT, LLMs adapt the manually designed features and demonstration for the reasoning process <ref type="bibr">[Wei et al., 2022b]</ref>. <ref type="bibr" target="#b25">Wang et al. [2022]</ref> proposes a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. Recently, Interactive-Chain-Prompting <ref type="bibr" target="#b16">[Pilault et al., 2023]</ref> is introduced to resolve the ambiguity for crosslingual conditional generation. Another line is conducting research on the zero-shot setting, where STaR <ref type="bibr" target="#b31">[Zelikman et al., 2022]</ref> is introduced for the self-generation and helps the model to self-improve, and Automatic Reasoning and Tool-use (ART) <ref type="bibr" target="#b15">[Paranjape et al., 2023]</ref> is a framework that uses frozen LLMs to automatically generate intermediate reasoning steps as a program.</p><p>GPT-based Systems GPT <ref type="bibr" target="#b0">[Brown et al., 2020]</ref> has shown promising performance improvements. A recent line of research has focused on integrating the GPT model into AI systems. HuggingGPT <ref type="bibr">[Shen et al.,</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02311</idno>
		<title level="m">Palm: Scaling language modeling with pathways</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Chung</forename><surname>Hyung Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.11416</idno>
		<title level="m">Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName><forename type="first">Dheeru</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Casey</forename><surname>Graff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Yingqiang</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyue</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianchao</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juntao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuyuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.04370</idno>
		<title level="m">Openagi: When llm meets domain experts</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Datasheets for datasets</title>
		<author>
			<persName><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Morgenstern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Briana</forename><surname>Vecchione</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hal</forename><forename type="middle">Daum?</forename><surname>Iii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Crawford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="86" to="92" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Leveraging passage retrieval with generative models for open domain question answering</title>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01282</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dense passage retrieval for open-domain question answering</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><forename type="middle">N</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<title level="m">Natural Questions: a benchmark for question answering research</title>
		<imprint>
			<publisher>TACL</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitnick</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014: 13th European Conference</title>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014-09-06">2014. September 6-12, 2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
	<note>Proceedings, Part V 13</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Model cards for model reporting</title>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zaldivar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><surname>Vasserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Spitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inioluwa</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Deborah</forename><surname>Raji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on fairness, accountability, and transparency</title>
		<meeting>the conference on fairness, accountability, and transparency</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="220" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<title level="m">Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Bhargavi</forename><surname>Paranjape</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Tulio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ribeiro</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2303.09014</idno>
		<title level="m">Automatic multi-step reasoning and tool-use for large language models</title>
		<meeting><address><addrLine>Art</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Interactive-chain-prompting: Ambiguity resolution for crosslingual conditional generation with interaction</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Pilault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Bra?inskas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.10309</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><surname>Pmlr</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Jack W Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Aslanides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><surname>Ring</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.11446</idno>
		<title level="m">Susannah Young, et al. 2021. Scaling language models: Methods, analysis &amp; insights from training gopher</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Ori</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itay</forename><surname>Dalmedigos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dor</forename><surname>Muhlgay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amnon</forename><surname>Shashua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Leyton-Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Shoham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.00083</idno>
		<title level="m">-context retrieval-augmented language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface</title>
		<author>
			<persName><forename type="first">Yongliang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiming</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.17580</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model</title>
		<author>
			<persName><forename type="first">Shaden</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Norick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shrimai</forename><surname>Prabhumoye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Zerveas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Korthikanti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.11990</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timoth?e</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozi?re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<title level="m">Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Self-consistency improves chain of thought reasoning in language models</title>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.11171</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.07682</idno>
		<title level="m">Emergent abilities of large language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.11903</idno>
		<title level="m">Chain of thought prompting elicits reasoning in large language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Visual chatgpt: Talking, drawing and editing with visual foundation models</title>
		<author>
			<persName><forename type="first">Chenfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengming</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhen</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zecheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.04671</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="418" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">An explanation of in-context learning as implicit bayesian inference</title>
		<author>
			<persName><forename type="first">Sang</forename><surname>Michael Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditi</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.02080</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Star: Bootstrapping reasoning with reasoning</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Zelikman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="15476" to="15488" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Knowing more about questions can help: Improving calibration in question answering</title>
		<author>
			<persName><forename type="first">Shujian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengyue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.01494</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Shujian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengyue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingchao</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.00915</idno>
		<title level="m">Passage-mask: A learnable regularization strategy for retriever-reader models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Mingkai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.10970</idno>
		<title level="m">Can gpt-4 perform neural architecture search? arXiv preprint</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
