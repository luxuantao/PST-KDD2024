<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automated building extraction using satellite remote sensing imagery</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-12-16">16 December 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qintao</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Key Laboratory of Optical Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>610209</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Optics and Electronics</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>610209</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100039</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liangli</forename><surname>Zhen</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Institute of High Performance Computing</orgName>
								<orgName type="institution" key="instit2">Agency for Science, Technology and Research</orgName>
								<address>
									<postCode>138632</postCode>
									<country>Singapore, Singapore</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yao</forename><surname>Mao</surname></persName>
							<email>maoyao@ioe.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Key Laboratory of Optical Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>610209</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Optics and Electronics</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>610209</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xi</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Key Laboratory of Optical Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>610209</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Optics and Electronics</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>610209</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guozhong</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Key Laboratory of Optical Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>610209</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Optics and Electronics</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>610209</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Key Laboratory of Optical Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>610209</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Automated building extraction using satellite remote sensing imagery</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-12-16">16 December 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1016/j.autcon.2020.103509</idno>
					<note type="submission">Received 29 November 2019; Received in revised form 23 September 2020; Accepted 29 September 2020</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Remote sensing Building extraction Urban planning Digital city construction 2019 MSC: 11-29 99-00</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic extraction of buildings from remote sensing images plays a critical role in urban planning and digital city construction applications. In real-world applications, however, real scenes can be highly complex (e.g., various building structures and shapes, presence of obstacles, and low contrast between buildings and surrounding regions), making automatic building extraction extremely challenging. To conquer this challenge, we propose a novel method called Deep Automatic Building Extraction Network (DABE-Net). It adopts squeeze-andexcitation (SE) operations and the residual recurrent convolutional neural network (RRCNN) to construct building-blocks. Furthermore, an attention mechanism is introduced into the network to improve segmentation accuracy. Specifically, to handle small buildings, we highlight small buildings and develop a multi-scale segmentation loss function. The theoretical analysis and experimental results show that the proposed method is effective in building extraction and outperforms several peer methods on the dataset of Mapping challenge competition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Automatic building extraction, which identifies buildings from the captured images, has been widely applied in many applications, such as urban planning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, geographic information system (GIS) data updating <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, damage assessment <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> and digital city construction <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. Early research studies on building extraction are usually done based on aerial imagery <ref type="bibr" target="#b8">[9]</ref> due to its high spatial resolution. Nevertheless, it is time-consuming to obtain the images of a large area like the whole city.</p><p>In recent decades, the availability of high-resolution satellite imaging sensors provides a new data source for automatic building extraction. The high spatial resolution of remote sensing imagery reveals fine details in urban areas and greatly facilitates the automatic building extraction. A large number of methods have been developed using remote sensing imagery. For instance, based on traditional image processing approaches <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>, traditional methods consider spectra, shape, and texture as the input features. Then they take support vector machine (SVM), random forest (RF), or AdaBoost as the classifier <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref> to extract buildings. However, designing the feature extractors requires high expertise in the area, and the obtained features may not be suitable for new datasets.</p><p>Inspired by the great success of deep learning in image classification <ref type="bibr" target="#b14">[15]</ref>, speech recognition <ref type="bibr" target="#b15">[16]</ref>, and machine translation <ref type="bibr" target="#b16">[17]</ref>, some researchers applied deep learning approaches to remote sensing segmentation tasks <ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref>. There are also a few attempts on building extraction <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref>. For example, Li et al. <ref type="bibr" target="#b2">[3]</ref> proposed a U-Net-based semantic segmentation method for the extraction of building footprints from high-resolution multispectral satellite images and multi-source GIS data. Lu et al. <ref type="bibr" target="#b24">[25]</ref> proposed a building edge detection model using a richer convolutional features (RCF) network. The RCF-building model could detect building edges more accurately and obtain a significant performance improvement over the baselines. Shrestha et al. <ref type="bibr" target="#b27">[28]</ref> proposed a fully connected network-based building extraction approach by combining the exponential linear unit (ELU) and conditional random fields (CRFs). Wu et al. <ref type="bibr" target="#b28">[29]</ref> presented a boundary regulated network called BR-Net for accurate aerial image segmentation and building outline extraction. The BR-Net achieves significantly higher performance than the U-Net model.</p><p>Although these methods have achieved promising performance in simple scenarios, real-world applications usually involve in highly complex scenes. For instance, the structure and the shape of buildings vary largely in different countries; the presence of obstacles posed by surrounding objects, like tress and billboards, and the contrast between buildings and surrounding regions may be extremely low. It makes automatic extraction of buildings from remote sensing images challenging, and the performance of existing methods deteriorates sharply.</p><p>To conquer the challenges above, in this paper, we propose a novel method called Deep Automatic Building Extraction Network (DABE-   Net). It adopts squeeze-and-excitation (SE) operations and the residual recurrent convolutional neural network (RRCNN) to construct buildingblocks. Furthermore, an attention mechanism is introduced into the network to improve segmentation accuracy. Specifically, to handle small buildings, we highlight small buildings and develop a multi-scale segmentation loss function. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, we design two loss functions for our proposed model, including the multi-scale segmentation loss and the segmentation loss. Following this learning strategy, our DABE-Net achieves promising performance on the building extraction dataset. The novelty and the main contribution of this work are two-fold: 1) A novel deep model is developed for automatic building extraction from remote sensing images. It includes SE and RRCNN blocks and involves attention gates to attach importance to network channel information and global information, and 2) Unlike existing methods, our method can effectively balance samples and focus on buildings of different scales. Extensive experiments on the Mapping challenge competition dataset have been conducted. The results demonstrate that our method outperforms several peer methods for automatic building extraction, which indicates the effectiveness of the proposed method.</p><p>The remainder of this paper is as follows. Section 2 presents our proposed method. Experimental results and discussion are provided in Section 3. We conclude this paper in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Method</head><p>Our proposed deep automatic building extraction network (DABE-Net) has a U-Net shape CNN structure, which involves an encoder and a decoder. We introduce the SE and RRCNN (SERRCNN)-block and the attention gates into the encoder and the decoder. The SERRCNN-block can enhance the discriminative features, and the attention gates can extract semantic contextual information. Meanwhile, we improve the lov Ã¡sz hinge loss to balance the background and the building.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">SERRCNN-block</head><p>Recently, the deep convolution neural network is used in the semantic segmentation of high-resolution remote sensing images with excellent performance. However, the information on convolutional features among different channels is not effectively utilized. We introduce the squeeze-and-excitation operations <ref type="bibr" target="#b29">[30]</ref> to learn the attention weights of different feature channels automatically. According to the attention weights, discriminative features are enhanced while redundant features for the target tasks are suppressed. Simultaneously, there are rich feature details in the remote sensing image. We further adopt the recurrent convolutional neural network (RCNN) <ref type="bibr" target="#b30">[31]</ref> to extract the information about the details from image features. The operation improves building boundary information.</p><p>Including the Squeeze-and-Excitation operations and the recurrent convolutional neural networks, we develop the Squeeze-and-Excitation Residual RCNN (SERRCNN)-block, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. RCNN and its variants have already shown superior performance on object recognition tasks using different benchmarks <ref type="bibr" target="#b30">[31]</ref>, we introduce it into DABE-Net, by following <ref type="bibr" target="#b30">[31]</ref>, we denote x l as the input of l th layer of SERRCNNblock and a pixel located at (i, j) in an input sample on the k th feature map in the Recurrent Convolutional Layers (RCL) <ref type="bibr" target="#b30">[31]</ref>. The output of the network at the time step t, O ijk l (t) can be expressed as Eq. ( <ref type="formula">1</ref>) <ref type="bibr" target="#b30">[31]</ref>:</p><formula xml:id="formula_0">O l ijk (t) = ( w f k ) T × x f (i,j) l (t) + ( w r k ) T × x r(i,j) l (t − 1) + b k (1)</formula><p>where x l f(i,j) (t) and x l r(i,j) (t − 1) are the inputs of the l th standard convolution layers and the l th RCL, respectively. The values of (w k f ) T and (w k r ) T are the weights of the standard convolutional layer and the RCL of the k th feature map respectively, and b k is the corresponding bias. The output of l th RCL with the activations can be expressed as Eq. ( <ref type="formula" target="#formula_1">2</ref>):</p><formula xml:id="formula_1">u = F(x l , w l ) = σ ( O l ijk (t) ) (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where F is the recurrent operation, σ refers to the ReLU <ref type="bibr" target="#b31">[32]</ref> function, and u is the output of l th RCL.</p><p>To tackle the issue of exploiting channel dependencies, we consider the signal to each channel in the output features. We compress the feature along the spatial dimension by turning each two-dimensional feature channel into z ∈ ℝ c . It has a global receptive field to some extent, and the output dimension matches the number of input feature channels. z is generated by shrinking u through spatial dimensions C × H × W into C × 1 × 1, where C is the number of channels and H × W is the size of feature map, the c th element of z is calculated via Eq. (3) by following <ref type="bibr" target="#b29">[30]</ref>:</p><formula xml:id="formula_3">Z c = F sq (u c ) = 1 H × W ∑ H i=1 ∑ W j=1 u c (i, j)<label>(3)</label></formula><p>To make use of the information aggregated in the squeeze operation, we adopt the following operation to capture channel-wise dependencies.</p><formula xml:id="formula_4">s = F ex (z, W) = σ(g(z, W) ) = σ(W 2 σ(W 1 z) )<label>(4)</label></formula><p>where</p><formula xml:id="formula_5">W 1 ∈ ℝ C 16 ×C and W 2 ∈ ℝ C× C</formula><p>16 . The final output of the block can be obtained by rescaling the transformation output u with the activations as</p><formula xml:id="formula_6">x = F scale (u c , s c ) = s c ⋅u c (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>and</p><formula xml:id="formula_8">x l+1 = x l + xc<label>(6)</label></formula><p>The activations act as channel weights adapted to the input. In this regard, SE blocks intrinsically introduce dynamics conditioned on the input, helping to boost feature discriminability <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Attention gates</head><p>The complexity of the background and the diversity of the object structures make it easy to cause error extraction and inaccurate segmentation of the semantic boundary of the ground objects. To capture a sufficiently large receptive field and semantic contextual information, Fig. <ref type="figure">3</ref>. Schematic of the attention gate (AG). x l is the input features and α is attention coefficient computed in AG. g is the input of the encoder, x l is the decoder featuremaps as the same size as g.</p><p>Q. <ref type="bibr">Hu et al.</ref> researchers usually downsample the feature-map grid gradually in standard CNN architectures. Note that the shallow convolutional features are important for extracting some low-level information like colors and edges. Discarding these detailed features may reduce false-positive predictions for building objects. Through attention operation, we progressively suppress feature responses in background regions, such as cars and roads, and improve the network's attention to the building features.</p><p>As shown in Fig. <ref type="figure">3</ref>, we introduce attention coefficient, α i ∈ [0, 1], identifies salient image regions and restrain feature responses to merely retain activations related to specific tasks. The output of AGs is the element multiplication of input feature mapping and attention coefficient:</p><p>xl i,c = x l i,c ⋅α l i , from Fig. <ref type="figure">3</ref>, where α i l is a single scalar attention value and computed for each pixel vector x i l ∈ ℝ Fl and F l corresponds to the number of feature-maps in layer l. g i ∈ ℝ Fg is used for each pixel i to determine focused regions called gating vector that contains contextual information to prune lower-lever feature responses. Regarding the gating coefficient, we use additive attention in <ref type="bibr" target="#b32">[33]</ref>, which has been proved being effective in feature extraction. Additive attention is formulated as</p><formula xml:id="formula_9">α l i = σ 2 ( W T 3 ( σ 1 ( W T 1 s l i + W T 2 g i + b 1 + b 2 ) ) + b 3 )<label>(7)</label></formula><p>where σ 1 refers to the ReLU <ref type="bibr" target="#b33">[34]</ref> function and σ 2 is a sigmoid <ref type="bibr" target="#b34">[35]</ref> function, W i , b i is the parameters and bias of Conv i , respectively.</p><formula xml:id="formula_10">W 1 ∈ Fl×Fint , W 2 ∈ ℝ Fg×Fint , W 3 ∈ ℝ Fint×1 , b 1, 2 ∈ ℝ Fint</formula><p>, and b 3 ∈ ℝ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Symmetric extension of the lov Ã¡sz hinge loss function</head><p>In building extraction, largely different scales of input images may make the network hard to handle. The imbalance between the building and the background further worsens the effect of the network. To conquer this issue, we improve the lov Ã¡sz hinge loss to balance the background and buildings.</p><p>Traditionally, the logic regression loss function is used to optimize cross-entropy loss for semantic segmentation. However, due to the measure of cross-entropy loss on a validation set is constantly a poor indicator of the quality of segmentation. The intersection-over-union (IoU) score, namely the Jaccard index is widely adopted to evaluate segmentation masks. According to definition in <ref type="bibr" target="#b35">[36]</ref>, F i (x) is the i-th element of the output of the network, given a vector of ground truth labels y* and predicted labels ỹ, the Jaccard index of class c is defined as</p><formula xml:id="formula_11">J c (y * , ỹ) = |{y * = c} ∩ {ỹ = c}| |{y * = c} ∪ {ỹ = c}| (8)</formula><p>The corresponding loss function used in empirical risk minimization to train the network as follows:</p><formula xml:id="formula_12">ΔJ c (y * , ỹ) = 1 − J c (y * , ỹ)<label>(9)</label></formula><p>where,</p><formula xml:id="formula_13">ỹi = sign(F i (x) )<label>(10)</label></formula><p>To use a max margin classifier, following <ref type="bibr" target="#b35">[36]</ref>, in the binary case, the original lovász hinge loss associated with the prediction of the pixel i is computed as</p><formula xml:id="formula_14">m i = max ( 1 − F i (x)y * i , 0 )<label>(11)</label></formula><p>Considering the imbalance between the background and the building, we evolve it by symmetric extension into the new symmetric lovász hinge loss to solve the problem of data imbalance in the binary segmentation:</p><formula xml:id="formula_15">m * i = ( max ( 1 − F i (x)y * i , 0 ) + max ( F i (x) ( 1 − y * i ) , 0 ) )/ 2 (<label>12</label></formula><formula xml:id="formula_16">)</formula><p>where m i and m i * ∈ ℝ + are the vector of hinge losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiment and discussion</head><p>In this section, we first introduce the used dataset. We then introduce the experiment setups and evaluation metrics. At last, we evaluate the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset and preprocessing</head><p>The dataset is derived from the mapping challenge competition dataset on crowdai consisting of 280,741 training data and 60,317 Validation data. The image includes 300 × 300 pixels; each pixel is divided into buildings and backgrounds.</p><p>Since image segmentation is a data-driven algorithm, accurate results can be obtained according to the high diversity and quality of datasets. Data expansion is an effective method to improve performance by using the same amount of data. In this study, we take several specific methods to expand the training dataset, such as rotation, flipping, inserting random color jitter, and randomly clipping to preprocess each input image. Each image is zoomed to 256 × 256. Some examples of the original images and labels of the dataset are shown in Fig. <ref type="figure" target="#fig_3">4</ref>.</p><p>To improve the accuracy of extracting small buildings through the network, we design the attention weight of learning, and give larger weights to small buildings, so that the network can focus on small buildings. As shown in Fig. <ref type="figure" target="#fig_5">5</ref>, we use the color to represent the weight of buildings, and the smaller buildings have larger weights, the darker the color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training</head><p>We implement our model in PyTorch. The network is randomly initialized under the default setting of PyTorch with no pretraining on any external dataset. All experiments are run on a desktop computer equipped with 2 Intel Xeon E5-2678v3 CPU, 64 GB memory, 4 NVIDIA 2080Ti GPUs (with 11 GB*4 video memory) and Ubuntu 16.04 OS. We use the Adam optimization algorithm for training the network and set default learning parameters. The initial learning rate is 2e − 4 and decays to 1e − 5 after 20 epochs. To fairly compare different methods, the batch size and epochs for training are fixed to 40 and 60, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Evaluation metrics</head><p>We adopt the evaluation metric of the COCO 2012 dataset, and the segmentation task is assessed by the precision and recall. Segmentations are determined as true or false positives according to the area of overlap with ground-truth. Eq. <ref type="bibr" target="#b12">(13)</ref> shows the definition of IoU for evaluating whether a detected building polygon is accurate, which equals the overlap region of a detected building polygon (denoted by B gt ) and a ground truth building polygon (denoted by B gt ) divided by the union area of B p and B gt <ref type="bibr" target="#b2">[3]</ref>.</p><formula xml:id="formula_17">IoU = aera ( B p ∩ B gt ) aera ( B p ∪ B gt )<label>(13)</label></formula><p>If IoU between a detected building polygon and a ground truth building polygon is larger than 0.5, we consider the building polygon as correctly detected. Precision (Eq. ( <ref type="formula">14</ref>)) and recall (Eq. ( <ref type="formula" target="#formula_18">15</ref>)) are the common evaluation metrics for COCO dataset. The results of each image are evaluated independently, and the final F1-score is the average value of F1-scores (Eq. ( <ref type="formula" target="#formula_20">16</ref>)) for each image.</p><formula xml:id="formula_18">Precision = TP TP + FP (14) Recall = TP TP + FN (<label>15</label></formula><formula xml:id="formula_19">)</formula><formula xml:id="formula_20">F1 − score = 2 × TP 2 × TP + FP + FN (<label>16</label></formula><formula xml:id="formula_21">)</formula><p>where true positive (TP) indicates the number of building polygons that are detected correctly, false positive (FP) is the number of other objects that are detected as building polygons by mistake, and false negative (FN) represents the number of building polygons not detected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Ablation experiment</head><p>To verify the effectiveness of different components, we construct and evaluate the following variants:   Table <ref type="table" target="#tab_0">1</ref> shows the precision, recall, and F1-scores of different tested methods in building extraction on validation datasets. In the four experiments, the full proposed model achieves the highest scores. A comparison of the results of our DABE-Net and other methods on some examples are shown in Fig. <ref type="figure">6</ref>, from which we can also see that the full model can achieve much better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Dscussion</head><p>From Table <ref type="table" target="#tab_0">1</ref>, by comparing between the U-Net model and the U-Net_i model, we can see that the precision value is increased by 0.7%, the recall rate is increased by 0.6%. From the result of DABE-Net and DABE-Net_i, we observe that the precision is increased by 1.0%, the recall rate is increased by 2.0%, and the advantage of symmetric extension lov Ã¡sz hinge loss is concluded. Similarly, the comparison between the results of U-Net and DABE-Net demonstrates the performance of ARRSEU-Net by the increment of 1.0% for precision and 0.7% for recall. Meanwhile, the comparison between the results of U-Net_i and DABE-Net_i demonstrates the performance of ARRSEU-Net by the increment of 1.3% in of precision and 2.1% in terms of recall. Finally, the comparison between the results of U-Net and DABE-Net_i indicates that our proposed method raises precision by 2% and recall by 2.7%.</p><p>As shown in Fig. <ref type="figure">6</ref>, our method (last column) can enhance the details of the buildings. Furthermore, the proposed method visibly reduces the interference of aground vehicles and other buildings. To some extent, the method is even able to correct a few mistakenly marked areas in ground-truth labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>In this paper, we proposed a new U-shaped network called DABE-Net to extract buildings from remote sensing images. We developed the SERCNN-block by integrating squeeze-and-excitation operations and RRCNN, which can extract image features more accurately and capture image details and network channel features. Furthermore, the attention gates are introduced into the network to enlarge the network's weight for important features and improve symmetric extension lov Ã¡sz hinge loss function. To enhance the accuracy of the network to small buildings, we increased the attention weights of small buildings and developed multi-scale segmentation loss for the learning process. We used the Mapping challenge competition dataset on CrowdAi. The experimental results show that our network achieved 94.1% precision, 95.2% recall and 94.6% F1-score. Compared with U-Net, it increased 2.3% F1-score. Experimental results support our conclusions and prove the effectiveness of the network. In the future, we will focus on accelerating our method's speed and extending it to handle more complex remote sensing image segmentation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declaration of Competing Interest</head><p>None. Fig. <ref type="figure">6</ref>. Comparison of experimental results of four different models, our method (last column) can enrich enhance the details so that buildings in output have more distinct, margin and small buildings are easier to be detected.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The flowchart of the proposed DABE-Net. It has a U-Net shape CNN structure and includes an encoder and a decoder, where A is the SERRCNN-block, B is the Attention gates, C is the SERRCNN-block with up-sampling operation, D is the up-sampling operation and E is a recurrent convolutional operation.</figDesc><graphic url="image-4.png" coords="2,41.64,55.57,511.61,251.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The proposed SERRCNN-block that combines Squeeze-and-Excitation operations with Recurrent Convolutional Neural Networks on the residual model.</figDesc><graphic url="image-5.png" coords="2,84.16,405.95,426.67,320.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Q</head><label></label><figDesc>.Hu et al.   </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Dataset on CrowdAI Mapping Challenge, In the figure, Row A shows the remote sensing images, Row B shows the masks, Row C shows the ground-truths.</figDesc><graphic url="image-7.png" coords="4,41.64,55.45,511.61,284.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>U-Net: U-Net + original loss function; U-Net_i: U-Net + improved loss function; DABE-Net: DABE-Net + original loss function; DABE-Net_i: DABE-Net + improved loss function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The weights of different buildings. We use the color to represent the weights of buildings, and the smaller buildings have larger weights, the darker the color. Row A shows the masks and Row B displays the weights.</figDesc><graphic url="image-8.png" coords="5,41.64,56.01,511.58,181.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-9.png" coords="6,41.64,55.70,511.60,364.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Ablation study of DABE-Net on the Mapping challenge competition dataset. We compare the baseline and analyze the impact of each module and the combination of modules.</figDesc><table><row><cell>Model</cell><cell>DABE-Net</cell><cell>Improved loss</cell><cell>Precision</cell><cell>Recall</cell><cell>F1-score</cell></row><row><cell>U-Net</cell><cell></cell><cell></cell><cell>92.1</cell><cell>92.5</cell><cell>92.3</cell></row><row><cell>U-Net_i</cell><cell></cell><cell>✓</cell><cell>92.8</cell><cell>93.1</cell><cell>92.9</cell></row><row><cell>DABE-Net</cell><cell>✓</cell><cell></cell><cell>93.1</cell><cell>93.2</cell><cell>93.1</cell></row><row><cell>DABE-Net_i</cell><cell>✓</cell><cell>✓</cell><cell>94.1</cell><cell>95.2</cell><cell>94.6</cell></row></table><note>Q.Hu et al.   </note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The work is partially supported by the Agency for Science, Technology and Research (A*STAR) under its AME Programmatic Funding Scheme (Project No. A18A1b0045).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Simultaneous extraction of roads and buildings in remote sensing imagery with convolutional neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Alshehhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Marpu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Woon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Dalla</forename><surname>Mura</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.isprsjprs.2017.05.002</idno>
		<ptr target="https://doi.org/10.1016/j.isprsjprs.2017.05.002" />
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="139" to="149" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Building extraction from rgb vhr images using shifted shadow algorithm</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2018.2819705</idno>
		<ptr target="https://doi.org/10.1109/ACCESS.2018.2819705" />
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="22034" to="22045" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic segmentation-based building footprint extraction using very high-resolution satellite images and multi-source GIS data</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs11040403</idno>
		<ptr target="https://doi.org/10.3390/rs11040403" />
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">403</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A multi-scale filtering building index for building extraction in very high-resolution satellite imagery</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs11050482</idno>
		<ptr target="https://doi.org/10.3390/rs11050482" />
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">482</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automated regional seismic damage assessment of buildings using an unmanned aerial vehicle and a convolutional neural network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.autcon.2019.102994</idno>
		<ptr target="https://doi.org/10.1016/j.autcon.2019.102994" />
	</analytic>
	<monogr>
		<title level="j">Autom. Constr</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page">102994</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detection of urban damage using remote sensing and machine learning algorithms: revisiting the 2010 Haiti earthquake</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Cooner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Campbell</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs8100868</idno>
		<ptr target="https://doi.org/10.3390/rs8100868" />
	</analytic>
	<monogr>
		<title level="s">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">868</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Fully connected conditional random fields for high-resolution remote sensing land use/land cover classification with convolutional neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs10121889</idno>
		<ptr target="https://doi.org/10.3390/rs10121889" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">1889</biblScope>
		</imprint>
	</monogr>
	<note>Remote Sens</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep learning based oil palm tree detection and counting for high-resolution remote sensing images, Remote Sens</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Cracknell</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs9010022</idno>
		<ptr target="https://doi.org/10.3390/rs9010022" />
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Extracting buildings from true color stereo aerial images using a decision making strategy</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tarantino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Figorito</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs3081553</idno>
		<ptr target="https://doi.org/10.3390/rs3081553" />
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1553" to="1567" />
		</imprint>
	</monogr>
	<note>Remote Sens. 3 (8</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A survey on object detection in optical remote sensing images</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.isprsjprs.2016.03.014</idno>
		<ptr target="https://doi.org/10.1016/j.isprsjprs.2016.03.014" />
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="11" to="28" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic urban building boundary extraction from high resolution aerial images using an innovative model of active contours</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ahmadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Zoej</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ebadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Moghaddam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohammadzadeh</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jag.2010.02.001</idno>
		<ptr target="https://doi.org/10.1016/j.jag.2010.02.001" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Appl. Earth Obs. Geoinf</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="150" to="157" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Comparing supervised and unsupervised multiresolution segmentation approaches for extracting buildings from very high resolution imagery</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belgiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Drȃgut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">¸</forename></persName>
		</author>
		<idno type="DOI">10.1016/j.isprsjprs.2014.07.002</idno>
		<ptr target="https://doi.org/10.1016/j.isprsjprs.2014.07.002" />
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="67" to="75" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Morphological building/shadow index for building extraction from high-resolution imagery over urban areas</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/JSTARS.2011.2168195</idno>
		<ptr target="https://doi.org/10.1109/JSTARS.2011.2168195" />
	</analytic>
	<monogr>
		<title level="j">IEEE J. Select. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="161" to="172" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic segmentation of raw lidar data for extraction of building roofs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Awrangjeb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fraser</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs6053716</idno>
		<ptr target="https://doi.org/10.3390/rs6053716" />
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3716" to="3751" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1145/3065386</idno>
		<ptr target="https://doi.org/10.1145/3065386" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Iformation Processing Systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The Microsoft 2017 Conversational Speech Recognition System</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Alleva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
		<idno type="DOI">10.1109/icassp.2018.8461870</idno>
		<ptr target="https://doi.org/10.1109/icassp.2018.8461870" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="5934" to="5938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Google&apos;s multilingual neural machine translation system: enabling zero-shot translation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00065</idno>
		<ptr target="https://doi.org/10.1162/tacl_a_00065" />
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="339" to="351" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semantic segmentation based building extraction method using multi-source gis map datasets and satellite imagery</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPRW.2018.00043</idno>
		<ptr target="https://doi.org/10.1109/CVPRW.2018.00043" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="238" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Maritime semantic labeling of optical remote sensing images with multi-scale fully convolutional network, Remote Sens</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zou</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs9050480</idno>
		<ptr target="https://doi.org/10.3390/rs9050480" />
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">480</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Towards operational satellite-based damage-mapping using u-net convolutional network: a case study of 2011 tohoku earthquaketsunami</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Koshimura</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs10101626</idno>
		<ptr target="https://doi.org/10.3390/rs10101626" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">1626</biblScope>
		</imprint>
	</monogr>
	<note>Remote Sens</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Weedmap: a large-scale semantic weed mapping framework using aerial multispectral imaging and deep neural network for precision farming</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Popovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lottes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liebisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs10091423</idno>
		<ptr target="https://doi.org/10.3390/rs10091423" />
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">1423</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improved road centerlines extraction in high-resolution remote sensing images using shear transform, directional morphological filtering and enhanced broken lines connection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Debayle</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jvcir.2016.06.024</idno>
		<ptr target="https://doi.org/10.1016/j.jvcir.2016.06.024" />
	</analytic>
	<monogr>
		<title level="j">J. Vis. Commun. Image Represent</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="300" to="311" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Accurate urban road centerline extraction from vhr imagery via multiscale segmentation and tensor voting</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2016.04.026</idno>
		<ptr target="https://doi.org/10.1016/j.neucom.2016.04.026" />
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">205</biblScope>
			<biblScope unit="page" from="407" to="420" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A scale robust convolutional neural network for automatic building extraction from aerial and satellite imagery</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1080/01431161.2018.1528024</idno>
		<ptr target="https://doi.org/10.1080/01431161.2018.1528024" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3308" to="3322" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Detecting building edges from high spatial resolution remote sensing imagery using richer convolution features network</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs10091496</idno>
		<ptr target="https://doi.org/10.3390/rs10091496" />
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">1496</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Large-scale semantic classification: outcome of the first year of inria aerial image labeling benchmark</title>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Audeberr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khalel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tarabalka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Le</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bradbury</surname></persName>
		</author>
		<idno type="DOI">10.1109/IGARSS.2018.8518525</idno>
		<ptr target="https://doi.org/10.1109/IGARSS.2018.8518525" />
	</analytic>
	<monogr>
		<title level="m">IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Symposium</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6947" to="6950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Building extraction in very high resolution imagery by dense-attention networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs10111768</idno>
		<ptr target="https://doi.org/10.3390/rs10111768" />
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1768</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improved fully convolutional network with conditional random fields for building extraction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vanneschi</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs10071135</idno>
		<ptr target="https://doi.org/10.3390/rs10071135" />
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">1135</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A boundary regulated network for accurate roof segmentation and outline extraction</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shibasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs10091496</idno>
		<ptr target="https://doi.org/10.3390/rs10091496" />
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">1195</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2019.2913372</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2019.2913372" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learn to pay attention</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jetley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Lord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1804.02391" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ArXiv</publisher>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deeplab: semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2017.2699184</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2017.2699184" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1409.0473" />
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>ArXiv</publisher>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The influence of the sigmoid function parameters on the speed of backpropagation learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Moraga</surname></persName>
		</author>
		<idno type="DOI">10.1007/3&amp;ndash;540-59497-3_175</idno>
		<ptr target="https://doi.org/10.1007/3-540-59497-3_175" />
	</analytic>
	<monogr>
		<title level="j">International Workshop on Artificial Neural Networks</title>
		<imprint>
			<biblScope unit="page" from="195" to="201" />
			<date type="published" when="1995">1995</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Representation of functions by superpositions of a step or sigmoid function and their applications to neural network theory</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ito</surname></persName>
		</author>
		<idno type="DOI">10.1016/0893-6080(91)90075-G</idno>
		<ptr target="https://doi.org/10.1016/0893-6080(91)90075-G" />
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="385" to="394" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The lovász-softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rannen Triki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00464</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2018.00464" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4413" to="4421" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
