<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Mechanical Engineering</orgName>
								<orgName type="institution" key="instit1">Univer-sity of Victoria</orgName>
								<orgName type="institution" key="instit2">Victoria B.C</orgName>
								<address>
									<postCode>V8W 3P6</postCode>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">945FA1BE55010A2B06062CE33C334D06</idno>
					<note type="submission">received Nov. 22, 1993; revised Apr. 5, 1995. Recommended for acceptance by S. Peleg.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The experiment uses synthesized images containing deformed rectangles as shown in Fig. <ref type="figure">7a</ref>. We deform the boundary on each of the four sides independently using a randomly generated cosine function. Let b(s) represents an undeformed boundary indexed by s E [0, 11. The deformed boundary d(s) is given by: (29)   where V : N ( O , O ~) is a normally distributed boundary deformation process and 4 is a random phase shift uniformly distributed in [0, 2 ~) .</p><p>Large values of av induce large values of deformation variance q, although the exact relationship cannot be determined. A plot of E[c(U)] versus will therefore illustrate the effect of deformation on matching performance.</p><p>The intensity images are generated by setting the pixel value to 1 if it is enclosed by the boundaries, and 0 otherwise. Image noise is then introduced using zero-mean Gaussian white noise of variance a , .</p><p>Fig. <ref type="figure">7a</ref> to d show the plots of E[c(U)] versus deformation with an = 0.1 and 0.3. These experiments confirm that matching performance of rigid template degrades with deformation. In contrast, the g-snake adapts well with deformation to yield high degree of correlation. Furthermore, by comparing the two plots, we observe that g-snake exhibits higher robustness to image noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>d(s) = b(s) + VCOS(W + 4)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>We considered the problems of modeling and extracting arbitrary deformable contours from noisy images. Based on a regenerative shape matrix, our model encompasses both global and local deformations. In addition, it is stable, invariant, and unique. Combined with the Markov random field to model local deformations, this yields invariant a priori distribution that exert influence over an arbitrary global model while allowing for deformation.</p><p>Using the Bayesian framework, the problem of extracting an object with unknown deformation from noisy images tums into MAP estimation. We showed that MAP estimation is equivalent to energy minimization in g-snake. Unlike snake, g-snake is capable of representing any arbitrary shape. We exploited the minimax principle to adaptively determine the optimal regularization when training samples are unavailable or unreliable. Furthermore, we may reliably and efficiently initialize the g-snakes using generalized Hough transform.</p><p>Finally, we demonstrated with experiments how one may apply the proposed g-snake in various applications. Quantitative measures obtained from the experiments confirm that g-snakes yield superior matching performance compared to rigid templates. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vanishing Point Detection by Line Clustering</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Vanishing points are defined as the image points at which the projection of parallel lines intersect. They are invariant features of a scene and have been employed for both qualitative and quantitative image analysis. Qualitatively, vanishing points can be used to group lines for image matching, quantitatively they have been suggested for use in camera calibration, or for solving inverse perspective mappings in structured machine vision applications. This paper presents a new method of detecting vanishing points in digital images which requires little supervision and provides accurate quantitative estimates of vanishing point locations.</p><p>Bamard proposed a methodology in 1981 which has established the paradigm for vanishing point detection [l]. This work was extended by Magee and Aggarwall [2] and Quan 131, using spherical geometry and hierarchical Hough transforms, respectively. More recent work on vanishing point detection begins to acknowledge the existence of errors in image measurements, but reduces this to a method that is sensitive to line lengths only [4]. All of these methods begin by assuming that a set of line endpoint coordinates is known a priori, presumably as the result of some feature extraction process. Based on the set of endpoints, an accumulator space of possible vanishing point locations is constructed (i.e., a Hough transform), usually on a Gaussian sphere. This space is then searched for local maxima, which define the (approximate) location of vanishing points. The general strategy has several shortcomings: 1) Vanishing point detection methods are based upon the detection and location of line endpoints, but use line equations in finding the vanishing points.</p><p>2) The difficulty involved in searching the accumulator space for local maxima when 'noisy' data are present requires that supervision be employed to estimate the number of vanishing points to be located, and to 'cull' the image data so that only a few dominant, well formed (i.e., long) lines are admitted into the vanishing point location process.</p><p>3) The location accuracy is limited by the definition of the accumulator space and may not achieve the true accuracy available from the data itself. 4) All lines are considered of equal value. Each set of line endpoints is used to find a line equation which then contributes on an equal basis with all other lines. Brillault-O'Mahoney <ref type="bibr">[4]</ref> devised a line error model which in the end equated line error with line length, but line length alone is not the only indicator of line quality, or measure of potential measurement errors.</p><p>Our goal for the present work has been to develop a method of vanishing point detection which would reduce the dependence of the technique on arbitrary thresholds, which would integrate the image processing and image analysis aspects of vanishing point detection, and which would recognize a realistic model of line errors. The resulting approach has the following properties:</p><p>1) The vanishing point estimation scheme is based upon a set of line equations as the fundamental features and does not use line segment end-points. 2) Vanishing point detection is cast as a clustering problem and is solved directly in the image plane without the use of any accumulator space techniques. 3) We recognize that the line equations contain errors and include estimates of these in the final vanishing point estimation process. Furthermore, the integration of image processing with the vanishing point detection scheme allows for the development of an error model based on the complete line description (not just the line length).</p><p>This paper presents a novel method of vanishing point detection and illustrates the complete process from initial image processing to final estimation of the number and location of the vanishing points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">PRELIMINARY IMAGE PROCESSING</head><p>The detection of straight line features is a necessary prerequisite for all methods of vanishing point detection. Lines are formed on the basis of grouping connected regions of pixels which share similar gradient orientations, followed by the computation of a line equation which fits the data described by the pixel grouping process, as illustrated in Fig. <ref type="figure" target="#fig_1">1</ref>. This method is equivalent to that described by Burns with the exception that gradient orientations are quantized using histogram analysis rather than the dual quantizatiodvoting scheme which is more common [ 5 ] . In addition to providing local information from which line regions are built up, the global distribution of gradient Orientations is used to initially estimate the number of vanishing points and their orientations. Therefore, the general method of quantizing gradient orientations is critical for the success of the vanishing point detection technique. A general 'N' level histogram quantization scheme is applied to the histogram of gradient orientations <ref type="bibr" target="#b4">[6]</ref>. The gradient magnitude (vu) and orientation (@U) at each pixel (i, J) are computed using 2 x 1 gradient masks, as in the Bums scheme </p><formula xml:id="formula_0">H(n) = Cvii v(i,j)I((n-l)e-x) &lt;av &lt; (ne-z), (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>O &lt; n &lt; k</head><p>A distribution which describes the curvature of the histogram is formed by subtracting a local average of the histogram from the initial histogram function. The local average is computed cyclically to avoid edge effects at orientation extremes:</p><p>Negative going zero crossings of the curvature function are detected to produce a set of thresholds Qj, 0 &lt; j 2 P.</p><p>The orientation thresholds are used to initially estimate both the number and orientation of vanishing points in the image. Each distinct quantization level is described by the average orientation within the quantization level:</p><p>(3)</p><p>The histogram analysis provides relevant information for the subsequent segmentation of the image into line support region data and also provides global estimates of dominant orientations.</p><p>The quantized gradient orientation image is analysed using a connected components analysis resulting in a segmented image of M "line support regions" (LSR) which associate groups of pixels with individual linear features. Each LSR contains a set of K,,, pixel locations and their associated gradient information: Weighting the contribution of each pixel by its gradient magnitude and performing a principal components analysis on the LSR data provides a good and efficient method of obtaining the line equation [7]. The centroid of the line (GI, GJ) is located as the centroid of the gradient weighted LSR position data and the orientation is given by the largest eigenvalue eigenvector A-= [x,, y,,] of the covariance of gradient weighted pixel positions. From these, the equation of the line can be estimated: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="111.">CLUSTERING FOR VANISHING POINT DETECTION</head><p>Given a set of features describing the linear structure in an image, the process of detecting vanishing points requires that the lines be classified into groups which share common vanishing points, followed by a process of estimating the vanishing point location using the line grouping.</p><p>Grouping features around a set of class prototypes and then using these features to update the prototype estimates form the essential components of a clustering scheme. By using a clustering approach, the formulation avoids the resolution limitations implied by accumulator space techniques, as well as eliminating the need to search the accumulator space for local maxima, which can be difficult when the data are noisy. Furthermore, the use of clustering eliminates the requirement of mapping the features onto the Gaussian sphere since there is no longer any requirement to work in a closed space.</p><p>In general, a clustering scheme can be defined with three components:</p><p>1) An initial estimate of the number of clusters and their prototype descriptions (forming initial vanishing point estimates). 2) A method of associating individual features with individual clusters (grouping lines). 3) A method of updating the prototype description of each cluster based on the features which have been assigned to it (updating vanishing point positions).</p><p>Each of the three components of the clustering scheme will be described separately.</p><p>In Section 11, the histogram of gradient orientations was quantized into P distinct orientations, associated with the dominant orientations of linear structure within the image. The average orientation within each quantization level, ij is used to specify vanishing point orientation, but since line orientation does not provide information about line direc-tion the P quantization levels are used to generate 2P initial vanishing points. To convert the orientations into vanishing point locations, all vanishing points are initially located on a circle of arbitrary radius r from the image origin: pi = (rcoskj, rsinQi 7 pp+j = (rcos(&amp;j +IT), rsin(&amp;j +IT))</p><p>(7)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>O &lt; j l P</head><p>Given a set of nK vanishing point estimates and a set of nL line descriptions some criterion for assigning each line to one of the vanishing points is required. The criterion used here is based on a recognition that each line has a unique quality, and computes the angle swept out between a point and a line which is then normalized by a measure of the lines orientation error. The line equation estimate may contain errors in the centroid and orientation estimation, which can be described by the eigenvalues: CENTROID ERROR. L.n is assumed to describe the variation of the data across the line, therefore representing the variance of the centroid estimate perpendicular to the chosen orientation.</p><p>ORIENTATION ERROR. If the across line variance is assumed to be concentrated towards the end of the LSR, then an estimate of the variance of the orientation estimate can be formed:</p><p>When the angle is small, the inverse tangent can be replaced by the quotient of the two eigenvalues.</p><p>Assuming the worst case error, an estimate of the error variance at a distance r from the computed centroid can be constructed:</p><formula xml:id="formula_1">of &lt; ami, + r2 Sinai &lt; Amin + r 2 2 oe (9)</formula><p>The error model therefore defines a fan shaped region extending away from the centroid location: Equal angles indicate equal error regardless of the distance from the centroid.</p><p>The perpendicular distance from a point (xp, yp) to a line (pi, Oi) is computed as</p><formula xml:id="formula_2">(10)</formula><p>This distance can be expressed as an angle and then normalized by the orientation error standard deviation:</p><formula xml:id="formula_3">dbS = xp cos@ + yp sine, -pi</formula><p>For large values of r, d(p,i) describes the normalized angular error between point p and line i. The normalization is based entirely upon the gradient weighted spatial distribution of pixels within the LSR and not on any presupposition about line quality.</p><p>The grouping process assigns each of the nL lines to one of the nk groups so as to minimize the overall normalized angular error, which equates to the assignment of each line to the group for which the normalized angular error is minimal:</p><formula xml:id="formula_4">(12) L~ E K~ fl 2(j,i) &lt; 2(m,i) V m z j go%- ! 0.2- 40.15 p 0.1 f 3 1093 - ~</formula><p>Once the line features have been grouped into sets of lines sharing a common vanishing point, the estimate of the vanishing point location can be updated by computing the point in the image plane which minimizes the sum of normalized angular errors to the lines sharing this common vanishing point. For a group of lines L = {(pi, Oi, GIi, GJi, croi), i = 1, . . ., n,,) associated with a vanishing point p = (xp, y,,) the error is:</p><formula xml:id="formula_5">0 n. / \ / line3 c ( p , L ) = Sipi i=l</formula><p>The point p which minimizes this error function defines the updated vanishing point estimate for the particular group of lines. In practise we use a gradient descent optimization algorithm to locate the position of the vanishing point for a given set of line equations. Using this cost function, each line contributes to the overall updating based on its underlying quality. Fig. <ref type="figure">2</ref> shows the estimated vanishing point location computed using this method for a set of three lines whose intersections form an equilateral triangle. The figure illustrates the movement of the estimated vanishing point location as the quality of the lines is varied, showing that lines with low eigenvalue ratios do not exert significant influence on the overall estimation. This eliminates the need to cull the set of lines used in the detection process to only the strongest of the set, and allows multiple lines of marginal quality to ultimately exert strong influence on the vanishing point location. This is advantageous in situations were there are multiple short colinear line segments. The orientation error is defined as the change in orientation which must be added to a line description in order for the line to pass exactly through the vanishing point:</p><p>x -GIi q p , i ) = t a n -' P -</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GJi -y p Oi</head><p>The median absolute deviation (MAD) of this orientation error is computed for the line set to provide an estimate of the typical angular error. The MAD is a resistant measure of scale and is thus not likely to be affected by atypical lines in the line group [8]: Fig. <ref type="figure">3</ref> shows the MAD for a set of nine lines which intersect perfectly at a vanishing point. Randomly oriented lines of one tehth the quality of the ideal lines are added to the group to simulate the effect of introducing errors into the computation. The MAD for the correct lines shows very little sensitivity to the introduction of the noise lines, even when 33% of the lines are arbitrarily oriented.</p><p>In this section the vanishing point detection problem has been cast in a traditional framework of nearest neighbour clustering, albeit using a nonlinear function to establish cluster membership and update cluster prototypes. During iteration, clusters with very low populations are deleted, and clusters with similar vanishing point locations are merged. These two heuristics are required since the histogram analysis overestimates the number of vanishing points. The overall technique usually converges within three or four iterations, providing accurate vanishing point location estimates which are not limited in resolution by any binning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>The performance of the technique is evaluated using two gray level images which were chosen to demonstrate particular aspects of the scheme's operation. The images and their corresponding line sets are shown in Fig. <ref type="figure" target="#fig_7">4</ref>. Histogram analysis was performed with k = 128 bins and m = 35 for low pass filtering. An initial radius of r = 3,000 was used in forming the preliminary vanishing point estimates for the first grouping process of the clustering. Table <ref type="table" target="#tab_0">I provides</ref>  The door image (Fig. <ref type="figure" target="#fig_7">4b</ref>) demonstrates very well-behaved vanishing point detection. The lines are oriented in two directions: near horizontal and near vertical corresponding to the vertical and horizontal lines of the door frame.  two vanishing points are detected corresponding to the convergence of the desk's horizontal features just to the right of the image boundary, and to the vertical structure in the image. As the LSR size is decreased we first notice that the vertical vanishing point is split into two vanishing points in positive and negative directions along the same almost vertical orientation. As the LSR size is decreased further, a new vanishing point is introduced, which upon inspection of the image, appears to arise from the large number of short horizontal lines corresponding to the venetian blinds and bulletin board in the upper right comer of the image. This last vanishing point is formed entirely from a collection of relatively small lines which would be rejected from most other vanishing point detection schemes.</p><p>To assess the accuracy of the detected vanishing points, a manual vanishing point estimation procedure was employed for the near vanishing point in the desk image. The other vanishing points are located at an infinite distance from the image centre and cannot be manually detected. Table <ref type="table" target="#tab_0">I1</ref> shows the vanishing point locations and MAD values for the automated and the two manual estimates. Note that the automated MAD values are similar to the manually detected vanishing point values. To the extent that we are able to assess accuracy we conclude that our automated method is at least as accurate as carefully executed manual vanishing point detection. The number and quality of the vanishing point estimates varies considerably as the LSR size is changed in the desk image (Fig. <ref type="figure" target="#fig_7">4a</ref>). When only large LSRs are admitted to the vanishing point detection process,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>A new method of vanishing point detection has been presented which integrates the image processing and image analysis aspects of the problem and produces a method which is realistic in the context of practical feature extraction. The use of histogram analysis, clustering, and numerical optimization to locate vanishing points eliminates the need for any a priori estimates of the number or location of vanishing points to be specified. Also, the incorporation of a line quality measure allows large line data sets to be used without sacrificing the overall quality of the vanishing point estimates, further reducing the requirement for supervision. No accumulator space methods are used, eliminating the need to map the problem onto the Gaussian sphere.</p><p>The technique has been demonstrated using both synthetic and real image data. Our results show competent detection with accuracies that are at least as good as the accuracy attained when manual drafting techniques are used. No user specified parameters are required beyond the initial image processing for line finding, and the results show low sensitivity to variations in the number of lines admitted to the location process.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>G.F. McLean and D. KotturiAbstract-This paper presents a new method for the detection of vanishing points based on sub-pixel line descriptions which recognizes the existence of errors in feature detection and which does not rely on supervision or the arbitrary specification of thresholds. Image processing and image analysis are integrated into a coherent scheme which extracts straight line structure from images, develops a measure of line quality for each line, estimates the number of vanishing points and their approximate orientations, and then computes optimal vanishing point estimates through combined clustering and numerical optimization. Both qualitative and quantitative evaluation of the algorithms performance is included in the presentation.Index Terms-Image analysis, perspective, vanishing point, lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The relationship between line support region data and the line equation. The eigenvector corresponding to the largest magnitude eigenvalue describes the orientation of the detected line. Eigenvalues indicate the variance along and across the detected line, eventually used as a measure of line quality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>[ 5 ] . The histogram of weighted gradient orientations is computed by selecting an angular step size 8 and choosing a set of k bins, where k = %. The histogram bins the distribution of orientations:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 17, NO. 11, NOVEMBER 1995</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>p, = xcos(e,)+ysin(e,) p, = GI, cose, + GJ, cose, Finally, the eigenvalues of the covariance matrix(k, &amp;, , ) are retained to provide an estimate of the overall quality of the line. The ratio of the eigenvalues (k) can be used to determine the overall elongation of the LSR data, directly indicating the quality of the line, which is not necessarily correlated with line length. This method of line detection provides accurate estimates of line equations[7] and requires no parameter specification once the orientation histogram has been quantized. By using the N-level histogram quantization, the line extraction can proceed using a single connected component analysis without the requirement for any voting process. The resulting features are a set of nt line descriptions: L = (L, = (pi, e,, ai, G J ~, ammi, amini)i = 1, ..., nL}(6)   </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Fig. 2. Synthetic example showing the effect of line quality on the vanishing point estimate. Three lines whose mutual intersection forms an equilateral triangle are plotted. When the lines are equally weighted the vanishing point estimate lies at the center of the triangle. As any individual line is preferentially weighted the VP estimate is pulled toward that line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Two images used to demonstrate the viability of unsupervised vanishing point detection and the corresponding set of detected line equations. The intensity of the lines in b, d ate propottional to the measured tine quality-brighter tines are of relatively higher quality than darker ones. a) Door frame. c) Desk.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I VANISHING POINT DETECTION RESULTS FOR TEST WAGES</head><label>I</label><figDesc></figDesc><table><row><cell cols="5">Door Image Y I 13 1 3662.0 379.0 Minimum I Total I LSR size I #lines I x 500 I -398.4 -3510.1 .0015 VP MAD .OW4 300 I 18 I 3972.4 399.7 ,0006</cell><cell>#lines 5 8 8</cell></row><row><cell></cell><cell></cell><cell>-415.0</cell><cell>-3579.9</cell><cell>.0016</cell><cell>10</cell></row><row><cell></cell><cell>18</cell><cell>3972.4</cell><cell>399.7</cell><cell>. OOO6</cell><cell>8</cell></row><row><cell></cell><cell></cell><cell>-415.0</cell><cell>-3579.9</cell><cell>.0016</cell><cell>10</cell></row><row><cell></cell><cell>25</cell><cell>3928.7</cell><cell>394.0</cell><cell>,0068</cell><cell>14</cell></row><row><cell></cell><cell></cell><cell>-414.2</cell><cell>-3576.1</cell><cell>.0022</cell><cell>11</cell></row><row><cell></cell><cell>32</cell><cell>3928.7</cell><cell>394.0</cell><cell>.0068</cell><cell>14</cell></row><row><cell></cell><cell></cell><cell>-410.5</cell><cell>-3562.5</cell><cell>,0026</cell><cell>18</cell></row><row><cell cols="3">si size I #lines I x 300 I 12 I 792.2</cell><cell>Y 149.3</cell><cell>MAD ,0034</cell><cell>#lines 6</cell></row><row><cell></cell><cell>12</cell><cell>423.6</cell><cell>-4620.4</cell><cell>,0015</cell><cell>6</cell></row><row><cell>200</cell><cell>19</cell><cell>845.5</cell><cell>143.6</cell><cell>.0154</cell><cell>10</cell></row><row><cell></cell><cell></cell><cell cols="3">-1.86e+15 -3.1419e+17 ,0036</cell><cell>9</cell></row><row><cell>100</cell><cell>32</cell><cell>843.0</cell><cell>144.6</cell><cell>.0156</cell><cell>14</cell></row><row><cell></cell><cell></cell><cell cols="2">-6.55e+ 15 -2.91e+ 18</cell><cell>.0016</cell><cell>9</cell></row><row><cell></cell><cell></cell><cell cols="2">8.74e+17 5.72e+19</cell><cell>,0058</cell><cell>9</cell></row><row><cell>50</cell><cell>73</cell><cell>1027.1</cell><cell>69.1</cell><cell>,0040</cell><cell>8</cell></row><row><cell></cell><cell></cell><cell cols="2">-7.05e+15 -9.39e+17</cell><cell>.0073</cell><cell>34</cell></row><row><cell></cell><cell></cell><cell cols="2">3.08e+15 4.62e+13</cell><cell>.0156</cell><cell>11</cell></row><row><cell></cell><cell></cell><cell cols="2">-1.68e+17 6.21e+15</cell><cell>.0129</cell><cell>20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 11 MANUALLY DETECTED VANISHING POINTS</head><label>11</label><figDesc></figDesc><table><row><cell>Image detection Min I 1 1 ; E 1 L E S I VP</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The work was supported by National Computer Board, Singapore, and Sin0 Software Research Center, Hong Kong.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported by the Natural Science and Research Council of Canada and by the Advanced Systems Institute of British Columbia.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Constraint Learning Feedback Dynamic Model for Stereopsis</head><p>Amol Bokil and Alireza Khotanzad </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The computational problem of determining depth from stereopsis (two images of the same scene taken from horizontally separated viewpoints) has generally been considered as a two-part process. First, features in the left and the right images are matched to one another, and second, the disparity between the locations of corresponding features in the two views is used to calculate the three-dimensional location of the features. While the calculation of depth from disparity is straightforward, the matching of the features in the two images, called the corre- spondence problem, has proved to be difficult. This is an underconstrained problem and often a set of simplifying assumptions in the form of imposed constraints are considered in order to develop a computationally feasible model. However, the implementation of these constraints requires the selection of many free parameters in an ad hoc manner.</p><p>There are many computational approaches to solving the correspondence problem [I], [2]. Prominent among the constraints used in these models are those originally proposed by Marr and Poggio [7] namely: 1) uniqueness-any feature in one image of a stereo pair can have at most one match in the other image, 2) smoothness (continuity)-3D surface depths change smoothly with abrupt changes happening at a few places such as object boundaries, and 3) compatibility of matches.</p><p>Constructing an explicit representation of the first two constraints gives rise to excitatory and inhibitory regions which yield positive and negative feedback respectively. No systematic approach to determine the optimal feedback has been proposed.</p><p>In this paper, we present a dynamic feedback model inspired by the model introduced by Marr and Poggio [7] along with a gradient descent approach to determine automatically the optimal values of model feedback without explicitly imposing the uniqueness and continuity constraints. Through a training process, the appropriate constraints are learned and coded in the resulting feedback. The gradient descent learning algorithm is a modification of a generalization of backpropagation learning for recurrent neural networks developed in [ 1 11. In </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Manuscript received July</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">lll-posed problems and regularization analysis in early vision</title>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AARPA Image Understanding Workshop</title>
		<meeting>AARPA Image Understanding Workshop</meeting>
		<imprint>
			<date type="published" when="1981">1984. 1981</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="111" to="122" />
		</imprint>
	</monogr>
	<note>Generalizing the Hough transform to detect arbitrary shapes</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Snakes: Active contour models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Witkin</surname></persName>
		</author>
		<author>
			<persName><surname>Terzopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. First Int&apos;l Con5 Cmputer Vision</title>
		<meeting>First Int&apos;l Con5 Cmputer Vision</meeting>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="259" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Feature extraction from faces using deformable templates</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Halliman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR 1989</title>
		<meeting>CVPR 1989</meeting>
		<imprint>
			<biblScope unit="page" from="104" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Boundary finding with parametrically deformable models</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Staib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Duncan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattem Analysis and Machine Intelligence</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bayesian method for the use of implicit polynomials and algebraic invariants in practical computer vision</title>
		<author>
			<persName><forename type="first">J</forename><surname>Subrahmonia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curves and Surfaces in Computer Vision and Graphics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive elastic models for hand-printed character recognition</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Revow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Moody</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="512" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Affine-invariant contour tracking with automatic control of spatiotemporal scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Curwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Con$ Computer Vision</title>
		<meeting>Int&apos;l Con$ Computer Vision</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On regularization, formulation, and initialization of the active contour models (snakes)</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">F</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Chin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Con$ Computer Vision</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Using dynamic programming for solving variational problems in vision</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Weymouth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="855" to="867" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
	<note>Computer Vision, Graphics, Image Processing</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On the sensitivity of the Hough transform for object recognition</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Srimson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattem Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="255" to="274" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recognizing 3D objects from 2D images: An error analysis</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Grimson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Alter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR 1992</title>
		<meeting>CVPR 1992</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="316" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Determining vanishing points from perspective images</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Bamard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Magee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, Graphics, and Image Processing</title>
		<imprint/>
	</monogr>
	<note>Interpreting perspective images</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Determining perspective structures using hierarchical Hough transform</title>
		<author>
			<persName><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mohr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="289" to="300" />
			<date type="published" when="1989">1989. 1991</date>
		</imprint>
	</monogr>
	<note>CVGIP: Image Understanding</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Extracting straight lines</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Bums</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Riseman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="425" to="255" />
			<date type="published" when="1986-07">July 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An amplitude segmentation method based on the distribution function of an image</title>
		<author>
			<persName><forename type="first">S</forename><surname>Boukharouba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rebordao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Wendel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, Graphics, andlmage Processing</title>
		<imprint>
			<date type="published" when="1985">1985</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="47" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hierarchical clustering for automated line detection</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Mclean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Prescott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kotturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Pacij7c Rim Con$ Computers, Communications, and Signal Proces-sing</title>
		<meeting>IEEE Pacij7c Rim Con$ Computers, Communications, and Signal es-sing</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="244" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Mosteller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Tukey</surname></persName>
		</author>
		<title level="m">Data Analysis and Regression</title>
		<meeting><address><addrLine>Reading, Mass</addrLine></address></meeting>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1977">1977. 1983. 1984</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="256" to="267" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
