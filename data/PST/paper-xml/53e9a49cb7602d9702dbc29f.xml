<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Tolerating Processor Failures in a Distributed Shared-Memory Multiprocessor Computer Architecture Lab at Carnegie Mellon (CALCM)</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Brian</forename><forename type="middle">T</forename><surname>Gold</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Architecture Laboratory (CALCM)</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Architecture Laboratory (CALCM)</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">James</forename><forename type="middle">C</forename><surname>Hoe</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Architecture Laboratory (CALCM)</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Tolerating Processor Failures in a Distributed Shared-Memory Multiprocessor Computer Architecture Lab at Carnegie Mellon (CALCM)</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2B982DF7B68D521784954B3229F94816</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scaling transistor geometries and increasing levels of integration lead to rising transient-and permanent-fault rates. Future server platforms must combine reliable computation with cost and performance scalability, without sacrificing application portability. Processor reliability-for both transient and permanent faults-represents the most challenging aspect of designing reliable, available servers.</p><p>In this paper, we investigate redundant operation of physically-distributed processors in the context of a distributed shared-memory (DSM) multiprocessor. Our design-LACROSS-addresses two key challenges, namely the coordination and the corroboration of decoupled, redundant processors. Evaluations based on a range of commercial and scientific workloads show that the average error-free performance overhead of LACROSS is within 5.8% (worst case 13%) of a baseline system.</p><p>We evaluate LACROSS with cycle-accurate full-system simulation of commercial and scientific workloads. The three main contributions of this paper are:</p><p>• Lockstep coordination -hardware mechanisms and protocols that maintain lockstep execution across decoupled processing modules separated by a scalable interconnect.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Many of the day-to-day digital services that we now take for granted, from accounting and commercial transactions to residential utilities, rely on available and reliable information processing and storage. Server reliability is already a key pillar for e-commerce where downtime directly impacts revenues and can cost greater than $6M per hour for availability-critical services <ref type="bibr" target="#b29">[30]</ref>. With the proliferation of digital services, reliability will become a key design metric for future server platforms.</p><p>While server availability and reliability are becoming crucial, there are a number of obstacles to designing, manufacturing, testing and marketing reliable server platforms <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b29">30]</ref>. One key obstacle is the drastic reduction in reliability in future gigascale CMOS technologies and circuits <ref type="bibr" target="#b6">[7]</ref>. Not only will these circuits exhibit high transient error rates <ref type="bibr" target="#b26">[27]</ref> (due to particle radiation), but also they will incur prohibitive rates of intermittent or permanent failure due to a myriad of factors including (but not limited to) manufacturing difficulties at gigascale level <ref type="bibr" target="#b7">[8]</ref>, thermally-induced or time-dependent transistor and interconnect degradation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b37">38]</ref>, and ineffectiveness of "burn-in" to mitigate chip infant mortality <ref type="bibr" target="#b39">[40]</ref>. Moreover, permanent failures can affect transistors, circuits, or entire chips-requiring solutions at the system level to achieve acceptable server reliability and availability.</p><p>Unfortunately, current solutions to server reliability suffer from a number of shortcomings. Tightly-coupled shared-memory mainframes-such as IBM's z-series <ref type="bibr" target="#b33">[34]</ref>-only recover from transient errors and intra-chip failure. As such, these designs preclude transparent recovery from a permanent chip failure, which will be needed in future gigascale integrated systems. Moreover, the use of custom processor cores and shared backplanes limit both cost and performance scalability in such designs.</p><p>In contrast, distributed memory solutions-such as HP/Tandem NonStop <ref type="bibr" target="#b5">[6]</ref>-tolerate permanent processor chip failures through redundant execution across chips. Moreover, these designs rely on commodity processor chips and multistage interconnects and as such offer both cost and performance scalability. Unfortunately, they also sacrifice software transparency by requiring a message-passing programming interface and as such preclude running conventional sharedmemory multiprocessor workloads.</p><p>In this paper, we investigate processor fault-tolerance in server architectures based on cache-coherent distributed shared memory (DSM). DSM is an attractive architecture because it offers a software-compatible scalability path beyond today's dominant commercial server architectures-chip and symmetric multiprocessors. By relying on commodity processor cores and multistage interconnects as building blocks, DSM offers an inherently cost-and performance-scalable design point. Highly-integrated DSMs <ref type="bibr" target="#b10">[11]</ref> also enable high-bandwidth and low-latency paths to both DRAM memory and neighboring nodes, further reducing system-level "glue" logic and improving complexity, cost and performance.</p><p>We make the observation that DSM is a natural platform for "node-level" dual-modular redundancy (DMR), where two physically-distributed processors execute an identical instruction stream. Unlike conventional DMR approaches that rely on tightly-coupled lockstep, node-level redundancy faces two key challenges: the DMR processor pair must (1) observe identical external inputs to maintain a redundant instruction stream; and (2) corroborate results under the limited bandwidth and nonnegligible latency of the interconnect.</p><p>We address these challenges with Lockstep Across System (LACROSS), a set of hardware mechanisms extending a baseline DSM to pair processors on different DSM nodes for dual modular redundant execution. LACROSS relies on a previously-proposed lightweight state comparison mechanism <ref type="bibr" target="#b34">[35]</ref> and checkpointing across DMR pairs to detect and recover from transient error in processor cores. Node-level redundancy enables LACROSS to survive not only circuit-and microarchitecture-level failure, but also the complete loss of a processor chip.</p><p>• Error detection and recovery -support for DMR error detection and recovery despite bandwidth and latency constraints of the interconnect.</p><p>• Early-release performance optimizations -a set of optimizations that significantly reduce the performance overhead induced by distributing redudancy. Our study shows that this performance overhead can be reduced to less than 5.8% on average (13% in the worst case) for a range of commercial and scientific workloads.</p><p>Paper Outline. This paper is organized as follows. In the next section, we provide background on distributed DMR in a DSM and elaborate on the key obstacles. We present the details of LACROSS coordination in Section 3 and describe error detection and recovery protocols in Section 4. Section 5 presents optimizations to remove common-case performance overheads. We evaluate the system's performance in Section 6 using cycle-accurate, full-system simulation. Section 7 presents related work, and we conclude in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>Figure <ref type="figure" target="#fig_0">1</ref> depicts the anatomy of the cache-coherent DSM multiprocessors we study in this paper. Each node consists of a number of processor cores with their cache hierarchies, a memory controller scheduling accesses to local DRAM memory, and a node controller implementing a full-map directory cache-coherence protocol and the network interface and router logic to extend the local shared-memory abstraction across DSM nodes <ref type="bibr" target="#b25">[26]</ref>. In a DSM, the inherent mirroring of physical resources across system nodes provides a natural platform for redundant execution. In this paper, we assume the nodes to be dual-core chip-multiprocessors (CMPs), although the ideas presented here extend to an arbitrary number of cores per node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Membrane: Isolating Faults in DSM</head><p>To reason about faults in DSM servers, recent work introduced Membrane <ref type="bibr" target="#b17">[18]</ref>, a decomposition framework that abstracts the components in a large-scale server into protection domains. Membrane requires that errors occurring within a protection domain are detected and corrected before propagating to the rest of the system.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> illustrates the Membrane decomposition used in this paper (excluding peripheral devices). Errors in instruction execution must be detected and corrected before propagating to DRAM. Cache-coherent data transfers from one processor to another must cross from the processor domain into the memory and interconnect domains. Because Membrane requires error detection and correction at each domain boundary, errors cannot propagate from one processor to another.</p><p>Membrane does not specify nor provide the error detection and correction mechanisms. Rather, the key enabler in this framework is that each domain can employ an optimized error detection and local recovery strategy. Although the CMP contains components from three Membrane protection domains, failure of the entire chip is orthogonal to where the Membrane boundaries are drawn. The fundamental requirement remains: components must detect and recover from errors within their domain boundary.</p><p>In this paper, we address failures in the processor domain, both transient and permanent. Recent work proposed parity-based error detection and recovery for soft errors in memory <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b31">32]</ref>, and industry uses various techniques <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12]</ref> to protect from permanent failures in DRAM and related circuitry. Membranecompatible techniques to protect other elements of the DSM (e.g., interconnect <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b36">37]</ref>, I/O devices <ref type="bibr" target="#b30">[31]</ref>, and node controllers <ref type="bibr" target="#b40">[41]</ref>) are beyond the scope of this paper.</p><p>Fault Model. Our fault model addresses the vulnerability in existing processor designs for both transient and permanent faults. We assume single event upsets, due to cosmic rays and alpha particles, can manifest in soft errors that disrupt execution but do not permanently damage the hardware. Permanent faults, however, can damage any portion of the chip, including single transistors, circuits, or the entire chip itself.</p><p>In the case of soft errors, we assume the processor pipeline is vulnerable from fetch to retirement. Retired machine statearchitectural register values and on-chip caches-are protected through information redundancy (e.g., ECC). With respect to permanent failure, we assume only one chip fails at a time.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">DMR and Distributed DMR</head><p>Reliable mainframes employ dual-modular redundancy (DMR) at various levels, ranging from replicating pipelines within the same die <ref type="bibr" target="#b33">[34]</ref> to mirroring complete processors on a single system board <ref type="bibr" target="#b5">[6]</ref>. In a conventional DMR design, computation is performed redundantly by duplicated hardware operating in lockstep.</p><p>To ensure the correctness of the processing module in a DSM, one could envision modifications pairing the two cores of a chip-multiprocessor for redundant computation <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b27">28]</ref>. Figure <ref type="figure">3</ref>(left) illustrates such an organization. The DMR core pairs, driven by a common clock and bus, would be presented with external inputs-cache line fills, coherence requests, interrupts, etc.-at precisely the same time. Provided the two cores making up the DMR pair are deterministic in operation, the correct external outputs generated by the two cores must mirror each other exactly in value and timing. Thus, a comparison circuit can check the output of the two cores to detect an erroneous output (mismatch in value or timing). Provided an error is detected soon enough, the execution can be recovered to an earlier, known-good state and restarted.</p><p>However, given the goal of surviving a node failure, the cores constituting a DMR pair must reside on different nodes. Figure <ref type="figure">3</ref>(right) suggests a possible distributed-DMR pairing of processing cores from different nodes. By distributing the redundant pair across nodes in the system, we are faced with two new fundamental problems:</p><p>(1) we cannot guarantee simultaneous delivery of external inputs to both cores of the DMR pair, and</p><p>(2) the two cores can only exchange information at a latency and bandwidth dictated by the interconnect.</p><p>In Section 3, we present an asymmetrical lockstep design that obviates the need for simultaneous delivery. In Sections 4 and 5, we present an error-detection technique and associated optimizations that minimize the performance overhead of distributed DMR despite the substantial communication cost between the partner cores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">COORDINATION IN LACROSS</head><p>When the DMR modules are situated on the same speciallydesigned chip or motherboard, DMR modules can achieve lockstep operation by sharing a common clock and using a common bus to receive input stimuli in precise synchrony. However, this direct approach to enforce lockstep is infeasible in a distributed DMR arrangement. Instead, our solution, LACROSS, takes an asymmetrical approach to lockstepping where true simultaneity is not needed.</p><p>For the sake of discussion, first assume the DSM nodes operate on a globally synchronous clock. (In Section 3.1, we explain how this assumption can be relaxed.) The node controller on each node maintains a local timestamp for each core of the processing module. We require the execution of one core (the slave) in a DMR pair be delayed by a fixed lag relative to the other core (the master). In other words, timestamps of the master and slave advance synchronously but differ by a constant amount.</p><p>External inputs (including all incoming cache coherence activity, cache line refills, uncached load values, asynchronous inputs and interrupts) are only directed toward the leading master of a DMR pair, which appears as a single, logical processor to the system. These inputs can arrive at the master freely without extraneous requirements on delivery time. The delivery time of the external input at the master is recorded and forwarded together with the input value to the slave's node controller in a coordination message. Provided the coordination messages always arrive early enough (i.e., before the slave core reaches the necessary delivery time), the perception of lockstep between master and slave is achieved by delivering the forwarded input to the slave at the correct corresponding local delivery time (illustrated in Figure <ref type="figure">4</ref>).</p><p>The fixed master-to-slave delay must be sufficiently long to permit worst-case transit time of the coordination message between the master and slave in a distributed DMR pair. To bound worst-case transit time, we require coordination messages to travel on the highest-priority channel of the interconnect (cannot be blocked by other message types of lower priority). Tighter bounds can be achieved by arranging master and slaves to be neighbors in the network and/or providing dedicated links for master-slave coordination exchanges. Due to network variations, coordination messages can arrive ahead of time at the slave. Therefore, a gated delivery queue needs to hold the coordination messages until the designated delivery time. If network messages can arrive out-of-order, then the gated delivery queue must also support sorting of coordination messages by delivery time.</p><p>To complete the illusion of a single logical processor, only one core of the DMR pair should emit external outputs to the rest of the system. In the simplest manifestation, all outputs generated by a master are intercepted by its node controller and forwarded to its slave. A master's output is compared against the corresponding output from the slave, and a single verified output is released to the rest of the system. This scheme is confluent with our requirement to verify the DMR outputs, as explained in the next section. However, this approach can interfere with normal interconnect flow control. In many flow control disciplines, a node is not allowed to receive a message unless it is assured that it can send a message in response to the received message. Because sending and receiving (on behalf of a logical node) are decoupled in LACROSS, the receiving master cannot directly sense network backpressure at the slave. The solution is a simple credit-debit system where the slave periodically informs the master of available space in its send buffers so the master knows, conservatively, when to stop receiving.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Distributed clock control</head><p>A globally synchronous clock in a distributed system is undesirable for many reasons and not strictly required by LACROSS. The only requirement in LACROSS is that the locally generated clocks at the master and slave do not drift to the point that the lag between the master and slave is insufficient to cover the worst-case transit latency of the coordination message. This condition can be detected when coordination messages begin to arrive at the slave too close to the delivery time minus some safety margin. In these cases, frequency control mechanisms such as down-spread spectrum modulation <ref type="bibr" target="#b20">[21]</ref> are needed to actively slow down the slave clock to rebuild the master-toslave delay. If necessary, large clock frequency adjustments (e.g., for thermal or power throttling) must be explicitly prepared with software assistance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">FAULT ISOLATION IN LACROSS</head><p>LACROSS provides redundant computation in the form of lockstepped processors and caches; however, LACROSS must also locally detect and recover from soft errors and permanent faults. In this section, we discuss the integration of error detection into a cache coherence protocol, provide a protocol for recovery from various faults, and suggest a lightweight checkpointing solution for saving processor and cache state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Soft error detection and recovery</head><p>We assume all on-chip architectural state (caches, architectural registers, condition codes, etc.) is protected from soft error.</p><p>Recent work <ref type="bibr" target="#b34">[35]</ref> proposed fingerprinting as a mechanism to detect transient errors in microarchitectural state and processor logic across a DMR pair. Each fingerprint consists of a hash of updates to architectural state. A single fingerprint captures the program execution up to the most recent committed instruction, and matching fingerprints across a DMR pair provides a high probabilistic guarantee that no soft error has occurred.</p><p>Because each fingerprint is a small, 16-bit hash, frequent comparisons do not impose significant network bandwidth overheads.</p><p>To isolate errors to a single DMR pair, we require a fingerprint comparison before any change is made to external architectural state. In a simple implementation, any outgoing request or reply would come from the slave after comparing fingerprints to ensure the outgoing message was free from error. In Section 5, we examine performance optimizations that relax these simple requirements.</p><p>Because the slave operates behind the master in LACROSS, a natural point for error detection is at the slave, as illustrated in Figure <ref type="figure" target="#fig_3">5</ref>(left). The master processor sends its slave a fingerprint and the slave responds with an acknowledgement indicating the master's fingerprint matched its own. If the slave detects a mismatch in fingerprints, it initiates recovery.</p><p>Figure <ref type="figure" target="#fig_3">5</ref>(right) illustrates the recovery protocol, where each DMR pair relies on a checkpoint of prior, correct state as the rollback-recovery point. Each fingerprint comparison verifies execution up to that point as correct; therefore, only a single checkpoint must be kept. Because the slave compares fingerprints for the pair, it also keeps the checkpoint for the pair. In the infrequent case of a fingerprint mismatch, the slave sends the checkpoint information to the master. The slave proceeds by restoring its state from the checkpoint and then waits for a signal from the master to resume execution.</p><p>Because every fingerprint comparison results in a new checkpoint and a fingerprint accompanies any irreversible modification to system state, only modifications to the processor state (registers, control codes, etc.) and on-chip caches need checkpointing. We discuss a low-overhead checkpoint implementation in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Permanent fault detection and recovery</head><p>Permanent faults come in a variety of forms, ranging from single-transistor faults in the processor logic to the complete loss of a node. The system detects these faults by either observing repetitive fingerprint mismatches or detecting a timeout on fin- gerprint or fingerprint acknowledgment. Because each node is built from a dual-core CMP, failure of an entire node affects two DMR pairs. Failure of one core in a node affects just one of the DMR pairs.</p><p>Two possible recovery actions exist. First, the functional member of each affected pair can continue to operate in a nonredundant mode until the failed node is replaced. This approach reduces coverage, exposing the system to further soft errors or permanent faults, but only requires additional hardware in the event a failure occurs. The second approach mitigates the loss of coverage by providing hot-spare DMR pairs that are automatically brought online to replace failed pairs. Whether the failed processor is a master or slave in the pair has no bearing on recovery from a permanent fault-the functional member of each pair copies its current state to the replacement pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Checkpoint creation</head><p>Recent interest in extending speculation support and enhancing reliability in microarchitecture has sparked a number of proposals for checkpoint mechanisms that recover beyond retirement <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37]</ref>. In general, our approach can work with any of these checkpoint mechanisms, provided the performance overhead of frequent checkpoint creation is negligible.</p><p>In this section, we suggest an example of a low-overhead recovery mechanism. In this approach, every checkpoint consists of a copy of the architectural registers and a log of previous cache values that were overwritten since the last checkpoint. Recent microarchitectures <ref type="bibr" target="#b16">[17]</ref> maintain an architectural register file (ARF) of committed state. The ARF can be implemented as a redundant register file (RRF) structure to hold a checkpoint of architectural state by adding a shadow cell to each bit in the ARF SRAM <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14]</ref>. When a new checkpoint is created, the most recent value is flash-copied into the backup copy across the entire RRF structure, creating a checkpoint of the architectural registers in a single cycle.</p><p>Because each slave keeps a single checkpoint and creates new checkpoints frequently, a simple approach to cache checkpointing performs a copy-on-write in the cache on every store (as in previously-proposed mechanisms <ref type="bibr" target="#b15">[16]</ref>). The previous value of the cache block is recorded in a FIFO checkpoint log, which, in the event of recovery, the slave replays in reverse to recreate the cache state. By executing ahead of the slave, the master keeps track of the FIFO log size and forces a fingerprint comparison before the log fills up. To remain in lockstep, the master and slave must both 'create checkpoints'-that is, the master must perform the copy-on-write operations in the cache to reproduce any minor performance overheads the slave incurs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">PERFORMANCE OPTIMIZATIONS</head><p>The basic design for fault isolation in Section 4 has the master receive all incoming traffic and the slave send all outgoing requests or replies. In this way, all interprocessor communication experiences an extra latency corresponding to the masterslave delay. If this extra delay is manifested on the critical path of computation, distributed redundant processing can result in a very significant performance overhead. In this section, we consider scenarios where a master processor can safely release outgoing requests or replies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Relaxing read requests</head><p>Our first optimization takes advantage of the fact that not all outputs of the processor have an irreversible effect on system state. For example, a read request for a shared copy of a memory location does not alter the system state irreversibly. In these cases, the master still forwards the outbound request to the slave for corroboration but can at the same time release the request directly. If corroboration fails at the slave, the erroneous outputs either have no ill effect or can be reversed silently. For example, the shared read request to an erroneous address will cause the directory controller to respond with a shared copy of the cache block; however, the system loses no data as the requester's cache and the directory retain the most up-todate value.</p><p>On the other hand, outputs that alter the server's system state must be verified as error-free before being released. A conceptually-simple case is non-idempotent loads or stores that modify peripheral devices (physical IOs). These requests cannot be released without corroboration, and the master-to-slave latency will be added to the critical path request latency. Accessing peripheral devices is slow-even relative to interprocessor communication-therefore, we expect little overhead from delaying these physical IOs.</p><p>A more subtle example consists of a request for a writable copy, which must be verified because such requests transfer sole ownership of a memory block to the requester. Improper handling leads to data loss. Fortunately, the additional latency in requesting a writable copy often does not impact the critical path because many mechanisms contribute to hide store latencies (e.g., relaxed memory models <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b42">43]</ref>).</p><p>The scenario that is most likely to impact performance is when a node has to respond to a read request for a dirty cache line. This response's output must be corroborated before being released by the slave. The extra delay in this request-andresponse turnaround is reflected directly in the load latency of the requester and has a good chance of contributing to the overall critical path. This is often the case in commercial workloads, including online transaction processing (OLTP) and web servers, where migratory access patterns <ref type="bibr" target="#b4">[5]</ref> result in frequent dirty reads, and low memory-level parallelism (MLP) <ref type="bibr" target="#b8">[9]</ref> exacerbates the effect of the master-slave corroboration delay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Validation filter</head><p>The impact of the master-to-slave delay on requests for dirty cache blocks can be avoided with the help of a validation filter, a mechanism similar in spirit to <ref type="bibr" target="#b24">[25]</ref>. This hardware mechanism leverages the fact that, in most cases, the last store (production) comes long before the consuming read by another processor <ref type="bibr" target="#b35">[36]</ref>. During this dead time, if the master and slave have compared even one fingerprint, then the dirty value in the master's cache can be declared error-free. Therefore, when the consuming read requests the dirty data, the master should respond directly without waiting for the slave. However, a more recently-written dirty cache location may still be pending fingerprint acknowledgement when a request for that block arrives. In these cases, the master must forward the requested dirty cache block value to the slave for corroboration and release. The validation filter is the mechanism at the master to help determine if a read request to a dirty cache block can bypass slave corroboration.</p><p>The validation filter tracks cache blocks that have been written but still await fingerprint acknowledgement. As shown in Figure <ref type="figure" target="#fig_4">6</ref>, the filter is divided into regions corresponding to checkpoints pending slave acknowledgement. The validation filter allows fully associative lookup for the presence of a block by checkpoint regions.</p><p>The validation filter handles three events: (1) a new store from the processor, (2) a snoop on behalf of a remote dirty read request, and (3) a fingerprint acknowledgement from the slave.</p><p>1. When a store is written to the master cache, its address is also added to the most recent checkpoint region if its cache block is not already present in that region.</p><p>2. When a snoop request arrives, the validation filter checks for the presence of the cache block in any outstanding checkpoint region. If present, the response to the dirty read request must be forwarded to the slave for corroboration before release.</p><p>3. A fingerprint acknowledgement returned by the slave causes the validation filter to clear the blocks in the oldest checkpoint region.</p><p>Because of the need for fully-associative lookup, the validation filter capacity is limited (e.g., 64 entries), depending on available timing budget. In the event the validation filter overflows, the master can no longer properly distinguish between dirty cache blocks that are known error-free versus those pending fingerprint acknowledgement. After an overflow, the master must assume any request to a dirty cache block is to an unvalidated block and forwards the response on to the slave.</p><p>To resynchronize after overflow, the master first clears the filter and then waits for the next checkpoint interval to start before logging new stores again. The validation filter resumes normal snoop handling only after all fingerprints sent prior to the overflow have been acknowledged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EVALUATION</head><p>We evaluate the error-free performance of LACROSS with FLEXUS, a cycle-accurate full-system simulator <ref type="bibr" target="#b19">[20]</ref>. We model a 16-node DSM running Solaris 8, where each dualcore node contains two speculative, 8-way out-of-order superscalar processor cores and an on-chip private cache hierarchy.</p><p>Our performance evaluations compare with a DSM of tightly lockstepped DMR pairs where each CMP operates as one redundant pair as illustrated previously in Figure <ref type="figure">3</ref>. This baseline has the same infrastructure cost as our distributed DMR approach, but removes the overhead due to distributing the lockstepped pairs. We use this baseline solely for performance comparison-the tightly-coupled pairs cannot tolerate node loss.</p><p>Directory state at each node is maintained with a microcoded controller, and the nodes communicate through a NACK-free, 3-hop cache-coherence protocol derived from Piranha <ref type="bibr" target="#b3">[4]</ref>. We model an interconnect based on the HP GS1280 <ref type="bibr" target="#b25">[26]</ref>. We implement the total store order (TSO) memory consistency model <ref type="bibr" target="#b42">[43]</ref> and perform speculative load and store prefetching <ref type="bibr" target="#b14">[15]</ref>. Other relevant system parameters are listed in Table <ref type="table" target="#tab_1">1</ref>.</p><p>We evaluate four commercial workloads and three scientific applications. We run both IBM DB2 v8 ESE and Oracle 10g Enterprise Database Server on an online transaction processing (OLTP) workload modeled after a 100 warehouse TPC-C installation. We evaluate web server performance using the SpecWeb99 workload running on both Apache HTTP Server v2.0 and Zeus Web Server v4.3. In addition to these server benchmarks, we evaluate three scientific workloads that show a range of computation and memory access patterns: em3d is a read-intensive benchmark dominated by memory access latency; ocean contains short bursts of parallel memory accesses followed by long computation phases; and gauss models a small, in-cache kernel dominated by coherent read miss latency.</p><p>For all workloads except gauss, we use a systematic sampling approach derived from SMARTS <ref type="bibr" target="#b44">[45]</ref>. We collect approxi-   mately 100 brief measurements of 50,000 cycles each. We launch all measurements from checkpoints with warmed caches, branch predictors, and directory state, then run for 100,000 cycles to warm queue and interconnect state prior to collecting statistics. We include 95% confidence intervals. We aggregate total cycles per user instruction as CPI, which is inversely proportional to overall system throughput <ref type="bibr" target="#b43">[44]</ref>. In gauss, we measure the user-mode CPI of the slowest processor over an iteration, which reflects the overall execution time for the workload.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">DMR Coordination</head><p>We isolate performance impact due to the additional load of coordination messages by allowing the master to respond to all coherence messages directly without corroborating with the slave. LACROSS introduces additional traffic in the network link between master and slave, effectively doubling the number of packets sent in the network. Every time the master receives any external input from the system, a coordination message is sent to the slave.</p><p>Figure <ref type="figure">7</ref> shows the performance overhead from LACROSS coordination, reported as cycles per user instruction normalized to the baseline. On average, a 4.4% overhead in performance occurs solely due to the additional network contention.</p><p>Most of the workloads we studied have relatively low network utilization in the baseline system.</p><p>Oracle/OLTP, the workload most sensitive to the additional LACROSS traffic, offers 0.22 packets/cycle of network load in the baseline system, which is slightly less than half the 0.55 packets/cycle of available network throughput, as measured with uniform random packet injection <ref type="bibr" target="#b12">[13]</ref>. LACROSS doubles the offered load, thereby approaching the available throughput and increasing latency on the lowest priority virtual channels where normal coherence requests travel. In OLTP/Oracle, we observe a 38% average increase in round-trip network latency, which corresponds to an 18% increase in time spent on remote misses and the 9% CPI overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Fault Isolation</head><p>To show the performance effects of master-slave corroboration delay without contributions from increased coordination traffic, we take the converse approach from Section 6.1 by providing dedicated network links for all master-to-slave connections.</p><p>Relaxing Read Requests. We first consider the case where requests for dirty data must wait for the slave to compare fingerprints, which imposes an additional master-to-slave delay on the latency of the request. The top three lines in Figure <ref type="figure" target="#fig_5">8</ref> show the overhead from fault isolation as a function of the master-to-slave delay. The observed increases are attributable to the increase in dirty coherence miss latency and the commensurate queuing effects in the system.</p><p>The minimal master-to-slave delay for our network model is 550 processor clock cycles, which is a function of the buffering in the network switches. Increasing the buffering increases the network's ability to tolerate bursts of data transfers, but also increases the worst-case latency.</p><p>A master-to-slave delay of 550 cycles increases the average dirty miss latency by nearly 50% in our system. On average, the four commercial workloads spend 19% of total time waiting for dirty coherence misses. Although em3d and ocean spend a negligible fraction of time (less than 3%) on dirty misses, gauss spends 16% of total time on dirty misses due to frequent barrier synchronization and pivot computation.</p><p>Clearly, scaling the master-to-slave delay has a major impact on system performance.</p><p>Validation Filter. The bottom three lines in Figure <ref type="figure" target="#fig_5">8</ref> show that the validation filter effectively removes the sensitivity to the master-to-slave delay. What little performance overhead remains is attributable to lock transfers or barrier synchronization-cases where cache blocks are consumed immediately after production. Across all the applications studied, over 98% of all dirty misses hit in the validation filter-that is, the master can reply to 98% of all requests for dirty data without waiting for the slave to compare fingerprints. Limiting the filter to 64 entries does not impact the hit rate-the filter overflows less than 0.02% of the time in the worst-case (OLTP/Oracle).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Delaying physical IOs</head><p>In this section, we highlight the impact of delaying only physical IOs in the system. We remove all other sources of LACROSS-based performance overhead and scale the latency of memory-mapped accesses to pages that the MMU indicates have side effects. Table <ref type="table" target="#tab_3">3</ref> shows the normalized slowdown as a function of increasing master-to-slave delay. We do not report results for the scientific applications, which perform no device IO.</p><p>We found that little sensitivity exists to delaying physical IOs, even in IO-intensive commercial workloads. In the worst case, OLTP suffers just 6.8% slowdown. Note that we do not stall the delivery of interrupts to a processor or delay DMA activity-only the reading or writing of peripheral device registers.</p><p>In practice, a disk or network driver spends little time accessing a controller's memory-mapped registers. The majority of time goes to setting up transfers or processing data from a DMA buffer, which are unaffected in our design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Full system performance</head><p>Figure <ref type="figure">9</ref> shows the overall LACROSS performance by combining distributed DMR pairs with the fault isolation protocols. We show the performance overhead for the minimal choice of master-to-slave delay (550 processor cycles). Because the validation filter optimization requires non-trivial changes to the processor core interface, we include overall results with and without the filter.</p><p>Without the validation filter, OLTP/DB2 incurs the highest performance overhead of 21%, due primarily to the frequent, migratory sharing of data, which incur additional latency from the master-to-slave delay. With the validation filter, the performance overhead in OLTP/DB2 is reduced to 6.5%, and the worst-case workload becomes OLTP/Oracle, which incurs a 13% performance overhead. Most of this overhead is due to network contention overhead from coordination messages. The remaining contributions come from delaying physical IOs and imposing the master-to-slave delay on highly-contended blocks.</p><p>On average, the system performs within 13% of the baseline without the validation filter and within 5.8% of the baseline with the filter. Although the improvements from the filter seem small, the real benefit comes from decoupling the performance overheads from the master-to-slave delay, as shown previously in Figure <ref type="figure" target="#fig_5">8</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Normalized CPI</head><p>No Filter Filter</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RELATED WORK</head><p>At least four decades of development have been devoted toward improving reliability and availability for mainframe systems. IBM's z-series <ref type="bibr" target="#b33">[34]</ref> machines include processor designs that detect and recover from virtually any transient error, and the system transparently provides redundant hardware for many permanent faults. Our contribution is to enable distributed redundancy that survives chip failure, a critical reliability factor as device scaling continues.</p><p>The HP (formerly Tandem) NonStop systems <ref type="bibr" target="#b5">[6]</ref> traditionally constructed fail-fast DMR pairs from tightly lockstepped processors. When an error is detected, the pair is assumed dead and the process is moved to a spare DMR pair to resume execution. This so-called 'pair-and-spare' approach requires software assistance to provide the spare process with sufficient state information to intervene should an error be detected. Our design obviates the need for specialized software-the hardware provides software-transparent error recovery.</p><p>Our approach is inspired by the observation that cache coherence protocols for DSM multiprocessors have to deal with a multitude of race conditions and failure scenarios. Sun Microsystems' S3.mp system <ref type="bibr" target="#b28">[29]</ref> included extensions to the coherence protocols to provide fault isolation and memory replication. Stanford's Dash DSM multiprocessor also was used to explore fault confinement capabilities <ref type="bibr" target="#b40">[41]</ref> that contributed insights to our approach. We extended these ideas to add dual-modular redundancy for transparent recovery.</p><p>Numerous proposals exist for creating coarse-grained, global checkpoints of DSM architectural state for the purposes of surviving processor or node failures <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32]</ref>. Our work advances these proposals by enabling local error detection and recovery at fine-grained intervals as required by IO-intensive workloads <ref type="bibr" target="#b34">[35]</ref>.</p><p>The idea of using a time-shifted slave processor for redundant checking has been employed before in Slipstream processors <ref type="bibr" target="#b38">[39]</ref>, simultaneous redundant threading <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b41">42]</ref>, chip-level redundant threading <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b27">28]</ref>, and microarchitecture checkers <ref type="bibr" target="#b1">[2]</ref>. However, these proposals rely on close physical proximity for the master and slave cores, so that latency and bandwidth issues are fairly benign. Our contribution is the development of a viable system that allows the physical distribution of the DMR pairs across a system area network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSIONS</head><p>This paper presented a DSM system design that supports lockstep operation of physically distributed processing modules. The proposed approach to distributed redundancy provides reliability in the presence of transient or permanent faults in the processing modules without sacrificing the scalability of the underlying DSM architecture. This paper addressed two key design challenges: the coordination of a master and slave DMR pair and the corroboration of the decoupled redudant processing modules. Evaluations based on a range of commercial and scientific workloads showed that in the worst case, the error-free performance overhead of distributed lockstepping is within 13% (5.8% on average) of a baseline system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIGURE 1 :</head><label>1</label><figDesc>FIGURE 1: Hardware DSM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIGURE 2 :</head><label>2</label><figDesc>FIGURE 2: Membrane decomposition of a DSM. A fourth domain-devices-is not shown.</figDesc><graphic coords="2,310.79,92.30,233.04,51.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FIGURE 3 :FIGURE 4 :</head><label>34</label><figDesc>FIGURE 3: Organizing DMR in a DSM: (left) tightly-coupled pairs, (right) distributed pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FIGURE 5 :</head><label>5</label><figDesc>FIGURE 5: Soft-error detection (left) and recovery (right) protocols.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>FIGURE 6 :</head><label>6</label><figDesc>FIGURE 6: Proposed CMP additions: validation filter and checkpoints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>FIGURE 8 :</head><label>8</label><figDesc>FIGURE 8: Performance overhead as a function of the master-to-slave delay.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 . DSM Server configuration.</head><label>1</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2 .</head><label>2</label><figDesc>Workload configuration.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>1.2</cell><cell></cell><cell></cell></row><row><cell cols="2">Commercial Applications 100 warehouses (10 GB), 64 clients, 450 MB BP OLTP/Oracle 100 warehouses (10 GB), 16 clients, 1.4 GB SGA OLTP/DB2 Web/Apache 16K connections, fastCGI</cell><cell>Normalized CPI</cell><cell>0.2 0.4 0.6 0.8 1</cell><cell></cell><cell></cell></row><row><cell>Web/Zeus</cell><cell>16K connections, fastCGI</cell><cell></cell><cell>0</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>OLTP</cell><cell>OLTP</cell><cell>Web</cell><cell>Web</cell><cell>em3d</cell><cell>ocean gauss Average</cell></row><row><cell></cell><cell></cell><cell></cell><cell>DB2</cell><cell>Oracle</cell><cell>Apache</cell><cell>Zeus</cell></row><row><cell></cell><cell>Scientific Applications</cell><cell></cell><cell cols="4">FIGURE 7: Overhead from coordinating DMR pairs.</cell></row><row><cell>em3d</cell><cell>3M nodes, degree 2, 15% remote, span 5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ocean</cell><cell>1026x1026 grid, 600s rel., 20km res., 1e-07 err</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>gauss</cell><cell>256-by-256 matrix reduction</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3 . Slowdown when delaying physical IOs.</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>OLTP</cell><cell>Web</cell><cell>Scientific</cell></row><row><cell></cell><cell>2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Normalized CPI</cell><cell>1.2 1.4 1.6 1.8</cell><cell></cell><cell></cell><cell></cell><cell>No Filter</cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell>Filter</cell></row><row><cell></cell><cell>500</cell><cell>1000</cell><cell>1500</cell><cell>2000</cell><cell>2500</cell></row><row><cell></cell><cell></cell><cell cols="3">Master-to-slave delay (processor clock cycles)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>The authors would like to thank the members of the TRUSS research group at CMU for their feedback on earlier drafts of this paper and the CMU SimFlex team for simulation infrastructure. This work was funded in part by NSF awards ACI-0325802 and CCF-0347560, Intel Corp., the Center for Circuit and System Solutions (C2S2), the Carngie Mellon CyLab, and fellowships from the Department of Defense and the Alfred P. Sloan Foundation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cache-aided rollback error recovery CARER algorithm for shared-memory multiprocessor systems</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Symposium on Fault-Tolerant Computing</title>
		<meeting>the 20th International Symposium on Fault-Tolerant Computing</meeting>
		<imprint>
			<date type="published" when="1990-06">June 1990</date>
			<biblScope unit="page" from="82" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">DIVA: A reliable substrate for deep submicron microarchitecture design</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Austin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 32nd Intl. Symp. on Microarchitecture</title>
		<meeting>of the 32nd Intl. Symp. on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1999-11">November 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Selective writeback: Improving processor performance and energy-efficiency</title>
		<author>
			<persName><forename type="first">D</forename><surname>Balkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1st IBM Watson conference on Interaction between Architecture Circuits and Compilers</title>
		<imprint>
			<date type="published" when="2004-10">October 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Piranha: A scalable architecture base on single-chip multiprocessing</title>
		<author>
			<persName><forename type="first">L</forename><surname>Barroso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 27th Intl. Symp. on Comp. Arch. (ISCA-27)</title>
		<meeting>of 27th Intl. Symp. on Comp. Arch. (ISCA-27)</meeting>
		<imprint>
			<date type="published" when="2000-06">June 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Memory system characterization of commercial workloads</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Barroso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 25th Intl. Symp. on Comp. Arch. (ISCA-25)</title>
		<meeting>of 25th Intl. Symp. on Comp. Arch. (ISCA-25)</meeting>
		<imprint>
			<date type="published" when="1998-06">June 1998</date>
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tandem&apos;s approach to fault tolerance</title>
		<author>
			<persName><forename type="first">W</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ball</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tandem Systems Rev</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="84" to="95" />
			<date type="published" when="1988-02">February 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Microarchitecture and design challenges for gigascale integration</title>
		<author>
			<persName><forename type="first">S</forename><surname>Borkar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>In Keynote address, MICRO-37</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Designing reliable systems from unreliable components: the challenges of transistor variability and degradation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Borkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="10" to="17" />
			<date type="published" when="2005-12">November-December 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Microarchitecture optimizations for exploiting memory-level parallelism</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 31st Intl. Symp. on Comp. Arch. (ISCA-31)</title>
		<meeting>of 31st Intl. Symp. on Comp. Arch. (ISCA-31)</meeting>
		<imprint>
			<date type="published" when="2004-06">June 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Corporation</surname></persName>
		</author>
		<title level="m">Intel E8500 chipset north bridge (nb)</title>
		<imprint>
			<date type="published" when="2005-03">March 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Performance analysis of the Alpha 21364-based HP GS1280 multiprocessor</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cvetanovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 30th Intl. Symp. on Comp. Arch. (ISCA-30)</title>
		<meeting>of 30th Intl. Symp. on Comp. Arch. (ISCA-30)</meeting>
		<imprint>
			<date type="published" when="2003-06">June 2003</date>
			<biblScope unit="page" from="218" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A white paper on the benefits of chipkillcorrect ecc for pc server main memory</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Dell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IBM Whitepaper</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Interconnection Networks: an Engineering Approach</title>
		<author>
			<persName><forename type="first">J</forename><surname>Duato</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Increasing processor performance through early register release</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ergin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCD</title>
		<meeting>ICCD</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Two techniques to enhance the performance of memory consistency models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gharachorloo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 1991 Intl. Conf. on Parallel Processing</title>
		<meeting>of the 1991 Intl. Conf. on Parallel essing</meeting>
		<imprint>
			<date type="published" when="1991-08">Aug. 1991</date>
			<biblScope unit="page" from="355" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Is SC + ILP = RC?</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gniady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 26th Intl. Symp. on Comp. Arch. (ISCA-26)</title>
		<meeting>of 26th Intl. Symp. on Comp. Arch. (ISCA-26)</meeting>
		<imprint>
			<date type="published" when="1999-05">May 1999</date>
			<biblScope unit="page" from="162" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The intel pentium m processor: Microarchitecture and performance</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gochman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intel Technology Journal</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2003-05">May 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">TRUSS: A reliable, scalable server architecture</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Gold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2005-12">Nov-Dec 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Transient-fault recovery for chip multiprocessors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gomaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Symposium on Computer Architecture</title>
		<meeting>the 30th International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2003-06">June 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SimFlex: a fast, accurate, flexible full-system simulation framework for performance evaluation of server architecture</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hardavellas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGMETRICS Performance Evaluation Review</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="31" to="35" />
			<date type="published" when="2004-04">April 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Design considerations of phase-locked loop systems for spread spectrum clock generation compatibility</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hardin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Symp. on Electromagnetic Compatibility</title>
		<meeting>Intl. Symp. on Electromagnetic Compatibility</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Back to the future: Time to return to longstanding problems in computer systems?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hennessy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Keynote address</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cherry: Checkpointed early resource recycling in out-of-order microprocessors</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 35th IEEE/ACM Intl. Symp. on Microarch. (MICRO 35)</title>
		<meeting>of 35th IEEE/ACM Intl. Symp. on Microarch. (MICRO 35)</meeting>
		<imprint>
			<date type="published" when="2002-11">Nov 2002</date>
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An efficient and scalable approach for implementing fault-tolerant dsm architectures</title>
		<author>
			<persName><forename type="first">C</forename><surname>Morin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="414" to="430" />
			<date type="published" when="2000-05">May 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">JETTY: filtering snoops for reduced energy consumption in SMP servers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Seventh IEEE Symp</title>
		<meeting>of Seventh IEEE Symp</meeting>
		<imprint>
			<date type="published" when="2001-03">Mar. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The alpha 21364 network architecture</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Mukherjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2002-02">Jan-Feb 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The soft error problem: an architectural perspective</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Mukherjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Symp. on High-Performance Computer Architecture</title>
		<meeting>Intl. Symp. on High-Performance Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Detailed design and evaluation of redundant multithreading alternatives</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Mukherjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Symposium on Computer Architecture</title>
		<meeting>the 29th International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2002-05">May 2002</date>
			<biblScope unit="page" from="99" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The s3.mp scalable shared memory multiprocessor</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Nowatzyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Annual Hawaii International Conference on System Sciences</title>
		<meeting>the 27th Annual Hawaii International Conference on System Sciences</meeting>
		<imprint>
			<date type="published" when="1994-01">Jan 1994</date>
			<biblScope unit="page" from="144" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Recovery oriented computing: A new research agenda for a new century</title>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Keynote address</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A case for redundant arrays of inexpensive disks (raid)</title>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 7th ACM Intl. Conf. on Management of Data (SIGMOD &apos;88)</title>
		<meeting>of 7th ACM Intl. Conf. on Management of Data (SIGMOD &apos;88)</meeting>
		<imprint>
			<date type="published" when="1988-06">June 1988</date>
			<biblScope unit="page" from="109" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">ReVive: cost-effective architectural support for rollback recovery in shared memory multiprocessors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Prvulovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 29th Intl. Symp. on Comp. Arch. (ISCA-29)</title>
		<meeting>of 29th Intl. Symp. on Comp. Arch. (ISCA-29)</meeting>
		<imprint>
			<date type="published" when="2002-06">June 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Transient fault detection via simultaneous multithreading</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Reinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Mukherjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Symposium on Computer Architecture</title>
		<meeting>the 27th International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2000-06">June 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">IBM&apos;s S/390 G5 microprocessor design</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Slegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="12" to="23" />
			<date type="published" when="1999-04">March/April 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fingerprinting: Bounding soft-error detection latency and bandwidth</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Smolens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Eleventh Intl. Conf. on Arch. Support for Program. Lang. and Op. Syst. (ASPLOS XI)</title>
		<meeting>of Eleventh Intl. Conf. on Arch. Support for Program. Lang. and Op. Syst. (ASPLOS XI)<address><addrLine>Boston, Massachusetts</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-10">Oct. 2004</date>
			<biblScope unit="page" from="224" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Memory coherence activity prediction in commercial workloads</title>
		<author>
			<persName><forename type="first">S</forename><surname>Somogyi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Third Workshop on Memory Perf. Issues (WMPI-2004)</title>
		<meeting>of Third Workshop on Memory Perf. Issues (WMPI-2004)</meeting>
		<imprint>
			<date type="published" when="2004-06">June 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">SafetyNet: improving the availability of shareed memory multiprocessors with global checkpoint/ recovery</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Sorin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 29th Intl. Symp. on Comp. Arch. (IS-CA-29)</title>
		<meeting>of 29th Intl. Symp. on Comp. Arch. (IS-CA-29)</meeting>
		<imprint>
			<date type="published" when="2002-06">June 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The impact of technology scaling on lifetime reliability</title>
		<author>
			<persName><forename type="first">J</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Conf. on Dependable Systems and Networks</title>
		<meeting>Intl. Conf. on Dependable Systems and Networks</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Slipstream processors: improving both performance and fault tolerance</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sundaramoorthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Ninth Intl. Conf. on Arch. Support for Program. Lang. and Op. Syst. (ASPLOS IX)</title>
		<meeting>of Ninth Intl. Conf. on Arch. Support for Program. Lang. and Op. Syst. (ASPLOS IX)</meeting>
		<imprint>
			<date type="published" when="2000-11">Nov. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Thermal challenges during microprocessor testing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tadayon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intel Technology Journal</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2000-08">August 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hardware fault containment in scalable shared-memory multiprocessors</title>
		<author>
			<persName><forename type="first">D</forename><surname>Teodosiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA97</title>
		<imprint>
			<date type="published" when="1997-06">June 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Transient-fault recovery using simultaneous multithreading</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Vijaykumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Symposium on Computer Architecture</title>
		<meeting>the 29th International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2002-05">May 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Weaver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Germond</surname></persName>
		</author>
		<title level="m">SPARC Architecture Manual</title>
		<imprint>
			<publisher>PTR Prentice Hall</publisher>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
	<note>Version 9</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Temporal streaming of shared memory</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 32nd Intl. Symp. on Comp. Arch. (ISCA-32)</title>
		<meeting>of 32nd Intl. Symp. on Comp. Arch. (ISCA-32)</meeting>
		<imprint>
			<date type="published" when="2005-06">June 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">SMARTS: Accelerating microarchitecture simulation through rigorous statistical sampling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wunderlich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 30th Intl. Symp. on Comp. Arch. (ISCA-30)</title>
		<meeting>of 30th Intl. Symp. on Comp. Arch. (ISCA-30)</meeting>
		<imprint>
			<date type="published" when="2003-06">June 2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
