<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HyGCN: A GCN Accelerator with Hybrid Architecture</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-01-07">7 Jan 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mingyu</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Computing Technology</orgName>
								<orgName type="department" key="dep2">School of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Computer Architecture</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit3">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Deng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Computing Technology</orgName>
								<orgName type="department" key="dep2">School of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Computer Architecture</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit3">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xing</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Computing Technology</orgName>
								<orgName type="department" key="dep2">School of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Computer Architecture</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit3">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ling</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Computing Technology</orgName>
								<orgName type="department" key="dep2">School of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Computer Architecture</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit3">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yujing</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Computing Technology</orgName>
								<orgName type="department" key="dep2">School of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Computer Architecture</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit3">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaochun</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Computing Technology</orgName>
								<orgName type="department" key="dep2">School of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Computer Architecture</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit3">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhimin</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Computing Technology</orgName>
								<orgName type="department" key="dep2">School of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Computer Architecture</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit3">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dongrui</forename><surname>Fan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Computing Technology</orgName>
								<orgName type="department" key="dep2">School of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Computer Architecture</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit3">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuan</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Computing Technology</orgName>
								<orgName type="department" key="dep2">School of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Computer Architecture</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit3">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">HyGCN: A GCN Accelerator with Hybrid Architecture</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-01-07">7 Jan 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2001.02514v1[cs.DC]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Inspired by the great success of neural networks, graph convolutional neural networks (GCNs) are proposed to analyze graph data. GCNs mainly include two phases with distinct execution patterns. The Aggregation phase, behaves as graph processing, showing a dynamic and irregular execution pattern. The Combination phase, acts more like the neural networks, presenting a static and regular execution pattern. The hybrid execution patterns of GCNs require a design that alleviates irregularity and exploits regularity. Moreover, to achieve higher performance and energy efficiency, the design needs to leverage the high intra-vertex parallelism in Aggregation phase, the highly reusable inter-vertex data in Combination phase, and the opportunity to fuse phase-by-phase execution introduced by the new features of GCNs. However, existing architectures fail to address these demands.</p><p>In this work, we first characterize the hybrid execution patterns of GCNs on Intel Xeon CPU. Guided by the characterization, we design a GCN accelerator, HyGCN, using a hybrid architecture to efficiently perform GCNs. Specifically, first, we build a new programming model to exploit the fine-grained parallelism for our hardware design. Second, we propose a hardware design with two efficient processing engines to alleviate the irregularity of Aggregation phase and leverage the regularity of Combination phase. Besides, these engines can exploit various parallelism and reuse highly reusable data efficiently. Third, we optimize the overall system via inter-engine pipeline for inter-phase fusion and priority-based off-chip memory access coordination to improve off-chip bandwidth utilization. Compared to the state-of-the-art software framework running on Intel Xeon CPU and NVIDIA V100 GPU, our work achieves on average 1509× speedup with 2500× energy reduction and average 6.5× speedup with 10× energy reduction, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Inspired by the powerful learning capability of neural networks, graph convolutional neural networks (GCNs) are proposed as an effective category of models to represent and process graph data <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b47">47]</ref>. GCNs convert the graph data into a low dimensional space while keeping both the structure and property information to the maximum extent, and then construct a neural network for the consequent train-ing and inference. Recently, GCNs attract substantial efforts from both the industrial and academic communities <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b44">44]</ref> to solve problems including node classification <ref type="bibr" target="#b25">[25]</ref>, link prediction <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b16">16]</ref>, graph clustering <ref type="bibr" target="#b44">[44]</ref>, and recommendation <ref type="bibr" target="#b12">[12]</ref>. As a result, GCNs gradually become a new workload family member in data-centers, such as in Google <ref type="bibr" target="#b13">[13]</ref>, Facebook <ref type="bibr" target="#b28">[28]</ref>, and Alibaba <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b47">47]</ref>.</p><p>The convolutional layers occupy the major execution time of GCNs through two primary execution phases: Aggregation and Combination <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b47">47]</ref>. The Aggregation phase maintains most graph processing behaviors. It heavily relies on the graph structure that is inherently random and sparse. Processing of each vertex requires aggregating features from all its source neighbours. Unfortunately, the amount and location of these source neighbors vary significantly among vertices. As a result, the computational graph <ref type="bibr" target="#b26">[26]</ref> and memory access pattern in the Aggregation phase of each vertex are dynamic and irregular. The Combination phase acts more like the neural networks. It transforms the feature vector of each vertex to a new one using a multi layer perceptron (MLP), which is usually expressed by a matrix-vector multiplication (MVM). Due to the identical connection pattern of each neuron within a neural network layer, the computational graph <ref type="bibr" target="#b26">[26]</ref> and memory access pattern in the Combination phase of each vertex are static and regular. Besides, there are additional characteristics in these two phases that distinguish GCNs from conventional workloads. First, the length of vertex property is short and fixed in conventional graph analytics. However, in GCNs, the feature vector of each vertex is quite long and variable across layers, which introduces high-degree intra-vertex parallelism in Aggregation phase. Second, the parameters in conventional MLP-based neural networks are never shared, while they can be fully shared among vertices in GCNs, which induces abundant highly reusable inter-vertex data in Combination phase. Third, the two phases are executed alternatively. An inherent dataflow exists between phases, providing an opportunity to fuse the phase-by-phase execution.</p><p>To achieve high-performance and energy-efficient acceleration of GCNs, aforementioned characteristics have imposed new requirements on architecture design. First, not only can the GCN architecture alleviate the irregularity in Aggregation phase, but it can also exploit the regularity in Combination phase. Second, it needs to exploit the high-degree intra-vertex parallelism and highly reusable inter-vertex data. Third, it is able to efficiently fuse the execution of these two phases.</p><p>Unfortunately, existing architectures fail to implement GCN-specific characteristics. For CPUs, although they can employ complex caching and prefetching techniques to offset the processor-memory disparity by exploiting the regular access pattern <ref type="bibr" target="#b11">[11]</ref>, they fail to address the abundant dynamic and irregular data accesses in the Aggregation phase since the irregularity harms the predictability of memory accesses <ref type="bibr" target="#b10">[10]</ref>. Besides, it is difficult to efficiently implement the reuse of the highly reusable parameter data between computing units in CPUs as like TPU <ref type="bibr" target="#b22">[22]</ref> and Eyeriss <ref type="bibr" target="#b8">[8]</ref>. Thus, the energyhungry data accesses to cache introduce high energy consumption <ref type="bibr" target="#b8">[8]</ref>. For GPUs, although they are well optimized for neural networks, they lack the ability to alleviate irregularity in Aggregation phase, which significantly hinders the performance improvement <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b41">41]</ref>. Furthermore, although they leverage the regularity in Combination phase, the data copy and synchronization between threads for the parameter reuse are expensive. For graph analytics and neural network accelerators, they are only optimized to alleviate irregularity or exploit regularity, rather than both simultaneously. At last, all of them are short of the ability to efficiently fuse the execution of these two phases. In conclusion, existing architectures are not the ideal platforms to execute GCNs.</p><p>In this work, we first characterize the hybrid execution patterns of GCN workloads on Intel Xeon CPU. Next, guided by the characterization, we propose a GCN accelerator, HyGCN, using a hybrid architecture to efficiently perform GCNs. Specially, we first propose a programming model to achieve the hardware transparency for programmers and exploit finegrained parallelism. It abstracts GCNs as edge-centric aggregation for the Aggregation phase and MVMs for the Combination phase. Second, we design HyGCN with two efficient processing engines, Aggregation Engine and Combination Engine, to accelerate the Aggregation and Combination phases, respectively. In Aggregation Engine, interval-shard graph partitioning and window sliding-shrinking methods are introduced to alleviate irregularity by increasing data reuse and decreasing unnecessary accesses for sparsity, respectively. Additionally, we implement a vertex-disperse processing method to exploit the edge parallelism and intra-vertex parallelism. In Combination Engine, to leverage the regularity, we build multi-granular systolic arrays to perform MVMs in parallel and reuse the shared parameters. Besides, they can be flexibly used either independently for lower latency or in combination for lower energy. Third, to improve the overall execution, on the basis of individual optimizations of these two phases, we build a fine-grained inter-engine pipeline to fuse the phase-by-phase execution and propose a priority-based memory access coordination for the off-chip data accesses between the two engines.</p><p>To summarize, we list our contributions as follows: • We study an emerging domain, GCNs, from a computer architecture perspective and show that hybrid execution patterns exist in GCNs. Specially, the Aggregation phase in GCNs presents a dynamic and irregular execution pattern, while Combination phase is static and regular. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BACKGROUND</head><p>GCNs follow a neighborhood aggregation scheme, where the feature vector of each vertex is computed by recursively aggregating and transforming the representation vectors of its neighbor vertices <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b47">47]</ref>. Fig. <ref type="figure" target="#fig_0">1</ref> illustrates the execution phases of GCN models. After k iterations of aggregation via the Aggregate function and transformation via the Combine function, a vertex is represented by its final feature vector, which captures the structural information within the vertex's k-hop neighborhood. Table <ref type="table" target="#tab_1">1</ref> lists the notations used in GCNs. In this work, we mainly focus on undirected graphs and the inference stage rather than training. Typically, the k-th layer/iteration of GCNs is formulated as</p><formula xml:id="formula_0">a k v = Aggregate h (k−1) u : u ∈ {N(v)} ∪ {v} , h k v = Combine a k v .<label>(1)</label></formula><p>where h k v is the representation feature vector of vertex v at the k-th iteration. Simply, the Aggregate function aggregates multiple feature vectors from source neighbors to one single feature vector, and the Combine function transforms the feature vector of each vertex to another feature vector using an MLP neural network. Note that the MLP parameters, including weights and biases, are shared between vertices.</p><p>In order to decrease the computational complexity, the Sample function is usually applied before the Aggregate function to sample a subset from the neighbor vertices of each vertex <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b18">18]</ref> as the new neighbors, specifically,</p><formula xml:id="formula_1">S(v) = Sample k N(v) .</formula><p>(2) Sometimes, the Pool function <ref type="bibr" target="#b44">[44]</ref> follows the Combine function to transform the original graph into a smaller graph.</p><p>After several iterations, the features will be used for final prediction or classification. For the node classification problem, vertex feature vectors h k v at the last iteration are used for prediction. For the graph classification problem, a Readout function further aggregates the h k v at the last iteration to obtain the entire graph's representation vector, i.e.</p><formula xml:id="formula_2">h G = Readout h k v | v ∈ G .</formula><p>(3) Next, we provide several typical GCN models as examples to explain the above operations in detail.</p><p>GCN is one of the most successful convolutional networks for graph learning <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b38">38]</ref>, which bridges the gap between spectral-based convolutions and spatial-based convolutions. Its inference model can be described as</p><formula xml:id="formula_3">a k v = ∑ 1 √ D v • D u h (k−1) u | ∀u ∈ {N(v)} ∪ {v} , h k v = ReLU(W k a k v + b k ).<label>(4)</label></formula><p>GraphSage further adopts uniform neighbor sampling to alleviate receptive field expansion that effectively trades off accuracy and execution time <ref type="bibr" target="#b18">[18]</ref>. It is formulated as</p><formula xml:id="formula_4">a k v = Mean {h (k−1) v } ∪ {h (k−1) u , ∀u ∈ S(v)} , h k v = ReLU(W k a k v + b k ).<label>(5)</label></formula><p>GINConv is a simple neural architecture, and its discriminative power is equal to the power of the Weisfeiler-Lehman graph isomorphism test <ref type="bibr" target="#b39">[39]</ref>. Vertex features learned by GIN-Conv can be directly used for tasks like node classification and link prediction. We can perform this model as</p><formula xml:id="formula_5">a k v = (1 + ε k ) • h (k−1) v + ∑ u∈N(v) h (k−1) u , h k v = MLP k (a k v , W k , b k ).<label>(6)</label></formula><p>For graph classification tasks, the following Readout function is further used to produce the representation of the entire graph, given the representations of individual vertices. It concatenates across all iterations of GINConv to acquire the final graph representation as</p><formula xml:id="formula_6">h G = Concat ( ∑ v∈G h k v ) | k = 1, ..., K .<label>(7)</label></formula><p>DiffPool provides a general tool to realize hierarchical graph-level transformation for a broad set of input graphs <ref type="bibr" target="#b44">[44]</ref>. It can be inserted after the Combine function of any GCNs to transform the original graph to a smaller one (like the pooling layer in convolutional neural networks (CNNs)). In fact, Diffpool uses two extra GCNs to implement the graph transformation, which follows</p><formula xml:id="formula_7">C (k−1) = so f tmax GCN k pool (A (k−1) , X (k−1) ) , Z (k−1) = GCN k embedding (A (k−1) , X (k−1) ), X k = C (k−1) T Z (k−1) , A k = C (k−1) T A (k−1) C (k−1) .<label>(8)</label></formula><p>After the DiffPool transformation, a new feature matrix X k and adjacent matrix A k are produced, which can be combined to construct a new and smaller graph. In the new graph, GCN k pool determines the number of vertices, and GCN k embedding determines the length of vertex feature vector. Summary. As explained above, we introduce several typical operations in GCNs: Sampling, Aggregation, Combination, Pooling, and Readout. Except for Combination, all the operations are graph structure-dependent, which involve graph processing. Combination usually is a typical MLP neural network (single layer or multiple layers). Sampling is used to sample a subset from neighbors, which can be done during preprocessing <ref type="bibr" target="#b20">[20]</ref> or with random selection during runtime <ref type="bibr" target="#b18">[18]</ref>. Aggregation aggregates the features from its 1-hop neighbors. Pooling acts like the pooling layer in CNNs to realize graph transformation by reducing the number of vertices and the length of feature vectors. Readout can be a simple summation <ref type="bibr" target="#b15">[15]</ref> across vertices or further concatenation across iterations <ref type="bibr" target="#b39">[39]</ref>. Therefore, Readout can be viewed as an extreme Aggregation. This work focuses on Aggregation and Combination, two major phases in GCNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MOTIVATION</head><p>In this section, we quantitatively characterize and identify the hybrid execution patterns in processing GCNs. Next, we explain our motivation behind designing a GCN accelerator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Characterization on CPU</head><p>We conduct quantitative characterizations using a state-ofthe-art GCN software framework PyTorch Geometric <ref type="bibr" target="#b15">[15]</ref> on Intel Xeon CPU. The execution time breakdown of GCN (GCN) <ref type="bibr" target="#b25">[25]</ref>, GraphSage (GSC) <ref type="bibr" target="#b18">[18]</ref>, and GINConv (GIN) <ref type="bibr" target="#b39">[39]</ref> on several datasets <ref type="bibr" target="#b23">[23]</ref> is illustrated in Fig. <ref type="figure" target="#fig_3">2</ref>. The profiling results of GCN <ref type="bibr" target="#b25">[25]</ref> on the COLLAB dataset <ref type="bibr" target="#b23">[23]</ref> are presented in Table <ref type="table" target="#tab_3">2</ref>. The details of system configuration and datasets are shown in Section 5.1. The Combination phase executes a MVM for each vertex with a shared MLP-based neural network, which performs static and regular computations and accesses. Table <ref type="table" target="#tab_3">2</ref> illustrates that each operation in the Combination phase requires only small amount of data to be accessed from DRAM. This is because the MVMs are very compute-intensive and the weight matrix of MLP is widely shared between vertices. Nevertheless, up to 36% of execution time for shared data copy and synchronization between threads is observed.  According to above analysis, hybrid execution patterns exist in GCNs, which are summarized in Table <ref type="table" target="#tab_4">3</ref>. The Aggregation phase performs dynamic and irregular execution pattern, bounded by memory, while the Combination phase is static and regular, bounded by computation. Differences from Conventional Workloads. Beside hybrid execution patterns in GCNs, there are additional characteristics that distinguishes GCNs from conventional workloads. Specifically, in the Aggregation phase, the length of feature vectors is variable rather than fixed as in traditional graph analytics, which is determined by the input dataset and MLP structure. Moreover, the length of the feature vectors in each vertex is usually orders of magnitude longer than that of traditional graph analytics. This introduces high intra-vertex parallelism. In the Combination phase, the MLP parameters are fully shared by all vertices while non-reusable in traditional MLP models if not using the batching technique. This induces numerous highly reusable inter-vertex data. Besides, these two phases are executed alternatively to produce the final result, while conventional workloads iteratively perform only the graph traversal or the neural network propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Need for a GCN Accelerator</head><p>GCNs are showing great potential in various tasks <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b46">46]</ref>. Many companies, such as Google <ref type="bibr" target="#b13">[13]</ref>, Facebook <ref type="bibr" target="#b28">[28]</ref>, and Alibaba <ref type="bibr" target="#b47">[47]</ref> have deployed GCNs in data centers, which reflects the increasing importance and scope of upcoming applications. An efficient architecture is timely to achieve high performance and stimulate GCN development. Therefore, given the above characterizations, we explain our motivation of designing a GCN accelerator. Design Requirements. Given the characteristics of GCNs, we present the design requirements to perform GCNs with high performance and energy efficiency. First, Aggregation phase demands efforts to alleviate the irregularity that degrades performance. On the other hand, Combination phase needs more attention to leverage the regularity to improve the intensive computations with better parallelism and faster synchronization. Second, the high-degree intra-vertex parallelism and the highly reusable inter-vertex data need to be exploited. Third, to achieve higher performance and energy efficiency, the execution of Aggregation phase and Combination phase need to be efficiently fused. Unfortunately, existing architectures fail to address these requirements, resulting in the following inefficiencies. Inefficiencies of General-Purpose Processors. On CPUs, the irregularity in Aggregation phase makes GCNs ill-suited to current cache hierarchy design and data prefetching techniques. Besides, it is hard to efficiently reuse the highly reusable parameter data between compute units <ref type="bibr" target="#b8">[8]</ref>.</p><p>GPUs are inherently optimized for compute-intensive workloads with regular execution pattern <ref type="bibr" target="#b29">[29]</ref> such as neural networks, but handling the Aggregation phase with irregular memory accesses suffers from low efficiency. Besides, the processing of Combination with strong parameter sharing needs costly data copy and thread synchronization.</p><p>Both CPUs and GPUs lack inter-phase optimization for GCN execution. To leverage the advantages of hardwareoptimized functions <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b31">31]</ref>, current programming framework for GCNs usually adopts coarse-grained execution, which results in phase-by-phase execution. This compromises the design space with phase interaction, hindering the improvement beyond the individual optimization for each phase. Inefficiencies of Conventional Accelerators. Specialized accelerators tailored to graph analytics or neural networks gain significant speedup and energy savings compared to general-purpose processors. Whereas, they are inefficient in processing GCNs due to following reasons: i) they are usually only designed to either alleviate irregularity or exploit regularity, while GCNs need both; ii) they fail to leverage the new kinds of parallelism and data reuse to further improve performance; iii) single-paradigm design make them hard to fuse the execution of the two phases. Opportunities for Customization. Designing a specialized accelerator for a specific domain is an efficient and prevalent solution to address the inefficiencies of existing architectures, since it can tailor the memory hierarchy and computation unit to the specific workload. For GCNs, we can build an accelerator with a hybrid architecture using different optimizations for the two phases. For the Aggregation phase, it is possible to obtain the knowledge of graph data in advance and schedule the accesses to alleviate the irregularity. Moreover, the computation for each vertex can also be scheduled to exploit edge parallelism and intra-vertex parallelism. For the Combination phase, we draw inspirations from current neural network accelerators to efficiently perform MVMs in parallel with parameter sharing. Beyond the individual optimizations of the two phases, the serial inter-phase dataflow can be pipelined in finer grain. Moreover, all off-chip memory accesses can be controlled to improve the overall memory access efficiency. Putting all these together, there are huge opportunities to design an efficient GCN accelerator with high performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">ARCHITECTURE DESIGN</head><p>In this section, we design HyGCN to support the efficient execution of GCNs. We first introduce the programming model and then present details of the architecture design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Edge-and MVM-Centric PM</head><p>The goal of building a programming model (PM) is to exploit available parallelisms and achieve hardware transparency for programmers <ref type="bibr" target="#b47">[47]</ref>. For Aggregation, there are gather-and scatter-based processing methods. Since the scatter-based method usually produces large amount of atomic operations and requires a synchronization after the processing of all vertices, the degree of parallelism will be degraded <ref type="bibr" target="#b17">[17]</ref>. On the contrary, the gather-based method can control the program behavior easily and preserve the execution parallelism. Therefore, we select the gather-based processing in our design. Nevertheless, this processing mode leads to intensive memory access and vertex computation. To address this problem, we employ an edge-centric PM to exploit the edge-level parallelism. Each vertex possesses many incoming edges (neighbors), which can be aggregated in an edge-byedge pipeline. In this way, workload for each vertex can be divided into subworkloads and assigned to each computation unit for processing in parallel. For Combination, the situation is relatively easier. Since the computation of each vertex acts like the MLP, we directly focus on the MVM operations.</p><p>Our edge-and MVM-centric PM for GCNs is shown in Algorithm 1. At each vertex v ∈ V , the sampled neighbor indices are read first, which is a subset of all neighbors. Each index corresponds to an edge connecting v and a neighbor vertex u, i.e. e(u, v). By traversing all sampled edges connected v, all the feature vectors of corresponding neighbors can be aggregated onto the feature vector of v. Then, a Combine function can start performing the Combination phase that is comprised of a series of MVMs. In this PM, the edge-level and MVM-level parallelism can be exploited.</p><p>Note that in Algorithm 1 we do not express the Pool and Readout operations explicitly since they are not always needed. In fact, the Pool operation can be represented by two GCNs and additional matrix operations. The GCNs can be performed entirely by the two engines, the matrix transposes can be executed by the flexible Aggregation engine, and the matrix multiplications can be executed by the Combination engine. The Readout operation can be expressed by an additional single vertex that connects all vertices in the graph, which can be accomplished by the Aggregation engine. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Architecture Overview</head><p>Based on the proposed PM, Fig. <ref type="figure" target="#fig_1">3</ref> depicts the architecture of HyGCN. We construct the system using a hybrid architecture, which includes two engines (Aggregation Engine and Combination Engine) and one memory access handler. A communication interface (Coordinator) is introduced to bridge these two engines. Therefore, the interference between them is mitigated and their execution pipeline is established.  The Aggregation Engine aims to realize the efficient execution of irregular accesses and computations. To exploit the edge-level parallelism, a task scheduler (eSched) is designed to assign the edge processing workloads onto SIMD cores. To support the Sampling operation, we introduce a Sampler into the Aggregation Engine. The Sampler selects edges from the edge list of each vertex using a uniform or predefined distribution in terms of index interval. The former indices for edge sampling are based on dynamic generation while the latter ones are predefined and can be read from off-chip memory like in <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b20">20]</ref>. To reduce the latency of data access, we employ embedded DRAM (eDRAM) to cache various data to improve data reuse. An Edge Buffer is used to cache edges to exploit spatial locality in the edge array. An Input Buffer is used to cache the vertex features in X k−1 and an Aggregation Buffer is used to cache the intermediate aggregation results, to exploit temporal locality. To hide the DRAM access latency, both the Edge Buffer and Input Buffer adopt the double buffer technique. Specifically, we design a Sparsity Eliminator to avoid redundant feature loads of the vertices that share no edges with the aggregating vertex.</p><p>The Combination Engine is designed to maximize the efficiency of regular accesses and computations. In order to improve the processing parallelism and data reuse, we adopt the well-known systolic array design <ref type="bibr" target="#b22">[22]</ref> and modify it to be compatible with GCNs. A Weight Buffer is used to cache the weight matrix to exploit their temporal locality, and an Output Buffer is used to coalesce the write accesses of the final features. Similarly, they also leverage the double buffer technique to hide off-chip access latency. The Combination engine takes the aggregation result of each vertex v from the Aggregation engine and the weight matrix from the Weight Buffer as inputs to execute the MVM operation. The vSched is responsible for the workload assignment. After the MVM operations, an activation operation is performed by Activate Unit to produce the new feature vector of vertex v. Different from normal systolic array, our systolic array is multi-granular that can be used as multiple smaller arrays or a whole large array under different optimization scenarios.</p><p>To improve the bandwidth utilization, a prefetcher is designed to explicitly prefetch graph data and parameter data. For example, the prefetching of the feature vectors is as follows. The prefetcher first prefetches the edges of current processing vertices. After receiving these edges, Sparsity Eliminator obtains the indices of neighbors from these edges and sends them to the prefetcher. The prefetcher uses them to prefetch the feature vectors immediately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Aggregation Engine</head><p>To optimize the computation of Aggregation, we introduce a vertex-disperse processing mode. To optimize memory accesses, we employ a static graph partition method to enhance data reuse and a dynamic sparsity elimination technique to reduce unnecessary data accesses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Execution Mode</head><p>There are two processing modes for SIMD cores to process edges in parallel. The first one is vertex-concentrated, where the workloads of each vertex are assigned to a single SIMD core. This mode can produce the aggregated features of vertices in burst mode, i.e. periodically processing a group of vertices. However, the processing latency of a single vertex (termed as vertex latency) is long, and the fast vertices have to wait for the slow vertices leading to workload imbalance. Furthermore, it also loses the parallelism that the aggregation of each element can be performed in parallel (i.e., intra-vertex parallelism). Therefore, we use the second processing mode, which is shown in Fig. <ref type="figure" target="#fig_5">4</ref>. It assigns the aggregation of elements inside the vertex feature vector of each vertex to all cores, termed as vertex-disperse mode. If a vertex cannot occupy all cores, free cores can be assigned to other vertices. Thus, all cores are always busy without workload imbalance. Moreover, since the intra-vertex parallelism has been exploited, the vertex latency for a single vertex is smaller than processing multiple vertices together. Furthermore, it also enables the immediate processing of each vertex in the following Combination Engine.</p><formula xml:id="formula_8">Feature 1 in X (k-1)</formula><p>Feature 8 in X (k-1)</p><p>Neighbor 1</p><p>Aggregating Feature 1</p><p>Aggregating Feature 8</p><formula xml:id="formula_9">Aggregating V 1 Feature 8i+1 in X (k-1)</formula><p>Feature 8i+8 in X (k-1)</p><p>Aggregating Feature 8i+1</p><p>Aggregating Feature 8i+8</p><formula xml:id="formula_10">T 1 Core 1 Core i Feature 1 in X (k-1) Feature 8 in X (k-1) Neighbor 2 Feature 8i+1 in X (k-1) Feature 8i+8 in X (k-1) T 2 Feature 1 in X (k-1)</formula><p>Feature 8 in X (k-1)</p><p>Neighbor n</p><formula xml:id="formula_11">Feature 8i+1 in X (k-1)</formula><p>Feature 8i+8 in X (k-1)</p><p>T n</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Time：</head><p>Figure <ref type="figure" target="#fig_5">4</ref>: Vertex-disperse processing mode where the workloads of each vertex are assigned to all SIMD cores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Graph Partitioning (Static)</head><p>We borrow the abstraction of vertex interval and edge shard from <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b27">27]</ref> to partition graph data, which is the basis of our data-aware sparsity elimination in the next subsection. We do not need explicit preprocessing to generate the intervals and shards since we directly take the data format of compressed sparse column (CSC) as input. As exampled in Fig. <ref type="figure" target="#fig_2">5</ref>(a), the 16 vertices are organized as several intervals (i.e. from I 1 to I 4 , each with four vertices), and the edges are organized as 4×4 shards (i.e. from S(1, 1) to S(4, 1), each with 16 edges at most). The intervals and shards are disjoint.</p><p>The feature vector length of each vertex is usually large, so exploiting the locality of features is critical. We group the vertices within the same interval together (e.g. I i ) and then process the aggregation of their source neighbors also interval by interval (i.e. traverse I j ), as expressed in Algorithm 2. Based on this flow, the feature accesses of all vertices in an interval are merged (see Fig. <ref type="figure" target="#fig_2">5(b)</ref>). The resulting benefits are twofold. First, the vertices in I i usually have overlapped neighbors in I j , therefore, the loaded feature data of I j can be reused when performing feature aggregation. Second, when traversing all I j , the intermediate aggregated results of I i are remained in buffer which can also be reused when performing feature update. In practice, edge shards usually are not square as our simplified illustration in Fig. <ref type="figure" target="#fig_2">5</ref>. The shard height is determined by the capacity of Input Buffer, while the shard width is determined by the capacity of Aggregation Buffer. The Edge Buffer size affects both height and width since it accommodates all edges of each shard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Data-Aware Sparsity Elimination (Dynamic)</head><p>With the data reuse optimization, we further attempt to reduce the redundant accesses since the graph connections are sparsely distributed. To eliminate the sparsity, we propose a window-based sliding and shrinking approach. The key idea is that we first slide the window (with the same size of an edge shard) downward until an edge appears in the top row, and then we shrink the window size by moving the bottom row upward until an edge is met. Window Sliding. Fig. <ref type="figure" target="#fig_2">5(c</ref>) illustrates the window sliding process. For each vertex interval, the top shard window gradually slides downward. It will not stop until an edge appears on its top row. Then a new window with the same size is created, whose top row follows the bottom row of its previous window. The stop criterion is the same for every window. In this way, windows continuously arise, slide downward, and stop. All the positions where windows stop are recorded as effectual shards. Window Shrinking. Although the window sliding can capture most effectual edges, sparsity still exists on the bottom side (within the purple dashed boxes). This is because the above sliding direction is downward. To reduce this part of sparsity, we propose window shrinking here. Specifically, the bottom row of each recorded window moves upward until it meets an edge, and then the window shrinks. Fig. <ref type="figure" target="#fig_2">5(d)</ref> illustrates the sliding and shrinking process of one window in detail and gives the final recorded effectual shards. Different from previous partition, the sizes of final shards are usually different due to the window shrinking. Given the effectual shards after sparsity elimination, the execution flow of Aggregation follows Algorithm 3. The only difference from Algorithm 2 is that the each neighbor interval I j is dynamically determined by window sliding and shrinking (see <ref type="bibr">Algorithm 4)</ref>. The starting row of each neighbor interval varies due to sliding and the interval length in the row dimension also varies due to shrinking. In this way, only the feature data of remaining neighbor vertices when performing the aggregation operation for each interval I i are loaded, which eliminates plenty of redundant accesses. Compared to traditional graph analytics, the feature data reuse from graph partitioning and redundant access reduction from sparsity elimination in GCNs are considerable efforts. This is because the feature of each vertex in GCNs is a vector with thousands of elements, while the feature data in traditional graph analytics are small, usually with one element for each vertex. Besides, our optimization achieves more when the Sampling operation is used, which increases sparsity since only sampled neighbors are required during Aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Combination Engine</head><p>The Combination operation at each vertex acts like a neural network, the execution of which is regular but computeintensive. Our design is based on the well-known systolic array. To adapt it for the two processing modes of Aggregation Engine (see Fig. <ref type="figure" target="#fig_5">4</ref>), we integrate multiple arrays rather than a single one, as shown in Fig. <ref type="figure" target="#fig_6">6(a)</ref>. A group of systolic arrays is assembled to form a systolic module. We allow a multigranular use of these systolic modules, including the independent working mode and cooperative working mode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Independent Working Mode</head><p>In this mode, the systolic modules work independently from each other. Each of them processes the MVM opera-  tions of a small group of vertices, as illustrated in Fig. <ref type="figure" target="#fig_7">7</ref>(a). The weight parameters for each module in this case are directly accessed from the Weight Buffer and just reused within module, as depicted in Fig. <ref type="figure" target="#fig_6">6(b</ref>). The advantage of this mode is the lower vertex latency because we can process the Combination operations of this small group of vertices immediately once their aggregated features are ready, without waiting for more vertices. This mode matches well with the vertex-disperse processing mode of Aggregation Engine in Fig. <ref type="figure" target="#fig_5">4</ref>, where the aggregated features are produced quickly but sequentially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Systolic Module 1</head><p>Aggregating Feature Vector</p><formula xml:id="formula_12">V 1 V 2 Systolic Array V 1 V 2 V 3 V (i+1) V (i+2) Weight V (i+1) V (i+2) Systolic Module i Weight Weight (a)<label>(b)</label></formula><p>Aggregating Feature Vector Aggregating Feature Vector </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Cooperative Working Mode</head><p>Besides working separately, these systolic modules can be further assembled together to simultaneously process more vertices, as shown in Fig. <ref type="figure" target="#fig_7">7(b)</ref>. Different from the immediate processing of vertices, this mode requires to assemble the aggregated features of a large group of vertices together before performing their Combination operations. The advantage is that, the weight parameters can flow from the Weight Buffer to the downstream systolic modules and then gradually to the upstream ones (see Fig. <ref type="figure" target="#fig_6">6(b</ref>)), which are greatly reused by all systolic arrays. This helps reduce the energy consumption.</p><p>No matter which working mode is selected in the Combination Engine, the weights can be reused inherently in Weight Buffer when processing different vertices. However, in traditional neural networks, especially MLPs, the weights cannot be shared without batching technique. The multi-granular systolic array design is also specific to our architecture in order to accommodate different application needs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Inter-Engine Optimization</head><p>To efficiently fuse the phase-by-phase execution, we orchestrate the execution pipeline and DRAM access of Aggre-gation engine and Combination engine by the Coordinator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Latency-or Energy-Aware Pipeline</head><p>Ping-Pong Aggregation Buffer. To reuse the aggregation results produced by the Aggregation engine, we add an Aggregation Buffer between the two engines. This buffer can be written by the Aggregation Engine and can be read by the Combination Engine. Before the final aggregated results are generated, the Aggregation Buffer stores the partial results that will be read by the Aggregation Engine for feature accumulation. In order to increase the parallelism of these two engines, we implement a ping-pong buffering mechanism where the Aggregation Buffer is split into two chunks. In this way, the executions of aggregation and combination are decoupled, which enables an inter-engine pipeline.</p><p>To accommodate the needs of different applications, we provide two pipeline modes as follows. Latency-Aware Pipeline. In this pipeline mode, the Combination Engine works in the systolic module independent mode. The aggregated features are produced vertex by vertex in the Aggregation Engine, and the following combination will be processed immediately once the aggregated features of a small group of vertices are ready. Therefore, the average processing latency for each vertex can be lower. The overall timing is illustrated in Fig. <ref type="figure" target="#fig_8">8(a)</ref>, where V denotes the vertices for aggregation, and I represents the neighbor intervals. Energy-Aware Pipeline. The energy-aware pipeline uses the systolic module cooperative mode in the Combination Engine. The vertex-by-vertex processing changes to a burst mode, where a large group of vertices will be processed together every time. Although the vertex latency is longer, the energy consumption can be reduced due to the weight propagation in the merged systolic arrays without redundant accesses. Fig. <ref type="figure" target="#fig_8">8</ref>(b) presents its timing sequence. </p><formula xml:id="formula_13">I1 Time V2 V3 V1 V4 (a) Aggregation Combination I2 X (k-1) X k I3 I4 (b) V1 V2 V3 V4 V1 V3 V2 V3 V1 V4 V1 V4 V1 Time V2 V3 V1 V4 X k V1 V2 V3 V4 V1 V3 V1 V4 V1 V4 V3 V2 V1 Small Group Large Group V4 V4</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Coordination of Off-chip Memory Access</head><p>It is hard to determine the memory bandwidth ratio between the two engines since the practical workloads usually vary between Aggregation and Combination. Moreover, the separation of memory systems will increase the configuration overheads and cause bandwidth waste. This is the reason why we use only one off-chip memory. Both the two engines access this memory at runtime, which causes a frequent switching of access locations, leading to inefficiencies. In total, there are four buffers (Edge Buffer &amp; Input Buffer in Aggregation Engine, and Weight Buffer &amp; Output Buffer in Combination Engine) that will be used for accessing the off-chip memory. Due to the interval processing and pipeline mechanism, these accesses usually come concurrently as shown in Fig. <ref type="figure" target="#fig_9">9</ref>(a). If we sequentially handle these access requests, the discontinuous addresses greatly degrade the utilization of row buffer within DRAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Edges Input Features Weights Output Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Request Issue</head><note type="other">Time</note><p>To solve this problem, we predefine an access priority (edges &gt; input f eatures &gt; weights &gt; out put f eatures) to assemble the discontinuous requests shown in Fig. <ref type="figure" target="#fig_9">9(b)</ref>. The motivation in using this priority is based on the access sequence when processing a vertex. The access requests are executed batch-by-batch. Therefore, low-priority accesses in the current batch are handled before high-priority accesses coming at the next batch, rather than always high-priority accesses first. With the improved continuity, the utilization of row buffer can be significantly enhanced. Next, we remap these reordered addresses to index the channel and bank using low bits. In this way, the memory channel-and bank-level parallelism can be further exploited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EVALUATION RESULTS</head><p>We first describe our experimental setup in Section 5.1. Next, to demonstrate the advantages of our design, we compare HyGCN to the state-of-the-art software framework in Section 5.2. Next, we give the detailed analysis of our optimization techniques in Section 5.3. Finally, we present a scalability exploration of our architecture in Section 5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>Methodology. The performance and energy of HyGCN are measured by using the following tools.</p><p>Architecture Simulator. We design and implement a cycleaccurate and execution-driven simulator to measure execution time in number of cycles. This simulator models the microarchitectural behaviors of each module, which is integrated with Ramulator <ref type="bibr" target="#b24">[24]</ref> to simulate the behaviors of memory accesses to High Bandwidth Memory (HBM).</p><p>CAD Tools. For the measurements of area, power, and critical path delay (in cycles) for each module, we implement and synthesize each module in Verilog. We use the Synopsys Design Compiler with the TSMC 12 nm standard VT library for the synthesis, and estimate the power using Synopsys PrimeTime PX. The slowest module has a critical path delay of 0.9 ns including the setup and hold time, putting the HyGCN comfortably at 1 GHz clock frequency.</p><p>Memory Measurements. The area, power, and access latency of the on-chip scratchpad memory are estimated using Cacti 6.5 <ref type="bibr" target="#b0">[1]</ref>. Since Cacti only supports down to 32 nm technologies, we apply four different scaling factors to convert them to 12 nm technology as shown in <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b36">36]</ref>. The energy of HBM 1.0 is estimated with 7 pJ/bit as in <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b41">41]</ref>. Benchmark Graph Datasets and GCN Models. Table <ref type="table" target="#tab_9">4</ref> and Table <ref type="table" target="#tab_10">5</ref> provide the information of the benchmark graph datasets and GCN models used in our evaluation. The datasets in Table <ref type="table" target="#tab_9">4</ref> are standard ones in the GCN domain. They are actually not small although the number of vertices is smaller than that used in conventional graph analytics, due to the long  </p><formula xml:id="formula_14">GCN (GCN) - Add &amp; |a k v |-128 GraphSage (GSC) 25 Max &amp; |a k v |-128 GINConv (GIN) - Add &amp; |a k v |-128-128 DiffPool (DFP) GCN pool GCN embedding Min &amp; |a k v |-128 Min &amp; |a k v |-128</formula><p>length of feature vectors. On CPU, the datasets with more than one graphs are tested by assembling randomly selected 128 graphs into a large graph before processing for GCN, GSC, and GIN or batching the same number of graphs for DFP. On HyGCN, the testing methods remain the same with CPU except that the selected graphs for DFP are processed one by one rather than in a batched mode. Baseline Platform. To compare the performance and energy consumption of HyGCN with state-of-the-art works, we evaluate PyTorch Geometric (PyG) <ref type="bibr" target="#b15">[15]</ref> on a Linux workstation equipped with two Intel Xeon E5-2680 v3 CPUs and 378 GB DDR4 memory and on an NVIDIA V100 GPU, denoted as PyG-CPU and PyG-GPU, respectively. Table <ref type="table" target="#tab_11">6</ref> lists the system configurations for above implementations. Note: GPU's on-chip memory includes the register files, and L1 and L2 caches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Overall Results</head><p>We first apply our algorithm optimization on PyTorch Geometric. And then, we compare our work (HyGCN) with PyG-CPU and PyG-GPU in terms of speedup, energy consumption, utilization of DRAM bandwidth, and DRAM access. Finally, the area and power of our design is presented.</p><p>• Algorithm Optimization on PyG Framework. To show the effect of our algorithm optimization on CPU and GPU platforms, we implement our algorithm optimization proposed in Section 4.3 on PyG framework. The graph is partitioned into multiple shards and they are executed shard by shard (see Fig. <ref type="figure" target="#fig_2">5(a)</ref>). The number of partitions is determined by the capacity of L2 Cache and the length of feature vectors. Note that, PyG leverages the Pytorch Scatter library <ref type="bibr" target="#b3">[4]</ref> for the acceleration of Aggregation on both CPU and GPU. It helps eliminate the sparsity and exploit the edge parallelism by executing each vertex's Aggregation in a hardware thread.  Furthermore, the hardware-optimized libraries such as Intel MKL <ref type="bibr" target="#b21">[21]</ref> and NVIDIA cuBLAS library <ref type="bibr" target="#b31">[31]</ref> are used to accelerate Combination on CPU and GPU, respectively. Fig. <ref type="figure" target="#fig_10">10</ref>(a) shows the speedup of PyG-CPU with our algorithm optimization (PyG-CPU-OP) over the naive one without optimization. Thanks to the algorithm improvement, PyG-CPU-OP achieves 2.3× speedup on average. The performance benefits come from the reduction of frequent replacement of feature vectors since the reusable features after graph partition and the intermediate results of Aggregation are buffered in L2 Cache. Fig. <ref type="figure" target="#fig_10">10(b</ref>) presents the same testing on GPU. The performance of PyG-GPU-OP degrades since only a small amount of vertices are processed for each graph partition, which cannot fully utilize thousands of hardware threads on GPU and miss the core advantage of GPU to hide the access latency through many parallel threads. As a result, it is inefficient for GPU to exploit our optimization to improve performance. The optimized PyG-CPU and the naive PyG-GPU are used as baselines in the following evaluation.</p><p>• Speedup. Fig. <ref type="figure" target="#fig_10">10(c</ref>) depicts that HyGCN achieves average 1509× and 6.5× speedup compared with PyG-CPU and PyG-GPU, respectively. The performance improvement comes from the individual optimizations in Aggregation Engine &amp; Combination Engine, and the inter-engine pipeline &amp; coordination. First, the parallel processing in SIMD cores and systolic arrays speed up the computations. Second, the graph partition and sparsity elimination increase the feature reuse and decrease redundant accesses in Aggregation Engine, which saves DRAM bandwidth. Third, the weight parameters are reused efficiently in Combination Engine, which also helps better utilize the bandwidth. Finally, the inter-engine pipeline further optimizes the parallelism and the off-chip memory access coordination improves the DRAM access efficiency.</p><p>For PyG-CPU and PyG-GPU, abundant DRAM accesses and synchronization overheads lead to performance degradation. Specifically, the high randomness of neighbor indices results in poor locality of neighbors' feature vectors, causing many unnecessary DRAM accesses. From the perspective of computation, PyG-CPU and PyG-GPU leverage the hardware-optimized functions (such as scatter <ref type="bibr" target="#b3">[4]</ref> and matrix multiplication <ref type="bibr" target="#b31">[31]</ref>) to perform GCNs in a coarse-grained fashion. Although it is the best way to utilize CPU and GPU, it loses the inter-phase parallelism and produces redundant operations. The delay for data copy and synchronization between threads further degrades the performance.</p><p>In term of models, GIN achieves better performance than others. The underlying reason is that GIN executes Aggregation first on PyG-CPU and PyG-GPU, which introduces abundant computations and accesses since the feature vector size is an order of magnitude larger than that after Combination. By contrast, other models execute Combination first, which greatly reduces the feature length before performing Aggregation. This difference causes the inefficient execution of GIN on CPU and GPU, while our HyGCN can maintain the performance to a great extent due to the parallel processing and data reuse. For DFP, it includes three matrix multiplications (see Equation ( <ref type="formula" target="#formula_7">8</ref>)) that can be efficiently executed on CPU and GPU. Therefore, our speedup when performing DFP is relatively lower. The GSC model consumes significant time on the Sampling operation in a preprocessing step, which is not included in the result of PyG-CPU and PyG-GPU. For example on the RD dataset, the preprocessing can cost up to 15 seconds while the execution time is only 0.65 second on PyG-CPU and 0.0025 second on PyG-GPU. In our work, the Sampling operation is executed together with Aggregation and considered in the reported result. Thus, the performance of our work is lower than PyG-GPU in Fig. <ref type="figure" target="#fig_10">10(c</ref>) but the overall execution time ratio is 0.136 second v.s. 15.7 seconds.</p><p>• Energy Consumption. As Fig. <ref type="figure" target="#fig_11">11</ref> shows, HyGCN consumes only 0.04% and 10% energy on average compared to PyG-CPU and PyG-GPU, respectively. The energy consumption of all platforms includes the off-chip memory. Note that, although the results of PyG-CPU and PyG-GPU do not include the overhead of the Sampling operation, they are still costly. For example, the Sampling energy of GSC is 2715J on the RD dataset. In contrast, our work consumes only 1.79J compared to the total 2716J in PyG-GPU.    As aforementioned, GIN causes additional computations and data accesses when performing Aggregation, which introduces extra energy consumption on PyG-CPU. Although HyGCN cannot reduce these computations, the optimizations of data reuse, sparsity elimination, and inter-engine pipeline can reduce redundant accesses to these additional   data. Among the architectural components, Combination Engine consumes most of the energy due to the intensive computation of MVMs as depicted in Fig. <ref type="figure" target="#fig_13">12</ref>, while Aggregation Engine consumes more energy on high-degree graph datasets (i.e., CL and RD).</p><p>• DRAM Bandwidth Utilization. As seen in Fig. <ref type="figure" target="#fig_14">13</ref>, HyGCN demonstrates 16× and 1.5× improvement on average on the utilization of DRAM bandwidth compared with PyG-CPU and PyG-GPU, respectively. The high bandwidth utilization of HyGCN and PyG-GPU derive from the high-degree parallelism. By contrast, PyG-CPU cannot sufficiently exploit the bandwidth, since there is only one thread most of time to reduce the heavy overheads of frequent thread creation. Our consistent lower bandwidth on the CL dataset is due to the higher data reuse, which benefits from denser connections.</p><p>• DRAM Access. Although the 16MB on-chip memory is much smaller than the 60MB L3 cache on CPU and 34MB on GPU, HyGCN accesses only 21% and 33% of off-chip data compared with PyG-CPU and PyG-GPU on average, respectively, as given in Fig. <ref type="figure" target="#fig_5">14</ref>. This benefits from our data reuse optimizations, sparsity elimination, and the immediate processing between two engines. On the CL dataset for GCN, GSC, and GIN, multiple graphs are assembled to form a larger one before being processed, which results in intensive sparsity. HyGCN can efficiently eliminate the sparsity via window sliding and shrinking, thus avoiding unnecessary data accesses. Whereas, PyG-CPU and PyG-GPU produce many unnecessary accesses due to the irregularity in Aggregation phase and without the fusion of phase-by-phase execution.</p><p>As aforementioned, the results of PyG-CPU and PyG-GPU do not include the data access of the Sampling operation. For example, the Sampling access volume of GSC is 56.5GB on the RD dataset. In contrast, our work only accesses 28GB data, compared with the total 58GB in PyG-GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Power and Area</head><p>The total power and area of HyGCN are only 6.7 W and 7.8 mm 2 , respectively. For the on-chip buffer, we use eDRAM to reduce both the area and energy consumption. For the com-putation precision, we use 32-bit fixed point that is enough to maintain the accuracy of GCN inference. Table <ref type="table" target="#tab_15">7</ref> provides area and power breakdown in terms of buffer, computation, and control. The computation resources of two engines consume most of power (&gt;64%) and area (&gt;44%) to perform the edge-centric aggregation and MVMs-based combination. The Coordinator occupies ∼35% of the total area since it has a large Aggregation Buffer. The control overhead is small (only 1.2% power and &lt;0.45% area) owing to the simple implementations of eSched, Sampler, Sparsity Eliminator, vSched, Coordinator, and Memory Handler. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Optimization Analysis</head><p>In this subsection, we analyze the effect of our optimization techniques including sparsity elimination, inter-engine pipeline, and off-chip memory access coordination. The benchmark model is GCN mentioned in Table <ref type="table" target="#tab_10">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Sparsity Elimination Optimization</head><p>We evaluate HyGCN with and without sparsity elimination. This experiment runs only Aggregation Engine to avoid the interference of other blocks. Fig. <ref type="figure" target="#fig_2">15</ref>(a) shows that HyGCN achieves 1.1∼3× speedup with the optimization of sparsity elimination. The performance gain is due to fewer redundant DRAM accesses as reflected in Fig. <ref type="figure" target="#fig_2">15(b)</ref>, which benefits from eliminated sparsity as given in Fig. <ref type="figure" target="#fig_2">15(c</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Inter-Engine Pipeline Optimization</head><p>First, we measure the overall performance with and without inter-engine pipeline optimization (PP v.s. N-PP). With the pipeline optimization, the execution time of GCN is reduced by 27%-53%, as shown in Fig. <ref type="figure" target="#fig_6">16(a</ref>). On one hand, the Aggregation Engine and Combination Engine work in parallel with inter-engine pipeline. On the other hand, the DRAM accesses occupy most of the execution time (see Fig. <ref type="figure" target="#fig_6">16(b)</ref>), therefore the inter-engine pipeline helps improve the performance by decreasing DRAM accesses of the intermediate aggregation results between two engines. It is observed from Fig. <ref type="figure" target="#fig_6">16(b</ref>) that total DRAM accesses are significantly reduced to only 50%-73% with this pipeline optimization.</p><p>Second, we compare the vertex latency and energy of Combination Engine with energy-aware pipeline and latencyaware pipeline (Epipe v.s. Lpipe). From Fig. <ref type="figure" target="#fig_6">16(c</ref>), the Lpipe reduces the average latency for each vertex by 7%-29% via the immediate processing without waiting for the aggregation results of many vertices. By contrast, as shown in Fig. <ref type="figure" target="#fig_6">16(d)</ref>, the Epipe saves energy consumption by 35% via assembling a large group of vertices to process together for reusing weight parameters aggressively. In practice, the application requirement determines the pipeline mode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Memory Coordination Optimization</head><p>To show the effect of the memory access coordination, we present the execution time and bandwidth utilization with and without coordination in Fig. <ref type="figure" target="#fig_7">17</ref>  b), respectively. With the memory access coordination for address continuity, the DRAM row buffers are better utilized and the channel-/bank-level parallelism is better exploited, which saves 73% of time and improves 4× bandwidth on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Scalability Exploration</head><p>The following evaluations are measured in GSC model.</p><p>• Sparsity Elimination with Sampling. The Sampling operation increases the sparsity, thus it has the potential to enlarge the benefits produced by sparsity elimination. In Fig. <ref type="figure" target="#fig_18">18(a)-(c</ref>), horizontal axis sweeps the sampling factor. It indicates that only 1 sampling f actor edges of each vertex are sampled to perform aggregation. As the increasing sampling factor, the performance is significantly improved on the PB dataset by reducing the DRAM accesses owing to the higher sparsity. For other datasets, since many edges have been removed, the Combination phase gradually dominates the execution time. Therefore, there is no significant speedup. Note that the sampling factor cannot be too high, as it might harm the accuracy of applications.</p><p>• Capacity of Aggregation Buffer. The size of the Aggregation Buffer affects the execution time, amount of data accesses, and even the effect of sparsity elimination. As the capacity of Aggregation Buffer increases from 2 MB to 32 MB, the exeuction time is decreased as shown in Fig. <ref type="figure" target="#fig_18">18(d)</ref>. This can be explained from two aspects: i) more intermediate aggregated feature data can be cached in on-chip buffer, leading to larger shard width when partitioning the graph and thus less execution loops; ii) larger shard means that the neighbor features can be reused more often, leading to less DRAM accesses (see Fig. <ref type="figure" target="#fig_18">18(e)</ref>). However, larger shard also enlarges the window size during the sparsity elimination, which results in higher sparsity that cannot be eliminated (see Fig. <ref type="figure" target="#fig_18">18(f)</ref>).</p><p>• Size of Systolic Module. In this experiment, we fix the number of total systolic arrays but change the size of each systolic module, and then to measure the cost of Combination Engine. Different from the systolic module with 4×128 systolic arrays in Table <ref type="table" target="#tab_11">6</ref>, here we treat 1×128 systolic arrays as a basic systolic module. Based on the initial 32 systolic modules, we gradually decrease the number of systolic modules under the restriction of fixed number of total systolic arrays. It is observed that longer latency for a vertex is consumed as the partition of systolic modules becomes more coarse-grained as shown Fig. <ref type="figure" target="#fig_18">18</ref>(g)(bar). This is caused by the longer time to assemble a larger group of vertices to be processed together. Fortunately, the energy consumption can be reduced as shown Fig. <ref type="figure" target="#fig_18">18</ref>(g)(red line) because the weight parameters are reused by more vertices within each larger systolic module. We only present the average energy result of these datasets for simplicity. In our architecture design, we set the systolic module with size of 4×128 arrays to achieve a good trade-off between the latency and energy costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">DISCUSSION</head><p>In order to leverage our proposed PM, PyG needs to be significantly modified for its coarse-grain message-passing mechanism to stream Aggregation and Combination for each vertex. Note that although these two phases can be streamed after modification, it also misses the advantage of hardwareoptimized operations, such as matrix multiplication operation <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b31">31]</ref>. Furthermore, further challenges exist with 1) inefficient memory subsystem due to workload-agnosticism <ref type="bibr" target="#b17">[17]</ref>, 2) difficulty in data reuse like systolic arrays <ref type="bibr" target="#b22">[22]</ref>, and 3) expensive on-line preprocessing for workload reorganization and streaming.</p><p>Following concerns make training unsuitable as a starting work to explore GCN hardware. First, training involves three passes with data dependency: forward, backward, and update, whose compute and memory patterns are more complex than that of inference with only the forward pass. Second, the gradient propagation in graphs is far more complicated than layer-by-layer propagation in neural networks. However, training accelerators can leverage our architecture to design the forward pass, and would need specialized blocks for other passes and an efficient memory hierarchy to connect them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">RELATED WORK</head><p>Plenty of software frameworks for graph analytics and neural networks have been presented to release the programming efforts while achieving high performance on modern generalpurpose architectures <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b37">37]</ref>. However, all of them only work well for the single-pattern workloads. Therefore, a large number of software frameworks for hybrid-pattern GCNs are proposed recently <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b47">47]</ref>. For instance, Py-Torch Geometric <ref type="bibr" target="#b15">[15]</ref> leverages message-passing framework to enhance its expression ability and the hardware-optimized operations (e.g. scatter and matrix multiplication) so that the GCN workloads can be accelerated. Unfortunately, the distinct execution pattern regarding computation and access between the Aggregation phase and the Combination phase produces processing inefficiencies on traditional platforms. GCNs demand specialized architecture design. With the emergence of graph analytics and neural networks workloads, a lot of hardware architecture designs are proposed to accelerate these workloads <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b33">33]</ref>. For example, Graphicionado <ref type="bibr" target="#b17">[17]</ref> is tailored for graph analtyics; while TPU <ref type="bibr" target="#b22">[22]</ref> focuses on the acceleration of neural networks. However, GCNs behave not only like the graph processing (Aggregation) but also like neural networks (Combination), leading to intrinsic hybrid design requirement. Therefore, current specialized architectures cannot efficiently perform GCNs since they just handle one of the two sides.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">CONCLUSION</head><p>GCNs are becoming widely adopted for analyzing graph data and are comprised of Aggregation and Combination phases. In this work, we identify that the execution patterns of these two phases are distinct, even almost opposite, which requires separate design requirements. Besides, the high intravertex parallelism in Aggregation phase, the highly reusable inter-vertex data in Combination phase, and the opportunity to fuse phase-by-phase execution introduced by the new features of GCNs need to be leveraged for better performance. To this end, we propose a GCN accelerator, HyGCN, with hybrid architecture. First, we build edge-and MVM-centric programming model to exploit various parallelisms and enable hardware transparency. Next, we propose the hardware design with two efficient engines to optimize the two phases correspondingly. The latency-and energy-aware inter-engine pipelines are orchestrated to improve the overall latency and energy according to system needs. The off-chip memory accesses between the two engines are carefully coordinated to improve the efficiency. Finally, through comprehensive evaluations, HyGCN demonstrates significant improvements compared to the software framework running on CPU and GPU. We believe our work will stimulate more attention on specialized hardware for increasingly important GCNs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of the GCN model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Architecture overview of HyGCN.The Aggregation Engine aims to realize the efficient execution of irregular accesses and computations. To exploit the edge-level parallelism, a task scheduler (eSched) is designed to assign the edge processing workloads onto SIMD cores. To support the Sampling operation, we introduce a Sampler into the Aggregation Engine. The Sampler selects edges from the edge list of each vertex using a uniform or predefined distribution in terms of index interval. The former indices for edge sampling are based on dynamic generation while the latter ones are predefined and can be read from</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Static graph partition for data reuse and dynamic sparsity elimination to reduce redundant accesses: (a) interval-shard partition; (b) interval-wise feature access; (c) window sliding; (d) window shrinking.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 2 :</head><label>2</label><figDesc>Interval-wise Aggregation 1 for each interval I i in X k do 2 agg_res ← init(); 3 for each interval I j in X (k−1) do 4 agg_res ← Aggregation(I j , agg_res);</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 3 : 2 row_pos ← 1 5 ( 8 I</head><label>32158</label><figDesc>Interval-wise Aggregation with Sparsity Elimination1 for each interval I i in X k do I j , row_pos) ← GetOneEffectInterval( X (k−1) , A, I i , row_pos); 6 agg_res ← Aggregation(I j , agg_res); 7 while (I j != ∅); i ← Combination(agg_res); 9 end</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 4 :</head><label>4</label><figDesc>GetOneEffectInterval Window Sliding 1 while (edge(row_pos, v) == ∅ f or ∀v ∈ I i ) do 2 row_pos ← row_pos + 1; 3 end 4 win start ← row_pos; 5 win end ← row_pos +Window height − 1; 6 row_pos ← win end + 1; Window Shrinking 7 while (edge(win end , v) == ∅ f or ∀v ∈ I i ) do 8 win end ← win end − 1; 9 end 10 I e f f ectual ← X (k−1) [win start : win end ]; 11 return I e f f ectual ;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Combination Engine design: (a) multiple systolic modules; (b) different dataflow patterns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Different use of the systolic arrays: (a) independent working mode; (b) cooperative working mode.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Timing illustration of different pipeline modes: (a) latency-aware pipeline; (b) energy-aware pipeline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Coordination of off-chip memory access.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Comparistion to PyG-CPU and PyG-GPU: Speedup of our algorithm optimization on (a) CPU and (b) GPU; (c) Speedup over the optimized PyG-CPU. OoM means the evaluation fails in running on GPU due to out of memory.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Normalized energy over PyG-CPU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>CL PB RD IB CR CS CL PB RD IB CR CS CL PB RD IB CL</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Energy breakdown of HyGCN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Bandwidth utilization of all platforms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 14 :Figure 15 :Figure 16 :Figure 17 :</head><label>14151617</label><figDesc>Figure 14: Normalized data access to PyG-CPU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>(a)  and Fig.17</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>(</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: Scalability exploration. i) sparsity elimination with different sampling factor: (a) Execution time, (b) DRAM access, and (c) sparsity reduction; ii) capacity of Aggregation Buffer: (d) execution time, (e) DRAM access, and (f) sparsity reduction; iii) size of the systolic module: (g) vertex latency and energy of Combination Engine.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Aggregation Engine and Combination Engine, respectively. • We propose a flexible inter-engine pipeline and a prioritybased memory access coordination to efficiently fuse the execution of Aggregation phase and Combination phase. • We implement our architecture design in RTL and evalu-</figDesc><table><row><cell>ate it using a detailed microarchitectural simulation. We</cell></row><row><cell>use four well-known GCN models on six popular graph</cell></row><row><cell>datasets. Compared to the state-of-the-art software frame-</cell></row><row><cell>work PyTorch Geometric [15] running on Intel Xeon CPU</cell></row><row><cell>and NVIDIA V100 GPU, our work achieves on average</cell></row><row><cell>1509× speedup with 2500× energy reduction and 6.5×</cell></row><row><cell>speedup with 10× energy reduction, respectively.</cell></row></table><note>• We propose a GCN accelerator, HyGCN, using a hybrid architecture to efficiently perform GCNs. First, we build a programming model to enable our hardware design to exploit various parallelisms inherent in this domain. Next, we propose a hardware design to tackle irregularity and leverage regularity with</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>GCN Notations.    </figDesc><table><row><cell>Notation</cell><cell>Meaning</cell><cell>Notation</cell><cell>Meaning</cell></row><row><cell>G</cell><cell>graph G = (V, E)</cell><cell>V</cell><cell>vertices of G</cell></row><row><cell>E</cell><cell>edges of G</cell><cell>Dv</cell><cell>degree of vertex v</cell></row><row><cell>e (i, j)</cell><cell cols="3">edge between vertex i and j N(v) (S(v)) (sampling subset of) v' neighbor set</cell></row><row><cell>A (Ai j)</cell><cell>(element of) adjacent matrix</cell><cell>av</cell><cell>aggregation feature vector of v</cell></row><row><cell>hG</cell><cell>feature vector of G</cell><cell>W</cell><cell>combination weight matrices</cell></row><row><cell>hv</cell><cell>feature vector of vertex v</cell><cell>b</cell><cell>combination bias vectors</cell></row><row><cell>X</cell><cell>initialized feature matrix</cell><cell>Z</cell><cell>embedding matrix</cell></row><row><cell>C</cell><cell>assignment matrix</cell><cell>ε</cell><cell>learnable parameter</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Quantitative Characterization on CPU.</figDesc><table><row><cell></cell><cell>Aggregation</cell><cell>Combination</cell></row><row><cell>DRAM Byte per Ops</cell><cell>11.6</cell><cell>0.06</cell></row><row><cell>DRAM Access Energy per Ops</cell><cell>170nJ</cell><cell>0.5nJ</cell></row><row><cell>L2 Cache MPKI</cell><cell>11</cell><cell>1.5</cell></row><row><cell>L3 Cache MPKI</cell><cell>10</cell><cell>0.9</cell></row><row><cell>Ratio of Synchronization Time</cell><cell>-</cell><cell>36%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Different Execution Patterns of Aggregation Phase and Combination Phase.</figDesc><table><row><cell></cell><cell>Aggregation</cell><cell>Combination</cell></row><row><cell>Access Pattern</cell><cell>Indirect &amp; Irregular</cell><cell>Direct &amp; Regular</cell></row><row><cell>Data Reusability</cell><cell>Low</cell><cell>High</cell></row><row><cell>Computation Pattern</cell><cell>Dynamic &amp; Irregular</cell><cell>Static &amp; Regular</cell></row><row><cell>Computation Intensity</cell><cell>Low</cell><cell>High</cell></row><row><cell>Execution Bound</cell><cell>Memory</cell><cell>Compute</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Combination Engine Activate Unit Aggregation Engine Weight Buffer Input Buffer Output Buffer DRAM</head><label></label><figDesc></figDesc><table><row><cell>PE</cell><cell>PE</cell><cell>PE</cell></row><row><cell>PE</cell><cell>PE</cell><cell>PE</cell></row><row><cell>PE</cell><cell>PE</cell><cell>PE</cell></row><row><cell>PE</cell><cell>PE</cell><cell>PE</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>SIMD Sampler eSched SIMD SIMD vSched Edge Buffer Memory Access Handler Coordinator Aggregation Buffer Sparsity Eliminator</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Dataset information<ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b42">42]</ref>.</figDesc><table><row><cell>Dataset</cell><cell cols="2">#Vertex Feature Length</cell><cell>#Edge</cell><cell>Storage</cell></row><row><cell>IMDB-BIN (IB)</cell><cell>2,647</cell><cell>136</cell><cell>28,624</cell><cell>1.5MB</cell></row><row><cell>Cora (CR)</cell><cell>2,708</cell><cell>1,433</cell><cell>10,556</cell><cell>15MB</cell></row><row><cell>Citeseer (CS)</cell><cell>3,327</cell><cell>3,703</cell><cell>9,104</cell><cell>47MB</cell></row><row><cell>COLLAB (CL)</cell><cell>12,087</cell><cell>492</cell><cell>1,446,010</cell><cell>28MB</cell></row><row><cell>Pubmed (PB)</cell><cell>19,717</cell><cell>500</cell><cell>88,648</cell><cell>38MB</cell></row><row><cell>Reddit (RD)</cell><cell>232,965</cell><cell>602</cell><cell cols="2">114,615,892 972MB</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Configuration of convolution layers. Here |a k v | denotes the length of feature vector a k v .</figDesc><table /><note>#Sampling Neighbors Aggregation &amp; Combination (MLP)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>System configurations.</figDesc><table><row><cell></cell><cell cols="2">PyG-CPU PyG-GPU</cell><cell>HyGCN</cell></row><row><cell>Compute</cell><cell>2.5 GHz @</cell><cell>1.25Ghz @</cell><cell>1 GHz @ 32 SIMD16 cores and</cell></row><row><cell>Unit</cell><cell>24 cores</cell><cell>5120 cores</cell><cell>8 systolic modules (each with 4×128 arrays)</cell></row><row><cell>On-chip Memory</cell><cell>60MB</cell><cell>34MB</cell><cell>128 KB (Input), 2 MB (Edge), 2 MB (Weight), 4 MB (Output) and 16 MB (Aggregation)</cell></row><row><cell>Off-chip</cell><cell>136.5GB/s</cell><cell>∼900GB/s</cell><cell>256GB/s</cell></row><row><cell>Memory</cell><cell>DDR4</cell><cell>HBM∼2.0</cell><cell>HBM∼1.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 7 :</head><label>7</label><figDesc>Layout characteristics of HyGCN</figDesc><table><row><cell>Module</cell><cell cols="3">Component Power (%) Area (%)</cell></row><row><cell></cell><cell>Buffer</cell><cell>2.37</cell><cell>5.41</cell></row><row><cell>Aggregation Engine</cell><cell>Computation</cell><cell>3.85</cell><cell>1.43</cell></row><row><cell></cell><cell>Control</cell><cell>0.48</cell><cell>0.18</cell></row><row><cell></cell><cell>Buffer</cell><cell>14.4</cell><cell>15.13</cell></row><row><cell>Combination Engine</cell><cell>Computation</cell><cell>60.52</cell><cell>42.96</cell></row><row><cell></cell><cell>Control</cell><cell>0.31</cell><cell>0.07</cell></row><row><cell>Coordinator</cell><cell>Buffer Control</cell><cell>17.66 0.41</cell><cell>34.64 0.19</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers of HPCA 2020 and the sealer in Scalable Energy-efficient Architecture Lab (SEAL) for their constructive and insightful comments. This work was supported by the National Key Research and Development Program of China (Grant No. 2018YFB1003501), the National Natural Science Foundation of China (Grant No. 61732018, 61872335, and 61802367), the Strategic Priority Research Program of Chinese Academy of Sciences (Grant No. XDA 18000000), the Innovation Project Program of the State Key Laboratory of Computer Architecture (Grant No. CARCH4408, CARCH4412, and CARCH4502), the National Science Foundation (Grant No. 1730309, 1725447 and CCF 1740352), and SRC nCORE NC-2766-A.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>Cacti</surname></persName>
		</author>
		<ptr target="http://www.hpl.hp.com/research/cacti/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep graph library</title>
		<ptr target="https://docs.dgl.ai" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A distributed graph deep learning framework</title>
		<ptr target="https://github.com/alibaba/euler" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Pytorch extension library of optimized scatter operations</title>
		<ptr target="https://github.com/rusty1s/pytorch_scatter" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th USENIX Symposium on Operating Systems Design and Implementation</title>
				<imprint>
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Savannah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-11">Nov. 2016</date>
			<publisher>USENIX Association</publisher>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fastgcn: Fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<idno>abs/1801.10247</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">DianNao: A small-footprint high-throughput accelerator for ubiquitous machine-learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Temam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems, ser. ASPLOS &apos;14</title>
				<meeting>the 19th International Conference on Architectural Support for Programming Languages and Operating Systems, ser. ASPLOS &apos;14</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="269" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Eyeriss: A spatial architecture for energy-efficient dataflow for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA)</title>
				<imprint>
			<biblScope unit="page" from="367" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Nxgraph: An efficient graph processing system on a single machine</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE 32nd International Conference on Data Engineering (ICDE)</title>
				<imprint>
			<date type="published" when="2016-05">May 2016</date>
			<biblScope unit="page" from="409" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Application data prefetching on the IBM blue gene/q supercomputer</title>
		<author>
			<persName><forename type="first">I.-H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 International Conference for High Performance Computing, Networking, Storage and Analysis</title>
				<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sequential hardware prefetching in shared-memory multiprocessors</title>
		<author>
			<persName><forename type="first">F</forename><surname>Dahlgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stenstrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="733" to="746" />
			<date type="published" when="1995-07">July 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning steady-states of iterative algorithms over graphs</title>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Graph nets library</title>
		<author>
			<persName><surname>Deepmind</surname></persName>
		</author>
		<ptr target="https://deepmind.com/research/open-source/graph-nets-library" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gómez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
				<meeting>the 28th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
	<note>ser. NIPS&apos;15</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Protein interface prediction using graph convolutional networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shariat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ben-Hur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="6530" to="6539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Graphicionado: A high-performance and energy-efficient accelerator for graph analytics</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Satish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
				<imprint>
			<date type="published" when="2016-10">Oct. 2016</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Representation learning on graphs: Methods and applications</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Eng. Bull</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="52" to="74" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adaptive sampling towards fast graph representation learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<author>
			<persName><surname>Mkl</surname></persName>
		</author>
		<ptr target="https://software.intel.com/en-us/mkl" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">In-datacenter performance analysis of a tensor processing unit</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Borchers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>-L. Cantin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Coriell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gelb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">V</forename><surname>Ghaemmaghami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gottipati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gulland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hagmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hogberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hundt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hurt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaffey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaworski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Khaitan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Killebrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Laudon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lundin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mackean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maggiore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mahony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Narayanaswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Norrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Omernick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Penukonda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Phelps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Salek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Samadiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Severn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sizikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Snelham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Souter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Steinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Thorson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Toma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tuttle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wilcox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual International Symposium on Computer Architecture</title>
				<meeting>the 44th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
	<note>ser. ISCA &apos;17. ACM</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Benchmark data sets for graph kernels</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<ptr target="http://graphkernels.cs.tu-dortmund.de" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ramulator: A fast and extensible dram simulator</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Archit. Lett</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="49" />
			<date type="published" when="2016-01">Jan. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>abs/1609.02907</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">How much parallelism is there in irregular applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burtscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Inkulu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pingali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Casãgaval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, ser. PPoPP &apos;09</title>
				<meeting>the 14th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, ser. PPoPP &apos;09</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">GraphChi: Large-scale graph computation on just a PC</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Blelloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th USENIX Conference on Operating Systems Design and Implementation, ser. OSDI&apos;12. USENIX Association</title>
				<meeting>the 10th USENIX Conference on Operating Systems Design and Implementation, ser. OSDI&apos;12. USENIX Association</meeting>
		<imprint>
			<biblScope unit="page" from="31" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pytorch-biggraph: A large-scale graph embedding system</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wehrstedt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Peysakhovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1903">1903.12287, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Nvidia tesla: A unified graphics and computing architecture</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lindholm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nickolls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Montrym</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="39" to="55" />
			<date type="published" when="2008-03">March 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Tigr: Transforming irregular graphs for gpu-friendly graph processing</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Nodehi Sabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems, ser. ASPLOS &apos;18</title>
				<meeting>the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems, ser. ASPLOS &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="622" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<author>
			<persName><surname>Cublas</surname></persName>
		</author>
		<ptr target="https://developer.nvidia.com/cublas" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Highlights of the high-bandwidth memory (hbm) standard</title>
		<author>
			<persName><forename type="first">M</forename><surname>O'connor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Memory Forum Workshop</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Energy efficient architecture for graph analytics accelerators</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Ozdal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yesil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ayupov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Greth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ozturk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA)</title>
				<imprint>
			<biblScope unit="page" from="166" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Graphmat: High performance graph analytics made productive</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Satish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M A</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Dulloor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Vadlamudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dubey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
				<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2015-07">Jul. 2015</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1214" to="1225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scaling the power wall: A path to exascale</title>
		<author>
			<persName><forename type="first">O</forename><surname>Villa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oconnor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bolotin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nellans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luitjens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sakharnykh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Scudiero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC &apos;14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</title>
				<imprint>
			<date type="published" when="2014-11">Nov 2014</date>
			<biblScope unit="page" from="830" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Gunrock: A high-performance graph processing library on the gpu</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Riffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Owens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, ser. PPoPP &apos;16</title>
				<meeting>the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, ser. PPoPP &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1901">1901.00596, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno>abs/1810.00826</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno>abs/1806.03536</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Alleviating irregularity in graph analytics acceleration: A hardware/software co-design approach</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Basak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Akgun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52Nd Annual IEEE/ACM International Symposium on Microarchitecture, ser. MICRO &apos;52</title>
				<meeting>the 52Nd Annual IEEE/ACM International Symposium on Microarchitecture, ser. MICRO &apos;52<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="615" to="628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining -KDD &apos;15</title>
				<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining -KDD &apos;15</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, ser. KDD &apos;18</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, ser. KDD &apos;18<address><addrLine>United Kingdom</addrLine></address></meeting>
		<imprint>
			<publisher>London</publisher>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="4800" to="4810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep learning on graphs: A survey</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<idno>abs/1812.04202</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Graph neural networks: A review of methods and applications</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1812.08434</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Aligraph: A comprehensive graph neural network platform</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<idno>abs/1902.08730</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
