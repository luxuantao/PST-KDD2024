<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GRAPH CONVOLUTIONS THAT CAN FINALLY MODEL LOCAL STRUCTURE</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-11-30">30 Nov 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Rémy</forename><surname>Brossard</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AnotherBrain</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Oriel</forename><surname>Frigo</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AnotherBrain</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Dehaene</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AnotherBrain</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GRAPH CONVOLUTIONS THAT CAN FINALLY MODEL LOCAL STRUCTURE</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-11-30">30 Nov 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2011.15069v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite quick progress in the last few years, recent studies have shown that modern graph neural networks can still fail at very simple tasks, like detecting small cycles <ref type="bibr" target="#b16">(Loukas, 2019;</ref><ref type="bibr" target="#b3">Chen et al., 2020)</ref>. This hints at the fact that current networks fail to catch information about the local structure, which is problematic if the downstream task heavily relies on graph substructure analysis, as in the context of chemistry. We propose a very simple correction to the now standard GIN convolution <ref type="bibr" target="#b24">(Xu et al., 2019)</ref> that enables the network to detect small cycles with nearly no cost in terms of computation time and number of parameters. Tested on real life molecule property datasets, our model consistently improves performance on large multi-tasked datasets over all baselines, both globally and on a per-task setting.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph learning encompass a wide and particularly diverse set of problems. A recent surge of interest in the deep learning community and the publication of many graph datasets already lead to many successful applications, opening the way to exciting new perspectives. Graphs can naturally model the data involved in many applications, such as visual reasoning or semantic segmentation, community detection and social network predictions, recommender systems, traffic predictions, and much more <ref type="bibr" target="#b25">(Zhou et al., 2018;</ref><ref type="bibr" target="#b23">Wu et al., 2020)</ref>. Among those problems, supervised whole graph property prediction is an important topic, essentially applied to molecular property prediction, which will be the center of interest of this work. Data-based approaches, as opposed to more traditional simulation-based methods, present a great potential as the required time for a prediction could be reduced by many orders of magnitude while increasing the predictive accuracy <ref type="bibr" target="#b8">(Gilmer et al., 2017;</ref><ref type="bibr" target="#b22">Wu et al., 2018;</ref><ref type="bibr" target="#b6">Fey et al., 2020)</ref>. Graph neural networks have been proven to be particularly effective in this task <ref type="bibr">(Hu et al., 2020;</ref><ref type="bibr" target="#b22">Wu et al., 2018)</ref>, exceeding other approaches, both based on other data representations or on manual feature engineering. Note that the computation time is as important an issue as the performance, as modern pipelines in pharmaceutical research often rely on scanning tens of thousands of candidate molecules in silico, before selecting only a handful for experimental tests.</p><p>Beside these promising perspectives, molecular property prediction still presents many difficulties. A first one is owed to the cost of producing experimental measurements of biochemical properties of compounds. Thus, despite the recent revolution of high throughput screening, public datasets are often rather small. Either they contain only a few thousands or even hundreds of molecules, or they are heavily multi-tasked and scarcely labelled, so that even apparently large datasets can contain only a few handful of positively labelled molecules. Thus, overfitting is a major problem in the context of molecule property prediction.</p><p>A second problem is that the most common framework, namely message passing neural network (MPNN) <ref type="bibr" target="#b24">(Xu et al., 2019)</ref>, is not an universal approximator of functions on graphs. More precisely, it is equivalent to the first order Weisfeiler Leman kernel (1-WL). Although such network could in principle have a strong discriminative power with enough layers, it was shown that practical networks can have trouble solving even basic structure related tasks, such as detecting small cycles <ref type="bibr" target="#b16">(Loukas, 2019;</ref><ref type="bibr" target="#b3">Chen et al., 2020)</ref>. This seems like a serious problem, as biochemistry is mainly based on the analysis of local graph structures, such as aromatic groups. Multiple attempts to solve this apparent lack of expressiveness have been proposed. A first approach is to develop methods to train larger and deeper networks <ref type="bibr" target="#b14">(Li et al., 2020)</ref>, which can already lead to a considerable boost in performance. A few works are fundamentally diverging from the standard convolution based framework, such as invariant graph networks <ref type="bibr" target="#b18">(Maron et al., 2019;</ref><ref type="bibr">2018)</ref> or relational pooling <ref type="bibr" target="#b20">(Murphy et al., 2019;</ref><ref type="bibr" target="#b3">Chen et al., 2020)</ref>. Despite being provably more powerful, the amount of calculation required by these methods is still prohibitively large, and they usually do not outperform more traditional methods. Another approach is to extend the current framework, to make it equivalent to higher order Weisfeiler Leman kernels <ref type="bibr" target="#b19">(Morris et al., 2019)</ref>, or to increase the receptive field of each convolution <ref type="bibr" target="#b7">(Flam-Shepherd et al., 2020;</ref><ref type="bibr" target="#b21">Nikolentzos et al., 2020;</ref><ref type="bibr" target="#b0">Abu-El-Haija et al., 2019)</ref>. However, these approaches make considerable changes to the architecture of the network and require much more parameters than the basic approach. Finally, based on the observation that the issue arises from the presence of cycles in the input graph, it was proposed to perform coupled convolution on both the input graph and a coarser version of it from which all cycles have been removed <ref type="bibr" target="#b6">(Fey et al., 2020)</ref>. Despite interesting results, this approach gives more information to the network and considerably changes the architecture.</p><p>In this work, we propose an alternative method to tackle this issue in the simplest possible fashion. We argue that increasing the effective receptive field of each convolution step should be enough to solve the problem when only small cycles are involved, which is mostly the case in the context of chemistry. However, the global receptive field should not be increased, as the additional information could be ill-used. We propose a variant of the standard GIN convolution to do so with a minimal cost in term of computation time and number of parameters. We show that this approach boosts performance on larger datasets. In particular, at the time this article is written, our model ranks first on the Open Graph Database leaderboard for the PCBA dataset, which is one of the largest benchmark for molecular bio-activity prediction. More generally, our results on several standard datasets show that our method is well adapted to multi-tasked settings, increasing the performance both on average and on isolated tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">GRAPH NEURAL NETWORK</head><p>In graph classification or regression tasks, a network operates on a labelled graph G(V, E) where V = 1..N is the set of nodes and E ⊂ V × V is the set of undirected edges, x v ∈ R dn is the label of the node v ∈ V and e ij ∈ R de is the label of the edge (i, j) ∈ E.</p><p>A very general framework to perform graph representation learning is message passing neural network (MPNN). The network consists in L message passing layers, also called graph convolutions, defined as:</p><formula xml:id="formula_0">m (l) ij = M ESSAGE h (l−1) i , h (l−1) j , e ij a (l) i = AGGREGAT E m (l) ij j∈N1(i) h (l) i = U P DAT E h (l−1) i , a<label>(l) i</label></formula><p>(1) where h</p><p>(l)</p><p>i is the embedding of the node i after the l-th layer. MESSAGE and UPDATE are learned functions. The AGGREGATE function acts on a multiset, i.e. a set with repetitions, defined by the neighbours of a node i, N 1 (i) = {j/(i, j) ∈ E}. This function has to accept multiple sizes of set and has to be node permutation invariant so that the network does not predict a different result when the node indexing is changed. In most cases, sum, mean or max operator is used. We set h (0) i = x i . After L message passing layers, the resulting node embeddings h (L) i are then globally pooled to produce a fixed size vector representation, usually using once again a sum, mean or max operator. The resulting representation lies in R n and is then used as input for downstream tasks.</p><p>Each layer should be able to aggregate the multiset of its neighbours in each node without loosing information. Thus, in order to discriminate different graphs, a requirement is that the operation is injective on multisets, or at least on a large finite ensemble of multisets. <ref type="bibr" target="#b24">(Xu et al., 2019)</ref> have proposed graph isomorphism network(GIN), an efficient implementation of message passing that enables this property, making it one of the most expressive convolution. In order to add edge information in the convolution, GINE a slight variant of GIN has been proposed by Hu* et al. ( <ref type="formula">2020</ref>) :</p><formula xml:id="formula_1">GINE :a (l) i = j∈N1(i) σ h (l−1) j + E(e ij ) h (l) i = M LP (1 + )h (l−1) i + a (l) i (2)</formula><p>where M LP is a standard multi-layer perceptron, is a learned parameter, σ is a nonlinearity such as ReLU and E(e ij ) is an embedding of the label of the edge (i, j).</p><p>In addition to convolutions, <ref type="bibr" target="#b15">Li et al. (2017)</ref> proposed the introduction of a virtual node, initially in order to allow the network to grasp long range dependencies between the nodes. A new node, so-called the virtual node, is added to the graph and connected to every other node. The same convolutions are applied on this node, although with specific weights, so that information about the whole graph can be used at every step. Thus if the main convolution operator is GIN, the virtual node aggregation step is written as:</p><p>Virtual Node :</p><formula xml:id="formula_2">H (l) = M LP ((1 + )H (l−1) + i∈V h (l) j ) h (l) i = h (l) i + H (l) (3)</formula><p>where H is the embedding of the virtual node and h refers to the node embedding before the virtual node step.</p><p>The structure of such network is summed up in figure <ref type="figure">1</ref> where we added nonlinear activations and standard regularisation layers. Hyperparameters and details for the different studied tasks can be found in appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">INCREASING THE KERNEL SIZE FOR SMALL CYCLE DETECTION</head><p>Without virtual node, convolution based networks are at most as discriminative as the Weisfeiler-Lehman test <ref type="bibr" target="#b16">(Loukas, 2019)</ref>. More practically, this means that the information contained in the Standard convolution (layer l=2) GIN+ (layer l=2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input graphs</head><p>Figure <ref type="figure">2</ref>: Illustration of the standard and the GINE+ convolutions on two different example inputs, shown in the top part of the figure. The first line in each case corresponds to the information received by the node with index 0 during the second convolution, with radius 2 in the case of GINE+. The second line corresponds to possible neighbourhood of node 0 given the information it will contain after this second convolution. The distance between the node transmitting the information and node 0 is recalled below. We can observe that the information obtained via standard convolution is not enough to infer a unique neighbourhood. Adding more convolution layers would actually not help in that case (see appendix). Note that node 0 does not have access to information from a distance greater than 2 in this example.</p><p>embedding of a node at layer l can represent at best a breadth first search (BFS) tree of depth l with repetition, rooted at this node. Importantly, the nodes are anonymous, so the repeated nodes between different layers are embedded as different ones. Hence, if cycles are present in the input graph, it is always possible to propose a different graph with the same embedding for a given node, as the example in figure <ref type="figure">2</ref> illustrates. As a result, cycle detection from the embedding of a node is fundamentally impossible. Thus, as there is no local information about the real structure of the graph, it can only be inferred after a global aggregation step where the BFS trees from each node must be assembled together. This step can become very complex or impossible as the graph size augments, partly due to the fact that global pooling is performed with very simple functions, such as sum or mean.</p><p>In order to circumvent this problem, there are three routes. First, one could propose more complex aggregation methods, e.g. adding attention or using more complex set invariant functions. Second, one could perform the global aggregation at multiple levels of granularity to ease the task. This ultimately leads to the virtual node update. Finally, one can design a new type of convolution with a better ability to detect cycles.</p><p>Because we believe that a method relying on global aggregation cannot be scalable, we focus on the third solution. We propose to increase the receptive field of each convolution layer, in the simplest possible fashion. Slightly modifying the original GINE convolution, each node aggregates a wider neighbourhood, encompassing cycles if there are some. This plays a role which is the local equivalent of the global pooling layer. Hence, any cycle smaller than twice the radius of such convolution can be detected. A naïve implementation of this idea is:</p><formula xml:id="formula_3">NaiveGINE+ :a (l,0) i = h (l−1) i a (l,1) i = j∈N1(i) σ(h (l−1) j + E(e ij ) a (l,k) i = j∈N k (i) σ(h (l−1) j ) h (l) i = M LP K k=0 (1 + k )a (l,k) i (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>Where K is the convolution radius and N k (i) is the set of all nodes at distance k of i. can be either a scalar, as in standard GIN, or a vector itself. We found it made almost no difference and used vector epsilon in this work. With K = 1, this convolution is almost equivalent to GINE.</p><p>In principle, this formulation can enable the network to detect structures more efficiently at the local level. However, after L convolutions, the receptive field of a node is K × L so that each node has access to much more information than with standard convolution. This could be detrimental, as this information could be ill-used to build specific discriminative features, leading to overfitting instead of more meaningful ones.</p><p>Aiming at allowing local structural reasoning without adding unnecessary information, we alter the previous formulation as follows :</p><p>GINE+ :a</p><formula xml:id="formula_5">(l,0) i = h (l−1) i a (l,1) i = j∈N1(i) σ(h (l−1) j + E(e ij ) a (l,k) i = j∈N k (i) σ(h (l−k) j ) h (l) i = M LP K k=0 (1 + k )a (l,k) i (5)</formula><p>The only difference resides in the definition of a (l,k) i</p><p>, which now takes its input in the layer l − k instead of l − 1. As a result, at layer l, a node will receive information from another at distance k, which has information about other nodes at distance at most l − k from itself. Thus, the receptive distance at layer l is still only l. Note that this amounts to signaling only redundancies at each depth of the BFS. The behaviour of GINE+ is illustrated in figure <ref type="figure">2</ref>.</p><p>Notice that the new form of this convolution requires only a handful of additional parameters. Indeed, the only additions are the vector k , which contain a small number of parameters compared to the matrices in the convolution. Moreover, most of the calculations have been performed in previous layers, so that only a few supplementary additions must be executed compared to GINE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK GRAPH CONVOLUTIONS</head><p>Neural networks acting directly on graphs were described in <ref type="bibr" target="#b12">Kipf &amp; Welling (2017)</ref> where a Graph Convolutional Network (GCN) performs a layer-wise propagation rule in the form of linear graph convolutions followed by a point-wise nonlinearity. The GCN convolution can be seen as a computationally simple first-order approximation of spectral graph convolution <ref type="bibr" target="#b1">(Bruna et al., 2014;</ref><ref type="bibr" target="#b4">Defferrard et al., 2016)</ref>  <ref type="table">1</ref>: Main characteristics of the datasets used in this study. The first column is the total number of compounds in the dataset. The second is the number of independent classification tasks. The third is the average size of molecules in the dataset. The fourth is the proportion of data labelled as missing. The fifth is the amount of positive classes among all non missing data. The sixth is the total number of positive classes in the dataset and the last is the total number of molecules which are positive in at least one task in the dataset.</p><p>While GCN is able to learn graph representations encoding structure and node features, some drawbacks have been pointed by followup works. In <ref type="bibr" target="#b24">Xu et al. (2019)</ref> it is shown that GCNs are limited in its capability to distinguish a number of simple graph structures, and they proposed the Graph Isomorphism Network (GIN), a variant convolution which is claimed to have discriminative power equivalent to the Weisfeiler-Lehman graph isomorphism test.</p><p>In Hu* et al. ( <ref type="formula">2020</ref>) it is proposed GINE, a slight modification from GIN where edge features are incorporated into the GIN convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUBSTRUCTURE DETECTION</head><p>Even with the increased expressiveness of GIN and GINE graph convolution, its inability to properly detect graph cycles remains a major limitation, in particular when dealing with molecular graphs such as found in organic chemistry. This problem has been investigated by many recent works <ref type="bibr" target="#b3">(Chen et al., 2020;</ref><ref type="bibr" target="#b21">Nikolentzos et al., 2020;</ref><ref type="bibr" target="#b0">Abu-El-Haija et al., 2019;</ref><ref type="bibr" target="#b16">Loukas, 2019;</ref><ref type="bibr" target="#b6">Fey et al., 2020)</ref>. In the context of molecular property prediction, we highlight the work of <ref type="bibr" target="#b6">Fey et al. (2020)</ref> who proposed to perform message passing between the original graph and a coarser graph containing no cycles. The authors rely on fixed rules similar to the junction tree approach <ref type="bibr" target="#b11">(Jin et al., 2018)</ref> to annotate graph cycles and represent them into specific nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OTHER IMPROVEMENTS ON GRAPH NEURAL NETWORKS</head><p>Learning features directly on the molecular graph instead of handcrafting them or using other representation is an idea that emerged a few years ago <ref type="bibr" target="#b5">(Duvenaud et al., 2015;</ref><ref type="bibr" target="#b8">Gilmer et al., 2017;</ref><ref type="bibr" target="#b13">Kong et al., 2020)</ref>. Many additions and modifications of the original formulation have been proposed over the years, which are complementary to our line of work. Although we did not explore this route, our method can in principle be combined with any of those other improvements. For example, regularisation or global pooling functions which may be more suited to small graphs have been proposed <ref type="bibr" target="#b2">(Cai et al., 2020;</ref><ref type="bibr" target="#b13">Kong et al., 2020), and</ref><ref type="bibr">Hu* et al. (2020)</ref> have shown that pre-training strategies relying on a form of data augmentation could be beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">DATASET</head><p>We performed experiments on five molecule classification datasets, from MoleculeNet <ref type="bibr" target="#b22">Wu et al. (2018)</ref> and Open Graph Benchmark (OGB) <ref type="bibr">Hu et al. (2020)</ref> collections, which cover a wide range of tasks:</p><p>• MUV is a dataset designed from PubChem bioactivity data specially designed for virtual screening methods benchmark.</p><p>• Tox21 and ToxCast contain molecules assayed against diverse toxicity-related targets, based on in vitro high-throughput screening.</p><p>• HIV is a single classification task dataset corresponding to the experimentally measured ability of a compound to inhibit the HIV virus replication.</p><p>• PCBA is a subset of PubChem BioAssay consisting of measured bio-activities of compounds.</p><p>The characteristics of all datasets are summed up in table 1. We noted that, despite sometimes deceptively large sizes, molecule datasets are extremely skewed. In particular, most datasets contain only a few thousand molecules of interest, in the sense that they are positively classified in at least one task. These datasets can be overfitted easily: if those few molecules are memorized, the network only has to be able to differentiate distinct molecules. This is a task for which graph neural networks are particularly well fitted. PCBA is the only dataset that does not suffer from this lack of positive samples: Being both large and heavily multi-tasked, it contains nearly two orders of magnitude more molecules of interest.</p><p>Different kind of splitting methods are typically used. Random splitting is simple, but can produce misleading results as there are a lot of small variations of the same sample in most molecule datasets. Scaffold splitting, where samples are separated in based on their general structure, is often used to perform experiments closer to the reality. In this work, we use the scaffold split provided with the Open Graph Benchmark framework for the MUV, HIV and PCBA datasets and random split for the Tox21 and ToxCast datasets.</p><p>For comparability with litterature, we use PRC-AUC as the metric for the two most skewed datasets, MUV and PCBA, while we use ROC-AUC on the others.</p><p>Finally, we augment the smallest datasets with PCBA, making them a supplementary task of the larger PCBA dataset. This is done to prevent overfitting by increasing the number of samples.</p><p>Validation and test are still performed only on the smaller sets. These datasets are used to study the performance on only one task in a multitasked setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">TRAINING DETAILS</head><p>We train a classifier network by minimizing cross-entropy loss with the target properties. Details about the architecture and hyperparameters are given in annex.</p><p>All experiments are run five times before measuring their average performance and standard deviation.</p><p>The code is available at https://github.com/RBrossard/GINEPLUS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">RESULTS</head><p>First, we evaluate the performance of our model against baselines on all datasets. The results are displayed in table 2. Our model ranks first on the large PCBA dataset, by a clear margin, making it the leading model for PCBA in the OGB leaderboard when this paper was written. However, on the four smaller datasets, the model does not outperform state-of-the-art models. In particular, it performs slightly lower than GINE with virtual node, which shares most similarities with our model. We believe this is due to the fact that the smaller datasets are easier to overfit, so that the addition in expressive power in our model was ill-used to memorize the dataset instead of learning useful structural features.</p><p>In order to limit the ability of the model to overfit, we evaluate the performance of our model on small datasets combined with PCBA. As detailed in section 5.1, the training is performed on the training data of both datasets, but evaluation is performed only on the validation set of the non-augmented dataset. Obviously, we observe a performance boost over the training on the nonaugmented datasets. Nonetheless, our model performs better on all augmented datasets compared to GINE.</p><p>Finally, we note that these improvements come at a very small price: in our experiments, the calculation time increased by 3 to 15 % while the increment in number of parameters is only a few fractions of percent. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We proposed GINE+ which builds on the idea of the standard GIN convolution by increasing the radius of the convolution in a non-naive way. Without increasing the perceptive field, which could lead to undesirable effects, such as overfitting, GINE+ produce node embeddings that are more informative about its local neighbourhood. In particular, it can efficiently learn to detect small cycles. GINE+ comes at a small cost in term of computation time and a negligible one in term of number of parameters.</p><p>We apply our method to molecular property prediction. Our model consistently improves performance on large datasets, and seems to produce useful features that transfer efficiently in a multitask setting. In particular, our model ranks first on the PCBA dataset when this article is written.</p><p>Beside this result, we believe that the ability to infer a node neighbourhood from its embedding is essential in numerous graph problems, in particular in the case of graph pooling and graph generation. We leave these development to future works.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>.</figDesc><table><row><cell>Dataset</cell><cell>Size</cell><cell cols="6"># tasks Avg. Size Missing Positive ratio # Positives # Pos. Molecules</cell></row><row><cell>MUV</cell><cell>93087</cell><cell>17</cell><cell>24.2</cell><cell>84 %</cell><cell>0.19 %</cell><cell>489</cell><cell>471</cell></row><row><cell>Tox21</cell><cell>7831</cell><cell>12</cell><cell>18.6</cell><cell>17%</cell><cell>7.52%</cell><cell>5862</cell><cell>2872</cell></row><row><cell>ToxCast</cell><cell>8576</cell><cell>617</cell><cell>18.8</cell><cell>71%</cell><cell>8.25%</cell><cell>126651</cell><cell>6186</cell></row><row><cell>HIV</cell><cell>41127</cell><cell>1</cell><cell>25.5</cell><cell>0%</cell><cell>3.5%</cell><cell>1443</cell><cell>1443</cell></row><row><cell>PCBA</cell><cell>437929</cell><cell>128</cell><cell>26</cell><cell>39%</cell><cell>1.39%</cell><cell>472637</cell><cell>184126</cell></row><row><cell>Table</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Test performance of the different models and datasets. (Mean ± std). Apart from our results, performances for the HIV and PCBA datasets are taken directly from the OGB leaderboardHu et al. (2020). We did not try to measure the performance of DeeperGCN on the MUV, ToxCast and Tox21 datasets, as the code has not been released. The result of HIMP on Tox21 is taken directly from the original paper and we used the released code to obtain results on MUV and ToxCast. All other performances are obtained from our implementation, with identical network structures. GINe+ K=3 w/ VN 0.1194 ± 0.0122 0.8828 ± 0.0043 0.7849 ± 0.0029 0.7919 ± 0.0092</figDesc><table><row><cell></cell><cell>MUV</cell><cell>Tox21</cell><cell></cell><cell>ToxCast</cell><cell></cell><cell>HIV</cell><cell>PCBA</cell></row><row><cell>GCN</cell><cell>0.115 ± 0.022</cell><cell>0.840 ± 0.004</cell><cell cols="2">0.735 ± 0.002</cell><cell cols="2">0.761 ± 0.010</cell><cell>0.202 ± 0.002</cell></row><row><cell>GCN w/ VN</cell><cell>0.105 ± 0.019</cell><cell>0.859 ± 0.005</cell><cell cols="2">0.743 ± 0.003</cell><cell cols="2">0.760 ± 0.012</cell><cell>0.242 ± 0.003</cell></row><row><cell>GINE</cell><cell>0.091 ± 0.033</cell><cell>0.850 ± 0.009</cell><cell cols="2">0.741 ± 0.004</cell><cell cols="2">0.756 ± 0.014</cell><cell>0.227 ± 0.003</cell></row><row><cell>GINE w/ VN</cell><cell>0.107 ± 0.044</cell><cell>0.872 ± 0.003</cell><cell cols="4">0.749 ± 0.002 0.771 ± 0.015</cell><cell>0.270 ± 0.002</cell></row><row><cell>DeeperGCN</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell cols="2">0.786 ± 0.012</cell><cell>0.278 ± 0.004</cell></row><row><cell>HIMP</cell><cell>0.114 ± 0.041</cell><cell>0.874 ± 0.005</cell><cell cols="2">0.721 ± 0.004</cell><cell cols="2">0.788 ± 0.008 0.274 ± 0.002</cell></row><row><cell cols="2">NaiveGINe+ K=3 w/ VN 0.069 ± 0.0146</cell><cell>0.870 ± 0.004</cell><cell cols="2">0.737 ± 0.007</cell><cell cols="2">0.758 ± 0.013</cell><cell>0.279 ± 0.002</cell></row><row><cell>GINe+ K=1 w/ VN</cell><cell>0.062 ± 0.025</cell><cell>0.864 ± 0.006</cell><cell cols="2">0.742 ± 0.002</cell><cell cols="2">0.757 ± 0.019</cell><cell>0.274 ± 0.002</cell></row><row><cell>GINE+ K=2 w/ VN</cell><cell>0.067 ± 0.020</cell><cell>0.867 ± 0.003</cell><cell cols="2">0.741 ± 0.006</cell><cell cols="2">0.761 ± 0.010</cell><cell>0.286 ± 0.003</cell></row><row><cell>GINE+ K=3 w/ VN</cell><cell>0.106 ± 0.026</cell><cell>0.867 ± 0.004</cell><cell cols="4">0.749 ± 0.008 0.766 ± 0.014</cell><cell>0.292 ± 0.002</cell></row><row><cell></cell><cell>MUV+PCBA</cell><cell cols="2">Tox21+PCBA</cell><cell cols="2">ToxCast+PCBA</cell><cell>HIV+PCBA</cell></row><row><cell>GINe w/ VN</cell><cell>0.1174 ± 0.0196</cell><cell cols="2">0.8810 ± 0.0.0045</cell><cell cols="2">0.7757 ± 0.0028</cell><cell>0.0.7875 ± 0.0099</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Test performance of GINE+ compared to GINE on datasets augmented by PCBA. (Mean ± std)</figDesc><table /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A ARCHITECTURE AND HYPERPARAMETERS</head><p>All networks trained have the same structure:</p><p>• A node embedding layer which produce an embedding vector x (0) i of dimension H. We use the AtomEmbedding function from the Open Graph Benchmark package.</p><p>• L layers of convolution with batch normalization, ReLU activation and dropout layer (p=0.5) with constant output dimension H, eventually with radius K.</p><p>• Optionally, a virtual node with same dimension H.</p><p>• A global mean pooling layer</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• A linear classifier</head><p>Each layer has its own trainable weights.</p><p>GIN type convolutions (GINE, NAIVEGINE+, GINE+, with or without virtual node) require a MLP. We use a basic implementation with both input and output dimensions set to H and a hidden layer of dimension 2H followed by batch normalization, ReLU activation and dropout.</p><p>For the PCBA dataset, we used L = 5 and H = 400. For all other datasets, we used L = 3 and H = 100.</p><p>The code is implemented using PyTorch and Pytorch-Lightning.</p><p>All datasets are taken from the Open Graph Benchmark package, and eventually shuffled when a random split is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B EXISTENCE OF GRAPHS THAT CANNOT BE DIFFERENTIATED BY STANDARD CONVOLUTIONS</head><p>From a graph G(V, E) of size N containing at least one cycle, we show that there exists another graph containing at least N nodes with the exact same embeddings after L layers of convolution than the nodes of the original graph, for any L.</p><p>Consider the graph G copy (V , E ), isomorphic to G with V ∩ V = ∅ i.e. a copy of the first graph without redundant indexing. We denote φ the isomorphism so that V = f (V ). Given a specific edge (i, j) ∈ E and (i , j ) = (φ(i), φ(j)) ∈ E the corresponding edge in G copy . We build a new edge set E = (E ∪ E) ∪ {(i, j ), (i , j)} / {(i, j), (i , j )}. That is we replace the edge (i, j) by (i, j ) and reciprocally. The final graph is G(V ∩ V , E). If edge labels are present, the label of the two added edges are the same than the two removed edges.</p><p>We note x</p><p>k the node embedding of node k in graph G and G respectively after l layers of convolution. We show recursively that x</p><p>First, for any k ∈ V , since initial node embedding does not depend of the neighbourhood, x</p><p>Then, for a given l &gt; 0 and k ∈ V , if x</p><p>φ(u) for any u ∈ V , then we consider three cases: First, if k = i or j, the set of the embeddings of its neighbours is identical in G and G. Second, if k = i, the set of the embeddings of its neighbours in G is the same as in G except without x (l−1) j and with the addition of x (l−1) j , which are equal by construction so that the two set are still identical. An identical argument goes for the last case k = j. Due to the definition of message passing in equation 1, this implies that x </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazanin</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrayr</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Ver Steeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00067</idno>
		<title level="m">Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno>ICLR 2014</idno>
		<ptr target="http://arxiv.org/abs/1312.6203" />
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations</title>
				<editor>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</editor>
		<meeting><address><addrLine>Banff, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">April 14-16, 2014. 2014</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Graphnorm: A principled approach to accelerating graph neural network training</title>
		<author>
			<persName><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.03294</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Can graph neural networks count substructures?</title>
		<author>
			<persName><forename type="first">Zhengdao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soledad</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.04025</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1606.09375" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alán</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hierarchical inter-message passing for learning on molecular graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Weichert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Graph Representation Learning and Beyond (GRL+) Workhop</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Flam-Shepherd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Friederich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10413</idno>
		<title level="m">Neural message passing on high order paths</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01212</idno>
		<title level="m">Neural message passing for quantum chemistry</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Strategies for pre-training graph neural networks</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HJlWWJSFDH" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Junction tree variational autoencoder for molecular graph generation</title>
		<author>
			<persName><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v80/jin18a.html" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="2323" to="2332" />
			<date type="published" when="2018-07-15">10-15 Jul 2018</date>
			<publisher>PMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SJU4ayYgl" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
				<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. April 24-26, 2017. 2017</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Flag: Adversarial data augmentation for graph neural networks</title>
		<author>
			<persName><forename type="first">Kezhi</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mucong</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gavin</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09891</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deepergcn: All you need to train deeper gcns</title>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07739</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning graph-level representation for drug discovery</title>
		<author>
			<persName><forename type="first">Junying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.03741</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.03199</idno>
		<title level="m">What graph neural networks cannot learn: depth vs width</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Invariant and equivariant graph networks</title>
		<author>
			<persName><forename type="first">Heli</forename><surname>Haggai Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaron</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><surname>Lipman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.09902</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Provably powerful graph networks</title>
		<author>
			<persName><forename type="first">Heli</forename><surname>Haggai Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hadar</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaron</forename><surname>Serviansky</surname></persName>
		</author>
		<author>
			<persName><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2156" to="2167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Eric Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Balasubramaniam</forename><surname>Ryan L Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinayak</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><surname>Ribeiro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02541</idno>
		<title level="m">Relational pooling for graph representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">George Dasoulas, and Michalis Vazirgiannis. k-hop graph neural networks</title>
		<author>
			<persName><forename type="first">Giannis</forename><surname>Nikolentzos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="195" to="205" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Moleculenet: a benchmark for molecular machine learning</title>
		<author>
			<persName><forename type="first">Zhenqin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caleb</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aneesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="513" to="530" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ryGs6iA5Km" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08434</idno>
		<title level="m">Graph neural networks: A review of methods and applications</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
