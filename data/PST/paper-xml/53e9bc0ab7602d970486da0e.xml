<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantic Concept-Based Query Expansion and Re-ranking for Multimedia Retrieval * A Comparative Review and New Approaches Apostol (Paul) Natsev</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ibm</forename><surname>Thomas</surname></persName>
						</author>
						<author>
							<persName><forename type="first">J</forename><surname>Watson</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Research</forename><surname>Center</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alexander</forename><surname>Haubold</surname></persName>
							<email>ahaubold@cs.columbia.edu</email>
						</author>
						<author>
							<persName><forename type="first">Jelena</forename><surname>Tešić</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lexing</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rong</forename><surname>Yan</surname></persName>
							<email>yanr@us.ibm.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">IBM Thomas J. Watson Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">IBM Thomas J. Watson Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">IBM Thomas J. Watson Research Center</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Semantic Concept-Based Query Expansion and Re-ranking for Multimedia Retrieval * A Comparative Review and New Approaches Apostol (Paul) Natsev</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B12C4711B21490447C8FE2406157F88D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: Content Analysis and Indexing</term>
					<term>H.3 [Information Storage and Retrieval]: Information Search and Retrieval</term>
					<term>I.2 [Artificial Intelligence]: Learning</term>
					<term>Algorithms, Experimentation, Performance</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study the problem of semantic concept-based query expansion and re-ranking for multimedia retrieval. In particular, we explore the utility of a fixed lexicon of visual semantic concepts for automatic multimedia retrieval and re-ranking purposes. In this paper, we propose several new approaches for query expansion, in which textual keywords, visual examples, or initial retrieval results are analyzed to identify the most relevant visual concepts for the given query. These concepts are then used to generate additional query results and/or to re-rank an existing set of results. We develop both lexical and statistical approaches for text query expansion, as well as content-based approaches for visual query expansion. In addition, we study several other recently proposed methods for concept-based query expansion. In total, we compare 7 different approaches for expanding queries with visual semantic concepts. They are evaluated using a large video corpus and 39 concept detectors from the TRECVID-2006 video retrieval benchmark. We observe consistent improvement over the baselines for all 7 approaches, leading to an overall performance gain of 77% relative to a text retrieval baseline, and a 31% improvement relative to a stateof-the-art multimodal retrieval baseline.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Result-Based Expansion (map results to concepts)</head><p>Relevant concepts with weights Baseline system retrieval results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multimedia Repository</head><note type="other">Concept</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concept-based retrieval results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>"Airplane taking off"</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-modal Fusion and Re-ranking</head><p>Concept-based expanded and re-ranked results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multimedia Repository</head><p>Figure <ref type="figure">1</ref>: Overview of concept-based retrieval and re-ranking framework. Three general approaches are illustrated for identifying relevant semantic concepts to a query-based on textual query analysis, visual content-based query modeling, and pseudorelevance feedback. A multi-modal fusion step leverages the relevant concepts to improve the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Search and retrieval are vital parts of multimedia content management, and are increasingly receiving attention with the growing use of multimedia libraries and the explosion of digital media on the Web. By its virtue, multimedia spans multiple modalities, including audio, video, and text. While search and retrieval in the text domain are fairly well-understood problems and have a wide range of effective solutions, other modalities have not been explored to the same degree. Most large-scale multimedia search systems typically rely on text-based search over media metadata such as surrounding html text, anchor text, titles and abstracts. This approach, however, fails when there is no such metadata (e.g., home photos and videos), when the rich link structure of the Web cannot be exploited (e.g., enterprise content and archives), or when the metadata cannot precisely capture the true multimedia content.</p><p>On the other extreme, there has been a substantial body of work in the research community on content-based retrieval methods by leveraging the query-by-example and relevance feedback paradigms. These methods do not require any additional metadata, but rely on users to express their queries in terms of query examples with low-level features such as colors, textures and shapes. Finding appropriate examples, however, is not easy, and it is still quite challenging to capture the user intent with just a few examples.</p><p>To address these issues, a new promising direction has emerged in recent years, namely, using machine learning techniques to explicitly model the audio, video, and image semantics. The basic idea is that statistical detectors can be learned to recognize semantic entities of interestsuch as people, scenes, events, and objects-by using more training examples than a user would typically provide in an interactive session (in most cases using hundreds or thousands of training examples). Once pre-trained, these detectors could then be used to tag and index multimedia content semantically in a fully automated fashion. Work in this field related to video concept detection and retrieval has been driven primarily by the TREC Video Retrieval Evaluation (TRECVID) community <ref type="bibr">[37]</ref>, which provides a common testbed for evaluating approaches by standardizing datasets, benchmarked concepts and queries <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b1">1,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr">7]</ref>. A major open problem is how to scale this approach to thousands of reliable concept detectors, each of which may require thousands of training examples in turn. Notable efforts in creating large training corpora include the collaborative annotation efforts undertaken by TRECVID participants, donated annotations by the MediaMill team from the University of Amsterdam <ref type="bibr" target="#b34">[34]</ref>, as well as the Large Scale Concept Ontology for Modeling (LSCOM) effort to define and annotate on the order of 1000 concepts from the broadcast news domain <ref type="bibr" target="#b21">[21]</ref>. At present, however, reliable concept detectors are still limited to tens of concepts only, while usable concept detectors exist for a few hundred concepts at best. It is therefore imperative to develop techniques that maximally leverage the limited number of available concept detectors in order to enable or improve video search when the metadata is limited or completely absent.</p><p>In this paper, we study the problem of semantic conceptbased query expansion and re-ranking for multimedia retrieval purposes. In particular, we consider a fixed lexicon of semantic concepts with corresponding visual detectors, and we explore their utility for automatic multimedia retrieval and re-ranking purposes. We propose several new approaches for query expansion, in which the query textual terms, query visual examples, or the query baseline results are analyzed to identify relevant visual concepts, along with corresponding weights. The most salient concepts are then used to generate additional query results (improve recall) or to re-rank an existing set of results (improve precision).</p><p>Specific contributions. We propose a novel lexical query expansion approach leveraging a manually constructed rulebased mapping between a lexicon of semantic text annotations and a lexicon of semantic visual concepts. The proposed approach uses deep parsing and semantic tagging of queries based on question answering technology. It outperforms two popular lexical approaches based on synonym expansion and WordNet similarity measures. We also propose a novel statistical corpus analysis approach, which identifies significant correlations between words in the English language and concepts from the visual concept vocabulary based on their co-occurrence frequency in the video corpus. This approach performs on par with, or better than, lexical query expansion approaches but has the advantage of not requiring any dictionaries, or manually constructed concept descriptions, and is therefore more scalable. We also study smart data sampling and content modeling techniques for concept-based expansion based on visual query examples. Finally, we study and evaluate several other previously proposed methods for concept-based query expansion and retrieval. In total, we compare and empirically evaluate 7 different approaches for expanding ad-hoc user queries with relevant visual concepts. We observe consistent improvement over the baselines for all 7 approaches, leading to an overall performance gain of 77% relative to a text retrieval baseline, and a 31% improvement relative to a multimodal retrieval baseline. To the best of our knowledge, this is one of the most comprehensive reviews and evaluations of concept-based retrieval and re-ranking methods so far, and it clearly establishes the value of semantic concept detectors for answering and expanding ad-hoc user queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Text-Based Query Expansion</head><p>Since concept-based query expansion is related to research in text-based query expansion, we give an overview of the main approaches from that domain. In principle, the idea is to expand the original query with additional query terms that are related to the query. The addition of related terms can improve recall-especially for short queries-by discovering related documents through matches to the added terms. It may also refine the meaning of overly broad queries, thereby re-ranking results and improving precision. This of course works only as long as the refined query is indeed consistent with the original one. Experiments in text document retrieval have shown that query expansion is highly topicdependent, however.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Lexical approaches (language-specific)</head><p>Lexical approaches leverage global language properties, such as synonyms and other linguistic word relationships (e.g., hypernyms). These approaches are typically based on dictionaries or other similar knowledge representation sources such as WordNet <ref type="bibr" target="#b37">[38]</ref>. Lexical query expansion approaches can be effective in improving recall but word sense ambiguity can frequently lead to topic drift, where the semantics of the query changes as additional terms are added.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Statistical approaches (corpus-specific)</head><p>Statistical approaches are data-driven and attempt to discover significant relationships based on term co-occurrence analysis and feature selection. These relationships are more general and may not have linguistic interpretation. Early corpus analysis methods grouped words together based on their co-occurrence patterns within documents <ref type="bibr" target="#b28">[28]</ref>. Related methods include term clustering <ref type="bibr" target="#b17">[17]</ref> and Latent Semantic Indexing <ref type="bibr" target="#b10">[10]</ref>, which group related terms into clusters or hidden orthogonal dimensions based on term-document cooccurrence. Later approaches attempt to reduce topic drift by looking for frequently co-occurring patterns only within the same context, as opposed to the entire document, where the context can be the same paragraph, sentence, or simply a neighborhood of n words <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b5">5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Statistical approaches (query-specific)</head><p>In contrast to global statistical approaches, which consider the distribution and co-occurrence of words within an entire corpus, local analysis uses only a subset of the documents to identify significant co-occurrence patterns. This subset is typically a set of documents explicitly provided or tagged by the user as being relevant to the query. In relevance feedback systems, for example, the system modifies the query based on users' relevance judgments of the retrieved documents <ref type="bibr" target="#b31">[31]</ref>. To eliminate or reduce the need for user feedback, some systems simply assume that the top N retrieved documents are relevant, where N is determined empirically and is typically between 20 and 100. This is motivated by the assumption that the top results are more relevant than a random subset and any significant co-occurrence patterns found within this set are more likely to be relevant to the query. This approach is called pseudo-relevance feedback, or blind feedback <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Visual Concept-Based Query Expansion</head><p>Existing approaches for visual concept-based retrieval can similarly be categorized into the three categories for textbased approaches-lexical, global statistical, and local statistical approaches. There are a few differences, however, in that the documents in this case are multimodal (e.g., video clips) as opposed to purely textual, and the correlations of interest involve visual features or concepts, as opposed to just words. Also, multimodal queries have additional aspects than text queries, and can include content-based query examples. Existing approaches can therefore also be broadly categorized depending on the required input-query text, visual query examples, or baseline retrieval results. Table <ref type="table">1</ref> summarizes related work along both dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Lexical approaches (language-specific)</head><p>Lexical approaches for visual concept-based query expansion are based on textual descriptions of the visual concepts, which essentially reduce the problem to that of lexical textbased query expansion. Typically, each concept is represented with a brief description or a set of representative terms (e.g., synonyms). Given a textual query, the query words are then compared against the concept descriptions, and any matched concepts are used for refinement, where the matching may be exact or approximate <ref type="bibr">[7,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b8">8]</ref>. In the latter case, lexical similarity is computed between the query and the concepts using WordNet-based similarity measures, such as Resnik <ref type="bibr" target="#b30">[30]</ref> or Lesk <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b25">25]</ref>. Alternatively, Snoek et al. <ref type="bibr" target="#b35">[35]</ref> also consider the vector-space model for similarity between queries and concept descriptions. In <ref type="bibr" target="#b35">[35]</ref>, word sense disambiguation is performed by taking the most common meaning of a word, as the authors found this to be the best approach from a number of disambiguation strategies they considered. In <ref type="bibr" target="#b13">[13]</ref>, the authors disambiguate term pairs by taking the term senses that maximize their pairwise Lesk similarity. In <ref type="bibr" target="#b8">[8]</ref>, the term similarities are modeled as a function of time to reflect the time-sensitive nature of broadcast news and to filter out "stale" correlations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Statistical approaches (corpus-specific)</head><p>To the best of our knowledge, there is no previous work on global corpus analysis approaches for visual conceptbased query expansion. We propose one such method in Section 4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Statistical approaches (query-specific)</head><p>Local statistical approaches for visual concept-based query expansion have not been explored as much as lexical approaches. Several works <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b36">36]</ref> consider contentbased retrieval where the query is specified with one or more visual examples represented by semantic feature vectors. They project each example onto a semantic vector space where the dimensions represent concept detection confidences for images/shots. Once queries are mapped to the semantic space, traditional content-based retrieval techniques are used. We previously proposed the probabilistic local context analysis method described in Section 4.6 <ref type="bibr" target="#b40">[41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CONCEPT-BASED RETRIEVAL AND RE-RANKING FRAMEWORK</head><p>The proposed concept-based retrieval and re-ranking framework is illustrated in Figure <ref type="figure">1</ref>. We consider approaches for concept-based expansion of text queries, visual contentbased queries, as well as initial retrieval results produced by any baseline retrieval system, be it text-based, contentbased, or multimodal. The identified relevant concepts are then used to retrieve matching shots based on statistical detectors for the 39 LSCOM-lite concepts used in TRECVID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Semantic concept lexicon</head><p>For the multimedia research community, TRECVID provides a benchmark for comparison of different statistical learning techniques. It also sparked off a healthy debate on identifying a lexicon and a taxonomy that would be effective in covering a large number of queries. One such exercise to address the issue of a shallow taxonomy of generic concepts that can effectively address a large number of queries resulted in the creation of the LSCOM-lite lexicon (Figure <ref type="figure" target="#fig_1">2</ref>) used in the TRECVID Video Retrieval evaluation <ref type="bibr">[37]</ref>. Recently, the full LSCOM effort has produced a concept lexicon of over 1000 visual concepts for the broadcast news domain, along with annotations for many of them over a large video corpus <ref type="bibr" target="#b21">[21]</ref>. This annotated corpus contains nearly 700K annotations, over a vocabulary of 449 concepts and a video set of 62K shots. Independently, the MediaMill team from the University of Amsterdam has donated a lexicon and annotations for 101 semantic concepts <ref type="bibr" target="#b34">[34]</ref> defined on the same corpus. These large, standardized, and annotated corpora are extremely useful for training large sets of visual concept detectors and allow better comparability across systems.For the experiments in this paper, we use the 39 LSCOM-lite concepts from Figure <ref type="figure" target="#fig_1">2</ref>  As a first step we built support vector machine models for all 39 concepts of the LSCOM-lite lexicon based on low-level visual features from the training collection <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b4">4]</ref>. These models are used to generate quantitative scores indicating the presence of the corresponding concept in any test set video shot. Quantitative scores are converted into confidence scores, which are used in our re-ranking experiments. The resulting concept detectors achieve some of the best scores in the TRECVID High Level Feature Detection task <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b4">4]</ref>.</p><p>Since each concept encompasses broad meaning and can be described by multiple words and phrases, we manually describe each concept with a set of synonyms and other words and phrases that represent its meaning. We also manually map these representative concept terms to WordNet synsets to remove any ambiguity in their meaning. This is a commonly used approach in the literature <ref type="bibr">[7,</ref><ref type="bibr" target="#b1">1,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b35">35]</ref>. Each concept in our lexicon therefore has a brief text description, a set of WordNet synsets, and a corresponding detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Query analysis</head><p>For textual queries, we perform common query analysis and normalization, including stop-word removal and Porter stemming. We also identify phrases using a dictionary lookup method based on WordNet, as well as statistical part-ofspeech tagging and sense disambiguation based on deep parsing and context modeling. We further filter queries to keep only nouns, verbs, and phrases. In addition, named entities are extracted and query terms are annotated using a semantic text annotation engine, which detects over 100 named and nominal semantic categories, such as named/nominal people, places, organizations, events, etc. <ref type="bibr" target="#b3">[3]</ref>.</p><p>For visual content-based queries, we extract low-level visual descriptors from all query examples, and evaluate each concept model with respect to these descriptors. This results in a 39-dimensional model vector of concept detection scores for each visual example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Concept-based query expansion</head><p>After query analysis, we apply one of 7 concept-based query expansion methods to identify a set of relevant concepts for the query, along with their weights. These methods are described in more detail in Section 4. Some of these methods have been previously proposed (Sections 4.1, 4.4, and 4.6), while others are new (Sections 4.2, 4.3, and 4.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Concept-based retrieval</head><p>In this step, we use the identified concepts, along with their relevance weights, to retrieve relevant results from the video corpus. In particular, each concept detector is used to rank the video shots in order of detection confidence with respect to the given concept. Given the set of related concepts for a query, the corresponding concept detection ranked lists are combined into a single concept-based retrieval result list. We use simple weighted averaging of the confidence scores, where the weights are proportional to the query-concept relevance score returned for each concept.</p><p>Concept confidence normalization. Approaches for confidence score normalization based on voting schemes, range, or rank normalization are frequently used in metasearch or multi-source combination scenarios <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b19">19]</ref>. Other approaches designed specifically for normalizing concept detection scores use score smoothing based on concept frequency or detector reliability <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b15">15]</ref>. Finally, approaches for calibrating raw SVM scores into probabilities are also relevant but typically require additional training data <ref type="bibr" target="#b27">[27]</ref>.</p><p>For our experiments we use statistical normalization of the confidence scores to zero mean and unit standard deviation:</p><formula xml:id="formula_0">S c (x) = Sc(x) -µc σ c , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where Sc(x) is the confidence score for concept c on shot x, and µ c and σ c are the collection mean and standard deviation of the confidence scores for concept c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Concept-based result re-ranking</head><p>In this step, the concept-based retrieval results are used to expand/re-rank the results of the baseline retrieval system. We differentiate between two fusion scenarios-expanding/reranking the results of a baseline retrieval system vs. fusing the results of multiple retrieval systems from multiple modalities for the final result ranking. To reduce complexity and alleviate the need for a large number of training queries needed to learn fusion parameters, we use simple non-weighted averaging in the case of baseline result expansion and re-ranking. This approach typically generalizes well and has proven to be robust enough for re-ranking purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Multi-modal/multi-expert fusion</head><p>Finally, we fuse the results across all available modalities and retrieval experts using query-dependent fusion techniques. This is motivated by the fact that each retrieval expert typically has different strengths/weaknesses. For example, text-based retrieval systems are good at finding named entities, while content-based retrieval systems are good for visually homogeneous query classes, such as sports, weather. We consider linear combination for multiple rank-lists, and use fusion weights that depend on the properties of the query topic. Such query-dependent fusion approaches have been found to consistently outperform query-independent fusion <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b18">18]</ref>, and we use a novel variant of query mapping that generates query classes dynamically <ref type="bibr" target="#b38">[39]</ref>.</p><p>We use a semantic text annotation engine <ref type="bibr" target="#b3">[3]</ref> to tag the query text with more than one hundred semantic tags in a broad ontology, designed for question-answering applications on intelligence and news domains. The set of over one hundred tags covers general categories, such as person, geographic entities, objects, actions, events. For instance, "Hu Jintao, president of the People's Republic of China" is tagged with "Named-Person, President, Geo-Political Entity, Nation". We manually design a mapping from all semantic tags to seven binary feature dimensions, intuitively described as Sports, Named-Person, Unnamed-Person, Vehicle, Event, Scene, Others. This mapping consists of rules based either on commonsense ontological relationships, e.g., "President" leads to Named-Person, or on frequent concept co-occurrence, such as "Road" implies Vehicle.</p><p>The query text analysis and feature mapping is performed on both the set of known (training) queries as well as on each of the new queries. We map a new query to a small set of neighbors among the training queries (with inner product of query features serving as the similarity measure), and then dynamically generate optimal query weights for those training queries on-the-fly. This dynamic query fusion scheme performs better than hard or soft query-class-dependent fusion in our case, since the query feature space is relatively clean and rather low-dimensional <ref type="bibr" target="#b38">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">QUERY EXPANSION APPROACHES</head><p>In the following, we describe several concept-based query expansion approaches, which are evaluated in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Lexical Concept Matching</head><p>The first two approaches we consider are the naive wordspotting approach, or matching query terms to concept descriptions directly, as well as a lexical query expansion approach based on a WordNet similarity measure between visual concept descriptions and text queries. These are by far the most commonly used approaches for visual conceptbased query expansion <ref type="bibr" target="#b1">[1,</ref><ref type="bibr">7,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b8">8]</ref>. With the naive approach, using manually established concept descriptions consisting of representative concept "trigger" words, we find direct matches between query terms and visual concepts (e.g. term "flight" resolves to concept Airplane, "soccer" resolves to Sports, etc.). While only applicable to a limited number of queries, this approach produces a highly accurate mapping between visual and textual modalities.</p><p>In a separate and a more generally applicable query expansion approach, the related concepts to a query are identified not only by exact matches between query terms and concept descriptions, but also by soft matching and similarity scoring based on WordNet relatedness between visual concept descriptions and query terms. In particular, we follow the approach from <ref type="bibr" target="#b13">[13]</ref>, where visual concepts are weighted based on their similarity to query terms through an adapted Lesk semantic relatedness score <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b25">25]</ref>. The Lesk score is computed for all pairs of query terms and concept representative terms. For a given term-concept pair, we select the highest Lesk similarity score based on the intuition that the most similar senses are most likely to be the ones used in the same context. Similarity score vectors of 39 concepts for each query term are aggregated and normalized by the number of query terms, resulting in the query's lexical similarity to the 39 visual concepts. In a departure from <ref type="bibr" target="#b13">[13]</ref>, we have a final de-noising step, where we threshold all similarity scores (i.e., concept weights), and keep only the significant weights that are larger than the mean plus 1 standard deviation, as calculated over the 39-dimensional concept weight vector. We find that this de-noising step generalizes better than the global top-k weight thresholding mechanism used in <ref type="bibr" target="#b13">[13]</ref>, as it results in a variable number of related concepts per query, which is more realistic for general queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Lexical Rule-based Ontology Mapping</head><p>Complimentary to the lexical word-based concept matching described in the previous section, we also utilize deep semantic analysis of the query text and derive a mapping from the semantic tags on the words/phrases to visual concepts. Deeper text analysis is beneficial, since words can have multiple senses, and direct word-spotting may not distinguish them. For instance, in the query people in uniform and in formation, the words "uniform" and "formation" have very distinct meanings, which are not apparent without considering the part-of-speech information and the context. In this case, a specially designed mapping from text semantics to visual concepts is useful, since (1) some semantic text categories have a closely-related visual category (e.g., Named-Person → Person, Face), while others may not have direct visual implications (e.g., Nation); (2) even when there is a strong text-to-visual connection, the mapping is rarely oneto-one, due to the currently limited ontology for both text and visual categories. For example, text annotations Vehicle, Road shall map to Car, Bus, while Vehicle, Waterbody shall map to Boat Ship in the LSCOM ontology.</p><p>Similar to Section 3.6, we use a semantic text annotation engine <ref type="bibr" target="#b3">[3]</ref> to tag the query text with more than one hundred semantic tags in a broad ontology. A set of a few dozen rules are manually designed to map the semantic tags to one or more of the LSCOM-lite concepts, each of which belongs to one of the following three types (1) one-to-one or oneto-many mappings, e.g., a text tag "Sport" maps to visual concepts Sports and Walking Running; (2) many-to-one or many-to-many mappings, e.g., "Vehicle" and "Flight" imply Sky, Airplane; and (3) negated relationships, such as "Furniture" implies NOT Outdoors. We note that even though this mapping is manual, it is not query dependent since it maps one fixed ontology (of text annotations) to another fixed ontology (of visual annotations). When both ontologies are in the order of hundreds of concepts, this is feasible to do manually, and provides a higher quality mapping than automatic mapping approaches based on WordNet (as seen in Section 5.1). However, when either ontology grows to thousands of concepts, this approach becomes less feasible. In such cases, we expect that statistical automatic mapping approaches, such as the one described in Section 4.3, will be the most feasible ones. In fact, as shown in Section 5.4, these approaches can outperform the lexical mapping approaches, even though the latter use a manually constructed ontology mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Statistical Corpus Analysis</head><p>Global corpus analysis approaches for query expansion typically perform correlation analysis between pairs of terms based on their co-occurrence counts in the corpus. Term cooccurrence can be measured within the same document, the same paragraph/sentence, or within a small window of a few neighboring words only. Given the term co-occurrence counts, a statistical test is usually performed to measure which correlations are significant. Identified term pairs with significant correlations are then linked together so that when either term appears in the query, the other can be used for query expansion purposes.</p><p>We adopt the same methodology for visual concept-based query expansion, except that we identify significant correlations between words in the English language vocabulary and visual concepts in the semantic concept lexicon (LSCOM-lite). To measure co-occurrence counts, we implicitly associate the visual concepts detected for a given video shot with all of the words from the video speech transcript that occur within a fixed temporal neighborhood around the given shot. To this end, we applied a likelihood ratio statistical significance test called the G 2 test <ref type="bibr" target="#b11">[11]</ref>. Dunning <ref type="bibr" target="#b11">[11]</ref> showed that this test is more accurate than Pearson's χ 2 test, especially for sparse contingency tables, and introduced the test to the computational linguistics community where it is now widely used. It can be shown that G 2 (X, Y ) can also be expressed in terms of mutual information I(X; Y ) as follows:</p><formula xml:id="formula_2">G 2 (X, Y ) = 2N •I(X; Y ) = 2N (H(X) + H(Y ) -H(X, Y )) ,</formula><p>where H(•) is the entropy function. Mutual information is considered highly effective for feature selection purposes <ref type="bibr" target="#b42">[43]</ref>, and the above formula shows that the G 2 statistic produces a proportional value, with the advantage that it can be evaluated for statistical significance using a χ 2 distribution table with 1 degree of freedom.</p><p>For the experiments in this paper, we use the G 2 test to identify strong correlations between terms from the speech transcript and visual concepts. All G 2 scores are thresholded using a certain significance level (e.g., 95%, 99%, 99.9%, or 99.99% confidence interval), and the remaining scores are normalized into Cramer's φ correlation coefficients:</p><formula xml:id="formula_3">φ(X, Y ) = p G 2 /N .<label>(2)</label></formula><p>Using the above correlations, we build an associative weighted map between speech terms and visual concepts. Given an arbitrary query, all concepts that are strongly correlated to any of the query terms are then used for query expansion purposes using weights proportional to the corresponding φ correlation coefficients. In Section 5.2 we evaluate this approach and show that it performs on par with, or even better than, the popular lexical approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Statistical Content-Based Modeling</head><p>This approach formulates the topic answering problem as a discriminant modeling one in a concept model vector space. The concept model vector space is constructed from concept detection confidences for each shot <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b22">22]</ref>. If the query is specified with one or more visual examples, we map each visual example onto the concept model vector space, and approach the problem as content-based retrieval. Unlike low-level descriptor spaces, the concept model vector space is highly non-linear due to the use of different modeling approaches and parameters, and is also non-orthogonal, due to correlations among concepts (e.g. Sky and Mountain). Standard content-based retrieval approaches based on simple nearest neighbor modeling do not work very well in this space. Instead, we use Support Vector Machine (SVM) modeling with nonlinear kernels in order to learn nonlinear decision boundaries in this highly skewed space. Due to the high dimension of the space and the limited number of distinct positive examples, we adopt a pseudo-bagging approach. In particular, we build multiple primitive SVM classifiers whereby the positive examples are used commonly across all classifiers but each has a different sampled set of pseudonegative data points. The SVM scores corresponding to all primitive SVM models are then fused using Boolean AND logic to obtain a final model. Figure <ref type="figure" target="#fig_2">3</ref> illustrates the main idea. For more details, see <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b36">36]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Statistical Content-Based Model Selection</head><p>Content-Based Model selection uses a set of statistical hypothesis tests to determine if a specific concept detector is relevant to the query based on the visual query examples. This is similar to the statistical corpus analysis-based approach from Section 4.3 but in this case we are looking for significant correlations between the set of visual query examples for a given topic and the concepts, as opposed to correlations between query terms and concepts. We use standard statistical hypothesis tests, such as t-score, χ 2 test, majority vote, one sided student test, and likelihood ratio statistical test to evaluate the importance and uniqueness of each concept with respect to the query examples. For one sided tests, we use the concept score distribution over the test set as a background distribution. For two-side tests, we use the concept score distribution over all query topic examples in the database as the hypothesis to test against. Thus, given the subset of related concepts for a specific query, and their weights, the corresponding concept detection ranked lists are combined using weighted averaging of the confidence scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Statistical Pseudo-Relevance Feedback</head><p>Pseudo-relevance feedback (PRF) examines initial retrieved documents per query topic as pseudo-positive/negative examples in order to select expanded discriminative concepts to improve the retrieval performance. Current PRF approaches use a small number of top-ranked documents to feed an automatic feedback process. If the initial retrieval performance returns poor results, as common in multimodal search scenarios, PRF is very likely to degrade the retrieval results. We take a more robust feedback approach, termed probabilistic local context analysis(pLCA) <ref type="bibr" target="#b40">[41]</ref>, that automatically leverages useful high-level semantic concepts to improve the initial retrieval output without assuming the top ranked examples are mostly relevant. The pLCA approach is derived from the probabilistic ranking principle, and it suggests ranking the retrieved shots in a descending order of the conditional probability of relevance. As combination weights for the semantic concepts are unknown, we treat them as latent variables wc and incorporate them into a relevance-based probabilistic retrieval model. If MT is the number of top-ranked shots defined by users, and S 0 (x j ) is the initial retrieval score for the shot x j , we can compute the conditional probability of relevance y by marginalizing the latent variables wc.</p><formula xml:id="formula_4">p(y|S 0 ) ∝ Z wc M T Y j=1 exp " y j S 0 (x j ) + y j X c w c S c (x j ) « dw c , (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>We adopt the mean field approximation <ref type="bibr" target="#b26">[26]</ref> to compute this marginal probability. First, we construct the family of variational distributions, q(y, w c ) = Q j q(w c |β c ) Q j q(y j |γ j ), as a surrogate to approximate the posterior distribution p(y|S 0 ), where q(ν c |β c ) is a Gaussian distribution with mean β c and the variance σ = 1, and q(y j |γ j ) is a Bernoulli distribution with a sample probability of γj. After some derivations <ref type="bibr" target="#b40">[41]</ref>, we can find that the variational distribution closest to p(y|S 0 ) must satisfy the following fix point equations,</p><formula xml:id="formula_6">γ j = » 1 + exp " 2S 0 (x j ) + 2 X c β c S c (x j ) «--1 β c = X j (2γ j -1)S c (x j ).<label>(4)</label></formula><p>These equations are invoked iteratively until the change of KL-divergence is small enough. Upon convergence (which is almost always guaranteed), we use the final q(y j |γ j ) as a surrogate to approximate the posterior probability without explicitly computing the integral, and we simply rank the documents in a descending order of the parameter γj as the retrieval outputs. This iterative update process typically converges in a small number of iterations and thus it can be implemented efficiently in a real retrieval system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">COMPARATIVE EVALUATION</head><p>We evaluate all approaches described in the previous section on the TRECVID 2005 and 2006 test corpora and query topics <ref type="bibr">[37]</ref>. Specifically, we use the TRECVID'05 corpus for parameter tuning and cross-validation, as needed, and evaluate final performance on TRECVID'06 corpus. Both collections consist of broadcast news video from U.S., Arabic, and Chinese sources, with durations of 30 minutes to 1 hour each. The TRECVID'05 test set consists of approximately 80 hours of video segmented into 45,765 shots, while the TRECVID'06 corpus has approximately 160 hours of video, segmented into 79484 shots. Each video comes with a speech transcript obtained through automatic speech recognition (ASR), as well as machine translation (MT) for the non-English sources. The quality of the ASR and MT transcripts is generally not very reliable but it is representative of the state of art in these fields and it is quite helpful for retrieval purposes, especially for named entity queries. For baseline comparison purposes, we use the speech-based retrieval approach described in <ref type="bibr" target="#b4">[4]</ref>. Each data set comes with ground truth for 24 query topics. We use Average Precision at depth 1000 to measure performance on a specific topic, and Mean Average Precision (MAP) to aggregate performance across multiple topics. Average Precision is the performance metric adopted by TRECVID, and essentially represents the area under the precision-recall curve. Three example query topics and corresponding concepts identified by the approaches we consider are listed in Table <ref type="table" target="#tab_4">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment I: Lexical query expansion</head><p>In the first experiment we evaluate the performance of the proposed lexical query expansion method leveraging a rule-based ontology mapping between a text annotation lexicon and the visual LSCOM-lite concept lexicon. We compare this approach against the text retrieval baseline as well as the two other lexical approaches for query expansionsynonym-based expansion (exact match) and WordNet-based expansion (soft match with Lesk-based similarity).</p><p>The results on both TRECVID collections are presented in Table <ref type="table" target="#tab_1">2</ref>, which lists performance of the text-based retrieval baseline and the three lexical query expansion approaches for three query classes and across all topics. From the results, it is evident that all of the lexical conceptbased query expansion approaches improve upon the text search baseline, and the proposed ontology rule-based mapping approach achieves the most significant improvements of the three lexical approaches. Improvement on the 2005 corpus ranges from a low of 12% on named person topics to a high of 85% on scene topics, with an average of about 27% gain over all topics. On 2006 data, the improvement is more modest, and we even see a loss of 10% on named entities. However, we can still observe substantial gains on the other query classes, including a 100% gain on generic people events, and a 34% gain on scene topics, leading to an overall improvement of 17% across all topics. From the experiments we also note that the three lexical approaches are somewhat complementary since top performance on each query class is typically achieved by different approaches. The proposed ontology mapping-based approach is most consistent, however, and achieves the best overall performance on both the 2005 and 2006 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experiment II: Statistical corpus analysis</head><p>We have evaluated the proposed statistical corpus analysisbased method for text query expansion using 2005 and 2006 TRECVID data sets and query topics. First, we empirically determine parameter values such as the thresholding method for concept confidence binarization, and the G 2score confidence-level thresholding for concept selection per query. Visual concept detection is inherently probabilistic and therefore its values are continuous. For statistical mapping of concepts to related text we binarize these values using a threshold as a function of the collection mean, µ, and standard deviation, σ, of concept confidence scores. We tried µ, µ + σ, and µ + 2σ and chose µ + 2σ based on performance on the TRECVID 2005 set. However, we observed very small performance variations for all three parameter settings so we believe the method is fairly robust with respect to this parameter. The other parameter we consider in our experiments determines the significance threshold for computed G 2 correlation scores between a visual concept and a text term. We tried various significance levels based on the χ 2 distribution with 1 degree of freedom, and finally select a confidence interval of 99.9%. Again, we note that the method is robust with respect to this parameter, and the optimal setting generalizes on both collections.  The final performance results are summarized across 3 query classes and over all topics in Figure <ref type="figure" target="#fig_3">4</ref>. On average, concept-based expansion improves the baseline retrieval result by 17% and 16% for the TRECVID 2005 and 2006 collections respectively. We note that specific person queries benefit the least and in fact, deteriorate on the 2006 set, but the other query classes improve significantly, with 30%-40% gains on 2005 and over 100% gains on 2006 non-named people event queries. The limited improvement (or even loss) on person-X topics can be explained by the fact that the named entity queries are very specific so the potential contribution of generic visual concepts is limited. The performance loss on the TRECVID06 queries is most likely due to the fact that the named entity queries in TRECVID 2006 were simply requests for a specific person, without additional constraints, such as entering/leaving a vehicle, a building, etc. In contrast, some of the named entity queries in 2005 included additional visual constraints, which benefited from visual concept-based filtering. Unlike the named entity query class, the general "people events" category benefited significantly from concept-based expansion and filtering on the 2006 data. The significantly larger performance gain, as compared to the same category on TRECVID 2005 data, is most likely due to the fact that the 2006 topics and data set were generally more difficult than their 2005 counterparts, which lowered the performance of the speechbased retrieval baseline and increased the dependency on other modalities and retrieval methods, such as conceptbased retrieval and re-ranking. In general, the empirical results confirm that concept-based query expansion and retrieval is very topic-dependent. In Section 5.4, we show however that the query-specific nature of the improvement can be leveraged by our query-dependent fusion approach, which can adaptively select and fuse the best query expansion approaches for each topic, leading to further performance improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experiment III: Content-based Modeling</head><p>In this experiment, we evaluate the overall impact of contentbased semantic query expansion and re-ranking as compared to a multimodal baseline MM-baseline. The MM-baseline is formed using query-independent fusion of speech-based and visual content-based retrieval runs to create a joint text-visual baseline (for details of both retrieval approaches, see <ref type="bibr" target="#b4">[4]</ref>). The mean average precisions for TRECVID 2005 and 2006 datasets are shown in Table <ref type="table">3</ref>. The content-based semantic query modeling component, termed MM-content run is constructed as a fusion of the multimodal baseline and the approach described in Sec. 4.4. The improvement of the MM-content run over the baseline is significant over a range of topics, and results in 25% overall MAP gain for the 2006 dataset. The content-based semantic model selection component, termed MM-selection run is constructed as a fusion of the multimodal baseline and the approach from Sec. 4.5. We evaluated T-score, χ 2 , majority vote, one-sided student test, and likelihood ratio statistical tests for LSCOM-lite concept relevance to the query topics from the TRECVID 2005 dataset. Thresholds were fixed on the global level based on the 95% statistical confidence level. Tscore had the highest MAP, and the fusion with the multimodal baseline offered 6.7% improvement over the baseline. This corresponding improvement on the 2006 query topics was 8.2%, as shown in Table <ref type="table">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Experiment IV: Overall Comparison</head><p>Table <ref type="table" target="#tab_4">4</ref> lists three sample query topics from TRECVID'06 and their corresponding concepts identified by the approaches we considered. It can be observed that although these methods found some common concepts with each other, a number of unique concepts are identified by various approaches. For example, text-based approaches tend to identify semantically related concepts for the query, and their relation is easy to be interpreted by human. On the other hand, statisticalbased approaches and visual-based approaches can find other visually related concepts such as US Flag, TV Screen for the "Dick Cheney" query, Vegetation for the "soccer" query, which are difficult to discover by textual relations alone. However, due to the noisy learning process, it is also possible for the last three approaches to introduce unexpected noise in the results, such as Weather for the "Dick Cheney" query in the pLCA method.</p><p>In order to quantitatively analyze the effects of modelbased retrieval, we evaluate the overall system performance for all the model-based retrieval experts in two stages, where the first stage only considers the textual query, and the second stage takes multimodal queries including image examples into account. Figure <ref type="figure" target="#fig_4">5</ref> (left) compares the MAP of the individual and fused retrieval experts. T-Baseline is the text retrieval system based on speech transcripts only. T-pLCA re-ranks this baseline using probabilistic local context analysis and the LSCOM models as described in Section 4.6. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T-Lexical and T-Corpus uses globally weighted linear combi-</head><note type="other">Query Topic Lexical WordNet Lexical rule</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean Average Precision</head><p>T-Fused MM-Fused and the baseline text runs, respectively. We can see that each of pLCA, Lexical and Statistical approaches improves the text retrieval baseline by 12% ∼ 15% (over a MAP of 0.052). Even though the 3 approaches perform similarly to each other, they are complementary, as query-dependent fusion of the four model-based retrieval runs brings the total improvement to 26%. Also, the proposed global corpus analysis-based approach performs on par with, or better than, all other approaches for text query expansion, without requiring any manual creation of synonym lists or mapping rules for each concept, as needed for the lexical approaches. This makes it the most scalable and generally applicable approach for concept-based text query expansion, and we expect it to significantly outperform lexical approaches on general topics which are not so easily linked to the concepts via synonyms (the TRECVID topics are somewhat biased towards the LSCOM-lite concepts by design).</p><p>In the second stage, we use query-independent fusion on text and content-based visual retrieval runs to create a joint text-visual baseline (MM-Baseline, which is 34% better than the text baseline. pLCA is again used to re-rank the baseline results to generate MM-pLCA. MM-Content augments MMbaseline by taking into account the concept presence of the visual examples as described in Section 4.4, resulting in 25% improvement over baseline alone for 2006 dataset. MMfused is produced by query-dependent fusion on the three MM-runs, generating 0.092 in MAP, or equivalently, a 77% improvement over the text-only baseline and a 31% gain over the multimodal baseline. In both experiments, pLCA can consistently bring a small improvement over the baseline approaches, but this improvement is not as significant as the MM-Content run in terms of average precision.</p><p>Figure <ref type="figure" target="#fig_4">5</ref> (right) shows the relative improvement in average precision of the T-fused and MM-fused runs for each of the 24 TRECVID06 queries sorted by the resulting improvement in MM-fused. We can see that the queries that map well to existing LSCOM concepts with high-performing detectors (e.g., Sports concept for query soccer goalpost) are indeed the ones with the most improvement, while the lack of related semantic detectors hampers the improvement on some other queries, e.g., the last query people with 10 books. These observations clearly confirm the advantage of leveraging additional semantic concepts over the text/image retrieval. Moreover, the complementary information provided by various expansion methods also allows us to further improve the retrieval results by combining their outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>In this paper we considered the problem of semantic conceptbased query expansion leveraging a set of visual concept detectors for video retrieval purposes. We presented a comprehensive review of existing approaches and proposed novel methods based on lexical rule-based ontology mapping and statistical correlation analysis. One of the proposed methods is the first global corpus analysis-based method for visual concept-based query expansion purposes, and performs on par with, or better than, the popular lexical approaches, without requiring manually constructed concept descriptions or mapping rules. All approaches were evaluated on the TRECVID datasets and query topics, and resulted in significant improvements over state-of-art retrieval baselines that do not use concept models. In particular, we observed 77% improvement over a text-based retrieval baseline and 31% improvement over a multimodal baseline.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The LSCOM-lite concept lexicon.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Statistical concept-based modeling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance improvement of statistical corpus analysis-based query expansion relative to text retrieval baseline on TRECVID 2005 and 2006.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Performance evaluation of multimodal fusion on text-based retrieval systems and multi-modal retrieval systems. (left) MAP on TRECVID06 dataset and query topics, see Section 5.4 for definitions of the runs. (right) Relative improvements over text retrieval baseline for each of the 24 TRECVID06 queries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>3.    </figDesc><table><row><cell>Concept-based</cell><cell>Text</cell><cell>Visual</cell><cell>Result-</cell></row><row><cell>query expansion</cell><cell>query</cell><cell>query</cell><cell>based</cell></row><row><cell>approaches</cell><cell>expansion</cell><cell>expansion</cell><cell>expansion</cell></row><row><cell>Lexical</cell><cell>[7, 35, 13, 8]</cell><cell></cell><cell></cell></row><row><cell cols="2">(language-specific) Sec. 4.1-4.2</cell><cell></cell><cell></cell></row><row><cell>Statistical</cell><cell>Section 4.3</cell><cell></cell><cell></cell></row><row><cell>(corpus-specific)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Statistical</cell><cell></cell><cell>[33, 22, 35, 29, 36]</cell><cell>[41]</cell></row><row><cell>(query-specific)</cell><cell></cell><cell>Sec. 4.4-4.5</cell><cell>Sec. 4.6</cell></row><row><cell cols="4">Table 1: Summary of concept-based query expan-</cell></row><row><cell cols="4">sion and retrieval approaches categorized along two</cell></row><row><cell cols="4">dimensions. See text for description of approaches.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance summary (Mean Average Precision scores) for text-based retrieval baseline and three lexical query expansion approaches.</figDesc><table><row><cell>Query Class</cell><cell cols="4">Text-Synonym WordNet Ontology</cell></row><row><cell>(# of topics)</cell><cell>Only</cell><cell cols="3">match similarity Mapping</cell></row><row><cell cols="5">TRECVID-2005 Mean Average Precision</cell></row><row><cell>PersonX (7)</cell><cell>0.217</cell><cell>0.228</cell><cell>0.204</cell><cell>0.244</cell></row><row><cell>People (9)</cell><cell>0.066</cell><cell>0.1037</cell><cell>0.092</cell><cell>0.088</cell></row><row><cell>Scenes (8)</cell><cell>0.036</cell><cell>0.061</cell><cell>0.068</cell><cell>0.067</cell></row><row><cell cols="2">All Topics (24) 0.099</cell><cell>0.124</cell><cell>0.116</cell><cell>0.125</cell></row><row><cell cols="5">TRECVID-2006 Mean Average Precision</cell></row><row><cell>PersonX (4)</cell><cell>0.148</cell><cell>0.127</cell><cell>0.123</cell><cell>0.133</cell></row><row><cell>People (12)</cell><cell>0.018</cell><cell>0.032</cell><cell>0.036</cell><cell>0.028</cell></row><row><cell>Scenes (8)</cell><cell>0.055</cell><cell>0.062</cell><cell>0.058</cell><cell>0.073</cell></row><row><cell cols="2">All Topics (24) 0.052</cell><cell>0.058</cell><cell>0.058</cell><cell>0.060</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Performance Improvement of Corpus Analysis-Based Query Expansion</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>140.0%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>126.2%</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>120.0%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Improvement on TRECVID-2005</cell></row><row><cell>Improvement over text retrieval baseline (%)</cell><cell>0.0% 20.0% 40.0% 60.0% 80.0% 100.0%</cell><cell>6.7%</cell><cell>29.7%</cell><cell>43.0%</cell><cell>12.9%</cell><cell>16.8% Improvement on TRECVID-2006 16.2%</cell></row><row><cell></cell><cell></cell><cell>PersonX Topics</cell><cell>People Events</cell><cell cols="2">Scenes/sites</cell><cell>All Topics</cell></row><row><cell></cell><cell>-20.0%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>-20.5%</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>-40.0%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>. The most relevant concept identified for the sample TRECVID 2006 topics using the T-score method are presented in Table4.</figDesc><table><row><cell cols="4">TRECVID MM-baseline MM-content MM-selection</cell></row><row><cell>2005</cell><cell>0.15143</cell><cell>0.16678</cell><cell>0.16169</cell></row><row><cell>2006</cell><cell>0.06962</cell><cell>0.08704</cell><cell>0.07535</cell></row><row><cell cols="4">Table 3: Content-based modeling impact on mean</cell></row><row><cell cols="4">average precision (MAP) over the optimized multi-</cell></row><row><cell cols="4">modal baseline for TRECVID 2005 and 2006 top-</cell></row><row><cell cols="4">ics for content-based semantic query modeling ap-</cell></row><row><cell cols="4">proach (Sec. 4.4) and content-based semantic model</cell></row><row><cell cols="3">selection approach (Sec. 4.5).</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Sample topics and most related concepts identified by various concept-based expansion approaches.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-based</cell><cell cols="3">Statistical global</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Prob. local</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Visual content</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Lesk similarity</cell><cell cols="3">ontology mapping</cell><cell cols="2">corpus analysis</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">context analysis</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">selection</cell></row><row><cell cols="2">U.S. Vice</cell><cell></cell><cell cols="3">Gov. Leader,</cell><cell></cell><cell cols="2">Gov. Leader,</cell><cell cols="4">Corp. Leader, Studio,</cell><cell></cell><cell cols="10">Corp. Leader, Weather</cell><cell></cell><cell></cell><cell cols="5">Corp. Leader, Face,</cell></row><row><cell cols="2">President</cell><cell></cell><cell cols="3">Corp. Leader</cell><cell></cell><cell>Face</cell><cell></cell><cell cols="4">Gov. Leader, Meeting,</cell><cell></cell><cell></cell><cell cols="8">Police, Vegetation</cell><cell></cell><cell></cell><cell></cell><cell cols="5">Gov. Leader, Person,</cell></row><row><cell cols="3">Dick Cheney</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">PC TV Screen, Face</cell><cell></cell><cell></cell><cell cols="10">PC TV Screen, US Flag</cell><cell></cell><cell></cell><cell></cell><cell cols="4">Studio, US Flag</cell></row><row><cell cols="3">Soldiers, police,</cell><cell cols="3">Military, Court,</cell><cell cols="3">Military, Court,</cell><cell cols="4">Military, Corp. Leader,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Court</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Military, Police,</cell></row><row><cell cols="3">guards escort</cell><cell cols="3">Prisoner, Police,</cell><cell cols="3">Prisoner, Police,</cell><cell cols="4">US Flag, Studio, Gov.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Prisoner, Gov. Leader,</cell></row><row><cell cols="2">a prisoner</cell><cell></cell><cell></cell><cell cols="2">Person</cell><cell cols="3">Person, Explosion</cell><cell cols="4">Leader, PC TV Screen</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Crowd, Walking Running</cell></row><row><cell cols="2">Soccer</cell><cell></cell><cell></cell><cell cols="2">Sports,</cell><cell cols="3">Sports, Person,</cell><cell cols="3">Sports, Vegetation,</cell><cell></cell><cell></cell><cell></cell><cell cols="8">Sports, Vegetation</cell><cell></cell><cell></cell><cell></cell><cell cols="5">Sports, Vegetation,</cell></row><row><cell cols="2">goalposts</cell><cell></cell><cell cols="3">Walking Running</cell><cell cols="3">Walking Running</cell><cell cols="3">Walking Running</cell><cell></cell><cell></cell><cell></cell><cell cols="8">Walking Running</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Walking Running</cell></row><row><cell></cell><cell>0.10 0.09</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.087</cell><cell>0.092</cell><cell>200%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mean Average Precision</cell><cell>0.02 0.03 0.04 0.05 0.06 0.07 0.08</cell><cell>0.052</cell><cell>0.058 0.059</cell><cell>0.060</cell><cell>0.066</cell><cell>0.070</cell><cell>0.074</cell><cell></cell><cell>0% 50% 100% 150%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">0.00 0.01 T-Baseline MM-Baseline MM-pLCA T-pLCA</cell><cell cols="2">T-Lexical MM-Content</cell><cell cols="2">T-Corpus MM-Fused</cell><cell>T-Fused</cell><cell>-50%</cell><cell>Average</cell><cell>people w. flag</cell><cell>tall buildings</cell><cell>soccer goalpost</cell><cell>a natural scene</cell><cell>soldiers or police</cell><cell>newspaper</cell><cell>leave/enter vehicle</cell><cell>uniformed people</cell><cell>water boat-ship</cell><cell>helicopters</cell><cell>ppl w. computer</cell><cell>smokestacks</cell><cell>protest w. building</cell><cell>adult and child</cell><cell>kiss on the cheek</cell><cell>escorting prisoners</cell><cell>snow scene</cell><cell>Saddam Hussein</cell><cell>Bush walking</cell><cell>Condoleeza Rice</cell><cell>emergency vehicles</cell><cell>burning w. flames</cell><cell>Dick Cheney</cell><cell>person &amp; 10 books</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Argillander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Haubold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Iyengar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ebadollahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Naphade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tešić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Volkmer</surname></persName>
		</author>
		<title level="m">IBM Research TRECVID-2005 video retrieval system. In NIST TRECVID Video Retrieval Workshop</title>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-11">Nov. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Extended gloss overlaps as a measure of semantic relatedness</title>
		<author>
			<persName><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference on Artificial Intelligence</title>
		<meeting><address><addrLine>Mexico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">Aug. 9-15 2003</date>
			<biblScope unit="page" from="805" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">IBM&apos;s PIQUANT II in TREC2004</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Czuba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ittycheriah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>-Goldensohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIST Text Retrieval Conference (TREC)</title>
		<meeting><address><addrLine>Gaithersburgh, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-11">November 2004</date>
			<biblScope unit="page" from="16" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">IBM Research TRECVID-2006 Video Retrieval System</title>
		<author>
			<persName><forename type="first">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Haubold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ebadollahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Naphade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tešić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIST TRECVID Video Retrieval Workshop</title>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-11">Nov. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic query refinement using lexical affinities with maximal information gain</title>
		<author>
			<persName><forename type="first">D</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Farchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Petruschka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Soffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>Tampere, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="11" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Columbia University TRECVID-2006 Video Search and High-Level Feature Extraction</title>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yanagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zavesky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIST TRECVID Video Retrieval Workshop</title>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-11">Nov. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Columbia University TRECVID-2005 Video Search and High-Level Feature Extraction</title>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yanagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zavesky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIST TRECVID Video Retrieval Workshop</title>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-11">Nov. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">TRECVID-2006 by NUS-I 2 R</title>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Neo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-K</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chaisorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIST TRECVID Video Retrieval Workshop</title>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date>Nov</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Combining approaches to information retrieval</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval</title>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Accurate methods for the statistics of surprise and coincidence</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dunning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comp. Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="74" />
			<date type="published" when="1993-03">Mar. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A corpus analysis approach for automatic query expansion</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gauch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;97: Proc. 6th Intl. Conf. on Information and Knowledge Management</title>
		<meeting><address><addrLine>Las Vegas, NV</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="278" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semantic multimedia retrieval using lexical query expansion and model-based reranking</title>
		<author>
			<persName><forename type="first">A</forename><surname>Haubold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Naphade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Conf. on Multimedia and Expo (ICME)</title>
		<meeting><address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07">July 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Christel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Concescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">CMU Informedia&apos;s TRECVID 2005 Skirmishes</title>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-11">Nov. 2005</date>
		</imprint>
	</monogr>
	<note>NIST TRECVID Video Retrieval Workshop</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Video search reranking via information bottleneck principle</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th Annual ACM Intl. Conference on Multimedia</title>
		<meeting>14th Annual ACM Intl. Conference on Multimedia<address><addrLine>Santa Barbara, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="35" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An association thesaurus for information retrieval</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. RIAO&apos;94</title>
		<meeting>RIAO&apos;94</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="146" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Automatic Keyword Classification for Information Retrieval</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Jones</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971">1971</date>
			<publisher>Butterworths</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic discovery of query-class-dependent models for multimodal search</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia 2005</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-11">Nov. 2005</date>
			<biblScope unit="page" from="882" to="891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A formal approach to score normalization for meta-search</title>
		<author>
			<persName><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT</title>
		<meeting>HLT</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="98" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On the detection of semantic concepts at TRECVID</title>
		<author>
			<persName><forename type="first">M</forename><surname>Naphade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Souvannavong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-11">Nov 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Large-Scale Concept Ontology for Multimedia</title>
		<author>
			<persName><forename type="first">M</forename><surname>Naphade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tešić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Curtis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE MultiMedia Magazine</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="86" to="91" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semantic representation, search and mining of multimedia content</title>
		<author>
			<persName><forename type="first">A</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Naphade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th ACM SIGKDD Intl. Conf. on Knowledge Discovery and Data Mining (KDD &apos;04)</title>
		<meeting><address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-08">Aug. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning the semantics of multimedia queries and concepts from a small number of examples</title>
		<author>
			<persName><forename type="first">A</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Naphade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tešić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia 2005</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-11">Nov. 6-11 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Video retrieval using high level features: Exploiting query matching and confidence-based weighting</title>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Neo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIVR 2006</title>
		<meeting><address><addrLine>Tempe, AZ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07">July 2006</date>
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Using measures of semantic relatedness for word sense disambiguation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Patwardhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Intelligent Text Processing and Computational Linguistics</title>
		<meeting><address><addrLine>Mexico</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003-02">Feb. 2003</date>
			<biblScope unit="page" from="16" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A mean field theory learning algorithm for neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Complex Systems</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="995" to="1019" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods</title>
		<author>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Large Margin Classifiers</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Concept based query expansion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Frei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 16th Annual Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval</title>
		<meeting>16th Annual Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="160" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Query by semantic example</title>
		<author>
			<persName><forename type="first">N</forename><surname>Rasiwasia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Moreno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIVR 2006</title>
		<meeting><address><addrLine>Tempe, AZ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07">July 2006</date>
			<biblScope unit="page" from="51" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Using Information Content to Evaluate Semantic Similarity in a Taxonomy</title>
		<author>
			<persName><forename type="first">P</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Joint Conf. Artificial Intelligence</title>
		<meeting><address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="448" to="453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Relevance feedback in information retrieval</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Rocchio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971">1971</date>
			<publisher>Prentice-Hall Inc</publisher>
			<biblScope unit="page" from="313" to="323" />
			<pubPlace>Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
	<note>Relevance Feedback in Information Retrieval</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A cooccurrence-based thesaurus and two applications to information retrieval</title>
		<author>
			<persName><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Information Retrieval Systems RIAO&apos;94</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="266" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multimedia semantic indexing using model vectors</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Naphade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Natsev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intl. Conference on Multimedia and Expo (ICME &apos;03)</title>
		<meeting><address><addrLine>Baltimore, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-07">July 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The challenge problem for automated detection of 101 semantic concepts in multimedia</title>
		<author>
			<persName><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Geusebroek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th Annual ACM Intl. Conference on Multimedia</title>
		<meeting>14th Annual ACM Intl. Conference on Multimedia<address><addrLine>Santa Barbara, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="421" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The MediaMill TRECVID 2005 Semantic Video Search Engine</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Geusebroek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huurnink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Koelma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">D</forename><surname>Rooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Seinstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Veenman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIST TRECVID Video Retrieval Workshop</title>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-11">Nov. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cluster-based data modeling for semantic video search</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tešić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM CIVR 2007</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-07">July 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Query expansion using lexical-semantic relations</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Vorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-08">August 1994</date>
			<biblScope unit="page" from="61" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dynamic multimodal fusion in video search</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tešić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME 2007</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Query expansion using local and global document analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-08">August 1996</date>
			<biblScope unit="page" from="18" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Probabilistic Models for Combining Multiple Knowledge Sources in Multimedia Retrieval</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning query-class dependent weights in automatic video retrieval</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th Annual ACM Intl. Conf. on Multimedia</title>
		<meeting>12th Annual ACM Intl. Conf. on Multimedia</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="548" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A comparative study on feature selection in text categorization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th Intl. Conf. on Machine Learning (ICML&apos;97)</title>
		<meeting>14th Intl. Conf. on Machine Learning (ICML&apos;97)</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
