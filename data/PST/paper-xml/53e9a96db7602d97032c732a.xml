<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Discovering Discriminative Graphlets for Aerial Image Categories Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Luming</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yahong</forename><surname>Han</surname></persName>
							<email>yahong@tju.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<email>yiyang@cs.cmu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Mingli</forename><surname>Song</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<postCode>310027</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<postCode>300072</postCode>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Tianjin Key Laboratory of Cogni-tive Computing and Application</orgName>
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<postCode>300072</postCode>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">School of Information Technology and Electrical Engineering</orgName>
								<orgName type="institution">University of Queensland</orgName>
								<address>
									<postCode>4072</postCode>
									<settlement>Brisbane</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Department of Electrical and Computer Engi-neering</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<postCode>119613</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Texas</orgName>
								<address>
									<addrLine>San Antonio</addrLine>
									<postCode>78249-1604</postCode>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Discovering Discriminative Graphlets for Aerial Image Categories Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BCEB4A132072543DA6EFA0A8C5ED27C3</idno>
					<idno type="DOI">10.1109/TIP.2013.2278465</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Aerial image category</term>
					<term>graphlets</term>
					<term>topologies selection</term>
					<term>discrimination</term>
					<term>redundancy</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recognizing aerial image categories is useful for scene annotation and surveillance. Local features have been demonstrated to be robust to image transformations, including occlusions and clutters. However, the geometric property of an aerial image (i.e., the topology and relative displacement of local features), which is key to discriminating aerial image categories, cannot be effectively represented by state-of-the-art generic visual descriptors. To solve this problem, we propose a recognition model that mines graphlets from aerial images, where graphlets are small connected subgraphs reflecting both the geometric property and color/texture distribution of an aerial image. More specifically, each aerial image is decomposed into a set of basic components (e.g., road and playground) and a region adjacency graph (RAG) is accordingly constructed to model their spatial interactions. Aerial image categories recognition can subsequently be casted as RAG-to-RAG matching. Based on graph theory, RAG-to-RAG matching is conducted by comparing all their respective graphlets. Because the number of graphlets is huge, we derive a manifold embedding algorithm to measure different-sized graphlets, after which we select graphlets that have highly discriminative and low redundancy topologies. Through quantizing the selected graphlets from each aerial image into a feature vector, we use support vector machine</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>A ERIAL image categories recognition is a key component of many practical applications, e.g., scene annotation <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b4">[5]</ref>, video surveillance <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, and robotics path planning <ref type="bibr" target="#b7">[8]</ref>. It helps to accept likely scene configurations and to rule out unlikely ones. For example, a successful scene annotation system should encourage the occurrence of a parking lot in an aerial image belonging to a "downtown area" while suppressing the occurrence of an airport in an aerial image belonging to a "residential area". However, successfully recognizing aerial image categories is still challenging due to the large number of basic components in an aerial image and their complicated spatial interactions. More specifically, the various spatial configurations are the key characteristics used to represent the aerial image categories, but state-of-the-art recognition models do not capture them effectively. As shown in Fig. <ref type="figure">1</ref>, the basic components of the aerial image from the "residental" category and that from the "intersection" category have similar appearances. Rough geometric modeling, such as SIFT <ref type="bibr" target="#b1">[2]</ref> based SPM <ref type="bibr" target="#b8">[9]</ref>, cannot effectively discriminate them. However, the discriminative subgraphs (i.e., graphlets) from the two aerial images are significantly different. As a result, we believe that satisfactory performance can be achieved if these discriminative graphlets are efficiently mined, and further integrated into a statistical model for categorization.</p><p>In this paper, we propose a new aerial image categories recognition model, as illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>. An aerial image can be interpreted by a set of basic components and the associated spatial relationships between them. Thus, it can be intuitively represented by a graph, i.e., basic components linked by a set of edges. In our approach, by localizing the basic components in an aerial image, a region adjacency graph (RAG) is constructed to describe the color/texture distribution and the geometric property of an aerial image. Based on RAG, aerial image categories recognition is conducted via RAG-to-RAG matching. Inspired by graph theory <ref type="bibr" target="#b42">[43]</ref>, RAG-to-RAG matching is conducted by matching all their respective graphlets. To effectively match different-sized Fig. <ref type="figure">1</ref>. Top: SPM cannot effectively discriminate between aerial images from the "residental" category and the "intersection" category, because the local features from each grid are similar. Bottom: discriminative graphlets from the two aerial images differ significantly.</p><p>graphlets, a manifold embedding algorithm is designed to transform graphlets into equal-length feature vectors. Due to the large number of graphlets in an RAG, on the basis of the post-embedding graphlets, we mine frequent topologies in the training aerial images, and a topology refining process that is geared toward being more discriminative and less redundant is conducted on the frequent topologies. On the basis of the refined topologies, we extract the corresponding discriminative graphlets in each aerial image and quantize them into a vector. An SVM classifier is then trained based on the vector for aerial image categorization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Our work is closely related to two research topics: graphbased object/scene representation and spatial pyramid matching (SPM)-based generic object recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Graph-Based Object/Scene Representation</head><p>As natural binary relationship descriptors, graph models are frequently used to exploit the geometric property of the components of an object or scene. In the graph models, each vertex represents an atomic region and pairwise spatially neighboring atomic regions are linked by an edge. In <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr">Harchaoui et al.</ref> proposed walk/tree kernel to capture the walk/tree structures among local image features using a finite sequence of neighboring regions. Unfortunately, the unavoidable totter phenomenon <ref type="bibr" target="#b12">[13]</ref> introduces noise, and hence limits the discriminative ability of the walk/tree kernel. To obtain a better discriminative power, in many cases, parameters are provided to tune the size of walk/tree. However, tuning the size of walk/tree results in a large number of walk/tree-structurally arranged atomic regions. Keselman et al. <ref type="bibr" target="#b13">[14]</ref> proposed acquisition of the lowest common abstraction (LCA) among a set of images, i.e., common subgraphs among a set of graphs corresponding to images. However, Keselman et al. approach is data set dependent; the output of the algorithm cannot be directly used by a classifier, such as SVM. Further, the results of experiments conducted indicate that the resulting LCA becomes unstable when there are more than two input images. In <ref type="bibr" target="#b14">[15]</ref>, Demirci et al. presented an object recognition model that establishes many-to-many correspondences between the nodes of two noisy and vertex-labeled weighted graphs. However, their approach is sensitive to clutter and occlusions. Felzenszwalb et al. <ref type="bibr" target="#b15">[16]</ref> modeled the connection between object parts as a spring and defined their mismatching cost accordingly. The matching of pairwise objects is computed by minimizing a function with respect to the mismatching cost. However, this model relies heavily on optimal background subtraction. In <ref type="bibr" target="#b16">[17]</ref>, the vertices of a graph are used to represent both known and unknown objects, and the semantics of each unknown object is inferred based on the semantics of its K spatially nearest neighboring known segments. However, only knowledge of spatially neighboring segments are exploited; their topology, another important cue for semantic inference, is not considered. In <ref type="bibr" target="#b37">[38]</ref>, Duchenne et al. proposed a graph matching kernel for object categorization; the vertices of which correspond to a set of image grids. The edges reflect the grid structure and function as springs to preserve the geometric property of spatially neighboring grids. However, the grid cannot represent basic aerial image components, such as the "intersection" in Fig. <ref type="figure">1</ref>. In <ref type="bibr" target="#b44">[45]</ref>, Wang et al. proposed using a hierarchical connection graph (HCG) to detect and extract gable roofs from aerial imagery, based on a self-avoiding polygon (SAP) model. The SAP model is a deformable shape model that can represent gable roofs of various shapes and appearances. In <ref type="bibr" target="#b45">[46]</ref>, Porway et al. proposed a hierarchical and contextual model for aerial image understanding, wherein different components in an aerial image (e.g., cars and roofs) are organized into hierarchical graphs whose appearances and configurations are determined by statistical constraints, such as relative position, relative scale, etc. Lin et al. <ref type="bibr" target="#b46">[47]</ref> presented an object categorization framework based on sketch graphs, wherein shape and structure cues are exploited. The key concept underlying this framework is a learnable And-Or graph model that hierarchically combines the reconfigurability of a stochastic context free grammar. In <ref type="bibr" target="#b47">[48]</ref>, Lin et al. further proposed a hierarchical generative model for recognizing compositional object categories. Specifically, objects are decomposed into different parts and the relationships between the parts are modeled by stochastic attribute graph grammars, which are embedded in an And-Or graph for each compositional object category. In <ref type="bibr" target="#b33">[34]</ref>, Zhang et al. proposed a scene categorization model created by boosting highly discriminative and low redundancy graphlets. However, the topology of the graphlets is again not considered in their selection process. Further, in <ref type="bibr" target="#b41">[42]</ref>, Zhang et al. proposed to measure the similarity between aerial images by enumeratively matching their respective graphlets. Unfortunately, in practice, this strategy is computationally intractable when the number of basic aerial image components is large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Spatial Pyramid Matching-Based Recognition</head><p>Although graph-based image representations model the geometric property of images explicitly and effectively, many of them are designed for a specific dataset or rely heavily on optimal image preprocessing. Toward a general and robust image representation, spatial pyramid is employed to incorporate a rough geometric property into the recognition model. Lazebnik et al. <ref type="bibr" target="#b8">[9]</ref> developed SPM, in which an image is partitioned into increasingly fine grids and histograms of local features inside each grid are computed. However, experimental  results show that good performance is achieved only when SPM is combined with a nonlinear SVM. As a consequence, SPM is susceptible to a high computational complexity, i.e., O(N 2 ∼N 3 ) in the training stage. Yang et al. <ref type="bibr" target="#b26">[27]</ref> proposed the encoding of image local descriptors by sparse coding <ref type="bibr" target="#b27">[28]</ref> for efficient recognition. Further, Wang et al. <ref type="bibr" target="#b25">[26]</ref> improved on conventional SPM by utilizing the locality constraints to encode each local descriptor. Experimental results show that Yang et al. and Wang et al. approaches perform well under a linear SVM. In <ref type="bibr" target="#b35">[36]</ref>, Li et al. proposed a more effective codebook to facilitate the conventional SPM, where the new codebook is orthogonal to the original one. In <ref type="bibr" target="#b36">[37]</ref>, Xie et al. proposed combining SIFT descriptors from both the original image and its edgemap, into an SPM. It is worth noting that the aforementioned SPM models rely completely on the low-level cues; therefore they capture no semantics and are potentially not descriptive enough for high-level recognition tasks. To integrate high-level cues, Li et. al. <ref type="bibr" target="#b32">[33]</ref> proposed to describe an image by a large number of pre-specified generic object detectors. In summary, there are two limitations with these SPM-based approaches. First, a single grid is not discriminative enough. Some grids, which may be noisy due to occlusions and variations in background, affect the recognition accuracy. Second, the spatial relations among grids are ignored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SVM classifier</head><p>Compared to the related works discussed above, the advantages of our proposed method are three-fold: 1) the proposed graphlets are general graphs that are capable of capturing arbitrary spatial relationships among atomic regions. This is in direct contrast to several related works, which use special types of graphs, such as tree <ref type="bibr" target="#b11">[12]</ref> or And-Or graphs <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, to capture the spatial relationships, which limits their descriptive ability; 2) among all the cited graph-based recognition models, only our proposed method can select highly discriminative and low redundancy graphlets for aerial image recognition, which greatly enhances recognition performance; and 3) SPM-based models only roughly capture the spatial relations among image grids, whereas the proposed graphlets can accurately model the spatial relations among aerial image components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. GRAPHLET-MATCHING-BASED AERIAL IMAGE CATEGORIZATION</head><p>An aerial image usually comprises millions of pixels. If we treat each pixel as a local feature, high computational complexity would make the aerial image category recognition task intractable. Therefore, we instead interpreting an aerial image by a set of atomic regions and their associated spatial interactions. As shown in Fig. <ref type="figure" target="#fig_2">3</ref>, we can model this representation as a labeled graph G = (V, E), termed region adjacency graph (RAG). Each label from set V denotes the local features (i.e., color/texture) from an atomic region. Each edge from set E links a pair of spatially neighboring atomic regions. To obtain the atomic regions, unsupervised fuzzy clustering (UFC), is employed. As a completely data-driven segmentation algorithm, UFC usually produces numerous imperfectly segmented regions. <ref type="foot" target="#foot_0">1</ref> To maximally preserve optimally segmented regions, we first generate a large number of atomic regions with UFC tolerance bound {0.1, 0.2, 0.3, 0.4, 0.5}. We then remove any imperfect segmented regions. That is, segmented regions that have low correlation with one particular basic aerial image's components are abandoned. The correlation is calculated using an entropy-based measure:</p><formula xml:id="formula_0">(r ) = - i p(c i (r )) • log p(c i (r )) (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where c i (r ) indicates the probability of segmented region r belonging to the i -th basic component category, which denotes the probabilistic output of the 13-class SVM <ref type="bibr" target="#b31">[32]</ref> trained from the basic aerial image components searched by using Google (see details in the appendix).</p><p>To identify whether two atomic regions are spatially adjacent, we first need to label the location of each atomic region. Inspired by spatial pyramid matching (SPM) <ref type="bibr" target="#b8">[9]</ref>, we construct a three-level spatial pyramid to label the location of each atomic region. As shown in Fig. <ref type="figure">4</ref>, an atomic region's corresponding cell denotes the cell into which the atomic region can be maximally enclosed, in a coarse-to-fine manner. Our approach is notably different from local feature location labeling in SPM, where each local feature corresponds to a pixel and can be perfectly divided into a cell. In many cases, it is difficult to completely divide an atomic region into a cell because each atomic region usually contains hundreds of pixels. In this work, if 90% of the pixels in an atomic region overlap a cell, we deem that that atomic region can be divided into the cell. After labeling the location of each atomic region, two regions are deemed to be spatially adjacent if their corresponding cells are identical or neighboring.</p><p>It is natural to try to recognize an aerial image by matching its RAG with another. However, as proved in <ref type="bibr" target="#b18">[19]</ref>, given a pair of graphs, determining whether they are isomorphic (i.e., that they share the same geometric property) is NP-hard. Thus, matching RAG-to-RAG directly is computationally intractable. Alternatively, based on graph theory, we can represent an RAG by a collection of its connected subgraphs, that is, graphlets. Formally, we call a graphlet with t vertices a t-sized graphlet. By introducing the concept of graphlets, aerial image categorization can be carried out by matching an aerial image's graphlets to those of another aerial image. Importantly, graphlet-to-graphlet matching-based image categorization is more robust than the previous tree-to-tree and walk-to-walk matching-based image categorizations. This is because walk and tree can be deemed special cases of the proposed graphlets, as illustrated in Fig. <ref type="figure">5</ref>. Other structures representative of aerial image categories, such as triangles and quadrangles, cannot be captured by walk or tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. SELECTING HIGHLY DISCRIMINATIVE AND LOW REDUNDANCY TOPOLOGIES</head><p>An RAG typically contains tens to hundreds of vertices. Given 40 vertices in an RAG and an average vertex degree of five, there are 40 * 5 7 /5! ≈ 26000 seven-sized graphlets, which makes recognition an aerial image by matching all its graphlets to those of another one in an enumerative manner impractical. Toward a discriminative and concise representation for aerial image categories recognition, only the graphlets with highly discriminative and low redundancy topologies are taken into account. We conduct topology-level feature selection because the number of candidate topologies is smaller than that of candidate graphlets. That is, different graphlets may have the same topology, such as the example given in Fig. <ref type="figure" target="#fig_4">6</ref>. In this paper, if graphlet G corresponds to topology S, we denote this relation as G ∼ S. Notably, to further accelerate the topology-level feature selection, in our implementation, only frequent topologies are used as candidates for selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. FSG-Based Frequent Topology Mining</head><p>Before selecting the highly discriminative and low redundancy topologies, <ref type="foot" target="#foot_1">2</ref> we need to select topologies that are representative of the training RAGs. That is, we need to select the frequent topologies in advance. FSG <ref type="bibr" target="#b19">[20]</ref>, an efficient frequent subgraph discovery algorithm, is employed in our approach to find frequent topologies. Given the RAGs of the training aerial images, FSG tabulates the number of times each topology occurs and contains the probabilities of all topologies. A topology with probability P(S) &gt; σ f is preserved. To determine the threshold σ f , we first note that our proposed approach discriminates aerial images in one category from that of another. Thus, we want topologies that are representative of at least one aerial image category. In our implementation, we set σ f = min(N c )/N, where N c is the number of training aerial images from the c-th category and N is the total number of training aerial images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Graphlet Embedding on the Grassmann Manifold</head><p>A topology that has a high frequency is unnecessarily highly discriminative for aerial image category recognition. Therefore, it is reasonable for us to carry out further selection among the frequent topologies toward being more discriminative and less redundant. To derive the discrimination of a topology and the redundancy between topologies, we need a metric between graphlets. Unfortunately, this task is challenging because: 1) it is essential that a graphlet's topology be incorporated into into the metric, and 2) comparing graphlets with different vertices is difficult.</p><p>To encode the topology of a graphlet, we represent each t-sized graphlet by a matrix M ∈ R t ×(t +137) that captures both its color/texture distribution and its topology:</p><formula xml:id="formula_2">M = [M 1 , M 2 ] (2)</formula><p>where M 1 is a t × 137 matrix, in which each row denotes the concatenated nine-dimensional color moment and 128-dimensional HOG extracted from each atomic region; and M 2 is a t × t adjacent matrix, i.e., M 2 (i, j ) = 1 if r i and r j are spatially adjacent 0 otherwise .</p><p>In accordance with <ref type="bibr" target="#b23">[24]</ref>, each matrix can be deemed as a point on the Grassmann manifold, and the Golub-Werman distance between matrices is defined as:</p><formula xml:id="formula_4">d GW (M, M ) = ||M U -M V || 2<label>(4)</label></formula><p>where M U and M V are the orthonormal bases of M and M respectively.</p><p>To compare graphlets that have different vertices, we design an embedding algorithm to transform all graphlets into equallength feature vectors. In addition, we note that preservation of the pairwise distances of the graphlets in the embedded space is essential. The distance between pairwise graphlets reflects their relative displacement. As shown on the right side of Fig. <ref type="figure" target="#fig_5">7</ref>, the Golub-Werman distance d GW between graphlets G 1 and G 3 reflects the relative position between two residential areas covered by G 1 and G 3 . If we straightforwardly preserve all the pairwise distances between graphlets in the embedding process, all their relative positions are kept. Further, this operation can implicitly preserve the global spatial layout. As shown in Fig. <ref type="figure" target="#fig_5">7</ref>, preserving three relative distances, between (G 1 , G 2 ), (G 1 , G 3 ) and (G 2 , G 3 ) roughly capture the global spatial layout, and intuitively, when more graphlets are considered, a more accurate global spatial layout can be kept. Furthermore, it is necessary to maximize the discrimination of graphlets in the embedding process. On the basis of these two observations, the objective function of the graphlet embedding is given as:</p><formula xml:id="formula_5">arg min Y h i j [d GW (M h i , M h j ) -d E (y h i , y h j )] 2</formula><p>pairwise graphlets distance within an aerial image</p><formula xml:id="formula_6">+ i, j ||y i -y j || 2 l w (i, j ) - i j ||y i -y j || 2 l b (i, j )</formula><p>discrimination of graphlets <ref type="bibr" target="#b4">(5)</ref> where Y = [y 1 , y 2 , . . . , y N ] ∈ R d×N is a matrix containing the post-embedding graphlets; M h i and M h j respectively denote the i -th and the j -th identical-sized matrix from the h-th aerial image; y h i and y h j are their low-dimensional representations; </p><formula xml:id="formula_7">M i ) = c(M j ) , then l w (i, j ) = 1/N c -1/N, otherwise l w (i, j ) = 0. If c(M i ) = c(M j ), l b (i, j ) = 1/N, otherwise l b (i, j ) = 0.</formula><p>Here N denotes the total number of training graphlets, N c denotes the number of graphlets from the c-th category, and N h is the number of graphlets from the h-th training photo. The term 1/N c -1/N is used to assign a larger weight to categories with fewer graphlets. It is important to note that, different-sized graphlets are embedded independently by <ref type="bibr" target="#b4">(5)</ref>, and they are transformed into equallength feature vectors.</p><p>Denote</p><formula xml:id="formula_8">D h GW = [d GW (M h i , M h j )</formula><p>] as a matrix wherein each element is the Golub-Werman distance between identicalsized graphlets extracted from the h-th photo, the inner product matrix is obtained by</p><formula xml:id="formula_9">τ (D h GW ) = -R N h S h GW R N h /2, where (S h GW ) i j = (D h GW ) 2 i j ; R N h = I N h -e N h e T N h</formula><p>/N is the centralization matrix; I N h is an N h × N h identity matrix and e N h = [1, 1, . . . , 1] T ∈ R N h . Therefore, the first term in ( <ref type="formula">5</ref>) can be rewritten as:</p><formula xml:id="formula_10">arg min Y h i j [d GW (x h i , x h j ) -d(y h i , y h j )] 2 = arg min Y h ||τ (D h GW ) -τ (D h Y )|| 2 = arg max Y h tr(Y τ (D h GW )Y T ) = arg max Y tr(Y τ (D GW )Y T ). (<label>6</label></formula><formula xml:id="formula_11">)</formula><p>The second term in (5) can be rewritten as:</p><formula xml:id="formula_12">arg max Y (Y L B Y T -Y L W Y T ) (<label>7</label></formula><formula xml:id="formula_13">)</formula><p>where L B ∈ R N×N is defined as:</p><formula xml:id="formula_14">L B (i, j ) = 1 N I N if c(M i ) = c(M j ) 0 otherwise (8) L W (i, j ) = ( 1 N C -1 N )I N if c(M i ) = c(M j ) 0 o t h e r w i s e<label>(9)</label></formula><p>Therefore, the above objective function can be rewritten into the following matrix form as:</p><formula xml:id="formula_15">arg max Y tr(Y (L B -L W + τ (D GW ))Y T ) = arg max U tr(Y ZY T ) s.t.Y Y T = I N (<label>10</label></formula><formula xml:id="formula_16">)</formula><p>where</p><formula xml:id="formula_17">Z = L B -L W + τ (D GW ) is an N × N matrix; and Y Y T = I N is a term to uniquely determine Y .</formula><p>We note that (10) is a quadratic problem with quadratic constraints that can be solved by eigenvalue decomposition, with a time complexity of O(N 3 ). However, Z is a large matrix because usually N &gt; 50, 000. Thus, solving the problem (10) using a global once-for-all eigenvalue decomposition on matrix Z is computationally intractable. In our approach, we decompose the eigenvalue into a set of subproblems. First, we solve an initial embedding Y (0) using <ref type="bibr" target="#b9">(10)</ref> under N (0)  training graphlets, where N (0) &lt;&lt; N. We then use coordinate propagation to embed the new graphlets, which is carried </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Highly Discriminative and Low Redundancy Topologies</head><p>On the basis of the Grassmann manifold embedding, all the graphlets are transformed into d-dimensional feature vectors, and their distances calculated by Euclidean distance. We define a topology's discriminative power (T D) on the basis of this metric. Inspired by the definition of discrimination in linear discriminant analysis <ref type="bibr" target="#b20">[21]</ref>, T D computes the distance ratio between identical topological graphlets from different categories and those from the same category. Given a topology S, its T D is defined as follows: Motivated by the concept that high correlation leads to high redundancy, a topology should be abandoned if it has high correlation with another. Thus, the topology correlation (T C) measure is given as follows:</p><formula xml:id="formula_18">T D(S) = G∼S∧G∈I G ∼S ∧G ∈I d(y(G), y(G )) • σ (I, I ) G∼S∧G∈I G ∼S ∧G ∈I d(y(G), y(G )) • σ (I, I )<label>(11</label></formula><formula xml:id="formula_19">T C(S, S ) = 1 N S • N S G∼S∧G∈I G ∼S ∧G ∈I d(y(G), y(G )) (<label>12</label></formula><formula xml:id="formula_20">)</formula><p>where the denominator functions as a normalization factor, and N S and N S denote the number of training graphlets corresponding to topologies S and S respectively. On the basis of these two new measures for topologies, we propose a topology refinement algorithm to obtain the highly discriminative and low redundancy topologies, where the stepwise operations are described in Table <ref type="table" target="#tab_2">I</ref>. Give N the number of training RAGs and H the number of candidate topologies, the computational complexity involved in computing T D and T C are both O(N 2 ). As shown in Table <ref type="table" target="#tab_2">I</ref>, the toplogy refinement algorithm contains a double recursion and the time complexity is O(N 2 * H 2 ) in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. GRAPHLETS EXTRACTION AND QUANTIZATION INTO FEATURE VECTORS</head><p>As the selected topologies are both highly discriminative and have low redundancy, it is necessary to extract the corresponding graphlets for aerial image categorization. Toward an efficient feature extraction, we design a depth-first-search <ref type="bibr" target="#b21">[22]</ref> algorithm to find a topology's corresponding graphlets in an RAG. Formally, given a selected topology S and an RAG G, the proposed algorithm outputs the collection of graphlets with topology S.</p><p>As shown in Fig. <ref type="figure" target="#fig_8">8</ref>, the proposed graphlet extraction algorithm comprises three steps. First, we check whether topology S contains fewer vertices than RAG G. If the answer is yes, an iterative process is carried out to extract the corresponding graphlets. Otherwise, the algorithm is terminated (Fig. <ref type="figure" target="#fig_8">8(a)</ref>). Second, for each vertex in RAG G, we treat it as the reference point and match S to the topologies in RAG G. A depth-first-search strategy is employed to do the matching. Only graphlets with the same topology as S are qualified. We perform the matching by traversing all vertices in G, and collecting the qualified graphlets iteratively (Fig. <ref type="figure" target="#fig_8">8(b,</ref><ref type="figure">c</ref>)). Finally, a collection of graphlets are obtained by the proposed algorithm (Fig. <ref type="figure" target="#fig_8">8(d)</ref>). Note that, RAGs are graphs with low vertex degrees, thus the computational cost correlates to an approximately linear increase as the number of vertices increases.</p><p>As stated above, an aerial image can be represented by a set of graphlets. The extracted graphlets are notably planar visual features in R 2 . Unfortunately, conventional classifiers, such as SVM, can only handle 1D vector form features. Moreover, both the number and the size of the extracted graphlets are different from one aerial image to another. Thus, it would be impractical for a conventional classifier such as SVM to carry out classification directly based on the extracted graphlets. To tackle this problem, a quantizing method is integrated into our approach to transform the extracted graphlets into 1D vectors.</p><p>The proposed quantizing method is based on the distances between aerial images, which are computed based on the extracted graphlets. Given an aerial image, we first extract the graphlets that correspond to the selected topologies. The extracted graphlets are then converted into a vector A = [a 1 , a 2 , . . . , a F ], where each element of A is computed as follows:</p><formula xml:id="formula_21">a i ∝ exp ⎛ ⎜ ⎜ ⎝ - 1 N I S i • N I S i G∼S i G ∼S i d y(G), y(G ) ⎞ ⎟ ⎟ ⎠<label>(13)</label></formula><p>where N I S i and N I S i respectively denote the number of graphlets with topology S i from aerial image I and I , and F is the number of selected topologies. On the basis of the feature vector obtained above, a multiclass SVM is trained. That is, for the purpose of training aerial images from the p-th and the q-th classes, we construct the following binary SVM classifier:</p><formula xml:id="formula_22">max α∈R N pq W (α) = N pq i=1 α i - 1 2 N pq i=1 α i α j l i l j k(A i , A j ) s.t. 0 ≤ α i ≤ C, N pq i=1 α i l i = 0<label>(14)</label></formula><p>where A i ∈ R F is the quantized feature vector from the ith training RAG; l i is the class label (+1 for the p-th class and -1 for the q-th class) to the i -th training aerial image; α determines the hyper -plane to separate aerial images in the p-th class from those in the q-th class; C &gt; 0 trades the complexity of the machine off the number of nonseparable aerial images; and N pq is the number of training aerial images from both the p-th class and the q-th class. Given a quantized feature vector A ∈ R F obtained from a test RAG, its label ( p or q) is classified by: sgn</p><formula xml:id="formula_23">N pq i=1 l i α i k(A i , A) + b (<label>15</label></formula><formula xml:id="formula_24">)</formula><p>where the bias b = 1-</p><formula xml:id="formula_25">N pq i=1 l i α i k(A i , A s )</formula><p>and A s is a support vector with class label +1. During testing, classification is conducted C(C -1)/2 times and the voting rule is utilized to get the final decision. Each binary classification can be deemed to be a voting process wherein votes can be cast for A, and A is assigned to a class with maximum number of votes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTAL RESULTS AND ANALYSIS</head><p>In this section, we evaluate the efficacy of our proposed graphlet-guided aerial image categorization model. The first set of experiments are designed to show the effectiveness of the proposed graphlets in capturing the discriminative characteristics of aerial images. The second set of experiments evaluated the proposed method in comparison with representative graphbased and SPM-based recognition models. The third set of experiments did a step-by-step evaluation of each component in the proposed method. The final set of experiments determined the influence of the maximum graphlet size, the segmentation, and the threshold of T C on the performance of aerial image categorization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets and Experimental Platform</head><p>The experiments were conducted using two datasets. The first dataset contains aerial images from the Lotus Hill (LHI) data set <ref type="bibr" target="#b24">[25]</ref>, which consists of five categories, each of which contains 20 aerial images. Each aerial image comes with a benchmark segmentation map. The second dataset we compiled ourselves. It comprises ten categories, with each category containing as different number of aerial images. The details of our data set are as follows. Our aerial image dataset was collected by searching for aerial images from Google Earth. The entire dataset contains 20,964 aerial images in ten categories. Since the aerial images from cities are usually clearer than those from remote areas, we collected most of our aerial images from metropolises, such as New York, Tokyo, and Beijing. Due to the varying degree of difficulty involved in acquiring aerial images in the different categories, the number of aerial images in each category varied, as shown in Table <ref type="table" target="#tab_3">II</ref>. Some example images from our dataset are shown in Fig. <ref type="figure">9</ref>.</p><p>The experiment was conducted on a system equipped with an Intel E8500 CPU and 4GB RAM memory. The algorithm for aerial image category recognition was implemented on the Matlab 2010 platform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Visualized Mined Discriminative Graphlets</head><p>In this experiment, we evaluated the effectiveness of the post-embedding graphlets in capturing the discriminative characteristics of each aerial image. In particular, we experimented using the LHI data set, because its benchmark annotation intuitively demonstrates which spatial interactions among basic aerial image components are discriminative. The maximum graphlet size was set to five. It can be observed from Fig. <ref type="figure">10</ref> that our approach has the following advantages. First, graphlets capture the spatial interactions among aerial image components, which are essential for aerial image categorization. As shown in Fig. <ref type="figure">10(a)</ref>, the star-structurally arranged aerial image components are discriminative patterns for aerial images from the "intersection" category. The linearly arranged components are discriminative for aerial images from the "marine" category. In contrast, when using other models, these essential features are ignored. Second, compared with the SPM-based approaches, which only capture rough geometric context, Airport Comm. Indust. Interse. park Railway seaport soccer temple Univer. our proposed graphlets capture geometric context more accurately. Thus, our proposed approach is a more descriptive aerial image categorization model. Third, compared with most graph-based approaches, which are heavily data set dependent, our proposed graphlets lead to a more general model that can capture arbitrary topological characteristics among aerial image components, owing to the intrinsic description of graphs on relationships.</p><p>In Fig. <ref type="figure">10</ref>(f)-(j), we present the discriminative graphlets discovered in our previous work <ref type="bibr" target="#b33">[34]</ref>. It can be seen that, the discriminative graphlets mined by the newly proposed method have more consistent topologies in each category, while the topologies between categories differ significantly. Specifically, in our previous method, the discriminative graphlets from each category were with moderately different topologies, such as those from the "marine" category. The discriminative graphlets from different categories are sometimes have the same topology, such as those from the "intersection" category and the "residential" category. In the new method, the discriminative graphlets from each category have the same topology, except for a few from the "parking" category. Simultaneously, discriminative graphlets from different categories are obviously different, such as those from the "intersection" category and the "residential" category. This observation implies that, the patterns discovered by the new method are more effective for aerial image categorization, because an optimal categorization system should maximize the discrepancies of patterns between different categories while minimizing the discrepancies of patterns from the same category. The reason for the advantage of the newly proposed method is the fact that the topological structures are explicitly modeled in the new model, i.e., the adjacent matrix in <ref type="bibr" target="#b1">(2)</ref>, which are subsequently integrated with the global spatial layout and discrimination via the manifold embedding. Thus, the graphlets are mined more effectively for aerial image category recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison to the State of the Art</head><p>In our experiments, validation of the efficacy of our proposed approach was carried out using the LHI and our own compiled aerial image data set, respectively. We compared our mined graphlets with six representative visual descriptors: the walk and tree kernel <ref type="bibr" target="#b11">[12]</ref>, and four SPM-based generic object recognition models, namely, SPM <ref type="bibr" target="#b8">[9]</ref>, SC-SPM <ref type="bibr" target="#b26">[27]</ref>, LLC-SPM <ref type="bibr" target="#b25">[26]</ref>, and object bank-based SPM (OB-SPM). The parameter settings for the six methods compared were as follows. For the walk and tree kernel: Their sizes were tuned from one to 10 and the best recognition accuracies recorded. For SPM, SC-SPM, and LLC-SPM: We constructed a three level spatial pyramid; then extracted over one million SIFT descriptors from 16 × 16 patches computed over a grid with spacing of 8 pixels from all the training aerial images. Finally, a codebook was generated by k-means clustering on these SIFT descriptors. In our experiments, different codebook sizes, specifically, 256, 512, and 1024, were applied. For OB-SPM, we followed the setting utilized in the experiment conducted by Li et. al <ref type="bibr" target="#b32">[33]</ref>: The number of generic object detectors is fixed at 200, different regularizers LR1,LRG, and LRG1 were used, and it also has three SPM levels.</p><p>To compare the above approaches, on both datasets, we utilized 50% of the aerial images as training images and used the rest for testing. As shown in Tables III and IV, our approach outperformed the others. This is attributable to four reasons: 1) SPM, SC-SPM, and LLC-SPM are based on SIFT descriptors only, and so it is difficult for them to simultaneously incorporate multiple types of low-level visual descriptors. Thus, they ignore other important cues such as color distribution; 2) the object detectors in OB-SPM are trained for generic objects, they cannot effectively detect basic aerial image components; 3) none of the four SPM-based recognition models incorporate any discriminative spatial information; 4) compared to our proposed approach, both walk and tree kernel are less representative of graph-based descriptors, due to the inherent totter phenomenon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Step-by-Step Model Evaluation</head><p>In this experiment, we evaluated the effectiveness of each component in our proposed approach, i.e., multiple UFC-based aerial image segmentation, the entropy-based basic aerial image components filtration, manifold graphlet embedding, and graphlet quantization.</p><p>To evaluate aerial image recognition under different segmentation settings, we experimented using the LHI dataset   because it has segmentation ground truth. As shown in Fig. <ref type="figure" target="#fig_10">11</ref>, different segmentation settings were adopted in our evaluations, inclusive of benchmark segmentation, deficient segmentation, and over segmentation. Benchmark segmentation was carried out by manual annotation, which takes into account semantic understanding. As shown in Fig. <ref type="figure" target="#fig_11">12</ref>, the T D value of each aerial image corresponding to the different segmentation settings was computed to measure the discrimination of each graphlet. Benchmark segmentation achieved the largest T D value, 5.8, while the values for deficient and over segmentation were 4.9 and 5.3, respectively. The explanation of these results can be stated as follows: In contrast to deficient segmentation, more atomic regions are produced in over segmentation, which means it is rarer for one segmented region to span several basic aerial image components, and fewer discriminative objects are neglected. Moreover, it is unavoidable that the UFC-based segmentation is less accurate than the benchmark segmentation. We then compared the recognition accuracy according to benchmark segmentation, over segmentation, and deficient segmentation. As displayed in Table <ref type="table" target="#tab_6">V</ref>, over segmentation had an approximately 2% lower accuracy than that of benchmark segmentation on average. Deficient segmentation performed worse than over segmentation as it had the lowest accuracy. The overall recognition result is consistent with what the T D reflects in Fig. <ref type="figure" target="#fig_11">12</ref>. On the basis of the above discussion, we can conclude that both deficient and over segmentation in isolation cannot guarantee good segmentation results. To maximally preserve perfectly segmented regions, we therefore use multiple segmentations to generate basic aerial image components. As shown in the last column of Table V, the multiple segmentation scheme achieved the best recognition accuracy.</p><p>To evaluate the efficacy of the entropy-based basic aerial image components filtration, we tuned the entropy threshold from 0.1 to 3.7 (log 2 13) and recorded the corresponding recognition accuracy using the LHI. As shown in Fig. <ref type="figure" target="#fig_2">13</ref>, the recognition accuracy increased significantly as the entropy threshold was adjusted from 0.1 to 1.0, then decreased for entropy threshold from 1.0 to 2.9, and finally remained stable when the entropy threshold was greater than 2.9. The explanation for this observation is as follows: A smaller entropy threshold signifies a stricter atomic regions selection rule, resulting in fewer atomic regions for the construction of subsequent RAGs. Obviously, if too few atomic regions are used, unsatisfactory recognition performance will result. Conversely, a larger entropy threshold implies a more relaxed atomic region selection rule, and more atomic regions are used to construct the RAGs. Unfortunately, more atomic regions potentially influence recognition performance negatively because some atomic regions with no semantics are selected, which introduces noise into the aerial image category recognition process.</p><p>To evaluate the efficacy of the manifold graphlet embedding, we utilized three experimental settings. In the first setting, we removed the second term in <ref type="bibr" target="#b4">(5)</ref> and kept only the first term. In the second setting, we abandoned the first term in <ref type="bibr" target="#b4">(5)</ref> and kept only the second term. In the third setting, we replaced the proposed embedding algorithm with kernel principle component analysis (KPCA) <ref type="bibr" target="#b34">[35]</ref> and kernel locality preserving projection (KLDA) <ref type="bibr" target="#b20">[21]</ref>, respectively, where the kernel is computed by k(M, M ) = exp(-d GW (M, M )). We present the recognition accuracy under these various experimental settings in Fig. <ref type="figure">14</ref>. As can be seen, when the dimensionality of the post-embedding graphlets was tuned from one to five, the proposed embedding and the second embedding term outperform the other embedding schemes.</p><p>When the dimensionality of the post-embedding graphlets was tuned between five to 130, the proposed embedding scheme outperformed all the competitors. Further, in Fig. <ref type="figure" target="#fig_13">15</ref>, we visualized the post-embedding graphlets by projecting them onto two-dimensional subspace, under various different embedding algorithms. As expected, our proposed embedding algorithm performed much better than the others.</p><p>To evaluate the effectiveness of the proposed graphlet quantization, we compared it with the boosting graphlet integration framework in <ref type="bibr" target="#b33">[34]</ref>. We repeated our experiment 10 times and observed that the boosting-based graphlet integration achieved a recognition accuracy of 0.641 ± 0.051, while our proposed quantization achieved 0.667 ± 0.037. This shows the new approach significantly outperforms our previous work, in both average accuracy and stability.</p><p>To evaluate the effectiveness of the proposed graphlet quantization, we compared it with the boosting graphlet integration framework in <ref type="bibr" target="#b33">[34]</ref> and the non-topology-selection-based kernel in <ref type="bibr" target="#b41">[42]</ref>. We also repeated this experiment 10 times and observed that the boosting-based graphlet integration achieved a recognition accuracy of 0.641 ± 0.051, the non-topologyselection-based kernel achieved 0.632 ± 0.056 while the proposed quantization achieved 0.667 ± 0.037. This shows that the new approach significantly outperforms our previous work, in both average accuracy and stability. We also evaluated the normalization term and distance measure on the quantization equation ( <ref type="formula" target="#formula_21">13</ref>) and also reported each experiment 10 times. We first removed the normalization term and observed that the recognition accuracy was 0.546 ± 0.765, a significant decrease compared to the original performance, which indicates that the normalization term is indispensable in the quantization. Next, we replaced the Euclidean distance in <ref type="bibr" target="#b12">(13)</ref> with five different distance metrics <ref type="bibr" target="#b43">[44]</ref>: 1) Squared Euclidean, 2) Manhattan distance, 3) Canberra distance, 4) Cosine Correlation, and 5) Pearson correlation. The recognition accuracies for the five distances were respectively 0.612 ± 0.078, 0.632 ± 0.079, 0.589 ± 0.065, 0.457 ± 0.043, 0.566 ± 0.057. As can be seen, all five distance metrics performed worse than the Euclidean distance, which indicates that Euclidean distance is effective for the proposed graphlet quantization scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Results of Different Experimental Settings</head><p>In this section, we report on the performance of the proposed method under different experimental settings using the LHI <ref type="bibr" target="#b24">[25]</ref> dataset. First, we compared the UFC segmentation with four well-known unsupervised image segmentation methods: k-means <ref type="bibr" target="#b10">[11]</ref>, normalized cut <ref type="bibr" target="#b39">[40]</ref>, graph-based segmentation <ref type="bibr" target="#b40">[41]</ref>, and watershed <ref type="bibr" target="#b38">[39]</ref>. For each method, we performed segmentation five times and obtain approximately {10, 20, 30, 40, 50} segmented regions from each aerial image in the LHI. The corresponding recognition accuracies were respectively, 0.62 (k-means), 0.62 (normalized cut), 0.68 (graph-based segmentation), 0.64 (watershed segmentation), and 0.72 (UFC). The comparative experimental results clearly demonstrate the advantage of UFC. In addition, the recognition accuracies under two different sets of UFC parameters {0.14, 0.24, 0.34, 0.44, 0.54}, and {0.18, 0.28, 0.38, 0.48, 0.58} were both 0.72, the same as that of the original UFC parameter {0.1, 0.2, 0.3, 0.4, 0.5}, which indicates our proposed method is not sensitive to the UFC parameters.</p><p>The T D and T C thresholds are another two important parameters of concerned. In our implementation, the value of δ td was set to 1/C, where C is the number of categories, to obtain enough candidate discriminative topologies in advance. For the T C threshold, we evaluated its influence on the performance of our proposed method by setting different values. As shown in Fig. <ref type="figure" target="#fig_14">16</ref>(a), the influence of the value of δ tc is negligible when δ tc &lt; 0.3. Consequently, we choose δ tc = 0.25 in our experiment. Finally, we reported on the recognition accuracy under different maximun graphlet sizes, over the range one and 10 (this is because a very large number of graphlets is generated when maximum graphlet size is larger than 10, which will result in the system running out of memory). As shown in Fig. <ref type="figure" target="#fig_14">16(b)</ref>, recognition accuracy increased significantly as the maximum graphlet size increased from one to five, and consequently remained in the steady state. This demonstrates that five-sized graphlets are descriptive enough to capture the discrimination of aerial image categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In this paper, we presented a new visual descriptor for aerial image categorization that exploits both the local features and the geometric property of an aerial image. The descriptor defines a region adjacency graph to encode the geometric property and color/texture distribution of the aerial image. It then statistically mines the frequent topologies in RAGs that correspond to the training aerial images. Further, refined topologies are selected from the frequent ones toward being more discriminative and less redundant. Finally, given a new aerial image, its highly discriminative and low redundancy graphlets are extracted in accordance with the selected topologies, and further quantized into a feature vector for aerial image categories recognition. We experimentally confirmed the efficacy of our proposed approach using two different datasets.</p><p>In the future, toward a more descriptive aerial image descriptor, more constraints on pairwise spatially neighboring atomic regions, such as their relative sizes and spatial angle, will be incorporated into RAG. In addition, we plan to enrich our aerial image dataset and release it for public evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. APPENDIX</head><p>We used keywords such as "skyscraper" to google images and collected the images from the first five search result pages. Most of the areas in the returned images were closely related to the keywords. For each keyword, we manually selected 50 to 100 images as positive samples and another 200 images as negative samples (it is much easier to obtain negative samples than positive samples). Finally, we extracted the 137-dimensional feature vector for each image and trained a 13-class SVM classifier. Some sample images for the basic aerial image components are displayed in Fig. <ref type="figure" target="#fig_15">17</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Flow of our proposed approach (The blue arrows show the training stage, while the green arrows denote the test stage).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Workflow from an aerial image to its corresponding RAG. (a) Original aerial image. (b) Atomic regions. (c) Region connected graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. Illustration of spatially adjacent atomic regions: the top-left number within each grid denotes the cell's XY-coordinates. The warships on the left correspond to cells C 3 21 ,C 3 31 , and C 3 41 , thus they are spatially adjacent. (a) The original aerial image. (b) First level spatial pyramid. (c) Second level spatial pyramid. (d) Third level spatial pyramid.</figDesc><graphic coords="4,67.07,241.01,83.06,51.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. An example of graphlets with their corresponding topologies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Example of the preservation of pairwise graphlets' distances.</figDesc><graphic coords="5,172.55,58.61,113.66,65.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>d E (•, •) is the Euclidean distance; and l w and l b are two indicating functions. Denoting c(•) as the category of a graphlet, if c(</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>) where σ (I, I ) and σ (I, I ) are functions indicating whether aerial image I and I belong to the same category. More specifically, if I and I belong to different categories, then σ (I, I ) = 1, σ (I, I ) = 0, otherwise σ (I, I ) = 0, σ (I, I ) = 1. y(G) is the post-embedding graphlet G, and d E (•, •) denotes the Euclidean distance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Graphical illustration of graphlet extraction based on one selected topology.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .Fig. 10 .</head><label>910</label><figDesc>Fig. 9. Sample images from our aerial image data set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Segmented regions under various segmentation settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. T D value for various segmentation schemes.</figDesc><graphic coords="10,88.43,493.37,152.18,101.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 13 .Fig. 14 .</head><label>1314</label><figDesc>Fig. 13. Recognition accuracy for various entropy thresholds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Graphlets distributions in the selected two-dimensional subspace with various different subspace selection algorithms using LHI (we selected 400 graphelts from each category). (a) PCA (b) KPCA (Gaussian kemel, σ = 2.0). (c) KLDA (Gaussian kemel, σ = 2.0). (d) PM. (e) First term in PM. (f) Second time in PM.</figDesc><graphic coords="11,312.23,117.41,169.46,65.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. Top: Recognition accuracy versus the value of σ tc ; Bottom: Recognition accuracy versus the value of T . (a) The value of σ tc . (b) The maximum graphlet size T .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 17 .</head><label>17</label><figDesc>Fig. 17. Sample images from 13 categories of basic aerial image components.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Training aerial images with labels Training RAGs with labels Extracted Graphlets with labels QuanƟzed vectors</head><label></label><figDesc></figDesc><table><row><cell>A</cell><cell>t s e t</cell><cell>a</cell><cell>r e</cell><cell>a i</cell><cell>l</cell><cell>m i</cell><cell>a</cell><cell>e g</cell><cell>T</cell><cell>h</cell><cell>e</cell><cell>t s e t</cell><cell>G A R</cell><cell>a r t x E</cell><cell>d e t c</cell><cell>G</cell><cell>a r</cell><cell>p</cell><cell>h</cell><cell>s t e l</cell><cell>u Q</cell><cell>a</cell><cell>n</cell><cell>d e z i t</cell><cell>v</cell><cell>o t c e</cell><cell>r</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Highly discri. and low redund. topologies</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I TD</head><label>I</label><figDesc>AND TC BASED TOPOLOGY REFINEMENT out quickly based on the iterative algorithm proposed by Xiang et al.<ref type="bibr" target="#b30">[31]</ref>.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II NUMBER</head><label>II</label><figDesc>OF AERIAL IMAGES IN EACH CATEGORY ( THE FOLLOWING ABBREVIATIONS ARE USED. AIR: AIRPORT, RAIL.: RAILWAY, COMME.: COMMERCIAL, INTER.: INTERSECTION, TEMP.: TEMPLATE, UNIV.: UNIVERSITY.)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III RECOGNITION</head><label>III</label><figDesc>RATE WITH STANDARD DEVIATION USING OUR OWN DATA SET (THE EXPERIMENT WAS REPEATED 10 TIMES; HC MEANS HOG + COLOR MOMENT WITH 1024-SIZED CODEBOOK.)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV RECOGNITION</head><label>IV</label><figDesc>RATE WITH STANDARD DEVIATION USING THE LHI (THE EXPERIMENT WAS REPEATED 10 TIMES; HC MEANS HOG + COLOR MOMENT WITH 1024-SIZED CODEBOOK.)</figDesc><table><row><cell>Original image</cell><cell>Benchmark</cell><cell>Deficiently segmented Overly segmented</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V RECOGNITION</head><label>V</label><figDesc>ACCURACY UNDER VARIOUS SEGMENTATION SCHEMES</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Imperfection means that some segmented regions partially cover one or more semantic components.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>For convenience, a highly discriminative and low redundancy topology means that graphlets that have this topology are highly discriminative and have low redundancy.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Natural Science Foundation of China under Grant 61170142, in part by the National Program on the Key Basic Research Project (973 Program) under Grant 2013CB329301, in part by the National Key Technology R&amp;D Program under Grant 2011BAG05B04, in part by the International Science &amp; Technology Cooperation Program of China under Grant 2013DFG12840, in part by the the Fundamental Research Funds for the Central Universities, in part by the NSFC under Grants 61202166, 61222210, and 61128007, in part by the Doctoral Fund of the Ministry of Education of China under Grant 20120032120042, in part by ARO under Grant W911BF-12-1-0057, in part by the NSF IIS under Grant 1052851, in part by the Faculty Research Awards by Google, in part by FXPAL, in part by the NEC Laboratories of America, and in part by the 2012 UTSA START-R Research Award respectively. The associate editor coordinating the review of this manuscript and approving it for publication was Prof. Joseph P. Havlicek.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Support vector machines and kernel methods: The new generation of learning machines</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Mag</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="31" to="41" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004-11">Nov. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Web and personal image annotation by mining label correlation with relaxed visual graph embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1339" to="1351" />
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SNMFCA: Supervised NMFbased image classification and annotation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4508" to="4521" />
			<date type="published" when="2012-11">Nov. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast semantic diffusion for large-scale context-based image and video annotation</title>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3080" to="3091" />
			<date type="published" when="2012-06">Jun. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Vehicle detection in aerial surveillance using dynamic bayesian networks</title>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2152" to="2159" />
			<date type="published" when="2012-04">Apr. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Integrating graph partitioning and matching for trajectory analysis in video surveillance</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4844" to="4857" />
			<date type="published" when="2012-12">Dec. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Vision-based motion planning of a pneumatic robot using a topology representing neural network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zeller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schulten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISIC</title>
		<meeting>ISIC</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Modeling scenes with local descriptors and latent aspects</title>
		<author>
			<persName><forename type="first">P</forename><surname>Quelhas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Monay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Odobez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gaticaperez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th IEEE ICCV</title>
		<meeting>10th IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2005-10">Oct. 2005</date>
			<biblScope unit="page" from="883" to="890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Data Mining Methods for Knowledge Discovery</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Cios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pedrycz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Swiniarski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998-08">Aug. 1998</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image classification with segmentation graph kernels</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2007-06">Jun. 2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient graphlet kernels for large graph comparison</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf</title>
		<meeting>Int. Conf</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="488" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generic model abstraction from examples</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Keselman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dickinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1141" to="1156" />
			<date type="published" when="2005-07">Jul. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Object recognition as many-to-many feature matching</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Demirci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shokoufandeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Keselman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bretzner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dickinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="222" />
			<date type="published" when="2006-08">Aug. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pictorial structures for object recognition</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="79" />
			<date type="published" when="2005-01">Jan. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Object-graphs for context-aware category discovery</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. CVPR</title>
		<meeting>IEEE Conf. CVPR</meeting>
		<imprint>
			<date type="published" when="2009-06">Jun. 2009</date>
			<biblScope unit="page" from="346" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Woods</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eddins</surname></persName>
		</author>
		<title level="m">Digital Image Processing Using Matlab</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="2003-12">Dec. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An algorithm for subgraph isomorphism</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Ullmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="42" />
			<date type="published" when="1976-01">Jan. 1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An efficient algorithm for discovering frequent subgraphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kuramochi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1038" to="1051" />
			<date type="published" when="2004-09">Sep. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Max-min distance analysis by using sequential SDP relaxation for dimension reduction</title>
		<author>
			<persName><forename type="first">W</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1037" to="1050" />
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Cormen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Leiserson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Rivest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stein</surname></persName>
		</author>
		<title level="m">Introduction to Algorithms</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="540" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Towards an unsupervised optimal fuzzy clustering algorithm for image database organization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th ICPR</title>
		<meeting>15th ICPR</meeting>
		<imprint>
			<date type="published" when="2000-09">Sep. 2000</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="897" to="900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Subspaces indexing model on Grassmann manifold for image search</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2627" to="2635" />
			<date type="published" when="2011-09">Sep. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Introduction to a large scale general purpose ground truth dataset: Methodology, annotation tool, and benchmarks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th Int. Conf. EMMCVPR</title>
		<meeting>6th Int. Conf. EMMCVPR</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="169" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Localityconstrained linear coding for image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. CVPR</title>
		<meeting>IEEE Conf. CVPR</meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="3360" to="3367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Linear spatial pyramid matching using sparse coding for image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. CVPR</title>
		<meeting>IEEE Conf. CVPR</meeting>
		<imprint>
			<date type="published" when="2009-06">Jun. 2009</date>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient sparse coding algorithms</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Battle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="801" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Similarity of color images</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Stricker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Orengo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IS&amp;T/SPIE&apos;s Symp</title>
		<meeting>IS&amp;T/SPIE&apos;s Symp</meeting>
		<imprint>
			<date type="published" when="1995-03">Mar. 1995</date>
			<biblScope unit="page" from="381" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2005-06">Jun. 2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Embedding new data points for manifold learning via coordinate propagation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="159" to="184" />
			<date type="published" when="2009-05">May 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Large Margin Classifiers</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="61" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Object bank: A highlevel image representation for scene classification and semantic feature sparsification</title>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1378" to="1386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Integrating local features into discriminative graphlets for scene classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICONIP</title>
		<meeting>ICONIP</meeting>
		<imprint>
			<date type="published" when="2011-11">Nov. 2011</date>
			<biblScope unit="page" from="657" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Face recognition using laplacianfaces</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="328" to="340" />
			<date type="published" when="2005-03">Mar. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multi-layer orthogonal visual codebook for image classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2312" to="2315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Spatial pooling of heterogeneous features for image applications</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 20th ACM Int. Conf. Multimedia</title>
		<meeting>20th ACM Int. Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="539" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A graph-matching kernel for object categorization</title>
		<author>
			<persName><forename type="first">O</forename><surname>Duchenne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2011-11">Nov. 2011</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hierarchies of partitions and morphological segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Meyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scale-Space and Morphology in Computer Vision</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="161" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000-08">Aug. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Efficient graph-based image segmentation</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="181" />
			<date type="published" when="2004-09">Sep. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Spatial graphlet matching kernel for recognizing aerial image categories</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st ICPR</title>
		<meeting>1st ICPR</meeting>
		<imprint>
			<date type="published" when="2012-11">Nov. 2012</date>
			<biblScope unit="page" from="2813" to="2816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Efficient graphlet kernels for large graph comparison</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf</title>
		<meeting>Int. Conf</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="488" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Comprehensive survey on distance/similarity measures between probability density functions</title>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Cha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Math. Models Methods Appl. Sci</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="300" to="307" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A hierarchical connection graph algorithm for gable-roof detection in aerial image</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="177" to="181" />
			<date type="published" when="2011-01">Jan. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A hierarchical and contextual model for aerial image understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Porway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2008-06">Jun. 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Object categorization with sketch representation and generalized samples</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chaoa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3648" to="3660" />
			<date type="published" when="2012-10">Oct. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A stochastic graph grammar for compositional object representation and recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Porway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1297" to="1307" />
			<date type="published" when="2009-07">Jul. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Semi-supervised node splitting for random forest construction</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
