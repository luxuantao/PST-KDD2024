<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Neural Networks for Multimodal Single-Cell Data Integration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hongzhi</forename><surname>Wen</surname></persName>
							<email>wenhongz@msu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Jiayuan</forename><surname>Ding</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
							<email>jinwei2@msu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Yuying</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Michigan State University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Michigan State University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Michigan State University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Michigan State University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Michigan State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Graph Neural Networks for Multimodal Single-Cell Data Integration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances in multimodal single-cell technologies have enabled simultaneous acquisitions of multiple omics data from the same cell, providing deeper insights into cellular states and dynamics. However, it is challenging to learn the joint representations from the multimodal data, model the relationship between modalities, and, more importantly, incorporate the vast amount of single-modality datasets into the downstream analyses. To address these challenges and correspondingly facilitate multimodal singlecell data analyses, three key tasks have been introduced: Modality prediction, Modality matching and Joint embedding. In this work, we present a general Graph Neural Network framework scMoGNN to tackle these three tasks and show that scMoGNN demonstrates superior results in all three tasks compared with the state-of-theart and conventional approaches. Our method is an official winner in the overall ranking of Modality prediction from NeurIPS 2021 Competition 1 . * indicates equal contribution to this research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The rapid advance of single-cell technologies makes it possible to simultaneously measure multiple molecular features at multiple modalities in a cell, such as gene expressions, protein abundance and chromatin accessibility. For instance, CITE-seq (cellular indexing of transcriptomes and epitopes by sequencing) <ref type="bibr" target="#b28">[29]</ref> enables simultaneous quantification of mRNA expression and surface proteins abundance; methods like sci-CAR <ref type="bibr" target="#b3">[4]</ref>, Paired-seq <ref type="bibr" target="#b36">[37]</ref>, and SNARE-seq <ref type="bibr" target="#b4">[5]</ref> enable joint profiling of mRNA expression and chromatin accessibility ( i.e. genome-wide DNA accessibility). The joint measurements from these methods provide unprecedented multimodal data for single cells, which has given rise to valuable insights for not only the relationship between different modalities but, more importantly, a holistic understanding of the cellular system.</p><p>Despite the emergence of joint platforms, single-modality datasets are still far more prevalent. How to effectively utilize complementary information from multimodal data to investigate cellular states and dynamics and to incorporate the vast amount of single-modality data while leveraging the multimodal data pose great challenges in single-cell genomics. To address these challenges, et al. <ref type="bibr" target="#b7">[8]</ref> summarized three major tasks: <ref type="bibr" target="#b0">(1)</ref> Modality prediction aims at predicting the features of one modality from the features of another modality <ref type="bibr" target="#b33">[34]</ref>; <ref type="bibr" target="#b1">(2)</ref> Modality matching focuses on identifying the correspondence of cells between different modalities <ref type="bibr" target="#b32">[33]</ref>; and (3) Joint embedding requires embedding the features of two modalities into the same low-dimensional space <ref type="bibr" target="#b28">[29]</ref>. The motivation of modality prediction and modality matching is to better integrate existing single-modality datasets, while joint embedding can provide more meaningful representations of cellular states from different types of measurements. In light of these benefits, computational biologists recently organized a competition for multimodal single-cell data integration at NeurIPS 2021 <ref type="bibr" target="#b7">[8]</ref> to benchmark these three tasks and facilitate the computational biology communities.</p><p>There is an emerging trend to leverage deep learning techniques to tackle the tasks mentioned above for multimodal single-cell data. BABEL <ref type="bibr" target="#b33">[34]</ref> translated between the transcriptome (mRNA) and chromatin (DNA) profiles of a single cell based on an encoderdecoder architecture; scMM <ref type="bibr" target="#b19">[20]</ref> implemented a mixture-of-experts deep generative model for joint embedding learning and modality prediction. Cobolt <ref type="bibr" target="#b9">[10]</ref> acquired joint embedding via a variant of Multimodal Variational Autoencoder (MVAE <ref type="bibr" target="#b35">[36]</ref>). MOFA2 <ref type="bibr" target="#b0">[1]</ref> used Bayesian group factor analysis to reduce dimensions of multimodality data and generate a low-dimensional joint representation. However, most of these approaches treat each cell as a separate input without considering possible high-order interactions among cells or different modalities. Such higher-order information can be essential for learning with high-dimensional and sparse cell features, which are common in single-cell data. Take the joint embedding task for example, the feature dimensions for GEX (mRNA) and ATAC (DNA) data are as high as 13,431 and 116,490, respectively; however, only 9.75% of GEX and 2.9% of ATAC features are nonzero on average over 42, 492 training samples (cells). Furthermore, integrated measuring often requires additional processing to cells, which can lead to extra noise and drop-out in the resulting data <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">19]</ref>. Therefore, it is desired for techniques that can mitigate the negative impact of such noise.</p><p>Recently, the advances of graph neural networks (GNNs) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b34">35]</ref> pave the way for addressing the aforementioned issues in single-cell data integration. Specifically, GNNs aggregate information from neighborhoods to update node embeddings iteratively <ref type="bibr" target="#b8">[9]</ref>. Thus, the node embedding can eventually encode high-order structural information through multiple aggregation layers. In addition, GNNs smooth the features by aggregating neighbors' embedding, which provides an extra denoising mechanism <ref type="bibr" target="#b16">[17]</ref>. Hence, by modeling the interactions between cells and their features as a graph, we can adopt GNNs to exploit the structural information and tackle the limitations of previous techniques for single-cell data integration. With the constructed graph, we can readily incorporate external knowledge (e.g., interactions between genes) into the graph to serve as additional structural information. Moreover, it enables a transductive learning paradigm with GNNs to gain additional semi-supervised signals to enhance representation learning.</p><p>Given those advantages, we aim to design a GNN framework for multimodal data integration. While several existing works attempted to introduce graph neural networks to single cell analysis <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32]</ref>, none of them tackle the challenging problem of multimodal data integration which requires handling different modalities simultaneously. Therefore, we aim to develop GNN methods for the tasks in multimodal data integration, especially for modality prediction, modality matching and joint embedding. Specifically, we propose a general framework scMoGNN for modeling interactions of modalities and leveraging GNNs in single-cell analysis 2 . Our framework is highly versatile: we demonstrate its use cases in the three different multimodal tasks. To the best of our knowledge, we are the first to develop a GNN framework in this emerging research topic, i.e., multimodal single-cell data integration. Our proposed framework achieves the best results in all of these three tasks on the benchmark datasets, providing a very strong baseline for follow-up research. Our contributions can be summarized as follows:</p><p>(1) We study the problem of multimodal single-cell data integration and propose a general GNN-based framework scMoGNN to capture the high-order structural information between cells and modalities. (2) The proposed general framework is highly flexible as it can be adopted in different multimodal single-cell tasks. (3) Our framework achieves remarkable performance across tasks.</p><p>It has won the first place in the modality prediction task in the Multimodal Single-Cell Data Integration competition, and currently outperforms all models for all three tasks on the leaderboard 3 . All of our results are based on publicly available data and are reproducible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we briefly introduce works related to our work including GNNs on single-modality data and multimodal data integration.</p><p>GNNs on Single-Modality Data. Graphs occur as a natural representation of single-cell data both as feature-centric (RNAs, DNAs, or proteins) and cell-centric. Thus, a few recent works have applied GNNs to the single-cell data. Song et al. <ref type="bibr" target="#b26">[27]</ref> propose scGCN model for knowledge transfer in single-cell omics (mRNA or DNA) based on Graph Convolutional Networks <ref type="bibr" target="#b11">[12]</ref>. scGNN <ref type="bibr" target="#b31">[32]</ref> formulates and aggregates cell-cell relationships with Graph Neural Networks for missing-data imputation and cell clustering using single-cell RNA sequencing (scRNA-seq) data. scDeepSort <ref type="bibr" target="#b23">[24]</ref> is a pre-trained cell-type annotation tool for scRNA-seq data that utilizes a deep learning model with a weighted GNN. Similar to our proposed model, scDeepSort also relies on feature-cell graphs. However, it does not incorporate any prior knowledge into GNNs. Using spatial transcriptomics (mRNA) data, DSTG <ref type="bibr" target="#b25">[26]</ref> utilizes semi-supervised 2 Our solution won the first place of the modality prediction task in the Multimodal Single-Cell Data Integration competition at NeurIPS 2021. 3 https://eval.ai/web/challenges/challenge-page/1111/leaderboard/2860 GCN to deconvolute the relative abundance of different cell types at each spatial spot. Despite its success on single-modality data, there are few efforts on applying GNNs to multimodal single-cell data.</p><p>Multimodal Data Integration. Most of the prior works in multimodal data integration can be divided into 1) matrix factorization <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b27">28]</ref> or statistical based methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33]</ref> and 2) autoencoder based methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b33">34]</ref>. Specifically, BABEl <ref type="bibr" target="#b33">[34]</ref> leverages autoencoder frameworks with two encoders and two decoders to take only one of these modalities and infer the other by constructing reconstruction loss and cross-modality loss. Cobolt <ref type="bibr" target="#b9">[10]</ref> acquires joint embedding via a variant of Multimodal Variational Autoencoder (MVAE <ref type="bibr" target="#b35">[36]</ref>). Unlike our proposed models, these aforementioned methods are unable to incorporate high-order interactions among cells or different modalities. To the best of our knowledge, we are the first to apply GNNs in the field of multimodal single-cell data integration and build a GNNs-based general framework to broadly work on these three key tasks from NeurIPS 2021 Competition<ref type="foot" target="#foot_0">4</ref> .</p><p>Our framework officially won the first place in the overall ranking of the modality prediction task. After the competition, we extended our framework to the other two tasks and achieved superior performance compared with the top winning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM STATEMENT</head><p>Before we present the problem statement, we first introduce the notations used in this paper. There are three modalities spanning through each task. They are GEX as mRNA data, ATAC as DNA data and ADT as protein data. Each modality is initially represented by a matrix M ? R ? ?? where ? indicates the number of cells, and ? denotes the feature dimension for each cell. In our work, we later construct a bipartite graph G = (U, V, E) based on each modality M, where U is the set of ? cell nodes {? 1 , ? 2 , ..., ? ? } and V is the set of ? feature nodes {? 1 , ? 2 , ..., ? ? }.</p><p>With the aforementioned notations, the problem of learning GNNs for single-cell data integration is formally defined as, Given a modality M ? R ? ?? , we aim at learning a mapping function ? ? which maps M to the space of downstream tasks.</p><p>In the following, we formally define these three key tasks of single-cell data integration: modality prediction, modality matching and joint embedding. We will also define the corresponding evaluation metrics for each task. Note that these metrics are also adopted by the competition to decide the top winners.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task 1: Modality Prediction</head><p>In this task, given one modality (like GEX), the goal is to predict the other (like ATAC) for all feature values in each cell. It can be formally defined as, Given a source modality M 1 ? R ? ??1 , the goal is to predict a target modality M 2 ? R ? ??2 via learning a mapping function ? ? parameterized by ? such that M 2 = ? ? (M 1 ).</p><p>Possible modality pairs of (M 1 , M 2 ) are (GEX, ATAC), (ATAC, GEX), (GEX, ADT) and (ADT, GEX), which correspond to four sub-tasks in Task 1. Root Mean Square Error<ref type="foot" target="#foot_1">5</ref> is used to quantify performance between observed and predicted feature values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Task 2: Modality Matching</head><p>The goal of this task is to identify the correspondence between two single-cell profiles and provide the probability distribution of these predictions. It can be formally defined as, Given modality M 1 ? R ? ??1 and modality M 2 ? R ? ??2 , we aim to learn two mapping functions ? ? 1 parameterized by ? 1 and ? ? 2 parameterized by ? 2 to map them into the same space such that</p><formula xml:id="formula_0">S = ?(? ? 1 (M 1 ), ? ? 2 (M 2 )) (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where ? is a score function to calculate probability distribution of correspondence predictions. S ? R ? ?? is an output score matrix with each row summing to 1. S ij is the correspondence probability between ?-th cell from modality M 1 and ?-th cell from modality M 2 .</p><p>Possible modality pairs of (M 1 , M 2 ) are (GEX, ATAC), (ATAC, GEX), (GEX, ADT) and (ADT, GEX), which correspond to four subtasks in Task 2. The sum of weights in the correct correspondences of S is used as final score to quantify prediction performance using</p><formula xml:id="formula_2">????? = ? ?=1 ? ?=1 S ?,? if ? = ?.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Task 3: Joint Embedding</head><p>In this task, the goal is to learn an embedded representation that leverages the information of two modalities. The quality of the embedding will be evaluated using a variety of criteria generated from expert annotation. It can be formally defined as, Given modality M 1 ? R ? ??1 and modality M 2 ? R ? ??2 , we aim to learn three mapping functions ? ? 1 , ? ? 2 and ? ? 3 parameterized by ? 1 , ? 2 and ? 3 accordingly to project them into downstream tasks,</p><formula xml:id="formula_3">H = ? ? 3 ?????? (? ? 1 (M 1 ), ? ? 2 (M 2 ))<label>(2)</label></formula><p>where ? ? 1 (M 1 ) ? R ? ??1 ? and ? ? 2 (M 2 ) ? R ? ??2 ? correspond to new representations learned from modality M 1 and M 2 separately, H ? R ? ??3 is a final output embedding learned through ? ? 3 on concatenation of ? ? 1 (M 1 ) and ? ? 2 (M 2 ).</p><p>H will be measured using six different metrics broken into two classes biology conservation and batch removal. Biology conservation metrics include "NMI cluster/label", "Cell type ASW", "Cell cycle conservation" and "Trajectory conservation" which aim to measure how well an embedding conserves expert-annotated biology. Batch removal metrics include "Batch ASW" and "Graph connectivity" which are to evaluate how well an embedding removes batch variation. Please refer to the Appendix A for more details about these metrics description. Possible modality pairs of (M 1 , M 2 ) are (GEX, ATAC) and (ADT, GEX), which correspond to two sub-tasks in Task 3.</p><p>In this work, we instantiate ? ? as a graph neural network model by first constructing a bipartite graph G = (U, V, E) based on modality M, and then learning better cell node representation through message passing on graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHOD</head><p>In this section, we first introduce the proposed general framework scMoGNN for multimodal data integration. Then we introduce how to adapt scMoGNN to advance three tasks: modality prediction, modality matching and joint embedding. An illustration of our framework is shown in Figure <ref type="figure" target="#fig_0">1</ref>. Specifically, our framework can be divided into three stages: graph construction, cell-feature graph convolution, and task-specific head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">A General GNN Framework</head><p>To develop a GNN-based framework for single cell data integration, we are essentially faced with the following challenges: (1) how to construct the graph for cells and its features (or modalities); (2) how to effectively extract meaningful patterns from the graph; and (3) how to adapt the framework to different multimodal tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Graph Construction.</head><p>With the single-cell data, our first step is to construct a cell-feature graph that GNNs can be applied to. We construct a cell-feature bipartite graph where the cells and their biological features (e.g. GEX, ADT or ATAC features) are treated as different nodes, which we term as cell nodes and feature nodes, respectively. A cell node is connected with the feature nodes that represent its features. With such graph structure, the advantage is that cell nodes can propagate information to its neighboring feature nodes, while feature nodes can also propagate their information to cell nodes.</p><p>Formally, we denote the bipartite graph as G = (U, V, E). In this graph, U is the set of ? cell nodes {? 1 , ? 2 , ..., ? ? } and V is the set of ? feature nodes {? 1 , ? 2 , ..., ? ? }, where each feature node refers to one feature dimension of the input data. E ? U ? V represents the set of edges between U and V, which describe the relations between cells and features. The graph can be denoted as a weighted adjacency matrix</p><formula xml:id="formula_4">A = O M M ? O ? R (? +?)?(? +?) ,<label>(3)</label></formula><p>where O is a matrix filled with constant 0 and M ? R ? ?? is the input feature matrix of cells. M can be also viewed as the features of one modality such as GEX. Note that A is a bipartite graph where two nodes within the same set (either feature nodes or cell nodes) are not adjacent. Based on the aforementioned process of graph construction, we can adjust it for specific tasks, e.g., incorporating a prior knowledge graph of genes, which can change the bipartite characteristic of A. Furthermore, since GNNs are mostly dealing with attributed graphs, we need to assign initial embeddings for feature and cell nodes. Specifically, we use X cell and X feat to denote the initial embeddings for cell and feature nodes, respectively. We have X cell ? R ? ?? ? and X feat ? R ??? ?? where ? ? and ? ?? are determined by the task-specific settings. As an illustrative example, the initial embeddings of feature nodes {? 1 , ..., ? ? } could be the one-hot index of each feature dimension; thus, X feat ? R ??? is an identity matrix, i.e., X feat = I ? . Meanwhile, we do not have any prior knowledge for each cell, thus, X cell = O ? ?1 . Together with the node features, the constructed cell-feature graph can be denoted as G = (A, X feat , X cell ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Cell-Feature Graph Convolution.</head><p>After we obtain the constructed cell-feature graph, the next challenge is how to extract high-order structural information from the graph. In this work, we 0 Features Nodes</p><formula xml:id="formula_5">0 ? 0 0 0 ? 0 ? ? ? ? 0 0 ? 0 ? ? Cell Nodes Cell Nodes 0 ? 0 ? ? ? ? ? ? 0 1 Features Cells 0 ? 0 0 0 ? 2 ? ? ? ? 2 0 ? 3 ? ? 0 Features 0.1 ? 0.3 0.1 0 ? 0.2 ? ? ? ? 0.3 0.2 ? 0 ? ? Features Modality Input Enhanced Components (optional) 0.1 0.3 0.1 0.2 0.3 0.2 Adjacent Matrix 1 0 ? 0 0 0 ? 2 ? ? ? ? 2 0 ? 3 ? 1 Feature Nodes 0 ? 2 0 0 ? 0 ? ? ? ? 0 2 ? 3 ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Construction</head><p>Cell Nodes Feature Nodes We first construct the cell-feature graph from given modality and then perform cell-feature graph convolution to obtain latent embeddings of cells, which are sent to a task-specific head to perform the downstream task.</p><p>utilize GNNs to effectively learn cell/feature representations over the constructed graph. Note that there are two types of nodes in the graph and we need to deal with them differently. For the ease of illustration, we first only consider one type of nodes (e.g., feature nodes V), and later we extend it to the whole graph. Typically, GNNs follow the message-passing paradigm <ref type="bibr" target="#b8">[9]</ref> and in each layer of GNNs, the embedding of each node is updated according to the messages passed from its neighbors. Let</p><formula xml:id="formula_6">H ? = {h ? 1 , ..., h ? ? }, h ? ? ? R ? ?</formula><p>be the input node embeddings in the ?-th layer, where h ? ? corresponds to node ? ? . Hence, the output embeddings of the ?-th layer can be expressed as follows:</p><formula xml:id="formula_7">h ?+1 ? = Update(h ? ? , Agg(h ? ? | ? ? N ? )),<label>(4)</label></formula><p>where N ? is the set of first-order neighbors of node ? ? , Agg(?) indicates an aggregation function on neighbor nodes' embedding, and Update(?) is an update function that generates a new node embedding vector from the previous one and aggregation results from neighbors. Notably, for the input node embedding in the first layer, we have</p><formula xml:id="formula_8">H 1 = ? (X feat W feat + b feat ), W feat ? R ? ? ??<label>(5)</label></formula><p>where W feat is a transformation matrix, ? is the dimension of the hidden space and ? is an activation function.</p><p>Though there exist a number of different GNN models, in this work, we focus on the most representative one, Graph Convolution Network (GCN) <ref type="bibr" target="#b11">[12]</ref>. Note that it is straightforward to extend the proposed framework to other GNN models. Considering that we have two different types of nodes in the graph (i.e., cells and features nodes), we make some modifications on the vanilla GCN to deal with different types of nodes/edges. We name it as cell-feature graph convolution, where we separately perform the aggregation function on different types of edges to capture interactions between cell and feature nodes. Moreover, we use different parameters for aggregation on different edges, thus allowing the embedding of each node type to have very different distributions. Specifically, from a message passing perspective, the operation in one cell-feature graph convolution layer can be expressed as two steps, i.e., aggregation and updating. In order to generate a message m for different types of nodes, there are at least two basic aggregation functions, one is:</p><formula xml:id="formula_9">m ?,? U?V = ? (b ? U?V + ?? ? ?N ? ,? ? ?V ? ?? ? ?? h ? ? W ? U?V )<label>(6)</label></formula><p>where ? corresponds to node ? ? ? V, ? corresponds to node ? ? ? U; ? ?? denotes the edge weight between ? ? and ? ? , W ? U?V and b ? U?V are trainable parameters, ? (?) is an activation function such as ReLU, and ? ?? is a normalization term defined as follows:</p><formula xml:id="formula_10">? ?? = ?? ?? ? ?N ? ? ?? ?? ?? ? ?N ? ? ??<label>(7)</label></formula><p>Obviously, Eq. ( <ref type="formula" target="#formula_9">6</ref>) is the aggregation function from cell nodes U to feature nodes V. Thus the other aggregation function from V to U can be written as:</p><formula xml:id="formula_11">m ?,? V?U = ? (b ? V?U + ?? ? ?N ? ,? ? ?U ? ?? ? ?? h ? ? W ? V?U )<label>(8)</label></formula><p>The transformation matrices W ? U?V and W ? V?U project the node embeddings from one hidden space to another vice versa. After generating the messages from neighborhoods, we then update the embedding for nodes in V and U accordingly:</p><formula xml:id="formula_12">h ?+1 ? = h ? ? + m ?,? U?V , h ?+1 ? = h ? ? + m ?,? V?U ,<label>(9)</label></formula><p>where ? ? ? V and ? ? ? U. In Eq. ( <ref type="formula" target="#formula_12">9</ref>), we adopt a simple residual mechanism in order to enhance self information. As mentioned earlier, we can have more than two types of edges depending on the downstream task and the graph structure can be much more complex than a bipartite graph. Despite such complexity, our proposed framework and cell-feature graph convolution have the capacity to handle more types of edges/nodes. We will introduce these details in Section 4.2.</p><p>4.1.3 Task-specific Head. After we learn node embeddings for feature and cell nodes, we need to project the embedding to the space of the specific downstream task. Hence, we design a task-specific head, which depends on the downstream task. Specifically, we first take the node embeddings of cell nodes from each convolution layer, aggregate them and project them into the space of downstream task ?:</p><formula xml:id="formula_13">? = Head Readout ? (H 1 U , ..., H ? U )<label>(10)</label></formula><p>where H ? U refers to embeddings of all cell nodes in ?-th layer, Readout(?) ? is a trainable aggregation function, Head(?) is a linear transformation that projects the latent embedding to the downstream task space. With the obtained output, we can then optimize the framework through minimizing the task-specific loss functions. In the following subsection, we give the details of training the general framework for different tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Specifications</head><p>With the proposed general framework scMoGNN , we are now able to perform different tasks by adjusting some of the components. In this subsection, we show how scMoGNN is applied in the three important tasks in single-cell data integration: modality prediction, modality matching and joint embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Modality Prediction.</head><p>In the modality prediction task, our objective is to translate the data from one modality to another. Given the flexibility of graph construction and GNNs in our framework, we can readily incorporate external knowledge into our method. In this task, we adjust the graph construction to include such domain knowledge to enhance the feature information. Specifically, in GEXto-ADT and GEX-to-ATAC subtasks, we introduce hallmark gene sets <ref type="bibr" target="#b13">[14]</ref> (i.e., pathway data) from the Molecular Signatures Database (MSigDB) <ref type="bibr" target="#b30">[31]</ref>. The pathway data describe the correlations between gene features (i.e., features of GEX data) and the dataset consists of 50 so-called gene sets. In each gene set, a group of correlated genes are collected. However, there is no numerical information in these gene sets to help us quantify the relations between those genes, resulted in homogeneous relations. Intuitively, we can construct a fully-connected inter-gene graph for each gene set, and incorporate those edges into our original graph G. Hence, the new adjacency matrix for G is:</p><formula xml:id="formula_14">A = O M M ? P<label>(11)</label></formula><p>where P ? R ??? is a symmetric matrix, which refers to the links between gene features, generated from gene sets data. Furthermore, we manually add some quantitative information, i.e., cosine similarity between gene features based on their distributions in GEX input data.</p><p>Due to the existence of extra type of edges in the graph (i.e., edges among feature nodes), we need to make corresponding adjustment on our cell-feature graph convolution. Taking an arbitrary feature node ? ? as example, we have</p><formula xml:id="formula_15">N ? = N u ? ? N v ? ,</formula><p>where N ? denotes the set of neighbors of node ? ? , N u ? ? U is the set of cell node neighbors of ? ? , N v ? ? V is the set of feature node neighbors of ? ? . Since cell and feature nodes have very different characteristics, when we aggregate the embedding from N u ? and N v ? respectively, we expect to get very different results. Thus, when we update the embedding of center node, we have different strategies to combine messages from different channels. As a starting point, we decide to enable a scalar weight. Formally speaking, similar to Eq. ( <ref type="formula" target="#formula_9">6</ref>) and Eq. ( <ref type="formula" target="#formula_11">8</ref>), we first generate two messages m ?,? U?V and m ?,?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V?V</head><p>for each node ? ? ? V, then we update the node embedding of ? ? following the formulation below:</p><formula xml:id="formula_16">h ?+1 ? = h ? ? + ? ? m ?,? V?V + (1 -?) ? m ?,? U?V (<label>12</label></formula><formula xml:id="formula_17">)</formula><p>where ? is either a hyper-parameter or a learnable scaler to control the ratio between inter-feature aggregation and cell-feature aggregation.</p><p>Next we elaborate on the modality prediction head and loss function for the task. The head structure for modality prediction is relatively simple. Note that we deliberately keep the same hidden dimension throughout the cell-feature graph convolution; thus we can simply use a trainable weight vector w to sum up cell node embeddings from different layers, as follows:</p><formula xml:id="formula_18">? = ? ?? ?=1 w ? ? H ? U (<label>13</label></formula><formula xml:id="formula_19">)</formula><p>where ?, H ? U ? R ? ?? , and ? is the dimension of hidden layer. After that, a simple fully connected layer is performed to transform it to the target space:</p><formula xml:id="formula_20">? = ?W + b.<label>(14)</label></formula><p>A rooted mean squared error (RMSE) loss is then calculated as:</p><formula xml:id="formula_21">L = 1 ? ? ?? ?=1 (Y ? -?? ) 2 ,<label>(15)</label></formula><p>which is a typical loss function for regression tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Modality Matching.</head><p>Our goal in the modality matching task is to predict the probability of that a pair of different modality data is actually from the same cell. Modality matching is very different from modality prediction task in two regards: (1) it requires interactions between two modalities; and (2) it does not demand the model to give detailed prediction but it emphasizes pairings between two modalities. Therefore, we need to adjust the framework in graph construction and task-specific head correspondingly. Since two different modalities are presented as input in this task, we construct two cell-feature graphs. Cell-feature graph convolutions are then performed on these two graphs separately to obtain two embedding matrices ?m1 ? R ? ?? 1 and ?m2 ? R ? ?? 2 . We set ? 1 = ? 2 so they can be directly multiplied together. Thus, we calculate the cosine similarity between each cell pair as follows:</p><formula xml:id="formula_22">S = ?? m1 ? ?? ? m2 (<label>16</label></formula><formula xml:id="formula_23">)</formula><p>where S ? R ? ?? denotes the symmetric score matrix; ?? m1 and ?? m2 indicate that we perform L2 normalization for each row (i.e., each cell) in ?m1 and ?m2 before we perform matrix multiplication. We further calculate softmax function for each row and each column of S to convert scores to probabilities. Then we can express the loss function as follows: , where Y ? R ? ?? denotes a binarized matching matrix that indicates the perfect matching (i.e., the ground truth label), and P ?,? ? R ? ?? represents the probability that ?-th data in modality 1 and ?-th data in modality 2 are actually referring to the same cell.</p><formula xml:id="formula_24">L match = - ? ?? ? 1 =1 ? ?? ? 2 =1 Y ? 1 ,? 2 log(P ? ? 1 ,? 2 ) + Y ? 1 ,? 2 log(P ? ? 1 ,? 2 )<label>(17</label></formula><p>In addition to the matching loss L match , we include a set of auxiliary losses to boost the performance, i.e., prediction losses and reconstruction losses:</p><formula xml:id="formula_25">L aux = L pred12 + L pred21 + L recon11 + L recon22 = 1 ? ? ?? ?=1 (X m2 -? ? 2 ( ?m1 )) 2 + 1 ? ? ?? ?=1 (X m1 -? ? 1 ( ?m2 )) 2 + 1 ? ? ?? ?=1 (X m1 -? ? 1 ( ?m1 )) 2 + 1 ? ? ?? ?=1 (X m2 -? ? 2 ( ?m2 )) 2</formula><p>(18) where X m1 and X m2 refer to the preprocessing results of two modalities respectively, ? ? 1 and ? ? 2 each refers to a fully connected network with one hidden layer, which project the node embeddings ? to the particular target modality space. These auxiliary losses provide extra supervision for our model to encode the necessary information within the hidden space ?. Hence they have the potential to improve the robustness of the model and reduce the risk of overfitting. In the training phase, we jointly optimize L match and L aux .</p><p>Lastly, in the inference phase, we introduce bipartite matching as an extra post-processing method to further augment our matching result. Specifically, we first use percentile threshold to filter the score matrix S in order to reduce the subsequent calculations, resulting in a symmetric sparse matrix S ? . We consider S ? as an adjacency matrix of a bipartite graph, which helps us model the two different modality data and their inter-class relations. Thus, our goal (i.e., matching between two modalities) becomes a rectangular linear assignment problem, where we try to find a perfect matching that maximizes the sum of the weights of the edges included in the matching. We can effectively solve this problem through Hungarian algorithm, also known as the Munkres or Kuhn-Munkres algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Joint Embedding.</head><p>The target of the joint embedding task is to learn cell embeddings from multiple modalities and thus better describe cellular heterogeneity. Several complex metrics are enabled in this task, there often exist trade-offs between metrics and metrics (e.g. to remove the batch effect while retain the batch information). To provide more training signal, we utilize both supervised and self-supervised losses to train our graph neural networks. Specifically, we first use the LSI for preprocessing to generate the input node features for two modalities and concatenate them as one joint modality, which allows us to construct a cell-feature graph. Based on the graph, we perform the proposed cell-feature graph convolution and generate the output cell node embedding in the same way as in Eq. ( <ref type="formula" target="#formula_18">13</ref>). As suggested by the work <ref type="bibr" target="#b14">[15]</ref>, cell type information plays a key role in the metrics of joint embedding evaluation and it is beneficial to extract ? dimensions from the hidden space to serve as supervision signals. Following this idea, we calculate the softmax function for ? ? {1, ...,? }, ? ? {1, ..., ? } with ? being the number of cells:</p><formula xml:id="formula_26">??,? = ? ??,? ? ?=1 ? ??,?<label>(19)</label></formula><p>where ? is the probability of that cell ? belongs to cell type ? and ? is set to be exactly equal to the total number of cell types. Then we introduce the loss functions:</p><formula xml:id="formula_27">L = L recon + L cell type + L regular = 1 ? ? ?? ?=1 (X LSI -? ? ( ?)) 2 + ? ?? ? =1 Y ? log( ?? ) + ? * ? ? J ? 2 (<label>20</label></formula><formula xml:id="formula_28">)</formula><p>where ? ? is a two-layer perceptron, Y ? R ? ?? is the sparse labels of cell types, and ? J refers to the other hidden dimensions aside from the ? dimensions that have been exclusive to cell type information. Eventually, the output hidden space ? would contain cellular information required by the task, although the loss functions do not directly optimize the final metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENT</head><p>In this section, we evaluate the effectiveness of our framework scMoGNN against three tasks and show how scMoGNN outperforms representative baselines over all three tasks by combining our general framework with task-specific design. Note that in this experiment, we follow the official settings and datasets in the multimodal single-cell data integration competition at NeurIPS 2021 <ref type="bibr" target="#b7">[8]</ref> and we compare the performance of the proposed framework with that from the top winners in the competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Modality Prediction</head><p>Datasets. In the modality prediction dataset, each modality is provided with source data, preprocessed data (with default methods), and batch labels. Data statistics are shown in Table <ref type="table" target="#tab_3">4</ref> in Appendix B. Note that the GEX-ATAC and ATAC-GEX subtasks are not entirely symmetric, because in the GEX-ATAC task, the output dimension is deliberately reduced, where 10,000 ATAC features are randomly selected out of the original 116,490 dimensions. Settings and Parameters. For each experiment, we report the average performance of 10 runs. In each run, we randomly choose 85% of cells for training and the remaining 15% of cells for validation. When the validation loss does not decrease within 300 epochs, the training is terminated, and the parameter with the best validation performance is adopted. Additionally, in practice, we utilized the initial residual instead of the skip connections in Eq.9. We used group normalization for aggregation results and dismissed the edge weight normalization stated in Eq. 7. Besides, we empirically set our parameter ? in Eq. 12 to 0.5 and the impact of this parameter will be discussed in the following sections.</p><p>Baselines. In Table such as extracting TF-IDF features and filtering highly variable genes/features. The source codes for all the methods above can be found in the official github <ref type="foot" target="#foot_2">6</ref> . It can be seen that the existing models are relatively simple, mainly based on traditional machine learning models and autoencoders. In contrast, our framework has a more advanced architecture, which provides the flexibility to different structural data (e.g. graph data) and different tasks. This makes our framework a very suitable backbone in the field of single-cell multimodal data integration. Results. As shown in Table <ref type="table" target="#tab_0">1</ref>, our method achieved the lowest overall score in the competition (the lower, the better). An interesting observation is that there is no team lead consistently across all subtasks, resulted in individual winner for each category, which is very different from the other two tasks in the competition. However, as far as we know, there is no team that focused on only one subtask. Such phenomenon may be caused by two reasons: (1) the modality prediction task is the most competitive task in the competition, and many participating teams only focused on this task (including our own team). As a result, over 40 individual teams appeared on the final leaderboard. (2) the modality prediction task has only 1,000 cells in the private test dataset, therefore, certain variance exists in the evaluation results. However, the charm of the competition lies in the emphasis on the best result rather than the average performance; as a result, some particularly good results decide the winner. Compared to the other models, our GNN model presented consistently better performance cross these four subtasks and became the overall winner in the competition.</p><p>Furthermore, we further improved our original models after the competition, including: a learning-rate decay training strategy, and more hidden units along with stronger regularization of dropout and weight decay. Eventually, we've effectively strengthened our graph neural network model hence significantly improved our results, especially in the toughest subtask GEX-to-ADT, where the output for each cell is a 134-dimensional dense vector. We now achieved an RMSE loss of 0.3809 which is lower than previous best score of 0.3854 in the competition. Overall, the results prove the effectiveness of our GNN framework, and in some specific cases, scMoGNN has tremendous advantage in view of performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Modality Matching</head><p>Datasets. The majority of the modality matching dataset is the same as the modality prediction dataset (as shown in Table <ref type="table" target="#tab_4">5</ref> in Appendix B, while several differences exist: (1) the number of the cells in the test dataset is much more than the previous task; (2) GEX-ATAC and ATAC-GEX subtasks now become perfectly dual tasks; and (3) in each modality, the order of cells is messed up. In the training data, the correspondence between two modalities is given, while for the test data, we have no correspondence between modalities, where our goal is to find a match. Settings and Parameters. The experimental settings are similar to the previous task, while in practice, we make some adjustments. For calculation convenience, we follow the recent practical trends in graph neural networks, to decouple the propagation and transformation. It is worth noting that batch labels are given in company with the test data, which provides a very strong prior knowledge for matching. More concretely, we can first divide the test data into different batches, and then match the data that belong to the same batch. This method dramatically reduced the search space, and resulted in a significant performance boost. To be fair, we have confirmed that the winner solution also used this strategy in the competition. Baselines. In Table <ref type="table" target="#tab_1">2</ref>, we only show teams that acquired top results in one or more subtasks in the competition. Next we briefly introduce those models: (1) Baseline, first projecting one modality to the other with linear regression, and then searching for the nearest neighbors in the same modality. (2) GLUE, a variational auto-encoder (VAE) model supervised by three auxiliary losses.</p><p>(3) Novel, a feed-forward neural network directly supervised by matching loss. Results. As shown in Table <ref type="table" target="#tab_1">2</ref>, scMoGNN outperforms the winner team and the runner-up team with a very large margin. Note that we didn't create any models for this task during the competition since we focus on the modality prediction task. That is why we don't have any results on the official leaderboard.</p><p>The score of the metric can be roughly understood as the accuracy of predicting a right correspondence for each piece of data. Meanwhile the search space grows with the total number of cells in the test data. For example, in the test phase of the ADT-to-GEX subtask, we have 15,066 cells to match, thus for each piece of ADT data, we have 15,066 candidates in GEX data. The expectation of the accuracy of randomly guess thus equals to 1  15,066 , which can indicate the difficulty of this task. Thus, scMoGNN has already achieved very high performance (e.g. 0.08 in ADT-GEX subtask).</p><p>It is also worth noticing that both team Novel's model and sc-MoGNN utilizes a symmetric matching algorithm, thus we have exactly the same performance for dual subtasks (e.g. GEX2ADT and ADT2GEX). Another interesting observation is that our proposed graph neural network model is especially good at GEX-ADT dual subtasks, where we improved the previous winning performance from 0.05 to 0.08.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Joint Embedding</head><p>Datasets. The training data of this task are basically the same as the modality prediction task. Moreover, data from different modalities are already aligned. Additionally, cell type labels are provided in  the training phase, while test data consist of all the train cells and unseen test cells but are not along with cell type labels. Settings and Parameters. Considering that the input in this task is very similar to modality matching, we follow Section 5.2 to decouple our graph neural networks model.</p><p>Baselines. We briefly describe the other three models in Table <ref type="table" target="#tab_2">3</ref>:</p><p>(1) Baseline, a concatenation of PCA results of two modalities. (2) JAE, an auto-encoder model that incorporates extra supervision (cell annotations). ( <ref type="formula" target="#formula_4">3</ref>) GLUE, an auto-encoder model guided by an external knowledge graph.</p><p>Results. As shown in Table <ref type="table" target="#tab_2">3</ref>, our scMoGNN significantly outperforms the other two models in GEX-ADT joint embedding task, with an improvement over 0.1 according to the average metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Study</head><p>Throughout the previous sections, we have examined that our graph neural network general framework is suitable for all these tasks in single-cell multimodal data integration. In this subsection, we investigate if we really benefit from graph structural information?</p><p>We choose modality matching task as an example. In the modality matching task we use decoupled GNNs, thus, we can easily remove the graph structural information by eliminating the propagation layers. The result is referred to as "w/o propagation" in Figure <ref type="figure">2</ref>.</p><p>The performance significantly drops from 0.0810 to 0.0745 in the GEX-ADT subtask and from 0.0630 to 0.0562 in the GEX-ATAC subtask, respectively. These observations indicate that the graph structural information extracted by the propagation layers indeed helped the performance of our method significantly. We also examined the importance of our auxiliary loss, shown in Figure <ref type="figure">2</ref>. Without the supervision of auxiliary losses, scMoGNN lost a lot of generalization ability, behaving as poor as without graph structural information. Another issue is that some negative values exist in the adjacency matrix, causing by dimensionality reduction, which could lead to the failure of conventional weight normalization methods. As an alternative, we proposed two solutions. One is to standardize results of the aggregation to replace the normalization of the adjacency matrix; the other is to use the min max scaling algorithm to project all the values on the adjacency matrix to [0, 1]. The results show that the first solution slightly outperforms the second one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Parameter Analysis</head><p>We analyzed an important parameter in our framework, i.e. w in Eq. 13, in order to gain a deeper insight on scMoGNN . Specifically, w is a learnable parameter that controls the weight between each propagation layer. Intuitively, the value of w can prove to us the effectiveness of graph structural information and help us understand how much higher-order structural information is valued by models in different tasks. Therefore we show values of w learned by scMoGNN in different tasks in Figure <ref type="figure">3</ref>. Note that in different tasks we have different numbers of layers, in modality prediction we have 4 layers and in modality matching and joint embedding we have 3 layers and 2 layers, respectively. The results consistently show that scMoGNN tends to synthesize the information in each layer, not just limited to the shallow layer, which suggests that the information of the higher-order graph structure is indeed effective. As for more details, joint embedding depends more on local information, which exists in source input data. While in modality prediction, more higher-order information is referenced, indicating that the model needs to enrich more information from similar cells or similar features. This can be explained from the need for more detailed information in modality prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this work, we proposed a general framework scMoGNN based on GNNs for multimodal single-cell data integration. It can be broadly applied on all three key tasks, modality prediction, modality matching and joint embedding, from the NeurIPS 2021 Competition. Our framework scMoGNN is able to capture high-order structural information between cells and features. To the best of our knowledge, we are the first to apply GNNs in this field. Our method officially wins first place in the overall ranking of the modality prediction task and now outperforms all models from three tasks on the leaderboard with remarkable advantage. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A DETAILS OF METRICS IN TASK 3 A.1 Biology Conservation Metrics</head><p>? NMI cluster/label: Normalized mutual information (NMI) <ref type="bibr" target="#b17">[18]</ref> compares the overlap of two clusterings. We use NMI to compare the cell type labels with an automated clustering computed on the integrated dataset (based on Louvain clustering 7 ). NMI scores of 0 or 1 correspond to uncorrelated clustering or a perfect match, respectively. Automated Louvain clustering is performed at resolution ranges from 0.1 to 2 in steps of 0.1, and the clustering output with the highest NMI with the label set is used. ? Cell type ASW: The silhouette width metric indicates the degree to which observations with identical labels are compact. The average silhouette width (ASW) <ref type="bibr" target="#b1">[2]</ref>, which ranges between -1 and 1, is calculated by averaging the silhouette widths of all cells in a set. We employ ASW to determine the compactness of the resulting embedding's cell types. The ASW of the cluster is calculated using the cell identity labels and scaled to a value between 0 and 1 using the equation:</p><formula xml:id="formula_29">??? = (??? ? + 1)/2<label>(21)</label></formula><p>where C denotes the set of all cell identity labels. ? Cell cycle conservation: The cell cycle conservation score serves as a proxy for the preservation of the signal associated with gene programs during data integration. It determines the amount of variance explained by cell cycle per batch prior to and following integration. The differences in variance before ? ?? ?? ? ??? and variance after ? ?? ?? ??? are aggregated into a final score between 0 and 1, using the equation:</p><formula xml:id="formula_30">?? ?????? ?????? = 1 - |? ?? ?? ??? -? ?? ?? ? ??? | ? ?? ?? ? ???<label>(22)</label></formula><p>where values near to 0 suggest less conservation of variance explained by the cell cycle, while 1 represents complete conservation. ? Trajectory conservation: The conservation score of a trajectory is a proxy for the conservation of a continuous biological signal within a joint embedding. We compare trajectories computed after integration for relevant cell types that depict a continuous cellular differentiation process to trajectories computed per batch and modality using this metric. The conservation of the trajectory is quantified via Spearman's rank correlation coefficient <ref type="bibr" target="#b22">[23]</ref>, ?, between the pseudotime values before and after integration. The final score is scaled to a value between 0 and 1 using the equation:</p><formula xml:id="formula_31">??? ??????????????????? = (? + 1)/2<label>(23)</label></formula><p>where a value of 1 or 0 indicates that the cells on the trajectory are in the same order before and after integration, or in the reverse order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Batch Removal Metrics</head><p>? Batch ASW: The ASW is used to quantify batch mixing by taking into account the incompatibility of batch labels per cell type cluster. We consider the absolute silhouette width, 7 https://en.wikipedia.org/wiki/Louvain_method on batch labels per cell, in particular. Here, zero shows that batches are thoroughly mixed, but any variation from zero indicates the presence of a batch effect. We rescale this score so that higher values imply better batch mixing and use the equation below to determine the per-cell type label, j:</p><p>batch ??? ? = 1</p><formula xml:id="formula_32">? ? ?? ? ?? ? 1 -|? (?)|<label>(24)</label></formula><p>where ? ? is the set of cells with the cell label j and |? ? | denotes the number of cells in that set. To obtain the final ???????? score, the label-specific ???????? ? scores are averaged:</p><p>batch</p><formula xml:id="formula_33">??? = 1 |? | ?? ? ?? batch ??? ? (<label>25</label></formula><formula xml:id="formula_34">)</formula><p>where M is the set of unique cell labels. A ???????? value of 1 suggests optimal batch mixing, whereas a value of 0 indicates severely separated batches. ? Graph connectivity: The graph connectivity metric determines whether cells of the same kind from various batches are embedded close to one another. This is determined by computing a k-nearest neighbor (kNN) <ref type="bibr" target="#b20">[21]</ref> graph using Euclidean distances on the embedding. Then, we determine if all cells with the same cell identity label are connected in this kNN graph. For each cell identity label ?, we generate the subset kNN graph ? = (? ? ; ? ? ), which contains only cells from a given label. Using these subset kNN graphs, we compute the graph connectivity score: where ? represents the set of cell identity labels, ??? () denotes the number of nodes in the largest connected component of the graph, and ? ? is the number of nodes with cell identity ?. The resulting score ranges from 0 to 1, where 1 means that all cells with the same cell identity are connected in the integrated kNN graph, while 0 indicates that no cell is connected in the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Metric Aggregation</head><p>Due to the differing nature of each metric, each metric would be assigned with a weight. An overall weighted average of batch correction and bio-conservation scores will be computed via the equation:</p><formula xml:id="formula_35">? overall ,? = 0.6 ? ? ???,? + 0.4 ? ? ?????,?<label>(27)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B DATA STATISTICS</head><p>As shown in the Table <ref type="table" target="#tab_3">4</ref> and Table <ref type="table" target="#tab_4">5</ref>, it lists statistics about dataset used in modality prediction task and modality matching task respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C REPRODUCIBILITY C.1 source code</head><p>All the source code of winning solutions can be found at official github (https://github.com/openproblems-bio/neurips2021_multimodal_ topmethods). These codes have been officially verified thus reproducibility is ensured.</p><p>For our modifications to our original model, and the new models we made after the competitions, please check our supplementary files. (We will release it later on github)</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: An overview of scMoGNN . We first construct the cell-feature graph from given modality and then perform cell-feature graph convolution to obtain latent embeddings of cells, which are sent to a task-specific head to perform the downstream task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>) with P ? ?,? = ? S ?,? ? ?=1 ? S ?,? , P ? ?,? = ? S ?,? ? ?=1 ? S ?,?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Ablation study for the modality matching task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(? ? ; ? ? ))| |? ? |(26)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>RMSE for Modality Prediction (Task 1)? . '*' indicates ensemble models.</figDesc><table><row><cell>1, we only show the teams that acquired top</cell></row><row><cell>results in one or more subtasks in the competition, because they</cell></row><row><cell>are officially required to declare their model details. For further</cell></row><row><cell>comparison, we briefly detail each method: (1) Baseline, a truncated-</cell></row><row><cell>SVD dimensionality reduction followed by linear regression. (2)</cell></row><row><cell>Dengkw, a well-designed model based on kernel ridge regression.</cell></row><row><cell>(3) Novel, an encoder-decoder structure with LSI preprocessing. (4)</cell></row><row><cell>Living System Lab, an ensemble model composed of random forest</cell></row></table><note><p><p><p><p><p><p><p>models, catboost</p><ref type="bibr" target="#b21">[22]</ref> </p>models, and k-nearest neighbors regression models. (</p>5</p>) Cajal, a feed forward neural network with heavy feature selection guided by prior knowledge. (</p>6</p>) ScJoint, an ensemble neural network model incorporated various strategies of preprocessing</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performances for Modality Matching (Task 2)?.</figDesc><table><row><cell></cell><cell cols="6">GEX2ADT ADT2GEX GEX2ATAC ATAC2GEX Overall</cell></row><row><cell>Baseline</cell><cell>0.0000</cell><cell></cell><cell>0.0000</cell><cell cols="2">0.0001</cell><cell>0.0001</cell><cell>0.0001</cell></row><row><cell>GLUE</cell><cell>0.0495</cell><cell></cell><cell>0.0516</cell><cell cols="2">0.0560</cell><cell>0.0583</cell><cell>0.0539</cell></row><row><cell>Novel</cell><cell>0.0373</cell><cell></cell><cell>0.0373</cell><cell cols="2">0.0412</cell><cell>0.0412</cell><cell>0.0392</cell></row><row><cell>scMoGNN</cell><cell>0.0810 ?0.0015</cell><cell></cell><cell>0.0810 ?0.0015</cell><cell cols="2">0.0630 ?0.0014</cell><cell>0.0630 ?0.0014</cell><cell>0.0720</cell></row><row><cell></cell><cell>0.0850</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.0810</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.0800</cell><cell></cell><cell>0.0787</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.0750</cell><cell>0.0757</cell><cell>0.0745</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.0700</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.0650</cell><cell></cell><cell></cell><cell>0.0630</cell><cell>0.0615</cell><cell>0.0619</cell></row><row><cell></cell><cell>0.0600</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.0562</cell></row><row><cell></cell><cell>0.0550</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.0500</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">GEX-ADT</cell><cell></cell><cell cols="2">GEX-ATAC</cell></row><row><cell></cell><cell cols="6">scMoGNN w/o auxiliary loss w/o propagation w/o negative edge weight</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performances for Joint Embedding (Task 3)?.</figDesc><table><row><cell></cell><cell>NMI cluster/label</cell><cell>Cell type ASW</cell><cell>Cc_con</cell><cell>Traj_con</cell><cell>Batch ASW</cell><cell cols="2">Graph connectivity Average metric</cell></row><row><cell>Baseline</cell><cell>0.6408</cell><cell>0.5266</cell><cell>0.9270</cell><cell>0.8325</cell><cell>0.7982</cell><cell>0.8945</cell><cell>0.7699</cell></row><row><cell>Amateur (JAE)</cell><cell>0.7608</cell><cell>0.6043</cell><cell>0.7817</cell><cell>0.8631</cell><cell>0.8432</cell><cell>0.9700</cell><cell>0.8039</cell></row><row><cell>GLUE</cell><cell>0.8022</cell><cell>0.5759</cell><cell>0.6058</cell><cell>0.8591</cell><cell>0.8800</cell><cell>0.9506</cell><cell>0.7789</cell></row><row><cell>scMoGNN</cell><cell cols="5">0.8499 ? 0.0032 0.6496 ? 0.0046 0.7084 ? 0.0316 0.8532 ? 0.0019 0.8691? 0.0020</cell><cell>0.9708 ? 0.0041</cell><cell>0.8168 ? 0.0051</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Dataset Statistics of modality prediction task. The number of feature dimensions, train/test samples, and batches.GEX-ADT ADT-GEX GEX-ATAC ATAC-GEX</figDesc><table><row><cell>Source Dim</cell><cell>13,953</cell><cell>134</cell><cell>13,431</cell><cell>116,490</cell></row><row><cell>Target Dim</cell><cell>134</cell><cell>13,953</cell><cell>10,000</cell><cell>13,431</cell></row><row><cell>Train Cells</cell><cell>66,175</cell><cell>66,175</cell><cell>42,492</cell><cell>42,492</cell></row><row><cell>Test Cells</cell><cell>1,000</cell><cell>1,000</cell><cell>1,000</cell><cell>1,000</cell></row><row><cell>Train Batches</cell><cell>9</cell><cell>9</cell><cell>10</cell><cell>10</cell></row><row><cell>Test Batches</cell><cell>3</cell><cell>3</cell><cell>3</cell><cell>3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Dataset Statistics of modality matching task. The number of feature dimensions, train/test samples, and batches.GEX-ADT ADT-GEX GEX-ATAC ATAC-GEX</figDesc><table><row><cell>Source Dim</cell><cell>13,953</cell><cell>134</cell><cell>13,431</cell><cell>116,490</cell></row><row><cell>Target Dim</cell><cell>134</cell><cell>13,953</cell><cell>116,490</cell><cell>13,431</cell></row><row><cell>Train Cells</cell><cell>66,175</cell><cell>66,175</cell><cell>42,492</cell><cell>42,492</cell></row><row><cell>Test Cells</cell><cell>15,066</cell><cell>15,066</cell><cell>20,009</cell><cell>20,009</cell></row><row><cell>Train Batches</cell><cell>10</cell><cell>10</cell><cell>10</cell><cell>10</cell></row><row><cell>Test Batches</cell><cell>3</cell><cell>3</cell><cell>3</cell><cell>3</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0"><p>https://openproblems.bio/neurips_2021/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1"><p>https://en.wikipedia.org/wiki/Root-mean-square_deviation</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2"><p>https://github.com/openproblems-bio/neurips2021_multimodal_topmethods</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">MOFA+: a statistical framework for comprehensive integration of multi-modal single-cell data</title>
		<author>
			<persName><forename type="first">Ricard</forename><surname>Argelaguet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damien</forename><surname>Arnol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danila</forename><surname>Bredikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Deloro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Britta</forename><surname>Velten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">C</forename><surname>Marioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Stegle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome biology</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Fatima</forename><surname>Batool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Hennig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.11339</idno>
		<title level="m">Clustering with the average silhouette width</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Relational inductive biases, deep learning, and graph networks</title>
		<author>
			<persName><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Peter W Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><surname>Faulkner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv preprint</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Joint profiling of chromatin accessibility and gene expression in thousands of single cells</title>
		<author>
			<persName><forename type="first">Junyue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darren</forename><forename type="middle">A</forename><surname>Cusanovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Ramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Delasa</forename><surname>Aghamirzaie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><forename type="middle">A</forename><surname>Pliner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><forename type="middle">L</forename><surname>Daza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">S</forename><surname>Mcfaline-Figueroa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lena</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName><surname>Christiansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">361</biblScope>
			<biblScope unit="page" from="1380" to="1385" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">High-throughput sequencing of the transcriptome and chromatin accessibility in the same cell</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Blue B Lake</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature biotechnology</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1452" to="1457" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">GNN-based embedding for clustering scRNA-seq data</title>
		<author>
			<persName><forename type="first">Madalina</forename><surname>Ciortan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Defrance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1037" to="1044" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Integrative analysis of single-cell genomics data by coupled nonnegative matrix factorizations</title>
		<author>
			<persName><forename type="first">Zhana</forename><surname>Duren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahdi</forename><surname>Zamanighomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanwen</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ansuman</forename><forename type="middle">T</forename><surname>Satpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howard</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung</forename><surname>Wing</surname></persName>
		</author>
		<author>
			<persName><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="7723" to="7728" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A sandbox for prediction and integration of DNA, RNA, and proteins in single cells</title>
		<author>
			<persName><surname>Luecken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Datasets and Benchmarks Track</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural Message Passing for Quantum Chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<title level="s">Proceedings of Machine Learning Research</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08">2017. 2017. August 2017</date>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cobolt: Joint analysis of multimodal single-cell sequencing data</title>
		<author>
			<persName><forename type="first">Boying</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Purdom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">scAI: an unsupervised approach for the integrative analysis of parallel single-cell transcriptomic and epigenomic profiles</title>
		<author>
			<persName><forename type="first">Suoqin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome biology</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Single-cell multiomics: technologies and data analysis methods</title>
		<author>
			<persName><forename type="first">Jeongwoo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Do</forename><surname>Young Hyeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daehee</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Experimental &amp; Molecular Medicine</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="1428" to="1442" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The molecular signatures database hallmark gene set collection</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Liberzon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chet</forename><surname>Birger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helga</forename><surname>Thorvaldsd?ttir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahmoud</forename><surname>Ghandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jill</forename><forename type="middle">P</forename><surname>Mesirov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Tamayo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell systems</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="417" to="425" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Simultaneous deep generative modelling and clustering of single-cell genomic data</title>
		<author>
			<persName><forename type="first">Qiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengquan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung</forename><surname>Wing</surname></persName>
		</author>
		<author>
			<persName><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature machine intelligence</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="536" to="544" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Graph Neural Networks with Adaptive Residual</title>
		<author>
			<persName><forename type="first">Xiaorui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayuan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A unified view on graph neural networks as graph signal denoising</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaorui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yozen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 30th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1202" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Derek</forename><surname>Aaron F Mcdaid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName><surname>Hurley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1110.2515</idno>
		<title level="m">Normalized mutual information to evaluate overlapping community finding algorithms</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scalable, multimodal profiling of chromatin accessibility, gene expression and protein levels in single cells</title>
		<author>
			<persName><forename type="first">Caleb</forename><forename type="middle">A</forename><surname>Eleni P Mimitou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><forename type="middle">Y</forename><surname>Lareau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhan</forename><surname>Zorzetto-Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuke</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendy</forename><surname>Takeshima</surname></persName>
		</author>
		<author>
			<persName><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><surname>Tse-Shun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bertrand</forename><forename type="middle">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Efthymia</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><surname>Papalexi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature biotechnology</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1246" to="1258" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hiroyoshi Nishikawa, and Teppei Shimamura. 2021. scMM: Mixture-of-experts multimodal deep generative model for single-cell multiomics data analysis</title>
		<author>
			<persName><forename type="first">Kodai</forename><surname>Minoura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ko</forename><surname>Abe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunha</forename><surname>Nam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Available at SSRN</title>
		<imprint>
			<biblScope unit="volume">3806072</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">K-nearest neighbor</title>
		<author>
			<persName><forename type="first">Leif</forename><forename type="middle">E</forename><surname>Peterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scholarpedia</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">1883</biblScope>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Aleksandr Vorobev, Anna Veronika Dorogush, and Andrey Gulin</title>
		<author>
			<persName><forename type="first">Liudmila</forename><surname>Prokhorenkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gleb</forename><surname>Gusev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note>CatBoost: unbiased boosting with categorical features</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Spearman&apos;s rank correlation coefficient</title>
		<author>
			<persName><forename type="first">Philip</forename><surname>Sedgwick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bmj</title>
		<imprint>
			<biblScope unit="volume">349</biblScope>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">scDeepSort: a pre-trained cell-type annotation method for single-cell transcriptomics using deep learning with a weighted graph neural network</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haihong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Penghui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohui</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Integrative clustering of multiple genomic data types using a joint latent variable model with application to breast and lung cancer subtype analysis</title>
		<author>
			<persName><forename type="first">Ronglai</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">B</forename><surname>Olshen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Ladanyi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">DSTG: deconvoluting spatial transcriptomics data through graph-based artificial intelligence</title>
		<author>
			<persName><forename type="first">Qianqian</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brief Bioinform</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">scGCN is a graph convolutional networks algorithm for knowledge transfer in single cell omics</title>
		<author>
			<persName><forename type="first">Qianqian</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Enter the matrix: factorization uncovers knowledge from omics</title>
		<author>
			<persName><forename type="first">Genevieve</forename><forename type="middle">L</forename><surname>Stein-O'brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raman</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aedin</forename><forename type="middle">C</forename><surname>Culhane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">V</forename><surname>Favorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lana</forename><forename type="middle">X</forename><surname>Garmire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Casey</forename><forename type="middle">S</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifeng</forename><surname>Goff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aloune</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">F</forename><surname>Ngom</surname></persName>
		</author>
		<author>
			<persName><surname>Ochs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Genetics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="790" to="805" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Simultaneous epitope and transcriptome measurement in single cells</title>
		<author>
			<persName><forename type="first">Marlon</forename><surname>Stoeckius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Hafemeister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Stephenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Houck-Loomis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratip</forename><forename type="middle">K</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harold</forename><surname>Swerdlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Satija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Smibert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="865" to="868" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Comprehensive integration of single-cell data</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Hafemeister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Efthymia</forename><surname>Papalexi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">M</forename><surname>Mauck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhan</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marlon</forename><surname>Stoeckius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Smibert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Satija</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">177</biblScope>
			<biblScope unit="page" from="1888" to="1902" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Gene set enrichment analysis: a knowledgebased approach for interpreting genome-wide expression profiles</title>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Tamayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vamsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sayan</forename><surname>Mootha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><forename type="middle">L</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Gillette</surname></persName>
		</author>
		<author>
			<persName><surname>Paulovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><forename type="middle">R</forename><surname>Pomeroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">S</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><surname>Lander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="15545" to="15550" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">2021. scGNN is a novel graph neural network framework for single-cell RNA-Seq analyses</title>
		<author>
			<persName><forename type="first">Juexin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzhou</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianting</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuexu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ren</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cankun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongjun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">MATCHER: manifold alignment reveals correspondence between single cell transcriptome and epigenome dynamics</title>
		<author>
			<persName><forename type="first">Joshua D</forename><surname>Welch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Hartemink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">F</forename><surname>Prins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome biology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">BABEL enables cross-modality translation between multiomic profiles at single-cell resolution</title>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">E</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><forename type="middle">E</forename><surname>Yost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howard</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Koen Van den Berge, et al. 2021. A transcriptomic and epigenomic cell atlas of the mouse primary motor cortex</title>
		<author>
			<persName><forename type="first">Zizhen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangming</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricky</forename><forename type="middle">S</forename><surname>Adkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">I</forename><surname>Aldridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><forename type="middle">A</forename><surname>Ament</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margarita</forename><surname>Behrens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">598</biblScope>
			<biblScope unit="page" from="103" to="110" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An ultra high-throughput method for single-cell joint analysis of open chromatin and transcriptome</title>
		<author>
			<persName><forename type="first">Chenxu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Juric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armen</forename><surname>Abnousi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacinta</forename><surname>Lucero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margarita</forename><surname>Behrens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature structural &amp; molecular biology</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1063" to="1070" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
