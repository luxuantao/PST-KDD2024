<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Object Grasping for Soft Robot Hands</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Changhyun</forename><surname>Choi</surname></persName>
							<email>cchoi@csail.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science &amp; Artificial Intelligence Lab</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wilko</forename><surname>Schwarting</surname></persName>
							<email>wilkos@csail.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science &amp; Artificial Intelligence Lab</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joseph</forename><surname>Delpreto</surname></persName>
							<email>delpreto@csail.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science &amp; Artificial Intelligence Lab</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniela</forename><surname>Rus</surname></persName>
							<email>rus@csail.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science &amp; Artificial Intelligence Lab</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Object Grasping for Soft Robot Hands</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6854A4A263AFF3072106929E3D3EAF41</idno>
					<idno type="DOI">10.1109/LRA.2018.2810544</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/LRA.2018.2810544, IEEE Robotics and Automation Letters IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED FEBRUARY, 2018 1 Manuscript received: September 10, 2017; Revised December 21, 2017; Accepted February 5, 2018.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Perception for Grasping and Manipulation, Deep Learning in Robotics and Automation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a 3D deep convolutional neural network (3D CNN) approach for grasping unknown objects with soft hands. Soft hands are compliant and capable of handling uncertainty in sensing and actuation, but come at the cost of unpredictable deformation of the soft fingers. Traditional modeldriven grasping approaches, which assume known models for objects, robotic hands, and stable grasps with expected contacts, are inapplicable to such soft hands, since predicting contact points between objects and soft hands is not straightforward. Our solution adopts a deep CNN approach to find good caging grasps for previously unseen objects by learning effective features and a classifier from point cloud data. Unlike recent CNN models applied to robotic grasping which have been trained on 2D or 2.5D images and limited to a fixed top grasping direction, we exploit the power of a 3D CNN model to estimate suitable grasp poses from multiple grasping directions (top and side directions) and wrist orientations, which has great potential for geometryrelated robotic tasks. Our soft hands guided by the 3D CNN algorithm show 87% successful grasping on previously unseen objects. A set of comparative evaluations shows the robustness of our approach with respect to noise and occlusions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>I N robotic manipulation, robust object grasping is an im- portant prerequisite for advanced autonomous manipulation tasks. While object grasping with robotic manipulators has been actively studied for decades <ref type="bibr" target="#b0">[1]</ref>, reliable grasping of previously unseen objects is still a challenging problem. The main challenges are the uncertainties in perception and action. Earlier work has leveraged prior knowledge of object shape, manipulator, stable grasps, etc. <ref type="bibr" target="#b1">[2]</ref>. These model-driven approaches, however, are problematic when prior knowledge is partial or not available. Recent work has focused more on learning from data with hope of generalizing to novel situations by learning a mapping function from raw sensory data to a grasp representation <ref type="bibr" target="#b2">[3]</ref>. However, these learned grasp representations are rather limited as they often represent 2D grasp location and 1D wrist orientation with a fixed grasping direction, which does not generalize to 6-DoF grasp reasoning and thus does not utilize full workspace of the robot arm for grasp planning. Robot hands with hard fingers require careful positioning to achieve closure grasps. The placement of the fingers is usually sensitive to uncertainties. To overcome Fig. <ref type="figure" target="#fig_1">1</ref>: Baxter with soft hands. Our Baxter robot has two soft hands on its end effectors. A depth sensor is affixed to the upper torso of the robot, and point clouds from the sensor are used to predict suitable grasps for soft hands so as to successfully grasp the objects on the table. The right four figures show our four-finger soft hand in action. Each finger is controlled by an external pneumatic actuator and the Baxter's original parallel gripper actuator is further controlled to maximize the acquisition region.</p><p>this limitation, soft robot hands have been actively studied and fabricated using soft materials <ref type="bibr" target="#b3">[4]</ref>. The main advantages of soft hands include compliance with external perturbation and tolerance of uncertainties in actuation and perception <ref type="bibr" target="#b4">[5]</ref>, which enable soft hands to be more suitable for manipulating unknown objects. Moreover, manufacturing of soft hands is faster and less expensive than their hard counterparts <ref type="bibr" target="#b3">[4]</ref>.</p><p>In this paper, we design a soft robotic manipulation system which is capable of grasping previously unseen objects. Fig. <ref type="figure" target="#fig_1">1</ref> shows our Baxter robot setup with two soft hands mounted on its end effectors. A depth sensor, which is affixed to the upper torso of the robot, obtains partial point clouds of the objects on the table. Given the input clouds, a 3D CNN model predicts the likelihood of success of a set of suitable grasps. Together with the grasp poses, the compliance and adaptability of the soft hands yield successful grasping of novel objects. The work described in this paper neither uses proprioceptive sensors nor 3D object models; it learns appropriate grasps from partial point cloud data and generalizes well to new objects the robot has never seen before. The main contributions of this paper are as follows:</p><p>• 3D CNN-based grasp prediction: Our approach exploits the power of a 3D CNN model to predict a set of suitable grasp poses from a partial 3D point cloud of an object. While many learning-based approaches have focused on predicting wrist orientations with a fixed top grasping direction, our approach predicts both grasping directions and wrist orientations which determine a set of suitable grasp poses. • Vision-based soft hands: Unlike most soft hands demon-strating object grasping with human operation or known object pose, we propose a whole system that combines vision and soft actuation. In particular, we combine soft hands with the 3D CNN grasping prediction to reliably grasp previously unseen objects. The rationale behind our approach is that the 3D CNN grasp prediction and soft hands complement each other. A set of discretized grasp poses from learning-based methods requires adaptable grasping as there is always a discrepancy between the predicted grasp pose and real object pose. At the same time, soft hands necessitate good grasping guidance in spite of their compliance and adaptability. To the best of our knowledge, this is the first work to employ a 3D CNN-based grasp prediction to soft hands. While the work in <ref type="bibr" target="#b5">[6]</ref> presented a learning from demonstration approach for soft hands, the work employed a marker system to obtain object trajectories, and thus no learning occurs on the visual perception side. In addition, their object grasping capability has hinged upon a set of human demonstrations for a known object. Our work is different in that it learns a suitable grasp policy from the partial point cloud of a previously unknown object.</p><p>The paper is organized as follows. Section II reviews prior work in robotic object grasping. After our problem is formalized in Section III, the details of our approach are described in Section IV. Section V presents experimental results on grasp pose prediction and object grasping with our soft manipulator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Robotic grasping and grasp synthesis have been actively studied in robotics literature <ref type="bibr" target="#b0">[1]</ref>. While there are many ways to categorize this literature <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b0">[1]</ref>, the research approaches of object grasping can be roughly divided into model-driven and data-driven approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Model-driven Approaches</head><p>Classic object grasping approaches rely on prior knowledge. Such knowledge includes known stable grasp and contact information, 3D models of objects and manipulators, and their physical properties such as weights, center of mass, friction coefficients, etc. <ref type="bibr" target="#b1">[2]</ref>. The goal of these approaches is to find a set of stable force closures to grasp the known objects <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. Since the models of objects are given, the grasping approaches are based on object recognition and pose estimation with known object grasps <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> or grasp candidates sampled from the known model or simpler geometric primitives <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. As reviewing extensive model-driven approaches is beyond the scope of this paper, we refer the reader to the comprehensive literature surveys <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Data-driven Approaches</head><p>While model-driven approaches assume rich prior knowledge, data-driven approaches gain knowledge of objects and grasping from data. The key idea is to map directly from visual sensor data to grasp representations. The most popular representation is the grasping rectangle describing suitable grasping in the image plane <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b2">[3]</ref>, which is of lower dimension than the traditional grasping parameters such as the grasping point, the approach vector, the wrist orientation, and the initial finger configuration <ref type="bibr" target="#b0">[1]</ref>. While a simple logistic regression was proven to be an effective learning algorithm for object grasping <ref type="bibr" target="#b13">[14]</ref>, more recently deep neural network models outperformed the previous method <ref type="bibr" target="#b2">[3]</ref>. In particular, convolutional neural networks (CNNs) have been successful in generic object recognition <ref type="bibr" target="#b14">[15]</ref> due to their end-to-end feature learning in a hierarchical structure. Following the success, the CNN models have recently been applied to robotic graping <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. Common approaches employ CNN models to classify feasibility of a set of grasping hypotheses. Training data has been generated via crowdsourcing <ref type="bibr" target="#b15">[16]</ref>, physics simulation with 3D model database <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b20">[21]</ref>, or trial-and-error <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b21">[22]</ref>. Since our approach uses soft hands which are hard to model in physics simulation and crowdsourcing, we adopt the trialand-error scheme to collect the training data with the ground truth grasp labels annotated manually.</p><p>While the most common modality for CNN models is monocular images, visual perception for object grasping can potentially benefit from depth data. The main advantages of using depth data include 1) invariance of photometric variations, such as color or texture, and 2) exploiting geometric information closely related to object grasping. Although some prior works employed depth as a sensory modality, their usages were restricted to object proposal <ref type="bibr" target="#b18">[19]</ref> or 2.5D depth information <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b22">[23]</ref> without exploiting the full 3D shape information. Robust 3D reasoning is important for the object grasping problem, as the problem is closely related to geometric characteristics and constraints of objects and their surrounding environments. The work in <ref type="bibr" target="#b22">[23]</ref> employed a CNN model that learns a grasp quality from a depth image. While their system uses point clouds as a visual input, the CNN model treats it as 2.5D image not 3D, and thus the object grasp pose is limited to top grasping. Recently, full 3D CNN models have been studied and show state-of-the-art performance for shape-based object recognition tasks <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. These 3D CNNs are relatively new models and have great potential for geometry-related robotic perception.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Soft Hands</head><p>Robust object manipulation in unstructured environments is a challenging problem due to the uncertainty associated with complex and unpredictable environments. Conventional robot hands, requiring multiple articulated fingers and sensors, are expensive to manufacture and control, and yet fragile in these unstructured environments. More adaptive and compliant robotic hands were explored via under actuation <ref type="bibr" target="#b25">[26]</ref>. Recently, new types of robot hands have been designed and fabricated using soft materials <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b3">[4]</ref>. The main advantage of soft hands is compliance, which is well suited for manipulation tasks handling delicate, irregular shaped, or unknown objects. In addition, soft hands are more tolerant of uncertainties in perception and actuation <ref type="bibr" target="#b4">[5]</ref>. voxelized to generate a voxel grid G. Our approach is two-fold. First, it predicts the most likely grasping direction δ from G (solid arrows). Second, given δ, the voxel grid is transformed so that the chosen direction δ is from the top of the transformed voxel grid G . The 3D CNN then estimates the most likely wrist orientation ω (dotted arrows). Finally, the chosen grasping direction δ and wrist orientation ω determine the rotation part of the grasp pose, and the translation part of the pose is determined by the contacting voxel with δ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROBLEM FORMULATION</head><p>The grasping problem we solve in this paper can be formalized as follows:</p><p>Definition 1. Given a point cloud P ⊂ R 3 , the goal is to find an appropriate grasp pose X g ∈ SE(3) for a previously unseen object o ∈ O that is placed with arbitrary pose in P within the field of view of the robot.</p><p>The grasp pose X g is in the Special Euclidean group SE(3), which represents the 3D rigid body transformation, and is defined with respect to the robot coordinate frame. The point cloud P is obtained via a depth sensor affixed to a robot with a known extrinsic parameter X r s , by which the cloud P is transformed from the sensor coordinate frame to the robot coordinate frame. In P, if multiple objects O exist, a grasping pose X g should be estimated for each object o ∈ O. An important assumption is that Assumption 1. There is no prior knowledge of the objects O (e.g. no shape model, weight distribution, center of mass, friction coefficients, stable grasps configurations, etc.).</p><p>We wish to learn to predict X g directly from data P. As there are infinite number of poses for a given object o, we constrain X g so that our 3D CNN learns effectively as follows:</p><p>Constraint 1. The grasping pose X g ∈ SE(3) is constrained such that the grasping direction δ is one of six directions: top, left, left-front, front, right-front, right, and the wrist orientation ω is one of four orientation: 0 • , 45 • , 90 • , and 135 • . Fig. <ref type="figure">2</ref> depicts six grasping directions and four wrist orientations. The rationale behind this grasping direction and wrist orientation is in <ref type="bibr" target="#b27">[28]</ref> which has shown that grasps orthogonal to objects' principal axes tend to be more stable than randomly sampled grasps. Discretization of grasping orientation is common in CNN-based grasping approaches as CNNs perform better on classification rather than regression problems <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b20">[21]</ref>. Although we chose six grasping directions and four wrist orientations in this work, this framework can be easily adapted to a different number of grasping directions and wrist orientations depending on task requirements. In general, the more outputs the CNN has, the more training data is required. The training data amount will increase linearly with respect to the number of outputs, and hence the computation cost will increase linearly as well. However, it was recently shown that most of the energy, about 80% of the entire computational effort, is consumed by the convolution layers <ref type="bibr" target="#b28">[29]</ref>. If we add additional approaching directions to the output layers, it will change only the last fully connected layer. Therefore, the computation cost will be increased at worst sub-linear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PROPOSED APPROACH A. System Overview</head><p>Our grasping system is composed of one Baxter robot and two soft hands attached to its end effectors as shown in Fig. <ref type="figure" target="#fig_1">1</ref>. A depth sensor is affixed to the upper body of the robot looking down on the table. The flow of our grasping system is described in Fig. <ref type="figure">3</ref>. When there are objects on the table, our system obtains a point cloud P and finds a set of segmented object point clouds S by removing the planar background in P 1 . Each segmented point cloud s ∈ S is then voxelized to a 3D voxel grid G ∈ Z Ng×Ng×Ng where each voxel in the grid is either -1 (not occupied) or 1 (occupied) and N g is the edge length of the cubic voxel grid. During the voxelization, the point cloud is aligned to the lower center of G. Given G, our 3D CNN model determines the most likely grasping direction δ and wrist orientation ω, and the chosen grasp is then executed with our soft hand robot manipulator. Algorithm 1 explains the details of the grasping prediction procedure. The algorithm takes the point cloud P and the trained 3D CNN model N as inputs and returns the set of grasp poses X ⊂ SE(3) for each segmented object cloud s ∈ S, i.e. |X | = |S|. The direction δ and orientation ω determine the most likely rotation of the grasp pose X g , while the translation t of X g (i.e. wrist location) is estimated via the voxel coordinates contacting with δ. One may sample the voxel along the principal axis of the voxel grid, but picking the center voxel has proven to be effective in our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. 3D Convolutional Neural Network</head><p>To determine appropriate grasping directions and wrist orientations given an input point cloud, we train a 3D conneural network (CNN). Inspired by <ref type="bibr" target="#b24">[25]</ref>, our 3D</p><p>1 While the tabletop manipulation assumption for object segmentation is considered in this work, our pipeline can easily accommodate advanced segmentation approaches, such as <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, in order to relax the tabletop assumption. G ← Voxelization(s)</p><formula xml:id="formula_0">4: p(δ) ← N .FeedForward(G) 5: δ ← arg max δ∈N + p(δ) 6: G ← VoxelTransformation(G, δ) 7: p(ω) ← N .FeedForward(G ) 8: ω ← arg max ω∈N + p(ω) 9: t ← VoxelCoordinates(G, δ) 10: Xg ← Rot( δ, ω) t 0 1 ∈ SE(3) 11: X ← X ∪ {Xg}</formula><p>CNN model is composed of convolution, pooling, and dense layers. The architecture of our model is shown in Fig. <ref type="figure" target="#fig_2">4</ref>. The input layer is a 32×32×32 3D voxel grid G which is voxelized from the raw 3D point cloud. There are two convolution layers where the first and second layers have 32 filters of 5 × 5 × 5 and 3 × 3 × 3 size, respectively. After the convolution layers, the data is fed into the max pooling layer of 2×2×2 followed by two dense layers, 128 and N δ + N ω each. Unlike the model in <ref type="bibr" target="#b24">[25]</ref>, the output layer of our model is activated via the sigmoid function instead of the softmax function because the output should be N δ + N ω independent probabilities not the probability distribution over N δ grasping directions and N ω wrist orientations. (i.e. 0 ≤ p(δ i ), p(ω j ) ≤ 1 where</p><formula xml:id="formula_1">i = 1, 2, • • • , N δ , j = 1, 2, • • • , N ω , not N δ i=1 p(δ i ) = 1 and</formula><p>Nω j=1 p(ω j ) = 1.) Hence, our loss function is defined by the binary cross-entropy instead of the categorical cross-entropy. We use the binary voxel grid in the input layer, which has only a binary state (occupied or unoccupied) in each voxel, as <ref type="bibr" target="#b24">[25]</ref> reported the performance difference between binary, hit, and density grid for object recognition tasks is negligible. Although we designed this model for soft hands grasping, we believe that this approach is general. So it should be applicable to arbitrary robot end effectors, hard and soft hands and parallel jaw grippers, by adjusting N δ and N ω depending on the compliance of the end effector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Soft Hand Control</head><p>Once the grasp pose is determined by the 3D CNN, the robot arm with our soft hand approaches to a target object and grasps it. Our soft hand in action is shown in Fig. <ref type="figure" target="#fig_1">1</ref>, and its detailed design and fabrication methods are explained in <ref type="bibr" target="#b26">[27]</ref>. Each hand has four soft fingers which are controlled by a set of four external pneumatic actuators. Two parts connect these fingers to the wrist of the Baxter robot arm; one part connects one soft finger and similarly the other part connects three fingers to the wrist. The wrist has a linear sliding actuator which controls the distance between the two parts. In total, there are five control inputs for one soft hand. While it is possible to learn to control this 5-D control parameters, it requires substantial grasp data for training. Instead, we employ a fixed two-state control policy which has open and close states 2 . One of the main benefits of soft hands is that it is simple to control due to the innate compliance of soft hands.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Training with Grasping Data</head><p>To prepare the object grasping dataset, we chose 10 training objects (Fig. <ref type="figure" target="#fig_3">5a</ref>). Each data entry has a voxel grid converted from a partial point cloud and its associated ground truth label by executing with our soft hands. 3 We have collected 719 labeled data entries for the 10 training objects. We further augment the training dataset by transforming the voxel grids, such as mirroring and translating, and the total number of data entries is 21, 570. 4 The ground truth labels were also adjusted when the voxel grids were mirrored. The network was trained with the adadelta optimizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>To evaluate the effectiveness of our approach in object grasping tasks, we run a set of comparative evaluations. Five approaches, including our approach, are compared: 1) 3DCNN, our approach described in this paper which predicts the probability of grasping directions and wrist orientations via our 3D CNN. 2) RAND, it randomly chooses one of the grasping directions and wrist orientations instead of estimating them from the CNN. It serves as a baseline showing the effectiveness of the soft hands without the use of visual perception.</p><p>3) SVM, an approach using Support Vector Machine (SVM). A voxel grid is flattened and used as a (32 × 32 × 32 = 32, 768) dimensional feature vector. As there are multiple feasible grasping directions and wrist orientations for a given voxel grid, a set of multiple binary SVM classifiers was separately trained on each grasping direction and wrist orientation. In this experiment, N δ + N ω = 10 SVM classifiers were trained on the training dataset. 4) PCA, an approach using Principal 2 In the open state, there is no air pressure applied to the soft fingers and the distance between one and three fingers is the longest. In the close state, maximum air pressure is applied to the fingers and the distance between fingers is the shortest. 3 Each ground truth label has two binary vectors</p><formula xml:id="formula_2">l δ ∈ R N δ + , lω ∈ R Nω +</formula><p>where l δ (i) = 1, lω(j) = 1 if δ i , ω j are successful grasps. Otherwise, l δ (i) = 0, lω(j) = 0.  Component Analysis (PCA). Given a voxel grid, this approach estimates the first principal component (PC) which is often aligned with the principal axis of the voxel grid. If the PC is upright (i.e. the orientation of PC is more than 45 • from ground surface), its grasping direction is one of the side grasps (δ 2 , δ 3 , • • • , δ 6 ). Otherwise, its grasping direction is the top grasp δ 1 . In the former case, the voxel grid is transformed as shown in Fig. <ref type="figure">3</ref>, and the wrist orientation is determined by the x, y values of the PC, which are corresponding to the wrist orientation as shown in Fig. <ref type="figure">2b</ref>. This is an example of human engineered approaches. Since the PCA is a purely geometric approach, it does not require a training phase and predicts only the best grasping direction and wrist orientation. 5) FCN, an approach based on a fully connected network which has two hidden layers followed by two dropout layers for regularization. This is a baseline highlighting the performance difference between convolutional network and multi-layer neural network. In this experiment, we evaluate the accuracy of grasp pose prediction in a test dataset. The test dataset is composed of 638 voxel grids from the 10 test objects and their corresponding grasping direction and wrist orientation labels. For each approach, if the chosen grasping direction and wrist orientation Fig. <ref type="figure">6</ref>: Prediction accuracy with respect to noise and occlusions. The prediction accuracies of grasping direction δ and wrist orientation ω for five approaches are reported with different degree of noise and occlusions. We added artificial random noise voxels to the set of test voxel grids. We randomly removed some of consecutive voxel planes to mimic occlusions. The solid lines represent means and the shaded areas depict standard deviation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Grasp Pose Prediction</head><p>belong to the labels, they are regarded as accurate prediction, otherwise inaccurate. As the RAND and PCA approaches only return one hypothesis for a given input, for fair comparison the best hypothesis is chosen for the other approaches and compared with the labels. The grasping accuracy on the test dataset is reported in Table <ref type="table" target="#tab_1">I</ref>. Among the five approaches, the RAND approach reports the worst performance in terms of accuracy. The random choice of grasping direction is slightly better than 25% since the expected chance is 1</p><formula xml:id="formula_3">N δ</formula><p>where N δ = 6 and some objects allow multiple grasping directions due to their symmetry. The PCA works reasonably well, but we noticed that it returns a wrong prediction when the partial voxel grid does not give a clue to its complete shape. The SVM is worse than the PCA, in particular in its wrist orientation. The FCN shows perfect prediction in grasping direction, but it turns out that the FCN was overfitted to top grasping direction δ 1 . Since all examples in both the training and test datasets allow δ 1 direction, it always predicts the top direction as the highest probability rather than considering the side grasps. Moreover, its wrist orientation prediction is the second worst among the five approaches. By comparing the FCN and 3DCNN, our 3D CNN is much more capable of predicting right wrist orientation, although both approaches use deep neural networks. We ascribe this outstanding performance of the 3D CNN to its 3D structure reasoning. Whereas the FCN simply treats the voxel grids as real valued features, the 3DCNN examines the geometric structures of the voxel grids with learned 3D voxel filters. This difference leads to the significant distinction in the prediction accuracy. The PCA is the second best approach among the five approaches, but we will see in the next section how it degrades with noise and occlusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Robustness to Noise and Occlusions</head><p>Since voxel grids are obtained via segmentation, it is common to have unexpected noise, wrong segments, or occlusions. In this section, we compare the robustness of the five approaches with respect to noise and occlusions. To this end, we add artificial noise to voxel grids or randomly remove some of voxels to simulate occlusions. The prediction accuracies of the five approaches with noise and occlusions are reported in Fig. <ref type="figure">6</ref>. For statistically meaningful results, we ran 30 times for each noise level and calculated mean and standard deviation of the average prediction outcome.</p><p>For the grasping direction, both the SVM and PCA approaches are getting more and more inaccurate as the number of noise voxels and the number of occluded voxel planes are increasing. Especially, the PCA is seriously perturbed by the noise voxels. Since the PCA mainly relies on the principal axis of objects to reason about the grasping direction, a few number of noise voxels are critical to the approach. The SVM is relatively less sensitive than the PCA, but the accuracy of the SVM monotonously decreases as noise and occlusions increase. The FCN is not affected at all by either noise or occlusions because the grasping direction by the FCN is overfitted to the top grasp, while the 3DCNN approach is slightly disrupted by occlusions.</p><p>For the wrist orientation, the 3DCNN approach clearly outperforms all other approaches. The FCN is not encouraging for predicting wrist orientation. It consistently performs worse than the SVM baseline. The PCA is seriously affected by noise, and when occlusions are severe its prediction is even worse than guessing randomly, RAND. From this evaluation, we notice that the 3DCNN approach is more robust than the other approaches. We attribute the superior performance of the 3DCNN to the hierarchical structure of CNN wherein the convolution with the learned filters effectively suppress noise voxels and the amalgamation of multi-layer responses enables our approach to predict robustly even with serious occlusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Object Grasping with a Soft Robot Hand</head><p>In this experiment, we run an object grasping experiment in which the goal is to pick up a given object on the table. We placed each object on the table with a random location and pose. The robot system and its experiment setting is shown in Fig. <ref type="figure" target="#fig_1">1</ref>. If the system can grasp and lift the test object for more than 3 seconds, it is regarded as success. If the system cannot grasp the object or the object slips from the hand within 3 seconds, it is counted as failure. While in the previous experiments each approach chose one best grasp pose, in this experiment each approach examines a set of grasp poses whose probability is over a certain threshold value τ p = 0.5. The set of grasp poses is sorted in decreasing order of their probability. The system tries the best grasp pose first to check for feasibility. If the best grasp pose is not feasible due to the kinematic constraints of the robot, it tries the next best pose until it is able to find a valid trajectory plan. We have tried 10 times for each object with varying locations and orientations. Since there are 10 test objects and 5 approaches, shows the effectiveness of our approach, 3DCNN, which achieves 87% of successful grasping for previously unseen objects.</p><p>Fig. <ref type="figure">8</ref>: Successful grasps of the 3DCNN approach. Our approach reliably grasps various objects even if these objects are unknown to the robot. The 3D CNN model generalizes for these previously unseen objects and enables our soft hands to approach to right directions and wrist orientations for grasping.</p><p>Fig. <ref type="figure">9</ref>: Unsuccessful grasps of the RAND approach. Though our soft hands are compliant and adaptable to a certain degree of discrepancy, random trials often result in poor grasping as shown here. It clearly shows that an appropriate grasp pose is crucial even for these compliant soft hands.</p><p>Fig. <ref type="figure" target="#fig_1">10</ref>: Unsuccessful grasps of the 3DCNN approach. For each object, two images depict the pre-grasping and post-grasping situations.</p><p>Although grasping directions and wrist orientations are right, final grasps are unsuccessful due to either the offsets in the gripper locations or challenging object poses.</p><p>the total number of grasping trials for this experiment was 10 × 10 × 5 = 500.</p><p>Fig. <ref type="figure" target="#fig_5">7</ref> presents the grasping success rate of the five approaches on the test objects, and the average grasping accuracy of the approaches are reported in the rightmost bars. Depending on the type and shape of the objects, the grasping rate of these approaches varies. However, the 3DCNN approach clearly outperforms the other approaches in terms of success rate. The 3DCNN approach achieves 87% of successful grasping for the previously unseen objects; the RAND approach shows about 15% chance of successful grasping; The performance of the PCA and SVM are similar, while the FCN shows slight inferior performance. Unlike the previous experiments, this experiment considers further challenges, such as kinematic constraints and workspace limitation of robot arms and feasibility of trajectory plan. As the RAND and PCA return only one grasp pose, they are rather handicapped, if their chosen grasp poses are infeasible due to these constraints. The FCN approach is also limited not only by its inaccurate wrist orientation prediction but also by its overfitted grasping direction, and hence it is easily affected by these constraints. This emphasizes the importance of model capability which is able to generalize to multiple grasping directions, and it turns out that 3DCNN approach is more capable of grasping under these constraints. Some successful grasps of our approach are shown in Fig. <ref type="figure">8</ref>. Our 3D CNN learns partial view invariance from the training data and generalizes to new objects. Moreover, we can see the synergy effect between our CNN grasping prediction and soft hands. Thanks to the compliance of the soft hands, the acquisition region of the soft hands for success grasping is large. This empowers the 3D CNN model to focus on learning the coarsely sampled grasping directions and wrist orientations, not worrying about other grasping relevant parameters, such as detailed shape of objects, minor offset in hand pose, more complex hand control, etc. Although the grasping directions and wrist orientations are coarsely discretized, our flexible soft hands can grasp objects with a discrepancy in orientation. We also notice the importance of guided grasping direction and wrist orientation information for the soft hands. Fig. <ref type="figure">9</ref> shows some unsuccessful grasps of the RAND approach. Even though our soft hands are flexible and compliant, a good enough grasp pose is an important prerequisite for successful grasping. By comparing the accuracies of the 3DCNN and RAND approaches in Fig. <ref type="figure" target="#fig_5">7</ref>, we notice that the 3D CNN enables the soft hands to perform 72% more successful grasping. The failure cases of our approach are presented in Fig. <ref type="figure" target="#fig_1">10</ref>. For each object, two images depict the pre-grasping and post-grasping situations to show how these grasps failed. Please note that grasping directions are correct, but the final grasps are unsuccessful due to either the offsets in the gripper locations or challenging object poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>A deep learning powered grasping approach was presented. A 3D CNN model was trained with the dataset obtained by executing grasping with soft hands. Our soft hands with the 3D CNN model achieved 87% of successful grasping on unknown objects, which outperforms the other compared approaches including one other deep neural network baseline. We noticed the synergy between our CNN grasping algorithm and soft hands. Our compliant soft hands were able to perform reliable grasping with the grasp poses determined by our CNN model, and the grasp prediction by the CNN significantly increased the success rate by about 72% compared to the approach without the 3D CNN-based grasp prediction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :Fig. 3 :</head><label>23</label><figDesc>Fig.2: Grasping directions and wrist orientations. Given an object, our approach discretizes grasping directions to six directions and wrist orientations to four orientations. The total number of grasp orientations is thus 6 × 4 = 24. The grasping directions include both top grasp δ1 as well as side grasps δ2, • • • , δ6. Each wrist orientation corresponds to the principal axis (dotted gray line) of the box-shaped object. The discretization step is 45 • . Although these grasping directions and wrist orientations are quite coarse, our soft hands are compliant enough to adapt to the discrepancy in object orientation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 :</head><label>1</label><figDesc>3D CNN Object Grasping Data: point cloud P, 3D CNN model N Result: the set of grasp poses X ⊂ SE(3) 1: S ← PlanarSegmentation(P) 2: for s ∈ S do 3:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>4</head><label>4</label><figDesc>For each voxel grid, we flipped left and right followed by 14 translations (4 translations in x and y (-2, -1, 1, 2) and 6 translations in z (1, 2, • • • , 6)). In total, 719 × 2 × (1 + 14) = 21, 570 training voxel grids were obtained.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: and testing objects. We use the 10 training objects to train our CNN and evaluate its performance on the 10 test objects. Please note the shape differences between the training and test objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Grasping success rates on real robot. The successful grasping rates of five approaches on the 10 test objects. The plot clearly</figDesc><graphic coords="7,49.20,189.50,62.92,62.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>3D CNN Architecture. Our network is composed of two convolution layers, one max pooling layer, and two dense (fully connected) layers. The input layer is the voxel grid G of 32 × 32 × 32 size, in which each voxel grid has either -1 (unoccupied) or 1 (occupied). The output layer returns the probabilities of N δ and Nω classes. In our problem, N δ = 6 directions and Nω = 4 orientations are considered. Details of convolution and max pooling layers are described in each layer.</figDesc><table><row><cell></cell><cell cols="2">Convolution</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>voxel grid G 32 ⇥ 32 ⇥ 32</cell><cell>32 filters 5 ⇥ 5 ⇥ 5 stride 2</cell><cell>14 ⇥ 14 ⇥ 14</cell><cell>Convolution 32 filters 3 ⇥ 3 ⇥ 3 stride 1</cell><cell>12 ⇥ 12 ⇥ 12</cell><cell>Max pooling 2 ⇥ 2 ⇥ 2 6 ⇥ 6 ⇥ 6</cell><cell>Dense</cell><cell>128</cell><cell>Dense Dense</cell><cell>N</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>N !</cell></row><row><cell>Fig. 4:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Grasping prediction accuracy on the test dataset.</figDesc><table><row><cell></cell><cell>RAND</cell><cell>PCA</cell><cell>SVM</cell><cell>FCN</cell><cell>3DCNN</cell></row><row><cell>Grasping Direction (%)</cell><cell>26.67</cell><cell>96.55</cell><cell>93.09</cell><cell>100.00</cell><cell>97.60</cell></row><row><cell>Wrist Orientation (%)</cell><cell>31.67</cell><cell>97.49</cell><cell>80.84</cell><cell>72.73</cell><cell>99.77</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper was recommended for publication by Editor Han Ding upon evaluation of the Associate Editor and Reviewers' comments. This work was funded in part by the Boeing Company, NSF IIS 1226883, and NSF Graduate Research Fellowship 1122374.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Data-Driven Grasp Synthesis-A Survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Asfour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Robotics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="289" to="309" />
			<date type="published" when="2014-04">Apr. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Graspit! A versatile simulator for robotic grasping</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics Automation Magazine</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="110" to="122" />
			<date type="published" when="2004-12">Dec. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep learning for detecting robotic grasps</title>
		<author>
			<persName><forename type="first">I</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l J. Robotics Research</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="705" to="724" />
			<date type="published" when="2015-04">Apr. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A novel type of compliant and underactuated robotic hand for dexterous grasping</title>
		<author>
			<persName><forename type="first">R</forename><surname>Deimel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Brock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l J. Robotics Research</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="161" to="185" />
			<date type="published" when="2016-01">Jan. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Using Vision for Pre-and Post-Grasping Object Localization for Soft Hands</title>
		<author>
			<persName><forename type="first">C</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Delpreto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Symposium on Experimental Robotics (ISER)</title>
		<meeting>Int&apos;l Symposium on Experimental Robotics (ISER)<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning dexterous manipulation for a soft robotic hand from human demonstrations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Eppner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/RSJ Int&apos;l Conf. Intelligent Robots Systems (IROS)</title>
		<meeting>IEEE/RSJ Int&apos;l Conf. Intelligent Robots Systems (IROS)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3786" to="3793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An overview of 3d object grasp synthesis algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sahbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>El-Khoury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bidaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="326" to="336" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Data-driven grasping with partial sensor data</title>
		<author>
			<persName><forename type="first">C</forename><surname>Goldfeder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ciocarlie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peretzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/RSJ Int&apos;l Conf. Intelligent Robots Systems (IROS)</title>
		<meeting>IEEE/RSJ Int&apos;l Conf. Intelligent Robots Systems (IROS)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1278" to="1283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pose error robust grasping from contact wrench space metrics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weisz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Robotics Automation (ICRA)</title>
		<meeting>IEEE Int&apos;l Conf. Robotics Automation (ICRA)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="557" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The MOPED framework: Object recognition and pose estimation for manipulation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Collet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Srinivasa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l J. Robotics Research</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1284" to="1306" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">RGB-D object pose estimation in unstructured environments</title>
		<author>
			<persName><forename type="first">C</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">I</forename><surname>Christensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="595" to="613" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Selection of robot pre-grasps using boxbased shape approximation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Huebner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/RSJ Int&apos;l Conf. Intelligent Robots Systems (IROS)</title>
		<meeting>IEEE/RSJ Int&apos;l Conf. Intelligent Robots Systems (IROS)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1765" to="1770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unions of balls for shape approximation in robot grasping</title>
		<author>
			<persName><forename type="first">M</forename><surname>Przybylski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Asfour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dillmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/RSJ Int&apos;l Conf. Intelligent Robots Systems (IROS)</title>
		<meeting>IEEE/RSJ Int&apos;l Conf. Intelligent Robots Systems (IROS)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1592" to="1599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Robotic grasping of novel objects using vision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Driemeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l J. Robotics Research</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="173" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Leveraging big data for grasp planning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kappler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Robotics Automation (ICRA)</title>
		<meeting>IEEE Int&apos;l Conf. Robotics Automation (ICRA)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4304" to="4311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generating multi-fingered robotic grasps via deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Varley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weisz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/RSJ Int&apos;l Conf. Intelligent Robots Systems (IROS)</title>
		<meeting>IEEE/RSJ Int&apos;l Conf. Intelligent Robots Systems (IROS)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4415" to="4420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dex-Net 1.0: A Cloud-Based Network of 3d Objects for Robust Grasp Planning Using a Multi-Armed Bandit Model with Correlated Rewards</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mahler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">T</forename><surname>Pokorny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Roderick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Laskey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kohlhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kuffner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Robotics Automation (ICRA)</title>
		<meeting>IEEE Int&apos;l Conf. Robotics Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Supersizing Self-supervision: Learning to Grasp from 50k Tries and 700 Robot Hours</title>
		<author>
			<persName><forename type="first">L</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Robotics Automation (ICRA)</title>
		<meeting>IEEE Int&apos;l Conf. Robotics Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">High precision grasp pose detection in dense clutter</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gualtieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Pas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/RSJ Int&apos;l Conf. Intelligent Robots Systems (IROS)</title>
		<meeting>IEEE/RSJ Int&apos;l Conf. Intelligent Robots Systems (IROS)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep learning a grasp function for grasping under gripper pose uncertainty</title>
		<author>
			<persName><forename type="first">E</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/RSJ Int&apos;l Conf. Intelligent Robots Systems (IROS)</title>
		<meeting>IEEE/RSJ Int&apos;l Conf. Intelligent Robots Systems (IROS)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pastor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Quillen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Symposium on Experimental Robotics (ISER)</title>
		<meeting>Int&apos;l Symposium on Experimental Robotics (ISER)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dex-Net 2.0: Deep Learning to Plan Robust Grasps with Synthetic Point Clouds and Analytic Grasp Metrics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mahler</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Niyaz</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Laskey</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Doan</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Ojea</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Goldberg</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Robotics: Science and Systems</title>
		<meeting>Robotics: Science and Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">3d ShapeNets: A deep representation for volumetric shapes</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015-06">June 2015</date>
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">VoxNet: A 3D convolutional neural network for real-time object recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/RSJ Int&apos;l Conf. Intelligent Robots Systems (IROS)</title>
		<meeting>IEEE/RSJ Int&apos;l Conf. Intelligent Robots Systems (IROS)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The highly adaptive SDM hand: Design and performance evaluation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Howe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l J. Robotics Research</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="585" to="597" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Haptic identification of objects using a modular soft robotic gripper</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Homberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Katzschmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Dogar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/RSJ Int&apos;l Conf. Intelligent Robots Systems (IROS)</title>
		<meeting>IEEE/RSJ Int&apos;l Conf. Intelligent Robots Systems (IROS)</meeting>
		<imprint>
			<date type="published" when="2015-09">Sept. 2015</date>
			<biblScope unit="page" from="1698" to="1705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Human-guided grasp measures improve grasp robustness on physical robot</title>
		<author>
			<persName><forename type="first">R</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Brook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsuoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Robotics Automation (ICRA)</title>
		<meeting>IEEE Int&apos;l Conf. Robotics Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="2010-05">May 2010</date>
			<biblScope unit="page" from="2294" to="2301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient Processing of Deep Neural Networks: A Tutorial and Survey</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2295" to="2329" />
			<date type="published" when="2017-12">Dec. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Object partitioning using local convexity</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Christoph</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schoeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Papon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Worgotter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="304" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cluttered scene segmentation using the symmetry constraint</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ecins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fermller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Aloimonos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Robotics Automation (ICRA)</title>
		<meeting>IEEE Int&apos;l Conf. Robotics Automation (ICRA)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2271" to="2278" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
