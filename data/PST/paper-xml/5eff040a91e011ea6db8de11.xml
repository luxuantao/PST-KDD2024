<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DAPPLE: A Pipelined Data Parallel Approach for Training Large Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shiqing</forename><surname>Fan</surname></persName>
							<email>shiqing.fsq@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Rong</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chen</forename><surname>Meng</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zongyan</forename><surname>Cao</surname></persName>
							<email>zongyan.cao@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Siyu</forename><surname>Wang</surname></persName>
							<email>siyu.wsy@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhen</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chuan</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The University of Hong Kong</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guoping</forename><surname>Long</surname></persName>
							<email>guopinglong.lgp@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lixue</forename><surname>Xia</surname></persName>
							<email>lixue.xlx@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lansong</forename><surname>Diao</surname></persName>
							<email>lansong.dls@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaoyong</forename><surname>Liu</surname></persName>
							<email>xiaoyong.liu@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DAPPLE: A Pipelined Data Parallel Approach for Training Large Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>deep learning</term>
					<term>data parallelism</term>
					<term>pipeline parallelism</term>
					<term>hybrid parallelism</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>It is a challenging task to train large DNN models on sophisticated GPU platforms with diversified interconnect capabilities. Recently, pipelined training has been proposed as an effective approach for improving device utilization. However, there are still several tricky issues to address: improving computing efficiency while ensuring convergence, and reducing memory usage without incurring additional computing costs. We propose DAPPLE, a synchronous training framework which combines data parallelism and pipeline parallelism for large DNN models. It features a novel parallelization strategy planner to solve the partition and placement problems, and explores the optimal hybrid strategies of data and pipeline parallelism. We also propose a new runtime scheduling algorithm to reduce device memory usage, which is orthogonal to re-computation approach and does not come at the expense of training throughput. Experiments show that DAPPLE planner consistently outperforms strategies generated by PipeDreams planner by up to 3.23× speedup under synchronous training scenarios, and DAPPLE runtime outperforms GPipe by 1.6× speedup of training throughput and saves 12% of memory consumption at the same time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The artificial intelligence research community has a long history of harnessing computing power to achieve significant breakthroughs <ref type="bibr" target="#b0">[1]</ref>. For deep learning, a trend has been increasing the model scale up to the limit of modern AI hardware. Many state-of-the-art DNN models (e.g., NLP <ref type="bibr" target="#b1">[2]</ref>, Internet scale E-commerce search/recommendation systems <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>) have billions of parameters, demanding tens to hundreds of GBs of device memory for training. A critical challenge is how to train such large DNN models on hardware accelerators, such as GPUs, with diversified interconnect capabilities.</p><p>A common approach is sychronous data parallel (DP) training. Multiple workers each performs complete model computation and synchronizes gradients periodically to ensure proper model convergence. DP is simple to implement and friendly in terms of load balance, but the gradients sychronization overhead can be a major factor preventing linear scalability. While the performance issue can be alleviated by optimizations such as local gradients accumulation <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b6">[7]</ref> or computation and communication overlap techniques <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, aggressive DP typically requires large training batch sizes, which makes model tuning harder. More importantly, DP is not feasible once the parameter scale of the model exceeds the memory limit of a single device.</p><p>Recently, pipeline parallelism <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b11">[12]</ref> has been proposed as a promising approach for training large DNN models. The idea is to partition model layers into multiple groups (stages) and place them on a set of inter-connected devices.</p><p>During training, each input batch is further divided into multiple micro-batches, which are scheduled to run over multiple devices in a pipelined manner. Prior research on pipeline training generally falls into two categories. One is on optimizing pipeline parallelism for synchronous training <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. This approach requires necessary gradients synchronizations between adjacent training iterations to ensure convergence. At runtime, it schedules as many concurrent pipe stages as possible in order to maximize device utilization. In practice, this scheduling policy can incur notable peak memory consumption. To remedy this issue, re-computation <ref type="bibr" target="#b12">[13]</ref> can be introduced to trade redundant computation costs for reduced memory usage. Re-computation works by checkpointing nodes in the computation graph defined by user model, and re-computing the parts of the graph in between those nodes during backpropagation. The other category is asynchronous(async) pipeline training <ref type="bibr" target="#b11">[12]</ref>. This manner inserts mini-batches into pipeline continuously and discards the original sync operations to achieve maximum throughput.</p><p>Although these efforts have made good contributions to advance pipelined training techniques, they have some serious limitations. While PipeDream <ref type="bibr" target="#b13">[14]</ref> made progress in improving the time-to-accuracy for some benchmarks with async pipeline parallelism, async training is not a common practice in important industry application domains due to convergence concerns. This is reflected in a characterization study <ref type="bibr" target="#b14">[15]</ref> of widely diversified and fast evolving workloads in industry scale clusters. In addition, the async approach requires the storage of multiple versions of model parameters. This, while friendly for increasing parallelism, further exacerbates the already critical memory consumption issue. As for synchronous training, current approach <ref type="bibr" target="#b9">[10]</ref> still requires notable memory consumption, because no backward processing(BW) can be scheduled until the forward processing(FW) of all micro-batches is finished. The intermediate results of some micro-batches in FW need to be stored in the memory (for arXiv:2007.01045v1 [cs.DC] 2 Jul 2020 corresponding BW's usage later) while the devices are busy with FW of some other micro-batches. GPipe <ref type="bibr" target="#b9">[10]</ref> proposes to discard some intermediate results to free the memory and re-computes them during BW when needed. But this may introduce additional ∼ 20% re-computation overhead <ref type="bibr" target="#b15">[16]</ref>.</p><p>In this paper, we propose DAPPLE, a distributed training scheme which combines pipeline parallelism and data parallelism to ensure both training convergence and memory efficiency. DAPPLE adopts synchronous training to guarantee convergence, while avoiding the storage of multiple versions of parameters in async approach. Specifically, we address two design challenges. The first challenge is how to determine an optimal parallelization strategy given model structure and hardware configurations. The optimal strategy refers to the one where the execution time of a single global step theoretically reaches the minimum for given resources. The target optimization space includes DP, pipelined parallelism, and hybrid approaches combining both. Current state-of-theart pipeline partitioning algorithm <ref type="bibr" target="#b11">[12]</ref> is not able to be applied to synchronous training effectively. Some other work <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b15">[16]</ref> relies on empirical and manual optimizations, and still lacks consideration of some parallelism dimensions. We introduce a synchronous pipeline planner, which generates optimal parallelization strategies automatically by minimizing execution time of training iterations. Our planner combines pipeline and data parallelism (via stage-level replication) together while partitioning layers into multiple stages. Besides pipeline planning, for those models that can fit into a single device and with high computation/communication ratio, the planner is also capable of producing DP strategies directly for runtime execution. Furthermore, the planner takes both memory usage and interconnect topology constraints into consideration when assigning pipeline stages to devices. The assignment of computation tasks to devices is critical for distributed training performance in hardware environments with complex interconnect configurations. In DAPPLE, three topology-aware device assignment mechanisms are defined and incorporated into the pipeline partitioning algorithm.</p><p>The second challenge is how to schedule pipeline stage computations, in order to achieve a balance among parallelism, memory consumption and execution efficiency. We introduce DAPPLE schedule, a novel pipeline stage scheduling algorithm which achieves decent execution efficiency with reasonably low peak memory consumption. A key feature of our algorithm is to schedule forward and backward stages in a deterministic and interleaved manner to release the memory of each pipeline task as early as possible.</p><p>We evaluate DAPPLE on three representative application domains, namely image classification, machine translation and language modeling. For all benchmarks, experiments show that our planner can consistently produce optimal hybrid parallelization strategies combining data and pipeline parallelism on three typical GPU hardware environments in industry, i.e. hierarchical NVLink + Ethernet, 25 Gbps and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. MOTIVATION AND DAPPLE OVERVIEW</head><p>We consider pipelines training only if DP optimizations <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b19">[20]</ref> are unable to achieve satisfactory efficiency, or the model size is too large to fit in a device with a minimum required batch size. In this section, we summarize key design issues in synchronous training with parallelism and motivate our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Synchronous Pipeline Training Efficiency</head><p>Pipeline training introduces explicit data dependencies between consecutive stages (devices). A common approach to keep all stages busy is to split the training batch into multiple micro-batches <ref type="bibr" target="#b9">[10]</ref>. These micro-batches are scheduled in the pipelined manner to be executed on different devices concurrently.Note that activation communication (comm) overhead matters in synchronous training. Here we incorporate comm as a special pipeline stage for our analysis. We define pipeline efficiency as average GPU utilization of all devices in the pipeline. The pipeline efficiency is 1 1+P <ref type="bibr" target="#b9">[10]</ref>, where P = (1+α) (S−1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M</head><p>. S, M and α are number of stages, number of equal micro-batches and communication-to-computation ratio, respectively. Given a pipeline partitioning scheme (namely fixed S and α), the larger the number of micro-batches M , the higher the pipeline efficiency. Similarly, the smaller S, the higher the efficiency for fixed α and M ,.</p><p>There are efforts to improve synchronous pipelines with micro-batch scheduling <ref type="bibr" target="#b9">[10]</ref>, which suffer from two issues.</p><p>(1) Extra memory consumption. State-of-the-art approach injects all micro-batches consecutively into the first stage of FW. Then the computed activations are the input to BW. Execution states (i.e. activations) have to be kept in memory for all micro-batches until their corresponding BW operations start. Therefore, while more injected micro-batches may imply higher efficiency, the memory limit throttles the number of micro-batches allowed.</p><p>(2) Redundant computations. State-of-the-art approach may adopt activation re-computation to reduce peak memory consumption <ref type="bibr" target="#b12">[13]</ref>, i.e. discarding some activations during the FW phase, and recomputing them in the BW phase when needed. However, redundant computations come with extra costs. It is reported that re-computation can consume approximately 20% more execution time <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Pipeline Planning</head><p>To maximize resource utilization and training throughput for pipelined training, it is crucial to find a good strategy for partitioning stages and mapping stages to devices. We refer to the stage partitioning and device mapping problem as pipeline planning problem. Current state-of-the-art planning algorithm <ref type="bibr" target="#b13">[14]</ref> may suffer from the following issues.</p><p>First, it does not consider synchronous pipeline training. Synchronous pipeline is very important as convergence is the prerequisite of training. Compared against async pipeline training, an additional step is needed for sync pipeline training at the end of all micro-batches to synchronize parameter updates. In more generic case where a stage may be replicated on multiple devices, there exists additional gradients synchronizations (AllReduce) overheads before parameter updates. Current pipeline planner does not take such overhead into consideration and thus can not accurately model the end-toend pipeline training time.</p><p>Second, previous approach does not consider the impact of the number of stages S, which is important for synchronous planning. As discussed in the previous subsection, with fixed micro-batches M and comm overhead ratio α, the fewer the number of stages(S) , the higher the pipeline efficiency.</p><p>Finally, uneven stage partitions have not been well studied in existing works. We show in Section IV that uneven stage partitions can sometimes produce better training performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The DAPPLE Approach</head><p>We propose the DAPPLE framework to address aforementioned scheduling and planning challenges with synchronous training. Fig. <ref type="figure" target="#fig_0">1</ref> shows high-level workflow of DAPPLE. It features a profiler, a planner and a runtime system. Overall, DAPPLE profiler takes a user's DNN model as input, and profiles execution time, activation sizes and parameter sizes   for each layer. Taking profiling results as input, DAPPLE planner generates an optimized (hybrid) parallelization plan on a given global batch size. In terms of the execution time, both DAPPLE profiler and planner are offline and can be completed within a few seconds for all our benckmark models (Table <ref type="table" target="#tab_6">II</ref>). Finally DAPPLE runtime takes the planner's results, and transforms the original model graph into a pipelined parallel graph. At this stage, global batch size is further split into multiple micro-batches and then been scheduled for execution by DAPPLE runtime. More specifically, the planner aims at minimizing the endto-end execution time of one training iteration. This module is responsible for stage partition, stage replication and device assignment and generates optimal parallelization plans. In particular, the device assignment process is aware of the hardware topology, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>We also explore the mapping of a single stage onto multiple devices. With the replication of pipeline stages on multiple devices, DAPPLE processes training with the hybrid of data and pipeline parallelism. In practice, this hybrid strategy can exploit hierarchical interconnects effectively. Fig. <ref type="figure" target="#fig_1">2</ref> gives an example where a model is partitioned into two stages and each stage is replicated on four devices within the same server(NVLink connections within server), while inter-stage communication goes over the Ethernet. This mapping exploits workload characteristics (Table <ref type="table" target="#tab_2">I</ref>) by leveraging the highspeed NVLink for heavy gradients sync, while using the slow Ethernet bandwidth for small activations communication. We discuss details of our planner in Section IV.</p><p>Finally, the DAPPLE runtime involves a carefully designed scheduling algorithm. Unlike existing pipeline scheduling algorithms <ref type="bibr" target="#b20">[21]</ref>, DAPPLE scheduler (Section III) significantly reduces the need for re-computation, retains a reasonable level of memory consumption, while saturates the pipeline with enough micro-batches to keep all devices busy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DAPPLE SCHEDULE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Limitations of GPipe Schedule</head><p>To improve pipeline training efficiency, GPipe <ref type="bibr" target="#b9">[10]</ref> proposes to split global batch into multiple micro-batches and injects them into the pipeline concurrently (Fig. <ref type="figure">3 (a)</ref>). However, this scheduling pattern alone is not memory-friendly and will not scale well with large batch. The activations produced by forward tasks have to be kept for all micro-batches until corresponding backward tasks start, thus leads to the memory demand to be proportional (O(M )) to the number of concurrently scheduled micro-batches (M ). GPipe adopts recomputation to save memory while brings approximately 20% extra computation. In DAPPLE, we propose early backward scheduling to reduce memory consumptions while achieving good pipeline training efficiency(Fig. <ref type="figure">3 (b)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Early backward scheduling</head><p>The main idea is to schedule backward tasks(BW) earlier and hence free the memory used for storing activations produced by corresponding forward tasks(FW). Fig. <ref type="figure">3(b</ref>) shows DAPPLE's scheduling mechanism, compared to GPipe in Fig. <ref type="figure">3 (a)</ref>. Firstly, instead of injecting all M micro-batches at once, we propose to inject K micro-batches (K &lt; M ) at the beginning to release memory pressure while retaining high pipeline efficiency. Secondly, we schedule one FW of a microbatch followed by one BW strictly to guarantee that BW can be scheduled earlier. Fig. <ref type="figure">3 (c</ref>) shows how the memory consumptions change over time in GPipe and DAPPLE. At the beginning, the memory usage in DAPPLE increases with time and is the same as GPipe's until K micro-batches are injected, then it reaches the maximum due to the early BW scheduling. Specifically, with strictly controlling the execution order of FW and BW, the occupied memory for activations produced by the FW of a micro-batch will be freed after the corresponding BW so that it can be reused by the next injected micro-batch. In comparison, GPipe's peak memory consumptions increases continuously and has no opportunity for early release. Moreover, DAPPLE does not sacrifice in pipeline training efficiency. Actually, DAPPLE introduces the exact same bubble time as GPipe when given the same stage partition, micro-batches and device mapping. We will present the details in section V-C. Note that the combination of early backward scheduling and re-combination allows further exploitation in memory usage. We present performance comparisons of DAPPLE and GPipe in section VI-D.</p><p>IV. DAPPLE PLANNER DAPPLE Planner generates an optimal hybrid parallelism execution plan given profiling results of DAPPLE profiler, hardware configurations and a global training batch size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Optimization Objective</head><p>For synchronous training, we use the execution time of a single global batch as our performance metric, which we call pipeline latency. The optimization objective is to minimize pipeline latency L with the consideration of all solution spaces of data/pipeline parallelism.</p><p>In synchronous pipelined training, computations and crossstage communication of all stages usually form a trapezoid, but not diamond formed by normal pipelines without backward phase. Fig. <ref type="figure">4</ref> shows a pipelined training example with well designed task scheduling arrangement. We use blue blocks for forward computation, and green ones for backwards, with numbers in them being the corresponding micro-batch index. Network communications are arranged as individual stages. Gray blocks are bubbles.</p><p>We denote the stage with the least bubble overhead as pivot stage, which will be the dominant factor in calculating pipeline latency L. Let its stage id be Q. We will discuss about how to choose pivot stage later.</p><p>A pipeline training iteration consists of warmup phase, steady phase and ending phase, as shown in Fig. <ref type="figure">4</ref> in which pivot stage is the last stage. Pivot stage dominates steady phase. We call the execution period from the start to pivot stage's first micro-batch as warmup phase in a pipeline iteration, the period from pivot stage's last micro-batch to the end as ending phase. Pipeline latency is the sum of these three phases. The optimization objective for estimating L is as follows:</p><formula xml:id="formula_0">T w = Q s=0 F s T s = (M − 1) × (F Q + B Q ) T e = S−1 max s=0 (AR(P s , g s ) + − s a=Q B a s ≥ Q Q a=s B a s &lt; Q )<label>(1)</label></formula><formula xml:id="formula_1">L = T w + T s + T e<label>(2)</label></formula><p>T w denotes the execution time of warmup phase, which is the sum of forward execution time of stages till Q for one micro-batch. T s denotes the steady phase, which includes both forward and backward time of stage Q for all micro-batches except for the one contributing to warmup and ending phase. T e corresponds to the ending phase. T e includes allreduce overhead and thus considers stages both before and after Q. Note that some stages before Q may contribute to T e with allreduce cost. M , S, F s and B s denote the total number of micro-batches, the number of stages (computation stages + network stages), forward and backward computation time of stage s, respectively. AR(P s , g s ) represents the gradients synchronization (AllReduce) time for stage s, with its parameter set P s on the device set g s .</p><p>Note we consider inter-stage communication as an independent stage alongside the computation stages. The AllReduce time AR(P s , g s ) is always 0 for communication stages. Moreover, we define F s and B s for a communication stages as its following forward and backward communication time.</p><p>In practice, synchronous pipelines in some cases include bubbles in stage Q, which may contribute a small fraction of additional delay to time. This objective does not model those internal bubbles, and thus is an approximation to the true pipeline latency. But it works practically very well for all our benchmarks (Section VI).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Device Assignment</head><p>Device assignment affects communication efficiency and computing resource utilization. Previous work <ref type="bibr" target="#b11">[12]</ref> uses hierarchical planning and works well for asynchronous training. However, it lacks consideration of synchronous pipeline training, in which the latency of the whole pipeline, rather than of a single stage, matters to overall performance. It cannot be used  to efficiently estimate the whole pipeline latency. Meanwhile, it does not allow stages to be placed on arbitrary devices. Our approach essentially allows a specific stage to be mapped to any set of devices, and therefore is able to handle more placement cases, at a reasonable searching cost.</p><p>Instead of enumerating all possibilities of placement plans using brute force, we designed three policies (Fig. <ref type="figure" target="#fig_3">5</ref>), and explore their compositions to form the final placement plan.</p><p>a) Fresh First: allocate GPUs from a fresh machine. It tends to put tasks within a stage onto the same machine, which can leverage high-speed NVLink <ref type="bibr" target="#b21">[22]</ref> for intra-stage communication. A problem of this policy is that, it can cause fragmentation if the stage cannot fully occupy the machine.</p><p>b) Append First: allocate from machines that already have GPUs occupied. It helps to reduce fragmentation. It also largely implies that the stage is likely to be within the same machine.</p><p>c) Scatter First: try to use available GPUs equally from all used machines, or use GPUs equally from all machines if they are all fresh. It is suitable for those stages that have negligible weights compared to activation sizes (less intrastage communication). This policy could also serve as an intermediate state to allocate GPU with minimal fragmentation.</p><p>The overall device placement policies reduce search space effectively down to less than O(2 S ), while retaining room for potential performance gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Planning Algorithm</head><p>Our planning algorithm use Dynamic Programming to find the optimal partition, replication and placement strategy, so that the pipeline latency L is minimized. Here we first present how to update the pivot stage ID Q along the planning process, and then the formulation of our algorithm 1) Determining The Pivot Stage Q: It is vital to select a proper pivot stage Q for the estimation of L. The insight is to find the stage with minimum bubbles, which dominates steady phase. We use a heuristic to determine Q (formula 3).</p><formula xml:id="formula_2">Q = arg 0 max s=S−1 max T Q st + Q−1 s =s+1 (F s + B s ), T s st (3)</formula><p>The initial Q is set to S − 1. DAPPLE updates Q iteratively from stage S − 1 to stage 0 according to formula 3. T j st = (M − 1) × (F j + B j ) means the duration of steady phase, </p><formula xml:id="formula_3">(G -m -m') GPUs</formula><p>Fig. <ref type="figure">6</ref>: Planning process for j'.</p><p>without bubbles, suppose pivot stage is j. For a stage s &lt; Q, if T s st is larger than the sum of T Q st and corresponding forward/backward costs between s and current Q, it means the steady phase will have less bubbles if pivot stage is set as s other than current Q. Q will then be updated to s.</p><p>2) Algorithm Formulation: We define the estimated pipeline latency T P L (j, m, g) as the subproblem, for which we have planned the strategy for the first j layers using m GPUs (with device id set g). The unplanned layers forms the last stage and replicates on the other (G − m) GPUs. Our objective is to solve for</p><formula xml:id="formula_4">T P L (N, G, G), G = {0, 1, ..., G − 1}.</formula><p>N , G and G denote the number of layers, number of GPUs and GPU set, respectively. Formula 4 describes the algorithm. T P L (j, m, g) (4) Fig. <ref type="figure">6</ref> describes the iterative planning process. Suppose we have already planned for the first j (0 ≤ j &lt; N ) layers and have the estimation T P L (j, m, g) as pipeline latency. The layers after j forms a stage s . Meanwhile, we get the optimal Q for current strategy along with the cost of F Q and B Q for stage Q. Next step, we try to add one more partition in stage s , supposing after layer j (j &lt; j ≤ N ), and split s into two new stages s 1 and s 2 . We assign m GPUs for s 1 and (G−m−m ) GPUs for s 2 , and estimate T P L (j , m+m , g+g ) according to formula 5. Note DAPPLE enumerates the three strategies in section IV-B for device placement of stage s 1 .</p><formula xml:id="formula_5">T P L (N, G, G) = min</formula><formula xml:id="formula_6">T P L (j , m + m , g + g ) = L<label>(5)</label></formula><p>Here, L is the same with that in formula 2. The key for the estimation of L in formula 5 is to find Q of subproblem T P L (j , m+m , g +g ). In the sample in Fig. <ref type="figure">6</ref>, we get Q j for T P L (j, m, g). We apply formula 3 to get Q j for T P L (j, m + m , g + g ) with the help of Q j : if Q j is not s , we do not need to iterate all stages before j, but use Q j for all stages before layer j instead in the iterative process.</p><p>Along the above process, we record the current best split, replication and placement for each point in our solution space using memorized search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Contributions over previous work</head><p>Previous works on pipeline planning includes PipeDream <ref type="bibr" target="#b11">[12]</ref> (for asynchronous training) and torchgpipe <ref type="bibr" target="#b22">[23]</ref>, a community implementation of GPipe <ref type="bibr" target="#b9">[10]</ref> which uses "Block Partitioning of Sequences" <ref type="bibr" target="#b23">[24]</ref>. Both aim to balance the workload across all GPUs. While this idea works good in PipeDream's asynchronous scenarios and gives reasonable solutions under GPipe's synchronous pipeline for its micro-batch arrangement, we found that our micro-batch arrangement could achieve even higher performance by 1) intentionally preferring slightly uneven partitioning with fewer stages, and 2) exploring a broader solution space of device assignment. The following sections highlight our contributions of planning for hybrid parallelism. The resulting strategies and performance gain on real-world models will be demonstrated in Section VI-F. 1) Uneven Pipeline Partitioning with Fewer Stages: In synchronous Pipeline Parallelism scenarios, we found two insights that could provide an additional performance improvements. The first one is to partition the model into as few stages as possible to minimize the bubble overhead under the same number of micro-batches. This conclusion is also mentioned in GPipe. The second one is that partitioning the model in a slightly uneven way yields much higher performance than a perfectly even split, like the example in Fig. <ref type="figure" target="#fig_5">7</ref>.</p><p>2) Versatile Device Placement: DAPPLE device assignment strategy covers a broad solution space for stage placement, and is a strict superset of PipeDream's hierarchical recursive partitioning approach. This allows us to handle various real world models. For example, for models that have layers with huge activations compared to their weights, DAPPLE allows such a layer to be replicated across multiple machines (Scatter First) to utilize high-speed NVLink for activation communication and low-speed Ethernet for AllReduce.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DAPPLE RUNTIME A. Overview</head><p>We design and implement DAPPLE runtime in Tensorflow <ref type="bibr" target="#b24">[25]</ref> (TF) 1.12, which employs a graph based execution paradigm. As common practices, TF takes a precise and complete computation graph (DAG), schedules and executes graph nodes respecting all data/control dependencies.</p><p>DAPPLE runtime takes a user model and its planning results as input, transforms the model graph into a pipelined parallel graph and executes on multiple distributed devices. It first builds forward/backward graphs separately for each pipe stage. Then additional split/concat nodes are introduced between adjacent stages for activation communication. Finally, it builds a subgraph to perform weights update for synchronous training. This step is replication aware, meaning it will generate different graphs with or without device replication of stages. We leverage control dependences in TF to enforce extra execution orders among forward/backward stage computations. Section V-B presents how to build basic TF graph units for a single micro-batch. Section V-C discusses how to chain multiple such units using control dependencies to facilitate DAPPLE execution. B. Building Micro-batch Units 1) Forward/Backward Stages: In order to enforce execution orders with control dependencies between stages, we need to build forward and backward graphs stage by stage to deal with the boundary output tensors such as activations.</p><p>Specifically, we first construct the forward graph of each stage in sequence and record the boundary tensors. No backward graphs should be built until all forward graphs are ready. Second, backward graphs will be built in reverse order for each stage.</p><p>2) Cross Stage Communication: DAPPLE replicates some stages such that the number of nodes running a stage can be different between adjacent stages, and the communication patterns between them are different from straight pipeline design. We introduce special split-concat operations between these stages.</p><p>Fig. <ref type="figure" target="#fig_6">8</ref>(a) shows the replication in DAPPLE for a 2-stage pipeline, whose first stage consumes twice as much time as the second stage for a micro-batch and thus is replicated on two devices. For the first stage, we split the micro-batch further into 2 even slices, and assign each to a device. An alternative approach <ref type="bibr" target="#b11">[12]</ref> (Fig. <ref type="figure" target="#fig_6">8(b)</ref>) is not to split, but to schedule an entire micro-batch to two devices in round robin manner. However, the second approach has lower pipeline efficiency due to tail effect <ref type="bibr" target="#b25">[26]</ref>. Though the second approach does not involve extra split-concat operations, the overhead of tail effect is larger than split-concat in practice. We hence use the first approach with large enough micro-batch size setting to ensure device efficiency.</p><p>The split-concat operations include one-to-many, many-toone and many-to-many communication. We need split for one-to-many(Fig. <ref type="figure">9(b</ref>)), that is, splitting the micro-batch into even slices and sending each slice to a device in the next stage. We need concat for many-to-one(Fig. <ref type="figure">9(c)</ref>), where all slices should be concatenated from the previous stage and fed into the device in the next stage. For many-to-many(Fig. <ref type="figure">9(d)</ref>) we need both split and concat of micro-batch slices to connect adjacent stages. If two adjacent stages have the same replication count, no split-concat is needed.</p><p>3) Synchronous Weights Update: Weights updating in DAP-PLE is different with naive training as there are multiple micro-batches injected concurrently. Meanwhile, the replication makes weights updating more complex. As is shown in Fig. <ref type="figure" target="#fig_0">10</ref>, each device produces and accumulates gradients for all micro-batches. There is an AllReduce operation to synchronize gradients among all replicas, if exists. A normal Apply operation updates weights with averaged gradients eventually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Micro-batch Unit Scheduling</head><p>The early backward scheduling strikes a trade-off between micro-batch level parallelism and peak memory consumption: feeding more micro-batches into pipeline at once implies higher parallelism, but may lead to more memory usage.</p><p>DAPPLE scheduler enforces special execution orders between micro-batches to reduce memory usage. For the first stage, we suppose K micro-batches are scheduled concurrently at the beginning for forward computation. Specifically, K i is the number of scheduled micro-batches at the beginning for stage i. The overall execution follows a round robin order with interleaving FW and BW.</p><p>We realize the scheduler with control dependency edges in TF. Fig. <ref type="figure" target="#fig_0">11</ref> shows how up to three micro-batches are connected via control dependencies to implement the schedule for a two stage pipeline. Control dependency is not necessary when there is only one micro-batch (Fig. <ref type="figure" target="#fig_0">11(a)</ref>). With two micro-batches (Fig. <ref type="figure" target="#fig_0">11(b</ref>)), two control edges are introduced. The control edge between B0 and F 1 in stage 1 is to form the round robin order of FW and BW. The early execution of B0 helps to free memory of F 0 and B0 in stage 1, which can be reused in following tasks. The edge between F 0 and F 1 in stage 0 is to enforce the order that micro-batch 1 is strictly executed after micro-batch 0, thus the backward of microbatch 0 can be executed earlier and its corresponding memory can be freed earlier. In practice, F 0 and F 1 are typically large chunks of computations. The lack of parallelism between F 0 and F 1 does not affect the gain of reducing memory usage.</p><p>The situation with three micro-batches (Fig. <ref type="figure" target="#fig_0">11(c</ref>)) is the same. An appropriate K i is essential as it indicates the peak memory consumption for stage i. There are two primary factors for deciding K i : memory demand for one micro-batch execution, and the ratio between communication latency and the average FW/BW computation time (referred as activation communication ratio, ACR in short). The former determines how many forward batches can be scheduled concurrently at most. We define the maximum number of micro-batches supported by the device memory as D; Lower ACR means less warm up forward batches K i (smaller K i ) are enough to saturate the pipeline. While notable ACR means larger K i is necessary.</p><p>We implement two policies to set K i in practice. Policy A (P A ): K i = min(S − i, D). P A works well when ACR is small, i.e. the impact of cross stage communication overhead is negligible. Policy B (P B ):</p><formula xml:id="formula_7">K i = min(2 * (S − i) − 1, D).</formula><p>Here we schedule twice the number of forward micro-batches than P A . The underlying intuition is that in some workloads, the cross stage communication overhead is comparable with forward/backward computations and thus more micro-batches is needed to saturate the pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Setup</head><p>Benchmarks. Table II summarizes all the six representative DNN models that we use as benchmarks in this section. The datasets applied for the three tasks are WMT16 En-De <ref type="bibr" target="#b26">[27]</ref>, SQuAD2.0 <ref type="bibr" target="#b27">[28]</ref> and ImageNet <ref type="bibr" target="#b28">[29]</ref>, respectively. Hardware Configurations.   We train GNMT-16, BERT-48 and XLNet-36 using the Adam optimizer <ref type="bibr" target="#b35">[37]</ref> with initial learning rate of 0.0001, 0.0001, 0.01 and 0.00003 respectively. For VGG19, we use SGD with an initial learning rate of 0.1. For AmoebaNet, we use RMSProp <ref type="bibr" target="#b36">[38]</ref> optimizer with an initial learning rate of 2.56. We use fp32</p><p>for training in all our experiments. Note that all the pipeline latency optimizations proposed in this paper give equivalent gradients for training when keeping global batch size fixed and thus convergence is safely preserved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Planning Results</head><p>Table V summarizes DAPPLE planning results of five models in the three hardware environments, where the total number of available devices are all fixed at 16. The first column also gives the global batch size (GBS) correspondingly.</p><p>We use three notations to explain the output plans. (1) A plan of P : Q indicates a two stage pipeline, with the first stage and the second stages replicated on P and Q devices,      respectively. For example, when P = 8 and Q = 8, we put each stage on one server, and replicate each stage on all 8 devices within the server(config-A). Besides, for plans where P &gt; 8 or Q &gt; 8 (e.g., 15 : 1) where some stages are replicated across servers, it will most likely be chosen for configurations with flat interconnections such as Config-B or Config-C, since for Config-A replicating one stage across servers incurs additional inter-server communication overhead.</p><formula xml:id="formula_8">2 × 8 (A) DP - - 16 × 1 (B) DP - - 16 × 1 (C) DP - - VGG-19 (2048) 2 × 8 (A) DP - - 16 × 1 (B) DP - - 16 × 1 (C)<label>15</label></formula><p>(2) A straight plan denotes pipelines with no replication. (3) A DP plan means the optimal strategy is data-parallel. We treat DP and straight as special cases of general DAPPLE plans. The Split Position column of Table <ref type="table" target="#tab_10">V</ref> shows the stage partition point of each model for the corresponding pipeline plan. The ACR column of the table shows the averaged ratio of cross-stage communication latency (i.e. communication of both activations in FW and gradients in BW) and stage computation time.</p><p>In the case of single server of config A, there is no relative low-speed inter-server connection, the intra-server bandwidth is fast enough (up to 130GB/s) to easily handle the magnitude (up to 3.7GB) of gradients communication of all benchmark models, and we find all models prefer DP plan for this case.</p><p>ResNet-50. The best plan is consistently DP for all three hardware configurations. This is not surprising due to its relatively small model size (100MB) yet large computation density. Even with low speed interconnects config C, DP with notably gradients accumulation and computation/communication overlap outperforms the pipelined approach.</p><p>VGG-19. Best plans in config A and B are also DP (Fig.</p><p>12 (a) and(b)), due to the moderate model size (548MB), relatively fast interconnects (25 Gbps), and the overlapping in DP. The weights and computation distributions of VGG19 are also considered overlapping-friendly, since most of the weights are towards the end of the model while computations are at the beginning, allowing gradients aggregation to be overlapped during that computation-heavy phase. In the case of low speed interconnects (config C), a 15 : 1 pipelined outperforms DP (Fig. <ref type="figure" target="#fig_10">12 (c</ref>). This is because most parameters in VGG-19 agglomerate in the last fully connected layer. A 15 : 1 twostage pipeline thus avoids most of the overheads of gradients synchronization due to replication (note we do not replicate the second stage). In this case gradients synchronization overheads outweigh benefits of DP with overlap. GNMT-16/BERT-48/XLNet-36. All three models have uniform layer structures, i.e., each layer has roughly the same scale of computations and parameters. And the parameter scales of these models vary from 1.2 GB up to 2.6 GB(Table <ref type="table" target="#tab_6">II</ref>). In config-A where all three models achieve low ACR values (0.10, 0.06 and 0.03, respectively, as shown in Table <ref type="table" target="#tab_10">V</ref>), a two stage 8 : 8 pipeline works best. Unlike VGG-19, the three models' layers are relatively uniformly distributed, thus a symmetric, evenly partitioning is more efficient. In config C, a straight pipeline works best for all three models. In this config, all devices have approximately the same workload. More importantly, no replication eliminates gradients sync overheads for relatively large models (1.2-2.6 GB) on a slow network (10 Gbps). The three models behave differently in config B. BERT-48 prefers straight pipeline in config B, while GNMT-16 and XLNet-36 keep the same plan results as shown in config A. This is because for fixed 16 devices, 16 and 48 uniform layers are more friendly for even partition compared to 36 layers for flat interconnections.</p><p>AmoebaNet-36. For AmoebaNet-36, DP is not available due to device memory limit. AmoebaNet-36 has more complex network patterns than other models we evaluated, and larger ACR in config A as well. Thus, more successive forward micro-batches are needed to saturate the pipeline. For all three configs, two-stage pipeline (8 : 8, 11 : 5 and 11 : 5, respectively) works best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance Analysis</head><p>In this work, we measure training speed-up as the ratio between the time executing all micro-batches sequentially on a single device and the time executing all micro-batches in parallel by all devices, with the same global batch size. Fig. <ref type="figure" target="#fig_10">12</ref> shows training speed-ups for all models except ResNet-50 on config A, B and C. For ResNet-50, the planning results are obvious and we simply present it in Table <ref type="table" target="#tab_10">V</ref>. For the other models, we compare training speed-ups of three different implementations: (1) Best Hybrid Speedup, performance of the best hybrid plan of pipeline and data parallelism returned by DAPPLE planner; (2) DP No Overlap, performance of DP with gradients accumulation but without computation/communication overlap; (3) DP Overlap, performance of DP with both gradients accumulation and intra-iteration computation/communication overlap between backward computation and gradients communication <ref type="bibr" target="#b19">[20]</ref>.</p><p>Overall analysis across these five models from Fig. <ref type="figure" target="#fig_10">12</ref>, for fixed GBS = 128, we can find that the hybrid approaches from DAPPLE outperform the DP approach with best intrabatch overlapping with averaged 1.71X/1.37/1.79X speedup for config-A, config-B and config-C, respectively. Specially, this speedup is up to 2.32X for GNMT-16 on config-C. Specific analysis for each model is given below.</p><p>VGG-19. For VGG-19, about 70% of model weights (about 400 MB) are in the last fully connected (fc) layer, while the activation size between any two adjacent layers gradually decreases from the first convolution layer to the last fc layer, varying dramatically from 384 MB to 3 MB for batch size 32. Thus, the split between VGG-19's convolutional layers and fully-connected layers leads to very small activation (3MB), and only replicating all the convolutional layers other than fully-connected layers greatly reduces communication overhead in case of relatively slow interconnects (Fig. <ref type="figure" target="#fig_10">12 (c)</ref>).</p><p>GNMT-16. GNMT-16 prefers a two-stage pipeline on hierarchical network (config A) and flat network with relative high-speed connection (config B). And the corresponding spit position is 9 : 7 but not 8 : 8, this is because the perlayer workloads of encoder layer and decoder of GNMT are unbalanced (approximately 1 : 1.45), thus the split position of DAPPLE plan shifts one layer up into decoder for pursuit of better system load-balance. For low speed interconnection environments (config C), straight pipeline ranks first when GBS = 1024. Each device is assigned exactly one LSTM layers of GNMT, and the GBS is large enough to fill the 16-stage pipeline.</p><p>BERT-48/XLNet-36. The best DAPPLE plan outperforms all DP variants for both models (Fig. <ref type="figure" target="#fig_10">12</ref> (g) to (l)) in all configurations. Compared to XLNet, the memory requirement for BERT is much smaller and thus allows more micro-batches on a single device. More computation per-step implies more backward computation time can be leveraged for overlapping communication overhead. As for config B and C, the slower the network is(from 25 Gbps to 10 Gbps), the higher the advantage of our approach has over DP variants. This is because the cross stage communication for both models is negligible with respect to gradients communication and the pipelined approach is more tolerant of slow network than DP.</p><p>AmoebaNet-36. The DAPPLE plan works best in all three configurations when GBS is fixed to 128. Unlike BERT-48 and XLNet-36, AmoebaNet has non uniform distributions of per layer parameters and computation density. The last third part of the model holds 73% of all parameters, and the per-layer computation time gradually increases for large layer id and the overall maximum increase is within 40%. As DAPPLE planner seeks for load-balanced staging scheme while considering the allreduce overhead across replicated stages, the split positions of pipelined approach for AmoebaNet-36 will obviously tilt to larger layer ID for better system efficiency. Take config A as an example, a two-stage pipeline is chosen and each stage is replicated over a separate server with 8 devices each. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Scheduling Policy</head><p>As discussed in Section V-C, the number of successive forward micro-batches (K i for stage i) scheduled in the warm up phase is an important factor to pipeline efficiency. We implement two policies, P A and P B , referring to smaller and larger K i numbers, respectively. Table <ref type="table" target="#tab_9">IV</ref> shows the normalized speedups for four benchmark models on hierarchical interconnects(config A), where all models' stage partition and replication schemes are consistent with the planning results of 2 servers of config A as shown in Table <ref type="table" target="#tab_10">V</ref>.</p><p>For VGG-19 and GNMT-16 (as well as AmoebaNet-36, which is not given in this figure yet), where the ACR ratio is relative high (0.16, 0.10, 0.18, respectively), there exists notable performance difference between these two policies (10%, 31% improvement from P A to P B , respectively). Hence we choose a larger K i to maximize pipeline efficiency. For the other models (BERT-48, XLNet-36), whose ACRs are very small (0.06, 0.03, respectively), the cross stage communication overhead is negligible compared to intra-stage computation time, leading to little performance difference. In this case, we prefer a smaller K i to conserve memory consumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Comparison with GPipe</head><p>Table VI shows the performance comparisons with GPipe. We focus on the throughput and peak memory usage on BERT-48 with a 2-stage pipeline on Config-B. To align with GPipe, we adopt the same re-computation strategy which stores activations only at the partition boundaries during forward. Note that all the pipeline latency optimizations in DAPPLE give equivalent gradients for training when keeping global batch size fixed and thus convergence is safely preserved and will not be further analysed here. When applying re-computation, both DAPPLE and GPipe save about 19% averaged peak memory at the expense of 20% on throughput when keeping M = 2 fixed.</p><p>When both without re-computation, DAPPLE gets 1.6× higher throughput with M = 16, and consumes 0.88× averaged peak memory compared to GPipe, which only supports up to 2 micro-batches. The speedup is mainly because higher M leads to lower proportion of bubbles. Note DAPPLE allows more micro-batches as the peak memory requirement is independent of M due to early backward scheduling.</p><p>The combination of DAPPLE scheduler and re-computation allows a further exploitation in memory usage. Compared with baseline GPipe (without re-computation), DAPPLE + RC achieves 0.70× memory consumption when M = 16, which allows us to handle larger micro-batch size or larger model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Comparison with PipeDream</head><p>We compare the results of our planner with those of PipeDream's under the synchronous training scenarios. We use the same configurations for both planners (e.g. same device topology, same interconnect and same profiling data), and evaluate both planners with DAPPLE Runtime. Table <ref type="table" target="#tab_13">VII</ref> shows the strategy results under a two-machine cluster of config-A. Fig. <ref type="figure" target="#fig_11">13</ref> shows the performance results for the strategies running in both 2 × 8 and 4 × 8 configurations.</p><p>As shown in Fig. <ref type="figure" target="#fig_11">13</ref>, in terms of speedup relative to to data parallelism, our strategies consistently outperform those generated by PipeDream's planner by up to 3.23× speedup under synchronous training scenarios, thanks to the advantages detailed in Section IV-D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Strong Scaling</head><p>Fig. <ref type="figure" target="#fig_12">14</ref> shows training speed-ups for four models. The number of GPUs ranges from 2 to 16. We use fixed but different global batch size for each model and apply config A. For AmoebaNet-36 when GBS = 256 (Fig. <ref type="figure" target="#fig_12">14(d)</ref>), both DP approaches achieve NaN as the model size is too   large to fit memory of single V100. For all these four models, we observe better scalability of DAPPLE over DP variants. Scalability weakens on all DP variants when the number of GPUs increases from 8 to 10, where gradients synchronization performance drops substantially due to slow Ethernet bandwidth. The performance of DAP P LE approach scales smoothly due to the rather small magnitude of crossstage activations as compared with weights(Table <ref type="table" target="#tab_2">I</ref>), which is insensitive to the relatively low-speed inter-server communications(25Gbps). In general, for hierarchical interconnects, the lower the cross-machine bandwidth, the more obvious the advantage DAP P LE approach as compared with DP .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Weak Scaling</head><p>Table <ref type="table" target="#tab_15">VIII</ref> shows the maximum model size that DAPPLE supports under reasonable input size with re-computation. We scale the model by varying the numbers of layers. We are able to scale BERT to 5.5B on 8 V100s with NVLink. There is a slight reduction in average GPU utilization due to more bubbles introduced by longer pipeline. In this case, the maximum model size scales linearly due to the balanced distribution of model params over encoder layers in BERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. RELATED WORK</head><p>Large DNN models are increasingly computational intensive. It is a common practice to parallelize training by leveraging multiple GPUs <ref type="bibr" target="#b37">[39]</ref>- <ref type="bibr" target="#b40">[42]</ref>. Data parallelism, model parallelism and pipeline parallelism are common approaches for distributed training of DNN models.</p><p>Data Parallelism <ref type="bibr" target="#b41">[43]</ref> . Some prior studies <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b42">[44]</ref>- <ref type="bibr" target="#b46">[48]</ref>   data parallelism. As a commonly used performance optimization method, gradients accumulation <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b47">[49]</ref> offers an effective approach to reduce communication-to-computation ratio. Another complementary approach is computation and communication overlap, with promising results reported in some CNN benchmarks <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b19">[20]</ref>. Model Parallelism. Model Parallelism <ref type="bibr" target="#b48">[50]</ref> partitions DNN models among GPUs to mitigate communication overhead and memory bottlenecks for distributed training <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b37">[39]</ref>, <ref type="bibr" target="#b38">[40]</ref>, <ref type="bibr" target="#b49">[51]</ref>- <ref type="bibr" target="#b52">[54]</ref>. This paper focuses on model partition between layers, namely, pipeline parallelism.</p><p>Pipeline parallelism. Pipeline Parallelism <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b53">[55]</ref> has been recently proposed to train DNN in a pipelined manner. GPipe <ref type="bibr" target="#b9">[10]</ref> explores synchronous pipeline approach to train large models with limited GPU memory. PipeDream <ref type="bibr" target="#b13">[14]</ref> explores the hybrid approach of data and pipeline parallelism for asynchronous training. <ref type="bibr" target="#b51">[53]</ref>, <ref type="bibr" target="#b53">[55]</ref>, <ref type="bibr" target="#b54">[56]</ref> make further optimization based on PipeDream. Pal et al. <ref type="bibr" target="#b38">[40]</ref> evaluated the hybrid approach without thorough study. Some researchers have been seeking for the optimal placement strategy to assign operations in a DNN to different devices <ref type="bibr" target="#b55">[57]</ref>- <ref type="bibr" target="#b57">[59]</ref> to further improve system efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>In this paper, we propose DAPPLE framework for pipelined training of large DNN models. DAPPLE addresses the need for synchronous pipelined training and advances current state-ofthe-art by novel pipeline planning and micro-batch scheduling approaches. On one hand, DAPPLE planner module determines an optimal parallelization strategy given model structure and hardware configurations. It considers pipeline partition, replication and placement, and generates a high-performance hybrid data/pipeline parallel strategy. On the other hand, DAPPLE scheduler module is capable of simultaneously achieving optimal training efficiency and moderate memory consumption, without storing multiple versions of parameters and getting rid of the strong demand of re-computation which hurts system efficiency at the same time. Experiments show that DAPPLE planner consistently outperforms strategies generated by PipeDreams planner by up to 3.23× speedup under synchronous training scenarios, and DAPPLE scheduler outperforms GPipe by 1.6× speedup of training throughput and saves 12% of memory consumption at the same time.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: DAPPLE framework overview.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Device mapping on hierarchical interconnects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :BubbleFig. 4 :</head><label>34</label><figDesc>Fig. 3: The different scheduling between GPipe(a) and DAP-PLE(b) and their memory consumptions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Device assignment examples: applying for 6 devices using three different strategies respectively from (a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Uneven pipeline minimum example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Efficiency of two stage replication approaches. Stage 0 consumes twice as much time as stage 1 for a micro-batch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 :Fig. 10 :</head><label>910</label><figDesc>Fig. 9: Split-Concat for Cross Stage Communication</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>1 Fig. 11 :</head><label>111</label><figDesc>Fig. 11: Micro-batches scheduling. The solid blue and dotted red arrows denote data and control dependencies, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 :</head><label>12</label><figDesc>Fig.12: Speedups on configurations with hierarchical/flat interconnects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 13 :</head><label>13</label><figDesc>Fig. 13: Performance comparison with PipeDream.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 14 :</head><label>14</label><figDesc>Fig. 14: Speedup with fixed GBS in config-A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>10 Gbps Ethernet interconnects. Besides large models, DAPPLE also works well for medium scale models with relatively large weights yet small activations (i.e. VGG-19). Performance results show that (1) DAPPLE can find optimal hybrid parallelization strategy outperforming the best DP baseline up to 2.32× speedup; (2) DAPPLE planner consistently outperforms strategies generated by PipeDreams planner by up to 3.23× speedup under synchronous training scenarios, and (3) DAPPLE runtime outperforms GPipe by 1.6× speedup of training throughput and reduces the memory consumption of 12% at the same time.</figDesc><table><row><cell>The contributions of DAPPLE are summarized as follows:</cell></row><row><cell>1) We systematically explore hybrid of data and pipeline</cell></row><row><cell>parallelism with a pipeline stage partition algorithm for</cell></row><row><cell>synchronous training, incorporating a topology-aware</cell></row><row><cell>device assignment mechanism given model graphs and</cell></row><row><cell>hardware configurations. This facilitates large model</cell></row><row><cell>training and reduces communication overhead of sync</cell></row><row><cell>training, which is friendly for model convergence.</cell></row><row><cell>2) We feature a novel parallelization strategy DAPPLE</cell></row><row><cell>planner to solve the partition and placement prob-</cell></row><row><cell>lems and explore the optimal hybrid strategies of data</cell></row><row><cell>and pipeline parallelism, which consistently outperforms</cell></row><row><cell>SOTA planner's strategies under synchronous training</cell></row><row><cell>scenarios.</cell></row><row><cell>3) We eliminate the need of storing multiple versions of pa-</cell></row></table><note>rameters, DAPPLE introduces a pipeline task scheduling approach to further reduce memory consumption. This method is orthogonal to re-computation approach and does not come at the expense of training throughput. Experiments show that DAPPLE can further save about 20% of device memory on the basis of enabling recomputation optimization. 4) We provide a DAPPLE runtime which realizes efficient pipelined model training with above techniques without compromising model convergence accuracy.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I :</head><label>I</label><figDesc>The traffic volume.</figDesc><table><row><cell>Benchmark</cell><cell>Activation Size at the Partition Boundaries</cell><cell>Gradient Size</cell></row><row><cell>GNMT-16</cell><cell>26MB</cell><cell>1.1GB</cell></row><row><cell>BERT-48</cell><cell>8.8MB</cell><cell>2.8GB</cell></row><row><cell>XLNET-36</cell><cell>4.2MB</cell><cell>2.1GB</cell></row><row><cell>AmoebaNet-36</cell><cell>11.2MB</cell><cell>3.7GB</cell></row><row><cell>VGG-19</cell><cell>6MB</cell><cell>550MB</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Table III summarizes three common hardware environments for DNN training in our experiments, where hierarchical and flat interconnections are both covered. In general, hierarchical interconnection is popular in</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE II :</head><label>II</label><figDesc>Benchmark models.</figDesc><table><row><cell>Task</cell><cell>Model</cell><cell># of Params</cell><cell>(cbch Size, Memory Cost)</cell></row><row><cell>Translation</cell><cell>GNMT-16 [30]</cell><cell>291M</cell><cell>(64, 3.9GB)</cell></row><row><cell>Language</cell><cell>BERT-48 [31]</cell><cell>640M</cell><cell>(2, 11.4GB)</cell></row><row><cell>Model</cell><cell>XLNet-36 [32]</cell><cell>500M</cell><cell>(1, 12GB)</cell></row><row><cell>Image Classification</cell><cell>ResNet-50 [33] VGG-19 [34] AmoebaNet-36 [35]</cell><cell>24.5M 137M 933M</cell><cell>(128, 1GB) (32, 5.6GB) (1, 20GB)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE III :</head><label>III</label><figDesc>Hardware configurations.</figDesc><table><row><cell>Config</cell><cell>GPU(s) per server(Ns)</cell><cell>Intra-server connnections</cell><cell>Inter-server connections</cell></row><row><cell>A</cell><cell>8x V100</cell><cell>NVLink</cell><cell>25 Gbps</cell></row><row><cell>B</cell><cell>1x V100</cell><cell>N/A</cell><cell>25 Gbps</cell></row><row><cell>C</cell><cell>1x V100</cell><cell>N/A</cell><cell>10 Gbps</cell></row><row><cell cols="4">industry GPU data centers. We also consider flat Ethernet net-</cell></row><row><cell cols="4">works interconnections because NVLink may not be available</cell></row><row><cell cols="4">and GPU resources are highly fragmented in some real-world</cell></row><row><cell cols="4">production clusters. Specifically, Config-A (hierarchical) has</cell></row><row><cell cols="4">servers each with 8 V100 interconnected with NVLink, and a</cell></row></table><note>25Gbps Ethernet interface. Config-B (flat) has servers each with only one V100 (no NVLink) and a 25Gbps Ethernet interface. Config-C (flat) is the same with Config-B except with only 10 Gbps Ethernet equipped. The V100 GPU has 16 GB of device memory. All servers run 64-bits CentOS 7.2 with CUDA 9.0, cuDNN v7.3 and NCCL 2.4.2 [36]. Batch Size and Training Setup. The batch sizes of offline profiling for the benchmarks are shown in the last column of TableII(ch size. As for AmoebaNet-36, it reaches OOM even if batch size = 1 on a single V100. Thus we extend to two V100s where batch size = 1 just works. We use large enough global batch size for each benchmark to ensure high utilization on each device. All global batch sizes we use are consistent with common practices of the ML community.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE IV :</head><label>IV</label><figDesc>Normalized training throughput speedup of scheduling policies P B compared to P A .</figDesc><table><row><cell>Model</cell><cell cols="4">Bert-48 XLNet-36 VGG-19 GNMT-16</cell></row><row><cell>Speedup</cell><cell>1.0</cell><cell>1.02</cell><cell>1.1</cell><cell>1.31</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE V :</head><label>V</label><figDesc>DAPPLE planning results.</figDesc><table><row><cell>Model (GBS)</cell><cell>#Servers × Ns</cell><cell>Output Plan</cell><cell>Split Position</cell><cell>ACR</cell></row><row><cell>ResNet-50</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(2048)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE VI :</head><label>VI</label><figDesc>DAPPLE vs. GPipe on BERT-48 with 2-stage pipeline when keeping micro-batch size fixed to 2 on Config-B. RC is short for re-computation.For this case a total of 36 normal cells layers are divided into 2 parts, namely 24 : 12, where the per-stage computation time ratio and allreduce time ratio of stage 0 and stage 1 is 1.57 : 1 and 1 : 2.74, respectively. For lower bandwidth network configurations (config B and C), the split position keeps shrinking to larger layer ID, because the allreduce comm overheads turn out to be the dominant factor with the absent of high-speed NVLink bandwidth.</figDesc><table><row><cell>Config</cell><cell># of micro batch (M )</cell><cell>Throughput (samples/sec)</cell><cell>Average Peak Memory (GB)</cell></row><row><cell>GPipe</cell><cell>2 3</cell><cell>5.10 -</cell><cell>12.1 OOM</cell></row><row><cell></cell><cell>2</cell><cell>4.00</cell><cell>9.9</cell></row><row><cell>GPipe + RC</cell><cell>5</cell><cell>5.53</cell><cell>13.2</cell></row><row><cell></cell><cell>8</cell><cell>-</cell><cell>OOM</cell></row><row><cell></cell><cell>2</cell><cell>5.10</cell><cell>10.6</cell></row><row><cell>DAPPLE</cell><cell>8</cell><cell>7.60</cell><cell>10.6</cell></row><row><cell></cell><cell>16</cell><cell>8.18</cell><cell>10.6</cell></row><row><cell></cell><cell>2</cell><cell>4.24</cell><cell>8.5</cell></row><row><cell>DAPPLE + RC</cell><cell>8</cell><cell>6.23</cell><cell>8.5</cell></row><row><cell></cell><cell>16</cell><cell>6.77</cell><cell>8.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE VII :</head><label>VII</label><figDesc>Strategy Comparison between DAPPLE and PipeDream, in the form of (start layer, end layer)@[GPU IDs].</figDesc><table><row><cell>Model (GBS)</cell><cell>DAPPLE</cell><cell>PipeDream</cell></row><row><cell></cell><cell></cell><cell>(0, 11) @ [G0 -G7]</cell></row><row><cell>VGG19 (1024)</cell><cell>(0, 16) @ [G0 -G13] (17, 25) @ [G14, G15]</cell><cell>(11, 17) @ [G8 -G13] (17, 19) @ G14</cell></row><row><cell></cell><cell></cell><cell>(19, 25) @ G15</cell></row><row><cell>AmoebaNet-36 (128)</cell><cell>(0, 30) @ [G0 -G7] (31, 43) @ [G8 -G15]</cell><cell>straight</cell></row><row><cell></cell><cell></cell><cell>(0, 4) @ [G0, G1]</cell></row><row><cell></cell><cell></cell><cell>(4, 13) @ [G2 -G7]</cell></row><row><cell>BERT Large (128)</cell><cell>(0, 13) @ [G0 -G7] (14, 26) @ [G8 -G15]</cell><cell>(13, 16) @ [G8, G9] (16, 19) @ [G10, G11]</cell></row><row><cell></cell><cell></cell><cell>(19, 22) @ [G12, G13]</cell></row><row><cell></cell><cell></cell><cell>(22, 26) @ [G14, G15]</cell></row><row><cell>XLNet-36 (128)</cell><cell>(0, 22) @ [G0 -G7] (23, 41) @ [G8 -G15]</cell><cell>straight</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE VIII :</head><label>VIII</label><figDesc>Maximum model size of BERT supported by DAPPLE + re-computation on V100 (16GB each) on config-A. BERT-L: BERT model with L encoder layers. Each model parameter needs 16 bytes since we applied Adam optimizer.</figDesc><table><row><cell>Config</cell><cell>BERT-L</cell><cell># of Model Params</cell><cell>Total Model Params Mem</cell><cell>Avg. GPU Util</cell></row><row><cell>Native-1</cell><cell>48</cell><cell>640M</cell><cell>10.2GB</cell><cell>93%</cell></row><row><cell>Pipeline-2</cell><cell>106</cell><cell>1.4B</cell><cell>21.9GB</cell><cell>89%</cell></row><row><cell>Pipeline-4</cell><cell>215</cell><cell>2.7B</cell><cell>43.8GB</cell><cell>89%</cell></row><row><cell>Pipeline-8</cell><cell>428</cell><cell>5.5B</cell><cell>88.2GB</cell><cell>87%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>focus on reducing the communication overheads for</figDesc><table><row><cell></cell><cell>16</cell><cell cols="2">DP No Overlap</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>16</cell><cell cols="2">DP No Overlap</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>16</cell><cell cols="2">DP No Overlap</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>16</cell><cell cols="2">DP No Overlap</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Training Speedup</cell><cell>4 8 12</cell><cell cols="3">DP+Normal Overlap Best Hybrid Speedup Straight Pipeline</cell><cell></cell><cell></cell><cell></cell><cell>Training Speedup</cell><cell>4 8 12</cell><cell cols="3">DP+Normal Overlap Best Hybrid Speedup</cell><cell></cell><cell></cell><cell></cell><cell>Training Speedup</cell><cell>4 8 12</cell><cell cols="3">DP+Normal Overlap Best Hybrid Speedup</cell><cell></cell><cell></cell><cell></cell><cell>Training Speedup</cell><cell>4 8 12</cell><cell cols="3">DP+Normal Overlap Best Hybrid Speedup</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell><cell>12</cell><cell>14</cell><cell>16</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell><cell>12</cell><cell>14</cell><cell>16</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell><cell>12</cell><cell>14</cell><cell>16</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell><cell>12</cell><cell>14</cell><cell>16</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Number of GPUs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Number of GPUs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Number of GPUs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Number of GPUs</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="7">(a) GNMT-16(GBS = 2048)</cell><cell></cell><cell cols="7">(b) BERT-48(GBS = 128)</cell><cell></cell><cell cols="7">(c) XLNet-36(GBS = 128)</cell><cell></cell><cell cols="7">(d) AmoebaNet-36(GBS = 256)</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Sutton</surname></persName>
		</author>
		<ptr target="http://www.incompleteideas.net/IncIdeas/BitterLesson.html" />
		<title level="m">The Bitter Lesson</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1910.10683" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Billion-scale commodity embedding for e-commerce recommendation in alibaba</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="839" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep neural networks for youtube recommendations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sargin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Conference on Recommender Systems</title>
				<meeting>the 10th ACM Conference on Recommender Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<ptr target="https://github.com/tensorflow/tensorflow/pull/32576" />
	</analytic>
	<monogr>
		<title level="j">Gradients Accumulation-Tensorflow</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<ptr target="https://gist.github.com/thomwolf/ac7a7da6b1888c2eeac8ac8b9b05d3d3" />
	</analytic>
	<monogr>
		<title level="j">Gradients Accumulation-PyTorch</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<ptr target="https://caffe2.ai/" />
		<title level="m">A New Lightweight, Modular, and Scalable Deep Learning Framework</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Priority-based parameter propagation for distributed dnn training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jayarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fedorova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pekhimenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.03960</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Horovod: fast and easy distributed deep learning in tensorflow</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Del Balso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05799</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gpipe: Efficient training of giant neural networks using pipeline parallelism</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="103" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pipe-torch: Pipeline-based distributed deep learning in a gpu cluster with heterogeneous networking</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 Seventh International Conference on Advanced Cloud and Big Data (CBD)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pipedream: generalized pipeline parallelism for dnn training</title>
		<author>
			<persName><forename type="first">D</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Harlap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Devanur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Ganger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM Symposium on Operating Systems Principles</title>
				<meeting>the 27th ACM Symposium on Operating Systems Principles</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Training deep nets with sublinear memory cost</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.06174</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Pipedream: Fast and efficient pipeline parallel dnn training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Harlap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Devanur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ganger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gibbons</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03377</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Characterizing deep learning training workloads on alibaba-pai</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.05930</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><surname>Gpipetalk</surname></persName>
		</author>
		<ptr target="https://www.youtube.com/watch?v=9s2cum25Kkc" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Fusionstitching: Deep fusion and code generation for tensorflow computations on gpus</title>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1811.05213" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Fusionstitching: Boosting execution efficiency of memory intensive computations for dl workloads</title>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1911.11576" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Poseidon: An efficient communication architecture for distributed deep learning on GPU clusters</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 USENIX Annual Technical Conference (USENIX ATC 17)</title>
				<meeting><address><addrLine>Santa Clara, CA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2017-07">Jul. 2017</date>
			<biblScope unit="page" from="181" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Aberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">De</forename><surname>Sa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.05124</idno>
		<title level="m">Pipemare: Asynchronous pipeline parallel dnn training</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName><surname>Nvlink</surname></persName>
		</author>
		<ptr target="https://www.nvidia.com/en-us/data-center/nvlink/" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yoon</surname></persName>
		</author>
		<ptr target="https://github.com/kakaobrain/torchgpipe" />
		<title level="m">torchgpipe, A GPipe implementation in PyTorch</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Block partitions of sequences</title>
		<author>
			<persName><forename type="first">I</forename><surname>Bárány</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Grinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Israel Journal of Mathematics</title>
		<imprint>
			<biblScope unit="volume">206</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="155" to="164" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<ptr target="http://tensorflow.org/" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>software available from tensorflow.org. [Online</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Cuda pro tip: Minimize the tail effect</title>
		<author>
			<persName><forename type="first">J</forename><surname>Demouth</surname></persName>
		</author>
		<ptr target="https://devblogs.nvidia.com/cuda-pro-tip-minimize-the-tail-effect/" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Edinburgh neural machine translation systems for wmt 16</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02891</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Know what you don&apos;t know: Unanswerable questions for squad</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03822</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Amoebanet: An sdnenabled network service for big data science</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A R</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sasidharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Demar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Macauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pouyoul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Network and Computer Applications</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="70" to="82" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COURSERA: Neural Networks for Machine Learning</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Large scale distributed deep networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1223" to="1231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Optimizing multi-gpu parallelization strategies for deep learning training</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zulfiqar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Migacz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nellans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="91" to="101" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Exploring the hidden dimension in accelerating convolutional neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aiken</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Horizontal or vertical?: A hybrid approach to large-scale distributed machine learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Workshop on Scientific Cloud Computing</title>
				<meeting>the 10th Workshop on Scientific Cloud Computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">One weird trick for parallelizing convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.5997</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName><surname>Baidu-Allreduce</surname></persName>
		</author>
		<ptr target="https://github.com/baidu-research/baidu-allreduce" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Highly scalable deep learning training system with mixed-precision: Training imagenet in four minutes</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.11205</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Massively multilingual neural machine translation in the wild: Findings and challenges</title>
		<author>
			<persName><forename type="first">N</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cherry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05019</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">A high performance and generic framework for distributed DNN training</title>
		<author>
			<persName><surname>Byteps</surname></persName>
		</author>
		<ptr target="https://github.com/bytedance/byteps" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Large-batch training for lstm and beyond</title>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hseu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</title>
				<meeting>the International Conference for High Performance Computing, Networking, Storage and Analysis</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Involving cpus into multi-gpu deep learning</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sekiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Negishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Imai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kawachiya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM/SPEC International Conference on Performance Engineering</title>
				<meeting>the 2018 ACM/SPEC International Conference on Performance Engineering</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="56" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Beyond data and model parallelism for deep neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aiken</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.05358</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Decoupled parallel backpropagation with convergence guarantee</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.10574</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Rima: An rdma-accelerated modelparallelized solution to large-scale matrix factorization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE 35th International Conference on Data Engineering (ICDE)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="100" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Efficient and robust parallel dnn training through model parallelism on multi-gpu platform</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02839</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Channel and filter parallelism for large-scale cnn training</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dryden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Maruyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Snir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</title>
				<meeting>the International Conference for High Performance Computing, Networking, Storage and Analysis</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
	<note>Essen</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Elasticpipe: An efficient and dynamic model-parallel solution to dnn training</title>
		<author>
			<persName><forename type="first">J</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Workshop on Scientific Cloud Computing</title>
				<meeting>the 10th Workshop on Scientific Cloud Computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Xpipe: Efficient pipeline model parallelism for multi-gpu dnn training</title>
		<author>
			<persName><forename type="first">L</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04610</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Device placement optimization with reinforcement learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2430" to="2439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Tiresias: A {GPU} cluster manager for distributed deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th {USENIX} Symposium on Networked Systems Design and Implementation</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="485" to="500" />
		</imprint>
	</monogr>
	<note>{NSDI} 19</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Supporting very large models using automatic dataflow graph partitioning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth EuroSys Conference</title>
				<meeting>the Fourteenth EuroSys Conference</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
