<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scalable Semi-Automatic Annotation for Multi-Camera Person Tracking</title>
				<funder ref="#_rZvP4AE">
					<orgName type="full">Agency for Innovation by Science and Technology</orgName>
				</funder>
				<funder ref="#_8aH83DH">
					<orgName type="full">Flemish Fund for Scientific Research</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jorge</forename><surname>Ni?o-Casta?eda</surname></persName>
							<email>jorge.nino@telin.ugent.be</email>
						</author>
						<author>
							<persName><forename type="first">Andr?s</forename><surname>Fr?as-Vel?zquez</surname></persName>
							<email>andres.friasvelazquez@telin.ugent.be</email>
						</author>
						<author>
							<persName><forename type="first">Nyan</forename><forename type="middle">Bo</forename><surname>Bo</surname></persName>
							<email>nyan.bobo@telin.ugent.be</email>
						</author>
						<author>
							<persName><forename type="first">Maarten</forename><surname>Slembrouck</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Junzhi</forename><surname>Guan</surname></persName>
							<email>junzhi.guan@telin.ugent.be</email>
						</author>
						<author>
							<persName><forename type="first">Glen</forename><surname>Debard</surname></persName>
							<email>glen.debard@thomasmore.be</email>
						</author>
						<author>
							<persName><forename type="first">Bart</forename><surname>Vanrumste</surname></persName>
							<email>bart.vanrumste@kuleuven.be</email>
						</author>
						<author>
							<persName><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
							<email>tinne.tuytelaars@esat.kuleuven.be</email>
						</author>
						<author>
							<persName><forename type="first">Wilfried</forename><surname>Philips</surname></persName>
							<email>philips@telin.ugent.be</email>
						</author>
						<author>
							<persName><forename type="first">Dimitrios</forename><forename type="middle">J</forename><surname>Tzovaras</surname></persName>
						</author>
						<author>
							<persName><forename type="first">A</forename><surname>Ni?o-Casta?eda</surname></persName>
						</author>
						<author>
							<persName><forename type="first">N</forename><forename type="middle">Bo</forename><surname>Fr?as-Vel?zquez</surname></persName>
						</author>
						<author>
							<persName><forename type="first">M</forename><surname>Bo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">J</forename><surname>Slembrouck</surname></persName>
						</author>
						<author>
							<persName><forename type="first">W</forename><surname>Guan</surname></persName>
						</author>
						<author>
							<persName><surname>Philips</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">are with the Image Processing and Interpretation Research Group</orgName>
								<orgName type="institution">Ghent University/iMinds</orgName>
								<address>
									<postCode>9000</postCode>
									<settlement>Ghent</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Thomas More Kempen</orgName>
								<address>
									<postCode>2440</postCode>
									<settlement>Geel</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Center for Dynamical Systems, Signal Processing and Data Analytics</orgName>
								<orgName type="institution">KU Leuven/iMinds</orgName>
								<address>
									<postCode>3001</postCode>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Center for Processing Speech and Images</orgName>
								<orgName type="institution">KU Leuven/iMinds</orgName>
								<address>
									<postCode>3001</postCode>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Scalable Semi-Automatic Annotation for Multi-Camera Person Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TIP.2016.2542021</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multi-camera tracking</term>
					<term>semi-automatic annotation</term>
					<term>performance evaluation</term>
					<term>people tracking</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes a generic methodology for the semi-automatic generation of reliable position annotations for evaluating multi-camera people-trackers on large video data sets. Most of the annotation data are automatically computed, by estimating a consensus tracking result from multiple existing trackers and people detectors and classifying it as either reliable or not. A small subset of the data, composed of tracks with insufficient reliability, is verified by a human using a simple binary decision task, a process faster than marking the correct person position. The proposed framework is generic and can handle additional trackers. We present results on a data set of ?6 h captured by 4 cameras, featuring a person in a holiday flat, performing activities such as walking, cooking, eating, cleaning, and watching TV. When aiming for a tracking accuracy of 60 cm, 80% of all video frames are automatically annotated. The annotations for the remaining 20% of the frames were added after human verification of an automatically selected subset of data. This involved ?2.4 h of manual labor. According to a subsequent comprehensive visual inspection to judge the annotation procedure, we found 99% of the automatically annotated frames to be correct. We provide guidelines on how to apply the proposed methodology to new data sets. We also provide an exploratory study for the multi-target case, applied on the existing and new benchmark video sequences.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing multi-view trackers are usually evaluated on relatively simple content such as people walking or standing without the presence of furniture and -for practical reasonson short sequences, e.g. <ref type="bibr" target="#b4">[5]</ref>. For real-life applications there is a considerable risk that existing solutions are over-tuned to specific datasets. Therefore, testing on more and larger video datasets is essential to explore when and how often trackers fail under diverse conditions. For single camera computer vision, ground truth is abundant, given the recent development of semi-automatic tools which exploit Amazon Mechanical Turk (MTurk) <ref type="bibr" target="#b5">[6]</ref>, e.g. VATIC <ref type="bibr" target="#b6">[7]</ref>. In comparison, for multi-camera video analysis, ground truth is far more scarce. Table <ref type="table">I</ref> lists the most popular multi-camera datasets (with overlapping views) which have been used to evaluate trackers. Most of these datasets were manually annotated using tools like ViPER <ref type="bibr" target="#b7">[8]</ref>. This available ground truth is very useful, but it does not always represent the desired experimental conditions for new research, which may require different camera resolutions or frame rates, different numbers or types of cameras with different viewpoints, different environmental conditions, etc. What is needed are more efficient procedures to generate annotations, or a substitute for it, in new datasets.</p><p>Generation of references for evaluation is of vital importance for assessing and improving the performance of detectors and trackers. Comprehensive ground truth databases are mostly created using labour-intensive manual annotation of videos, which usually restricts the database to short sequences <ref type="bibr" target="#b4">[5]</ref>; one approach is to rely on crowd-sourcing, as has been done for detailed annotation of persons in single images <ref type="bibr" target="#b8">[9]</ref> and for delineating bounding boxes in videos <ref type="bibr" target="#b6">[7]</ref>. However, even this approach is still labour intensive and requires considerable training and validation to ensure quality of the results.</p><p>For creating ground truth in multi-camera video only manual annotators <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> have been reported so far. One solution would be to apply a single camera system like VATIC for every view independently. However, even such semi-automatic approaches are very labour intensive. For instance, Vondrick et al. <ref type="bibr" target="#b6">[7]</ref> report that annotating 24 hours of video took about 8 man months of manual annotation time; the reported application was car tracking of possibly many cars in a single view. Moreover, this strategy of employing semiautomatic systems for single view annotation leaves open the question of how to fuse the results from multiple cameras. In any case, we are not aware of any method for generating reference tracking results for multi-camera tracking in a semiautomatic and efficient fashion.</p><p>In this paper we therefore propose a procedure for semiautomatic annotation of people tracking in long multi-camera sequences, and provide a new annotated video database where a person performs different activities involving different poses. The methodology aims to produce reference tracking data with minimal human effort and with an estimation of precision under given values of desired position accuracy, i.e. the fraction of the resulting annotations with a positional error larger than a specified threshold. With a given amount of human effort, our proposed approach allows producing reference data for much longer video sequences than is currently possible. This allows (future) trackers to be evaluated on a much richer test data set, encompassing a wider variety of environmental conditions, with more diverse camera setups, scenes and person appearances.</p><p>Specifically, we calculate a position consensus between the output of a so called oracle and many multi-camera trackers. The oracle is a person detector which can be tuned so as to produce highly reliable results, albeit only on a small fraction of video frames. The trackers (imperfect oracles <ref type="bibr" target="#b11">[12]</ref>) on the other hand produce an output for every video frame but are usually less reliable. In order to fuse the results produced by the oracle and the trackers, we propose a probabilistic framework which is robust to outliers (large tracking errors) and produces a consensus tracking result for all frames. The consensus tracker also outputs a reliability measure for each frame, which reflects the level of agreement between trackers and the oracle.</p><p>In this paper we demonstrate and evaluate the approach on a dataset of 6 hours, taken from 4 views, with furniture present, persons performing different activities, involving different poses and under different light conditions. The consensus tracking result is based on 6 multi-camera trackers, using publicly available software and our own implementations. We quantify the manual labour involved in the proposed methodology by the fraction of video frames that has to be visually inspected and the time that this operation takes.</p><p>To evaluate the performance of our methodology, we inspected the consensus tracking results for the full dataset frame-by-frame, correcting the generated annotations where needed. For solely the automatic part, we found that for the whole dataset, considering an accuracy of 60cm, 97.8% of the dataset (420000 frames) was correctly annotated. Using the proposed semi-automatic method, we show that it is possible to isolate most of the wrong annotations and to obtain up to 99% correct annotations. We also present additional results for several performance specifications.</p><p>To the best of our knowledge, the proposed probabilistic framework for automatic annotation of person position, using the consensus from many trackers and an oracle, is the first of its kind. Currently our approach is limited to single target tracking. However, we suggest an extension of our method for the multi-target case, which although not fully developed, it shows promising results in common benchmark datasets. In summary, the main contributions of this paper are:</p><p>? A methodology for generating reference tracking data in long multi-camera videos, based on the consensus of a detector and several trackers. For multi-camera annotation, our methodology is the first to estimate the reliability of annotations, and to offer the possibility of balance accuracy and human-effort in the final annotation result.</p><p>? A novel probabilistic framework to learn the error models of trackers, and how to apply them to estimate target position. Previous methods did not model the tracking error statistics of multiple trackers to increase the reliability of the estimated position. ? The evaluation of the accuracy and reliability of the proposed methodology on a 6 hour dataset, by a comprehensive visual inspection. Scalability of a semi-automatic methodology for annotation in multi-camera data sets of such length is addressed for the first time.</p><p>? We make the new calibrated multi-camera dataset available to the community, and provide the annotation results. <ref type="foot" target="#foot_0">1</ref>The rest of the paper is organized as follows. In Sect. II we summarize related work on generating reference data for tracker evaluation. Sect. III introduces the concepts of our method. It presents the consensus tracker and the procedure for manual inspection. Sect. IV discusses some implementation details. Sect. V presents a long-duration multi-camera tracking experiment for which we generate reference data using the proposed approach. We evaluate the performance of the proposed methodology in Sect. VI. We present an extension and preliminary results for the multi-target case in Sect. VII. We conclude with a discussion of the approach and its limitations in Sect. VIII and future work in Sect. IX.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section we present related work on the evaluation of tracking results. For methods that use position references, we limit our discussion to those where annotations are generated directly on video sequences, either manually or assisted by computer algorithms. Although we are aware of existing methods that generate position references using external sensors and different modalities (e.g. infrared <ref type="bibr" target="#b18">[19]</ref>, MoCap <ref type="bibr" target="#b19">[20]</ref>, inertial <ref type="bibr" target="#b20">[21]</ref>), we do not include them here as such sensors are not always available or their use is limited to controlled environments.</p><p>Recent work on the evaluation of visual trackers focuses on defining objective metrics for accuracy, failure and robustness <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b27">[28]</ref> and using these methods to estimate the performance of existing trackers <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b28">[29]</ref>- <ref type="bibr" target="#b30">[31]</ref>. This evaluation depends crucially on sufficient ground truth or reference positional data being available and that reference data being of sufficient quality, as demonstrated in the experimental survey in <ref type="bibr" target="#b21">[22]</ref>. For the purpose of generating such reference data, several manual annotation tools have been developed for videos from single and multiple calibrated views <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b31">[32]</ref>- <ref type="bibr" target="#b34">[35]</ref>. However, these methods do not scale to long video sequences, as manual input is required in every frame. In <ref type="bibr" target="#b35">[36]</ref>, the editors of a recent special issue on ground truth collection discuss the importance of building efficient semi-automatic tools to generate ground truth for multi-media applications and computer vision research in general. In the following we limit our discussion to tools for positional annotation at the level of image bounding boxes and point references in world coordinates. Thus, we will not consider tools for pixel-wise label propagation in video, like those presented in <ref type="bibr" target="#b36">[37]</ref> and <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Semi-Automatic Tools for Single Camera Annotation</head><p>Editing tools can help to significantly reduce human effort in semi-automatic ground truth generation. One such annotation tool is proposed in <ref type="bibr" target="#b38">[39]</ref> and made available as a web-based collaborative platform <ref type="bibr" target="#b39">[40]</ref>. Generic contour detectors and a tracker are used to reduce human effort for the annotation of bounding boxes in single camera video. However, their contour detection method works only on targets with stable shape and with boundaries that are easily distinguishable from the background, while in more cluttered scenes segmentation errors may cause the tracker to drift. Thus, although a well segmented target may help the tracker, the human has to manually edit or re-draw wrong contours in complicated scenes, which is a tedious task. This method was evaluated on the problem of annotating fish in a fish tank.</p><p>An extensive quantitative evaluation of video annotation with crowd-sourced marketplaces and an annotation tool called VATIC, both validated on MTurk, are presented in <ref type="bibr" target="#b6">[7]</ref>. In order to reduce human input, humans are asked to manually annotate targets with bounding boxes at periodically selected keyframes. These bounding boxes are then enforced as positional constraints in a dynamic programming framework, which also takes into account target appearance and a simple motion model. The authors propose an annotation cost measure which takes into account both human effort and computational complexity. However, the analysis in the paper is restricted to variants of one tracker which are compared in a single example. In a related contribution by the same authors <ref type="bibr" target="#b40">[41]</ref> active learning is proposed to reduce the annotation time of the VATIC tool: an estimate of annotation uncertainty is used as reference to iteratively propose new keyframes to annotate manually. The VATIC approach is one of the most successful ones reported in literature. However, it is restricted to single-view video, and it has some limitations compared to our approach. Firstly, it is not easy to integrate new trackers into the framework. Secondly, it requires humans to annotate bounding boxes in key frames. We believe our approach is less time consuming as the human only has to accept/reject a fraction of the automated annotations, rather than draw bounding boxes, which takes more time. Note that while the VATIC-MTurk framework combines manual effort and computation time, in our work we focus on reducing the manual effort only because computer use is cheap compared to human labor.</p><p>In general, compared to our method, the aforementioned semi-automatic techniques do not use the consensus of multiple sources of target position. Because it is well known that a single algorithm cannot cope with scene variability <ref type="bibr" target="#b21">[22]</ref>, annotation errors are more frequent and human input is still demanding. Methods that fuse the results of multiple algorithms for the purpose of semi-automatic annotation have been proposed recently. A tool that speeds-up the generation of ground truth is described in <ref type="bibr" target="#b41">[42]</ref>. The tool is applied in annotations of faces, and combines different face detectors and a tracker for the automatic part of the method, and the ViPER tool <ref type="bibr" target="#b7">[8]</ref> for the human intervention. The fusion of the different algorithms relies on the score given by the external detectors. Compared to our methodology, statistics of the errors are not taken into account to fuse the results of the different algorithms. Furthermore, the tool is designed for single camera sequences and there is no estimation of the necessary effort in the manual part of the method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Related Work on Fusing Tracking Results</head><p>The idea of fusing the results of several trackers to increase tracking performance appeared in <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b42">[43]</ref>- <ref type="bibr" target="#b44">[45]</ref>, and <ref type="bibr" target="#b45">[46]</ref>. Specifically, Zhong et al. <ref type="bibr" target="#b11">[12]</ref> approach single-view object tracking as a weakly supervised learning problem, in which the outputs of several trackers are treated as noisy labels. The probabilistic framework for optimal integration of labellers proposed in <ref type="bibr" target="#b46">[47]</ref> is used to jointly infer the object position and the accuracy of each tracker. A heuristic that measures agreement among trackers is used for training data selection and to update the trackers models. By sampling not only the state of the target but also the trackers involved in its computation, the method proposed in <ref type="bibr" target="#b42">[43]</ref> generates proposals for the best combination of tracker components. The target position is then estimated from the selected combination. In <ref type="bibr" target="#b43">[44]</ref>, a disagreement-based approach calculates the weights of a linear combination of probability maps obtained from several trackers. Such a disagreement-based approach is also presented in <ref type="bibr" target="#b44">[45]</ref>. In order to find an optimal target bounding box, a measure of attraction in a 4 dimensional space (position and size) between bounding boxes from tracking fusion candidates and the trackers is maximised. A heuristic for bad trackers removal is proposed to improve the estimate of the bounding box. Other researchers <ref type="bibr" target="#b45">[46]</ref> have proposed a symbiotic tracker ensemble framework for learning an optimal combination of results from several trackers. This combination is based on estimates of temporary consistency of individual trackers and correlation of pair-wise trackers. In another form of consensus tracking <ref type="bibr" target="#b47">[48]</ref>, a factorial hidden Markov model learns both the unknown trajectory of the target and the reliability of each tracker. In this case, the consensus position is computed recursively using a particle filter in order to handle multiple hypotheses. The likelihood of each observation (position estimate output by a tracker) is based on the distances between the bounding boxes and the actual target state, where the scale parameter of the chosen distributions is proposed as a reliability measure for each tracker.</p><p>The aforementioned methods focus on building a better tracker rather than on presenting a framework for minimaleffort reference data generation by fusing the results of several trackers. However, such consensus tracking improves automatic annotations as well, as we will discuss in Sect. V-B. Note that all of these papers deal with single camera tracking only. Moreover, they measure consensus in terms of degree of overlap between bounding boxes, which is only an indirect measure of real-world positional accuracy as the overlap is measured in pixel coordinates which do not have a one-to-one relationship with real-world coordinates. Also, most of these papers report results on small, albeit challenging sequences only, except by the work in <ref type="bibr" target="#b44">[45]</ref>, which uses the benchmark presented in <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Semi-Automatic Tools for Multi-Camera Annotation</head><p>A semi-automatic tool for annotating players in football video footage proposed in <ref type="bibr" target="#b17">[18]</ref> generates proposal tracks using several algorithms: A foreground/background detector extracts blobs for target initialisation; next a simple tracking strategy based on colour information and supported by heuristics, links the blobs over time. A graphical interface is used to edit the bounding boxes produced. Compared to our method, the main disadvantage of this tool is that the user has to browse the video frame-by-frame in order to check/correct the resulting annotations. The authors claim that the annotation tool can also be used for surveillance scenes, even though that type of video content is very different from football. While this is correct in principle, it requires adapting the heuristic rules to specific video content, which is not a trivial task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE PROPOSED APPROACH</head><p>In this section, we present our proposed method for semiautomatic annotation. Fig. <ref type="figure" target="#fig_0">1</ref> shows the flow diagram of the proposed method. We now summarise this workflow. Our methodology aims at producing consensus tracks by fusing the outputs of several trackers along with a reliability measure for In between initialisations, statistics on the position error (white arrows) are gathered by comparing to the output of the reliable person detector (red dot with red square). Note that although ideally a tracker would always output a position estimate, in practice trackers may lose the target (trackers 3 and 4 in this example). Additionally, some trackers may detect target loss and inhibit their output until new evidence is found (tracker 3). Thus, properties like robustness and accuracy are often conflicting between each other and therefore it is desirable to collect tracking statistics of complementary trackers. each annotation on these tracks; the latter is a measure of the level of agreement between the outputs of the various trackers. This measure is weighted by the positional error model of each tracker, which is learnt automatically in a previous step. By thresholding this measure, we split the consensus annotations according to their reliability. The annotations deemed as reliable are automatically accepted. The annotations deemed as unreliable are subject to a manual procedure, where a human decides on acceptance/rejection of annotated frames. This decision is based on an efficient visual inspection, applied on strategically sampled annotations. To illustrate the concept behind the consensus tracker, we will use the toy example of Figs. <ref type="figure" target="#fig_1">2a-c</ref>, showing a person walking between two locations, observed from four views. Note that although the toy example is in itself an easy case for state-of-the-art trackers, it explains the basic ideas. These ideas generalize to more complex tracking scenarios, as we will show in Sect. IV. While conceptually simple, the consensus tracking approach presents a number of problems. First of all, trackers need to be initialized before tracking can start and after tracking loss occurs, but not all available trackers implement an initialization method. Also, many trackers are based on similar underlying principles (e.g., foreground background segmentation, colour analysis ?) and therefore tend to fail under similar circumstances. Thus, a simple majority-vote consensus or averaging their outputs may end up in wrong position estimates.</p><p>Our proposed consensus tracker is based on simple probabilistic models of the tracking errors of individual trackers. One approach would be to ask a human to annotate a number of video frames, and then use this data to compute ground truth positions from which error statistics can be derived. However, we propose a different approach which avoids human labour in this "training" phase. We propose to use an oracle, i.e., a method producing position estimates with a high reliability that the postulated position indeed corresponds to a person, rather than to another type of object. This oracle almost never proposes very large "outlier" errors, at the expense of producing position estimates only on a fraction of the dataset. Ideally the oracle should be based on different principles than the trackers so as to have an independent failure mode.</p><p>As the oracle, we propose a solution based on a person detector <ref type="bibr" target="#b48">[49]</ref> applied independently in each camera view. Such a detector typically performs higher level analysis to distinguish between people and other objects. We tune it to a very low false positive detection rate, at the cost of fewer true positives. The people detector is run on each view separately and outputs a bounding box. When a person is detected in multiple views simultaneously, the position of that person is calculated in world coordinates using triangulation. The oracle solves most of the aforementioned problems: it provides initialisation data for those trackers that require it; when its output is available, it is highly reliable and provides an accurate estimate of the position error of the trackers at this point.</p><p>To produce annotation data in between oracle positions, we run multiple multi-camera trackers and fuse their output into a consensus tracking result. Fig. <ref type="figure" target="#fig_1">2</ref>.d illustrates this approach, which involves estimating the uncertainty on the position estimate of each individual tracker and using this uncertainty estimate in a Bayesian fusion approach. To estimate the uncertainty of a tracker, we compare the tracker's output to the oracle position estimates, which are assumed to be highly accurate. Tracking errors are then modelled with distributions, obtained on those frames for which the oracle produces a position estimate. The estimated distributions are then used in the consensus tracker, to properly weigh the contributions of individual trackers in a maximum likelihood approach.</p><p>The results will show that the consensus tracker combined with person detection considerably outperforms individual trackers, but is still not sufficiently reliable to classify all of the data automatically. As a result we still only obtain position references for part of the data, but this time the subset of annotated frames is already much larger than what we had based only on the oracle.</p><p>In order to increase the amount of annotated data even further, we propose a targeted visual inspection step: we present some of the annotated video frames to a human inspector. The human inspector either accepts those frames as being accurate enough or rejects them as having excessive tracking errors. In the latter case, the human inspector is presented with an alternative hypothesis, which again the human can either accept or reject. In order to make the visual inspection more efficient, the decision made on a single inspected frame is assumed valid for a whole set of video frames surrounding it. The start and end of this tracklet are selected based on the consensus of the trackers, as explained later. As we will show in the experimental results, in most cases this procedure renders an appropriate decision. The aim of this human inspection is twofold: firstly it allows us to increase the number of annotated frames significantly, with relatively little effort. Secondly, it allows us to separate the more difficult frames for the tracking task: those whose reliability measure is low and no alternative hypothesis is found.</p><p>In the following subsections, we first present the idea of obtaining a highly reliable detector as the main oracle of person position (III-A). We then discuss the multi-camera tracking setup (III-B), the estimation of the probabilistic error models for the trackers (III-C) and the consensus tracker (III-D, III-E, III-F). Finally, we discuss the visual inspection step: how frames are selected (III-G, III-H) and the procedure followed by the human inspector (III-I, III-J).</p><p>Table <ref type="table" target="#tab_0">II</ref> summarizes the notations used in this paper. While our approach handles multiple video sequences naturally, to simplify the notations we treat all video sequences as a single multi-camera video sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Reference Positions From Person Detection</head><p>In principle our method can operate with any method for person detection as the oracle. Sect. IV provides more details on the specific choice we made for this paper. The person detector is run in each view n independently and produces a bounding box Q(i ) n in image coordinates. It is possible that for a given frame i a person is detected in some of the views but not in others. In the following we will discard detections which are not accompanied by simultaneous detections in at least one other view. When for a given frame i no person is detected, we will consider Q(i ) n undefined. We extract reference positions q(i ) by multi-view triangulation. The image points for this triangulation are taken from the bounding boxes Q(i ) n (as we will explain in Sect. IV-A). We also define a binary variable b(i ), which indicates whether or not a reference position q(i ) was computed. The case b(i ) = 0 occurs when an insufficient number of bounding boxes is available, preventing triangulation.</p><p>The reference positions play two important roles: they will be used to initialize trackers and to analyse their errors statistically, so as to build a probabilistic model of these errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multi-Camera Tracking</head><p>Multiple trackers are initialized with a reference position produced by the people detector and then executed. Fig. <ref type="figure" target="#fig_1">2</ref>.d illustrates the procedure on an artificial example with four trackers. Comparing the tracks to the results of person detection, it can be observed that some trackers are more accurate, some are more robust to target loss, or some trackers may detect target loss and inhibit their output until new evidence is found. However we can only extract this information automatically at frames in which a position reference is available. On the other hand, it is possible to estimate the level of agreement between trackers in all frames. We will use this information later to estimate models of tracking performance.</p><p>We first need to define initialisation frames for the trackers. These initialisation frames form a subset of the reference positions, selected such that initialisation frames occur every ? L seconds when possible, as will be explained in Sect. IV-B. We then execute the multi-camera trackers, re-initializing them at each initialisation frame. Let r m, j (i ) be the track produced by tracker m = 1 . . . M of the individual trackers, when initialised from the person detector result in frame j , the index i ? j being the frame number. Note that some trackers produce an output for each frame, whereas others do not produce an output for some frames, e..g., when failing to detect or loosing a target or when an internally computed reliability is too low. To be able to handle all cases, we define the binary tracker output variable o m (i ), which is 1 iff. tracker m produces an output in frame i . The tracks r m, j (i ) will be compared to the reference positions q(i ) with i &gt; j and b(i ) = 1, to estimate tracking errors, to build models for these errors and to compute the final consensus tracking result. This procedure will be explained in the following subsections.</p><p>To not over complicate the notations, we will drop j and i when confusion is not possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Modelling Tracking Errors</head><p>We model tracking error using statistics of tracking loss and tracking accuracy. A tracker m is considered to have lost the target in frame i if r m (i )r(i ) ? d max , where d max is a specified threshold and r(i ) the unknown true person position. We define the binary random variable t m (i ) with</p><formula xml:id="formula_0">t m (i ) = 0 ?? r m (i ) -r(i ) ? d max .</formula><p>In case a tracker does not produce an output we assume by convention that t m (i ) = 0. We statistically characterize trackers using the following distributions:</p><p>? P(t 1 . . . t M ): The joint probability of a specific joint tracking loss failure mode (t 1 . . . t M ). We assume this distribution (as an approximation) to be independent of r, and of i , i.e., to have no spatial or temporal variation. This allows P(t 1 . . . t M ) to be specified using 2 M probability values. For instance, in the case M = 3, P(t 1 = 0, t 2 = 0, t 3 = 1) is the probability that trackers 1 and 2 simultaneously loss the target while tracker 3 functions properly.</p><p>? P(t m = 0|o m = 1): This is the probability that tracker m's output is wrong when it produces an output. Note that trackers which always produce an output will typically have higher values of P(t m = 0|o m = 1) than trackers which refuse to produce an output in difficult cases. ? P(o m = 0): Probability that tracker m produces no output.</p><p>? P( r mr |t m = 1): The probability density of the tracking errors of a given tracker, for the frames in which no tracking loss occurs. In order to estimate numerical values for these distributions, we rely on the oracle detector, and focus on those frames for which and oracle output is available. Hence, we assume that the frames for which person detection output is available are representative for the whole dataset. Specifically, we compare the output positions r m, j (i ) of each tracker m with the corresponding oracle q(i ), which is assumed to be perfect, i.e., q(i ) = r(i ). This way we obtain a training set of position errors d m, j (i ) = r m, j (i )q(i ) . For P( r mr |t m = 1), we compute the normalised histogram of the errors d m, j (i ) with d m, j (i ) &lt; d max using equally spaced bins of size B in the range [0, d max ). By counting the number of frames with d m, j (i ) ? d max , we determine P(t m = 0|o m = 1). We estimate P(t 1 . . . t M ) by counting the number of times each tracking failure mode (t 1 . . . t M ) occurs. For this estimation we assume that t m = 1 ?? r m (i )q(i ) &lt; d max . For P(o m = 0) we calculate the fraction from the frames i |b(i ) = 1 for which the tracker m does not produce an output.</p><p>Note that the models presented do not take into account temporal dependencies in tracker errors, which could be modeled in terms of a conditional distribution P(r m (i )|r m (i -1)) or temporal dependencies in tracker failure mode. Also, a more advanced probabilistic model <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref> could also take into account spatially varying probabilities P(t 1 . . . t M |r). However, estimating such distributions on training data would be both complex and would require too much training data. For this reason we have opted not to use them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. The Consensus Tracker</head><p>The consensus tracker computes an estimate r(i ) of the true person position r(i ) from the position estimates r m, j (i ), m = 1 . . . M of the individual trackers, which in turn have been initialized by the oracle in frame j . The consensus tracker finds the position r(i ) that maximises the posterior distribution</p><formula xml:id="formula_1">P(r, t 1 . . . t M |r 1 . . . r M , o 1 . . . o M ) ? P(r 1 . . . r M , o 1 . . . o M |r, t 1 . . . t M )P(t 1 . . . t M |r)P(r).</formula><p>Assuming that the prior P(r) is uninformative and that the probability of successful tracking and the probability of having a tracker output are the same everywhere in the room, we obtain:</p><formula xml:id="formula_2">r = arg max r P(r 1 . . . r M , o 1 . . . o M |r, t 1 . . . t M )P(t 1 . . . t M ).</formula><p>We also assume conditional statistical independence between individual tracker outputs, given a specific failure mode:</p><formula xml:id="formula_3">P(r 1 . . . r M , o 1 . . . o M |r, t 1 . . . t M ) ? M m=1 P(r m , o m |r, t m ).</formula><p>Although this assumption is somewhat weak (trackers may be affected by occlusions in similar ways and therefore their errors may be correlated), it is mitigated by modelling the joint probability of tracking failure P(t 1 . . . t M ). Thus, this assumption still allows for trackers to lose the target in dependent ways, but only assumes that the details of tracking errors (histograms of errors when t m = 1) are statistically independent. We support this assumption experimentally. We have computed the correlation coefficients of the positional errors between pairs of trackers. Most of the absolute values of these correlation coefficients fall below 0.1, except for three pairs of trackers, where they reach values close to 0.2. Using this assumption, we find r that maximizes</p><formula xml:id="formula_4">P(t 1 . . . t M ) M m=1 P (r m , o m |r, t m )<label>(1)</label></formula><p>In practice, given inputs r m and o m , the consensus tracker output is computed as follows: for each hypothesized position r, and for all trackers we compute t m , by comparing rr m to the threshold d max . The computed t m is then input, together with the inputs r m and o m into eq. ( <ref type="formula" target="#formula_4">1</ref>). The consensus tracker output r is then found by optimising eq. ( <ref type="formula" target="#formula_4">1</ref>). Now we explain how to calculate the factors P (r m , o m |r, t m ). As a first case, we consider o m = 0, i.e., tracker m does not produce an output. In this case r m is undefined and its value is irrelevant. Moreover, by convention t m is always 0 in this case (see above). Therefore we assume that P(r m , o m = 0|r, t m = 0) = P(o m = 0|t m = 0). Using Bayes' rule, P(o m = 0|t m = 0) becomes P(o m = 0) P(o m = 0) + P(t m = 0|o m = 1)P(o m = 1) because P(t m = 0|o m = 0) is always 1: if no output, tracker is considered lost. As the second case, we consider o m = 1, i.e" when tracker m produces an output. In this case, we distinguish between t m = 0 and t m = 1. Assuming that once tracking is lost, all spatial positions are equally likely, we have:</p><formula xml:id="formula_5">P(r m , o m = 1|r, t m = 0) = B/(G -?d 2 max )</formula><p>where G is the area of the ground plane where people is expected to be tracked. On the other hand, if we assume that the tracking error r mr is independent of position and if its distribution is radially symmetric, then we have:</p><formula xml:id="formula_6">P(r m , o m = 1|r, t m = 1) = P( r m -r |t m = 1)</formula><p>Note that all factors in the probabilities and distributions used in the calculation of (1) are estimated in a training phase, with training data using the person detector as a substitute for ground truth reference, as explained in Sect. III-C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Alternative Hypotheses to the Optimal Consensus</head><p>For every frame i we obtain not only an estimated optimal value r(i ), but also the corresponding optimal tracking failure mode estimate ( t 1 . . . t M )(i ), which is a deterministic function of the tracker positions r m (i ) and r(i ). During visual inspection, a human inspector may reject r(i ). In this case, we generate a second best alternative hypothesis for r(i ) by considering other tracker failure modes if possible. This second hypothesis is found by artificially excluding any hypothesized position r(i ) that results in</p><formula xml:id="formula_7">(t 1 . . . t M )(i ) = ( t 1 . . . t M )(i )</formula><p>, when maximising (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Automated Annotation Reliability Measure</head><p>In this section, we propose a reliability measure to assess the reliability of the position estimates produced by the consensus tracker. This measure is very important, since it will be used to classify automatic annotations into subsets called "accepted," (will not be visually inspected) and "candidate for visual inspection". As a reliability measure R, we propose the logarithm of the probability maximised in eq. ( <ref type="formula" target="#formula_4">1</ref>): (</p><formula xml:id="formula_8">)<label>2</label></formula><p>This measure takes into account the estimated tracking error model and the probability of tracking failure modes. Note that this value is always negative, with 0 being the highest possible value, corresponding to maximal consensus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Selecting Frames for Visual Inspection</head><p>Now we will explain how to select frames for visual inspection, based on the resulting reliability measure, and an estimation of the precision of the consensus tracker annotations. This estimation we will be calculated using the people detections as reference, and will allow us to estimate the time a user would spend during the visual inspection step.</p><p>By applying a threshold ? R to the computed reliability measure R(i ) for each frame i , we decide if a frame is automatically accepted or not: it is automatically accepted iff. R(i ) ? ? R . An optimal threshold ? R = ? * R will be computed based on user specifications as detailed in Sect. III-H. In the current section, we investigate how the selected threshold affects the number of frames to be visually inspected, the quality of the resulting annotations and the time needed for the inspection. These numbers are estimated and then presented to the user as part of the process for selecting a threshold which optimally meets user requirements. This will become more clear when presenting our case study in Sect. V.</p><p>In the following, let S c be the set of all frames for which a consensus tracker output is available, and let S a (? R ) and S v (? R ) be the subsets of automatically accepted annotations and annotations to be visually inspected, respectively. The dependency on the threshold ? R is due to the fact that by thresholding R in S c we obtain S a and S v . For the sake of readability, we may omit this dependency in the notations when confusion is not possible. After human visual inspection of the subset S v (? R ), we obtain the subset S m of annotations that have been manually accepted or corrected by the user (when possible), and a subset S r of rejected annotations. We define the set of semi-automatic final annotations S f = S a (? R ) ? S m . We also define the set S o as the subset of annotations from S c for which the detector output is available.</p><p>Let S be a set of annotations obtained in any way (e.g., automatic, by visual inspection, ?). We will use the term annotation fraction of a set S for the value #S/#S c . We also define the precision of a set of annotations as follows:</p><formula xml:id="formula_9">c(S, d r ) = # i ? S| r(i ) -r(i ) &lt; d r /#S.</formula><p>(</p><formula xml:id="formula_10">)<label>3</label></formula><p>where d r is the desired annotation accuracy (see Sect. III-H).</p><p>In words, this precision is the fraction of annotations in a set which has an accuracy of at least d r centimeters. In the following discussions we will evaluate and predict precision on many different annotation sets, but the notation will make clear which set we mean. Obviously the precision c(S, d r ) is an important quality criterion. However, it cannot be computed on the full dataset because of the absence of ground truth. We can estimate the precision c(S ? S o , d r ) on the frames for which the people detector output is available. In this case we assume that r(i ) = q(i ), i.e., that the people detector produces a perfect position estimate (it is an "oracle"), i.e., we estimate it by evaluating eq. ( <ref type="formula" target="#formula_10">3</ref>), but with r(i ) replaced with q(i ). We call this estimate ?(S, d r ) and apply it to the different annotation sets, except for S m and S f , for which it does not make sense to estimate precision before we apply visual inspection. Note that for instance, ?c (S c , d r ) is not necessarily a good estimate of c c (S c , d r ) because the set of frames for which a people detector is available may not be representative for the whole data set. This will become more clear when discussing the results in Sect. VI-B.</p><p>After we apply visual inspection, we obtain a set of manually accepted annotations S m . In the following we will assume that the set S m has a precision of 100%, i.e., that c m (S m , d r ) = 1 because we trust that the human inspector does not make any mistakes. We also assume that when finding wrong annotations, a second hypothesis is always correct, thus S r = ?. This assumption implies that at least one of the trackers is always close enough to the real position of the person. We will investigate the validity of these assumption later in this section and evaluate it in Sect. VI-C. Therefore we can compute an estimate ? f (S f , d r ) of c f (S f , d r ) as follows:</p><formula xml:id="formula_11">? f (S f , d r ) ? ?a (S a , d r )#S a (? R ) + c m (S m , d r )#S m #S c ? ?a (S a , d r )#S a (? R ) + #S v (? R ) #S c (<label>4</label></formula><formula xml:id="formula_12">)</formula><p>which depends on the precision ?a (S a , d r ) which is estimated on S a ? S o as explained above.</p><p>As the next topic in this section, we estimate the time needed for visual inspection which depends on ? R (which determined which frames are visually inspected). To keep human intervention at a minimum, in the visual inspection step the user only has to inspect well-selected frames of S v (? R ). Some temporally close frames are automatically added, whenever a user accepts a frame as "correct," taking advantage of the temporal continuity in the joint failure mode of trackers. We define an annotation tracklet as a sequence of consecutive consensus annotations with the identical computed tracker failure mode. We have experimentally observed that frames within the same tracklet are likely to have the same accuracies, so we assume that they are all either correct or incorrect. This assumption is reasonable in practice provided that tracklets are sufficiently short. For this reason we restrict tracklet duration to at most l max = 20 frames. Given the assumption, after sampling an unreliable annotation for visual inspection, we therefore extrapolate the decision of the user to accept or reject the annotation for a single frame to the complete tracklet. The total time needed for visual inspection will therefore be a function of the number of tracklets T v rather than of #S v (? R ). As mentioned earlier in Sect. III-E, when an annotation r(i ) is rejected during visual inspection, a second annotation hypothesis is presented to the user. Therefore, the estimate of the time needed for visual inspection is proportional to the number of tracklets within S v (? R ), plus the number of times a second hypothesis is presented to the user, which depends on ?v (S v , d r ). More specifically the visual inspection time is estimated as</p><formula xml:id="formula_13">?v (S v , d r ) = ? 1 T v (2 -?v (S v , d r )), (<label>5</label></formula><formula xml:id="formula_14">)</formula><p>where T v is the number of annotation tracklets contained within S v (? R ), and ? 1 is the average time the user needs to inspect one frame. After doing some experimental work on judging annotations, we found that the average time spent on inspecting one frame is ? 1 ? 1sec. Finally, we define the order in which annotations are selected for visual inspection. This order is important, because each selected frame deems a whole tracklet as either correctly or incorrectly annotated. In order to reduce the error introduced by the assumption that the precision of the subset S m is 1, we must try to replace as much bad annotations as possible or discard them. Therefore, it is best to show to the user first the annotation that is more likely to be wrong within a tracklet. It is more likely to find wrong annotations at lower values of reliability measure. Therefore, among the annotations remaining in the set S v (? R ), we select the frame i with the lowest value of R(i ) at every visual inspection iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Meeting User-Defined Specifications</head><p>To determine which frames will be visually inspected, the user must set specifications for performance measures, which will then be translated into an appropriate threshold ? R , for deciding which frames to automatically accept and to visually inspect. The specifications are: Accuracy of the annotations d r (the maximum positional error allowed for "correct" annotations) and Minimum precision c min (the fraction of final semi-automatic annotations with an actual accuracy no worse than d r ). From these numbers, the corresponding optimal value ? * R of ? R can be computed as follows: Eq. ( <ref type="formula" target="#formula_11">4</ref>) allows us to estimate ? f (S f , d r ) in terms of the known d r and the unknown ? R . Using a numerical root finding procedure we compute the value of ? R for which ? f (S f , d r ) ? c min .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Human Visual Inspection</head><p>The question that remains is how a human evaluator should assess the quality of an annotation. We define a quality criterion based on the desired accuracy d r and a binary decision h(i ) made by the user. Thus, the human evaluator either rejects (h(i ) = 0) or accepts (h(i ) = 1) an annotation at frame i according to how well it fits visually the target, within the minimum accuracy d r . Because an annotation r(i ) is defined as a 2-dimensional position on the ground plane (point), it would be difficult for a user to visually judge it. Therefore, we augment the annotations by projecting a rectangular cuboid of height h p (assumed average height of a person) and squared base of side equal to 2 ? d r + w p centred at r(i ), where w p is the assumed average width of a person and 2 ? d r is a margin for the maximally allowed position error. The user should consider an annotation correct iff. the target is completely enclosed by the projected cuboid in all views, as illustrated in Fig. <ref type="figure" target="#fig_3">3</ref>.c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J. Overview of the Annotation Procedure</head><p>In summary, the user must perform the following actions to annotate a new multi-camera video database:</p><p>1) Run the automatic stages of our method (detection, tracking, consensus tracking). 2) Graphs showing estimated values of precision, annotation fraction and visual inspection time are displayed for several specification working points. (See Sect. V-D). 3) Set the desired specifications: d r and c min . 4) Perform the visual inspection (See Fig. <ref type="figure" target="#fig_3">3</ref>)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. IMPLEMENTATION DETAILS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Obtaining Reliable People Detections</head><p>The proposed annotation procedure is quite flexible. For instance, it can deal with various types of people detectors.   <ref type="bibr" target="#b48">[49]</ref>). Note that with respect to other body parts, the head would be the best reference to reconstruct the real world coordinates of the person position. Therefore, the top-center of the red box (yellow point) is used for triangulation.</p><p>In this subsection we discuss the specific choices we used in the case study of sect. V. In this paper we used the poselet-based people detector presented in <ref type="bibr" target="#b48">[49]</ref>, specifically the implementation provided by the authors. We also tested our approach using the people detector presented in <ref type="bibr" target="#b51">[52]</ref> and obtained slightly different results. <ref type="foot" target="#foot_1">2</ref>In order to generate an initial sequence of the most reliable references for person position, we set the "detection score threshold" ? d following the procedure proposed in <ref type="bibr" target="#b48">[49]</ref> using the available ROC curves. We set this parameter for approximately 5% of false positives per image. This resulted in ? d = 2.8. Note that we have not re-trained the detector with the images used in our experiments but rather used the existing empirical model made available by the authors. This choice means that there is no need for the user of our proposed annotation procedure to spend additional time on training a people detector. In order to obtain ground plane positions q(i ) from image bounding boxes Q(i ) n , we rely on multi-view geometry <ref type="bibr" target="#b52">[53]</ref> triangulation. We assume that the top of the head of a person lies on the top-center coordinate of the poselet bounding box, as illustrated in Fig. <ref type="figure" target="#fig_5">5</ref>. With this point as reference, we estimate the real world coordinates (X, Y, Z ) of the top of the head when image poselet detections in at least 2 views are available. Note that we keep only the ground plane coordinates (X, Y ), as we focus our study on tracking person positions in 2D. In order to ensure reliable references, we only keep ground plane detections where the error in the reprojection of the person head is below a threshold ? t . We use a value of ? t = 50 pi xels for the video sequences used in this paper, which we have found experimentally. For other video sequences a different value can be better, which depends mainly on the multi-camera configuration. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. of</head><p>As explained in Sect. III-B, we aim to process the video sequences in segments of length L to allow for tracking initialisation. The choice of the segments length is rather empirical, as for an optimal value that ensures training/testing data quality one should consider several factors that are unknown beforehand, such as the amount and diversity of motion present. Therefore, we set L to 30 seconds, which results in 410 video segments. In practice however, the value assigned to L is approximate, as we define a window of 5 seconds where the ground plane detection reference with the smallest reprojection error is taken for tracking initialisation. This choice allows us to minimise the risk of wrong initialisation positions. With the initialisation set already defined, we run 6 available multi-camera trackers, five of which have been published in the literature <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b53">[54]</ref>- <ref type="bibr" target="#b56">[57]</ref> and a six-th one for which a paper has been submitted <ref type="bibr" target="#b57">[58]</ref>. We provide details on the trackers specifications in Table <ref type="table" target="#tab_1">III</ref>.</p><p>The maximum positional error d max , under which a tracker is considered to be on target, is set to 50cm, an experimentally found value which has been reported in state of the art multicamera trackers <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b58">[59]</ref>. To model the accuracy of the trackers, we set the size of the histogram bins to B = 10cm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CASE STUDY</head><p>As a case study, we apply the proposed annotation procedure, to a data set of 27 multi-view video sequences, containing about 6 hours of content. These sequences were recorded with 4 calibrated cameras in a holiday bungalow with a combined living room and kitchen area, as illustrated in Fig. <ref type="figure" target="#fig_4">4</ref>, which shows the ground plane coverage of the cameras. Different sorts of actions (walking, eating, cooking, watching TV, reading, cleaning, using laptop, laying down) involving  interaction with moving and static furniture, were recorded, making the video database rich in content and suitable for evaluation of video analysis algorithms.</p><p>Although the case study features only single-target sequences, we included two multi-target sequences from the dataset proposed for the multi-target analysis, which is discussed later in Sect. VII. We now discuss the procedure applied to this case study. Later, in Sect. VI we perform a metaanalysis by comparing the annotations to ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. People Detection Results</head><p>From a total of 420 000 frames, we were able to extract 110 000 frames with reliable person detections (around 26%): 83 000 of those allowed triangulation from 2 simultaneous views, 22000 from 3 views and 5000 from 4 views. In 310 000 frames triangulation was not possible and therefore no position references were obtained. As stated before, detections q(i ) should contain a large variety of person instances in pose and space. Regarding pose, the poselet detector can deal with a wide range of poses <ref type="bibr" target="#b48">[49]</ref>. Regarding space, in Fig. <ref type="figure" target="#fig_6">6</ref> we show that the spatial distribution of detections (left plot) is reasonably similar to the distribution of the positions output by the consensus tracker (right plot).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Empirical Tracking Error Distributions Per tracker</head><p>Fig. <ref type="figure" target="#fig_7">7</ref> shows the distribution of the tracking accuracy P( r mq |t m = 1) for the six trackers, the target loss ratio and and the probability that a tracker produces no output. These distributions are used in the computation of the consensus tracker, as explained in Sect. III-D. They show that some trackers are more accurate on average (e.g., m = 4 versus m = 6) when no tracking loss occurs. The top-left plot in Fig. <ref type="figure">8</ref> shows the predicted cumulative distribution of the tracking errors for all individual trackers, the consensus tracker and the average of the trackers outputs. They predict that the proposed consensus tracker outperforms the trackers and the average of the trackers outputs. Note that the consensus tracker benefits from the trade-off between trackers with high accuracy Fig. <ref type="figure">8</ref>. Top: Cumulative density function of the tracking error (CDF) for the evaluated trackers, the centroid of their outputs and the consensus tracker: predicted (left) and actual (right). The plots at the bottom show the same CDFs, but using the people detector from <ref type="bibr" target="#b51">[52]</ref> as reference instead of <ref type="bibr" target="#b48">[49]</ref>. In this case, the results show a poorer performance when modelling the tracking errors. However, the overall consensus tracker result is not affected significantly: for the position accuracy, 50% of the annotations have distance error below 20 cm when using <ref type="bibr" target="#b48">[49]</ref> and 47% when using <ref type="bibr" target="#b51">[52]</ref>. To build these plots, tracking error is considered infinite when a tracker does not produce an output. and trackers with high recall. For instance, Tracker 3 would not reach more than 40% of the frames, but almost always its positional error would be below 50cm. On the other hand, Tracker 5 doubles the number of frames with a positional error below 50cm (? 80%), at the cost of a great deal of frames with higher positional error (? 20%). Recall that the results in this subsection were obtained on the subset of annotations S o , for which people detector references are available. The bottom-left plot in the same Fig. <ref type="figure">8</ref> shows the results on the same data set for a variant of our method in which we replace the person detector of <ref type="bibr" target="#b48">[49]</ref> by the pedestrian detector of <ref type="bibr" target="#b51">[52]</ref>. In general, the results show a poorer performance in modelling the tracking errors. This may be due to the fact that this detector is intended for pedestrian detection, which does not include human poses different than walking and standing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Consensus Tracker Computation and Analysis</head><p>The consensus tracker fuses the outcomes of the trackers, taking into account their positional error model. This fusion results in better estimates of the person position, as explained above and illustrated in Fig. <ref type="figure">8</ref>. The reliability measure R quantifies how trustful this fusion is. In Fig. <ref type="figure" target="#fig_8">9</ref>, the top-left plot shows estimated distributions of R, conditioned on the status of the consensus tracker (correct or not), for several values of d r . These distributions predict the range of values R for which is more likely to find wrong annotations, and are estimated on the subset of consensus tracker annotations S o , for which detector references exist. The bottomleft plot of Fig. <ref type="figure" target="#fig_8">9</ref> illustrates the estimated precision value for automatically accepted annotations as a function of ? R . The left plot in Fig. <ref type="figure" target="#fig_0">10</ref> shows the fraction of annotations that are automatically accepted for several values of estimated precision, under different values of the user specification d r . We will discuss these predictions again, when analysing the actual results in Sect. VI-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Analysis of Visual Inspection</head><p>Fig. <ref type="figure" target="#fig_0">11</ref> illustrates the predicted visual inspection time and the predicted precision for both the automatic and  semi-automatic annotations, for several values of d r and c min . This figure serves as a guide to the user to select a combination of precision c min and accuracy d r . The corresponding actual results for these working points will be presented in Sect. VI-C. They will show that in general, between 8%-10% of additional time is spent in visual inspection, regarding the predicted values. This error is due to the overestimation of the precision ?v , which results in predicting less wrong annotations when a second hypothesis is shown to the user. Details on the errors introduced by the estimation of annotation precision will be discussed in Sect. VI-B. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RESULTS</head><p>In this section, we evaluate the consensus tracker results and visual inspection procedure by comparing the results of the case study to ground truth, obtained by exhaustively inspecting all consensus tracking results, and correcting them where needed. We perform this analysis to illustrate both the strong points and the limitations of the proposed approach. We also compare the manual annotation time of our method against the publicly available VATIC system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Actual Performance of Trackers and Consensus Tracker</head><p>The top-right plot in Fig. <ref type="figure">8</ref> shows the cumulative density function of the tracking error (CDF) for the evaluated trackers and the consensus tracker, which is computed with respect to the ground truth. Note that, compared to the predicted tracking error CDF (left plot), the prediction is optimistic for trackers 1, 2, 3 and 4 while pessimistic for 5 and 6. The prediction for the consensus tracker is rather optimistic as well. However, it can be observed that the CDF is still similar in both cases, and the general ranking of the trackers remains the same. Thus, in general, the predicted CDF can be used as tool to evaluate trackers, provided a reliable set of position references, and a representative subset of sampled annotations S o . Note that the results confirm that the proposed consensus tracker outperforms the trackers alone and the average of their outputs. Thus, when fusing the different outputs of the trackers, the proposed modelling of the tracking error results in better position estimates. This is illustrated in Fig. <ref type="figure" target="#fig_10">12</ref>.a, b, c, where we show 3 cases where the different trackers are challenged by difficult situations in the dataset. For instance, Fig. <ref type="figure" target="#fig_10">12</ref>.b shows a person cleaning the floor, where the vacuum cleaner produces motion disturbance for the trackers. In each case we show that while some trackers fail to locate the target, the consensus tracker retrieves the correct position. The figure also illustrates that, in the other hand, in some difficult cases the human visual inspection needs to be triggered (Fig. <ref type="figure" target="#fig_10">12</ref>.d).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Meta-Analysis of the Consensus Tracker</head><p>In the proposed procedure, the selection of an appropriate working point is an important decision to be made by the user. This decision relies on the estimated precision of the annotations, which at the same time depends on the reliability measure as explained in Sect. III-G. In the following we evaluate the impact of the reliability measure on the quality of the final set of semi-automatic annotations. The top-right plot in Fig. <ref type="figure" target="#fig_8">9</ref> illustrates the actual distribution of R for correct and incorrect annotations. For the analysis, we compare this plot against the top-left plot in the same figure, which shows the corresponding predicted distributions of R. Note that for the distribution of R conditioned on the annotations being wrong (P(R| r(i ) -r(i ) ? d r )), the difference is relatively high, compared to P(R| r(i ) -r(i ) &lt; d r ). This behaviour is normal, as far less samples are available to estimate the first one, with respect to the second one. Specifically, for high values of R, the fraction of wrong annotations is over estimated by the samples from S o . Instead, for low values of R, this fraction is under estimated. This modelling error impacts the estimation ?a (S a , d r ) of c a (S a , d r ), which can be observed by comparing the bottom plots in <ref type="bibr">Fig 9,</ref><ref type="bibr"></ref> for which corresponding values of annotation fraction are plot in Fig. <ref type="figure" target="#fig_0">10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results of the Visual Inspection</head><p>To judge the visual inspection results, we analyse the resulting annotation fraction and precision of the semi-automatic annotations and the time spent during human visual inspection. Fig. <ref type="figure" target="#fig_11">13</ref> illustrates these numbers for several values of position accuracy d r . The plots for results of the automatic annotation (solid lines) and visual inspection (dashed lines) must be compared against the corresponding plots in Fig. <ref type="figure" target="#fig_0">11</ref>. In reality the user would of course perform the visual inspection for only one of the working points. For instance, Fig. <ref type="figure" target="#fig_0">11</ref> shows that for an accuracy of 60cm, and a desired precision c min of 0.991 our method predicts an increase of the annotation fraction from 0.817 (automatic) to 1.0 and (semi-automatic); it also predicts that visual inspection will take 2.2 hours. As illustrated in Fig. <ref type="figure" target="#fig_11">13</ref>, the corresponding actual value of precision c f for this working point is 0.988, for a final annotation fraction of 0.992; the actual visual inspection takes 2.4 hours.</p><p>In general, the error in the obtained precision c f respect to the desired precision c min is due to modelling errors when calculating the working point for visual inspection (as explained in Sect. VI-B) and the error introduced by manually accepted tracklets that contain wrong annotations. Regarding the obtained annotation fraction #S f /#S c , ideally after visual inspection all frames in the dataset are annotated. In practice however, in some inspected frames the user rejects both the optimal and second hypothesis for annotation. In those cases, the corresponding tracklet is discarded and moved to the subset of rejected annotations S r . Frames that end up in this set are deemed as difficult cases.</p><p>These results were obtained for the value of maximum tracklet size l max = 20 f rames, which we set as a good compromise between visual inspection time and precision of the inspected annotations. We have explored this compromise, as illustrated in Fig. <ref type="figure" target="#fig_12">14</ref>. In this figure, precision and visual inspection time are plotted for several values of maximum tracklet size l max , the specification values for c min given in the corresponding plot Fig. <ref type="figure" target="#fig_0">11</ref> and the specific value d r = 50cm. The yellow lines show the extreme case where every frame in S v is inspected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison Against Semi-Automatic Annotator VATIC</head><p>To compare the manual labour involved in our approach to that of other methods, we also annotated 12 minutes of our multi-camera video with the VATIC system. It took 2 hours of time to annotate 13728 frames. In the proposed semiautomatic method, with d r = 50 cm, 12800 of these frames were automatically annotated. It took 2 minutes of manual work to inspect the remaining frames. In this set of frames, 6 frames (0.043%) had "wrong" annotations, when compared to the ground truth. One reason for the large speed-up is that with VATIC, semi-automatic annotation has to be carried out on each view separately. The user has to draw bounding boxes (806 in total in this specific case) and check the results of the automatic part of the method using a slider in a video player. In our proposed system, the user judges sampled frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. THE MULTIPLE TARGETS CASE</head><p>In this section, we present an extension to the proposed consensus tracker that handles multiple targets. The main problem faced in this case is that of identity switching between targets. This problem could be addressed by modelling the statistics of this specific error mode, in a similar fashion that we deal with the positional error. However, we would need reliable identity references, which are currently difficult to obtain. Instead, we analyse the changes of identity assignment by the different trackers, and detect some of the identity mistakes by relying on the consensus measure of the consensus tracker. Thus, if every tracker is aware of the presence of several targets, the single target approach remains valid but the search space increases according to number of targets present. It is also still possible to trigger manual input in cases where the value R is low and check for the identity switches. In the following subsections we present the specifics of the proposed multi-target extension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Algorithm</head><p>We will extend the notation used before in order to adapt it to the multi-target case. Let r k m be the ground plane position estimated by tracker m of the target k. Let K be the number of targets present at the scene, which we assume to be known by all the M trackers at any moment. In this scenario, we have to calculate K estimates r of the K target positions r. Besides the tracking failures discussed before, we have to account for the possible target identity switch caused mainly by occlusion/disocclusion between targets. Therefore, we have K M possible tracker-target combinations, from which we must find K not mutually exclusive. To illustrate the situation better, let us see the example of Table IV which shows the case for M = 4 and K = 2. For instance, combinations 1 and 2 are mutually exclusive, because trackers 1,2 and 3 identify the same target (k = 1). In the other hand, hypotheses 1 and 16 are plausible together, since in each case the trackers identify a different target.</p><p>In order to estimate the K consensus position estimates, we proceed as follows:</p><p>1) For every multi-tracker/target combination, we maximse eq. ( <ref type="formula" target="#formula_4">1</ref>) and obtain the corresponding K M estimates r. 2) From this resulting set of possible position estimates, we select the one with the highest value of R. This position estimate corresponds to one of the K targets, and we call it r1 . Notice however that at this point we have not yet linked the identity of the target between frames. We will come to that assignment later in Sect. VII-B. 3) For the remaining K -1 targets we repeat step 2, but excluding at every time the mutually exclusive combinations of the preceding estimates. We end up with the set of position estimates rk , with k = 1..K</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Target Identity Between Frames</head><p>For linking target identities between frames, we assume that the true identity of each target is available in at least one frame in the video sequence. In our application this assumption is valid, since for semi-automatic annotation manual user intervention is necessary. We then link target identities between two consecutive frames i and i +1 by matching the optimal trackertarget combinations that were found when computing rk . Let us assume that we know the identity of the targets at frame i . We compare each of the tracker-target combinations of rk (i ) and rk (i +1), and rank the pairings according to the number of configurations tracker/target (m, k) that remain equal between them. We finally select the pairings with higher rank and propagate the target identities from frame i to frame i + 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results</head><p>We tested our multi-target extension of the proposed consensus tracker in 4 video datasets, from which two are already publicly available (EPFL-C <ref type="bibr" target="#b16">[17]</ref> and UvA-T <ref type="bibr" target="#b4">[5]</ref>), one has been tested in a recent publication (UGent-L <ref type="bibr" target="#b53">[54]</ref>) and one is new (UGent-H). Fig. <ref type="figure" target="#fig_5">15</ref> shows some sample frames from these datasets.</p><p>Although our proposed approach to extend the consensus tracker to the multi-target case performs acceptably (see Fig. <ref type="figure" target="#fig_6">16</ref>), there is still room for improvement. The main limitations are the complexity of the algorithm as the number of targets present in the scene increases, and the difficulty of detecting identity switches between targets. Note that, since 2 of the trackers used in the single target case are not capable of handling multiple targets, we excluded them from this scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. DISCUSSION AND CONCLUSION</head><p>We proposed a novel method for semi-automated generation of positional annotations in calibrated multi-camera systems, targeted for long video sequences. A key contribution of this paper is a novel procedure to fuse the outcomes of multiple trackers that involves estimating the error statistics of the individual trackers (and their joint failure modes) by comparing their output to a reliable people detector. The consensus tracker based on these statistics results in more accurate tracking than averaging the trackers outputs, and the individual output of the trackers. Another novelty is that we proposed a method to estimate the time needed for the manual part of the proposed procedure. We also estimate the precision of the resulting semi-automatically annotated data set, under given values of desired accuracy.</p><p>We demonstrated the performance of our method with experiments on a multi-camera video dataset of about 6 hours duration, showing scalability in semi-automatic annotation for long multi-camera sequences for the first time. This leads us to the final contribution of this paper: the annotated data itself, which is available in two versions: the first version results from the proposed procedure; the second from the exhaustive inspection to create ground truth. Since annotated multi-camera data sets of such length are not yet reported in literature, we hope these annotated sequences will be useful to the research community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. FUTURE RESEARCH</head><p>The proposed approach allows comparing the results of a given tracker to the consensus of other trackers. The ultimate goal is to provide a graphical user interface which allows easy visualization of those portions of a data set with large discrepancies between tracking results. In some cases these discrepancies will be due to errors in the reference data. Based on our current internal graphical user interface, it would be easy to allow researchers to indicate segments of video data in which their new tracker outperforms the consensus tracker.</p><p>Obviously the proposed methodology has some limitations. For instance, although an extension to deal with multi-target sequences was proposed, we only evaluated the single-person case fully. However, the overall concept applies to multiperson tracking as well. We leave this part as future work.</p><p>Regarding the model of positional tracking error, our proposed approach opens the possibility to explore models that take into account the dependency on variables like target pose and location.</p><p>Finally, a very important aspect is the time and effort the user has to invest for the manual part of our semiautomatic method of annotation. The visual inspection time can be reduced by optimising the order in which frames with unreliable annotations are shown to the user. In this paper we propose a strategy that prioritizes the seeking of most likely bad annotations; however, other ways of sampling annotations can be explored. Furthermore, by exploring different values of l max , more efficient working points for visual inspection could be estimated. We leave this part as future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Flow-chart of the method proposed for semi-automatic annotation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of the consensus tracker. (a) The white dots represent the real trajectory of the person. A highly reliable multi-camera person detector produces reference data for only some frames; two successful detections are shown (red dots at frames 1 and 7). (b) Several multi-camera trackers are initialised at frame 1 and track the person independently. (c) Consensus tracking result (green dots) and a reliability measure estimate (dotted green circles: the smaller the circle, the higher the consensus among trackers.). (d) Gathering multi-camera tracking statistics. The trackers are re-initialised at frame 28.In between initialisations, statistics on the position error (white arrows) are gathered by comparing to the output of the reliable person detector (red dot with red square). Note that although ideally a tracker would always output a position estimate, in practice trackers may lose the target (trackers 3 and 4 in this example). Additionally, some trackers may detect target loss and inhibit their output until new evidence is found (tracker 3). Thus, properties like robustness and accuracy are often conflicting between each other and therefore it is desirable to collect tracking statistics of complementary trackers.</figDesc><graphic url="image-1.png" coords="4,314.15,59.69,94.34,70.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>R</head><label></label><figDesc>= log P(t 1 . . . t M ) + M m=1 log P (r m , o m |r, t m ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. (a) Visual inspection algorithm. Note that the manual step is limited to decide if an annotation is correct or not. For this decision a cuboid (b) is projected from the annotations r, which allows the user to visualise the target within a maximum allowed error. Accepted (c) and Rejected (d) annotation.</figDesc><graphic url="image-2.png" coords="9,87.71,118.73,85.58,64.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Multi-Camera setup used to generate the video database and ground plane coverage of the cameras.</figDesc><graphic url="image-5.png" coords="9,324.47,58.97,226.15,90.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Poselet detection in 3 views. Light blue corresponds to the poselet hypothesis with the highest score. The red box is the body detection (details in<ref type="bibr" target="#b48">[49]</ref>). Note that with respect to other body parts, the head would be the best reference to reconstruct the real world coordinates of the person position. Therefore, the top-center of the red box (yellow point) is used for triangulation.</figDesc><graphic url="image-7.png" coords="9,405.47,195.41,57.38,68.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Spatial distribution (across the room) of detector references (left plot) and positions output by the consensus tracker (right plot). The logarithm of normalised 2-D histograms is calculated, where the color bar represents corresponding values for the different ground plane positions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig.7. For each tracker m: estimated tracking accuracy distribution, resulting target loss ratio, and probability that a tracker produces no output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Top: Distribution of the reliability measure R(i) conditioned on the status of consensus tracker (correct or not according to d r ): estimated (left) and actual (right). Bottom: Precision of the automatic annotations (S a ) as a function of the reliability measure threshold ? R , for several values of the specification d r : estimated ?a (S a , d r ) (left) and actual c a (S a , d r ) (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .Fig. 11 .</head><label>1011</label><figDesc>Fig. 10. Annotation Fraction of the Consensus Tracker as a function of Predicted (left) and Actual (right) precision, for several values of d r .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Cases where the different trackers are challenged by difficult situations in the dataset. a.) The target interacts with moving furniture. b.) The target interacts with objects that move along the target. c.) The target undergoes different poses while performing activities. d.) The scene undergoes illumination changes prompt by the target. The color correspondence for the trackers is the same as in Fig. 8.</figDesc><graphic url="image-13.png" coords="12,84.47,128.09,88.10,66.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Actual results of our semi-automatic annotation method for different specification values. In each row, curves for a given value of d r are presented. The dashed lines correspond to the corresponding different desired values of c min shown in Fig. 11. In each case, the dashed line starts at the actual working point (c a (S a (? * R ), d r ), #S a (? * R )/#S c ) over the solid line of automatic annotation, and ends at the actual point (c f , #S f /#S c )). The labels in white boxes show the final annotation fraction and the actual visual inspection time.</figDesc><graphic url="image-11.png" coords="12,175.79,58.61,87.98,66.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Results of our semi-automatic annotation method for d r = 50cm and c min values given in the corresponding plot in Fig. 11. In this case, we show the visual inspection time for different values of max. tracklet size l max .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 15 .Fig. 16 .</head><label>1516</label><figDesc>Fig. 15. Some frames from the test sequences for the multi-target case. a.) UGent-L b.) EPFL-C c.) UGent-H d.) UvA-T.</figDesc><graphic url="image-19.png" coords="14,84.47,127.73,88.04,65.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE II NOTATIONS</head><label>II</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE III LIST</head><label>III</label><figDesc>OF TRACKERS USED IN OUR EXPERIMENTS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV POSSIBLE</head><label>IV</label><figDesc>TRACKER-TARGET COMBINATIONS FOR M = 4 AND K = 2</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>All the supplemental electronic material (code, results, datasets, videos) necessary to illustrate the concepts described in this paper can be found by accessing: http://telin.ugent.be/~jonino/tip2016/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>For the sake of completeness, we provide some results obtained with this person detector, specifically the cumulative distribution of tracking errors.</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>This work was supported in part by the <rs type="funder">Flemish Fund for Scientific Research</rs> under Grant <rs type="grantNumber">G0398111N</rs> and in part by the <rs type="funder">Agency for Innovation by Science and Technology</rs>. Digital Object Identifier 10.1109/TIP.2016.2542021 <rs type="programName">TABLE I COMMON MULTI-CAMERA DATASETS USED IN TRACKING EVALUATION (V=VIEWS, L=LENGTH(MIN) -NOTE THE SHORT LENGTH OF MOST SEQUENCES AND THEIR SPECIFIC CONTENT</rs>)</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_8aH83DH">
					<idno type="grant-number">G0398111N</idno>
				</org>
				<org type="funding" xml:id="_rZvP4AE">
					<orgName type="program" subtype="full">TABLE I COMMON MULTI-CAMERA DATASETS USED IN TRACKING EVALUATION (V=VIEWS, L=LENGTH(MIN) -NOTE THE SHORT LENGTH OF MOST SEQUENCES AND THEIR SPECIFIC CONTENT</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>He is currently a Professor with the IPI Group, TELIN Department, Ghent University, where he is a member of the interuniversitary institute iMinds. He has authored over 300 papers in peer-reviewed journals and international conferences. His main research interests are image and video restoration, image analysis, and lossless and lossy data compression of images and video and processing of multimedia data.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Monocular model-based 3D tracking of rigid objects: A survey</title>
		<author>
			<persName><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graph. Vis</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="316" to="323" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Object tracking: A survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recent advances and trends in visual tracking: A review</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="3823" to="3831" />
			<date type="published" when="2011-11">Nov. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A survey of appearance models in visual object tracking</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">58</biblScope>
			<date type="published" when="2013-09">Sep. 2013</date>
			<pubPlace>Art</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A comparative study on multi-person tracking using overlapping cameras</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th Int</title>
		<meeting>9th Int</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="203" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Amazon Mechanical Turk</title>
		<author>
			<persName><surname>Amazon</surname></persName>
		</author>
		<ptr target="http://www.mturk.com" />
		<imprint>
			<date type="published" when="2016">Mar. 29. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Efficiently scaling up crowdsourced video annotation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-012-0564-1</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="184" to="204" />
			<date type="published" when="2013-01">Jan. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tools and techniques for video performance evaluation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mihalcik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICPR</title>
		<meeting>ICPR</meeting>
		<imprint>
			<date type="published" when="2000-09">Sep. 2000</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="167" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Utility data annotation with Amazon mechanical turk</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sorokin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPRW</title>
		<meeting>CVPRW</meeting>
		<imprint>
			<date type="published" when="2008-06">Jun. 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A multi-view annotation tool for people detection evaluation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Utasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Benedek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VIGTA</title>
		<meeting>VIGTA</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">MediaDiver: Viewing and annotating multi-view video</title>
		<author>
			<persName><forename type="first">G</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 30th Conf</title>
		<meeting>30th Conf</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1141" to="1146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Visual tracking via weakly supervised learning from multiple imperfect oracles</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-J</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1395" to="1410" />
			<date type="published" when="2014-03">Mar. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ETISEO, performance evaluation for video surveillance systems</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Nghiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bremond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thonnat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Valentin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AVSS</title>
		<meeting>AVSS</meeting>
		<imprint>
			<date type="published" when="2007-09">Sep. 2007</date>
			<biblScope unit="page" from="476" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title/>
		<author>
			<persName><surname>Pets</surname></persName>
		</author>
		<author>
			<persName><surname>Pets</surname></persName>
		</author>
		<ptr target="http://www.cvg.reading.ac.uk/pets2001/pets2001-dataset.html" />
	</analytic>
	<monogr>
		<title level="j">Dataset</title>
		<imprint>
			<date type="published" when="2001-03-29">2001. Mar. 29. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">PETS2009: Dataset and challenge</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ferryman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shahrokni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. PETS-Winter</title>
		<meeting>PETS-Winter</meeting>
		<imprint>
			<date type="published" when="2009-12">Dec. 2009</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Basketball Dataset From the European Project Apidis</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">De</forename><surname>Vleeschouwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Delannay</surname></persName>
		</author>
		<ptr target="http://www.apidis.org/Dataset" />
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multiple object tracking using K-shortest paths optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Berclaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Turetken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1806" to="1819" />
			<date type="published" when="2011-09">Sep. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A semiautomatic system for ground truth generation of soccer video sequences</title>
		<author>
			<persName><forename type="first">T</forename><surname>D'orazio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Leo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mosca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spagnolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Mazzeo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AVSS</title>
		<meeting>AVSS</meeting>
		<imprint>
			<date type="published" when="2009-09">Sep. 2009</date>
			<biblScope unit="page" from="559" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Qualitative evaluation of detection and tracking performance</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bremond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tax</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AVSS</title>
		<meeting>AVSS</meeting>
		<imprint>
			<date type="published" when="2012-09">Sep. 2012</date>
			<biblScope unit="page" from="362" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">HumanEva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="4" to="27" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A dataset and evaluation methodology for template-based tracking algorithms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lieberknecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Benhimane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISMAR</title>
		<meeting>ISMAR</meeting>
		<imprint>
			<date type="published" when="2009-10">Oct. 2009</date>
			<biblScope unit="page" from="145" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visual tracking: An experimental survey</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1442" to="1468" />
			<date type="published" when="2014-07">Jul. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An information theoretic approach for tracker performance evaluation</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Michael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2009-10">Sep./Oct. 2009</date>
			<biblScope unit="page" from="1523" to="1529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Performance evaluation of multi-camera visual tracking</title>
		<author>
			<persName><forename type="first">L</forename><surname>Marcenaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Morerio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Regazzoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AVSS</title>
		<meeting>AVSS</meeting>
		<imprint>
			<date type="published" when="2012-09">Sep. 2012</date>
			<biblScope unit="page" from="464" to="469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A protocol for evaluating video trackers under real-world conditions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nawaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1354" to="1361" />
			<date type="published" when="2013-04">Apr. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Measures of effective video tracking</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nawaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Poiesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="376" to="388" />
			<date type="published" when="2014-01">Jan. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Is my new tracker really better than yours</title>
		<author>
			<persName><forename type="first">L</forename><surname>?ehovin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WACV</title>
		<meeting>WACV</meeting>
		<imprint>
			<date type="published" when="2014-03">Mar. 2014</date>
			<biblScope unit="page" from="540" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Challenges of ground truth evaluation of multi-target tracking</title>
		<author>
			<persName><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPRW</title>
		<meeting>CVPRW</meeting>
		<imprint>
			<date type="published" when="2013-06">Jun. 2013</date>
			<biblScope unit="page" from="735" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Online empirical evaluation of tracking algorithms</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1443" to="1458" />
			<date type="published" when="2010-08">Aug. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The visual object tracking VOT2013 challenge results</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCVW</title>
		<meeting>ICCVW</meeting>
		<imprint>
			<date type="published" when="2013-12">Dec. 2013</date>
			<biblScope unit="page" from="98" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The visual object tracking VOT2014 challenge results</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV Workshops</title>
		<meeting>ECCV Workshops</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="191" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">LabelMe video: Building a video database with human annotations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2009-10">Sep./Oct. 2009</date>
			<biblScope unit="page" from="1451" to="1458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">LabelMe: A database and Web-based tool for image annotation</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="157" to="173" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A rule-based video annotation system</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dorado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Calic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Izquierdo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="622" to="633" />
			<date type="published" when="2004-05">May 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A tool for vision based pedestrian detection performance evaluation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bertozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Broggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grisleri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tibaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Rose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intell. Vehicles Symp</title>
		<meeting>IEEE Intell. Vehicles Symp</meeting>
		<imprint>
			<date type="published" when="2004-06">Jun. 2004</date>
			<biblScope unit="page" from="784" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">MTAP special issue on methods and tools for ground truth collection in multimedia applications</title>
		<author>
			<persName><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Boom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools Appl</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="409" to="412" />
			<date type="published" when="2014-05">May 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Active frame selection for label propagation in videos</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="496" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Active frame, location, and detector selection for automated and manual video annotation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Karasev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2123" to="2130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A semi-automatic tool for detection and tracking ground truth generation in videos</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kavasidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Salvo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Giordano</surname></persName>
		</author>
		<author>
			<persName><surname>Spampinato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VIGTA</title>
		<meeting>VIGTA</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">An innovative Web-based collaborative platform for video annotation</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kavasidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Salvo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Giordano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools Appl</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="413" to="432" />
			<date type="published" when="2013-05">May 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Video annotation and tracking with active learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="28" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A tool for fast ground truth generation for object detection and tracking from video</title>
		<author>
			<persName><forename type="first">F</forename><surname>Comaschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stuijk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Basten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Corporaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICIP</title>
		<meeting>ICIP</meeting>
		<imprint>
			<date type="published" when="2014-10">Oct. 2014</date>
			<biblScope unit="page" from="368" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Tracking by sampling trackers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2011-11">Nov. 2011</date>
			<biblScope unit="page" from="1195" to="1202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Disagreement-based multi-system tracking</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACCV Workshop Detection Tracking Challenging Environ</title>
		<meeting>ACCV Workshop Detection Tracking Challenging Environ</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="320" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A superior tracking approach: Building a strong tracker through fusion</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bailer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pagani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="170" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Symbiotic tracker ensemble toward a unified tracking framework</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1122" to="1131" />
			<date type="published" when="2014-07">Jul. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Whose vote should count more: Optimal integration of labels from labelers of unknown expertise</title>
		<author>
			<persName><forename type="first">J</forename><surname>Whitehill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bergsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Movellan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Ruvolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="2035" to="2043" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Ensemble-based tracking: Aggregating crowdsourced structured time series data</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1107" to="1115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Detecting people using mutually consistent poselet activations</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="168" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Place-dependent people tracking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Luber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Tipaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">O</forename><surname>Arras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Robot. Res</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="280" to="293" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Independent Markov chain occupancy grid maps for representation of dynamic environment</title>
		<author>
			<persName><forename type="first">J</forename><surname>Saarinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Andreasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst</title>
		<meeting>IEEE/RSJ Int. Conf. Intell. Robots Syst</meeting>
		<imprint>
			<date type="published" when="2012-10">Oct. 2012</date>
			<biblScope unit="page" from="3489" to="3495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Seeking the strongest rigid detector</title>
		<author>
			<persName><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013-06">Jun. 2013</date>
			<biblScope unit="page" from="3666" to="3673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Optimal algorithms in multiview geometry</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACCV</title>
		<meeting>ACCV</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="13" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Low-complexity scalable distributed multicamera tracking of humans</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gr?enwedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Sensor Netw</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2014-01">Jan. 2014</date>
			<pubPlace>Art</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Template matching based people tracking using a smart camera network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="2014-03">Mar. 2014</date>
			<biblScope unit="volume">9026</biblScope>
			<biblScope unit="page" from="90260Y" to="90261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Robust multi-camera people tracking using maximum likelihood estimation</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Bo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACIVS</title>
		<meeting>ACIVS</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="584" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Self-learning voxel-based multi-camera occlusion maps for 3D reconstruction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Slembrouck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis. Theory Appl</title>
		<meeting>Int. Conf. Comput. Vis. Theory Appl</meeting>
		<imprint>
			<date type="published" when="2014-01">Jan. 2014</date>
			<biblScope unit="page" from="502" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Camera-based fall detection using a particle filter</title>
		<author>
			<persName><forename type="first">G</forename><surname>Debard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Baldewijns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Goedem?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Vanrumste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 37th Annu. Int. Conf</title>
		<meeting>37th Annu. Int. Conf</meeting>
		<imprint>
			<date type="published" when="2015-08">Aug. 2015</date>
			<biblScope unit="page" from="6947" to="6950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Joint multi-person detection and tracking from overlapping cameras</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Liem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Understand</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="36" to="50" />
			<date type="published" when="2014-11">Nov. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
