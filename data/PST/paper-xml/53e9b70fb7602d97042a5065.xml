<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Tone-mapping high dynamic range images by novel histogram adjustment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiang</forename><surname>Duan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Economic Information Engineering</orgName>
								<orgName type="institution">Southwestern University of Finance and Economics</orgName>
								<address>
									<settlement>Chengdu City</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Sichuan Province</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marco</forename><surname>Bressan</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Xerox Research Centre Europe</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chris</forename><surname>Dance</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Xerox Research Centre Europe</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guoping</forename><surname>Qiu</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">The University of Nottingham</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Tone-mapping high dynamic range images by novel histogram adjustment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">06B205BDB9E8949C9B5D22B72F810332</idno>
					<idno type="DOI">10.1016/j.patcog.2009.12.006</idno>
					<note type="submission">Received 9 December 2008 Received in revised form 6 November 2009 Accepted 5 December 2009</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Tone mapping Tone reproduction Adaptation High dynamic range Adaptive Bilateral Histogram</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present novel histogram adjustment methods for displaying high dynamic range image. We first present a global histogram adjustment based tone mapping operator, which well reproduces global contrast for high dynamic range images. We then segment images and carry out adaptive contrast adjustment using our global tone mapping operator in the local regions to reproduce local contrast and ensure better quality. We demonstrate that our methods are fast, easy to use and a fixed set of parameter values produce good results for a wide variety of images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The dynamic range of a scene, image or imaging device is defined as the ratio of the highest to the lowest luminance or signal level. The real world thus has a dynamic range of approximately fourteen orders of magnitude. The current performance of digital image capture and display devices can match or even exceed those of silver halide film in many aspects like accuracy and resolution. However, these devices still suffer from a limited dynamic range, typically spanning only two to three orders of magnitude. We call these devices Low Dynamic Range (LDR) reproduction devices in this paper.</p><p>Recently there has been increasing interests in high dynamic range (HDR) imaging to better capture and reproduce real world images. Fig. <ref type="figure" target="#fig_0">1</ref> shows an HDR indoor/outdoor scene with a dynamic range of about 25,000:1. In order to make features in the dark areas visible, most cameras need longer exposure times, but this will render the bright area saturated. On the other hand, using shorter exposure times to capture bright area details will obscure features in the darker areas. These limitations are addressed by recent cameras which allow capturing higher ranges, typically 12-16 bit resolutions, or through the use of HDR radiance maps <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. These HDR radiance maps are obtained by merging properly-exposed portions of many bracketed images and thus record the full dynamic range of the scene in 32-bit floating-point number format. However, these numbers still dwarf the dynamic range of most LDR reproduction devices, such as computer monitors, printers and projectors.</p><p>In order to reproduce HDR radiance maps on LDR devices, technologies are used to convert 32-bit floating-point values to 8-bit integer values in such a way that the visual impressions and feature details of the original real scenes are still faithfully reproduced. These technologies are called tone mapping or tone reproduction. Fig. <ref type="figure" target="#fig_0">1</ref> is an example of our tone mapping method applied to the HDR radiance map of the original scene whose contrast greatly exceeds the contrast available for displaying and printing.</p><p>Our work in this paper is motivated by the need for fast and automatic tone mapping operators which can effectively and efficiently compress high dynamic range scene to display on low dynamic range devices such that the reproduced images look pleasing and evoke our visual experiences in the original HDR scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Review of tone mapping methods</head><p>In the literature, tone reproduction techniques for compressing dynamic range are usually described in two broad categories <ref type="bibr" target="#b3">[4]</ref>: global and local tone mapping operators.</p><p>Global tone mapping operators are computationally very simple and preserve the intensity orders of the original scenes thus avoiding ''halo'' artifacts. Tumblin and Rushmeier <ref type="bibr" target="#b4">[5]</ref> and Ward <ref type="bibr" target="#b5">[6]</ref> pioneered in addressing the tone mapping problem in computer graphics and developed global tone mapping operators. Tumblin and Rushmeier's global operator aimed to match the perceived brightness of displayed image with that of the scene, while Ward <ref type="bibr" target="#b5">[6]</ref> solved the problem in a different approach and aimed to match perceived contrast between displayed image and the scene. Later Ferwerda et al. <ref type="bibr" target="#b6">[7]</ref> proposed a global tone mapping operator based on comprehensive visual model which successfully simulated several important visual effects like adaptation, color appearance, visual acuity and time-course of adaptation. Further, Larson et al. <ref type="bibr" target="#b7">[8]</ref> used a histogram adjustment technique to map real world luminance to display level by capping the display contrast (mapped by histogram equalization) when it exceeded that produced by a linear tone mapping operator and the idea was then refined by human contrast sensitivity. Further the authors achieved a more sophisticated operator by including models of human visual system like glare, color sensitivity, and acuity. More recently, Drago et al. <ref type="bibr" target="#b8">[9]</ref> used an adaptive logarithmic mapping strategy to display high dynamic range images. Their method was based on the observation that the steep slopes in the lower parts of the logarithmic function with a smaller base were likely to increase the contrast and brightness for the low luminance values, ensuring good contrast and visibility in dark areas; while the relatively moderate slopes in the higher part of the logarithmic function with the larger base were likely to compress the higher luminance values. Thus they changed the base of logarithm function based on the luminance level to map scene luminance to display levels. More recently, Duan et al. <ref type="bibr" target="#b9">[10]</ref> proposed a novel global tone mapping operator which use learning-based technique to display high dynamic range image. In the literature, an interesting work was proposed to visualize high dynamic range image with a series of its tone mapped versions produced by different monotonic global mapping <ref type="bibr" target="#b24">[27]</ref>. Since each version is more easily optimized to clearly reveal one part of the image, the whole sets of tone mapped versions can present all the information of the image in a better way. However, this work focused on information visualization of high dynamic range images, rather than tone mapping which we discuss in this paper.</p><p>Global tone mapping operators generally cause loss of details in the mapped images, so local tone mapping operators are applied, at a higher cost, to better preserve the details and local contrast in the images, as local tone mapping technologies consider pixel neighborhood information in the mapping processing for each individual pixel. Tumblin and Turk <ref type="bibr" target="#b10">[11]</ref> used a layer-based method to display high contrast images. They extended anisotropic diffusion that is the edges-preserving low-pass filter <ref type="bibr" target="#b23">[26]</ref> to a new low-curvature image simplifier (LCIS) to decompose an image into different layers. Then different layers were compressed using different coefficients. Afterwards, these compressed layers were reconstructed to obtain displayable image. Their LCIS in the decomposition stage ensured extracting details from high-contrast images and avoiding halo artifacts. More recently, using only two ''layers'', Durand and Dorsey <ref type="bibr" target="#b11">[12]</ref> introduced a simpler layer-based method to display high dynamic range images. They filtered the image with a bilateral filter into a base layer and a detail layer. Only the base layer had its contrast reduced and the detail layer was left untouched. The processed layers are reconstructed to produce the mapped image. The choice of the bilateral filter ensured that the edges be protected in the decomposition process as LCIS did, which thus avoided introducing halo artifacts in the reconstruction stage. Bilateral filtering method was also adopted by Li et al. <ref type="bibr" target="#b22">[25]</ref>. They first decomposed images into two ''layers'' using bilateral filter, then they used global mapping function to adjust the base layer and enhanced the detail layer adaptively using gain map obtained in the base layer. Finally, they combined both the layers together to obtain the tone mapped images with good local contrast. However, the reported computational speed is very slow for this method. In <ref type="bibr" target="#b12">[13]</ref>, the authors presented a multiscale image processing technique to display high dynamic range image. In their method, they used a symmetrical analysis-synthesis filter bank, and computed a smooth gain map for multiscale subband images. Their methods successfully avoided haloes that were usually related with multiscale methods. Reinhard et al. <ref type="bibr" target="#b13">[14]</ref> used a novel local approach to map high dynamic range images to a display. They based their method on the well-known photographic practice of dodging-and-burning to reproduce the local contrast for high contrast images. Fattal et al. <ref type="bibr" target="#b14">[15]</ref> proposed a method based on the manipulation of the gradient domain in the logarithmic space. They calculated the gradient in the logarithmic luminance domain and thus detected the contrast magnitude in the corresponding position in the original luminance domain. Then scale factors were used to attenuate the larger gradient more than the smaller gradient, which was meant to compress the larger contrast more than the smaller contrast. Finally the attenuated gradient field was converted back by integration to the logarithmic luminance domain followed by exponentiation in order to return to the luminance domain. More recently, Krawczyk et al. <ref type="bibr" target="#b15">[16]</ref> derived a tone mapping operator which was based on the segmentation of an HDR image into frameworks (consistent areas) using anchoring theory of lightness perception and on the local calculation of the lightness values. The final lightness of an image was computed by merging the frameworks proportional to their strength. Lischinski et al. <ref type="bibr" target="#b16">[17]</ref> proposed a novel interactive local adjustment method for tonal values and other visual parameters in an image. In their method, the users can use a few simple brush strokes to indicate the regions of interest, then the system adjusts the brightness, contrast and other parameters in these regions automatically.</p><p>In this paper, we also investigate tone mapping for displaying high dynamic range images. Manipulation of image histogram is usually a way to control image contrast. In our work, we first propose a novel fast global histogram adjustment based tone mapping operator, which effectively utilizes the full dynamic range of display and thus well reproduces global contrast for high dynamic range images. However, this operator cannot better preserve local contrast and details, which is the common drawback of global tone mapping operators. When viewing high dynamic range scenes, the local adaptation mechanism in human visual system helps us to see the details in all parts in the scenes. There have been researches to model and better understand this mechanism like <ref type="bibr" target="#b17">[18]</ref>. Inspired by these works, but not computationally building on these models, we extend our global tone mapping operator to a novel local tone mapping operator. The technique segments the images and carries out adaptive contrast adjustment using our developed global tone mapping operator in the local areas so that each local area can better utilize the full dynamic range of the display and at the same time avoid noise artifacts. Since the adjustment is based on each independent local area, sharp jumps among different blocks are unavoidable after the adaptive adjustment. To solve this problem, we present a novel bilateral weighting scheme to eliminate associated boundary artifacts and halo artifacts caused by the segmentation. We have tested our proposed local tone mapping operator for a wide range of HDR images and it shows good results that are comparable with the state-of-the-art tone mapping methods in image quality, automation and computational speed. To present our method, we organize this paper as follows. In the next section, we first present our fast global histogram adjustment based tone mapping operator. In Section 4, we describe our local histogram adjustment based tone mapping operators. Sections 5 and 6 discuss the performance of our proposed operators. Section 7 concludes our presentation and briefly discusses future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Global tone mapping operator</head><p>Applying the logarithm function to luminance increases the contrast and brightness for the low luminance values while compressing the higher luminance values. This has been applied in different ways for the display of HDR images <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b18">19]</ref>. As an initial step, we use the following function to compress the luminance of the high dynamic range image I to compact luminance D</p><formula xml:id="formula_0">DðIÞ ¼ ðD max ÀD min Þ Ã logðI þ tÞÀlogðI min þ tÞ logðI max þ tÞÀlogðI min þ tÞ þD min<label>ð1Þ</label></formula><p>where I min and I max are the minimum and maximum luminance of the scene, D max and D min are the maximum and minimum display levels of the visualization devices (which are usually 0 and 255) and t controls the overall brightness of the mapped image. In general, a larger t makes the mapped image darker and smaller t makes the mapped image brighter as shown in Fig. <ref type="figure" target="#fig_2">2</ref>. The choice of t is a trial-and-error process, in order to avoid this difficulty, we propose an approach to automatically set the parameter t.</p><p>The method works well for a wide range of images and can automatically tune the mapped images to an appropriate overall brightness.</p><p>The idea is inspired by the work of <ref type="bibr" target="#b19">[20]</ref>, and based on the assumption that the log-average luminance of a scene should be mapped to a specific point (key value) in the display dynamic range depending on scene brightness. More specifically, the brighter the scene is, the closer the point that the log-average luminance is mapped to should be towards the top end of the display dynamic range (higher key value). We have developed the following steps to automatically estimate t.</p><p>1. Calculate the log-average luminance I ave of the scene as</p><formula xml:id="formula_1">I ave ¼ exp 1 N X x;y logðe þ Iðx; yÞÞ " #<label>ð2Þ</label></formula><p>where N is the total pixel number in the image. As I(x,y) is the luminance value whose minimal can be 0 for pure black point, a</p><p>small value e is used to avoid the singularity that occurs with 0 values in these cases when taking logarithm operation.</p><p>2. Calculate the key value (larger key values correspond to brighter images and smaller ones correspond to darker images) of the image on the scale between 0 and 1 using</p><formula xml:id="formula_2">k ¼ A Â B ð2 log IaveÀlog I min Àlog Imax=log ImaxÀlog I min Þ<label>ð3Þ</label></formula><p>where constants A and B are empirically set to 0.4 and 2, and thus k ranges from 0.2 to 0.8. The above equation suggests that if the average brightness of the image goes to the lower end between the maximum brightness and minimum brightness, the image should be a darker image and thus assigned a relatively smaller key value, otherwise, a larger key value is assigned.</p><p>3. Decide the offset such that I ave in the image is mapped to the key value determined above on the scale between 0 and 1:</p><formula xml:id="formula_3">k ¼ logðI ave þ tÞÀlogðI min þ tÞ logðI max þtÞÀlogðI min þ tÞ<label>ð4Þ</label></formula><p>The offset t can be solved using numerical calculation (Newton method is applied in our study and the offset t can be found within 20 iterations). An automatically computed offset t is shown in Fig. <ref type="figure" target="#fig_2">2</ref> (right bottom), which sets the image to a proper brightness.</p><p>With Eq. ( <ref type="formula" target="#formula_0">1</ref>), although the overall brightness of the mapped image can be appropriately set, the mapped images do not have sufficient details and appear with low contrast as shown in Fig. <ref type="figure" target="#fig_3">5</ref> (Most right one).</p><p>The reason that directly rendering compact luminance D(I) for display will result in the lack of detail and contrast is caused by linear quantization as illustrated in Fig. <ref type="figure" target="#fig_1">3(a)</ref>. In this case, the range of D(I) is divided into equal-length N=256 intervals using cutting points l n , and pixels falling into the same interval are grouped to have the same integer display level d in the range between 0 and 255. Quantization is done purely on the basis of the actual pixel values without taking into account the image's pixel distribution characteristics. As a consequence, in densely populated intervals, too many pixels are squeezed into one display level, resulting in a loss of detail and contrast, whilst in sparsely populated intervals, too few pixels occupy quite a few valuable display levels thus resulting in the under utilization of display levels. A traditional technique that takes into account pixel distribution is histogram equalization as shown in Fig. <ref type="figure" target="#fig_1">3(b)</ref>. In this case, the method divides the range of D(I) into N=256 intervals using cutting points e n based on equally distributing pixel population. The division of these intervals is purely based on the pixel population distributions and the actual pixel  values spanned in the range of D(I) are not taken into account. Again pixels falling into the same interval are mapped (quantized) into the same integer display level d. Although, the display levels are fully utilized in such kinds of circumstances, densely populated luminance intervals can result in the exaggeration of contrast while in sparsely populated luminance intervals compression is too aggressive.</p><p>Fortunately the drawbacks of linear quantization and histogram equalization mapping are compensated by one another. In order to achieve the desirable results, the designed tone mapping operator should strike a balance between the linear quantization and histogram equalization using cutting points le n as shown in Fig. <ref type="figure" target="#fig_1">3(c)</ref>.</p><p>Therefore, the next step of our global tone mapping operator is to take D(I) as input and adjust its histogram using cutting points le n according to Fig. <ref type="figure" target="#fig_1">3</ref>(c), i.e., to map D(I) to the integer display level d using an approach that is between linear quantization and histogram equalization.</p><p>Here, we present a global tone mapping operator called histogram adjustment based linear to equalized quantizer (HALEQ) to perform the mapping of Fig. <ref type="figure" target="#fig_1">3(c)</ref>, which makes the linear to equalized quantization process extremely easy and fast.</p><p>With reference to Fig. <ref type="figure" target="#fig_1">3</ref>, for linear mapping, the range of D(I) is cut at l 1 , l 2 ,y,l N À 1 . For histogram equalization, the range of D(I) is cut at e 1 , e 2 ,y,e N À 1 . If we want to achieve linear to equalized quantization, a more straight solution is to find the cuts le 1 , le 2 ,y,le N À 1 , which satisfy the following relation:</p><formula xml:id="formula_4">le n ¼ l n þ bðe n Àl n Þ ð 5Þ</formula><p>where 0 rb r1 is a controlling parameter. If b=0, the quantization is linear, b=1, the quantization is histogram equalized. In a sense, b controls the contrast enhancement level in the mapping process. Setting 0 obo1, we can strike a balance between the two extreme forms.</p><p>To implement HALEQ, we have developed a highly efficient recursive binary cut approach. In this developed scheme illustrated in Fig. <ref type="figure">4</ref>, we first find the cut l 0 that divides the range of D(I), which spans between [D min , D max ], into two equal-length intervals (linear quantization to two levels), then another cut e 0 such that the number of pixels on either side of the cut is identical and equal to half of the total pixel population (histogram equalized quantization to two levels). We then find a cut le 0 that is between these two cuts depending on the value of b in Eq. <ref type="bibr" target="#b4">(5)</ref>. In this way we have divided the pixels into two groups [D min , le 0 ] and [le 0 , D max ]. Each group is then subsequently and independently divided into two groups following the same operation and we get 4 groups [D min , le 1,0 ], [le 1,0 , le 0 ,], [le 0 , le 1,1 ] and [le 1,1 , D max ]. The process is applied recursively 8 times to generate the desired number of groups that is 255 and pixels within the same group are mapped to the same low dynamic range display level.</p><p>Fig. <ref type="figure" target="#fig_3">5</ref> shows an example of the mapping results and mapping curves of our operator and those of histogram equalization, Larson et al.'s <ref type="bibr" target="#b7">[8]</ref> and linear compression. For this image, there are very few very bright pixels (correspond to the area of the lamps), yet these are very important pixels. Histogram equalization clearly compresses these pixels too aggressively as the population of these pixels is relatively small, so does the method of Larson et al. <ref type="bibr" target="#b7">[8]</ref> because this is essentially an approximate histogram equalization method in treating sparsely populated intervals. It is shown clearly that the areas of the lamps in the images mapped by Larson et al.'s method and histogram equalization have been saturated because compression is too aggressive in this part and not enough display levels have been allocated to this part, this fact is demonstrated more clearly in the mapping curves. For the linear compression method (discussed in Fig. <ref type="figure" target="#fig_1">3(a)</ref>), pixel population is never taken into account and the display level is equally assigned, thus, relative large portion of display levels can be assigned to the bright part (lamp region), but at the cost of reserving less display levels for other parts of the image. Thus more details in the lamp show up in the image, however, the overall contrast and details of the image are not well presented as shown in Fig. <ref type="figure" target="#fig_3">5</ref> (Most right one). In contrast, our algorithm HALEQ strikes a balance between the methods discussed above and is more flexible in assigning display levels according to image content, thus keeping good overall contrast and details for both the lamp region and other parts of the image.</p><p>Our operator is related to the histogram adjustment technique of <ref type="bibr" target="#b7">[8]</ref> in the sense that both manipulate the histogram. In fact, we were encouraged by the results of <ref type="bibr" target="#b7">[8]</ref> in which the authors demonstrated that simple histogram manipulation might offer an optimal solution if the goals were to produce ''good, natural looking image without regard to how well a human observer would be able to see in the real environment''. However, the histogram adjustment method of <ref type="bibr" target="#b7">[8]</ref> has several drawbacks. Firstly, the method only caps the display contrast (mapped by histogram equalization) when it exceeds that produced by a linear tone mapping operator and the idea is further refined by human contrast sensitivity. However, in sparsely populated luminance intervals, compression is approximately achieved by histogram equalization. This means that some sparse areas that span a wide range will be compressed too aggressively. Features (details) in such intervals which are visible to human eyes could be lost. This unsatisfactory aspect of this algorithm is clearly illustrated in the example in Fig. <ref type="figure" target="#fig_3">5</ref>. Secondly, if the dynamic range of the scene is already in the device range, the method of <ref type="bibr" target="#b7">[8]</ref> uses linear scaling. However, as it is well-known that sometimes linear mapping would result in the under utilization of display levels and thus leading to low contrast images, even though the dynamic ranges of the scenes fit within that of the visualization devices. Our method overcomes these drawbacks and offers a more comprehensive and flexible histogram adjustment solution to the mapping of HDR images. Despite these advantages, the HALEQ technique described above has the common drawback of global tone mapping operators, i.e. the mapped images still lack local contrast and details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Local tone mapping operator</head><p>The human visual system has a more limited response range than the 10 14 :1 range found in real world scenarios. We can cope Fig. <ref type="figure">4</ref>. Recursive binary cut approach implementation of HALEQ. The algorithm first divides the range of D(I) into two segments according to Eq. ( <ref type="formula">5</ref>). Then these two segments are each independently divided into 2 segments according to Eq. ( <ref type="formula">5</ref>). The process is then applied recursively onto each resultant segment to divide it into 2 new segments based on Eq. ( <ref type="formula">5</ref>) until the predefined number of segments (256) are created.</p><p>with the large dynamic range by a visual mechanism called adaptation, which can be explained as the quality to accommodate to the level of a certain visual field around the current fixation point <ref type="bibr" target="#b7">[8]</ref>. Stevens and Stevens <ref type="bibr" target="#b17">[18]</ref> carried out experiments to scientifically explain human visual response versus different luminance levels under different adaptation levels. In their experiments, an observer was first adapted to a background with uniform luminance, i.e. an adaptation level. Then they were gradually presented a small target luminance which was different with the adaptation luminance level. Then their visual response (brightness) for this target luminance was recorded. Their experiment results explained the phenomenon, i.e. higher luminance points can be perceived darker than lower ones when they are located in different adaptation levels. More importantly, their results showed that different luminance intervals could result in overlapped reactions on the limited response range of the visual system, thus extending our visual response range to cope with the full dynamic range of high contrast scenes.</p><p>Inspired by the local adaptation mechanism of human visual system as described above, we design a novel Adaptive Local Histogram Adjustment (ALHA) based tone mapping operator to better render high dynamic range image for display.</p><p>A typical characteristic of HALEQ is its effective utilization of the dynamic range of the display based on global pixel statistics as described before. If we can segment the image into small regions and apply HALEQ in each local area, each area would have a full display dynamic range to utilize based on its local pixel statistics. This is equivalent to extending the limited display dynamic range.</p><p>Local regions may obtain larger display dynamic range, and they might overlap with each other. This is consistent with our local adaptation mechanism as described before and thus better matches our visual experience. In the following, we detail our approach, which applies the HALEQ technique in local areas to better map high dynamic range image for display.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">HALEQ in local regions</head><p>In our adaptive local histogram adjustment based operator, the first step is the logarithmic mapping (Eq. ( <ref type="formula" target="#formula_0">1</ref>)). The following step is to determine to which local regions we will apply the HALEQ technique. There are different ways of segmenting an image into local areas, such as clustering and the method used in <ref type="bibr" target="#b15">[16]</ref>. In our study, we aim at providing a fast local operator, so we simply divide the image into non-overlapping regular rectangular blocks. Fig. <ref type="figure" target="#fig_4">6</ref> (left) illustrates our approach. Based on the pixel statistics in each block, we compute local HALEQ n (nAN, where N is the number of blocks. In Fig. <ref type="figure" target="#fig_4">6</ref>, we only use 9 blocks for easy illustration, i.e., N= 9) in the same way as in the global case described in the last section. We use a common parameter b=0.6 for all the blocks as the initial study in our local operator and we will introduce an adaptive method to choose b later. If we regard HALEQ n as the mapping function, for an individual pixel value D(x,y) output by Eq. ( <ref type="formula" target="#formula_0">1</ref>), output integer display level d(x,y) is given by dðx; yÞ ¼ HALEQ n ½Dðx; yÞ ðx; yÞ A n ð6Þ</p><p>In Fig. <ref type="figure" target="#fig_5">7</ref>, we show how higher luminance pixels might be transferred to a darker display level compared with lower luminance pixels, and how the same pixel values could be transferred to different display levels. In this figure, there are two mapping functions HALEQ A and HALEQ B , which are separately developed based on the pixel local statistics in two individual blocks (area A and area B) for an image (The right image of Fig. <ref type="figure" target="#fig_7">9</ref>). These mapping functions are drawn in the plot of D(I) against display levels. In addition, the histograms of the original D(I) are plotted using linear quantization in the same figure, which indicates that the overall brightness of area A is lower than that of area B. Now we pick up two points P A and P B with luminance levels respectively above and below their corresponding adaptation luminance (average luminance levels for areas A and B). From the figure, we can see that our mapping functions map P B to lower display level than P A (d1od2), which matches our visual experience, i.e. a point with a luminance level higher than the adaptation luminance (average luminance level) in a dark region can be perceived brighter than a point with a luminance level that is lower than the adaptation luminance in a bright region, though the actual luminance level of the latter can be equal or even higher than that of the former. In the same figure, we can also see that two non-overlapped intervals in D(I) can result in overlapped display level intervals (marked as red and blue) thus increasing the chance of a local area to use wider spans of display dynamic range to convey more contrast appearance.</p><p>Fig. <ref type="figure" target="#fig_6">8</ref> shows the normalized histograms of area A after applying different approaches to D(I). The histogram on the top is generated from the linear quantization. The histogram in the middle is generated from the original HALEQ, which exhibits a wider shape than that on the top. This is why the output image from HALEQ shows improved contrast compared with that from linear quantization (Fig. <ref type="figure" target="#fig_3">5</ref>). The histogram at the bottom is generated from local HALEQ that breaks through the constraint of monotonic mapping relationship and utilizes a much wider display dynamic range to improve local contrast. Fig. <ref type="figure" target="#fig_7">9</ref> shows the mapping results from the local HALEQ method. Obviously, these images show more details and local contrast in either dark or bright regions. However, the direct application of HALEQ in each independent local area causes sharp jumps among different blocks. The result is the boundary artifacts in the images as can be seen in Fig. <ref type="figure" target="#fig_7">9</ref>, making the mapped images unacceptable despite the improvement in detail visibility and local contrast.</p><p>In order to solve the boundary artifacts, we introduce an approach as illustrated in Fig. <ref type="figure" target="#fig_4">6</ref> (right). For each pixel value D(x,y) in the image, the final mapped pixel value is the weighted average of the results from tone mapping functions HALEQ 1 [D(x,y)], HALEQ 2 [D(x,y)], HALEQ 3 [D(x,y)], etc. according to a distance weighting function as following: dðx; yÞ ¼</p><formula xml:id="formula_5">P n ¼ K n ¼ 1 HALEQ n ½Dðx; yÞ Á w d ðnÞ P n ¼ K n ¼ 1 w d ðnÞ<label>ð7Þ</label></formula><p>where distance weighting function w d is calculated as</p><formula xml:id="formula_6">w d ðnÞ ¼ e Àðdn=s d Þ<label>ð8Þ</label></formula><p>In our experiment, we used a fixed block size of 32 Â 24 pixels when dividing the images into blocks. d n is the Euclidean distance between the current pixel position and the centers of each of the  With setting the value for the boundary smoothing factor s d to 20, 5 Â 5 neighborhood blocks work well so the sum in Eq. ( <ref type="formula" target="#formula_5">7</ref>) is only made over 25 blocks, i.e., K =25.</p><p>Fig. <ref type="figure" target="#fig_0">10</ref> shows the image after considering the distance weighting function. We can see that good results are obtained and the disturbing boundary artifacts are gone. These images show details and local contrast and give a much more natural appearance than those in Fig. <ref type="figure" target="#fig_7">9</ref>. However, close observation of the resulting images show that there are still two issues that need to be solved in order to ensure the quality.</p><p>The first is related with the nature of HALEQ and the fact that it applies a contrast enhancement approach on images. It is wellknown that contrast enhancement approaches can introduce noise in uniform areas when the enhancement level is too large. In our case of manipulating contrast using HALEQ, b controls the enhancement level as described in Section 3. Thus, an adaptive approach for determining the values of parameter b is necessary in order to avoid noise artifacts in uniform area. From Eq. ( <ref type="formula">5</ref>), lower values of b will decrease the level of equalization and be more adequate for uniform areas. The second issue focuses on the halo artifacts that arise when objects' contour neighbor uniform areas. This is mainly due to the nature of the distance weighting function.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Adaptive selection of parameter b in uniform areas</head><p>In the previous section, a common parameter b was applied to all the blocks across the image to improve the local contrast, and this can introduce noise artifacts in relatively uniform areas. In order to solve this problem, a direct approach is to decrease the degree of contrast enhancement in the blocks with relatively uniform areas, which can be achieved by decreasing the parameter b of HALEQ technique. The main challenge is to measure uniformity in order to detect these relatively flat areas in the images. One typical characteristic of blocks with uniform areas is that these blocks exhibit a narrow shaped histogram after logarithmic mapping, as shown in Fig. <ref type="figure" target="#fig_0">11</ref> (left) and (middle). The block located in the uniform sky area has a more concentrated distribution than that in another part in the image. A number of statistics such as entropy or measures of dispersion can be used for measuring this situation. In our case we proposal the following Eq. ( <ref type="formula" target="#formula_7">9</ref>) as the uniformity measurement for each block n, which has proved to give consistently good results.</p><formula xml:id="formula_7">SD n ¼ P i ¼ M i ¼ 0 jHistðiÞÀmean n j Bin number<label>ð9Þ</label></formula><p>where M is the bin number and Hist(i) is the pixel population in ith bin and the mean is the mean pixel population in each bin as shown in Fig. <ref type="figure" target="#fig_0">11</ref>. A larger value of SD n means that pixel population distribution is further away from the mean pixel population in each bin, corresponding to more uniform luminance distributions. Now if</p><formula xml:id="formula_8">SD n Z Z<label>ð10Þ</label></formula><p>Then the region can be regarded as a uniform area and the contrast enhancement should become moderate. Z is the threshold which determines that at what level, the block can be regarded as a uniform area. Smaller Z considers more areas to be uniform areas and decreases the contrast enhancement in these areas. The effect is that the resulting images show few noise artifacts at the cost of an overall decreased local contrast. In our experiment, we find setting the values to 17.0 when choosing 20 bin number (M=20) gives good results. Fig. <ref type="figure" target="#fig_0">11</ref> (right) uses the black blocks to mark the detected uniform regions in the images using the above approach.</p><p>After detecting the uniform areas in the image, we propose Eq. ( <ref type="formula">11</ref>) based on our experiment to smoothly decrease the parameter b in these areas to decrease the contrast enhancement depending on the degree of uniformity.</p><p>b ¼ 0:6 Ã ½1Àe Àð20ÀSDnÞ ð 11Þ Eq. ( <ref type="formula">11</ref>) means that the larger the value of SD n , the smaller the value of b. This corresponds to what we discussed above, larger values of SD n correspond to more uniform area and the contrast enhancement should become moderate. Fig. <ref type="figure" target="#fig_9">12</ref> shows the plot of b against SD n according to Eq. <ref type="bibr" target="#b10">(11)</ref>. Fig. <ref type="figure" target="#fig_10">13</ref> shows the mapped image before and after the consideration of the uniform areas in the image. For this example we can see the sky has a more natural appearance. We now turn to the removal of halo artifacts in the contours of objects neighboring uniform areas with strong difference in luminance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Removing halo artifacts</head><p>From Fig. <ref type="figure" target="#fig_10">13</ref> (right), it can be seen that halo artifacts are likely to occur where contours of objects neighbor uniform areas. Fig. <ref type="figure" target="#fig_11">14</ref> illustrates the reason for this situation. When we divide the image into regular blocks, we cannot guarantee that pixels belonging to a uniform area are always grouped into the same block, and they might be separated into different blocks. Since the corresponding local HALEQ operators can be based on very different luminance distributions, two pixels with similar value but on different regions can be projected to have very different values. The distance weighting operation (Eq. ( <ref type="formula" target="#formula_5">7</ref>)) can smoothen the difference; but cannot completely eliminate it because it gives preference to the block where the pixel is located. Although this is what we want in order to show local contrast, we do not want this to happen when pixels obviously belong to uniform areas and exhibit uniform characteristics. In order to solve the halo artifact problem, we add a similarity weighting function to Eq. ( <ref type="formula" target="#formula_6">8</ref>   propose a bilateral weighting scheme as shown in the following Eq. ( <ref type="formula" target="#formula_9">12</ref>).</p><p>dðx; yÞ ¼</p><formula xml:id="formula_9">P n ¼ N n ¼ 1 HALEQ n ½Dðx; yÞ Á w d ðnÞ Á w s ðnÞ P n ¼ N n ¼ 1 w d ðnÞ Á w s ðnÞ<label>ð12Þ</label></formula><p>where</p><formula xml:id="formula_10">w d ðnÞ ¼ e Àðdn=s d Þ<label>ð13Þ</label></formula><formula xml:id="formula_11">w s ðnÞ ¼ e Àðsn=ssÞ<label>ð14Þ</label></formula><formula xml:id="formula_12">s n ¼ jDðx; yÞÀDmean n j D max<label>ð15Þ</label></formula><p>w d (n) is the distance weighting function introduced before, w s (n) is the similarity weighting function, D max is the maximum value in D(x,y), S n is the normalized difference between current pixel value and the average pixel value (Dmean n ) of block n. Although the distance weighting function smoothes out big jumps among blocks, it still ensures preserving local contrast because it always assigns the largest weighting to the HALEQ in the block where the pixel is located. The similarity weighting function increases the chance that similar pixels in the uniform areas are mapped to similar values even when they belong to different blocks. Smaller values of s s facilitate the elimination of haloes but will produce image with less local contrast. This is because the weights from the distance weighting function are likely to be undermined by the similarity function as shown in Fig. <ref type="figure" target="#fig_12">15</ref>. In our experiment, we find that using 0.1 for s s gives balanced good results, which not only preserves the local contrast but also attenuates halo artifacts always occurring close to the contours of the objects that neighbor uniform areas as shown in Fig. <ref type="figure" target="#fig_12">15</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Computational efficiency</head><p>In our implementation of HALEQ global tone mapping operator, we aim to provide a highly efficient processing approach. In order to achieve this goal, we approximate the floating point number by densely quantizing them to integers so that we can use LUT to store the tone mapping function obtained using the approach described in Section 3 and then quickly use it to map pixel radiance values to display levels. The approach is very efficient and it only takes about 0.1 s to compute a 1024 Â 768 pixel image on a T7300 with 2.0 GHz CPU, which also facilitates the fast computation of the first step of ALHA, i.e., obtain all the tone mapping functions in blocks. In the ALHA implementation, the most computationally extensive part is the weighting process. However, this part can also be easily accelerated by using the following strategies: 1. Pre-calculate the distance weighting function for all the pixels only for one block and then repeatedly use the computed values for all other blocks, 2. The calculation of the similarity function can be approximated, pre-calculated and stored in LUT to avoid the extensive exponential calculation for each pixel. By using these approaches, our ALHA is very computationally efficient. Here we compare our ALHA operator with 3 state-of-the-art tone mapping operators. To map a 1024 Â 768 pixel image on a T7300 with 2.0 GHz CPU takes about 0.4 s using our ALHA operator, which is about 5 times faster than Fattal et al.'s operator <ref type="bibr" target="#b14">[15]</ref> and 6 times faster than Reinhard et al.'s <ref type="bibr" target="#b13">[14]</ref> operator, and slightly slower than Durand &amp; Dorsey's bilateral filter technique <ref type="bibr" target="#b11">[12]</ref>. Another property of our ALHA tone mapping operator is that the derivation of the tone mapping functions in different blocks is independent of each other and the weighting process for each pixel only associates with 5 Â 5 neighboring elements. The potential advantage of such kind of design makes our ALHA tone mapping algorithm very suitable for parallel calculation, which stand a very good chance of being accelerated to real-time computation using GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Compare ALHA tone mapping operator with other tone mapping operators</head><p>The assessment of tone mapping algorithms is an important research topic in the tone mapping literature. However, it is still hard to find a computational model which can determine the superiority of the tone mapping algorithms. Thus, a better way of understanding the superiority of the tone mapping algorithms is to use human observers and psychophysical experimentation to evaluate their performance. In addition, the rendering intent of assessment such as accurate or pleasing rendering <ref type="bibr" target="#b20">[21]</ref> should be specified, as different rendering intents would cause different assessment results for the same tone mapping algorithms. Recently, some work investigated the performance of tone mapping algorithm upon non-pictorial (scientific) high dynamic range images <ref type="bibr" target="#b25">[28]</ref>, which again led to different assessment results for this particular rendering intent. In our we assess the performance of pleasing rendering for tone mapping algorithms, because it is hard for the users to experience the real scenes in order to carry out the accurate rendering assessment, and also in most of the circumstances users' judgments are only based on how pleasing the tone mapped image looks. In the literature <ref type="bibr" target="#b20">[21]</ref>, Durand &amp; Dorsey's bilateral filter technique is considered to perform best overall in pleasing rendering aspect. Thus, in our experiment, we setup psychophysical experimentation to compare our ALHA algorithm with theirs. Since the main focus of the paper is the development of the tone mapping algorithms, we did not seek a comprehensive way of doing the evaluation. Thus a relatively simple but fair pair comparison psychophysical experimentation was used to evaluate the performance of both tone mapping algorithms.</p><p>We select 21 images for our experiment. These images include both indoor and outdoor photographs covering wide range of high dynamic range sceneries. Among these images, some of them are standard HDR radiance maps that are widely used in tone mapping algorithm test and the others are selected from the sample image sets in some of the HDRI software companies' webpages <ref type="bibr">[22,</ref><ref type="bibr">23]</ref> as shown in Fig. <ref type="figure" target="#fig_4">16</ref>, which are then combined to HDR radiance maps.</p><p>Each image was rendered by both Durand &amp; Dorsey's bilateral filter technique and our ALHA tone mapping technique. As we know, careful choice of parameters for the tone mapping algorithms can usually produce more satisfactory image quality. Thus in order to carry out a relative fair comparison, we use the default parameter sets of each algorithm to compute all the test images in our psychophysical experimentation. For our algorithm, all the proposed default parameters discussed in the previous sessions are used to compute all the images; for Durand &amp; Dorsey's bilateral filter technique, we used the software provided by them and used the same default parameter setting to compute all the images. For each image, the pairs from both tone mapping algorithms were displayed side by side on a CRT monitor in a dim room, but the left and right positions of the two images rendered with the two algorithms are switched randomly for different pairs. All of these pairs are presented to the participants in a fixed sequence order. thirty-one participants who are university students with normal color vision and varying imaging experience were asked to compare the two rendered images and choose the one they preferred most. If they had no preference, they could choose ''Don't care'' option.</p><p>Table <ref type="table" target="#tab_0">1</ref> summarizes our experiment result. In the table, the performance for each case is calculated as the ''Win'' score plus half of the ''Don't Care'' score. The yellow color marks the cases where our algorithm performs better than Durand &amp; Dorsey's technique. Overall, their technology wins 3 more times than ours in total 21 cases. After the experiment, attendants usually reported that Durand &amp; Dorsey's bilateral filter approach produced images with more local contrast in comparison with ours, while ours had the advantage of preserving more details in comparison with theirs. Fig. <ref type="figure" target="#fig_13">17</ref> shows one example of these images pairs rendered by both methods in the experiment. It can be seen that Durand &amp; Dorsey's approach produces images with more local contrast, while ours clearly shows more details like areas in the windows region (amplified area in Fig. <ref type="figure" target="#fig_13">17</ref>). The experiment results suggest that users tend to prefer images showing more local contrast if the difference of details between images are not very big and these are cases when their methods win ours. Although our technology loses in case numbers, we still win in total counts in comparison with theirs, i.e., our technology wins 318 times totally, while theirs wins 316 totally as marked in pink. The result is interesting and suggests that, for some cases, our algorithm performs much better than theirs so that we can still win in the total counts in spite of losing in total case numbers. This implies that their algorithm is likely to produce much less satisfactory results for some particular cases under their default parameter settings. For example, the ''Apartment'' image produced by their method exhibits a very degraded image quality, and thus our method wins 27 times while theirs wins only 3 times. In comparison, the image quality produced using our algorithm would not vary considerably for various image contents and gives consistent nice-rendering results. This means that our algorithm has better automation to produce good tone mapped images in comparison with theirs. Thus we believe that our algorithm is more suitable for automatical batch processing purpose in comparison with theirs.</p><p>Besides the above detailed comparison with a super tone mapping operator, we would like to carry out general comparisons between our ALHA operator and another 3 well established tone mapping operators in the literature as shown in Fig. <ref type="figure" target="#fig_15">18</ref>. The learning based method <ref type="bibr" target="#b9">[10]</ref> is a global tone mapping algorithm, which has very good ability in treating global contrast. However, this method obviously performs less effective than our ALHA operator in treating local contrast and details as shown in Fig. <ref type="figure" target="#fig_15">18</ref>. The dark corners are very hard to see, while our ALHA operator clearly reveals the details in these parts. Thus the images produced by ALHA looks more pleasing than the learning based method. The gradient domain compression method <ref type="bibr" target="#b14">[15]</ref> provides good local contrast image, however, these images are likely to have some kind of noise artifacts as shown in Fig. <ref type="figure" target="#fig_15">18</ref>. In comparison, our ALHA produces clearer and cleaner results. In addition their method is much slower than ours-ours is about 5 times faster than theirs as Fig. <ref type="figure" target="#fig_4">16</ref>. Test images used in our psychophysical experimentation. reported in Section 5. their method is less automatic and requires certain parameter adjustments in order to produce good results. In contrast, the proposed default parameters of our algorithm discussed in the previous sessions produce good tone mapped images for various cases as shown in the supplementary material submitted with this paper. The photographic tone mapping technique <ref type="bibr" target="#b13">[14]</ref> needs as little parameter adjustments as ours. However, our method is able to better preserve details and   local contrast than this method as shown in the figure. Moreover, our method is almost 6 times faster than the non-approximate solution of this method. In summary, our ALHA algorithm is very competitive with the stateof-the-art tone mapping operator speed, quality and automation aspects. We believe that our technology would be a very good candidate for practical applications. More results produced by our ALHA are shown in Fig. <ref type="figure" target="#fig_7">19</ref> and supplementary material submitted with this paper, which are all produced using our default parameter sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion and future work</head><p>In this paper, we have presented a novel histogram adjustment method for displaying high dynamic range images. Our global tone mapping operator HALEQ works very fast and well reproduces global contrast for high dynamic range images, which overcomes some drawbacks of well-known histogram adjustment tone mapping operator <ref type="bibr" target="#b7">[8]</ref> and provides a better base for our local tone mapping algorithm. However, global tone mapping operator cannot better reproduce the local contrast. Aiming at developing a tone mapping operator that can produce high quality images, we adapt the global HALEQ to a local implementation and design a local tone mapping operator called ALHA tone mapping operator. Applying HALEQ directly into local areas introduces new problems relating to different artifacts. For all of these issues, we have developed successful solutions and obtain good results. Our ALHA tone mapping operator is able to produce high quality results with very faster computation speed and fewer parameter adjustments, which is comparable to the state-of-the-art tone mapping technologies. Despite these advantages, there is still room for improving the current local tone mapping algorithm. GPU implementation of the proposed ALHA algorithm in this paper is a research topic in the future so that we can achieve real-time computation for local tone mapping algorithm. The similarity weighting function used in this work is simply based on the pixel average value in each local region. We will explore more advanced statistics in each region to carry on the similarity weighting function.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Tone mapping example by our local operator described in this paper. Left small images are the selected multi-exposed image set of the original scene. Right image is a tone mapped result of the HDR radiance map constructed using left multi-exposed image set. Images courtesy of Fattal et al., The Hebrew University of Jerusalem.</figDesc><graphic coords="2,130.76,58.64,324.00,183.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Mapping the output of Eq. (1) for display. (a) Linear mapping divides compact luminance range [D min , D max ] into N=256 equal length intervals and maps pixels falling into the same interval to the same integer display level d. l n is the cutting points. (b) Histogram equalized mapping divides [D min , D max ] into N=256 intervals such that the number of pixels falling into each interval is the same. All pixels falling into the same interval are again mapped to the same integer display level d. e n is the cutting points. (c) The quantizer of our algorithm divides [D min , D max ] into N=256 intervals in such a way that the cutting points le n should fall in between those of the linear quantization and histogram equalization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. A high dynamic range image mapped using Eq. (1) with different values of t. The offset of the image on the right bottom is computed automatically. Radiance map courtesy of Paul Debevec.</figDesc><graphic coords="4,82.64,58.61,420.18,300.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig.5. Top row: left to right, AtriumNight images mapped by our new method HALEQ for t=0.5 and b=0.5, the method of Larson et al.<ref type="bibr" target="#b7">[8]</ref>, histogram equalization and linear compression. Middle row: amplified regions of the images above them. In our monitor, setting t=0.5 seems to give the most appropriate overall brightness for this image. Bottom row: tone mapping curves of these used methods for the image. Radiance map courtesy Karol Myszkowski.</figDesc><graphic coords="6,82.80,58.64,420.12,408.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Left: divide the image into blocks and then apply HALEQ technique developed in Section 3 to each individual block. Right: distance weighting function is introduced to eliminate the boundary artifacts. For easy illustration, only 9 blocks are used in this figure.</figDesc><graphic coords="7,140.58,560.46,324.00,161.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The mapping functions and histograms for two different local areas A and B of an example image (The right image of Fig. 9).</figDesc><graphic coords="7,319.47,342.87,235.44,170.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Normalized histograms of area A after different approaches are applied to D(I) output by Eq. (1). Top: the histogram from the linear quantization. Middle: histogram from the original HALEQ. Bottom: histogram from local HALEQ.</figDesc><graphic coords="8,130.76,58.64,324.00,193.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Mapping results from local HALEQ. Memorial radiance map courtesy of Paul Debevec, University of California at Berkeley; Clock building radiance map courtesy of Greg Ward.</figDesc><graphic coords="8,130.77,304.24,324.00,224.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .Fig. 11 .</head><label>1011</label><figDesc>Fig. 10. Mapped results after the consideration of distance weighting function.</figDesc><graphic coords="9,140.59,224.16,324.00,223.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. The function used to smoothly decrease the parameter b depends on the degree of uniformity (standard deviation SD n ) in corresponding blocks.</figDesc><graphic coords="10,70.40,321.71,198.47,101.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. The mapped results before (left) and after (right) considering the uniform areas in the image. Halo artifacts are presented in some mapped images (The image on the right is an example of these images).</figDesc><graphic coords="10,130.76,503.02,324.00,210.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. The reason of causing halo artifacts at the contours of the objects that neighbor uniform areas.</figDesc><graphic coords="10,310.02,362.00,234.72,86.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Left: mapping result without considering similarity weighting function. Middle and Right: mapped results with considering similarity weighting function and setting s s to different values. Smaller s s produces image with less halo artifacts but with less local contrast.</figDesc><graphic coords="11,92.59,528.27,420.12,193.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 17</head><label>17</label><figDesc>Fig. 17. Pairs comparison of tone mapping results of Memorial Church image. Left: result mapped by our ALHA operator and an amplified area. Right: result mapped by fast bilateral filtering tone mapping [12] and an amplified area.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>.</head><label></label><figDesc>Fig. 17. Pairs comparison of tone mapping results of Memorial Church image. Left: result mapped by our ALHA operator and an amplified area. Right: result mapped by fast bilateral filtering tone mapping [12] and an amplified area.</figDesc><graphic coords="13,122.59,341.08,360.00,322.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 18 .</head><label>18</label><figDesc>Fig. 18. Tone mapping results of the Memorial Church image. From top to bottom and left to right: result of Ward Larson's method [8], result of learning based method [10], result of our ALHA operator, result of Ashikhmin's methods [24]. All the images are courtesy of various authors.</figDesc><graphic coords="14,130.77,58.64,324.00,481.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="12,112.77,560.04,360.00,170.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Statistic result of psychophysical experimentation.</figDesc><table><row><cell>Images</cell><cell>Counts statistics</cell><cell></cell><cell></cell><cell cols="2">Percentage statistics</cell><cell></cell><cell cols="2">Overall performance</cell></row><row><cell></cell><cell>Ours win</cell><cell>MIT's win</cell><cell>Don't care</cell><cell>Ours win</cell><cell>MIT's win</cell><cell>Don't care</cell><cell>Ours</cell><cell>MIT's</cell></row><row><cell>Bright sun</cell><cell>9</cell><cell>22</cell><cell>0</cell><cell>0.2903</cell><cell>0.7097</cell><cell>0.0000</cell><cell>0.2903</cell><cell>0.7097</cell></row><row><cell>Bristolb</cell><cell>12</cell><cell>18</cell><cell>1</cell><cell>0.3871</cell><cell>0.5806</cell><cell>0.0323</cell><cell>0.4032</cell><cell>0.5968</cell></row><row><cell>Clockbui</cell><cell>13</cell><cell>15</cell><cell>3</cell><cell>0.4194</cell><cell>0.4839</cell><cell>0.0968</cell><cell>0.4677</cell><cell>0.5323</cell></row><row><cell>Desk</cell><cell>13</cell><cell>18</cell><cell>0</cell><cell>0.4194</cell><cell>0.5806</cell><cell>0.0000</cell><cell>0.4194</cell><cell>0.5806</cell></row><row><cell>Room2</cell><cell>13</cell><cell>18</cell><cell>0</cell><cell>0.4194</cell><cell>0.5806</cell><cell>0.0000</cell><cell>0.4194</cell><cell>0.5806</cell></row><row><cell>Pavilion</cell><cell>23</cell><cell>8</cell><cell>0</cell><cell>0.7419</cell><cell>0.2581</cell><cell>0.0000</cell><cell>0.7419</cell><cell>0.2581</cell></row><row><cell>Grandcanal</cell><cell>21</cell><cell>10</cell><cell>0</cell><cell>0.6774</cell><cell>0.3226</cell><cell>0.0000</cell><cell>0.6774</cell><cell>0.3226</cell></row><row><cell>GroveD</cell><cell>22</cell><cell>9</cell><cell>0</cell><cell>0.7097</cell><cell>0.2903</cell><cell>0.0000</cell><cell>0.7097</cell><cell>0.2903</cell></row><row><cell>Memorial</cell><cell>10</cell><cell>21</cell><cell>0</cell><cell>0.3226</cell><cell>0.6774</cell><cell>0.0000</cell><cell>0.3226</cell><cell>0.6774</cell></row><row><cell>Oaks</cell><cell>16</cell><cell>13</cell><cell>2</cell><cell>0.5161</cell><cell>0.4194</cell><cell>0.0645</cell><cell>0.5484</cell><cell>0.4516</cell></row><row><cell>Office</cell><cell>7</cell><cell>23</cell><cell>1</cell><cell>0.2258</cell><cell>0.7419</cell><cell>0.0323</cell><cell>0.2419</cell><cell>0.7581</cell></row><row><cell>Peyrou</cell><cell>16</cell><cell>14</cell><cell>1</cell><cell>0.5161</cell><cell>0.4516</cell><cell>0.0323</cell><cell>0.5323</cell><cell>0.4677</cell></row><row><cell>Rosette</cell><cell>11</cell><cell>19</cell><cell>1</cell><cell>0.3548</cell><cell>0.6129</cell><cell>0.0323</cell><cell>0.3710</cell><cell>0.6290</cell></row><row><cell>Tahoe1</cell><cell>13</cell><cell>15</cell><cell>3</cell><cell>0.4194</cell><cell>0.4839</cell><cell>0.0968</cell><cell>0.4677</cell><cell>0.5323</cell></row><row><cell>Tinterna</cell><cell>22</cell><cell>8</cell><cell>1</cell><cell>0.7097</cell><cell>0.2581</cell><cell>0.0323</cell><cell>0.7258</cell><cell>0.2742</cell></row><row><cell>Vinesunset</cell><cell>15</cell><cell>16</cell><cell>0</cell><cell>0.4839</cell><cell>0.5161</cell><cell>0.0000</cell><cell>0.4839</cell><cell>0.5161</cell></row><row><cell>Windmill</cell><cell>17</cell><cell>14</cell><cell>0</cell><cell>0.5484</cell><cell>0.4516</cell><cell>0.0000</cell><cell>0.5484</cell><cell>0.4516</cell></row><row><cell>Apartment</cell><cell>27</cell><cell>3</cell><cell>1</cell><cell>0.8710</cell><cell>0.0968</cell><cell>0.0323</cell><cell>0.8871</cell><cell>0.1129</cell></row><row><cell>Room1</cell><cell>8</cell><cell>22</cell><cell>1</cell><cell>0.2581</cell><cell>0.7097</cell><cell>0.0323</cell><cell>0.2742</cell><cell>0.7258</cell></row><row><cell>Lab</cell><cell>12</cell><cell>18</cell><cell>1</cell><cell>0.3871</cell><cell>0.5806</cell><cell>0.0323</cell><cell>0.4032</cell><cell>0.5968</cell></row><row><cell>Tree</cell><cell>18</cell><cell>12</cell><cell>1</cell><cell>0.5806</cell><cell>0.3871</cell><cell>0.0323</cell><cell>0.5968</cell><cell>0.4032</cell></row><row><cell>Global statistics</cell><cell>318</cell><cell>316</cell><cell>17</cell><cell>0.4885</cell><cell>0.4854</cell><cell>0.0261</cell><cell>0.5015</cell><cell>0.4985</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>J. Duan et al. / Pattern Recognition 43 (2010) 1847-1862</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to sincerely thank the following researchers for their help. Fredo Durand kindly provided their bilateral filter tone mapping software to us so that we can have a much more proper comparison between their methods and ours. Greg Ward has kindly provided us his radiance software package. Fredo Durand, Raanan Fattal, Erik Reinhard, Jack Tumblin, Greg Ward have patiently explained many technical questions. We also thank various authors for making their data available on the Internet for experiments. This project Sponsored by National Natural Science Foundation of China (Grant No. 60903128) and Sponsored by the Scientific Research Foundation for the Returned Overseas Chinese Scholars, State Education Ministry.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Supplementary material</head><p>Supplementary data associated with this article can be found in the online version at doi:10.1016/j.patcog.2009.12.006.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Recovering high dynamic range radiance maps from photographs</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Debevec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGGRAPH&apos;97</title>
		<meeting>the SIGGRAPH&apos;97</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="369" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Radiometric self calibration</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mitsunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computer Vision and Pattern Recognition</title>
		<meeting>the Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="374" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On being &apos;undigital&apos; with digital cameras: extending dynamic range by combining differently exposed pictures</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IS&amp;T&apos;s 48th Annual Conference</title>
		<meeting>the IS&amp;T&apos;s 48th Annual Conference</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="422" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rendering high dynamic range images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dicarlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wandell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Pro ceedings of the SPIE</title>
		<imprint>
			<biblScope unit="volume">3965</biblScope>
			<biblScope unit="page" from="392" to="401" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Tone reproduction for realistic images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tumblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rushmeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="42" to="48" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A contrast-based scalefactor for luminance display</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graphics Gems IV</title>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="415" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A model of visual adaptation for realistic image synthesis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Ferwerda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Pattanaik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shirley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Greenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGGRAPH&apos;96</title>
		<meeting>the SIGGRAPH&apos;96</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="249" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A visibility matching tone reproduction operator for high dynamic range scenes</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rushmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Piatko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="291" to="306" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive logarithmic mapping for displaying high contrast scenes</title>
		<author>
			<persName><forename type="first">F</forename><surname>Drago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Myszkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Annen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chiba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="419" to="426" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to display high dynamic range images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M D</forename><surname>Finlayson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2641" to="2655" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">LCIS: a boundary hierarchy for detail preserving contrast reduction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tumblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Turk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGGRAPH&apos;99</title>
		<meeting>the ACM SIGGRAPH&apos;99</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="83" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast bilateral filtering for the display of high-dynamicrange images</title>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dorsey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="257" to="266" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>special issue SIGGRAPH 2002</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Compressing and companding high dynamic range images with subband architectures</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="836" to="844" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Photographic tone reproduction for digital images</title>
		<author>
			<persName><forename type="first">E</forename><surname>Reinhard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shirley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ferwerda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="267" to="276" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>special issue SIGGRAPH 2002</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gradient domain high dynamic range compression</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fattal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Werman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="249" to="256" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>special issue SIGGRAPH 2002</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Computational model of lightness perception in high dynamic range imaging</title>
		<author>
			<persName><forename type="first">G</forename><surname>Krawczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Myszkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Vision and Electronic Imaging XI</title>
		<editor>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Rogowitz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Pappas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Daly</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Interactive local adjustment of tonal values</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Farbman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Uyttendaele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="646" to="653" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Brightness function: parametric effects of adaptation and contrast</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Stevens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Optical Society of America</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image processing in the context of a visual model</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Stockham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="828" to="842" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Parameter estimation for photographic tone reproduction</title>
		<author>
			<persName><forename type="first">E</forename><surname>Reinhard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Graphics Tools</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="52" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Evaluating HDR rendering algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Fairchild</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Applied Perception</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A tone mapping algorithm for high contrast images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ashikhmin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Eurographics Workshop on Rendering</title>
		<meeting>Eurographics Workshop on Rendering</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="145" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An adaptive algorithm for the display of high-dynamic range images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="397" to="405" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Scale-space and edge detection using anisotropic diffusion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="629" to="639" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Visualization of high dynamic range images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="639" to="647" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Evaluating tone mapping algorithms for rendering non-pictorial (scientific) high-dynamic-range images</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Montag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="415" to="428" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">About the Author-MARCO BRESSAN manages textual and visual pattern analysis group at the Xerox Research Centre Europe (XRCE) in Grenoble, France. He received his Ph.D. in Computer Science and Artificial Intelligence from the Universitat Aut onoma de Barcelona, Spain. His research interests include object recognition and enhancement, image and video semantic scene understanding, etc. Fig. 19. Various HDR images tone mapped with our ALHA operator</title>
		<imprint>
			<pubPlace>China; Grenoble, France</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Computer Science Department of Southwestern University of Finance and Economics</orgName>
		</respStmt>
	</monogr>
	<note>He received his Ph.D. from the University of Nottingham, UK. In 2005, he worked as an intern at Xerox Research Centre Europe (XRCE). Radiance maps courtesy of corresponding author(s)</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">About the Author-GUOPING QIU is a Reader in Visual Information Processing at the University of Nottingham, UK. He received his Ph.D. from the University of Central Lancashire, UK. His research interests include high dynamic range imaging, learning-based image processing, content-based image indexing and retrieval</title>
	</analytic>
	<monogr>
		<title level="m">About the Author-CHRIS DANCE manages both the research laboratory and the data mining group at the Xerox Research Centre Europe (XRCE) in Grenoble, France. Dance received his Ph</title>
		<imprint/>
		<respStmt>
			<orgName>D. from Cambridge University, England</orgName>
		</respStmt>
	</monogr>
	<note>His research interests include text and image categorization and tagging, knowledge extraction from text, semantic search, schema management, etc. and interactive computational visual information processing using optimization</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
