<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Shading-Aware Multi-view Stereo</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Fabian</forename><surname>Langguth</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Darmstadt, Darmstadt</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kalyan</forename><surname>Sunkavalli</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Adobe Research</orgName>
								<address>
									<settlement>San Francisco</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sunil</forename><surname>Hadap</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Adobe Research</orgName>
								<address>
									<settlement>San Francisco</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Goesele</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Darmstadt, Darmstadt</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Shading-Aware Multi-view Stereo</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3297EC4C24BAF060194705DC4997C9FB</idno>
					<idno type="DOI">10.1007/978-3-319-46487-9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel multi-view reconstruction approach that effectively combines stereo and shape-from-shading energies into a single optimization scheme. Our method uses image gradients to transition between stereo-matching (which is more accurate at large gradients) and Lambertian shape-from-shading (which is more robust in flat regions). In addition, we show that our formulation is invariant to spatially varying albedo without explicitly modeling it. We show that the resulting energy function can be optimized efficiently using a smooth surface representation based on bicubic patches, and demonstrate that this algorithm outperforms both previous multi-view stereo algorithms and shading based refinement approaches on a number of datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>High-quality digitization of real world objects has been of great interest in recent years. The demand for effective and accurate digitization methods is increasing constantly to support applications such as 3D printing and visual effects. Passive reconstruction methods such as multi-view stereo <ref type="bibr" target="#b0">[1]</ref> are able to achieve high quality results. However, stereo methods typically operate on image patches and/or use surface regularization in order to be robust to noise. As a result, they often cannot recover fine-scale surface details accurately. These details are often captured by shading variations, and recent work has focused on shadingbased refinement of the geometry obtained from multi-view stereo (or in some cases using depth sensors or template models). Starting from the work of Wu et al. <ref type="bibr" target="#b1">[2]</ref> that can only be used for objects with constant albedo, algorithms have evolved to operate on implicit surfaces <ref type="bibr" target="#b2">[3]</ref> and real time settings <ref type="bibr" target="#b3">[4]</ref>. All these methods treat the coarse input geometry as a fixed ground truth estimate of the shape and use it to regularize their optimization. Consequently, uncertainties in the inital reconstruction method are discarded and cannot be resolved reliably.</p><p>Another challenge for shading-based refinement techniques is that observed image intensities combine shading and surface albedo. Inferring fine-scale detail from shading thus requires reasoning about surface albedo. This significantly increases the number of variables in the optimization. Most current techniques either assume constant albedo or apply strong regularization on the albedo, which can often fail on real-world surfaces.</p><p>In contrast to previous work, we propose a new multi-view surface reconstruction approach that combines stereo and shading-based data terms into a single optimization scheme. At the heart of our algorithm is the observation that stereo-matching and shape-from-shading have complementary strengths. While stereo correspondences are more accurate in regions with many large image gradients, shape-from-shading is typically more robust in flat regions with no albedo variations. The resulting algorithm provides three distinct advantages over previous work:</p><p>-It leads to a combined multi-view stereo and shading-based reconstruction that balances the two terms without committing, a priori, to either of them. -It uses a simple image gradient-based trade-off between stereo and shading energies that maximizes their effectiveness. -It treats spatially varying albedo implicitly, i.e. our optimization is robust against spatially varying albedo without explicitly modeling it.</p><p>We show that this combined energy can be optimized efficiently using a continuous surface representation <ref type="bibr" target="#b4">[5]</ref>. We demonstrate the effectiveness of this technique on various datasets and show that it outperforms previous MVS and shading-based refinement techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>High-quality surface reconstruction has been an active field of research over the past decade, and approaches have been developed for various forms of input data. Our technique uses an unstructured set of images (with camera parameters) of an approximately Lambertian scene and does not require any special hardware setup. We will review related methods that either operate on similar input data or use ideas similar to our approach.</p><p>Multi-view Stereo. Multi-view stereo algorithms <ref type="bibr" target="#b0">[1]</ref> are arguably one of the most general passive reconstruction techniques. Approaches such as Goesele et al. <ref type="bibr" target="#b5">[6]</ref> and Furukawa and Ponce <ref type="bibr" target="#b6">[7]</ref> have shown that geometry can be recovered even for large scale and uncontrolled Internet data. Other approaches use more controlled settings or additional input such as object silhouettes <ref type="bibr" target="#b7">[8]</ref>. Multi-view stereo approaches usually add a form of regularization to deal with structureless areas that are not well matched by classical stereo terms such as photo consistency. Similarly regularization is used in two-view stereo methods such as Hirschmueller <ref type="bibr" target="#b8">[9]</ref>, Bleyer et al. <ref type="bibr" target="#b9">[10]</ref>, and Galliani et al. <ref type="bibr" target="#b10">[11]</ref>, which can also be applied to multi-view scenarios by combining many two-view estimates into a robust multi-view estimate. In contrast, our goal is to avoid explicit regularization; instead, we use a new shading-based data term to handle sparsely textured regions where a traditional stereo term is not very effective. To do this we optimize both depth and normals of a continuous surface. In terms of surface representation, stereo algorithms usually recover a single depth per-pixel <ref type="bibr" target="#b5">[6]</ref>, a global point cloud <ref type="bibr" target="#b6">[7]</ref>, or an implicit surface model <ref type="bibr" target="#b7">[8]</ref>, all of which we found difficult to apply to our approach. Recently another surface representation was proposed inside a multi-view framework by Semerjian <ref type="bibr" target="#b4">[5]</ref>. This approach uses bicubic patches to define a surface per view that has continuous depth and normals. We found this representation to be appropriate for our method and adopt it as described later.</p><p>Combining Multi-view and Photometric Cues. To recover more detail in regions where depth reconstruction is not very accurate, several methods have combined multi-view and photometric principles. Most of them, however, rely on a controlled and complex capture setup. The approach by Nehab et al. <ref type="bibr" target="#b11">[12]</ref> combines two separate reconstructions. They capture depth using structured light, acquire surface normals using photometric stereo, and integrate both these estimates in a separate step. Other approaches such as Hernandez et al. <ref type="bibr" target="#b12">[13]</ref> and Zhou et al. <ref type="bibr" target="#b13">[14]</ref> combine photometric stereo information from multiple view points into a single framework. This requires a large amount of input data and a complex acquisition system as both light and camera positions need to be controlled. Beeler et al. <ref type="bibr" target="#b14">[15]</ref> augment the geometry of captured faces with fine details using the assumption that small concavities in the skin appear darker than flat areas. They do not require a lot of input data but are still dependent on a calibrated capture setup as they do not have a variable lighting model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shading-Based Refinement for General Illumination.</head><p>Most recently, a new line of work uses shading cues from images captured under uncontrolled illumination to improve a given geometry. Wu et al. <ref type="bibr" target="#b1">[2]</ref> presented the first approach that uses a precomputed multi-view stereo reconstruction to estimate a spherical harmonics approximation of the lighting. They use this lighting and a shading model to improve the stereo reconstruction. Their approach is able to recover fine-scale details but is limited to objects with a single, constant albedo. Later, Yu et al. <ref type="bibr" target="#b15">[16]</ref> and Han et al. <ref type="bibr" target="#b16">[17]</ref> both presented algorithms that operate on a single RGB-D input image (e.g., from a Kinect sensor). These sensors usually generate very coarse geometry and shading-based refinement increases the quality and resolution of the output. Xu et al. <ref type="bibr" target="#b17">[18]</ref> also extended the idea and developed a simultaneuos opimization of lighting and shape parameters. They do, however, require additional information about the visual hull of the object. Using GPU-based parallel solvers, Wu et al. <ref type="bibr" target="#b18">[19]</ref> and Or-El et al. <ref type="bibr" target="#b3">[4]</ref> were able to achieve real-time performance on similar input data. All these techniques are still limited to a single albedo <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>, a fixed set of constant albedo clusters <ref type="bibr" target="#b15">[16]</ref>, or a coarse initial albedo estimate <ref type="bibr" target="#b18">[19]</ref>. Other methods focus on more specific scenarios such as faces. Chai et al. <ref type="bibr" target="#b19">[20]</ref> fit a parametric face model to an input image and use it for lighting estimation and shading-based refinement. The first technique to include a spatially varying albedo was proposed by Zollhoefer et al. <ref type="bibr" target="#b2">[3]</ref>. They include the albedo in the optimization and constrain it using a chromaticity-based regularization scheme similar to Chen and Koltun <ref type="bibr" target="#b20">[21]</ref>. While, this prevents shading from being absorbed into the albedo, it can fail in scenes where the albedo variation is not accurately predicted by chromaticities (e.g., albedos with the same chromaticity but different brightness).</p><p>Although shading-based refinement techniques have improved significantly in recent years, the basic principle of all existing methods remains the same: They use fixed input geometry, estimate lighting, and later refine the geometry using shading cues. While we also compute a lighting function on a coarse estimate of the geometry, we integrate the geometry refinement directly into the multiview stereo reconstruction method. This allows us to balance stereo matching and shading cues as we can resolve ambiguities in the multi-view stereo energy, instead of treating the input geometry as fixed. This approach ultimately also enables us to optimize the geometry independent of the (potentially spatiallyvarying) albedo, i.e., without explicitly including albedo terms into our energy. This is a significant advantage because we do not have to rely on albedo regularization models that can often fail on real-world scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Energy Formulation</head><p>Our energy balances geometric errors versus shading errors depending on the local image gradient. This is motivated by Land's Retinex theory <ref type="bibr">[22]</ref>, which assumes that shading introduces only small image gradients, changing the surface brightness gradually. Strong gradients on the other hand are usually caused by changes in surface materials and are thus independent of the illumination. Retinex theory has been commonly used to separate surface albedo and shading <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> (see Fig. <ref type="figure" target="#fig_0">1</ref>).</p><p>In our context, this observation has two implications. First, in multi-view reconstruction the geometric stereo term is usually accurate and robust in regions with strong gradients but fails for small gradients. Many stereo methods therefore use surface regularization to keep textureless areas smooth. We instead utilize the fact that small gradients are most likely caused by lighting and define an additional data term based on a shading function that specifically constrains the direction in which the surface should change. Second, we show that, in regions of small gradients, we can factor the surface albedo out completely, resulting in an albedo-free shading term. Our error terms are based purely on point wise image gradients and do not involve image values or larger patches of pixels.</p><p>The input to our algorithm is an unstructured set of images as well as known camera parameters which can be either pre-calibrated or recovered by stucture from motion tools such as VisualSFM <ref type="bibr" target="#b24">[25]</ref>. We aim to compute a depth map for evey view i using a set of neighbor views j ∈ N i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Geometric Error</head><p>Our camera model follows standard definitions <ref type="bibr" target="#b25">[26]</ref>. A 3D point X is transformed into an image location x in the camera coordinate system according to a camera calibration matrix K, rotation R, and translation t as</p><formula xml:id="formula_0">x = K (RX + t) . (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>For homogeneous coordinates the projection from a pixel coordinate x i in camera i into another camera j can then be defined according to a depth value d i (x i ) along the principal ray of view i:</p><formula xml:id="formula_2">P j (x i , d i (x i )) = K j R j R -1 i K -1 i x i • d i (x i ) -t i + t j (2)</formula><p>The geometric error is now defined as a stereo term based on matching intensity gradients from the main view into neighboring views according to the current depth function. Traditional stereo methods often optimize using image values over a local patch of pixels. Even for illumination invariant measures such as normalized cross-correlation, this would be more difficult to integrate into our Retinex assumption as a patch of pixels is more likely to be affected by both albedo and shading changes. Instead, we specifically optimize this energy for local image gradients. A gradient-based stereo term was introduced by Scharstein <ref type="bibr" target="#b26">[27]</ref> but has only been adapted in some specific scenarios like gradient domain rendering <ref type="bibr" target="#b27">[28]</ref>. Semerjian <ref type="bibr" target="#b4">[5]</ref> recently showed that a point-wise measure of gradients can be very effective for surface reconstruction if used correctly. We adopt this measure as it is well suited for our approach. For any two views i, j and their intensity functions I i , I j , and a pixel coordinate x i it can be written as:</p><formula xml:id="formula_3">E j g (d i , x i ) = ∇I i (x) -∇I j (P j (x i , d i (x i ))).<label>(3)</label></formula><p>Here, and in further equations, ∇ denotes image gradients which are the derivatives computed with respect to image coordinates x i . Note that this also involves the derivative of the projection P j which transforms the gradient into the correct coordinate system. In addition to constraints beween the main view and its neighbors, we also define pairwise terms between two neighbors as used by Semerjian <ref type="bibr" target="#b4">[5]</ref>. Still using the depth of the main view d i we get:</p><formula xml:id="formula_4">E j,k g (d i , x i )=E j g (d i , x i )-E k g (d i , x i )=∇I j (P j (x i , d i (x i ))-∇I k (P k (x i , d i (x i )),<label>(4)</label></formula><p>where E i,j g = E j g . This essentially measures the difference in error between neighbors and avoids overfitting to only one neighbor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Shading Error</head><p>Lighting Model: Similar to previous work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> we assume Lambertian reflectance. This allows us to define shading as a function of the surface normal n, and independent of the viewing direction. We also use third-order spherical harmonics basis functions B h to approximate the incoming illumination. The outgoing radiance R(x) at a point x, with albedo a(x) and normal n(x), is a weighted sum of these bases, which we define as our shading function S:</p><formula xml:id="formula_5">R(x) = a(x) • 16 h=1 B h (n(x)) • l h = a(x) • S(n(x), l)<label>(5)</label></formula><p>The lighting parameters l are computed ahead of surface optimization using a coarse initial surface model derived from basic stereo. This optimization is identical to Zollhoefer et al. <ref type="bibr" target="#b2">[3]</ref>, i.e., we initialize the albedo as constant and simply solve a linear least squares system. In contrast to Zollhoefer et al., we optimize l using only our single main image. Using more images would make this estimation more robust, but we explicitly want to optimize for a separate lighting model per image to be invariant to changing light conditions, e.g., an object moving on a turn table or outdoor scenes with uncontrolled lighting. We also set our albedo to a constant value. As we will describe later, we are able to optimize the geometry without explicitly modeling the albedo. This has many advantages for the optimization procedure, but unlike Zollhoefer et al. <ref type="bibr" target="#b2">[3]</ref> we cannot create an improved lighting model in further iterations. While there are obvious scenarios that will break this approach, the low number of lighting parameters causes the estimation to be robust enough for a variety of objects, as we will demonstrate in the results. In fact, we observed that in practical scenarios it is much more likely that errors appear due to specular surfaces, self shadowing and inter-reflections, which cannot be dealt with in either case.</p><p>Shading Error: Our shading term is also based on image gradients. Similar to <ref type="bibr" target="#b2">[3]</ref>, we assume that the observed image gradient, ∇I, should be identical to the gradient of the reflected intensity predicted by our model, ∇R, with:</p><formula xml:id="formula_6">∇R(x) = ∇a(x) • S(n(x), l) + a(x) • ∇S(n(x), l).<label>(6)</label></formula><p>However, at this point we do not have an accurate model of the albedo. Previous approaches therefore include the albedo in the optimization leading to a significantly bigger, under-constrained problem. This requires an explicit regularization on the albedo using approximate measures such as pairwise differences based on chromaticity. Instead, we use the Retinex assumption to create an albedo independent optimization that does not require any explicit regularization. A common approach for intrinsic images <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">23]</ref> is to operate in the log domain as this makes albedo and shading terms additive instead of multiplicative:</p><formula xml:id="formula_7">log(R(x)) = log(a(x)) + log(S(n(d i (x)), l)).<label>(7)</label></formula><p>If we take the gradient with respect to image coordinates we get:</p><formula xml:id="formula_8">∇ log(R(x)) = ∇a(x) a(x) + ∇S(n(d i (x)), l) S(n(d i (x)), l) . (<label>8</label></formula><formula xml:id="formula_9">)</formula><p>If we now assume-according to the Retinex theory-that small gradients are caused solely by lighting, the albedo gradient vanishes and we can write:</p><formula xml:id="formula_10">∇ log(R(x)) = ∇S(n(d i (x)), l) S(n(d i (x)), l) . (<label>9</label></formula><formula xml:id="formula_11">)</formula><p>This means that the difference, ∇ log(I(x)) -∇ log(R(x)), can in fact be minimized by solely optimizing over the shading function, S(n(x), l). This indicates an albedo invariance which can also be thought of in the following way: If the albedo is locally constant, an intensity gradient is only caused by a change in surface normals, and given a lighting model, the surface normals have to change in a particular direction which does not depend on the actual value of the albedo. Our shading error is therefore defined as</p><formula xml:id="formula_12">E s (d i , x) = ∇I(x) I(x) - ∇S(n(d i (x)), l) S(n(d i (x)), l) . (<label>10</label></formula><formula xml:id="formula_13">)</formula><p>Note that this is a simple point-wise measure which matches the point-wise nature of our gradient-based stereo term and suggests a balanced optimization if both are combined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Combined Energy</head><p>To formulate our final energy function we combine both data terms in a simple but effective way. For pixels with strong gradients, we rely on the geometric stereo term as it is very robust. For small gradients, we additionally use our shading error as it constrains the surface according to the given lighting model. As we want to do this on a per-pixel basis, we need a continuous trade-off to avoid artifacts. Our solution is to use the magnitude of the image gradient to compute a weight on the shading error term, see Fig. <ref type="figure" target="#fig_0">1</ref> for an example. For a set of neighbors, N i , including i itself, and a set of pixels, V i , that are visible in the corresponding neighbors, the final energy is defined as:</p><formula xml:id="formula_14">E(d i ) = k&gt;j j,k∈Ni xv∈Vi |E j,k g (d i , x v )| + α ∇I(x v ) 2 |E s (d i , x v )|,<label>(11)</label></formula><p>where α = 0.01 balances the scale of both terms as the shading error is measured in the log domain. We use the same value for all our datasets. We also experimented with normalizing the weight across pixels. The new weight β would then also affect the geometric error, i.e., (1β)E g + βE s , resulting in a total weight of 1 for each pixel. However, this led to worse results. Note that the final energy is constructed only with local measures and does not contain any explicit regularization terms. Instead it is implicitly regularized by the Retinex assumption Fig. <ref type="figure">2</ref>. Surface representation based on bicubic patches. Each patch is defined via 4 nodes (illustrated as circles) that are located at pixel corners (illustrated as dots on the pixel grid). When moving to a higher scale the patch is subdivided and some patches are removed if they have a high error.</p><p>and the lighting model. We also use the L1 norm for both our data terms as it is more robust to outliers that do not correspond to our Retinex assumption. It also avoids scale issues in the optimization that can be caused by the shading energy becoming very large in dark areas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Surface Representation and Optimization</head><p>As discussed in Sect. 2, we use the framework of Semerjian <ref type="bibr" target="#b4">[5]</ref> to optimize our energy function. It provides a surface representation with a continuous definition of depth values and surface normals which is very beneficial for our combined energy. Optimizing a depth map for each view allows us to handle datasets with varying lighting conditions and enables straight forward parallel processing. As this framework uses a different approach compared to simple pixel-wise depth values, we briefly summarize the main aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Surface Representation</head><p>The surface is not represented as depth values per pixel but rather as a set of bicubic surface patches. Every patch is defined by bicubic interpolation between 4 nodes, and neighboring patches share two nodes (see Fig. <ref type="figure">2</ref>). A node itself represents 4 optimization variables: the depth, the first derivatives of the depth and the mixed second derivative. The nodes are located at image coordinates of the main view and each bicubic patch covers a set of pixels. This also enables an easy formulation of scale, as patches can cover more pixels to represent a coarser scale and can be subdivived to move to a finer scale. At the finest scale the patches cover a 2 × 2 set of pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Optimization</head><p>Given this representation, we can efficiently optimize the non-linear energy (Eq. 11) using a Gauss-Newton type solver. As our shading error is albedo-free, we do not need to introduce additional variables and can operate solely on the surface representation. Starting from an initial guess the current energy is linearized, and we solve for an update to the optimization variables. Let d be the vector of optimization parameters, d the update, and f (d) the vector of residuals generated by our energy E. Linearizing the error function around the current solution using the Jacobian, J f , leads to the common linear system:</p><formula xml:id="formula_15">f (d + d) ≈ f (d) + J T f d, J T J d = -J T f<label>(12)</label></formula><p>The approximate Hessian J T J consists of 4 × 4 blocks that correspond to the 4 optimization variables at each node. It is also very sparse due to the limited support of the bicubic patches; each node is used for a maximum of 4 patches.</p><p>The linear system can therefore be solved efficiently using a conjugate gradient solver. The inverse of the block diagonal of J T J is a good preconditioner and can be computed quickly using Cholesky decompositions on the blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Final Algorithm</head><p>We first create an initial geometry using the multi-scale formulation and surface operations of Semerjian <ref type="bibr" target="#b4">[5]</ref> for coarse scales. Smaller patch sizes of 8×8 and lower are then optimized using our new energy. Applying our shading term for coarse scales would not improve the final result as geometry details are only revealed at finer scales. Another reason is efficiency; the shading error additionally involves the gradient of the shading function and is therefore more complicated to compute which increases the runtime compared to simple regularization. Finally, the reconstructed surfaces from all views are converted to a point set with normals and can be fused with any surface reconstruction algorithm <ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref>. Each view can also be represented as a depth or normal map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>In the following, we evaluate our method using a variety of datasets. For all our results we used 6-9 neighbor images (except for the sparse Middlebury datasets) and fused them into a global model using Floating Scale Surface Reconstruction (FSSR) <ref type="bibr" target="#b29">[30]</ref>. We chose this approach because it does not fill holes that may appear in the geometry due to large errors in our stereo and/or shading energy.</p><p>We first evaluate our approach on the well known Middlebury benchmark <ref type="bibr" target="#b0">[1]</ref>. Comprehensive results are available on the website. The Dino dataset has many areas that are affected by self shadowing and interreflections. As Fig. <ref type="figure" target="#fig_1">3</ref> shows, our optimization can handle these effects in many cases if enough stereo information from multiple views is available. Note that our optimization handles cast shadows to some extent implicitly since the weight for the shading term is low at the shadow boundaries, and cast shadows can be matched well with stereo matching. The lighting model is, however, still wrong inside the shadowed [7], (f) Galliani et al. <ref type="bibr" target="#b10">[11]</ref>, and (g) Semerjian <ref type="bibr" target="#b4">[5]</ref>; and (h) ground truth.</p><p>areas since the incoming illumination is partially occluded. On the full dataset our result has an accuracy of 0.49 mm and a completeness of 96.9 %. For the sparse Dino dataset where stereo cues are not very strong, our shading term causes holes in the shadowed areas as we cannot find consistent normals in these areas. However, compared to other approaches, we are able to recover a significant amount of detail in areas that are not affected by shadows. In fact, we reconstruct the same amount of detail independent of the sparsity of the input data, which highlights another strength of our shading term. Even for the very sparse input data of 16 images and using only 2 neighbors we can reconstruct more detail than top scoring approaches on the full dataset. For the full Temple dataset (Fig. <ref type="figure" target="#fig_2">4</ref>), we are able to achieve a high accuracy even though the back of the object has many concavities leading to strong interreflections that cannot be represented by our global lighting model. Compared to the results submitted by Semerjian <ref type="bibr" target="#b4">[5]</ref> our shading term improves the accuracy on the full dataset by 0.15 mm to 0.47 mm and we achive a completeness of 98.7 %. Figures <ref type="figure" target="#fig_3">5</ref> and<ref type="figure" target="#fig_4">6</ref> show fountain-P11, an outdoor dataset from the Strecha et al. <ref type="bibr" target="#b31">[32]</ref> benchmark. The normal maps in Fig. <ref type="figure" target="#fig_3">5</ref> show the effect of different surface regularization weights on the original approach of Semerjian <ref type="bibr" target="#b4">[5]</ref>. There is no globally correct weight as the reconstructed geometry is either too smooth or too noisy. In contrast, our approach reconstructs smooth but detailed geometry due to the image gradient magnitude-based weight. Figure <ref type="figure" target="#fig_4">6</ref> demonstrates that this also translates to the fused geometry as integrating multiple views cannot remove the noise inherent in Semerjian's reconstruction effectively.</p><p>Next we present a multi-scale outdoor dataset included in the FSSR paper <ref type="bibr" target="#b29">[30]</ref>. Figure <ref type="figure" target="#fig_5">7</ref> shows that our approach can recover detailed geometry in such a setting. The normal map captures even the finest details recovered in a single view. Our results from vastly different scales can be combined into a consistent model with FSSR. However, we can observe the boundaries between scales as the resolution and accuracy of the geometry changes drastically. This still illustrates an advantage compared to other systems that operate on a global model: our approach can scale to any amount of images and can easily reconstruct different levels of detail in a single dataset, whereas keeping a multi-scale global model in an efficient data structure is challenging and not arbitrarily scalable.</p><p>Figure <ref type="figure" target="#fig_6">8</ref> shows a dataset presented by Zollhoefer et al. <ref type="bibr" target="#b2">[3]</ref>. This object already provides many gradients for stereo matching so we do not expect our shading term to result in a substantial improvement. Note, however, that our reconstruction has significantly better quality compared to the normal map reconstructed with Semerjian's approach, and compared to the Zollhoefer et al. <ref type="bibr" target="#b2">[3]</ref> reconstruction provided on their project web page.</p><p>Finally, Fig. <ref type="figure" target="#fig_7">9</ref> presents results on a dataset captured under varying lighting conditions. The Owl was captured on a turn-table with fixed lights and a fixed camera, resulting in different lighting for each image (w.r.t. the image coordinates). The object is nearly diffuse apart from the dark specular areas where all the methods shown here fail. We compare against a patch based stereo method <ref type="bibr" target="#b5">[6]</ref>, which has no effective regularization as each pixel is optimized independently. This results in a very uneven surface and noise in (almost) textureless regions. Semerjian <ref type="bibr" target="#b4">[5]</ref> uses a simple regularization term that keeps the surface variation low. This is effective in producing a continuous surface, but cannot recover  details in regions without strong gradients. In contrast, our combined method recovers a smooth surface and is able to relate small gradients to surface details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Runtime</head><p>A C++ implementation of our technique is available as open source software<ref type="foot" target="#foot_0">1</ref> . This unoptimized prototype shows a roughly 20 % runtime increase compared to our implementation of Semerjian <ref type="bibr" target="#b4">[5]</ref>. In practice, the full Dino and Temple datasets were computed in 75 and 63 min on a 32-core machine. The multi-scale outdoor dataset from Fuhrmann et al. <ref type="bibr" target="#b29">[30]</ref> included 204 high resolution images and was computed in 115 min on the same machine, while the Owl dataset with 10 images took around 7 min. For a fair comparison to other stereo methods, we are reporting the run-times of our complete multi-view algorithm and not only the time required for solving our shading-based optimization. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Limitations</head><p>We make two main assumptions in our method that can lead to errors in the final geometry if they are violated. First, we assume that the scene is Lambertian and a low frequency spherical harmonics lighting can accurately represent the illumination. As we show in the Middlebury Dino dataset, shadows and interreflections will cause errors in the reconstruction but we are still able to reconstruct details in areas where our lighting model is correct. A more sophisticated lighting model could solve the issues in future work, and would require only minor changes to our geometry optimization. Second, we assume that we can separate albedo and lighting according to the magnitude of the image gradient. While this holds for many datasets, there are objects where the albedo changes gradually, and this violation of Retinex can show up in our geometry if we relate these small gradients to shading and therefore changes in the surface normal. This suggests that some geometry regularization might still be needed in certain regions where we cannot easily decide between albedo and shading. As we rely solely on the stereo error for strong gradients, we are also limited by its accuracy. In certain configurations, e.g., observing horizontal lines under horizontal camera motion, or   <ref type="bibr" target="#b5">[6]</ref>; by our implementation of Semerjian <ref type="bibr" target="#b4">[5]</ref>; and using our new optimization. Our results capture more structural details (see the eyes for example) with less overall noise.</p><p>fine structures with aliasing effects, the stereo term might lead to wrong depth estimates that we cannot fix with our normal-based shading term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we have presented a novel multi-view surface optimization algorithm that efficiently combines a stereo energy term with a shading-based energy term in a single, combined approach, to create high quality reconstructions. Building on the Retinex assumption, we are able to completely remove the albedo from the shading-based error, which has not been done before. Our formulation relies solely on pixel-wise data terms and an implicit regularization via the shading function and surface representation. We present results that improve on previous multi-view stereo algorithms and shading based refinement systems. Our approach is limited by the basic lighting model and cannot account for self-shadowing, indirect illumination, and specular materials. In future work we will improve on this to create a more robust system that can be applied to more complex scenes. Overall, we believe that the idea of combining stereo and shading energies can be very powerful and will lead to more general approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Left: An illustration of our Retinex-based assumption of separating albedo from shading. Large gradients in the image are usually caused by albedo changes; small gradients on the other hand are observed due to lighting. Based on this we compute a trade-off between stereo and shading energies. Right: Visualization of the trade-off for an input image. For every pixel we use mainly our stereo term (dark regions) or our shading term (bright regions) based on the magnitude of the image gradient.</figDesc><graphic coords="4,43.80,53.96,337.00,59.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Results on the Dino dataset of the Middlebury benchmark with decreasing number of input images. This dataset has strong shadowing which can be seen in the input image. However, in areas where our lighting model is correct we are able to recover a high amount of detail in the geometry even for sparse input data. Top: (a) Input image; (b) our reconstruction on full dataset, 363 images, using 9 neighbors; (c) ring dataset, 46 images, using 4 neighbors; and (d) sparse ring dataset, 16 images, using only 1 or 2 neighbors. Bottom: Results on full dataset submitted by (e) Furukawa et al.[7], (f) Galliani et al.<ref type="bibr" target="#b10">[11]</ref>, and (g) Semerjian<ref type="bibr" target="#b4">[5]</ref>; and (h) ground truth.</figDesc><graphic coords="10,41.79,54.23,340.24,187.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Results on the Temple dataset of the Middlebury benchmark. From left to right: (a) Galliani et al. [11]; (b) Fuhrmann et al. [30] using the stereo from Goesele et al. [6]; (c) our reconstruction; and (d) ground truth. Our reconstruction achieves a good balance between capturing fine-scale detail without introducing noise.</figDesc><graphic coords="11,59.97,54.59,332.80,109.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The fountain-P11 dataset from Strecha et al. [32]. From left to right: Closeup normal maps for single views of the bottom left area for different weights on surface regularization (a) high, (b) medium, and (c) low; and (d) normal map of our reconstruction. Basic regularization cannot find a good trade-off between overly smooth and noisy geometry. Our result reveals fine details without introducing noise.</figDesc><graphic coords="12,41.79,216.62,340.12,110.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. The fountain-P11 dataset from Strecha et al.<ref type="bibr" target="#b31">[32]</ref>. From left to right: Reconstruction by our implementation of Semerjian<ref type="bibr" target="#b4">[5]</ref> using a low regularization weight to recover details; by our new optimization; and ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Results on an outdoor dataset. Top: Input images at different scales, and our global model with details. Our method recovers more detail in regions that are imaged at higher-resolution. Bottom: A closeup input image; the reconstructed depth map shaded with the lighting; and close-up normals with regular (10 -2 ) and low (10 -4 ) value for α -decreasing the weight of the shading term results in more noise and less detail as the stereo term dominates the energy.</figDesc><graphic coords="13,55.98,53.78,340.12,236.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. The Figure dataset. Top from left to right: An input image of the dataset; normal maps from the surface computed by our implementation of Semerjian [5], and our shading based approach. Bottom: Result presented by Zollhoefer et al. [3] (available at project website); and our fused model.</figDesc><graphic coords="14,42.30,295.31,339.40,92.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Reconstruction of the Owl dataset with changing lighting in each image. From left to right: An input image; reconstruction by Goesele et al.<ref type="bibr" target="#b5">[6]</ref>; by our implementation of Semerjian<ref type="bibr" target="#b4">[5]</ref>; and using our new optimization. Our results capture more structural details (see the eyes for example) with less overall noise.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/flanggut/smvs.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work was supported in part by the European Commissions Seventh Framework Programme under grant agreements no. ICT-611089 (CR-PLAY).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A comparison and evaluation of multi-view stereo reconstruction algorithms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Diebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2006)</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="519" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">High-quality shape from multiview stereo and shading under general illumination</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wilburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2011</title>
		<meeting>the 2011 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2011</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Shading-based refinement on volumetric signed distance functions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Innmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<idno type="DOI">10.1145/2766887</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">RGBD-fusion: real-time high precision depth recovery</title>
		<author>
			<persName><forename type="first">R</forename><surname>Or-El</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rosman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wetzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Bruckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A new variational framework for multiview surface reconstruction</title>
		<author>
			<persName><forename type="first">B</forename><surname>Semerjian</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10599-4_46</idno>
	</analytic>
	<monogr>
		<title level="m">ECCV 2014</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Fleet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8694</biblScope>
			<biblScope unit="page" from="719" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-view stereo for community photo collections</title>
		<author>
			<persName><forename type="first">M</forename><surname>Goesele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Accurate, dense, and robust multi-view stereopsis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Pattern Anal. Mach. Intell. (PAMI)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1362" to="1376" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Variational patchmatch multiview reconstruction and refinement</title>
		<author>
			<persName><forename type="first">P</forename><surname>Heise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Klose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Knoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="882" to="890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Accurate and efficient stereo processing by semi-global matching and mutual information</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hirschmüller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2005</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2005<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Patchmatch stereo -stereo matching with slanted support windows</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bleyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="14" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Massively parallel multiview stereopsis by surface normal diffusion</title>
		<author>
			<persName><forename type="first">S</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lasinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="873" to="881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">combining positions and normals for precise 3D geometry</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nehab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rusinkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2005 Papers</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multiview photometric stereo</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hernandez Esteban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Vogiatzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="548" to="554" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-view photometric stereo with spatially varying isotropic materials</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1482" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">High-quality single-shot capture of facial geometry</title>
		<author>
			<persName><forename type="first">T</forename><surname>Beeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Beardsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sumner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<idno type="DOI">10.1145/1778765.1778777</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Shading-based shape refinement of RGB-D images</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">High quality shape from a single RGB-D image under uncalibrated natural illumination</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1617" to="1624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recovering surface details under general unknown illumination using shading and coarse multi-view stereo</title>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Cham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1526" to="1533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Realtime shading-based refinement for consumer depth cameras</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">High-quality hair modeling from a single portrait photo</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hadap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A simple model for intrinsic image decomposition with depth cues</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="241" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The retinex theory of color vision</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Land</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Am</title>
		<imprint>
			<biblScope unit="volume">237</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="108" to="128" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Determining lightness from an image</title>
		<author>
			<persName><forename type="first">B</forename><surname>Horn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graph. Image Process</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="277" to="299" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ground truth dataset and baseline evaluations for intrinsic image algorithms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2335" to="2342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multicore bundle adjustment</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3057" to="3064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Multiple View Geometry in Computer Vision, 2nd edn</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">I</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Matching images by comparing their gradient fields</title>
		<author>
			<persName><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ICPR International Conference on Pattern Recognition</title>
		<meeting>the 12th ICPR International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="572" to="575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image-based rendering in the gradient domain</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Langguth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goesele</surname></persName>
		</author>
		<idno type="DOI">10.1145/2508363.2508369</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Screened poisson surface reconstruction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kazhdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
		<idno type="DOI">10.1145/2487228.2487237</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Floating scale surface reconstruction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fuhrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goesele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGGRAPH</title>
		<meeting>ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">dense multiscale reconstruction for a billion points</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ummenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12">December 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On benchmarking camera calibration and multi-view stereo for high resolution imagery</title>
		<author>
			<persName><forename type="first">C</forename><surname>Strecha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Von Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Thoennessen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
