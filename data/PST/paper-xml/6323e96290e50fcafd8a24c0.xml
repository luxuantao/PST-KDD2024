<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Knowledge Graph Completion with Pre-trained Multimodal Transformer and Twins Negative Sampling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-09-15">15 Sep 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yichi</forename><surname>Zhang</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Wen</forename><surname>Zhang</surname></persName>
							<email>wenzhang2015@zju.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University Hangzhou</orgName>
								<address>
									<settlement>Zhejiang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">College of Software Technology</orgName>
								<orgName type="institution">Zhejiang University Hangzhou</orgName>
								<address>
									<settlement>Zhejiang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">KDD-UC &apos;22</orgName>
								<address>
									<addrLine>August 14-18</addrLine>
									<postCode>2022</postCode>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Knowledge Graph Completion with Pre-trained Multimodal Transformer and Twins Negative Sampling</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-09-15">15 Sep 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2209.07084v1[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Knowledge Graph</term>
					<term>Multimodal Knowledge Graph</term>
					<term>Knowledge Graph Embedding</term>
					<term>Negative Sampling</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge graphs (KGs) that modeling the world knowledge as structural triples are inevitably incomplete. Such problems still exist for multimodal knowledge graphs (MMKGs). Thus, knowledge graph completion (KGC) is of great importance to predict the missing triples in the existing KGs. As for the existing KGC methods, embedding-based methods rely on manual design to leverage multimodal information while finetune-based approaches are not superior to embedding-based methods in link prediction. To address these problems, we propose a VisualBERT-enhanced Knowledge Graph Completion model (VBKGC for short). VBKGC could capture deeply fused multimodal information for entities and integrate them into the KGC model. Besides, we achieve the co-design of the KGC model and negative sampling by designing a new negative sampling strategy called twins negative sampling. Twins negative sampling is suitable for multimodal scenarios and could align different embeddings for entities. We conduct extensive experiments to show the outstanding performance of VBKGC on the link prediction task and make further exploration of VBKGC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Computing methodologies ? Knowledge representation and reasoning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Knowledge graphs (KGs) represent world knowledge as structured triples in the form of (head entity, relation, tail entity), (?, ?, ?) for short, which means entity ? and ? have a relation ? . KGs contain a wealth of structural information of world knowledge and have become the infrastructure of AI research and can benefit a lot of tasks like recommendation systems <ref type="bibr" target="#b35">[36]</ref>, language modeling <ref type="bibr" target="#b12">[13]</ref> and question answering <ref type="bibr" target="#b32">[33]</ref>.</p><p>Multimodal knowledge graphs (MMKGs) <ref type="bibr" target="#b13">[14]</ref> are KGs containing a wealth of modal information (images and text), which greatly enhances the expressiveness of the KGs. How to leverage the modal information in multimodal knowledge graphs is a current research hotspot in KG-related research.</p><p>However, KGs and of course MMKGs are far from complete. They contain only the knowledge we have observed while plenty of triples are not discovered among the entities and relations. Therefore, knowledge graph completion (KGC) is an important task in KG research which aims to discover the missing triples in KGs. As for MMKGs, the approaches to achieve multimodal knowledge graph completion (MMKGC) can be divided into two main categories: embedding-based approaches <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b29">30]</ref> and finetune-based approaches. Embedding-based approaches follow the paradigm of knowledge graph embedding (KGE) which embed the entities and relations into a low-dimensional vector space and define a score function to estimate the plausibility of triples. We could call the embedding-based approaches multimodal knowledge graph embedding (MMKGE) as multimodal information in the KGs is also considered in the embedding model. finetune-based approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32]</ref> usually employ pre-trained language models like BERT <ref type="bibr" target="#b4">[5]</ref> and encode the triples with their textual descriptions. Then the models are finetuned with the KG-related tasks like triple classification <ref type="bibr" target="#b31">[32]</ref> and relation prediction <ref type="bibr" target="#b9">[10]</ref>. The triple scores are based on the output of the BERT model.</p><p>Although existing methods mentioned above have made strides in MMKGC, these methods face the following problems. (1) For the embedding-based methods, their ability to utilize multimodal information about entities is insufficient. The extraction and fusion of multimodal information are highly dependent on the manual design and need more work to find the most suitable method for each dataset. For example, <ref type="bibr" target="#b27">[28]</ref> propose three different methods to achieve modal fusion and search for the best strategy on two datasets. (2) For the finetune-based approaches, though they leverage the textual information by fine-tuning the pre-trained model, the inference speed on the test set is unbearably slow <ref type="bibr" target="#b24">[25]</ref> due to the deep architecture of the pre-trained model and rank-based evaluation protocol for KGs. The evaluation results of them are still not significantly better than embedding-based methods either. (3) Design of negative sampling is ignored in all of the approaches. Existing methods just apply the normal negative sampling for model training, which might not be suitable for the multimodal scenario. For the multiple embeddings in the multimodal scenario, aligning them is also important. Normal negative sampling is entity-level and has no such ability.</p><p>To address the problems mentioned above, we propose a multimodal knowledge graph completion model called VisualBERTenhanced Knowledge Graph Completion model (VBKGC for short). VBKGC is an embedding-based model which employs a pre-trained multimodal transformer model (VisualBERT <ref type="bibr" target="#b10">[11]</ref> for example) to extract deeply fused multimodal feature which is free of finetuning and have a fast inference speed like many other embedding-based methods. Besides, we achieve the co-design of the MMKGE model and negative sample strategy. We propose a negative sample strategy called twins negative sampling for MMKGE. Twins negative sampling could align the different embeddings of each entity during training and achieve better performance on link prediction tasks.</p><p>In general, our contributions in this paper can be summarized as follows:</p><p>? We propose an MMKGC model called VBKGC, which is an embedding-based model and employs VisualBERT as a multimodal encoder to capture the deeply fused multimodal features of entities. It is a universal approach and needs no more manual design for modal feature extraction and fusion.</p><p>Besides, VBKGC has fast inference speed. ? We achieve the co-design of the model and negative sampling for KGC. We propose a new negative sampling method called twins negative sampling for multimodal scenarios. Twins negative sampling could align the structural and multimodal embeddings for entities to perform better on the link prediction task. ? We conduct comprehensive experiments on link prediction tasks with two benchmark datasets. We make further exploration of VBKGC model and twins negative sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS 2.1 Knowledge Graph Embedding</head><p>Knowledge graph embedding (KGE) <ref type="bibr" target="#b25">[26]</ref> aims to embed entities and relations of KGs into a low-dimensional continuous vector space and measure the plausibility of triples by a well-defined score function, which is a popular topic in KG-related research.</p><p>Existing KGE models are diversified. Translation-based methods like TransE <ref type="bibr" target="#b1">[2]</ref> and TransH <ref type="bibr" target="#b28">[29]</ref> modeling the relation in each triple as a translation from head entity to tail entity. Sematic-based methods like DistMult <ref type="bibr" target="#b30">[31]</ref> and ComplEx <ref type="bibr" target="#b21">[22]</ref> apply similaritybased score function to modeling the triples. Other method such as RotatE <ref type="bibr" target="#b20">[21]</ref> and ConE <ref type="bibr" target="#b37">[38]</ref> also modeling triples with various mathematical structures. Convolutional neural networks and graph neural networks are also employed in some KGE models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23]</ref>, which play a role as feature encoders. Rule-enhanced methods <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref> integrate rule learning in KGE for better performance and explainability.</p><p>Meanwhile, negative sampling (NS) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b28">29]</ref> is a key technology for KGE. It would generate negative triples and teach the KGE model to distinguish between positive and negative triples. Many researchers propose better negative sampling strategies. GAN-based methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27]</ref> apply GAN <ref type="bibr" target="#b6">[7]</ref> to generate hard negative triples. NSCaching <ref type="bibr" target="#b36">[37]</ref> simply stores the high-quality negative triples with cache during training. Other methods like SANS <ref type="bibr" target="#b0">[1]</ref> and CAKE <ref type="bibr" target="#b15">[16]</ref> leverage information from original KGs and sample high-quality negative triples.</p><p>However, existing NS methods are usually designed for general KGE. In this context, general KGE means KGE with no extra information outside the triplet structure. In the paragraphs that follow, we still use such a concept. In the multimodal scenario, each entity in a KG might have multiple embeddings (structural embedding and multimodal embedding for example) rather than only one structural embedding, which means the aligning the multiple embeddings is also of great significance. Unfortunately, the existing NS methods do not have this feature and a new NS method urgently needs to be proposed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multimodal Knowledge Graph Completion</head><p>Multimodal knowledge graph completion (MMKGC) is an important task for MMKGs, which would predict the missing triples in MMKGs with multimodal information of entities and relations. Previous methods of MMKG could be roughly divided into two categories: embedding-based approaches and finetune-based approaches.</p><p>Embedding-based approaches <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b29">30]</ref> can be called multimodal knowledge graph embedding (MMKGE) as well. It follows the paradigm of general KGE and represent each entity and relation with several embeddings. To leverage the multimodal information, these approaches extract multimodal features with pre-trained models like VGG <ref type="bibr" target="#b19">[20]</ref> and GloVe <ref type="bibr" target="#b16">[17]</ref>, then the multimodal features would be fused into the multimodal embeddings of entities. These methods are backward in extracting multimodal information, rely on a lot of manual design and have poor ability to represent the extracted modal information.</p><p>Finetune-based approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32]</ref> would employ pre-trained models like BERT <ref type="bibr" target="#b4">[5]</ref> to score the triples directly instead of training entity and relation embeddings. KG-BERT <ref type="bibr" target="#b31">[32]</ref> extend the triples into text sequences as inputs of BERT and then finetune BERT with the triple classification task. MTL-KGC <ref type="bibr" target="#b9">[10]</ref> is a multi-task version of KG-BERT. StAR <ref type="bibr" target="#b24">[25]</ref> apply siamese-style textual encoder to speed up the inference stage. These approaches would are slower and less accurate than the traditional methods of inference due to the rank-based evaluation of KGs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DEFINITION</head><p>A MMKG can be denoted as G ? = (E, R, I, D, T ), where E is the entity set,R is the relation set, I is the image set, D is the text description set, T = {(?, ?, ?) | ?, ? ? E, ? ? R} is the triple set where ?, ?, ? are the head entity, relation, and tail entitiy of a triple. For each entity ? ? ? E, it has a textual description ? ? ? D. Each textual description ? ? consists of several words, denoted as ? ? = (? 1 , ? 2 , . . . , ? ? ), where ? ? ? V and V is the vocabulary of the MMKG G ? . Besides, each entity ? ? might have 0 to any numbers of images in I, the images of ? ? is denoted as set ? ? . We denote e ? and e ? as the structural embedding and multimodal embedding for an entity ?, respectively. Therefore, the entity ? can be represented by two embedding vectors e ? , e ? . Besides, we denote r as the embedding of relation ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHOD</head><p>The model architecture of our model VBKGE is shown in Figure <ref type="figure" target="#fig_0">1</ref>. VBKGE has three modules: encoding module, projection module, and scoring module. The detailed design of each module would be shown below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Encoding Module</head><p>The main role of the encoding module is to encode entities based on their textual and image information. In this paper, we use Visual-BERT <ref type="bibr" target="#b10">[11]</ref>, a pre-trained multimodal transformer <ref type="bibr" target="#b23">[24]</ref> model as our multimodal encoder. VisualBERT learned the knowledge about how to achieve modal alignment and fusion, which is implicitly stored in the parameters. Thus, we could use pre-trained VisualBERT to obtain deeply integrated multimodal features.</p><p>For each entity ? ? ? E, the textual description of ? ? is ? ? = (? 1 , ? 2 , . . . , ? ? ) and the image set of ? ? is ? ? . We use VGG <ref type="bibr" target="#b19">[20]</ref> to extract the visual feature and process the images of ? ? into several visual tokens (? 1 , ? 2 , . . . , ? ? ). The inputs d? of VisualBERT model includes both word tokens and visual tokens:</p><formula xml:id="formula_0">d? = ( [CLS], ? 1 , . . . , ? ? , [SEP], ? 1 , . . . , ? ? , [SEP])<label>(1)</label></formula><p>We add several special tokens like [CLS] and [SEP] following the original VisualBERT paper. The output of VisualBERT is:</p><formula xml:id="formula_1">VisualBERT( d? ) = (h CLS , h ?1 , . . . , h SEP , h ?1 , . . . , h SEP ) (2)</formula><p>The hidden state of [CLS] is employed as the initial multimodal feature of each entity ? ? , denoted as h ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Projection Module</head><p>The main function of the projection module is to project the modal features of entities into the same representation space of structural embeddings. As the modal features and structured embeddings are heterogeneous, they could not participate in triple scoring together. Thus, we apply a projection matrix W and obtain the multimodal embedding e ?? of each entity ? ? by linear projection:</p><formula xml:id="formula_2">e ?? = Wh CLS<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Scoring Module</head><p>Scoring module would define a score function and estimate the plausibility of each triple. The general priciple of score function is to give higher scores for positive triples and lower scores for negative ones.</p><p>For each triple (?, ?, ?) ? T , the score function F of VBKGE can be divided into five different parts:</p><formula xml:id="formula_3">F ?? = ? (h ? , r, t ? )<label>(4)</label></formula><formula xml:id="formula_4">F ?? = ? (h ? , r, t ? )<label>(5)</label></formula><formula xml:id="formula_5">F ?? = ? (h ? , r, t ? )<label>(6)</label></formula><formula xml:id="formula_6">F ?? = ? (h ? , r, t ? )<label>(7)</label></formula><formula xml:id="formula_7">F ??? = ? (h ? + h ? , r, t ? + t ? )<label>(8)</label></formula><formula xml:id="formula_8">F (?, ?, ?) = F ?? + F ?? + F ?? + F ?? + F ???<label>(9)</label></formula><p>In VBKGE, we apply TransE as function ? , which can be denoted as:</p><formula xml:id="formula_9">? (h, r, t) = -||h + r -t|| ?<label>(10)</label></formula><p>In our scoring function F , the multimodal embeddings and structure embeddings of entities could interact fully with each other as we define multiple score functions. The five score functions could be divided into two parts, unimodal scores, and multimodal scores.</p><p>Unimodal scores are calculated by one kind of embeddings (structural or multimodal) while multimodal scores need both. Thus, the overall score function F can be expressed in another way: </p><formula xml:id="formula_10">F ???????? = F ?? + F ??<label>(11)</label></formula><formula xml:id="formula_11">F ?????????? = F ?? + F ?? + F ??? (12) F (?, ?, ?) = F ???????? + F ??????????<label>(13</label></formula><formula xml:id="formula_12">T ? = ?, ?, ? ? | ? ? ? E ? ?, ?, ? ? ? T ? ? ? , ?, ? | ? ? ? E ? ?, ?, ? ? ? T<label>(14)</label></formula><p>where (?, ?, ?) is a positive triple. We genearate ? negative samples for each postive triple. Therefore, we apply a margin-rank loss for postive-negative contrast during training:</p><formula xml:id="formula_13">L = ?? (?,?,? ) ? T max ? -F (?, ?, ?) + 1 ? ?? (? ? ? ,? ? ? ,? ? ? ) ? T ? F (? ? ? , ? ? ? , ? ? ? ), 0<label>(15)</label></formula><p>where ? is the margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Twins Negaitve Sampling.</head><p>In this paper, we also propose a negative sampling called twins for better MKGE model training.</p><p>Traditional negative sampling in KGE is entity-level which would replace the whole head or tail entity to generate a negative triple. It works in general KGE as general KGE usually defines only one embedding for each entity. However, in MKGE models, each entity would have multiple embeddings such as structural and multimodal embeddings. In the multimodal scenario, entity-level negative sampling would replace all the embeddings of the selected entity, which implicitly assumes that different embeddings of this entity have been aligned. Therefore, such an assumption might be too strong for model training.</p><p>Hence, we propose a more fine-grained negative sampling strategy called twins to solve the problem. Twins negative sampling employs different negative sampling strategies for unimodal and multimodal parts of the model. That's why we call it twins. With twin negative sampling, the model could not only learn to discriminate the plausibility of triples (just like traditional negative sampling) but also align the different embeddings for each entity.</p><p>As for the unimodal scores, twins employs just normal negative sampling. The head or tail entity ? in the positive triple is randomly replaced by another entity ? ? . For multimodal scores, however, we only sample negative multimodal features for the replaced entity. We still denote the sampled entity as ? ? , but the structural and multimodal embeddings of ? ? are e ? , e ? ? while they are e ? ? , e ? ? in normal negative sampling. By contrasting with the negative modal features, the model can further align the two kinds of embeddings. With such a fine-grained and modal-level negative sampling strategy for multimodal scores, the model could learn to align the embeddings for each entity during training. In this section, we will report the experiment details including datasets, evaluation protocols, parameter settings, and the results. In addition to the conventional link prediction experiments, we have three exploratory questions:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>(1) Question 1 (Q1): Does twins negative sampling works? (2) Question 2 (Q2): Does our methods inference faster than finetune-based approaches? (3) Question 3 (Q3): Whether the design of each part of the model is valid?</p><p>Following the three questions, we would expolore more about VBKGC next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>In our experiments, we employ two public benchmarks WN9 <ref type="bibr" target="#b29">[30]</ref> and FB15K-237 <ref type="bibr" target="#b31">[32]</ref>. The image resources of FB15K-237 is collected from <ref type="bibr" target="#b13">[14]</ref>. The detailed information about the datasets is shown in Table <ref type="table" target="#tab_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Protocols</head><p>Following classic KG research, we apply link prediction task and rank-based evaluation protocol. Given a correct triple, we rank it against all candidate triples with their scores. Both head and tail entity prediction would be applied in link prediction. The whole entity set E would be the candidate entity set.</p><p>We use mean reciprocal rank (MRR) and Hit@K (K=1,3,10) as evaluation metrics. They can be denoted as:</p><formula xml:id="formula_14">MRR = 1 2|T ???? | ?? ? ?T ???? (<label>1</label></formula><formula xml:id="formula_15">???? ?? + 1 ???? ?? ) (<label>16</label></formula><formula xml:id="formula_16">)</formula><formula xml:id="formula_17">Hit@K = 1 2|T ???? | ?? ? ?T ???? 1(???? ?? ? ?) + 1(???? ?? ? ?) (17)</formula><p>where T ???? is the test triple set and ???? ?? , ???? ?? are predicted ranks of head/tail entity prediction for each test triple ?.</p><p>Besides, all the metrics are in the filter setting <ref type="bibr" target="#b1">[2]</ref>. It would remove the candidate triples which have already appeared in train and valid data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experiment Settings</head><p>For experiments, we set both structural embedding and multimodal embedding size ? ? = 128 for each model. The dimension of multimodal features captured by the pre-trained VisualBERT model is ? ? = 768. For those entities which have no image, we employ Table <ref type="table">2</ref>: Expeirment results of the link prediction task. The baselines marked with * are our reproduction based on the original paper. The best results in each metric are bold and the second-best results are underlined. Some results of baselines that are hard for reproduction and have no results in origin paper are marked as -.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>WN9 FB15K-237 MRR Hit@10 Hit@3 Hit@1 MRR Hit@10 Hit@3 Hit@ Xavier initialization <ref type="bibr" target="#b5">[6]</ref> for their visual features. We set the amount of negative sample ? = 16.</p><p>During training, we divide each dataset into mini-batches and apply TransE <ref type="bibr" target="#b1">[2]</ref> as base score functions ? . We use default Adam optimizer for optimization and tune the hyper-parameters of our model with grid search. The number of batches is tuned in {100, 400} The margin ? is tuned in {4.0, 6.0, 8.0, 10.0} and learning rate is tuned in {2? -5, 1? -4, 5? -4, 1? -3}. The parameter settings are based on existing research findings <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b29">30]</ref>. All the experiments are conducted on Nvidia GeForce 3090 GPUs.</p><p>As for baselines, we employ several embedding-based methods ( TransE <ref type="bibr" target="#b1">[2]</ref>, IKRL <ref type="bibr" target="#b29">[30]</ref>, TransAE <ref type="bibr" target="#b27">[28]</ref>, MTKRL <ref type="bibr" target="#b14">[15]</ref>, all of them apply TransE as score function) and finetune-based approaches (KG-BERT <ref type="bibr" target="#b31">[32]</ref>, StAR <ref type="bibr" target="#b24">[25]</ref>). For fair comparisions, we use the same embeddings dim for embedding-based approaches (? = 128) and reproduce some of the baselines with the same hyperparamters as original papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Link Prediction Results</head><p>The main experiment results on the link prediction task are shown in Table <ref type="table">2</ref>. We reproduce some classic baselines on the datasets and those results are marked with *. Some results of baselines that are hard for reproduction and have no results in origin paper are marked as -. We could visualize that our method VBKGC could perform better than baselines on most of the metrics, except Hit@10 on WN9.</p><p>Besides, the twins negative sampling could behave better than normal negative sampling in the multimodal scenario. The performance gains it brings are particularly noticeable in the WN9 dataset. Though twins negative sampling gets limited improvement on the FB15K237 dataset, we would make a further exploration in the analysis of Q1.</p><p>Another surprising conclusion that can be deduced from the experimental results is that VBKGC with twins negative sampling could perform precise reasoning. Compared with other embeddingbased baselines <ref type="bibr">([2, 15, 30]</ref>), VBKGC with twins could achieve outstanding improvement on Hit@1 and MRR metrics. VBKGC with twins obtains nearly 25% (from 0.641 to 0.803) and 20% (from 0.177 to 0.213) on Hit@1 with WN9 and FB15K-237 respectively. Compared with finetune-based approaches, the improvements brought about by our model are also clearly perceptible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Analysis of Q1: Negative Sampling</head><p>We could conclude that twins negative sampling could improve the link prediction performance from the previous results. In this section, we would dive deeper into the negative sampling in MMKGC and try to answer Q1.</p><p>We employ several state-of-the-art negative sampling methods for general KGE (NSCaching <ref type="bibr" target="#b36">[37]</ref>, NoSampling <ref type="bibr" target="#b11">[12]</ref>) for comparison. We implement the VBKGC model based on their open-source code and conduct experiments with the same embedding dimension ? ? = 128 on the WN9 dataset. We tuned the hyperparameters according to the original paper and the results are shown in Table <ref type="table" target="#tab_3">3</ref>. We could found that negative sampling methods for general KGE might not be acclimatized for the multimodal scenario as we make our best to tune hyperparameters for better results. They even get a marked regression on some metrics. The performance of twins negative sampling on the WN9 dataset exceeds the existing baselines in all aspects. It could align structural and multimodal embeddings to achieve better performance in the multimodal scenario.</p><p>Besides, as twins negative sampling gets limited improvement on FB15K237 dataset, we make a further exploration about this problem. We trained several VBKGC models with different amounts of negative samples for both normal and twins negative sampling on FB15K-237 dataset. The evaluation results are plotted as a line graph (Figure <ref type="figure" target="#fig_1">2</ref>). We could observe that when ? = 1, twins negative sampling could perform better than normal negative sampling. But when ? increases, the impact of the twins negative sampling on the VBKGC model seems to be less significant than normal negative sampling. Existing research shows that increasing the number of negative samples is also an effective means <ref type="bibr" target="#b20">[21]</ref> to improve the model performance. It might be a better choice than twins negative sampling which aligns different embeddings in the FB15K-237 dataset. Nonetheless, it is still a good design for the multimodal scenario as it brings effective enhancement to the WN9 dataset, only it has yet more exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Analysis of Q2: Inference Speed</head><p>To answer Q2, we employ several MMKGC models and measure the overall time they need to infer on the FB15K-237 test data. We plot the results of the measurements as a scatter plot shown in Figure <ref type="figure" target="#fig_2">3</ref>. From the figure we can learn that VBKGC could inference fast like many other embedding-based approaches (TransE <ref type="bibr" target="#b1">[2]</ref>, IKRL <ref type="bibr" target="#b29">[30]</ref>). Compare with finetune-based methods (KG-BERT <ref type="bibr" target="#b31">[32]</ref>, StAR <ref type="bibr" target="#b24">[25]</ref>), VBKGC could achieve both better link prediction performance and faster inference speed. Besides, VBKGC would perform a bit slower than other embedding-based methods as VBKGC employs a more complex score function than baselines.  The results of the ablation study are shown in Table <ref type="table" target="#tab_4">4</ref>. We could find that all of S1 to S6 get worse performance than the full model (VBKGC with normal negative sampling). Thus, the design of each part of our model VBKGC is necessary to get better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Analysis of Q3: Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we present an embedding-based model VBKGC for MMKGC, which employs VisualBERT as a multimodal feature encoder. We achieve co-design of both model and negative sampling by proposing twins negative sampling to align different embeddings for the multimodal scenario. VisualBERT extracts deeply fused multimodal information for better link prediction, which is free of finetuning and makes VBKGC inference fast and precise. Extensive experiment results on two datasets and link prediction tasks with three further explorations demonstrate the effectiveness of VBKGC.</p><p>In the future, we plan to 1) explore more effective ways to leverage multimodal information in knowledge graphs to benefit more kinds of in-KG and out-KG tasks; 2) find a more expressive architecture and try to pre-train KGs on it to capture the deep knowledge in KGs; 3) borrowing solutions from other multimodal machine learning tasks for multimodal knowledge graphs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Model architecture of VBKGE, our model consists of three modules: encoding module, projection module, and scoring module.</figDesc><graphic url="image-1.png" coords="3,66.41,83.69,479.17,188.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Link prediction performance (MRR) on FB15K-237 with different ? (numbers of negative samples).</figDesc><graphic url="image-2.png" coords="6,59.81,83.69,228.23,171.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Inference time and MRR results of different models on FB15K-237 dataset.</figDesc><graphic url="image-3.png" coords="6,59.81,502.30,228.23,171.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>To further prove the effects of different modules in VBKGC, we conduct the ablation study with six different settings of experiments (S1-S6). The settings of S1 to S6 are as follows: (1)(Encoding Module) S1 refers to the model with random multimodal embeddings without VisualBERT to verify the quality of multimodal features captured by VisualBERT. (2) (Scoring Module) S2 to S5 apply just several parts of the overall score function F (?, ?, ?). S2 only employs F ?? as a score function. S3 employs F ??? . S4 employs the unimodal scores F ?? + F ?? . S5 employs the multimodal scores F ?? + F ?? . (3) (Training Objective) S6 sets ? = 1 and samples only 1 negative triple during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>) 4.4 Training Objective And Negative Sampling Strategy 4.4.1 Contrastive Training Objective. As a general paradigm, KGE models would give higher scores for positive triples and lower scores for negative ones. We first generate a negative triple set by randomly replacing the head or tail entity in each positive triple, which is called negative sampling. The negative triple set can be denoted as:</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistical information of datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Entities Relations Train</cell><cell>Valid</cell><cell>Test</cell></row><row><cell>WN9</cell><cell>6555</cell><cell>9</cell><cell>11741</cell><cell>1337</cell><cell>1319</cell></row><row><cell>FB15K-237</cell><cell>14541</cell><cell>237</cell><cell cols="3">272115 17535 20466</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Expeirment results of different negative sampling methods on WN9. The best results in each metric are bold and the second best results are underlined.</figDesc><table><row><cell></cell><cell cols="4">MRR Hit@10 Hit@3 Hit@1</cell></row><row><cell>Normal</cell><cell>0.749</cell><cell>0.919</cell><cell>0.901</cell><cell>0.592</cell></row><row><cell cols="2">NSCaching 0.725</cell><cell>0.868</cell><cell>0.804</cell><cell>0.630</cell></row><row><cell cols="2">NoSampling 0.426</cell><cell>0.662</cell><cell>0.492</cell><cell>0.306</cell></row><row><cell>Twins</cell><cell>0.857</cell><cell>0.922</cell><cell cols="2">0.904 0.803</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Expeirment results of ablation study on FB15K-237. The exact meaning of S1 to S6 could be found in the main text.</figDesc><table><row><cell></cell><cell cols="4">MRR Hit@10 Hit@3 Hit@1</cell></row><row><cell>VBKGC</cell><cell>0.299</cell><cell>0.477</cell><cell>0.331</cell><cell>0.210</cell></row><row><cell cols="2">S1: w/o VisualBERT 0.226</cell><cell>0.409</cell><cell>0.261</cell><cell>0.132</cell></row><row><cell>S2: only F ??</cell><cell>0.147</cell><cell>0.242</cell><cell>0.153</cell><cell>0.097</cell></row><row><cell>S3: only F ???</cell><cell>0.261</cell><cell>0.435</cell><cell>0.293</cell><cell>0.173</cell></row><row><cell cols="2">S4: only F ?? + F ?? 0.251</cell><cell>0.407</cell><cell>0.273</cell><cell>0.174</cell></row><row><cell cols="2">S5: only F ?? + F ?? 0.285</cell><cell>0.464</cell><cell>0.317</cell><cell>0.195</cell></row><row><cell cols="2">S6: only 1 negative 0.269</cell><cell>0.436</cell><cell>0.297</cell><cell>0.184</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A REPRODUCIBILITY A.1 Implemention Details</head><p>We implement our methods based on the open-source library OpenKE. We utilize Pytorch to conduct experiments with one NVIDIA RTX 3090 GPU. Other information about parameter selection is mentioned in the previous section 5. <ref type="bibr" target="#b2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Datasets</head><p>The FB15k-237 dataset is available here. The WN9 datasets is available in here . We use the images of FB15K-237 released here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Optimal parameters</head><p>The optimal hyper-parameters of our model VBKGC with both negative sampling methods on WN9 dataset is:</p><p>(1) number of batches: 100 (2) margin ?: 8</p><p>(3) learning rate: 2e-5</p><p>The optimal hyper-parameters on FB15K-237 dataset is:</p><p>(1) number of batches: 400 (2) margin ?: 6 (3) learning rate: 2e-5</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Kian</forename><surname>Ahrabian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aarash</forename><surname>Feizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasmin</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avishek Joey</forename><surname>Bose</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.11355</idno>
		<title level="m">Structure aware negative sampling in knowledge graphs</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Kbgan: Adversarial learning for knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04071</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v9/glorot10a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2010, Chia Laguna Resort</title>
		<editor>
			<persName><forename type="first">Yee</forename><surname>Whye</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Teh</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">Mike</forename><surname>Titterington</surname></persName>
		</editor>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2010, Chia Laguna Resort<address><addrLine>Sardinia, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-05-13">2010. May 13-15, 2010</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
	<note>JMLR.org</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Openke: An open toolkit for knowledge embedding</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shulin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 conference on empirical methods in natural language processing</title>
		<meeting>the 2018 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="139" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive convolution for multi-relational learning</title>
		<author>
			<persName><forename type="first">Xiaotian</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="978" to="987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-task learning for knowledge graph completion with pre-trained language models</title>
		<author>
			<persName><forename type="first">Bosung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesuk</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjoong</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungyun</forename><surname>Seo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1737" to="1743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Liunian</forename><surname>Harold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Da Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03557</idno>
		<title level="m">Visualbert: A simple and performant baseline for vision and language</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient Non-Sampling Knowledge Graph Embedding</title>
		<author>
			<persName><forename type="first">Zelong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianchao</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuohui</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqiang</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuyuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1727" to="1736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">K-bert: Enabling language representation with knowledge graph</title>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiruo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haotang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2901" to="2908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">MMKG: multi-modal knowledge graphs</title>
		<author>
			<persName><forename type="first">Ye</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Onoro-Rubio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">S</forename><surname>Rosenblum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="459" to="474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A multimodal translation-based approach for knowledge graph representation learning</title>
		<author>
			<persName><forename type="first">Hatem</forename><surname>Mousselly-Sergieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teresa</forename><surname>Botschen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics</title>
		<meeting>the Seventh Joint Conference on Lexical and Computational Semantics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="225" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">CAKE: A Scalable Commonsense-Aware Framework For Multi-View Knowledge Graph Completion</title>
		<author>
			<persName><forename type="first">Guanglin</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.13785</idno>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Embedding multimodal relational data for knowledge base completion</title>
		<author>
			<persName><forename type="first">Pouya</forename><surname>Pezeshkpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liyan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.01341</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European semantic web conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Rotate: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10197</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Knowledge graph completion via complex tensor factorization</title>
		<author>
			<persName><forename type="first">Th?o</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Christopher R Dance</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ric</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName><surname>Bouchard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.06879</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Composition-based multi-relational graph convolutional networks</title>
		<author>
			<persName><forename type="first">Shikhar</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitin</forename><surname>Vikram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03082</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Structure-augmented text representation learning for efficient knowledge graph completion</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1737" to="1748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding: A survey of approaches and applications</title>
		<author>
			<persName><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2724" to="2743" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Incorporating Multimodal Information in Open-Domain Web Keyphrase Extraction</title>
		<author>
			<persName><forename type="first">Yansen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carolyn</forename><surname>Rose</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.140</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.emnlp-main.140" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1790" to="1800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multimodal data enhanced representation learning for knowledge graphs</title>
		<author>
			<persName><forename type="first">Zikang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiudan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Imageembodied knowledge representation learning</title>
		<author>
			<persName><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07028</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6575</idno>
		<title level="m">Embedding entities and relations for learning and inference in knowledge bases</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">KG-BERT: BERT for knowledge graph completion</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengsheng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03193</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Qa-gnn: Reasoning with language models and knowledge graphs for question answering</title>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.06378</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Iteratively learning embeddings and rules for knowledge graph reasoning</title>
		<author>
			<persName><forename type="first">Wen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bibek</forename><surname>Paudel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaoyan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abraham</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2366" to="2377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Interaction embeddings for prediction and explanation in knowledge graphs</title>
		<author>
			<persName><forename type="first">Wen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bibek</forename><surname>Paudel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abraham</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Twelfth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="96" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Billion-scale Pre-trained E-commerce Product Knowledge Graph Model</title>
		<author>
			<persName><forename type="first">Wen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi-Man</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganqiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE 37th International Conference on Data Engineering (ICDE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2476" to="2487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">NSCaching: simple and efficient negative sampling for knowledge graph embedding</title>
		<author>
			<persName><forename type="first">Yongqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingxia</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE 35th International Conference on Data Engineering (ICDE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="614" to="625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cone: Cone embeddings for multi-hop reasoning over knowledge graphs</title>
		<author>
			<persName><forename type="first">Zhanqiu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
