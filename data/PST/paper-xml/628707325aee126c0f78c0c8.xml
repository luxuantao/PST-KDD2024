<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AutoFAS: Automatic Feature and Architecture Selection for Pre-Ranking System</title>
				<funder ref="#_vZ6gMmW">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-05-19">19 May 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
							<email>lixiang110@meituan.com</email>
						</author>
						<author>
							<persName><forename type="first">Xiaojiang</forename><surname>Zhou</surname></persName>
							<email>zhouxiaojiang@meituan.com</email>
						</author>
						<author>
							<persName><forename type="first">Yao</forename><surname>Xiao</surname></persName>
							<email>xiaoyao06@meituan.com</email>
						</author>
						<author>
							<persName><forename type="first">Peihao</forename><surname>Huang</surname></persName>
							<email>huangpeihao@meituan.com</email>
						</author>
						<author>
							<persName><forename type="first">Dayao</forename><surname>Chen</surname></persName>
							<email>chendayao@meituan.com</email>
						</author>
						<author>
							<persName><forename type="first">Sheng</forename><surname>Chen</surname></persName>
							<email>chensheng19@meituan.com</email>
						</author>
						<author>
							<persName><forename type="first">Yunsen</forename><forename type="middle">2018</forename><surname>Xian</surname></persName>
							<email>xianyunsen@meituan.com</email>
						</author>
						<author>
							<persName><surname>Autofas</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Meituan Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Meituan Inc</orgName>
								<address>
									<settlement>Beijing, Peihao Huang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Meituan Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Dayao Chen</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Meituan Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Sheng Chen</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Meituan Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country>China Yunsen Xian</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="institution">Meituan Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AutoFAS: Automatic Feature and Architecture Selection for Pre-Ranking System</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-05-19">19 May 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/1122445.1122456</idno>
					<idno type="arXiv">arXiv:2205.09394v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>pre-ranking</term>
					<term>feature and architecture selection</term>
					<term>effectiveness</term>
					<term>efficiency</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Industrial search and recommendation systems mostly follow the classic multi-stage information retrieval paradigm: matching, preranking, ranking, and re-ranking stages. To account for system efficiency, simple vector-product based models are commonly deployed in the pre-ranking stage. Recent works consider distilling the high knowledge of large ranking models to small pre-ranking models for better effectiveness. However, two major challenges in pre-ranking system still exist: (i) without explicitly modeling the performance gain versus computation cost, the predefined latency constraint in the pre-ranking stage inevitably leads to suboptimal solutions; (ii) transferring the ranking teacher's knowledge to a pre-ranking student with a predetermined handcrafted architecture still suffers from the loss of model performance. In this work, a novel framework AutoFAS is proposed which jointly optimizes the efficiency and effectiveness of the pre-ranking model: (i) AutoFAS for the first time simultaneously selects the most valuable features and network architectures using Neural Architecture Search (NAS) technique; (ii) equipped with ranking model guided reward during NAS procedure, AutoFAS can select the best pre-ranking architecture for a given ranking teacher without any computation overhead. Experimental results in our real world search system show Auto-FAS consistently outperforms the previous state-of-the-art (SOTA) approaches at a lower computing cost. Notably, our model has been adopted in the pre-ranking module in the search system of Meituan 1 , bringing significant improvements.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Due to the information overload, search engine and recommendation system are becoming increasingly indispensable in assisting users to find their preferred items in web-scale applications such as Amazon and Meituan. As it is shown in the Fig. <ref type="figure" target="#fig_0">1</ref>, a typical industrial searching system consists of four sequential stages: matching, pre-ranking, ranking and re-ranking. The effectiveness of search system not only influences the final revenue of whole platform, but also impacts user experience and satisfaction. In this paper, we mainly focus on the pre-ranking stage.</p><p>There already exist numerous works on ranking models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b33">34]</ref>. However, less attention is paid to pre-ranking models. The biggest obstacle to the development of pre-ranking system is the computation constraint. Taking the search engine of Meituan App for example. The size of the candidates to be scored for the pre-ranking system scales up to thousands, which is five times more than the subsequent ranking model. However, the latency limit is even more strict, e.g. 20 milliseconds at most. Specifically, approximately half of the latency is caused by feature retrieval, and the other half for model inference. Thus both features and architectures are needed to be optimized to achieve optimal results.</p><p>To meet the computation constraint, logistic regression is a widely used lightweight model in the age of shallow machine learning. Due to the success of deep learning <ref type="bibr" target="#b8">[9]</ref>, representation-focused architectures <ref type="bibr" target="#b6">[7]</ref> become dominant in the pre-ranking system. However, these representation-focused methods fail to utilize the interactive features, which turn out to be less efficient for computation but very effective for the expression ability <ref type="bibr" target="#b24">[25]</ref>. Recently, COLD <ref type="bibr" target="#b27">[28]</ref> firstly introduced interactive features into pre-ranking models. However, computation cost in COLD cannot be optimized jointly with model performance in an end-to-end manner. PFD <ref type="bibr" target="#b29">[30]</ref> approaches this problem from a different angle by distilling the interactive features from more accurate teachers. But PFD does not take computation cost into account. Current state-of-the-art method FSCD <ref type="bibr" target="#b18">[19]</ref> proposes a learnable feature selection method based on feature complexity and variational dropout. Nevertheless, as pointed in <ref type="bibr" target="#b13">[14]</ref>, variational dropout methods bias feature selection by ignoring hard-to-learn features. Moreover, none of these methods considers selecting the pre-ranking model architectures, which is also important for model efficiency.</p><p>Inspired by the recent work <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">22]</ref> in neural architecture search, we achieve a better trade-off between effectiveness and efficiency by designing a new pre-ranking methodology, which we name as AutoFAS: Automatic Feature and Architecture Selection for preranking system. In terms of feature selection, we formulate it as a feature pruning process. Specifically, we first train a regular ranking model with all input features, mainly including user features, item features and interactive features. To automatically learn which feature is important, we explicitly introduce feature mask parameters to control whether its output should be passed to the next layer. Masked or not totally depends on the contribution of each feature to the final prediction. Those insignificant features are pruned out at the end of training for searched pre-ranking models.</p><p>In parallel with feature selection, we relax the choices of candidate architectures to be continuous by introducing a set of architecture parameters (one for each architecture) so that the relative importance of each architecture can be learned by gradient descent. Similar to feature selection, only the top strength architectures are retained at the end of training for searched pre-ranking model. In order to distill the ranking teacher's knowledge into both parameters and architecture of the pre-ranking student, we introduce a knowledge distillation(KD) loss during searching process. As will be shown in table <ref type="table" target="#tab_5">3</ref>, even trained on the same task and dataset, Aut-oFAS can select different optimal student architectures for different teachers and they consistently outperform conventional students with handcrafted architectures <ref type="bibr" target="#b29">[30]</ref>. Finally, we model feature and architecture latency as a continuous function and optimize it as regularization loss with the aim of meeting the strict limitation of latency and computation resources. In our Table <ref type="table" target="#tab_4">4</ref>, we show that compared to previous state-of-the-art results, AutoFAS is able to achieve 2.04% improvement in AUC (Area Under Curve), 11% improvement in Recall rate <ref type="bibr" target="#b27">[28]</ref> and 1.22% improvement in CTR (Click Through Rate) with a significant 10.3% decrease in latency. A CTR lift of 0.1% is considered significant improvement <ref type="bibr" target="#b26">[27]</ref>.</p><p>To summarize, the main contribution of the paper can be highlighted as follows:</p><p>? To the best of our knowledge, AutoFAS is the first algorithm that simultaneously learns features and architectures in search, recommendation and online advertising systems.</p><p>In particular, it achieves a better trade-off between effectiveness and efficiency in pre-ranking stage. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Pre-Ranking Methods The structure of pre-ranking model has evolved from shallow to deep. As a pioneer work of deep vectorbased method, DSSM <ref type="bibr" target="#b11">[12]</ref> trains a non-linear projection to map the query and the documents to a common semantic space, where the relevance is calculated as the cosine similarity between vectors. MNS <ref type="bibr" target="#b30">[31]</ref> uses a mixture of batch and uniformly sampled negatives to tackle the selection bias in vector-based approaches. But as pointed in DIN <ref type="bibr" target="#b34">[35]</ref>, the lack of interaction features between user and item significantly hampers the performance of vectorbased pre-ranking models. With the increase of computing power, COLD <ref type="bibr" target="#b27">[28]</ref> firstly introduces interactive features to pre-ranking system by pruning unimportant features. However, the trade-off between model performance and computation cost in COLD is decided offline, inevitably leading to inferior performance. FSCD <ref type="bibr" target="#b18">[19]</ref> optimizes the efficiency and effectiveness in a learnable way. But it ignores the influence of underlying model structures.</p><p>NAS in Search and Recommendation System Neural Architecture Search (NAS) has been an active research area since 2017 <ref type="bibr" target="#b36">[37]</ref>. NIS <ref type="bibr" target="#b12">[13]</ref> utilizes NAS to learn the optimal vocabulary sizes and embedding dimensions for categorical features. AutoFIS <ref type="bibr" target="#b15">[16]</ref> formulates the problem of searching the effective feature interactions as a continuous searching problem using DARTS <ref type="bibr" target="#b16">[17]</ref> technique. Via modularizing representative interactions as virtual building blocks and wiring them into a space of direct acyclic graphs, AutoCTR <ref type="bibr" target="#b22">[23]</ref> searches the best CTR prediction model. AMEIR <ref type="bibr" target="#b31">[32]</ref> focuses on automatic behavior modeling, interaction Exploration and multilayer perceptron (MLP) Investigation. AutoIAS <ref type="bibr" target="#b28">[29]</ref> unifies existing interaction-based CTR prediction model architectures and propose an integrated search space for a complete CTR prediction model. However, none of these works focuses on the pre-ranking models. Knowledge Distillation in Search and Recommendation System Ranking Distillation <ref type="bibr" target="#b23">[24]</ref> firstly adopts the idea of knowledge distillation to large-scale ranking problems by generating additional training data and labels from unlabeled data set for student model. Rocket Launching <ref type="bibr" target="#b32">[33]</ref> proposes a mutual learning style framework to train well-performing light CTR models. PFD <ref type="bibr" target="#b29">[30]</ref> transfers the knowledge from a teacher model that additionally utilizes the privileged features to a regular student model. <ref type="bibr" target="#b35">[36]</ref> explores the use of a powerful ensemble of teachers for more accurate CTR student model training. CTR-BERT <ref type="bibr" target="#b19">[20]</ref> present a lightweight cache-friendly factorized model for CTR prediction that consists of twin-structured BERT-like encoders using cross-architecture knowledge distillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODS</head><p>Our work is built on top of neural architecture search (NAS), thus we first present an overview of this topic. Then we will give a brief introduction of pre-ranking and describe our proposed methods for pre-ranking in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Neural Architecture Search</head><p>Neural network design requires extensive experiments by human experts. In recent years, there has been a growing interest in developing algorithmic NAS solutions to automate the manual process of architecture design <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b36">37]</ref>. Some works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b21">22]</ref> attempt to improve search efficiency via sharing weights across models, which further divided into two categories: continuous relaxation method <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17]</ref> and One-Shot method <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8]</ref>. Basically we follow the weight sharing methodology which includes three steps:(1) Design an overparameterized network as search space containing every candidate architecture.</p><p>(2) Make architectural decisions directly on the training set or a held-out validation set. (3) Re-train the most promising architectures from scratch and evaluate their performance on the test set. Notice that one big difference between our scenario and previous results is that we need to jointly search for both features and architectures at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Introduction of Search and Recommendation System</head><p>The overall structure of search and recommendation system is already illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. Basically, the matching stage takes events from the user's activity history as well as current query (if exists) as input and retrieves a small subset (thousands) of items from a large corpus (millions). These candidates are intended to be generally relevant to the user with moderate precision. Then the pre-ranking stage provides broad personalization and filters out top hundreds items with high precision and recall. Some companies may choose to combine matching and pre-ranking stages, like Youtube <ref type="bibr" target="#b5">[6]</ref>. Then the complex ranking network assigns a score to each item according to a desired objective function using a rich set of features describing the item and user. The highest scoring items are presented to the user, ranked by their score, if without re-ranking.</p><p>In general, pre-ranking shares similar functionality of ranking. The biggest difference lies in the scale of the problem. Directly applying ranking models in the pre-ranking system will face severe challenge of computing power cost. How to balance the model performance and the computing power is the core part of designing the preranking system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Development History of Pre-Ranking in Meituan</head><p>As mentioned before, pre-ranking module can be viewed as a transition stage between matching and ranking. Meituan is the largest Chinese shopping platform for locally found consumer products and retail services including entertainment, dining, delivery, travel and other services. At main search of Meituan, it receives thousands of candidates from matching stage and filters out top hundreds for the ranking stage. Our underlying pre-ranking architecture evolved from two-tower models <ref type="bibr" target="#b11">[12]</ref>, Gradient Boosting Decision Tree (GBDT) models to the current deep neural network models during past years. As the performance increases, the excessive computational complexity and massive storage make it a greater challenge to deploy for real-time serving. The bottleneck of our online inference engine mainly contains two parts: feature retrieve from the database and deep neural network inference. Thus the feature selection and neural architecture selection are both important for the successful deployment of efficient and effective pre-ranking models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Feature and Architecture Selection in Pre-Ranking</head><p>One key motivation behind our approach is that we should co-build the pre-ranking model and subsequent ranking model such that the knowledge from ranking model can automatically guide us to find the most valuable features and architectures for pre-ranking model. Thus instead of training pre-ranking models separately, we co-train it with regular ranking model. We first describe the construction of the search space, then introduce how we leverage feature and architecture parameters to search for the most valuable features and architectures. Finally, we present our technique to handle the latency and KD-guided reward.</p><p>Search Space As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, the left half of the graph is our ranking network, while the right half is the over-parametered network that contains all candidate pre-ranking models. The two parts share the same input features ? = {? 1 , ? 2 , ..., ? ? }. In our setup, ? mainly consists of user features, item features and interactive features. We train the standard ranking model with all ? feature inputs and then zero out large portions of features of the ranking model to evaluate their importance, choosing the best feature combinations.</p><p>In parallel with feature selection, we need to search for the optimal architecture. Let O be a building block that contains ? different candidate operators: O = {? 1 , ? 2 , ? ? ? , ? ? }. In our case, O includes zero operator or multilayer perceptrons(MLP) <ref type="bibr" target="#b20">[21]</ref> with various hidden units. Zero operator is the operator that keeps the input as output. Some references also consider it as identity operator. Notice that zero operator allows the reducing of number of layers. Other operators such as outer product <ref type="bibr" target="#b25">[26]</ref> and dot product could also be similarly abstracted and integrated into the framework, which is left for future exploration. To construct the over-parameterized network that includes every candidate architecture, instead of setting each edge to be a definite primitive operation, we set each edge to be a mixed operation (Mixop) that has N parallel paths, denoted as ? ? . Then our over-parameterized network can be expressed as</p><formula xml:id="formula_0">N (? 1 = ? 1 ? , ? ? ? , ? ? = ? ? ? )</formula><p>, where ? is the total number of Mixops. Feature and Architecture Parameters To select the most efficient features, We introduce ? real-valued mask parameters {? ? } ? ?=1 , where ? is the number of features involved. Unlike <ref type="bibr" target="#b4">[5]</ref> which binarizes individual weights, we binarize entire feature embedding. Thus the independent mask ? ? for feature ? ? is defined as the following Bernoulli distribution:</p><formula xml:id="formula_1">? ? = [1, ? ? ? , 1], with probability ? ? [0, ? ? ? , 0], with probability 1 -? ?<label>(1)</label></formula><p>where the dimensions of 1s and 0s are determined by the embedding dimension of ? ? . ? independent Bernoulli distribution results are sampled for each batch of examples. Since the binary masks {? ? } ? ?=1 are involved in the computation graph, feature parameters {? ? } ? ?=1 can be updated through backpropagation.</p><p>In terms of architecture parameters, we will shown how to get the ? outputs of Mixop ? + 1, given the outputs of ? paths of Mixop ?. As shown in Fig. <ref type="figure" target="#fig_2">3</ref>, denote the paths of Mixop ? as</p><formula xml:id="formula_2">? ? ? = {? ? 1 , ? ? 2 , ? ? ? , ? ? ? }, we introduce ? real-valued architecture parameters {? ?+1 ? } ? ?=1 .</formula><p>Then the ?-th output of Mixop ? + 1 is computed as follows:</p><formula xml:id="formula_3">? ?+1 ? = ? ?? ?=1 ? ?+1 ? MLP ? ? (? ? ? ) = ? ?? ?=1 exp (? ?+1 ? ) ? ?=1 exp (? ?+1 ? ) MLP ? ? (? ? ? )<label>(2)</label></formula><p>where the multi-layer perceptron MLP ? has the same number of units as</p><formula xml:id="formula_4">? ?+1 ? , ? ?+1 ? := exp (? ?+1 ? ) ? ?=1 exp (? ?+1 ? )</formula><p>can be seen as the strength of the ?-th operator in Mixop ? + 1. After this continuous relaxation, our goal is to jointly learn the architecture parameters and the weight parameters within all the mixed operations. Latency Constraint Besides accuracy, latency (not FLOPs or embedding dimensions) is another very important objective when designing pre-ranking systems for real-world application. To make latency differentiable, we model the latency of a network as a continuous function of the neural network architectures. In our scenario, there exist two factors: feature related latency and architecture related latency. The features can be further divided into two categories from latency perspective: the ones passed from matching stage and the ones retrieved from in-memory dataset, denoted as ? 1 and ? 2 respectively. As such, we have the expected latency of a specific feature ? ? :</p><formula xml:id="formula_5">E[latency ? ] = ? ? ? ? ?<label>(3)</label></formula><p>where ? ? is the return time that can be recorded by the server.</p><p>Then the gradient of E[latency ? ] w.r.t. architecture parameters can thereby be given as: ?E[latency ? ]/?? ? = ? ? . Then the expected feature related latency of the network can be calculated as follows:</p><formula xml:id="formula_6">E[latency] = max ? ? ?? 1 ,? ? ?? 2 (E[latency ? ] + ? ? |? 1 |, E[latency ? ] + ? ? |? 2 |)<label>(4)</label></formula><p>where |? ? | denotes the number of features in ? ? , ? = 1, 2, ? and ? reflect the different concurrencies of the underlying system and can be decided empirically. We incorporate this expected feature latency into the regular loss function by multiplying a scaling factor ? which controls the trade-off between accuracy and latency. The final loss function for feature selection is given by: Loss1 = Loss Ranking (?, ? (? ; ?,? Ranking )) + ?E[latency]</p><p>(5)</p><p>where ? denotes the ranking network. Similarly, for architecture latency of Mixop ? + 1, we can compute its expected latency E[latency ??+1 ] by recursion, as shown in right figure of Fig. <ref type="figure" target="#fig_2">3</ref>. Since these operations are executed sequentially during inference, the expected latency of the pre-ranking network can be expressed as the expected latency of the last Mixop:</p><formula xml:id="formula_7">E[latency ? ] = E[latency ?L ]<label>(6)</label></formula><p>Ranking System Supervision Knowledge distillation <ref type="bibr" target="#b9">[10]</ref>, the process of transferring the generalization ability of the teacher model to the student, has recently received increasing attention from the research community and industry. While conventional onehot label in supervision learning constrains the 0/1 label, the soft probability output from teacher model contributes to the knowledge for student model. Remember that one drawback of current KD method <ref type="bibr" target="#b29">[30]</ref> in pre-ranking system is that it only transfers the teacher's knowledge to a student with fixed neural architecture. Inspired by the success of AKD <ref type="bibr" target="#b17">[18]</ref>, we propose to add a distillation loss to the architecture search process. Specifically, we employ the soft targets produced by ranking models as the supervision signal to guide the selection in each Mixop. Thus the final loss function for architecture selection is given: Loss2 =(1 -? 1 )Loss pre-Ranking (?, ?(? ; ?, ?,? pre-Ranking ))</p><formula xml:id="formula_8">+ ? 1 ||? (?) -? (?)|| 2 2 + ? 2 E[latency ? ]<label>(7)</label></formula><p>where ? is the pre-ranking network, Loss pre-Ranking denotes the pre-ranking pure loss with the known hard labels ?. ? (?) and ? (?) are final softmax activation outputs of ranking and pre-ranking network, respectively. We will further discuss the effectiveness of ? 1 and distillation loss in section 4.5. ? 2 is the scaling factor that controls the tradeoff between accuracy and latency. Loss1 and Loss2 are optimized together, resulting in the final multi-task loss function:</p><formula xml:id="formula_9">Loss = Loss1 + Loss2 (8)</formula><p>The absence of balancing hyperparameter between Loss1 and Loss2 comes from that Loss1 only optimizes feature mask parameters, while Loss2 optimizes architecture parameters and weights in the pre-ranking model. We choose this strategy because it is empirically better than the model without gradient block (both feature parameters and architecture parameters can be optimized by Loss2), shown in Table <ref type="table" target="#tab_7">5</ref>. Loss1 and Loss2 are related to each other by the fact that the input of Loss2 is the masked embedding, where the mask parameters are continuously optimized by Loss1 during training. To derive the final pre-ranking architecture, we retain the strongest features and operators in each Mixop and retrain it from scratch. The whole training process of AutoFAS can be summerized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we present the experiment results in detail. First, we introduce experiment datasets, training details and evaluation metric. Then we compare our proposed AutoFAS with competitors in terms of both feature selection and architecture selection. Finally, we discuss the effectiveness of critical technical designs in AutoFAS through ablation study. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>To the best of our knowledge, there is no public dataset for preranking task. Previous works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b27">28]</ref> in this area present results in their own dataset. To verify the effectiveness of AutoFAS, we conduct experiments on the industrial dataset of Meituan. It is collected from the platform searching system of Mobile Meituan App. Samples are constructed from impression logs, with 'click' or 'not' as label. This dataset contains more than 10 billion display/click logs of 20 million users and 400 million 'click' in 9 days. To alleviate the sample selection bias in pre-ranking, we preprocess these impression samples by adding non-displayed examples, depending on the sample orders in later ranking model. Training set is composed of samples from the first 7 days, validation and test set are from the following 2 days, a classic setting for industrial modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Settings</head><p>We choose the size ? of feature set as 500, mainly including user features, item features and interactive features. To build architecture space, we allow ? = 5 Mixops, including multi-layer perceptron (MLP) with various units {1024, 512, 256, 128, 64}. To enable a direct trade-off between width and depth, we add zero operation to the candidate set of its mixed operation. In this way, with a limited latency budget, the network can either choose to be shallower and wider by skipping more blocks and using MLPs with more unit or choose to be deeper and thinner by keeping more blocks and using MLPs with less units. The size of joint search space for both feature and architecture can be approximated as 2 500 ? 6 5 ? 10 155 .</p><p>We first train our ranking network for ? = 6 million steps without any mask to obtain reasonable embedding weights for input features. Then we continue regular optimization of Loss with respect to mask parameters ? , architecture parameters ? and weights in pre-ranking networks. The optimizer is Adagrad with learning rate 0.01 and the batch size is 50. Note that the feature embedding parameters and weights in ranking networks are fixed after initial 6 million steps.</p><p>The training cost of a pre-ranking model like COLD <ref type="bibr" target="#b27">[28]</ref> in our setup is 2 days, while AutoFAS needs to be trained for 3 days from scratch. In practice, the training cost could be reduced to 2 days by loading a well-trained ranking model, which is the common case in industry. Thus our AutoFAS methods will not add much training overhead compared to previous SOTA methods.</p><p>In terms of effectiveness measurement, we use three popular indexes: AUC (Area Under Curve) and Recall <ref type="bibr" target="#b27">[28]</ref> as offline metric, CTR (Click Through Rate) as online metric. Notice that the Recall serves to measure the alignment degree between the pre-ranking model and subsequent ranking model. For evaluation of system performance, we use metrics including RT (return time, which measures the latency of model) and CPU consumption rate metrics. All models reported in this paper run on a CPU machine with Intel(R) Xeon(R) CPU E5-2650 v4 @ 2.20GHz (12 cores) with 256GB RAM. Generally speaking, lower RT and CPU consumption means lower computation cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines</head><p>? VPDM <ref type="bibr" target="#b30">[31]</ref>: VPDM (Vector-Product based DNN Model) is a widely used method at the early stage of deep CTR prediction task. It maps the query and candidate items into common low-dimensional space where the ranking score of a item is readily computed as the inner product between the query and this item.</p><p>? COLD <ref type="bibr" target="#b27">[28]</ref>: Besides some engineered optimization tricks on its specific platform, COLD calculates the importance of every feature by Squeeze-and-Excitation block <ref type="bibr" target="#b10">[11]</ref> and selects the top ones based on the computation cost.</p><p>? FSCD <ref type="bibr" target="#b18">[19]</ref>: FSCD is the current state-of-the-art pre-ranking architecture that learns efficient features by considering feature complexity and variational dropout. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis of Feature Selection</head><p>Overall Result To fairly compare the effects of our feature selection algorithm with others, we fix a 3-layers' MLP, with the number of hidden neurons being 512 and 256, as the common pre-ranking structure for all methods. Table <ref type="table" target="#tab_1">1</ref> lists the model effectiveness and system efficiency for all models with different number N of features.</p><p>Particularly, when ? = 100, AutoFAS beats the current state-ofthe-art result FSCD by 0.03% and 1.0% in terms of AUC and Recall, respectively. However, the latency is 18.4% lower than FSCD. If we relax our model to have approximately the same latency and CPU consumption rate as FSCD, we can keep top-120 features and obtain significant performance boost by 2.61% and 10.0% in terms of AUC and Recall, respectively. We also observes that, when ? &gt; 120, the AUC and Recall increase slowly, while the latency and CPU cost increase remarkably. Detailed Examination In this part, we will investigate the effectiveness of features selected by different methods. Inspired by AutoFIS <ref type="bibr" target="#b15">[16]</ref>, we use statistics_AUC to represent the contribution of one single feature to the final prediction. For a given feature, we evaluate a well trained predictor with all feature inputs except for this one. Then the decrease of AUC is referred to as statistics_AUC of this feature. Thus higher statistics_AUC indicates a greater impact on the final prediction. We can visualize the statistics_AUC distribution of different methods in Figure <ref type="figure" target="#fig_3">4</ref>. Notice that the quantile is referred to the top-100 statistics_AUC features and 0 quantile means not among top-100. As it is shown, AutoFAS can select more high statistics_AUC features than FSCD and COLD. Specially, we find that some high statistics_AUC features like userItemDis (the distance between user and item) and userGeoItemView (the history interaction between user and item) are among top-100 of AutoFAS, but not FSCD or COLD. Since Meituan is a e-commerce platform for local services,  <ref type="table" target="#tab_3">2</ref>. We argue that there exist too much duplicated information in top statistics_AUC features, leading to inferior performance conjointly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Analysis of Architecture Selection</head><p>In this part, we investigate the power of our architecture selection module. The experiment is designed as follows. In Table <ref type="table" target="#tab_5">3</ref> we have two teachers, which are the same ranking model ? 0 with different distillation hyperparameter ? 1 . Besides the teacher model, the other settings, random seed, latency target, input features and dataset are fixed to be the same. As is shown in Table <ref type="table" target="#tab_5">3</ref>, student1 and student2 are two student architectures searched by two teachers, respectively. We also add the popular handcrafted architecture which has decreasing width (the detailed architectures of these three models are shown in Figure <ref type="figure" target="#fig_6">5</ref>). We observe that while student1 outperforms student2 with teacher1 as teacher, student2 works better with teacher2 as teacher, implying different teachers could have different best students. Note that the different best students supervised by different teachers consistently outperform the handcrafted architecture. This result indicates that our architecture selection module in the knowledge distillation is indeed necessary and can further boost the effectiveness without any overhead. We choose student1 as our final architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Analysis of Feature and Architecture Selection</head><p>In this experiment, we compare AutoFAS with all baseline models including VPDM, COLD and FSCD (all with 100 features). The dimension in VPDM is chosen to be 32. The underlying architecture for COLD and FSCD is the handcrafted one in Figure <ref type="figure" target="#fig_6">5</ref>. Apart from offline results, we also conduct a strict online A/B testing experiment to validate the proposed AutoFAS model, from 2021-07-18 to 2021-07-24. 10% of the total traffic is distributed for each model. As is shown in    In the above experiments, we utilize the gradient block technique to cancel the effect of distillation loss's back-propagation on feature mask parameters. Moreover, to maintain the maximal knowledge, the teacher ? 0 's output is produced without feature masks. Here we test the effects of such training manners. The result is also shown in Table <ref type="table" target="#tab_7">5</ref>. We can see discarding the gradient block technique brings 0.68% decrease in AUC. If we acquire the teacher output with feature masks, the AUC degenerates notably 0.95%. These two results together imply that the current training manner lead us much better performance while adding no burden to training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>In this paper, we device an end-to-end AutoML pre-ranking pipeline AutoFAS. Instead of simply considering feature combinations, Aut-oFAS simultaneously selects both features and model architectures. The joint optimization of computation cost and model performance ensures a better trade-off between effectiveness and efficiency. Furthermore, our tailored neural architecture search algorithm with KD-guided reward empowers AutoFAS knowledge from subsequent cumbersome ranking models. Experimental results on the realworld dataset demonstrate the effectiveness of the proposed preranking approach. Future work includes enriching search space,  exploring more efficient search strategies as well as automatic distributing computation power among matching, pre-ranking, ranking and re-ranking modules.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Real-world multi-stage ranking architecture with item numbers.</figDesc><graphic url="image-1.png" coords="2,89.84,83.69,168.17,174.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Network architecture of the proposed AutoFAS framework. AutoFAS is composed of two main parts. The left subnetwork is our regular ranking network with feature mask module. Since the search engine of Meituan serves multiple business domains with overlapping user groups and items, our ranking model has multi-partition structure. The right sub-network consists of ? Mixops including all candidate pre-ranking architectures. The selected strongest operator in each Mixop which denoted in dark color forms the final architecture of the pre-ranking model.</figDesc><graphic url="image-2.png" coords="4,104.24,83.69,403.53,212.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An example of computing the expected latency of each Mixop by recursion. Take the notation ? 1024?1024 in above equation for illustration. It means the latency of a multi-layer perceptron with input dimension 1024 and output dimension 1024. It is counted by replaying the real-world request of our search engine to this particular network architecture. Every ? in the figure is the operator strength defined in equation 2.</figDesc><graphic url="image-3.png" coords="5,104.24,83.69,403.53,149.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Statistics_AUC distribution of different methods</figDesc><graphic url="image-4.png" coords="7,317.96,83.69,240.25,139.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>4. 7</head><label>7</label><figDesc>Ablation Study4.7.1 Sensitivity of Hyper-parameter. In the above experiments, we first train our ranking network without any mask for ? = 6M global steps. Here we test the sensitivity of ?. The result is shown in Table 5. Different global steps can make a noticeable difference. Static global steps 6M improve the model performance than 3M significantly. However, 10M has only slightly performance improvement than 6M at the cost of almost double training time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>4. 7 . 2</head><label>72</label><figDesc>Effects of Training Manner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Selected and handcrafted neural architectures.</figDesc><graphic url="image-5.png" coords="8,366.00,330.60,144.15,89.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Algorithm 1 :</head><label>1</label><figDesc>AutoFAS: Automatic Feature and Architecture Selection for Pre-Ranking System Input: ? and ? set of input features and ranking network ? and ? ? feature mask and the ? th Mixop ?, ?, ? and ??? number of Mixops, initial step, total step and latency constraint SaveCheckpoint ? 0 // ? 0 serves as teacher model 6 for ? ?S+1 to ? do 7 Train(?, ? 1:? ; ? 0 , ? , ???) // optimize the parameters in mask ? and ? by minimizing the Loss in Equ. 8 SelectedFeaturesAndArchitectures = Top(?, ? 1:? ) // select the largest strength features and architecture in each Mixop 10 ? pre-ranking = Retrain(SelectedFeaturesAndArchitectures; ? 0 ) // distill the knowledge from ? 0 to our searched pre-ranking model Output: ? pre-ranking</figDesc><table><row><cell cols="2">1 SelectedFeaturesAndArchitectures = ?</cell></row><row><cell cols="2">2 for ? ? 1 to ? do</cell></row><row><cell>3</cell><cell>Train(? , ?) // train a regular ranking model</cell></row><row><cell cols="2">4 end</cell></row><row><cell cols="2">5 8 end</cell></row><row><cell>9</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Performance of different feature selection methods for pre-ranking system. Note that the latency in the table refers to the feature related latency.</figDesc><table><row><cell cols="3">N Method AUC Recall CPU Latency</cell></row><row><cell></cell><cell>COLD 0.7495 0.33</cell><cell>22% 6.32 ms</cell></row><row><cell>50</cell><cell>FSCD 0.7503 0.35</cell><cell>21% 5.75 ms</cell></row><row><cell></cell><cell cols="2">AutoFAS 0.7505 0.36 17% 4.59 ms</cell></row><row><cell></cell><cell>COLD 0.8001 0.52</cell><cell>30% 7.27 ms</cell></row><row><cell cols="2">100 FSCD 0.8009 0.52</cell><cell>29% 7.19 ms</cell></row><row><cell></cell><cell cols="2">AutoFAS 0.8012 0.53 26% 5.87 ms</cell></row><row><cell cols="2">120 AutoFAS 0.8270 0.62</cell><cell>30% 7.28 ms</cell></row><row><cell></cell><cell>COLD 0.8274 0.63</cell><cell>39% 9.18 ms</cell></row><row><cell cols="2">150 FSCD 0.8283 0.65</cell><cell>39% 9.11 ms</cell></row><row><cell></cell><cell cols="2">AutoFAS 0.8285 0.67 35% 7.99 ms</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison of top-100 features selected by AutoFAS and Statistics_AUC</figDesc><table><row><cell>N</cell><cell>Method</cell><cell>AUC Recall CPU Latency</cell></row><row><cell>100</cell><cell cols="2">Statistics_AUC 0.7781 0.50 28% 7.03 ms AutoFAS 0.8012 0.53 26% 5.87 ms</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table /><note><p>, AutoFAS achieves great gain of 1.8% in CTR in Meituan main search scene. In terms of system efficiency, compare to current state-of-the-art model FSCD, AutoFAS decrease</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison of handcrafted pre-ranking architectures with different students searched by different teachers.</figDesc><table><row><cell>Teachers</cell><cell cols="3">AUC Student1 Student2 handcrafted</cell><cell>Comparison</cell></row><row><cell>Teacher1(? 1 = 0.2)</cell><cell>0.8576</cell><cell>0.8559</cell><cell>0.8551</cell><cell>handcrafted &lt;student2 &lt; student1</cell></row><row><cell>Teacher2(? 1 = 0.6)</cell><cell>0.8457</cell><cell>0.8471</cell><cell>0.8447</cell><cell>handcrafted &lt;student1 &lt; student2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Performance of different pre-ranking models. Notice that the CTR is reported through a online A/B test and latency is the entire latency including feature retrieval latency and model inference latency. Method AUC Recall CTR CPU Latency VPDM 0.7535 0.49 -46% 18.7ms COLD 0.8350 0.70 +0.34% 51% 22.8ms FSCD 0.8372 0.72 +0.58% 51% 22.4ms AutoFAS 0.8576 0.83 +1.80% 48% 20.1ms the latency by 10.3% with a surprising absolute CTR lift of 1.22%, which is significant to the business.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Comparisons of different framework design's result. Method AUC Recall CPU Latency AutoFAS(S=6M) 1 0.8576 0.83 48% 20.1ms AutoFAS(S=3M) 0.8552 0.82 47% 19.4ms AutoFAS(S=10M) 0.8579 0.83 48% 19.8ms AutoFAS(w/o GB) 2 0.8508 0.80 48% 20.3ms AutoFAS(w FM) 3 0.8481 0.78 48% 19.6ms 1 base model 2 w/o GB means removing gradient block technique 3 w FM means teacher inference with feature masks</figDesc><table /></figure>
		</body>
		<back>

			<div type="funding">
<div><p>: <rs type="projectName">Automatic Feature and Architecture Selection for Pre-Ranking System. In Proceedings of Woodstock '18: ACM Symposium on Neural Gaze Detection</rs> (Woodstock '18). ACM, New York, NY, USA, 9 pages.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_vZ6gMmW">
					<orgName type="project" subtype="full">Automatic Feature and Architecture Selection for Pre-Ranking System. In Proceedings of Woodstock &apos;18: ACM Symposium on Neural Gaze Detection</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Understanding and Simplifying One-Shot Architecture Search</title>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v80/bender18a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning (Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">Jennifer</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</editor>
		<meeting>the 35th International Conference on Machine Learning ( Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="550" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">SMASH: One-Shot Model Architecture Search through HyperNetworks</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theodore</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Weston</surname></persName>
		</author>
		<idno>ArXiv abs/1708.05344</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno>ArXiv abs/1812.00332</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Wide &amp; Deep Learning for Recommender Systems</title>
		<author>
			<persName><forename type="first">Heng-Tze</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Levent</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremiah</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrishi</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Glen</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Ispir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zakaria</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vihan</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hemal</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.07792</idno>
		<ptr target="http://arxiv.org/abs/1606.07792" />
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BinaryConnect: Training Deep Neural Networks with Binary Weights during Propagations</title>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Pierre</forename><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
		<title level="s">NIPS&apos;15</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems<address><addrLine>Montreal, Canada; Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3123" to="3131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep neural networks for youtube recommendations</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emre</forename><surname>Sargin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM conference on recommender systems</title>
		<meeting>the 10th ACM conference on recommender systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep Neural Networks for YouTube Recommendations</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emre</forename><surname>Sargin</surname></persName>
		</author>
		<idno type="DOI">10.1145/2959100.2959190</idno>
		<ptr target="https://doi.org/10.1145/2959100.2959190" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Conference on Recommender Systems</title>
		<meeting>the 10th ACM Conference on Recommender Systems<address><addrLine>Boston, Massachusetts, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
	<note>RecSys &apos;16</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Single Path One-Shot Neural Architecture Search with Uniform Sampling</title>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyuan</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58517-4_32</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58517-4_32" />
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="544" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Distilling the Knowledge in a Neural Network</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Squeeze-and-Excitation Networks</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2019.2913372</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2019.2913372" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="2011" to="2023" />
			<date type="published" when="2020-08">2020. Aug. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning Deep Structured Semantic Models for Web Search Using Clickthrough Data</title>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
		<idno type="DOI">10.1145/2505515.2505665</idno>
		<ptr target="https://doi.org/10.1145/2505515.2505665" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 22nd ACM International Conference on Information &amp; Knowledge Management<address><addrLine>San Francisco, California, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2333" to="2338" />
		</imprint>
	</monogr>
	<note>CIKM &apos;13)</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Manas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Joglekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taibai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><forename type="middle">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Khaitan</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394486.3403288</idno>
		<ptr target="https://doi.org/10.1145/3394486.3403288" />
		<title level="m">Neural Input Search for Large Scale Recommendation Models</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2387" to="2397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">On Dropout, Overfitting, and Interaction Effects in Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Lengerich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.00823</idno>
		<ptr target="https://arxiv.org/abs/2007.00823" />
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving One-Shot NAS by Suppressing the Posterior Fading</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020">2020. 2020. 2020</date>
			<biblScope unit="page" from="13833" to="13842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">AutoFIS: Automatic Feature Interaction Selection in Factorization Models for Click-Through Rate Prediction. Association for Computing Machinery</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jincai</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394486.3403314</idno>
		<ptr target="https://doi.org/10.1145/3394486.3403314" />
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2636" to="2645" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<title level="m">DARTS: Differentiable Architecture Search</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuhui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raviteja</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bradley</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.00756</idno>
		<ptr target="https://doi.org/10.1109/CVPR42600.2020.00756" />
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7536" to="7545" />
		</imprint>
	</monogr>
	<note>Search to Distill: Pearls Are Everywhere but Not the Eyes</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Towards a Better Tradeoff between Effectiveness and Efficiency in Pre-Ranking: A Learnable Feature Selection Based Approach</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoguo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuhan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuang-Chih</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="DOI">10.1145/3404835.3462979</idno>
		<ptr target="https://doi.org/10.1145/3404835.3462979" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, Canada) (SIGIR &apos;21)</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, Canada) (SIGIR &apos;21)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2036" to="2040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">CTR-BERT: Cost-effective knowledge distillation for billion-parameter teacher models</title>
		<author>
			<persName><forename type="first">Aashiq</forename><surname>Muhamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iman</forename><surname>Keivanloo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujan</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">A</forename><surname>Mracek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santosh</forename><surname>Rajagopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Belinda</forename><surname>Zeng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deep Learning Recommendation Model for Personalization and Recommendation Systems</title>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Naumov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Hao-Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narayanan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongsoo</forename><surname>Sundaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Udit</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carole-Jean</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmytro</forename><surname>Azzolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Dzhulgakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mallevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghai</forename><surname>Cherniavskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghuraman</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ansha</forename><surname>Krishnamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Kondratenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianjie</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenlin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><surname>Smelyanskiy</surname></persName>
		</author>
		<idno>ArXiv abs/1906.00091</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient Neural Architecture Search via Parameters Sharing</title>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melody</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v80/pham18a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning (Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">Jennifer</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</editor>
		<meeting>the 35th International Conference on Machine Learning ( Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="4095" to="4104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Towards Automated Neural Interaction Discovery for Click-Through Rate Prediction</title>
		<author>
			<persName><forename type="first">Qingquan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehua</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanning</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiyan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ranking Distillation: Learning Compact Ranking Models With High Performance for Recommender System</title>
		<author>
			<persName><forename type="first">Jiaxi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3219819.3220021</idno>
		<ptr target="https://doi.org/10.1145/3219819.3220021" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining<address><addrLine>London, United Kingdom; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2289" to="2298" />
		</imprint>
	</monogr>
	<note>KDD &apos;18)</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Billion-scale commodity embedding for e-commerce recommendation in alibaba</title>
		<author>
			<persName><forename type="first">Jizhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pipei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhibo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binqiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dik</forename><surname>Lun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="839" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Deep &amp; Cross Network for Ad Click Predictions</title>
		<author>
			<persName><forename type="first">Ruoxi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingliang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3124749.3124754</idno>
		<ptr target="https://doi.org/10.1145/3124749.3124754" />
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">DCN V2: Improved Deep &amp; Cross Network and Practical Lessons for Web-Scale Learning to Rank Systems</title>
		<author>
			<persName><forename type="first">Ruoxi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rakesh</forename><surname>Shivanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sagar</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="1785" to="1797" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">COLD: Towards the Next Generation of Pre-Ranking System</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biye</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guorui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
		<idno>ArXiv abs/2007.16122</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">AutoIAS: Automatic Integrated Architecture Searcher for Click-Trough Rate Prediction</title>
		<author>
			<persName><forename type="first">Zhikun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3459637.3482234</idno>
		<ptr target="https://doi.org/10.1145/3459637.3482234" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="2101" to="2110" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Privileged Features Distillation at Taobao Recommendations</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junfeng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhua</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Ou</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394486.3403309</idno>
		<ptr target="https://doi.org/10.1145/3394486.3403309" />
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="2590" to="2598" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Mixed Negative Sampling for Learning Two-tower Neural Networks in Recommendations</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">Zhiyuan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taibai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">AMEIR: Automatic Behavior Modeling, Interaction Exploration and MLP Investigation in the Recommender System</title>
		<author>
			<persName><forename type="first">Pengyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kecheng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaigui</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2021/290</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2021/290MainTrack" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21</title>
		<editor>
			<persName><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</editor>
		<meeting>the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2104" to="2110" />
		</imprint>
	</monogr>
	<note>International Joint Conferences on Artificial Intelligence Organization</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Rocket Launching: A Universal and Efficient Framework for Training Well-performing Light Net</title>
		<author>
			<persName><forename type="first">Guorui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runpeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep interest network for click-through rate prediction</title>
		<author>
			<persName><forename type="first">Guorui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenru</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junqi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1059" to="1068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning Tree-Based Deep Model for Recommender Systems</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengye</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guozheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
		<idno type="DOI">10.1145/3219819.3219826</idno>
		<ptr target="https://doi.org/10.1145/3219819.3219826" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining<address><addrLine>London, United Kingdom; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1079" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Jieming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jincai</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zibin</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="DOI">10.1145/3340531.3412704</idno>
		<ptr target="https://doi.org/10.1145/3340531.3412704" />
		<title level="m">Ensembled CTR Prediction via Knowledge Distillation (CIKM &apos;20)</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2941" to="2958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Neural Architecture Search with Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno>ArXiv abs/1611.01578</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
