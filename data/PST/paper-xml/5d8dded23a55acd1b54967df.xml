<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Under review as a conference paper at ICLR 2020 GRAPHMIX: REGULARIZED TRAINING OF GRAPH NEURAL NETWORKS FOR SEMI-SUPERVISED LEARN-ING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">Under review as a conference paper at ICLR 2020 GRAPHMIX: REGULARIZED TRAINING OF GRAPH NEURAL NETWORKS FOR SEMI-SUPERVISED LEARN-ING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present GraphMix, a regularized training scheme for Graph Neural Network based semi-supervised object classification, leveraging the recent advances in the regularization of classical deep neural networks. Specifically, we propose a unified approach in which we train a fully-connected network jointly with the graph neural network via parameter sharing, interpolation-based regularization and self-predicted-targets. Our proposed method is architecture agnostic in the sense that it can be applied to any variant of graph neural networks which applies a parametric transformation to the features of the graph nodes. Despite its simplicity, with GraphMix we can consistently improve results and achieve or closely match state-of-the-art performance using even simpler architectures such as Graph Convolutional Networks, across three established graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as three newly proposed datasets : Cora-Full, Co-author-CS and Co-author-Physics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Due to the presence of graph structured data across a wide variety of domains, such as biological networks, social networks and telecommunication networks, there have been several attempts to design neural networks that can process arbitrarily structured graphs. Early work includes <ref type="bibr" target="#b16">(Gori et al.;</ref><ref type="bibr" target="#b34">Scarselli et al., 2009)</ref> which propose a neural network that can directly process most type of graphs e.g., acyclic, cyclic, directed, and undirected graphs. More recent approaches include <ref type="bibr" target="#b6">(Bruna et al., 2013;</ref><ref type="bibr" target="#b19">Henaff et al., 2015;</ref><ref type="bibr" target="#b8">Defferrard et al., 2016;</ref><ref type="bibr" target="#b20">Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b15">Gilmer et al., 2017;</ref><ref type="bibr" target="#b18">Hamilton et al., 2017;</ref><ref type="bibr" target="#b39">Veličković et al., 2018;</ref><ref type="bibr">2019;</ref><ref type="bibr" target="#b33">Qu et al., 2019;</ref><ref type="bibr" target="#b13">Gao &amp; Ji, 2019;</ref><ref type="bibr" target="#b28">Ma et al., 2019)</ref>, among others. Many of these approaches are designed for addressing the important problem of Semi-supervised learning over graph structured data <ref type="bibr">(Zhou et al., 2018)</ref>. Much of this research effort has been dedicated to developing novel architectures.</p><p>Unlike many existing works which try to come up with the new architectures, we re-synthesize ideas from the recent advances in the regularization of the classical neural network, and propose an architecture-agnostic framework for regularized training of graph neural network based semisupervised object classification. Recently, Data-Augmentation based regularization has been shown to be very effective in other types of neural networks but how to apply these techniques in graph neural networks is still under-explored. Our proposed method GraphMix<ref type="foot" target="#foot_0">1</ref> is a unified framework that utilizes interpolation based data augmentation <ref type="bibr" target="#b46">(Zhang et al., 2018;</ref><ref type="bibr" target="#b41">Verma et al., 2019a)</ref> and self-targetprediction based data-augmentation techniques <ref type="bibr" target="#b25">(Laine &amp; Aila, 2016;</ref><ref type="bibr" target="#b36">Tarvainen &amp; Valpola, 2017;</ref><ref type="bibr" target="#b42">Verma et al., 2019b;</ref><ref type="bibr">Berthelot et al., 2019)</ref>. We show that with our proposed unified framework, we can achieve state-of-the-art performance even when using simpler graph neural network architectures such as Graph Convolutional Networks <ref type="bibr" target="#b21">(Kipf &amp; Welling, 2017)</ref> and without incurring any significant additional computation cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PROBLEM DEFINITION AND PRELIMINARIES 2.1 PROBLEM SETUP</head><p>We are interested in the problem of semi-supervised object classification using graph structured data. We can formally define such graph structured data as G = (V, A), where V represents the set of nodes {v 1 , . . . , v n }, and A is the adjacency matrix representing the edges between the nodes of V.</p><p>Each node v i in the graph has a corresponding d-dimensional feature vector x i ∈ R d . The feature vectors of all the nodes X = [x 1 , . . . , x n ] are stacked together to form the entire feature matrix X ∈ R n×d . Each node belongs to one out of C classes and can be labeled with a C-dimensional one-hot vector y i ∈ {0, 1} C . Given the labels of Y L for few of the labeled nodes V L ⊂ V, the task is to predict the labels Y U of the remaining nodes V U = V \ V L .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">GRAPH NEURAL NETWORKS</head><p>Graph Neural Networks (GNN) learn the l th layer representations of a sample i by leveraging the representations of the samples N B(i) in the neighbourhood of i. This is done by using an aggregation function that takes as an input the representations of all the samples and the graph structure and outputs the aggregated representation. The aggregation function can be defined using the Graph Convolution layer <ref type="bibr" target="#b21">(Kipf &amp; Welling, 2017)</ref>, Graph Attention Layer <ref type="bibr" target="#b39">(Veličković et al., 2018)</ref>, or any general message passing layer <ref type="bibr" target="#b15">(Gilmer et al., 2017)</ref>. Formally, let H l ∈ R n×k be a matrix containing the k-dimensional representation of n nodes in the l th layer, then:</p><formula xml:id="formula_0">H l+1 = a(H l W, A)<label>(1)</label></formula><p>where W ∈ R k×k is a linear transformation matrix, k is the dimension of (l + 1) th layer and a is the aggregation function that utilizes the graph adjacency matrix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">INTERPOLATION BASED REGULARIZATION TECHNIQUES</head><p>Recently, interpolation-based techniques have been proposed for regularizing neural networks. We briefly describe some of these techniques here. Mixup <ref type="bibr" target="#b46">(Zhang et al., 2018)</ref> trains a neural network on the convex combination of input and targets, whereas Manifold Mixup <ref type="bibr" target="#b41">(Verma et al., 2019a</ref>) trains a neural network on the convex combination of the hidden states of a randomly chosen hidden layer and the targets. While Mixup regularizes a neural network by enforcing that the model output should change linearly in between the examples in the input space, Manifold Mixup regularizes the neural network by learning better (more discriminative) hidden states.</p><p>Formally, suppose g : x − → h is a function that maps input sample to hidden states, f : h − → ŷ is a function that maps hidden states to predicted output, λ is a random variable drawn from Beta(α, α) <ref type="figure">(x, y</ref>) and (x , y ) is a pair of examples sampled from distribution D and be a loss function such as cross-entropy loss, then the Manifold Mixup Loss is defined as:</p><formula xml:id="formula_1">distribution, Mix λ (a, b) = λ * a + (1 − λ) * b is an interpolation function, D is the data distribution,</formula><formula xml:id="formula_2">L = E (x,y)∼D E (x ,y )∼D E λ∼Beta(α,α) (f (Mix λ (g(x), g(x ))), Mix λ (y, y )).</formula><p>(2)</p><p>We use above Manifold Mixup loss for training an auxiliary Fully-connected-network as described in Section 3 and Line 6 and 12 of Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GRAPHMIX</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MOTIVATION</head><p>Data Augmentation is arguably the simplest and most efficient technique for regularizing a neural network. In some domains, such as computer vision, speech and text, there exist efficient data augmentation techniques, for example, random cropping, translation or Cutout (Devries &amp; Taylor,  <ref type="formula">2017</ref>) for text domain. However, data augmentation for the graph-structured data remains under-explored.</p><p>There exists some recent work along these lines but the prohibitive additional computation cost (see Section 5.3) introduced by these methods make them impractical for real-world large graph datasets.</p><p>Based on these limitations, our main objective is to propose an efficient data augmentation technique for graph datasets.</p><p>Recent work based on interpolation-based data augmentation <ref type="bibr" target="#b46">(Zhang et al., 2018;</ref><ref type="bibr" target="#b41">Verma et al., 2019a)</ref> has seen sizable improvements in regularization performance across a number of tasks. However, these techniques are not directly applicable to graphs for an important reason: Although we can create additional nodes by interpolating the features and corresponding labels, it remains unclear how these new nodes must be connected to the original nodes via synthetic edges such that the structure of the whole graph is preserved. To alleviate this issue, we propose to train an auxiliary Fully-connected-net (FCN) using Manifold Mixup as discussed in Section 3.2. Note that the FCN only uses the node features ( not the graph structure), thus the Manifold mixup loss in Eq. 2 can be directly used for training the FCN. Furthermore, drawing inspiration from the success of selfsupervised semi-supervised learning algorithms (self-predicted-targets based algorithms which can be also interpreted as a form of data-augmentation techniques) <ref type="bibr" target="#b42">(Verma et al., 2019b;</ref><ref type="bibr">Berthelot et al., 2019)</ref>, we explore self-supervision in the training of GNNs. We note that self-supervision has already been explored for unsupervised representation learning from graph structured data <ref type="bibr" target="#b40">(Veličković et al., 2019)</ref>, but not for semi-supervised object classification over graph structured data. Based on these challenges and motivations we present our proposed approach GraphMix for training Graph Neural Networks in the following Section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">METHOD</head><p>GraphMix augments the vanilla GNN with a Fully Connected Network (FCN). The FCN loss is computed using the Manifold Mixup as discussed in Section 2.3 and the GNN loss is computed in the standard way. Both the FCN loss and GNN loss are optimized in an alternating fashion during training. Furthermore, the predicted targets from the GNN are used to augment the training set of the FCN. In this way, both FCN and GNN facilitate each other's learning process. At inference time, the predictions are made using only GNN. The diagrammatic representation of GraphMix is presented in Figure <ref type="figure" target="#fig_0">1</ref> and the full algorithm is presented in Algorithm 1.</p><p>The GraphMix framework can be applied to any underlying GNN as long as the underlying GNN applies parametric transformations to the node features. In our experiments, we show the improvements over <ref type="bibr">GCN (Kipf &amp; Welling, 2016)</ref>, GAT <ref type="bibr" target="#b39">(Veličković et al., 2018)</ref> and Graph U-Net using GraphMix . Furthermore, GraphMix(GCN) framework does not add any significant computation cost over the underlying GNN, because the underlying GNN is trained in the standard way and the FCN training requires trivial additional computation cost for computing the predicted-targets (Section 3.2.1 and 3.2.1) and the interpolation function ( Mix λ (a, b) in Section 2.3). There are no additional memory requirements for GraphMix(GCN) , since FCN and GNN share the parameters. Some implementation considerations. For Manifold Mixup training of FCN, we apply mixup only in the hidden layer. Note that in <ref type="bibr" target="#b41">Verma et al. (2019a)</ref>, the authors recommended applying mixing in a randomly chosen layer (which also includes the input layer) at each training update. However, we observed under-fitting when applying mixup randomly at the input layer or hidden layer. Applying mixup only in the input layer also resulted in underfitting and did not improve test accuracy.</p><p>The performance of self-supervision based algorithms such as GraphMix is greatly affected by the accuracy of the predicted targets. To improve the accuracy of the predicted targets, we applied the average of the model prediction on K random perturbations of an input sample as discussed in Section 3.2.1 and sharpening as described in Section 3.2.2. Further, we draw similarities and difference of GraphMix w.r.t. Co-training framework in the Section 3.2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">ACCURATE TARGET PREDICTION FOR UNLABELED DATA</head><p>Recent state-of-the-art semi-supervised learning methods use a teacher model to accurately predict targets for the unlabeled data. These predicted targets on the unlabeled data are used as "true labels" for further training of the model. The teacher model can be realized as a temporal ensemble of the student model (the model being trained) <ref type="bibr" target="#b25">(Laine &amp; Aila, 2016)</ref> or by using an Exponential Moving Average (EMA) of the parameters of the student model <ref type="bibr" target="#b36">(Tarvainen &amp; Valpola, 2017)</ref>. Another recently proposed method for accurate target predictions for unlabeled data is to use the average of the predicted targets across K random augmentations of the input sample <ref type="bibr">(Berthelot et al., 2019)</ref>.</p><p>Along these lines, in this work, we compute the predicted target as the average of predictions on K drop-out versions of the input sample. We also used the EMA of the student model but it did not improve test accuracy across all the datasets (see Section 4.4 for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">ENTROPY MINIMIZATION</head><p>Many recent semi-supervised learning algorithms <ref type="bibr" target="#b25">(Laine &amp; Aila, 2016;</ref><ref type="bibr" target="#b29">Miyato et al., 2018;</ref><ref type="bibr" target="#b36">Tarvainen &amp; Valpola, 2017;</ref><ref type="bibr" target="#b42">Verma et al., 2019b)</ref> are based on the cluster assumption <ref type="bibr" target="#b7">(Chapelle et al., 2010)</ref>, which posits that the class boundary should pass through the low-density regions of the marginal data distribution. One way to enforce this assumption is to explicitly minimize the entropy of the model's predictions p(y|x, θ) on unlabeled data by adding an extra loss term to the original loss term <ref type="bibr" target="#b17">(Grandvalet &amp; Bengio, 2005)</ref>. The entropy minimization can be also achieved implicitly by modifying the model's prediction on the unlabeled data such that the prediction has low entropy and using these low-entropy predictions as targets for the further training of the model. Examples include "Pseudolabels" <ref type="bibr" target="#b26">(Lee, 2013)</ref> and "Sharpening" <ref type="bibr">(Berthelot et al., 2019)</ref>. Pseudolabeling constructs hard(1-hot) labels for the unlabeled samples which have "high-confidence predictions". Since many of the unlabeled samples may have "low-confidence predictions", they can not be used in the Pseudolabeling technique. On the other hand, Sharpening does not require "high-confidence predictions" , and thus it can be used for all the unlabelled samples. Hence in this work, we use Sharpening for entropy minimization. The Sharpening function over the model prediction p(y|x, θ) can be formally defined as follows <ref type="bibr">(Berthelot et al., 2019)</ref>, where T is the temperature hyperparameter and C is the number of classes:</p><formula xml:id="formula_3">Sharpen(p i , T ) := p 1 T i C j=1 p 1 T j (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">CONNECTION TO CO-TRAINING</head><p>The GraphMix approach can be seen as a special instance of the Co-training framework <ref type="bibr" target="#b4">(Blum &amp; Mitchell, 1998)</ref>. Co-training assumes that the description of an example can be partitioned into two distinct views and either of these views would be sufficient for learning if we had enough labeled data. In this framework, two learning algorithms are trained separately on each view and then the prediction of each learning algorithm on the unlabeled data is used to enlarge the training set of the other. Our method has some important differences and similarities to the Co-training framework. Similar to Co-training, we train two neural networks and the predictions from the GNN are used to enlarge the training set of FCN. The important difference is that instead of using the predictions from the FCN to enlarge the training set for the GNN, we employ parameter sharing for passing the learned information from FCN to GNN. In our experiments, directly using the predictions of the FCN for GNN training resulted in reduced accuracy. This is due to the fact that the number of labeled samples for training the FCN is sufficiently low and hence the FCN does not make accurate enough predictions. Another important difference is that unlike the co-training framework, FCN and GNN do not use completely distinct views of the data: the FCN uses feature vectors X and the GNN uses the feature vector and adjacency matrix (X, A).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We present results for the GraphMix algorithm using standard benchmark datasets and the standard architecture in Section 4.2 and 4.3. We also conduct an ablation study on GraphMix in Section 4.4 to understand the contribution of various components to its performance. Refer to Appendix A.5 for implementation and hyperparameter details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">DATASETS</head><p>We use three standard benchmark citation network datasets for semi-supervised node classification, namely Cora, Citeseer and Pubmed. In all these datasets, nodes correspond to documents and edges correspond to citations. Node features correspond to the bag-of-words representation of the document. Each node belongs to one of C classes. During training, the algorithm has access to the feature vectors and edge connectivity of all the nodes but has access to the class labels of only a few of the nodes.</p><p>For semi-supervised link classification, we use two datasets Bitcoin Alpha and Bitcoin OTC from <ref type="bibr" target="#b23">(Kumar et al., 2016;</ref><ref type="bibr">2018)</ref>. The nodes in these datasets correspond to the bitcoin users and the edge weights between them correspond to the degree of trust between the users. Following <ref type="bibr" target="#b33">(Qu et al., 2019)</ref>, we treat edges with weights greater than 3 as positive instances, and edges with weights less than -3 are treated as negative ones. Given a few labeled edges, the task is to predict the labels of the remaining edges. The statistics of these datasets as well as the number of training/validation/test nodes is presented in Appendix A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">SEMI-SUPERVISED NODE CLASSIFICATION</head><p>For baselines, we choose <ref type="bibr">GCN (Kipf &amp; Welling, 2017)</ref>, and the recent state-of-the-art methods GAT <ref type="bibr" target="#b39">(Veličković et al., 2018)</ref>, GMNN <ref type="bibr" target="#b33">(Qu et al., 2019)</ref> and Graph U-Net <ref type="bibr" target="#b13">(Gao &amp; Ji, 2019)</ref>. We additionally use two self-training based baselines: in the first one, we trained a GCN with self-generated predicted targets, and in the second one, we trained a FCN with self-generated predicted targets, named "GCN (with predicted-targets)" and "FCN (with predicted-targets)" respectively in Table <ref type="table" target="#tab_1">1</ref>. For generating the predicted-targets in above two baselines, we followed the procedure of Section 3.2.1 and Section 3.2.2. GraphMix(GCN) , GraphMix(GAT) and GraphMix(Graph U-Net) refer to the methods where underlying GNNs are GCN, GAT and Graph U-Net respectively. Refer to Appendix Section A.5 for implementation and hyperparameter details. <ref type="bibr" target="#b35">Shchur et al. (2018)</ref> has demonstrated that the performance of the current state-of-the-art Graph Neural Networks on the standard train/validation/test split of the popular benchmark datasets (such as Cora, Citeseer, Pubmed, etc) is significantly different from their performance on the random splits. For fair evaluation, they recommend using multiple random partitions of the datasets. Along these lines, we created 10 random splits of the Cora, Citeseer and Pubmed with the same train/ validation/test number of samples as in the standard split. We also provide the results for the standard train/validation/test split in Table <ref type="table">5</ref> in Appendix A.2.</p><p>The results are presented in Table <ref type="table" target="#tab_1">1</ref>. We observe that GraphMix always improves the accuracy of the underlying GNNs such as GCN, GAT and Graph U-Net across all the dataset, with GraphMix(GCN) achieving the best results.</p><p>We further present results with fewer labeled samples and results on larger datasets (Cora-Full, Co-author-CS and Co-author-Physics) Section A.3 and Section A.4 respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">SEMI-SUPERVISED LINK CLASSIFICATION</head><p>In the Semi-supervised Link Classification problem, the task is to predict the labels of the remaining links, given a graph and labels of a few links. Following <ref type="bibr" target="#b37">(Taskar et al., 2004)</ref>, we can formulate the link classification problem as a node classification problem. Specifically, given an original graph G, we construct a dual Graph G . The node set V of the dual graph corresponds to the link set E of the original graph. The nodes in the dual graph G are connected if their corresponding links in the graph G share a node. The attributes of a node in the dual graph are defined as the index of the nodes of the corresponding link in the original graph. Using this formulation, we present results on link classification on Bit OTC and Bit Alpha benchmark datasets in the Table <ref type="table" target="#tab_2">2</ref>. As the numbers of the positive and negative edges are strongly imbalanced, we report the F1 score. Our results show that GraphMix(GCN) improves the performance over the baseline GCN method for both the datasets. Furthermore, the results of GraphMix(GCN) are comparable with the recently proposed state-of-the-art method GMNN <ref type="bibr" target="#b33">(Qu et al., 2019)</ref>. Since GraphMix consists of various components, some of which are common with the existing literature of semi-supervised learning, we set out to study the effect of various components by systematically removing or adding a component from GraphMix . We measure the effect of the following:</p><p>• Removing the Manifold Mixup and predicted targets from the FCN training.</p><p>• Removing the predicted targets from the FCN training.</p><p>• Removing the Manifold Mixup from the FCN training.</p><p>• Removing the Sharpening of the predicted targets.</p><p>• Removing the Average of predictions for K random perturbations of the input sample</p><p>• Using the EMA <ref type="bibr" target="#b36">(Tarvainen &amp; Valpola, 2017)</ref> of GNN for target prediction.</p><p>The ablation results for semi-supervised node classification are presented in Table <ref type="table" target="#tab_3">3</ref>. We did not do any hyperparameter tuning for the ablation study and used the best performing hyperparameters found for the results presented in Table <ref type="table" target="#tab_1">1</ref>. We observe that all the components of GraphMix contribute to its performance, with Manifold Mixup training of FCN contributing possibly the most. Furthermore, we observe that using the EMA model (which is an emsemble model) <ref type="bibr" target="#b36">(Tarvainen &amp; Valpola, 2017)</ref> for computing the predicted-targets could improve the performance of GraphMix for all the datasets. Under review as a conference paper at ICLR 2020</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">VISUALIZATION OF THE LEARNED FEATURES</head><p>In this section, we present the analysis of the features learned by GraphMix for Cora dataset. Specifically, we present the 2D visualization of the hidden states using the t-SNE (van der <ref type="bibr">Maaten &amp; Hinton, 2008)</ref> in Figure <ref type="figure">2a</ref>, 2b and 2c . We observe that GraphMix learns hidden states which are better separated and condensed than GCN and GCN(predicted-targets). We further evaluate the Softrank (refer to Appendix A.7) of the class-specific hidden states to demonstrate that GraphMix(GCN) makes the class-specific hidden states more concentrated than GCN and GCN(predicted-targets), as shown in 2d. Refer to Appendix A.8 for 2D representation of other datasets. 5 RELATED WORK</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">SEMI-SUPERVISED LEARNING OVER GRAPH DATA</head><p>There exists a long line of work for Semi-supervised learning over Graph Data. Earlier work included using Graph Laplacian Regularizer for enforcing local smoothness over the predicted targets for the nodes <ref type="bibr">(Zhu &amp; Ghahramani, 2002;</ref><ref type="bibr">Zhu et al., 2003;</ref><ref type="bibr" target="#b1">Belkin et al., 2006)</ref>. Another line of work learns node embedding in an unsupervised way <ref type="bibr" target="#b32">(Perozzi et al., 2014)</ref> which can then be used as an input to any classifier, or learns the node embedding and target prediction jointly <ref type="bibr" target="#b45">(Yang et al., 2016)</ref>. Many of the recent Graph Neural Network based approaches (refer to <ref type="bibr">Zhou et al. (2018)</ref> for a review of these methods) are inspired by the success of Convolutional Neural Networks in image and text domains, defines the convolutional operators using the neighbourhood information of the nodes <ref type="bibr" target="#b21">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b39">Veličković et al., 2018;</ref><ref type="bibr" target="#b8">Defferrard et al., 2016)</ref>. These convolution operator based method exhibit state-of-the-results for semi-supervised learning over graph data, hence much of the recent attention is dedicated to proposing architectural changes to these methods <ref type="bibr" target="#b33">(Qu et al., 2019;</ref><ref type="bibr" target="#b13">Gao &amp; Ji, 2019;</ref><ref type="bibr" target="#b28">Ma et al., 2019)</ref>. Unlike these methods, we propose a regularization technique that can be applied to any of these Graph Neural Networks which uses a parameterized transformation on the node features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">DATA AUGMENTATION</head><p>It is well known that the generalization of a learning algorithm can be improved by enlarging the training data size. Because labeling more samples is labour-intensive and costly, Data-augmentation has become de facto technique for enlarging the training data size, especially in the computer vision applications such as image classification. Some of the notable Data Augmentation techniques include Cutout <ref type="bibr" target="#b10">(Devries &amp; Taylor, 2017)</ref> and DropBlock <ref type="bibr" target="#b14">(Ghiasi et al., 2018)</ref>. In Cutout, a contiguous part of the input is zeroed out. DropBlock further extends Cutout to the hidden states. In another line of research, such as Mixup and BC-learning <ref type="bibr" target="#b46">(Zhang et al., 2018;</ref><ref type="bibr" target="#b38">Tokozume et al., 2017)</ref>, additional training samples are generated by interpolating the samples and their corresponding targets. Manifold Mixup <ref type="bibr" target="#b41">(Verma et al., 2019a)</ref> proposes to augment the data in the hidden states and shows that it learns more discriminative features for supervised learning. Furthermore, ICT <ref type="bibr" target="#b42">(Verma et al., 2019b)</ref> and MixMatch <ref type="bibr">(Berthelot et al., 2019)</ref> extend the Mixup technique to semi-supervised learning, by computing the predicted targets for the unlabeled data and applying the Mixup on the unlabeled data and their corresponding predicted targets. Even further, for unsupervised learning, ACAI <ref type="bibr">(Berthelot* et al., 2019)</ref> and AMR <ref type="bibr" target="#b0">(Beckham et al., 2019)</ref> explore the interpolation techniques for autoencoders. ACAI interpolates the hidden states of an autoencoder and uses a critic network to constrain the reconstruction of these interpolated states to be realistic. AMR explores different ways of combining the hidden states of an autoencoder other than the convex combinations of the hidden states. Unlike, all of these techniques which have been proposed for the fixed topology datasets, in this work, we propose interpolation based data-augmentation techniques for graph structured data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">REGULARIZING GRAPH NEURAL NETWORKS</head><p>Regularizing Graph Neural Networks has drawn some attention recently. GraphSGAN <ref type="bibr" target="#b11">(Ding et al., 2018)</ref> first uses an embedding method such as DeepWalk <ref type="bibr" target="#b32">(Perozzi et al., 2014)</ref> and then trains generator-classifier networks in the adversarial learning setting to generate fake samples in the low-density region between sub-graphs. BVAT <ref type="bibr" target="#b9">(Deng et al., 2019)</ref> and <ref type="bibr" target="#b12">Feng et al. (2019)</ref> generate adversarial perturbations to the features of the graph nodes while taking graph structure into account. While these methods improve generalization in graph-structured data, they introduce significant additional computation cost: GraphScan requires computing node embedding as a preprocessing step, <ref type="bibr">BVAT and Feng et al. (2019)</ref> require additional gradient computation for computing adversarial perturbations. Unlike these methods, GraphMix does not introduce any significant additional computation since it is based on interpolation-based techniques and self-generated predicted targets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head><p>We presented GraphMix , a simple and efficient regularized training scheme for graph neural networks. GraphMix is a general scheme that can be applied to any graph neural network that uses a parameterized transformation on the feature vector of the graph nodes. Through extensive experiments, we demonstrated state-of-the-art performances or close to state-of-the-art performance using this simple regularization technique on various benchmark datasets, more importantly, GraphMix improves test accuracy over vanilla GNN across all the datasets, even without doing any extensive hyperparameter search. Further, we conduct a systematic ablation study to understand the effect of different components in the performance of GraphMix . This suggests that in parallel to designing new architectures, exploring better regularization for graph structured data is a promising avenue for research.</p><p>Jie Zhou, Ganqu Cui, Zhengyan Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. Semi-supervised learning using gaussian fields and harmonic functions. In ICML, 2003.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>A.1 DATASETS</p><p>The statistics of these datasets as well as the number of training/validation/test nodes is presented in Table <ref type="table" target="#tab_5">4</ref>. We present the comparion of GraphMix with the recent state-of-the-art methods as well as earlier methods using the standard Train/Validation/Test split in Table <ref type="table">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 RESULTS WITH FEWER LABELED SAMPLES</head><p>We further evaluate the effectiveness of GraphMix in the learning regimes where fewer labeled samples exist. For each class, we randomly sampled K ∈ {5, 10} samples for training and the same number of samples for the validation. We used all the remaining labeled samples as the test set. We repeated this process for 10 times. The results in Table <ref type="table">6</ref> show that GraphMix achieves even better improvements when the labeled samples are fewer ( Refer to Table <ref type="table" target="#tab_1">1</ref> for results with 20 training samples per class).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 RESULTS ON LARGER DATASETS</head><p>In this section, we provide results on three recently proposed datasets which are relatively larger than standard benchmark datasets (Cora/Citeseer/Pubmed). Specifically, we use Cora-Full dataset proposed in <ref type="bibr">Bojchevski &amp; Günnemann (2018)</ref> and Coauthor-CS and Coauthor-Physics datasets proposed in <ref type="bibr" target="#b35">Shchur et al. (2018)</ref>. We took processed versions of these dataset available here<ref type="foot" target="#foot_1">2</ref> . We did 10 random splits of the the data into train/validation/test split. For the classes which had more than 100 samples, we choose 20 samples per class for training, 30 samples per class for validation and the remaining samples as test data. For the classes which had less than 100 samples, we chose 20% samples, per class for training, 30% samples for validation and the remaining for testing. For each split we run experiments using 100 random seeds. The statistics of these datasets in presented in Table <ref type="table">8</ref> and the results are presented in Table <ref type="table" target="#tab_6">7</ref>. We observe that GraphMix(GCN) improves the results over GCN for all the three datasets. We note that we did minimal hyperparameter search for GraphMix(GCN) as mentioned in Section A.8.1, and doing more rigorous hyperparameter search can further improve the performance of GraphMix . of FCN, the temparature T in sharpening and the number of random perturbations K applied to the input data for the averaging of the predictions.</p><p>We conducted minimal hyperparameter seach over only α and γ max and fixed the hyperparameters T and K to 0.1 and 10 respectively. The other hyperparameters were set to the best values for underlying GNN (GCN or GAT), including the learning rate, the L2 decay rate, number of units in the hidden layer etc. We observed that GraphMix is not very sensitive to the values of α and γ max and similar values of these hyperparameters work well across all the benchmark datasets. Refer to Appendix A.5 and A.6 for the details about the hyperparameter values and the procedure used for the best hyperparameters selection.</p><p>A.5.1 FOR RESULTS REPORTED IN SECTION 4.2</p><p>For GCN and GraphMix(GCN), we used Adam optimizer with learning rate 0.01 and L2-decay 5e-4, the number of units in the hidden layer 16 , dropout rate in the input layer and hidden layer was set to 0.5 and 0.0, respectively. For GAT and GraphMix(GAT), we used Adam optimizer with learning rate 0.005 and L2-decay 5e-4, the number of units in the hidden layer 8 , and the dropout rate in the input layer and hidden layer was searched from the values {0.2, 0.5, 0.8}.</p><p>For α and γ max of GraphMix(GCN) and GraphMix(GAT) , we searched over the values in the set [0.0, 0.1, 1.0, 2.0] and [0.1, 1.0, 10.0, 20.0] respectively.</p><p>For GraphMix(GCN) : α = 1.0 works best across all the datasets. γ max = 1.0 works best for Cora and Citeseer and γ max = 10.0 works best for Pubmed.</p><p>For GraphMix(GAT) : α = 1.0 works best for Cora and Citeseer and α = 0.1 works best for Pubmed. γ max = 1.0 works best for Cora and Citeseer and γ max = 10.0 works best for Pubmed. Input droputrate=0.5 and hidden dropout rate=0.5 work best for Cora and Citeseer and Input dropout rate=0.2 and hidden dropout rate =0.2 work best for Pubmed.</p><p>We conducted all the experiments for 2000 epochs. The value of consistency coefficient γ (line 13 in Algorithm 1) is increased from 0 to its maximum value γ max from epoch 500 to 1000 using the sigmoid ramp-up of Mean-Teacher <ref type="bibr" target="#b36">(Tarvainen &amp; Valpola, 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5.2 FOR RESULTS REPORTED IN SECTION A.3</head><p>For α of GraphMix(GCN) , we searched over the values in the set [0.0, 0.1, 0.5, 1.0] and found that 0.1 works best across all the datasets. For γ max , we searched over the values in the set [0.1, 1.0, 10.0] and found that 0.1 and 1.0 works best across all the datasets. Rest of the details for GraphMix(GCN) and GCN are same as Section A.5.1.</p><p>A.5.3 FOR RESULTS REPORTED IN SECTION 4.3</p><p>For α of GraphMix(GCN) , we searched over the values in the set [0.0, 0.1, 0.5, 1.0] and found that 0.1 works best for both the datasets. For γ max , we searched over the values in the set [0.1, 1.0, 10.0] and found that 0.1 works best for both the datasets. We conducted all the experiments for 150 epochs.</p><p>The value of consistency coefficient γ (line 13 in Algorithm 1) is increased from 0 to its maximum value γ max from epoch 75 to 125 using the sigmoid ramp-up of Mean-Teacher <ref type="bibr" target="#b36">(Tarvainen &amp; Valpola, 2017)</ref>.</p><p>Both for GraphMix(GCN) and GCN, we use Adam optimizer with learning rate 0.01 and L2-decay 0.0, the number of units in the hidden layer 128 , dropout rate in the input layer was set to 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 HYPERPARAMETER SELECTION</head><p>For each configuration of hyperparameters, we run the experiments with 100 random seeds. We select the hyperparameter configuration which has the best validation accuracy averaged over these 100 trials. With this best hyperparameter configuration, for 100 random seeds, we train the model again and use the validataion set for model selection ( i.e. we report the test accuracy at the epoch which has best validation accuracy.)</p><p>A.7 SOFT-RANK Let H be a matrix containing the hidden states of all the samples from a particular class. The Soft-Rank of matrix H is defined by the sum of the singular values of the matrix divided by the largest singular value. A lower Soft-Rank implies fewer dimensions with substantial variability and it provides a continuous analogue to the notion of rank from matrix algebra. This provides evidence that the concentration of class-specific states observed when using GraphMix in Figure <ref type="figure" target="#fig_2">3</ref> can be measured directly from the hidden states and is not an artifact of the T-SNE visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8 FEATURE VISUALIZATION</head><p>We present the 2D visualization of the hidden states learned using GCN and GraphMix(GCN) for Cora, Pubmed and Citeseer datasets in Figure <ref type="figure" target="#fig_2">3</ref>. We observe that for Cora and Citeseer, GraphMix learns substantially better hidden states than GCN. For Pubmed, we observe that although there is no clear separation between classes, "Green" and "Red" classes overlap less using the GraphMix, resulting in better hidden states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8.1 HYPERPARAMETER DETAILS FOR RESULTS IN TABLE 7</head><p>For all the experiments we use the standard architecture mentioned in Section A.5 and used Adam optimizer with learning rate 0.001 and 64 hidden units in the hidden layer. For Coauthor-CS and Coauthor-Physics, we trained the network for 2000 epochs. For Cora-Full, we trained the network for 5000 epochs because we observed the training loss of Cora-Full dataset takes longer to converge.</p><p>For Coauthor-CS and Coauthor-Physics: We set the input layer dropout rate to 0.5 and weight-decay to 0.0005, both for GCN and GraphMix(GCN) . We did not conduct any hyperparameter search over the GraphMix hyperparameters α, λ max , temparature T and number of random permutations K applied to the input data for GraphMix(GCN) for these two datasets, and set these values to 1.0, 1.0, 0.1 and 10 respectively.</p><p>For Cora-Full dataset: We found input layer dropout rate 0.2 and weight-decay 0.0 to be the best for both GCN and GraphMix(GCN) . For GraphMix(GCN) we fixed α, temparature T and number of random permutations K to 1.0 0.1 and 10 respectively. For λ max , we did search over {1.0, 10.0, 20.0} and found that 10.0 works best.</p><p>For all the GraphMix(GCN) experiments, the value of consistency coefficient γ (line 13 in Algorithm 1) is increased from 0 to its maximum value γ max from epoch 500 to 1000 using the sigmoid ramp-up of Mean-Teacher <ref type="bibr" target="#b36">(Tarvainen &amp; Valpola, 2017)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The procedure for training with GraphMix . The Fully-Connected Network (FCN) and the Graph Neural Network (GNN) share linear transformation matrix (W ) applied on the node features. The FCN is trained using Manifold Mixup by interpolating the hidden states H F CN and the corresponding labels Y . This leads to better features which are transferred to the GNN via parameter sharing. The predicted targets generated by the GNN for unlabeled data are used to augment the input data for the FCN. The FCN and the GNN losses are minimized jointly by alternate minimization.</figDesc><graphic url="image-1.png" coords="3,157.50,81.86,297.00,181.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2: 2D representation of the hidden states of Citeseer dataset using (a) GCN, (b) GCN(predictedtargets), (c) GraphMix, and (d) Soft-Rank of Class-specific hidden states (lower Soft-Rank reflects more concentrated class-specific hidden states)</figDesc><graphic url="image-2.png" coords="8,112.92,204.37,95.04,71.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: T-SNE of hidden states for Cora (left), Pubmed (middle), and Citeseer (right). Top row is GCN baseline, bottom row is GraphMix.</figDesc><graphic url="image-9.png" coords="17,105.51,168.73,146.14,109.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The Manifold Mixup training of FCN facilitates learning more discriminative node representations. How to transfer these discriminative node representations to the GNN? We apply parameter sharing between FCN and GNN to facilitate this. Using these more discriminative representations of the nodes, as well as the graph structure, GNN loss is computed in the usual way. Input: A GCN: g(X, A, θ), a FCN: f (X, θ, λ) which shares parameters with the GCN. Beta distribution parameter α for Manifold Mixup . Number of random perturbations K, Sharpening temperature T . Consistency parameter γ. Number of epochs N . γ(t): rampup function for increasing the importance of consistency regularization. (XL, YL) represents labeled samples and XU represents unlabeled samples.</figDesc><table><row><cell cols="4">Algorithm 1 GraphMix : A procedure for improved training of Graph Neural Networks (GNN)</cell></row><row><cell cols="2">2: for t = 1 to N do</cell><cell></cell></row><row><cell>3:</cell><cell cols="3">i = random(0,1) // generate randomly 0 or 1</cell></row><row><cell>4:</cell><cell>if i=0 then</cell><cell></cell></row><row><cell>5:</cell><cell cols="3">λ ∼ Beta(α, α) // Sample a mixing coefficient from Beta distribution</cell></row><row><cell>6:</cell><cell>Lsup = L f (XL, θ, λ), YL</cell><cell></cell><cell>// supervised loss from FCN using the Manifold Mixup</cell></row><row><cell>7: 8:</cell><cell cols="3">for k = 1 to K do XU,k = RandomP erturbations(XU ) // Apply k th round of random perturbation to XU</cell></row><row><cell>9: 10: 11:</cell><cell cols="3">end for ȲU = 1 K using the GCN k g(Y | XU,k ; θ, A) // Compute average predictions across K perturbations of XU YU = Sharpen( ȲU , T ) // Apply temperature sharpening to the average prediction</cell></row><row><cell>12:</cell><cell cols="2">Lusup = L f (XU , θ, λ), YU</cell><cell>// unsupervised loss from FCN using the Manifold Mixup</cell></row><row><cell>13:</cell><cell cols="3">L = Lsup + γ(t)  *  Lusup // Total loss is the weighted sum of supervised and unsupervised FCN</cell></row><row><cell></cell><cell>loss</cell><cell></cell></row><row><cell>14:</cell><cell>else</cell><cell></cell></row><row><cell>15:</cell><cell>L = L g(XL, θ, A), YL</cell><cell cols="2">// Loss using the vanilla GCN</cell></row><row><cell>16:</cell><cell>end if</cell><cell></cell></row><row><cell cols="2">17: end for</cell><cell></cell></row><row><cell cols="2">18: return L</cell><cell></cell></row></table><note>1:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results of node classification (% test accuracy) using 10 random Train/Validation/Test split of datasets. [*] means the results are taken from the corresponding papers. We conduct 100 trials and report mean and standard deviation over the trials (refer to Table5in the Appendix for comparison with other methods on standard Train/Validation/Test split).</figDesc><table><row><cell>Algorithm</cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell></row><row><cell>GCN</cell><cell cols="3">77.84±1.45 72.56±2.46 78.74±0.99</cell></row><row><cell>GAT</cell><cell cols="3">77.74±1.86 70.41±1.81 78.48±0.96</cell></row><row><cell>Graph U-Net</cell><cell cols="3">77.59±1.60 67.55±0.69 76.79±2.45</cell></row><row><cell cols="4">GCN(with predicted-targets) 80.41±1.78 73.62±2.11 79.81±2.85</cell></row><row><cell cols="4">FCN(with predicted-targets) 75.19±3.53 70.49±1.91 73.40±2.48</cell></row><row><cell>GraphMix (GCN)</cell><cell cols="3">82.07±1.17 76.45±1.57 80.72±1.08</cell></row><row><cell>GraphMix (GAT)</cell><cell cols="3">80.63±1.31 74.08±1.26 80.14±1.51</cell></row><row><cell>GraphMix (Graph-U-Net)</cell><cell cols="3">80.18±1.62 72.85±1.71 78.47±0.64</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on Link Classification (%F1 score). [*] means the results are taken from the corresponding papers</figDesc><table><row><cell>Algorithm</cell><cell>Bit OTC</cell><cell>Bit Alpha</cell></row><row><cell>DeepWalk (Perozzi et al., 2014)</cell><cell>63.20</cell><cell>62.71</cell></row><row><cell>GMNN*(Qu et al., 2019)</cell><cell>66.93</cell><cell>65.86</cell></row><row><cell>GCN</cell><cell cols="2">65.72±0.38 64.00±0.19</cell></row><row><cell>GCN(with predicted-targets)</cell><cell cols="2">65.15±0.29 64.56±0.21</cell></row><row><cell>FCN(with predicted-targets)</cell><cell cols="2">60.13±0.40 59.74±0.32</cell></row><row><cell>GraphMix (GCN)</cell><cell cols="2">66.35±0.41 65.34±0.19</cell></row><row><cell>4.4 ABLATION STUDY</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation study results using 10 labeled samples per class (% test accuracy). We report mean and standard deviation over ten trials.</figDesc><table><row><cell>Ablation</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Zhang, Cheng Yang, Zhiyuan Liu, and Maosong Sun. Graph neural networks: A review of methods and applications. CoRR, abs/1812.08434, 2018. URL http://arxiv.org/abs/1812.08434. Xiaojin Zhu and Zoubin Ghahramani. Learning from labeled and unlabeled data with label propagation. Technical report, 2002.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Dataset statistics.</figDesc><table><row><cell>Dataset</cell><cell cols="7"># Nodes # Edges # Features # Classes # Training # Validation # Test</cell></row><row><cell>Cora</cell><cell>2,708</cell><cell>5,429</cell><cell>1,433</cell><cell>7</cell><cell>140</cell><cell>500</cell><cell>1,000</cell></row><row><cell>Citeseer</cell><cell>3,327</cell><cell>4,732</cell><cell>3,703</cell><cell>6</cell><cell>120</cell><cell>500</cell><cell>1,000</cell></row><row><cell>Pubmed</cell><cell>19,717</cell><cell>44,338</cell><cell>500</cell><cell>3</cell><cell>60</cell><cell>500</cell><cell>1,000</cell></row><row><cell>Bitcoin Alpha</cell><cell>3,783</cell><cell>24,186</cell><cell>3,783</cell><cell>2</cell><cell>100</cell><cell>500</cell><cell>3,221</cell></row><row><cell>Bitcoin OTC</cell><cell>5,881</cell><cell>35,592</cell><cell>5,881</cell><cell>2</cell><cell>100</cell><cell>500</cell><cell>5,947</cell></row><row><cell cols="5">A.2 COMPARISON WITH STATE-OF-THE-ART METHODS</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Comparison of GraphMix with other methods (% test accuracy ), for Cora-Full, Coauthor-CS, Coauthor-Physics, and NELL. * refers to the results reported in<ref type="bibr" target="#b35">Shchur et al. (2018)</ref>.</figDesc><table><row><cell>Method</cell><cell>Cora-Full</cell><cell cols="4">Coauthor-CS Coauthor-Physics NELL</cell></row><row><cell>GCN*</cell><cell>62.2±0.6</cell><cell cols="2">91.1±0.5</cell><cell cols="2">92.8±1.0</cell><cell>-</cell></row><row><cell>GAT*</cell><cell>51.9±1.5</cell><cell cols="2">90.5±0.6</cell><cell cols="2">92.5±0.9</cell><cell>-</cell></row><row><cell>MoNet*</cell><cell>59.8±0.8</cell><cell cols="2">90.8±0.6</cell><cell cols="2">92.5±0.9</cell><cell>-</cell></row><row><cell>GS-Mean*</cell><cell>58.6±1.6</cell><cell cols="2">91.3±2.8</cell><cell cols="2">93.0±0.8</cell><cell>-</cell></row><row><cell cols="2">GCN (Kipf &amp; Welling, 2017) -</cell><cell>-</cell><cell></cell><cell>-</cell><cell>66.0</cell></row><row><cell>GCN</cell><cell cols="3">60.13±0.57 91.27±0.56</cell><cell cols="2">92.90±0.92</cell><cell>63.65±1.17</cell></row><row><cell>GraphMix (GCN)</cell><cell cols="3">61.80±0.54 91.83±0.51</cell><cell cols="2">94.49±0.84</cell><cell>66.32±1.04</cell></row><row><cell></cell><cell cols="3">Table 8: Dataset statistics</cell><cell></cell></row><row><cell>Datasets</cell><cell cols="4">Classes Features Nodes</cell><cell>Edges</cell></row><row><cell>Cora-Full</cell><cell></cell><cell>67</cell><cell cols="2">8710 18703</cell><cell>62421</cell></row><row><cell>Coauthor-CS</cell><cell></cell><cell>15</cell><cell cols="2">6805 18333</cell><cell>81894</cell></row><row><cell cols="2">Coauthor-Physics</cell><cell>5</cell><cell cols="3">8415 34493 247962</cell></row><row><cell>NELL</cell><cell cols="2">210</cell><cell cols="3">5414 65755 266144</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">code available at https://github.com/anon777000/GraphMix</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://github.com/shchur/gnn-benchmark</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr" target="#b1">(Belkin et al., 2006)</ref> <p>59.5% 60.1% 70.7% SemiEmb <ref type="bibr" target="#b43">(Weston et al., 2012)</ref> 59.0% 59.6% 71.7% LP <ref type="bibr">(Zhu et al., 2003)</ref> 68.0% 45.3% 63.0% DeepWalk <ref type="bibr" target="#b32">(Perozzi et al., 2014)</ref> 67.2% 43.2% 65.3% ICA <ref type="bibr" target="#b27">(Lu &amp; Getoor, 2003)</ref> 75.1% 69.1% 73.9% Planetoid <ref type="bibr" target="#b45">(Yang et al., 2016)</ref> 75.7% 64.7% 77.2% Chebyshev <ref type="bibr" target="#b8">(Defferrard et al., 2016)</ref>   <ref type="bibr" target="#b30">(Monti et al., 2016)</ref> 81.7 ± 0.5% -78.8 ± 0.3% GAT <ref type="bibr" target="#b39">(Veličković et al., 2018)</ref> 83.0 ± 0.7% 72.5 ± 0.7% 79.0 ± 0.3% GraphScan <ref type="bibr" target="#b11">(Ding et al., 2018)</ref> 83.3 ±1.3 73.1±1.8 -GMNN <ref type="bibr" target="#b33">(Qu et al., 2019)</ref> 83.7% 73.1% 81.8% DisenGCN <ref type="bibr" target="#b28">(Ma et al., 2019)</ref> 83.7% 73.4% 80.5% Graph U-Net <ref type="bibr" target="#b13">(Gao &amp; Ji, 2019)</ref> 84.4% 73.2% 79.6% BVAT <ref type="bibr" target="#b9">(Deng et al., 2019)</ref> 83 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 IMPLEMENTATION AND HYPERPARAMETER DETAILS</head><p>We use the standard benchmark architecture as used in <ref type="bibr">GCN (Kipf &amp; Welling, 2017)</ref>, GAT <ref type="bibr" target="#b39">(Veličković et al., 2018)</ref> and GMNN <ref type="bibr" target="#b33">(Qu et al., 2019)</ref>, among others. This architecture has one hidden layer and the graph convolution is applied twice : on the input layer and on the output of the hidden layer. The FCN in GraphMix shares the parameters with the GCN.</p><p>GraphMix introduces four additional hyperparameters, namely the α parameter of Beta distribution used in Manifold Mixup training of the FCN, the max-consistency coefficient γ max which controls the trade-off between the supervised loss and the unsupervised loss (loss computed using the pseudolables)</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sina</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farnoosh</forename><surname>Ghadiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02709</idno>
		<imprint>
			<date type="published" when="2019-03">Mar 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">On Adversarial Mixup Resynthesis. arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Manifold regularization: A geometric framework for learning from labeled and unlabeled examples</title>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Sindhwani</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=1248547.1248632" />
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<idno type="ISSN">1532-4435</idno>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2399" to="2434" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02249</idno>
		<title level="m">MixMatch: A Holistic Approach to Semi-Supervised Learning. arXiv e-prints, art</title>
				<imprint>
			<date type="published" when="2019-05">May 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Understanding and improving interpolation in autoencoders via an adversarial regularizer</title>
		<author>
			<persName><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=S1fQSiCcYm" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
		<idno type="DOI">10.1145/279943.279962</idno>
		<ptr target="http://doi.acm.org/10.1145/279943.279962" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh Annual Conference on Computational Learning Theory, COLT&apos; 98</title>
				<meeting>the Eleventh Annual Conference on Computational Learning Theory, COLT&apos; 98<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="92" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep gaussian embedding of graphs: Unsupervised inductive learning via ranking</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1ZdKJ-0W" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno>CoRR, abs/1312.6203</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Semi-Supervised Learning</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schlkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Zien</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>The MIT Press</publisher>
			<biblScope unit="page">9780262514125</biblScope>
		</imprint>
	</monogr>
	<note>1st edition</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Batch virtual adversarial training for graph convolutional networks</title>
		<author>
			<persName><forename type="first">Zhijie</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinpeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<idno>CoRR, abs/1902.09192</idno>
		<ptr target="http://arxiv.org/abs/1902.09192" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno>CoRR, abs/1708.04552</idno>
		<ptr target="http://arxiv.org/abs/1708.04552" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-supervised learning on graphs with generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3269206.3271768</idno>
		<ptr target="http://doi.acm.org/10.1145/3269206.3271768" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Information and Knowledge Management, CIKM &apos;18</title>
				<meeting>the 27th ACM International Conference on Information and Knowledge Management, CIKM &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="913" to="922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Graph adversarial training: Dynamically regularizing based on graph structure</title>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<idno>CoRR, abs/1902.08226</idno>
		<ptr target="http://arxiv.org/abs/1902.08226" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Graph u-nets</title>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/gao19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
				<editor>
			<persName><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-15">09-15 Jun 2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="2083" to="2092" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dropblock: A regularization method for convolutional networks</title>
		<author>
			<persName><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/8271-dropblock-a-regularization-method-for-convolutional-networks.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="10727" to="10737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scarselli</forename><surname>Franco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Joint Conference on Neural Networks</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/2740-semi-supervised-learning-by-entropy-minimization.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="529" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deep convolutional networks on graph-structured data</title>
		<author>
			<persName><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno>ArXiv, abs/1506.05163</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Audio augmentation for speech recognition</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijayaditya</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Edge weight prediction in weighted signed networks</title>
		<author>
			<persName><forename type="first">Srijan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesca</forename><surname>Spezzano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Vs Subrahmanian</surname></persName>
		</author>
		<author>
			<persName><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rev2: Fraudulent user prediction in rating platforms</title>
		<author>
			<persName><forename type="first">Srijan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Hooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Disha</forename><surname>Makhija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><surname>Vs Subrahmanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno>CoRR, abs/1610.02242</idno>
		<ptr target="http://arxiv.org/abs/1610.02242" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Pseudo-label : The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Link-based classification</title>
		<author>
			<persName><forename type="first">Qing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=3041838.3041901" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth International Conference on International Conference on Machine Learning, ICML&apos;03</title>
				<meeting>the Twentieth International Conference on International Conference on Machine Learning, ICML&apos;03</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="496" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Disentangled graph convolutional networks</title>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Shin Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno>CoRR, abs/1611.08402</idno>
		<ptr target="http://arxiv.org/abs/1611.08402" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekin</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Specaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08779</idno>
		<title level="m">Simple Data Augmentation Method for Automatic Speech Recognition. arXiv e-prints, art</title>
				<imprint>
			<date type="published" when="2019-04">Apr 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">GMNN: Graph Markov neural networks</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
				<editor>
			<persName><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019-06-15">09-15 Jun 2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="5241" to="5250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename><surname>Ah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><surname>Monfardini</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNN.2008.2005605</idno>
		<ptr target="http://dx.doi.org/10.1109/TNN.2008.2005605" />
	</analytic>
	<monogr>
		<title level="j">Trans. Neur. Netw</title>
		<idno type="ISSN">1045-9227</idno>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009-01">January 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Pitfalls of graph neural network evaluation</title>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<idno>CoRR, abs/1811.05868</idno>
		<ptr target="http://arxiv.org/abs/1811.05868" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Link prediction in relational data</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Fai</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">Yuji</forename><surname>Tokozume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
		<ptr target="http://www.jmlr.org/papers/v9/vandermaaten08a.html" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2008">2018. 2017. 2008</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
		</imprint>
	</monogr>
	<note>Between-class learning for image classification</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Deep graph infomax</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Hjelm</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Manifold mixup: Better representations by interpolating hidden states</title>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/verma19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
				<editor>
			<persName><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06">Jun 2019a</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Interpolation consistency training for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kannala</forename><surname>Juho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2019</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2019" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019</title>
				<editor>
			<persName><forename type="first">Sarit</forename><surname>Kraus</surname></persName>
		</editor>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019<address><addrLine>Macao, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">August 10-16, 2019. ijcai.org, 2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep learning via semisupervised embedding</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frédéric</forename><surname>Ratle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
				<editor>
			<persName><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Geneviève</forename><surname>Orr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Müller</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Ziang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sida</forename><forename type="middle">I</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Lévy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aiming</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<idno>ArXiv, abs/1703.02573</idno>
		<title level="m">Data noising as smoothing in neural network language models</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1Ddp1-Rb" />
		<title level="m">mixup: Beyond empirical risk minimization. International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
