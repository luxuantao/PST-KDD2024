<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">XLM-E: Cross-lingual Language Model Pre-training via ELECTRA</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-06-30">30 Jun 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zewen</forename><surname>Chi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shuming</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Saksham</forename><surname>Singhal</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Payal</forename><surname>Bajaj</surname></persName>
						</author>
						<author>
							<persName><roleName>Furu</roleName><forename type="first">Xia</forename><surname>Song</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><forename type="middle">Microsoft</forename><surname>Corporation</surname></persName>
						</author>
						<title level="a" type="main">XLM-E: Cross-lingual Language Model Pre-training via ELECTRA</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-06-30">30 Jun 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2106.16138v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we introduce ELECTRA-style tasks <ref type="bibr" target="#b10">(Clark et al., 2020b)</ref> to cross-lingual language model pre-training. Specifically, we present two pre-training tasks, namely multilingual replaced token detection, and translation replaced token detection. Besides, we pretrain the model, named as XLM-E, on both multilingual and parallel corpora. Our model outperforms the baseline models on various cross-lingual understanding tasks with much less computation cost. Moreover, analysis shows that XLM-E tends to obtain better cross-lingual transferability.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>It has become a de facto trend to use a pretrained language model <ref type="bibr" target="#b14">(Devlin et al., 2019;</ref><ref type="bibr" target="#b15">Dong et al., 2019;</ref><ref type="bibr" target="#b45">Yang et al., 2019b;</ref><ref type="bibr" target="#b2">Bao et al., 2020)</ref> for downstream NLP tasks. These models are typically pretrained with masked language modeling objectives, which learn to generate the masked tokens of an input sentence. In addition to monolingual representations, the masked language modeling task is effective for learning cross-lingual representations. By only using multilingual corpora, such pretrained models perform well on zero-shot cross-lingual transfer <ref type="bibr" target="#b14">(Devlin et al., 2019;</ref><ref type="bibr" target="#b11">Conneau et al., 2020)</ref>, i.e., fine-tuning with English training data while directly applying the model to other target languages. The cross-lingual transferability can be further improved by introducing external pre-training tasks using parallel corpus, such as translation language modeling <ref type="bibr" target="#b12">(Conneau and Lample, 2019)</ref>, and crosslingual contrast <ref type="bibr" target="#b6">(Chi et al., 2021b)</ref>. However, previous cross-lingual pre-training based on masked language modeling usually requires massive computation resources, rendering such models quite expensive. As shown in Figure <ref type="figure">1</ref>, our proposed XLM-E achieves a huge speedup compared with well-tuned pretrained models. * Equal contribution. XLM-E (45K)</p><p>XLM-E (90K)</p><p>XLM-E (125K)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Speedup 130x</head><p>Figure <ref type="figure">1</ref>: The proposed XLM-E pre-training (red line) achieves 130× speedup compared with an in-house pretrained XLM-R augmented with translation language modeling (XLM-R + TLM; blue line), using the same corpora and code base. The training steps are shown in the brackets. We also present XLM-R <ref type="bibr" target="#b11">(Conneau et al., 2020)</ref>, InfoXLM <ref type="bibr" target="#b6">(Chi et al., 2021b)</ref>, and XLM-Align <ref type="bibr" target="#b7">(Chi et al., 2021c)</ref>. The compared models are all in base size.</p><p>In this paper, we introduce ELECTRA-style tasks <ref type="bibr" target="#b10">(Clark et al., 2020b)</ref> to cross-lingual language model pre-training. Specifically, we present two discriminative pre-training tasks, namely multilingual replaced token detection, and translation replaced token detection. Rather than recovering masked tokens, the model learns to distinguish the replaced tokens in the corrupted input sequences. The two tasks build input sequences by replacing tokens in multilingual sentences, and translation pairs, respectively. We also describe the pretraining algorithm of our model, XLM-E, which is pretrained with the above two discriminative tasks. It provides a more compute-efficient and sampleefficient way for cross-lingual language model pretraining.</p><p>We conduct extensive experiments on the XTREME cross-lingual understanding benchmark to evaluate and analyze XLM-E. Over seven datasets, our model achieves competitive results with the baseline models, while only using 1% of the computation cost comparing to XLM-R. In addition to the high computational efficiency, our model also shows the cross-lingual transferability that achieves a reasonably low transfer gap. We also show that the discriminative pre-training encourages universal representations, making the text representations better aligned across different languages.</p><p>Our contributions are summarized as follows:</p><p>• We explore ELECTRA-style tasks for crosslingual language model pre-training, and pretrain XLM-E with both multilingual corpus and parallel data.</p><p>• We demonstrate that XLM-E greatly reduces the computation cost of cross-lingual pretraining.</p><p>• We show that discriminative pre-training tends to encourage better cross-lingual transferability.</p><p>2 Background: ELECTRA ELECTRA <ref type="bibr" target="#b10">(Clark et al., 2020b)</ref> introduces the replaced token detection task for language model pre-training, with the goal of distinguishing real input tokens from corrupted tokens. That means the text encoders are pretrained as discriminators rather than generators, which is different from the previous pretrained language models, such as BERT <ref type="bibr" target="#b14">(Devlin et al., 2019)</ref>, that learn to predict the masked tokens.</p><p>ELECTRA trains two Transformer <ref type="bibr" target="#b40">(Vaswani et al., 2017)</ref> encoders, serving as generator and discriminator, respectively. The generator G is typically a small BERT model trained with the masked language modeling (MLM; <ref type="bibr" target="#b14">Devlin et al. 2019)</ref> task. Consider an input sentence x = {x i } n i=1 containing n tokens. MLM first randomly selects a subset M ⊆ {1, . . . , n} as the positions to be masked, and construct the masked sentence x masked by replacing tokens in M with <ref type="bibr">[MASK]</ref>. Then, the generator predicts the probability distributions of the masked tokens p G (x|x masked ). The loss function of the generator G is:</p><formula xml:id="formula_0">L G (x; θ G ) = − i∈M log p G (x i |x masked ). (1)</formula><p>The discriminator D is trained with the replaced token detection task. Specifically, the discriminator takes the corrupted sentences x corrupt as input, which is constructed by replacing the tokens in M with the tokens sampled from the generator G:</p><formula xml:id="formula_1">x corrupt i ∼ p G (x i |x masked ), i ∈ M x corrupt i = x i , i ∈ M (2)</formula><p>Then, the discriminator predicts whether x corrupt i is original or sampled from the generator. The loss function of the discriminator D is</p><formula xml:id="formula_2">L D (x; θ D ) = − n i=1 log p D (z i |x corrupt )<label>(3)</label></formula><p>where z i represents the label of whether x corrupt i is the original token or the replaced one. The final loss function of ELECTRA is the combined loss of the generator and discriminator losses,</p><formula xml:id="formula_3">L E = L G + λL D .</formula><p>Compared to generative pre-training, XLM-E uses more model parameters and training FLOPs per step, because it contains a generator and a discriminator during pre-training. However, only the discriminator is used for fine-tuning on downstream tasks, so the size of the final checkpoint is similar to BERT-like models in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>Figure <ref type="figure" target="#fig_1">2</ref> shows an overview of the two discriminative tasks used for pre-training XLM-E. Similar to ELECTRA described in Section 2, XLM-E has two Transformer components, i.e., generator and discriminator. The generator predicts the masked tokens given the masked sentence or translation pair, and the discriminator distinguishes whether the tokens are replaced by the generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pre-training Tasks</head><p>There are two pre-training tasks, including multilingual replaced token detection (MRTD), and translation replaced token detection (TRTD).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multilingual Replaced Token Detection</head><p>The multilingual replaced token detection task requires the model to distinguish real input tokens from corrupted multilingual sentences. Both the generator and the discriminator are shared across languages. The vocabulary is also shared for different languages. The task is the same as in monolingual ELECTRA pre-training (Section 2).  difference is that the input texts can be in various languages.</p><p>We use uniform masking to produce the corrupted positions. We also tried span masking <ref type="bibr" target="#b21">(Joshi et al., 2019;</ref><ref type="bibr" target="#b2">Bao et al., 2020)</ref> in our preliminary experiments. The results indicate that span masking significantly weakens the generator's prediction accuracy, which in turn harms pre-training.</p><p>Translation Replaced Token Detection Parallel corpora are easily accessible and proved to be effective for learning cross-lingual language models <ref type="bibr" target="#b12">(Conneau and Lample, 2019;</ref><ref type="bibr" target="#b6">Chi et al., 2021b)</ref>, while it is under-studied how to improve discriminative pre-training with parallel corpora. We introduce the translation replaced token detection task that aims to distinguish real input tokens from translation pairs. Given an input translation pair, the generator predicts the masked tokens in both languages. Consider an input translation pair (e, f ). We construct the input sequence by concatenating the translation pair as a single sentence. The loss function of the generator G is:</p><formula xml:id="formula_4">L G (e, f ; θ G ) = − i∈Me log p G (e i | [e; f ] masked ) − i∈M f log p G (f i | [e; f ] masked )</formula><p>where [; ] is the operator of concatenation, and M e , M f stand for the randomly selected masked positions for e and f , respectively. This loss function is identical to the translation language modeling loss (TLM; Conneau and Lample 2019). The discriminator D learns to distinguish real input tokens from the corrupted translation pair. The corrupted translation pair (e corrupt , f corrupt ) is con-structed by replacing tokens with the tokens sampled from G with the concatenated translation pair as input. Formally, e corrupt is constructed by</p><formula xml:id="formula_5">e corrupt i ∼ p G (e i | [e; f ] masked ), i ∈ M e e corrupt i = e i , i ∈ M e (4)</formula><p>The same operation is also used to construct f corrupt . Then, the loss function of the discriminator D can be written as</p><formula xml:id="formula_6">L D (e, f ; θ D ) = − ne+n f i=1 log p D (r i | [e; f ] corrupt ) (5)</formula><p>where r i represents the label of whether the i-th input token is the original one or the replaced one.</p><p>The final loss function of the translation replaced token detection task is L G + λL D .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pre-training XLM-E</head><p>The XLM-E model is jointly pretrained with the masked language modeling, translation language modeling, multilingual replaced token detection and the translation replaced token detection tasks.</p><p>The overall training objective is to minimize</p><formula xml:id="formula_7">L = L MLM (x; θ G ) + L TLM (e, f ; θ G ) + λL MRTD (x; θ D ) + λL TRTD (e, f ; θ D )</formula><p>over large scale multilingual corpus X = {x} and parallel corpus P = {(e, f )}. We jointly pretrain the generator and the discriminator from scratch. Following <ref type="bibr" target="#b10">Clark et al. (2020b)</ref>, we make the generator smaller to improve the pre-training efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Gated Relative Position Bias</head><p>We propose to use gated relative position bias in the self-attention mechanism. Given input tokens</p><formula xml:id="formula_8">{x i } |x| i=1 , let {h i } |x| i=1</formula><p>denote their hidden states in Transformer. The self-attention outputs { hi } |x| i=1 are computed via:</p><formula xml:id="formula_9">q i , k i , v i = h i W Q , h i W K , h i W V (6) a ij ∝ exp{ q i • k j √ d k + r i−j } (7) hi = |x| j=1 a ij v i (8)</formula><p>where r i−j represents gated relative position bias, each h i is linearly projected to a triple of query, key and value using parameter matrices</p><formula xml:id="formula_10">W Q , W K , W V ∈ R d h ×d k , respectively.</formula><p>Inspired by the gating mechanism of Gated Recurrent Unit (GRU; <ref type="bibr" target="#b8">Cho et al. 2014)</ref>, we compute gated relative position bias r i−j via:</p><formula xml:id="formula_11">g (update) , g (reset) = σ(q i • u), σ(q i • v) ri−j = wg (reset) d i−j r i−j = d i−j + g (update) d i−j + (1 − g (update) )r i−j</formula><p>where d i−j is learnable relative position bias, the vectors u, v ∈ R d k are parameters, σ is a sigmoid function, and w is a learnable value.</p><p>Compared with relative position bias <ref type="bibr" target="#b34">(Parikh et al., 2016;</ref><ref type="bibr" target="#b35">Raffel et al., 2020;</ref><ref type="bibr" target="#b2">Bao et al., 2020)</ref>, the proposed gates take the content into consideration, which adaptively adjusts the relative position bias by conditioning on input tokens. Intuitively, the same distance between two tokens tends to play different roles in different languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>Data We use the CC-100 <ref type="bibr" target="#b11">(Conneau et al., 2020)</ref> dataset for the replaced token detection task. CC-100 contains texts in 100 languages collected from the CommonCrawl dump. We use parallel corpora for the translation replaced token detection task, including translation pairs in 100 languages collected from MultiUN <ref type="bibr" target="#b49">(Ziemski et al., 2016)</ref>, IIT Bombay <ref type="bibr" target="#b24">(Kunchukuttan et al., 2018)</ref>, OPUS (Tiedemann, 2012), WikiMatrix <ref type="bibr" target="#b37">(Schwenk et al., 2019), and</ref><ref type="bibr">CCAligned (El-Kishky et al., 2020)</ref>.</p><p>Following XLM <ref type="bibr" target="#b12">(Conneau and Lample, 2019)</ref>, we sample multilingual sentences to balance the language distribution. Formally, consider the pretraining corpora in N languages with m j examples for the j-th language. The probability of using an example in the j-th language is</p><formula xml:id="formula_12">p j = m α j N k=1 m α k (9)</formula><p>The exponent α controls the distribution such that a lower α increases the probability of sampling examples from a low-resource language. In this paper, we set α = 0.7.</p><p>Model We use a base-size 12-layer Transformer <ref type="bibr" target="#b40">(Vaswani et al., 2017)</ref> as the discriminator, with hidden size of 768, and FFN hidden size of 3, 072. The generator is a 4-layer Transformer using the same hidden size as the discriminator <ref type="bibr" target="#b29">(Meng et al., 2021)</ref>. We use the same vocabulary with XLM-R <ref type="bibr" target="#b11">(Conneau et al., 2020</ref>) that consists of 250K subwords tokenized by Sentence-Piece <ref type="bibr" target="#b23">(Kudo and Richardson, 2018)</ref>.</p><p>Training We jointly pretrain the generator and the discriminator of XLM-E from scratch, using the Adam (Kingma and Ba, 2015) optimizer for 125K training steps. We use dynamic batching of approximately 1M tokens for each pre-training task. We set λ, the weight for the discriminator objective to 50. The whole pre-training procedure takes about 1.7 days on 64 Nvidia A100 GPU cards. See Appendix A for more details of pre-training hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Cross-lingual Understanding</head><p>We evaluate XLM-E on the XTREME <ref type="bibr" target="#b19">(Hu et al., 2020b)</ref> benchmark, which is a multilingual multitask benchmark for evaluating cross-lingual understanding. The XTREME benchmark contains seven cross-lingual understanding tasks, namely part-of-speech tagging on the Universal Dependencies v2.5 <ref type="bibr" target="#b46">(Zeman et al., 2019)</ref>, NER named entity recognition on the Wikiann <ref type="bibr" target="#b33">(Pan et al., 2017;</ref><ref type="bibr" target="#b36">Rahimi et al., 2019)</ref> dataset, cross-lingual natural language inference on XNLI <ref type="bibr" target="#b13">(Conneau et al., 2018)</ref>, cross-lingual paraphrase adversaries from word scrambling (PAWS-X; <ref type="bibr" target="#b44">Yang et al. 2019a)</ref>, and cross-lingual question answering on MLQA <ref type="bibr" target="#b25">(Lewis et al., 2020)</ref>, XQuAD <ref type="bibr" target="#b0">(Artetxe et al., 2020)</ref>, and TyDiQA-GoldP <ref type="bibr" target="#b9">(Clark et al., 2020a)</ref>.</p><p>Baselines We compare our XLM-E model with the cross-lingual language models pretrained with multilingual text, i.e., Multilingual BERT  <ref type="bibr" target="#b6">(Chi et al., 2021b)</ref>, and XLM-ALIGN <ref type="bibr" target="#b7">(Chi et al., 2021c)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We use the cross-lingual transfer setting for the evaluation on XTREME <ref type="bibr" target="#b19">(Hu et al., 2020b)</ref>, where the models are first fine-tuned with the English training data and then evaluated on the target languages. In Table <ref type="table" target="#tab_1">1</ref>, we report the accuracy, F1, or Exact-Match (EM) scores on the XTREME cross-lingual understanding tasks. The results are averaged over all target languages and five runs with different random seeds. We divide the pretrained models into two categories, i.e., the models pretrained on multilingual corpora, and the models pretrained on both multilingual corpora and parallel corpora. For the first setting, we pretrain XLM-E with only the multilingual replaced token detection task. From the results, it can be observed that XLM-E outperforms previous models on both settings, achieving the averaged scores of 67.6 and 69.3, respectively. Compared to XLM-R base , XLM-E (w/o TRTD) produces an absolute 1.2 improvement on average over the seven tasks.</p><p>For the second setting, compared to XLM-ALIGN, XLM-E produces an absolute 0.4 improvement on average. XLM-E performs better on the question answering tasks and sentence classification tasks while preserving reasonable high F1 scores on structured prediction tasks. Despite the effectiveness of XLM-E, our model requires substan-  tially lower computation cost than XLM-R base and XLM-ALIGN. A detailed efficiency analysis in presented in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training Efficiency</head><p>We present a comparison of the pre-training resources, to explore whether XLM-E provides a more compute-efficient and sample-efficient way for pre-training cross-lingual language models. Table 2 compares the XTREME average score, the number of parameters, and the pre-training computation cost. Notice that INFOXLM base and XLM-ALIGN are continue-trained from XLM-R base , so the total training FLOPs are accumulated over XLM-R base .</p><p>Table <ref type="table" target="#tab_3">2</ref> shows that XLM-E substantially reduces the computation cost for cross-lingual language model pre-training. Compared to XLM-R base and XLM-ALIGN that use at least 9.6e21 training FLOPs, XLM-E only uses 9.5e19 training FLOPs in total while even achieving better XTREME performance than the two baseline models. For the setting of pre-training with only mul- Table <ref type="table">3</ref>: Average accuracy@1 scores for Tatoeba crosslingual sentence retrieval. The models are evaluated under two settings with 14 and 36 of the parallel corpora for evaluation, respectively.</p><p>tilingual corpora, XLM-E (w/o TRTD) also outperforms XLM-R base using 6.3e19 FLOPs in total. This demonstrates the compute-effectiveness of XLM-E, i.e., XLM-E as a stronger cross-lingual language model requires substantially less computation resource.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Cross-lingual Alignment</head><p>To explore whether discriminative pre-training improves the resulting cross-lingual representations, we evaluate our model on the sentence-level and word-level alignment tasks, i.e., cross-lingual sentence retrieval and word alignment.</p><p>We use the Tatoeba (Artetxe and Schwenk, 2019) dataset for the cross-lingual sentence retrieval task, the goal of which is to find translation pairs from the corpora in different languages. Tatoeba consists of English-centric parallel corpora covering 122 languages. Following <ref type="bibr" target="#b6">Chi et al. (2021b)</ref> and <ref type="bibr" target="#b19">Hu et al. (2020b)</ref>, we consider two settings where we use 14 and 36 of the parallel corpora for evaluation, respectively. The sentence representations are obtained by average pooling over hidden vectors from a middle layer. Specifically, we use layer-7 for XLM-R and layer-9 for XLM-E. Then, the translation pairs are induced by the nearest neighbor search using the cosine similarity. Table <ref type="table">3</ref> shows the average accuracy@1 scores under the two settings of Tatoeba for both the xx → en and en → xx directions. XLM-E achieves 74.4 and 72.3 accuracy scores for Tatoeba-14, and 65.0 and 62.3 accuracy scores for Tatoeba-36, providing notable improvement over XLM-R base . XLM-E performs slightly worse than INFOXLM base . We believe the cross-lingual contrast <ref type="bibr" target="#b6">(Chi et al., 2021b)</ref> task explicitly learns the sentence representations, which makes INFOXLM base more effective for the crosslingual sentence retrieval task.</p><p>For the word-level alignment, we use the word for the word alignment task on four English-centric language pairs. Results of the baseline models are from <ref type="bibr" target="#b7">Chi et al. (2021c)</ref>. We use the optimal transport method to obtain the resulting word alignments, where the sentence representations are from the 9-th layer of XLM-E.</p><p>alignment datasets from EuroParl<ref type="foot" target="#foot_0">1</ref> , WPT2003<ref type="foot" target="#foot_1">2</ref> , and WPT2005<ref type="foot" target="#foot_2">3</ref> , containing 1,244 translation pairs annotated with golden alignments. The predicted alignments are evaluated by alignment error rate (AER; Och and Ney 2003):</p><formula xml:id="formula_13">AER = 1 − |A ∩ S| + |A ∩ P | |A| + |S|<label>(10)</label></formula><p>where A, S, and P stand for the predicted alignments, the annotated sure alignments, and the annotated possible alignments, respectively. In Table 4 we compare XLM-E with baseline models, i.e., fast align <ref type="bibr" target="#b16">(Dyer et al., 2013)</ref>, XLM-R base , and XLM-ALIGN. The resulting word alignments are obtained by the optimal transport method <ref type="bibr" target="#b7">(Chi et al., 2021c)</ref>, where the sentence representations are from the 9-th layer of XLM-E. Over the four language pairs, XLM-E achieves lower AER scores than the baseline models, reducing the average AER from 21.05 to 19.32. It is worth mentioning that our model requires substantial lower computation costs than the other cross-lingual pretrained language models to achieve such low AER scores. See the detailed training efficiency analysis in Section 4.3. It is worth mentioning that XLM-E shows notable improvements over XLM-E (w/o TRTD) on both tasks, demonstrating that the translation replaced token detection task is effective for cross-lingual alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Universal Layer Across Languages</head><p>We evaluate the word-level and sentence-level representations over different layers to explore whether the XLM-E tasks encourage universal representations.</p><p>As shown in Figure <ref type="figure" target="#fig_2">3</ref>, we illustrate the accuracy@1 scores of XLM-E and XLM-R base on Tatoeba cross-lingual sentence retrieval, using sentence representations from different layers. For each layer, the final accuracy score is averaged over all the 36 language pairs in both the xx → en and en → xx directions. From the figure, it can be observed that XLM-E achieves notably higher averaged accuracy scores than XLM-R base for the top layers. The results of XLM-E also show a parabolic trend across layers, i.e., the accuracy continuously increases before a specific layer and then continuously drops. This trend is also found in other cross-lingual language models such as XLM-R and XLM-Align <ref type="bibr" target="#b20">(Jalili Sabet et al., 2020;</ref><ref type="bibr" target="#b7">Chi et al., 2021c)</ref>. Different from XLM-R base that achieves the highest accuracy of 54.42 at layer-7, XLM-E pushes it to layer-9, achieving an accuracy of 63.66. At layer-10, XLM-R base only obtains an accuracy of 43.34 while XLM-E holds the accuracy score as high as 57.14.</p><p>Figure <ref type="figure" target="#fig_3">4</ref> shows the averaged alignment error rate (AER) scores of XLM-E and XLM-R base on the word alignment task. We use the hidden vectors from different layers to perform word alignment, where layer-0 stands for the embedding layer. The final AER scores are averaged over the four test sets in different languages. Figure <ref type="figure" target="#fig_3">4</ref> shows a similar trend to that in Figure <ref type="figure" target="#fig_2">3</ref>, where XLM-E not only provides substantial performance improvements over XLM-R, but also pushes the best-performance layer to a higher layer, i.e., the model obtains the best performance at layer-9 rather than a lower   layer such as layer-7.</p><p>On both tasks, XLM-E shows good performance for the top layers, even though both XLM-E and XLM-R base use the Transformer <ref type="bibr" target="#b40">(Vaswani et al., 2017)</ref> architecture. Compared to the masked language modeling task that encourages the top layers to be language-specific, discriminative pretraining makes XLM-E producing better-aligned text representations at the top layers. It indicates that the cross-lingual discriminative pre-training encourages universal representations inside the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Cross-lingual Transfer Gap</head><p>We analyze the cross-lingual transfer gap <ref type="bibr" target="#b19">(Hu et al., 2020b)</ref> of the pretrained cross-lingual language models. The transfer gap score is the difference between performance on the English test set and the average performance on the test set in other languages. This score suggests how much end task knowledge has not been transferred to other languages after fine-tuning. A lower gap score indicates better cross-lingual transferability. Table <ref type="table" target="#tab_7">5</ref> compares the cross-lingual transfer gap scores on five of the XTREME tasks. We notice that XLM-E obtains the lowest gap score only on PAWS-X.</p><p>Nonetheless, it still achieves reasonably low gap scores on the other tasks with such low computation cost, demonstrating the cross-lingual transferability of XLM-E. We believe that it is more difficult to achieve the same low gap scores when the model obtains better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Learning self-supervised tasks on large-scale multilingual texts has proven to be effective for pretraining cross-lingual language models. Masked language modeling (MLM; <ref type="bibr" target="#b14">Devlin et al. 2019</ref>) is typically used to learn cross-lingual encoders such as Multilingual BERT (mBERT; <ref type="bibr" target="#b14">Devlin et al. 2019)</ref> and XLM-R <ref type="bibr" target="#b11">(Conneau et al., 2020)</ref>. Masked sequence to sequence pre-training (MASS; <ref type="bibr" target="#b38">Song et al. 2019)</ref>, denoising auto-encoding, and span corruption <ref type="bibr" target="#b35">(Raffel et al., 2020)</ref> are designed for learning cross-lingual sequence-to-sequence models, i.e., MASS, mBART <ref type="bibr" target="#b26">(Liu et al., 2020)</ref>, and mT5 <ref type="bibr">(Xue et al., 2020)</ref>.</p><p>The cross-lingual language models can be further improved by introducing external pre-training tasks using parallel corpora. XLM <ref type="bibr" target="#b12">(Conneau and Lample, 2019)</ref> introduces the translation language modeling (TLM) task that predicts masked tokens from concatenated translation pairs. ALM <ref type="bibr" target="#b43">(Yang et al., 2020)</ref> utilizes translation pairs to construct code-switched sequences as input. InfoXLM <ref type="bibr" target="#b6">(Chi et al., 2021b)</ref> considers an input translation pair as cross-lingual views of the same meaning, and proposes the cross-lingual contrastive learning task that aims to maximize the InfoNCE <ref type="bibr" target="#b31">(Oord et al., 2018)</ref> lower bound of the mutual information of the two views. Contrastive learning is also used in Hictl <ref type="bibr" target="#b41">(Wei et al., 2021)</ref> and post-pretrained multilingual BERT <ref type="bibr" target="#b32">(Pan et al., 2020)</ref>. Several pre-training tasks utilize the token-level alignments in parallel data to improve cross-lingual language models <ref type="bibr" target="#b3">(Cao et al., 2020;</ref><ref type="bibr">Zhao et al., 2020;</ref><ref type="bibr" target="#b18">Hu et al., 2020a;</ref><ref type="bibr" target="#b7">Chi et al., 2021c)</ref>.</p><p>In addition, parallel data are also employed for cross-lingual sequence-to-sequence pre-training. XNLG <ref type="bibr" target="#b5">(Chi et al., 2020)</ref> presents cross-lingual masked language modeling and cross-lingual autoencoding for cross-lingual natural language generation, and achieves the cross-lingual transfer for NLG tasks. VECO <ref type="bibr" target="#b27">(Luo et al., 2020)</ref> utilizes cross-attention MLM to pretrain a variable crosslingual language model for both NLU and NLG. mT6 <ref type="bibr" target="#b4">(Chi et al., 2021a)</ref> improves mT5 by learn-ing the translation span corruption task on parallel data. ∆LM <ref type="bibr" target="#b28">(Ma et al., 2021)</ref> proposes to align pretrained multilingual encoders to improve crosslingual sequence-to-sequence pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We introduce XLM-E, a cross-lingual language model pretrained by ELECTRA-style tasks. Specifically, we present two pre-training tasks, i.e., multilingual replaced token detection, and translation replaced token detection. XLM-E outperforms baseline models on cross-lingual understanding tasks although using much less computation cost. In addition to improved performance and computational efficiency, we also show that XLM-E obtains the cross-lingual transferability with a reasonably low transfer gap. For future work, we would like to scale up XLM-E to larger model size.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Overview of two pre-training tasks of XLM-E, i.e., multilingual replaced token detection, and translation replaced token detection. The generator predicts the masked tokens given a masked sentence or a masked translation pair, and the discriminator distinguishes whether the tokens are replaced by the generator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Evaluation results on Tatoeba cross-lingual sentence retrieval over different layers. For each layer, the accuracy score is averaged over all the 36 language pairs in both the xx → en and en → xx directions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Evaluation results of cross-lingual word alignment over different layers. Layer-0 stands for the embedding layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Attention is all you need. is we ? Attention is all need we ? Discriminator Yes Yes Yes Yes No No</head><label></label><figDesc>The only</figDesc><table><row><cell>Is original?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Is original?</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>No</cell><cell>Yes</cell><cell>No</cell><cell>Yes</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Discriminator</cell></row><row><cell>Replaced</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Replaced</cell><cell>你</cell><cell>好</cell><cell>世界</cell><cell>？</cell><cell cols="2">Hello earth</cell><cell>.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>你</cell><cell></cell><cell></cell><cell>？</cell><cell></cell><cell>earth</cell></row><row><cell></cell><cell></cell><cell cols="2">Generator</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Generator</cell></row><row><cell>Masked</cell><cell>Attention &lt;M&gt;</cell><cell>all</cell><cell>&lt;M&gt;</cell><cell>need</cell><cell>&lt;M&gt;</cell><cell>Masked</cell><cell>&lt;M&gt;</cell><cell>好</cell><cell cols="3">世界 &lt;M&gt; Hello</cell><cell>&lt;M&gt;</cell><cell>.</cell></row><row><cell>Original</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Original</cell><cell></cell><cell cols="2">你好世界。</cell><cell></cell><cell cols="2">Hello world.</cell></row><row><cell cols="6">(a) Multilingual replaced token detection (MRTD)</cell><cell></cell><cell cols="6">(b) Translation replaced token detection (TRTD)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Evaluation results on XTREME cross-lingual understanding tasks. We consider the cross-lingual transfer setting, where models are only fine-tuned on the English training data but evaluated on all target languages. Results with "*" are taken from<ref type="bibr" target="#b19">(Hu et al., 2020b)</ref>. Results of XLM-E and XLM-R base are averaged over five runs.</figDesc><table><row><cell>Model</cell><cell cols="2">Structured Prediction POS NER</cell><cell cols="5">Question Answering XQuAD MLQA TyDiQA XNLI PAWS-X Classification</cell><cell>Avg</cell></row><row><cell>Metrics</cell><cell>F1</cell><cell>F1</cell><cell>F1 / EM</cell><cell>F1 / EM</cell><cell>F1 / EM</cell><cell>Acc.</cell><cell>Acc.</cell><cell></cell></row><row><cell cols="2">Pre-training on multilingual corpus</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MBERT*</cell><cell>70.3</cell><cell>62.2</cell><cell cols="4">64.5 / 49.4 61.4 / 44.2 59.7 / 43.9 65.4</cell><cell>81.9</cell><cell>63.1</cell></row><row><cell>MT5 base</cell><cell>-</cell><cell>55.7</cell><cell cols="4">67.0 / 49.0 64.6 / 45.0 57.2 / 41.2 75.4</cell><cell>86.4</cell><cell>-</cell></row><row><cell>XLM-R base</cell><cell>75.6</cell><cell>61.8</cell><cell cols="4">71.9 / 56.4 65.1 / 47.2 55.4 / 38.3 75.0</cell><cell>84.9</cell><cell>66.4</cell></row><row><cell cols="2">XLM-E (w/o TRTD) 74.2</cell><cell>62.7</cell><cell cols="4">74.3 / 58.2 67.8 / 49.7 57.8 / 40.6 75.1</cell><cell>87.1</cell><cell>67.6</cell></row><row><cell cols="4">Pre-training on both multilingual corpus and parallel corpus</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>XLM*</cell><cell>70.1</cell><cell>61.2</cell><cell cols="4">59.8 / 44.3 48.5 / 32.6 43.6 / 29.1 69.1</cell><cell>80.9</cell><cell>58.6</cell></row><row><cell>INFOXLM base</cell><cell>-</cell><cell>-</cell><cell>-/ -</cell><cell>68.1 / 49.6</cell><cell>-/ -</cell><cell>76.5</cell><cell>-</cell><cell>-</cell></row><row><cell>XLM-ALIGN</cell><cell>76.0</cell><cell>63.7</cell><cell cols="4">74.7 / 59.0 68.1 / 49.8 62.1 / 44.8 76.2</cell><cell>86.8</cell><cell>68.9</cell></row><row><cell>XLM-E</cell><cell>75.6</cell><cell>63.5</cell><cell cols="4">76.2 / 60.2 68.3 / 49.8 62.4 / 45.7 76.6</cell><cell>88.3</cell><cell>69.3</cell></row></table><note>(MBERT;<ref type="bibr" target="#b14">Devlin et al. 2019)</ref>, MT5(Xue et al.,  2020), and XLM-R<ref type="bibr" target="#b11">(Conneau et al., 2020)</ref>, or pretrained with both multilingual text and parallel corpora, i.e., XLM<ref type="bibr" target="#b12">(Conneau and Lample, 2019)</ref>, INFOXLM</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison of the pre-training costs. The models with '*' are continue-trained from XLM-R base rather than pre-training from scratch.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Alignment error rate scores (lower is better)</figDesc><table><row><cell>Model</cell><cell cols="2">Alignment Error Rate ↓ en-de en-fr en-hi en-ro</cell><cell>Avg</cell></row><row><cell>fast align</cell><cell>32.14 19.46 59.90</cell><cell>-</cell><cell>-</cell></row><row><cell>XLM-Rbase</cell><cell cols="3">17.74 7.54 37.79 27.49 22.64</cell></row><row><cell cols="4">XLM-ALIGN 16.63 6.61 33.98 26.97 21.05</cell></row><row><cell>XLM-E</cell><cell cols="3">16.49 6.19 30.20 24.41 19.32</cell></row><row><cell>−TRTD</cell><cell cols="3">17.87 6.29 35.02 30.22 22.35</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>The cross-lingual transfer gap scores on the XTREME tasks. A lower transfer gap score indicates better cross-lingual transferability. We use the EM scores to compute the gap scores for the QA tasks.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">www-i6.informatik.rwth-aachen.de/ goldAlignment/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">web.eecs.umich.edu/ ˜mihalcea/wpt/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">web.eecs.umich.edu/ ˜mihalcea/wpt05/</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Hyperparameters for Pre-Training</head><p>As shown in Table <ref type="table">6</ref>, we present the hyperparameters for pre-training XLM-E, where D and G stand for the discriminator and the generator, respectively. We use the batch size of 1M tokens for each pretraining task. In multilingual replaced token detection, a batch is constructed by 2,048 length-512 input sequences, while the input length is dynamically set as the length of the original translation pairs in translation replaced token detection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Hyperparameters for Fine-Tuning</head><p>In Table <ref type="table">7</ref>, we report the hyperparameters for finetuning XLM-E on the XTREME end tasks. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On the cross-lingual transferability of monolingual representations</title>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.421</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4623" to="4637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Massively multilingual sentence embeddings for zeroshot cross-lingual transfer and beyond</title>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="597" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">UniLMv2: Pseudo-masked language models for unified language model pre-training</title>
		<author>
			<persName><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songhao</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12804</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multilingual alignment of contextual word representations</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Zewen</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian-Ling</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heyan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08692</idno>
		<title level="m">mT6: Multilingual pretrained text-to-text transformer with translation pairs</title>
				<imprint>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cross-lingual natural language generation via pre-training</title>
		<author>
			<persName><forename type="first">Zewen</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian-Ling</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heyan</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020-02-07">2020. February 7-12, 2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="7570" to="7577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">In-foXLM: An information-theoretic framework for cross-lingual language model pre-training</title>
		<author>
			<persName><forename type="first">Zewen</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saksham</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian-Ling</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heyan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.280</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter</title>
				<meeting>the 2021 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021b</date>
			<biblScope unit="page" from="3576" to="3588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Improving pretrained cross-lingual language models via self-labeled word alignment</title>
		<author>
			<persName><forename type="first">Zewen</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian-Ling</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heyan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.06381</idno>
		<imprint>
			<date type="published" when="2021">2021c</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1179</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">TyDi QA: A benchmark for information-seeking question answering in typologically diverse languages</title>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Nikolaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00317</idno>
		<imprint>
			<date type="published" when="2020">2020a</date>
			<publisher>Transactions of the Association for Computational Linguistics</publisher>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="454" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Electra: Pretraining text encoders as discriminators rather than generators</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Crosslingual language model pretraining</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7057" to="7067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">XNLI: Evaluating cross-lingual sentence representations</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1269</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2475" to="2485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="13063" to="13075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A simple, fast, and effective reparameterization of ibm model 2</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Chahuneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter</title>
				<meeting>the 2013 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="644" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">CCAligned: A massive collection of cross-lingual web-document pairs</title>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>El-Kishky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.480</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5960" to="5969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Explicit alignment objectives for multilingual bidirectional encoders</title>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.07972</idno>
		<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.11080</idno>
		<title level="m">XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalization</title>
				<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SimAlign: High quality word alignments without parallel training data using static and contextualized embeddings</title>
		<author>
			<persName><forename type="first">Jalili</forename><surname>Masoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Sabet</surname></persName>
		</author>
		<author>
			<persName><surname>Dufter</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.147</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
				<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1627" to="1643" />
		</imprint>
	</monogr>
	<note>Franc ¸ois Yvon, and Hinrich Schütze</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Daniel S Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><surname>Levy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10529</idno>
		<title level="m">Span-BERT: Improving pre-training by representing and predicting spans</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
				<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-2012</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="66" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The IIT Bombay English-Hindi parallel corpus</title>
		<author>
			<persName><forename type="first">Anoop</forename><surname>Kunchukuttan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratik</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation</title>
				<meeting>the Eleventh International Conference on Language Resources and Evaluation<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">MLQA: Evaluating cross-lingual extractive question answering</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7315" to="7330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Multilingual denoising pre-training for neural machine translation</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08210</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">VECO: Variable encoder-decoder pre-training for cross-lingual understanding and generation</title>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.16046</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">DeltaLM: Encoder-decoder pre-training for language generation and translation by augmenting pretrained multilingual encoders</title>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Muzio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saksham</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hany</forename><surname>Hassan Awadalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13736</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Payal</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.08473</idno>
		<title level="m">COCO-LM: Correcting and contrasting text sequences for language model pretraining</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A systematic comparison of various statistical alignment models</title>
		<author>
			<persName><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="51" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Lin</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung-Wei</forename><surname>Hang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haode</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saloni</forename><surname>Potdar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12547</idno>
		<title level="m">Multilingual bert post-pretraining alignment</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Crosslingual name tagging and linking for 282 languages</title>
		<author>
			<persName><forename type="first">Xiaoman</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Nothman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1178</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1946" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A decomposable attention model for natural language inference</title>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1244</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2249" to="2255" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-totext transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Massively multilingual transfer for NER</title>
		<author>
			<persName><forename type="first">Afshin</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1015</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="151" to="164" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05791</idno>
		<title level="m">Wiki-Matrix: Mining 135M parallel sentences in 1620 language pairs from wikipedia</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">MASS: Masked sequence to sequence pre-training for language generation</title>
		<author>
			<persName><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02450</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Parallel data, tools and interfaces in OPUS</title>
		<author>
			<persName><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Language Resources and Evaluation</title>
				<meeting>the Eighth International Conference on Language Resources and Evaluation<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2214" to="2218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">On learning universal representations across languages</title>
		<author>
			<persName><forename type="first">Xiangpeng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongxiang</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luxi</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Linting</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihir</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11934</idno>
		<title level="m">Aditya Barua, and Colin Raffel. 2020. mT5: A massively multilingual pre-trained text-to-text transformer</title>
				<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Alternating language modeling for cross-lingual pre-training</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuangzhi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">PAWS-X: A cross-lingual adversarial dataset for paraphrase identification</title>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Tar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1382</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019a</date>
			<biblScope unit="page" from="3687" to="3692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">XLNet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Daniel Zeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName><surname>Abrams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m">LINDAT/CLARIAH-CZ digital library at the Institute of Formal and Applied Linguistics ( ÚFAL), Faculty of Mathematics and Physics</title>
				<imprint/>
		<respStmt>
			<orgName>Charles University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Eger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.09112</idno>
		<title level="m">Johannes Bjerva, and Isabelle Augenstein. 2020. Inducing languageagnostic multilingual representations</title>
				<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The united nations parallel corpus v1. 0</title>
		<author>
			<persName><forename type="first">Michał</forename><surname>Ziemski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Pouliquen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3530" to="3534" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
