<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Manuscript received August 7, 2006; revised October 27, 2006. This work was supported in part by the NSFC under Grant 60520120099. The associate editor coordinating the review of this manuscript and approving it for publication was Dr</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Til</forename><forename type="middle">W</forename><surname>Aach</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">D</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
							<email>dxie@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">W</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Zeng</surname></persName>
							<email>wrzeng@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">S</forename><surname>Maybank</surname></persName>
							<email>sjmaybank@dcs.bbk.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Information Sys-tems</orgName>
								<orgName type="institution">Birkbeck College</orgName>
								<address>
									<postCode>WC1E 7HX</postCode>
									<settlement>London</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Manuscript received August 7, 2006; revised October 27, 2006. This work was supported in part by the NSFC under Grant 60520120099. The associate editor coordinating the review of this manuscript and approving it for publication was Dr</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C5F6E6291CB52704E84FC8EB9F72F3F7</idno>
					<idno type="DOI">10.1109/TIP.2006.891352</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic-Based Surveillance Video Retrieval</head><p>Weiming Hu, Dan Xie, Zhouyu Fu, Wenrong Zeng, and Steve Maybank, Senior Member, IEEE Abstract-Visual surveillance produces large amounts of video data. Effective indexing and retrieval from surveillance video databases are very important. Although there are many ways to represent the content of video clips in current video retrieval algorithms, there still exists a semantic gap between users and retrieval systems. Visual surveillance systems supply a platform for investigating semantic-based video retrieval. In this paper, a semantic-based video retrieval framework for visual surveillance is proposed. A cluster-based tracking algorithm is developed to acquire motion trajectories. The trajectories are then clustered hierarchically using the spatial and temporal information, to learn activity models. A hierarchical structure of semantic indexing and retrieval of object activities, where each individual activity automatically inherits all the semantic descriptions of the activity model to which it belongs, is proposed for accessing video clips and individual objects at the semantic level. The proposed retrieval framework supports various queries including queries by keywords, multiple object queries, and queries by sketch. For multiple object queries, succession and simultaneity restrictions, together with depth and breadth first orders, are considered. For sketch-based queries, a method for matching trajectories drawn by users to spatial trajectories is proposed. The effectiveness and efficiency of our framework are tested in a crowded traffic scene.</p><p>Index Terms-Activity models, semantic-based, video retrieval, visual surveillance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>C AMERA systems for surveillance are widely used and pro- duce large amounts of video data which are stored for future or immediate use. In this context, effective indexing and retrieval from surveillance video databases are very important. Surveillance videos are rich in motion information which stands out as the most important cue to identify the dynamic content of videos. Extraction, storage, and analysis of motion information in videos are critical in content-based surveillance video retrieval.</p><p>Video retrieval using motion information has been investigated in the recent years <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. The existing approaches emphasize the decomposition and approximation of motion trajectories, where semantic information about object motions is not reflected. Such approaches to video retrieval can answer sketch-based or example-based queries, but cannot answer semantic-based queries. Although manual annotation of video data makes it possible to implement semantic-based video retrieval, the complete manual annotation of large video databases is too expensive.</p><p>In visual surveillance systems, the cameras are mainly fixed, and object motions in the monitored scenes tend to be regular. These facts make visual surveillance data very suitable for investigating semantic-based video retrieval. Visual surveillance systems need the support of techniques of video retrieval, and in the meanwhile supply a platform for investigating semanticbased video retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Related Work</head><p>There are two main aspects of semantic-based video retrieval for visual surveillance: motion-based video retrieval and the semantic description of object motions. a) Motion-based video retrieval: Some pioneering video retrieval systems using motion information have been developed. Chang et al. <ref type="bibr" target="#b0">[1]</ref> propose an online video retrieval system supporting automatic object-based indexing and spatiotemporal queries. The system includes algorithms for automated video-object segmentation and tracking. Real-time video editing techniques are used to respond to user queries. Bashir et al. <ref type="bibr" target="#b1">[2]</ref> apply principal component analysis (PCA) to reduce the dimensionality of trajectory data. A two-level PCA operation with coarse-to-fine retrieval is used to identify trajectories close to a query trajectory. Sahouria and Zakhor <ref type="bibr" target="#b2">[3]</ref> propose an object trajectory-based system for video indexing, where the indexing is based on Haar wavelet transform coefficients. Chen and Chang <ref type="bibr" target="#b3">[4]</ref> use wavelet decomposition to segment each trajectory and produce an index based on velocity features. Jung et al. <ref type="bibr" target="#b4">[5]</ref> base their motion model on polynomial curve fitting. The motion model is used as an indexing key for accessing individual objects. They also propose an efficient way for indexing and retrieval based on object-specific features. Dagtas et al. <ref type="bibr" target="#b5">[6]</ref> present models that use the object motion information to characterize events to allow subsequent video retrieval. Scale-invariant algorithms for spatiotemporal search have been developed. Chikashi et al. <ref type="bibr" target="#b24">[25]</ref> propose a trajectory extraction, encoding, and query method based on the spatiotemporal relationships between objects. Dimitrova and Golshani <ref type="bibr" target="#b25">[26]</ref> propose a motion description method based on the motion compensation component of the MPEG video encoding scheme. Trajectories of blocks are acquired by computing their forward and backward motion vectors, and they are used as feature vectors for retrieval of videos.</p><p>The above approaches emphasize the reduction of the dimension of the trajectory data in order to acquire a set of parameters to represent trajectory features. A semantic gap exists between users and retrieval systems because there is no mapping between the parameters of the trajectories and the semantic meanings of object motions.</p><p>b) Semantic Description of Motions: In recent years, the semantic description of object motions has been investigated. Haag and Nagel <ref type="bibr" target="#b26">[27]</ref> describe a system for incremental recognition of traffic situations. They use fuzzy metric temporal logic (FMTL) as an inference tool to handle uncertainty and temporal aspects of action recognition. Mohnhaupt and Neumann <ref type="bibr" target="#b27">[28]</ref> establish a "3-D scene description sequence," which includes traffic data such as directions, positions, and times of vehicles, etc. Bell and Pau <ref type="bibr" target="#b28">[29]</ref> develop an object-oriented logic program system for image interpretation and apply it to vehicle recognition in real scenes. Huang et al. <ref type="bibr" target="#b29">[30]</ref> and Buxton et al. <ref type="bibr" target="#b30">[31]</ref> use a Bayesian belief network and inference engine in sequences of highway traffic scenes to produce high-level concepts like "car changing lane" and "car stalled." Remagnino et al. <ref type="bibr" target="#b31">[32]</ref> present an event-based visual surveillance system for monitoring vehicles and pedestrians, which supplies verbal descriptions of dynamic activities in 3-D scenes. Liu et al. <ref type="bibr" target="#b32">[33]</ref> interpret dynamic-object interactions in temporal image sequences using fuzzy sets. The descriptions of objects are handled by fuzzy logic and fuzzy measures. Kojima et al. <ref type="bibr" target="#b33">[34]</ref> propose a method for describing human activities in video images based on a hierarchy of concepts related to actions. Thonnat and Rota <ref type="bibr" target="#b34">[35]</ref> propose a method for representing human activities using scenarios, which are translated into text by filling entries in a template of natural language sentences. Ayers and Shah <ref type="bibr" target="#b35">[36]</ref> propose a method for generating textual descriptions of human activities in video images using state transition models.</p><p>Current approaches for the semantic description of object motions mainly depend on manual definition of object motion models. There is a strong demand for automatic methods for learning object motion models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Our Work</head><p>A novel framework for semantic-based surveillance video retrieval is proposed in this paper. The aim is to bridge the semantic gap between users and video retrieval systems. Fig. <ref type="figure" target="#fig_0">1</ref> gives an overview of our framework. Video sequences taken from digital cameras are input to the system. Objects in the scene are tracked and object trajectories are extracted. The spectral algorithm is used to cluster trajectories to learn object activity models. Semantic descriptions are added to the activity models to form semantic activity models, and semantic indexes are constructed for video databases. Keywords-based queries, multiple object queries, and sketch-based queries are supported.</p><p>Our framework is original in the following ways.</p><p>• Visual surveillance and video retrieval are combined to explore semantic-based video retrieval through learning semantic object activity models. • We learn object activity models from motion trajectories automatically by clustering trajectories. A method for learning activity models is proposed. Trajectories are hierarchically clustered using spectral clustering. Spatial information is used to cluster all trajectories into some categories, and trajectories in each of the categories are further clustered into subcategories using the temporal information. Each cluster of trajectories corresponds to an activity model. Each activity model represents the semantic content of the corresponding category of activities. • We propose a hierarchical structure of semantic indexing and retrieval of object activities. The activity models are used as indexing keys for accessing individual objects. Each individual activity automatically inherits all the semantic descriptions of the activity model to which it belongs. In our framework, only the semantic description item for an activity model is given manually, and other items for the activity model and all items for an activity are obtained automatically. So, compared with current video retrieval approaches with complete manual annotation, our approach greatly reduces the workload of manual annotation. • Based on our hierarchical structure of semantic indexing and retrieval, keywords-based queries, multiple object queries, and sketch-based queries are supported. For multiple object queries, the temporal restrictions "succession" and "simultaneity" are considered. For sketch-based queries, a robust algorithm for matching trajectories drawn by users to spatial trajectories is proposed. • We test our framework experimentally in a crowded traffic scene. The experimental results show the robustness of the tracking algorithm, the efficiency of the algorithm for learning activity models, and the encouraging performance of the algorithms for video retrieval. This paper is organized as follows. Section II briefly introduces the method for tracking multiple objects. Section III describes the learning of activity models. Section IV presents the mechanism of semantic indexing and retrieval. Section V illustrates experimental results. The last section summarizes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. TRACKING</head><p>Although the motivation of the paper is to investigate learning of semantic activity models and semantic-based video retrieval, these important tasks are based on tracking of moving objects. In this paper, we focus on real traffic scenes where there are many vehicles and mutual occlusions between multiple vehicles. Although some algorithms <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref> have been proposed to track multiple objects, most of them fail due to the complexity of motions. In this section, we only briefly introduce our cluster-based algorithm for tracking multiple objects. Readers may read our previous paper <ref type="bibr" target="#b15">[16]</ref> for more details.</p><p>Our tracking algorithm is based on the principle that a moving object is associated with a cluster of pixels in the feature space and the distributions of the clusters change little between consecutive frames <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. Foreground pixels are identified using background subtraction, and features of foreground pixels are extracted and clustered. Each cluster centroid corresponds to a moving object or a part of a moving object. The clusters in the current frame contribute to the initialization of the cluster centroids of foreground pixels in the subsequent frame. A cluster centroid in the subsequent frame and the cluster centroid in the current frame, which participates in the initialization of cluster centroid , both correspond to the same object or the same part of an object. Clusters can grow dynamically: be created or be erased. A predication algorithm is used to ensure that each cluster centroid correctly corresponds to a moving object in spite of the occurrence of partial occlusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Acquisition of Pixel Features</head><p>Foreground pixels are acquired by a self-adaptable background update model. Each foreground pixel is described with a feature vector containing its coordinates , velocity , and color in the space</p><p>(1)</p><p>Compared with coordinate and color features, velocity features are difficult to acquire. In the algorithm, velocities are estimated using an optical flow algorithm, which is an improved one, inspired by <ref type="bibr" target="#b9">[10]</ref> and <ref type="bibr" target="#b10">[11]</ref>. We only estimate the optical flow for foreground pixels. This greatly reduces the computational cost, compared with the methods which estimate the optical flow for all pixels in the current frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Clustering of Foreground Pixels</head><p>After the pixel features are acquired, we apply the fuzzy -means algorithm <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b13">[14]</ref> to cluster foreground pixels.</p><p>In order to improve the speed of the clustering process, regions of foreground pixels are averaged. The image plane is partitioned into square regions with equal size. The feature vector of each region is the mean of the feature vectors of the foreground pixels in the region, and it is associated with a weight , which is number of foreground pixels in this region.</p><p>The sample feature vectors are clustered using the weighting fuzzy -means algorithm which is modified to support weighted feature vectors. Let denote the number of sample feature vectors; the dimension of the sample feature vectors, and the number of cluster centroids. Cluster centroid is represented by the vector . Given all sample feature vectors , , the Euclidean distance from sample feature vector to cluster centroid is calculated by <ref type="bibr" target="#b1">(2)</ref> The fuzzy membership of sample feature vector to cluster centroid is computed by</p><formula xml:id="formula_0">(3)</formula><p>The value of each cluster centroid vector is updated iteratively by <ref type="bibr" target="#b3">(4)</ref> where is the number of iteration. In the frames rather than the first frame, the initial values ( , ) of the cluster centroid vectors are derived from the previous frames. In the first frame, the values of are randomly selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Prediction of Cluster Centroids</head><p>For a cluster centroid in the current frame, a prediction algorithm is used to produce an initial value for the corresponding cluster centroid in the next frame. This prediction ensures that each centroid in the current frame corresponds to a correct foreground pixel cluster in the next frame. As a result, the linkages between moving objects and corresponding cluster centroids are maintained in spite of the occurrence of partial occlusions.</p><p>In our work, many objects need to be tracked simultaneously, so there is a high computational cost. For this reason, we keep the computational cost of the tracking algorithm as small as possible. A fast prediction algorithm, the double exponential smoothing-based prediction algorithm <ref type="bibr" target="#b14">[15]</ref>, is selected. The running speed of the double exponential smoothing-based prediction algorithm is faster than that of the Kalman filter and the extended Kalman filter <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Modeling of Trajectories</head><p>In order to describe the entire trajectories of moving objects, cluster centroids should be created or erased as objects enter or leave the scene. This is called dynamic growing of cluster centroids. More details on the dynamic growing of cluster centroids are given in <ref type="bibr" target="#b15">[16]</ref>.</p><p>Once a video clip is processed, centroid trajectories are grouped into object trajectories. The centroid trajectories with the similar motions are merged into one object trajectory: for two centroid trajectories, if they exist over the same sequence of frames, we calculate the distance between the centroids in each frame. If these distances are approximately constant and small, the two centroid trajectories are merged. The merged trajectory is the mean of these centroid trajectories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SEMANTIC ACTIVITY MODELS</head><p>Activity models are learned from the trajectories obtained by tracking. We define the activity of an object as the total of all information on the motion of the object from the time the object enters the scene until it either leaves the scene or it remains stationary for so long that it is treated as part of the background for motion detection, or from the time it begins to move from stop until it either leaves the scene or remains stationary for so long again that it is treated as part of the background. We only consider the activity of each individual object; the interactions between multiple objects are not described. An activity is described by a spatiotemporal trajectory , , which contains not only the spatial information represented by the spatial trajectory but also the temporal information represented by the velocities in each sample point in the trajectory, where the spatial trajectory only contains subvectors in the vectors . An activity model describes a category of activities with similar semantic meanings. Activity models are also described by spatial-temporal trajectories. Given sufficient motion trajectories (generally thousands of trajectories accumulated over hours of tracking), activity models can be learned through trajectory clustering. An activity model, represented by a trajectory cluster center, is the representation of a category of activities that have common motion features and semantic meanings (how the activity models relate to the semantic meanings are introduced in Sections III-B and IV-A). In this paper a spectral clusteringbased approach is proposed to cluster trajectories to learn activity models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Trajectory Clustering</head><p>Clustering of motion trajectories is a relatively new research topic. While early work models trajectories in indirect ways and does not cluster trajectories directly, more recent work handles trajectories in a more direct way by using the whole trajectory for clustering <ref type="bibr" target="#b16">[17]</ref>. Current approaches include vector quantization <ref type="bibr" target="#b17">[18]</ref>, self-organizing feature mapping <ref type="bibr" target="#b18">[19]</ref>, joint co-occurrence statistics <ref type="bibr" target="#b19">[20]</ref>, fuzzy -means clustering <ref type="bibr" target="#b20">[21]</ref>, agglomerative clustering <ref type="bibr" target="#b21">[22]</ref>, agglomerative complete-link hierarchical clustering <ref type="bibr" target="#b36">[37]</ref>, and deterministic annealing pairwise clustering <ref type="bibr" target="#b36">[37]</ref>, etc.</p><p>Spectral clustering is a novel class of clustering algorithms, which operate on the matrix of similarities between pairs of input feature vectors instead of on the input feature vectors themselves, and seek an optimal partition of the graph whose vertexes represent feature vectors and whose edges represent the similarities. Different versions of spectral clustering exist. In this paper the one presented in <ref type="bibr" target="#b22">[23]</ref> is used based on multiway normalized graph cutting.</p><p>In order to handle a large number of object trajectories, we hierarchically cluster trajectories according to different features. We consider the following two points.</p><p>• Activities observed in different road lanes or routes are assigned to different activity models, i.e., lane information is included in the activity models. Therefore, we cluster sample trajectories into different lanes or routes using spatial information of trajectories. • Objects which pass along the same lane or route may have different activities producing different activity models. For example, a vehicle may move rapidly or slowly, or on an irregular pathway through a lane or route. These activities are treated as different, although they occur in the same lane or route. Therefore, temporal information is used to further cluster the trajectories which have been assigned to the same lane or route. a) Spatial-based clustering: In spatial-based clustering, only the spatial parts of the trajectories are used. Each spatial trajectory is linearly interpolated to ensure that all trajectory have the same number of points. Let the resulting set of trajectories be , where is the number of sample trajectories. It is assumed that each of these trajectories has points.</p><p>We construct the matrix of similarities between trajectories in based on the pairwise distances between the trajectories. For trajectories and , the average coordinate distance between the corresponding points on them is calculated by <ref type="bibr" target="#b4">(5)</ref> Then, the similarity between trajectories and is measured by <ref type="bibr" target="#b5">(6)</ref> where the scaling parameter controls the decay of similarity as distance increases. We perform a simple correlation test to select the appropriate value of by increasing in log scale and tracking the correlation of similarity matrix between adjacent scales. is chosen before correlation converges.</p><p>It should be mentioned that Hausdorff distance is not adopted here, as it does not preserve sequential information; for example, the Hausdorff distance does not distinguish between two vehicles following the same path but heading in opposite directions.</p><p>We cluster the data set into groups using the procedure as follows.</p><p>Step 1: Compute the similarity matrix for the data set by (6).</p><p>Step 2: Construct matrix <ref type="bibr" target="#b22">[23]</ref> (7)</p><p>where is a diagonal matrix whose th diagonal element is the sum of all elements in the th row of matrix .</p><p>Step 3: Apply the eigenvalue decomposition to matrix to find out the eigenvectors , corresponding to the largest eigenvalues. The eigenvectors are represented as column vectors.</p><p>Step 4: Form a new matrix by stacking the eigenvectors in columns, and normalize each row of to unit length.</p><p>Step 5: Cluster the row vectors of into clusters, using the fuzzy -means algorithm by treating each row as a new feature vector corresponding to the vector in the original data set . The eigen-transformation is the key for spectral clustering. Data points which belong to different clusters but are close to each other in the original data space become well separated in the transformed eigenspace. We can find nearly orthogonal row vectors from as initial centers for the fuzzy -means clustering in Step 5. To do this, we select the first row vector randomly, and then select the second row vector with the smallest correlation value with the first row vector. The next row vector with the smallest sum of correlation values with the selected row vectors is selected until rows are selected from .</p><p>To more efficiently represent trajectory clusters, each cluster of trajectories is represented by a template trajectory . The template trajectory is defined as the one with the minimal sum of square of distances to all trajectories in cluster . We compute the covariance of each point in each template trajectory using the corresponding points in all the sample trajectories corresponding to this template trajectory. Then the envelope of each trajectory cluster is obtained.</p><p>The validity of clustering results is evaluated with a tightness and separation coefficient <ref type="bibr" target="#b7">(8)</ref> where is the fuzzy membership of to . The coefficient is a ratio of the mean of the square of the distance between each input sample and its corresponding template trajectory, to the minimum of the square of the distance between any two template trajectories. The clustering result should make the distance between any two template trajectories as large as possible, and the distance between an input sample and its corresponding template trajectory as small as possible. We estimate the number of clusters by checking if can be reduced as the number of clusters is varied.</p><p>It is hard to achieve accurate clustering results by a single application of spatial-based clustering to cluster the set of all sample trajectories whose number is very large, as current clustering approaches cannot accurately cluster large and complicated data which contain many clusters. Hence, we adopt a "divide and conquer" strategy for hierarchically clustering spatial trajectories. In the first layer of spatial-based clustering, the set of trajectories is divided into a small number of large clusters, in For any two spatiotemporal trajectories and , it is assumed that trajectory contains sampling points, trajectory contains sampling points, and . The average distance between corresponding points on trajectories and is calculated by <ref type="bibr" target="#b8">(9)</ref> The first part of the right hand side of the equation measures the spatial-temporal similarity between trajectory and the subtrajectory composed of the first points in trajectory . The second part of the right hand side of the equation approximately measures the spatial-temporal similarity between trajectory and the subtrajectory composed of the latter points in trajectory , using the distances from points in trajectory to point in trajectory . In the equation, the greedy alignment is used to take advantage of the temporal features of trajectories for the temporal-based clustering. If , the second part of the equation is omitted.</p><p>The procedure of temporal-based clustering is similar to that of spatial-based clustering except for the measure of the similarity between trajectories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Semantic Activity Models</head><p>By clustering trajectories, a set of activity models is learned, where each activity model corresponds to a template trajectory. Our approach for learning activity models maps activities with similar semantic meanings to the same activity model. Thus, each constructed activity model represents the semantic content of the corresponding category of activities. Semantic meanings of activity models are endowed manually with keywords (see Section IV-A for more details). So, our activity models are semantic-based ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. SEMANTIC INDEXING AND RETRIEVAL</head><p>The learned activity models are used as indexing keys for accessing the individual activities of objects in video databases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Semantic Indexing</head><p>In our retrieval framework, the basic units for retrieval are individual activities. In order to effectively describe object activities, an "activity descriptor" is constructed for each activity. Table <ref type="table">I</ref> gives the data structure of activity descriptors. As shown in Table <ref type="table">I</ref>, an object activity descriptor contains the identity of the activity, the identity of the video clip showing the activity, the frame numbers of the birth and death of the activity, the trajectory representing the activity, and the static information of the object such as color, size, etc. For each activity model, an "activity model descriptor" is also constructed. Table <ref type="table">II</ref> shows the data structure of activity model descriptors. As shown in Table <ref type="table">II</ref>, an activity model descriptor contains the identity of the activity model, a list of activities which belong to the activity model, the corresponding template trajectory, and the semantic descriptions. The activity descriptors and the activity model descriptors, together with the original video clips, are stored in databases and are used for indexing and retrieval, as the resumptive representation of the original video clips.</p><p>As aforementioned, an activity model represents the common motion features of a class of similar activities; in our approach, keywords that describe these common motion features are assigned to the activity model manually. The keywords are added to the semantic description item in the activity model descriptor, in order to achieve an indexing at the semantic level. Fig. <ref type="figure" target="#fig_2">3</ref> illustrates our hierarchical structure of semantic indexing of object activities. In the structure, each individual activity automatically inherits all the concepts of the activity model to which it belongs. By the structure of indexing, only the semantic description item of an activity model, which is a compressed form of a class of activities, is given manually. Other items in an activity model descriptor and all items in an activity descriptor are obtained automatically. So, compared with current video retrieval approaches with complete manual annotation, our approach greatly reduces the workload of manual annotation.</p><p>It is noted that semantic meanings of speed (i.e., "normal speed," "high speed," and "low speed") can be acquired statistically. The mean and covariance of values of speed in the sample trajectories are calculated. "Normal speed" lies between and . "Low speed" is lower than . "High speed" is higher than . So, the keywords of the speed information can be automatically added to the semantic description item of each activity mode.</p><p>In order to adapt to the changes of object activities in a complex scene, the activity models are adjusted dynamically over time. When a new video clip is input to a database, the activities of moving objects are extracted. For each of these activities, the best matching activity model is found. Then it is checked to see if this activity can be classified into the best matching activity model. If the distance from the spatiotemporal trajectory of this activity to the template trajectory of the activity model is small enough, the activity is classified to the activity model and added to the activity list of this activity model. Otherwise, this activity is not classified to an existing activity model, and instead it is treated as a temporary abnormal activity and added to the abnormal activity model. As shown in Fig. <ref type="figure" target="#fig_2">3</ref>, the abnormal activity model is a predefined model in the framework. It is used as a temporary store for all temporary abnormal activities. The spectral algorithm is used periodically to cluster the activities in the abnormal activity model. If there is a cluster which contains enough activities, the activities in this cluster are considered to be normal. A new activity model is then established for this cluster, and added to the collection of existing activity models. With the above strategy, our system has the ability to capture the slow and long-term changes of object activities in a complex scene. Thus, the adaptability and robustness of the video retrieval framework are increased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Applicable Query Types</head><p>In our framework, the following responses are made to queries from users: The object activities best matching the queries and their associated video clips are found. The birth and death frames of each of the activities are located, and the subvideo between these two frames is supplied to users for The responses to a keyword-based query such as "a blue car ran from south to north at a high speed" has two stages. First, a keyword matching process is used to find out the activity models whose semantic descriptions well match the concepts of "ran from south to north" and "high speed." Second, the activity lists of these activity models are searched for the activities whose color feature matches "blue." Activities found in this way form the responses to the query.</p><p>In the process of keyword matching, a similarity measurement in vector spaces <ref type="bibr" target="#b23">[24]</ref> is used to determine the degree of matching between a query and an activity model. It is assumed that an activity model contains a set of keywords , where each keyword in is endowed with a weight of value , and there is a set of keywords in the query sentence(s), where each keyword is endowed with a weight of value . Then, the degree of matching between the activity model and the query sentence(s) is <ref type="bibr" target="#b9">(10)</ref> It should be mentioned that users can input their queries using natural language, but the retrieval process is implemented by keyword matching. Of course, this way may cause errors in the retrieval results.</p><p>The quality of the retrieval results is measured by precisionrecall curves which are generated by increasing the size of the return list and computing the corresponding precision values.</p><p>b) Multiple object queries: In our video retrieval framework, multiple object queries are also supported. Given the descriptions of the motions of two or more objects and a temporal restriction between occurrences of these objects, the best matching activities of multiple objects are searched for and output under the temporal restriction.</p><p>In this paper, the following two temporal restrictions are considered.</p><p>• "Succession": There is succession between occurrences of activities (especially succession of beginning time).</p><p>• "Simultaneity": There is time overlap of occurrences of activities. For instance, we input a query with two objects.</p><p>• Object 1: "A white car turned left."</p><p>• Object 2: "A red car turned left." If the restriction of succession of beginning time is used, this query means to find objects 1 and 2, where the start of the activity of object 1 is earlier than that of object 2. If the "simultaneity" restriction is used, this query means that there is time overlap of occurrences of the activities of objects 1 and 2.</p><p>For multiple object queries, the precision-recall cures are affected by the permutation order of retrieval results, caused by errors in trajectory clustering. Fig. <ref type="figure" target="#fig_3">4</ref> shows an example of permutation order of two object retrieval results. In Fig. <ref type="figure" target="#fig_3">4</ref>, each letter in a rectangle represents a cluster of trajectories, and each number in a circle represents a trajectory in the cluster represented by the rectangle linked to it by a solid line. It is assumed that trajectory 1 in cluster and either of the trajectories 1 and 2 in cluster satisfy the query; trajectory 2 in cluster and either of the trajectories 1 and 2 in cluster also satisfy the query. There are two ways to order the retrieval results.</p><p>• Depth-first: The output order of the retrieval result is . If it is a clustering mistake that trajectory 1 or 2 is clustered to cluster A, there may be a big drop in precision in the precision-recall curve.</p><p>• Breadth-first: The output order of the retrieval result is . If it is an error that trajectory 1 or 2 is in cluster , there may be two drops of precision in the precision-recall curve, but the deepness of the two drops is smaller than that produced by depth-first ordering. The precision-recall curve is overall improved, compared with that produced by depth-first ordering. c) Query by sketch: There are queries that cannot be expressed by keywords, such as "a vehicle moved in this way." A sketch-based query is designed to solve such query requirements. A drawing interface allows users to draw motion trajectories which approximately represent the activities that the users expect to query. Sketches drawn by users are matched to the spatial trajectories associated with activity models. The best matching activity models are found. Then, the activity lists of these activity models are searched for the best matching activities.</p><p>In this paper, a method for matching trajectories drawn by users to spatial template trajectories of activity models (or spatial trajectories of activities) is proposed. Let trajectory drawn by a user be represented by , and let be the spatial template trajectory in an activity model:</p><p>. In order to measure the spatial similarity between trajectories and , we re-sample trajectory to obtain a trajectory with the same number of points as trajectory , scale the re-sampled trajectory to be the same length as trajectory , and then translate the scaled trajectory to fit trajectory as closely as possible.</p><p>• Re-sampling: On trajectory , we sample points which are linked to form trajectory to represent trajectory , where point in trajectory is prorated in the line segment in trajectory . • Scaling: Trajectory is scaled, by , to form trajectory , where and are respectively the lengths of trajectories and . • Translation: Trajectory is translated by to form trajectory . The sum of squares of the distances of corresponding points in trajectories and is <ref type="bibr" target="#b10">(11)</ref> The minimum of is reached when its partial derivatives are zero <ref type="bibr" target="#b11">(12)</ref> The solution to ( <ref type="formula">12</ref>) is <ref type="bibr" target="#b12">(13)</ref> Then, the spatial similarity between trajectories and is measured by with and given by ( <ref type="formula">13</ref>). This spatial similarity is used to match spatial trajectories for sketch-based queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL RESULTS</head><p>In order to verify the efficiency and accuracy of our video retrieval framework, different video sequences from a crowded traffic scene were tested. First, the performance of the tracking algorithm is introduced briefly. The results of learning activity models are then illustrated. Finally, the results of video retrieval are demonstrated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Tracking</head><p>The experimental results of our clustering-based tracking are shown in Fig. <ref type="figure" target="#fig_5">5</ref>. In Fig. <ref type="figure" target="#fig_5">5</ref>(a), an example of pixel segmentation and clustering in one frame is shown. The image size is fixed to 320 240 pixels. The extracted trajectories are illustrated in Fig. <ref type="figure" target="#fig_5">5</ref>(b), represented by black lines. In the tested video sequences, occlusions between moving vehicles, and those between moving vehicles and static street attachments such as street lamps and trees occur frequently. The correctness of the tracking results is checked by our own visual judgment. For the tested video sequences, 1216 motion trajectories were produced with our tracking algorithm. Of these, 1184 motion trajectories were seen to be correct. So, the correct rate of tracking for the tested video sequences is 97.4%. This indicates that the proposed tracking algorithm is suitable for traffic surveillance. The major reasons why 2.6% trajectories are unacceptable are local disturbances from groups of pedestrians and long lasting occlusions between moving vehicles. Our tracker runs at the speed of 5-10 frames in a second on a P4-1.8-GHz computer with a moderate number of vehicles presenting in the scene. More details on the performance of the tracking algorithm are given in <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Learning of Activity Models</head><p>The results of learning activity models are displayed below in both quantitative form and visual form. First, we statistically compare the spectral approach with the fuzzy -means approach. Then, some activity learning results are visually shown.</p><p>In the experiments, we use the tightness and separation co-   tude, but is also much more stable. The values of also tell us how many clusters we should select for spectral clustering. From Fig. <ref type="figure" target="#fig_6">6</ref>, we find a leap in as the number of clusters increases from 17 to 18. This suggests that the proper number of clusters for the first layer of spatial-based spectral clustering is 17.</p><p>Fig. <ref type="figure" target="#fig_7">7</ref> shows the results of the second layer of spatial-based clustering. Fig. <ref type="figure" target="#fig_7">7</ref>(a) illustrates the best result of the fuzzy -means clustering in ten runs, and Fig. <ref type="figure" target="#fig_7">7</ref>(b) the result of spectral clustering. In Fig. <ref type="figure" target="#fig_7">7</ref>(a) and (b), each cluster is represented by the template trajectory overlapped on the original trajectories. As shown in Fig. <ref type="figure" target="#fig_7">7</ref>(a), some lanes or routes are not separated using the fuzzy -means clustering. However, as shown in Fig. <ref type="figure" target="#fig_7">7</ref>(b), all lanes and routes are correctly separated using the spectral clustering.</p><p>Fig. <ref type="figure" target="#fig_4">8</ref> shows some contrasts between the results of the first and second layers of spatial-based spectral clustering. Some dominant paths of vehicle motion extracted using the first layer of spatial-based spectral clustering are shown in Fig. <ref type="figure" target="#fig_4">8</ref>(a), represented in the envelope form. There are more extracted dominant paths than displayed here. We just choose the six most frequent paths for better visualization. Moreover, to avoid overlapping, the envelopes shown here are narrower than the actual ones. Fig. <ref type="figure" target="#fig_8">8(b)</ref> shows some results of the second layer of spatial-based spectral clustering with the envelope representation, where each of the two straight dominant paths shown in Fig. <ref type="figure" target="#fig_8">8(a</ref>) is further divided into three lanes. Fig. <ref type="figure" target="#fig_9">9</ref> shows the 76 activity models finally learned. The spatial-temporal template trajectories corresponding to these activity models are represented by white lines. As shown in Fig. <ref type="figure" target="#fig_9">9</ref>, the learned activity models are consistent with the sample trajectories, so the results can be treated as satisfactory.</p><p>In our experiment, 156 keywords are assigned manually to these activity models. We only need to manually organize the predicates (containing appropriate verbs) to describe the 76 activity models. It is not necessary to annotate the 1216 activities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Retrieval</head><p>In the following, we illustrate visually and statistically the retrieval results for keywords-based queries, multiple object queries, and sketch-based queries.</p><p>1) Keywords-Based Retrieval: With the learned activity models, we can retrieve videos at the semantic level. Fig. <ref type="figure" target="#fig_10">10</ref> shows an example of keyword-based video retrieval. If the query of an user only contains the keyword of "turned left," the vehicle activities shown in Fig. <ref type="figure" target="#fig_10">10(a)-(e</ref>) are all matched. The video clips which show these activities are retrieved, and the associated frames are located and supplied to the user for browsing. When a query from a user contains more descriptive keywords such as "turned left and then ran toward west," the activities shown in Fig. <ref type="figure" target="#fig_10">10(a)-(d</ref>) are eliminated due to the nonmatching of the semantic descriptions, and the activities shown in Fig. <ref type="figure" target="#fig_10">10</ref>(e) are the retrieval results. If static information about objects is further added into the query, for example "a white car" is specified, by matching to the item of Obj_Color in activity descriptors, the first, second, and third vehicle activities shown in Fig. <ref type="figure" target="#fig_10">10</ref>(e) are eliminated. Only the fourth activity shown in Fig. <ref type="figure" target="#fig_10">10</ref>(e) and the video clip which shows this activity are retrieved. From this hierarchical query example, we see that our video retrieval can retrieve activities with different semantic levels.</p><p>For the query "a red car turned left," the keyword "turned left" is matched to the semantic description item of the corresponding activity models, and the keyword "red" is matched to the Obj_Color item in the corresponding activities. There are 40 activities found, of which 35 ones satisfy the query. Fig. <ref type="figure" target="#fig_11">11</ref> shows the precision-recall curve for this retrieval, where the size of the return list is increased in steps of length 5. From the precision-recall curve, we can see that the precision is maintained above 80%. The retrieval result overall satisfies the requirement of the query.   For the query "a white car ran from south to north by the right lane," the keywords "ran from south to north by the right lane" are matched to the semantic description item of the corresponding activity models, and the keyword "white" is matched to the Obj_Color item in the corresponding activities. There are 284 activities found, of which 280 ones satisfy the query. Fig. <ref type="figure" target="#fig_12">12</ref> shows the precision-recall curve for this retrieval, where the size of the return list is increased in steps of length 10. The precision-recall curve shows to be windless. The retrieval result is good.</p><p>2) Multiple Object Query: For multiple object queries, the "succession" and "simultaneity" restrictions are considered in our experiments. For each restriction, retrieval results are ordered in depth-first and breadth-first ways.</p><p>a) "Succession" restriction: We consider the following two-object query: "a car ran from south to north" and "a car turned right" where the restriction of succession of beginning time is used. There are 29 pairs of activities found, of which 22 ones satisfy the query. Fig. <ref type="figure" target="#fig_13">13</ref> shows the precision-recall curve for this retrieval with the depth-first permutation order (introduced in Section IV-B1b), where the size of the return list is increased in steps of length 3. The precision values are all above 50%. The precision-recall curve illustrates that there is a big drop at the second point. The reason for the big drop is that a trajectory satisfying the first object query is assigned to an incorrect cluster by the algorithm for clustering trajectories. Fig. <ref type="figure" target="#fig_14">14</ref> shows the precision-recall curve with the breadth-first permutation order. From Fig. <ref type="figure" target="#fig_14">14</ref>, we can see that the curve becomes smoother; and the precision values are all above 60%. The precision-recall curve is overall improved.</p><p>For the query with three objects: "a car ran from east to south," "car turned right," and "a car ran from south to north," with the succession restriction, 99 triples of activities are found, of which 89 ones satisfy the query. Figs. <ref type="figure" target="#fig_15">15</ref> and<ref type="figure" target="#fig_16">16</ref> show the precision-recall curves for the retrievals with the depth-first and breadth-first permutation orders respectively, where the size of the return list is increased in steps of length 9. For the curve with the depth-first order, the precision values are overall above 80%, except the first value lying between 70% and 80%. For the curve with the breadth-first order, all the precision values are above 80%. So, the retrieval result with the breadth-first order is better than that with the depth-first order.</p><p>b) "Simultaneity" restriction: We replace the "succession" restriction in the above two object queries with the "simultaneity" restriction. For the two-object query, there are 14 matched pairs of activities, of which 12 ones satisfy the query. Figs. 17 and 18 show the precision-recall curves with the depth-first and breadth-first orders, respectively, where the size of the return list is increased in unit steps. It is shown that the lowest precision in both the curves is 50%. However, the precision values in the curve with the breadth-first order are overall higher than those with the depth-first order.   For the three-object query with the simultaneity restriction, there are only two matched triples of activities, both of which satisfy the query. Fig. <ref type="figure" target="#fig_19">19</ref> shows the precision-recall curve. The precision of the retrieval is 100%.</p><p>3) Sketch-Based Query: Fig. <ref type="figure" target="#fig_1">20</ref> shows a trajectory drawn by a user. For this query, 153 activities are found, of which 152 ones satisfy the query. Fig. <ref type="figure" target="#fig_20">21</ref> shows the precision-recall curve for this retrieval, where the size of the return list is increased in steps of length 10. Fig. <ref type="figure" target="#fig_21">22</ref> shows another trajectory drawn by a user. For this query, five activities are found, all of which satisfy the query. Fig. <ref type="figure" target="#fig_22">23</ref> shows the precision-recall curve for this retrieval, where the size of the return list is increased in unit steps. From these two examples, we can see that the retrieval results         effectively, and the workload of manual annotation is greatly reduced. The framework has been experimentally tested in a crowded traffic scene, with good results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Our framework for surveillance video retrieval.</figDesc><graphic coords="2,115.14,66.66,360.00,123.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Hierarchical spatial-based clustering.</figDesc><graphic coords="5,335.16,67.06,186.00,122.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Hierarchical structure of semantic indexing.</figDesc><graphic coords="7,146.64,66.78,300.00,171.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Permutation order of retrieval results.</figDesc><graphic coords="8,103.62,66.30,120.00,114.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>efficient in ( 8 )</head><label>8</label><figDesc>to evaluate the performance of clustering. Smaller values of indicate superior performance. For both fuzzy means clustering and spectral clustering, we examine the values of in the first layer of spatial-based clustering, as the number of clusters is changed. The test is repeated for ten times for a given number of clusters. The results are shown in Fig. 6. In Fig. 6, only a single line is used to show the results of spectral clustering, because the results are the same for all times we repeat the test. However, in the fuzzy -means clustering, the value of always varies when the test is repeated. The three lines show respectively the minimal, average and maximal values of . Large variances are observed between the results acquired in different testing times. From Fig. 6, we can see that the values of spectral clustering are much smaller than those of the fuzzy -means clustering. So, spectral clustering not only outperforms the fuzzy -means clustering by one order of magni-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Results of tracking: (a) Pixel segmentation and clustering in one frame; (b) trajectories.</figDesc><graphic coords="9,146.64,66.90,300.00,120.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Comparison between fuzzy c-means clustering and spectral clustering.</figDesc><graphic coords="9,62.28,233.76,205.00,159.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Results of second layer of spatial-based clustering: (a) fuzzy c-means; (b) spectral clustering.</figDesc><graphic coords="10,145.14,66.66,300.00,126.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Contrast between first and second layers of spatial-based spectral clustering: (a) first layer; (b) second layer.</figDesc><graphic coords="10,145.14,227.00,300.00,124.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Results of learning activity models.</figDesc><graphic coords="10,82.62,391.84,162.00,122.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Results of keyword-based queries: (a) "Turned left and then ran toward north." (b) "Turned left and then turned round toward south." (c) "Turned left and then turned round toward north." (d) "Turned left; turned round toward north; and then parked." (e) "Turned left and then ran toward west."</figDesc><graphic coords="11,54.42,68.70,221.40,559.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Precision-recall curve for the keywords-based query 1.</figDesc><graphic coords="11,333.60,66.98,189.00,142.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Precision-recall curve for the keywords-based query 2.</figDesc><graphic coords="11,338.16,243.24,180.00,135.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Precision-recall curve for two-object query with succession restriction and depth-first order.</figDesc><graphic coords="11,329.16,412.34,198.00,145.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Precision-recall curve for two-object query with succession restriction and breadth-first order.</figDesc><graphic coords="12,70.62,66.86,186.00,139.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Precision-recall curve for three-object query with succession restriction and depth-first order.</figDesc><graphic coords="12,71.22,251.06,184.00,136.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. Precision-recall curve for three-object query with succession restriction and breadth-first order.</figDesc><graphic coords="12,69.90,431.86,187.00,140.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 17 .</head><label>17</label><figDesc>Fig. 17. Precision-recall curve for two-object query with simultaneity restriction and depth-first order.</figDesc><graphic coords="12,332.64,65.82,186.00,141.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 18 .</head><label>18</label><figDesc>Fig. 18. Precision-recall curve for two-object query with simultaneity restriction and breadth-first order.</figDesc><graphic coords="12,335.52,419.48,180.00,145.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 19 .</head><label>19</label><figDesc>Fig. 19. Precision-r</figDesc><graphic coords="12,347.64,589.28,156.00,118.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Fig. 21 .</head><label>21</label><figDesc>Fig. 21. Precision-recall curve for Fig. 20.</figDesc><graphic coords="13,81.12,66.94,168.00,125.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Fig. 22 .</head><label>22</label><figDesc>Fig. 22. Trajectory 2 drawn by a user.</figDesc><graphic coords="13,87.12,225.80,156.00,118.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Fig. 23 .</head><label>23</label><figDesc>Fig. 23. Precision-recall curve for Fig. 22.</figDesc><graphic coords="13,75.12,377.66,180.00,136.00" type="bitmap" /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A fully automated content-based video search engine supporting spatiotemporal queries</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="602" to="615" />
			<date type="published" when="1998-09">Sep. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Segmented trajectory based indexing and retrieval of video data</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">I</forename><surname>Bashir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Khokhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schonfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Image Processing</title>
		<meeting>Int. Conf. Image essing</meeting>
		<imprint>
			<date type="published" when="2003-09">Sep. 2003</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="623" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Motion indexing of video</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sahouria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zakhor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Image Processing</title>
		<meeting>Int. Conf. Image essing<address><addrLine>Santa Barbara, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-10">Oct. 1997</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="526" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Motion trajectory matching of video objects</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="2000-01">Jan. 2000</date>
			<biblScope unit="volume">3972</biblScope>
			<biblScope unit="page" from="544" to="553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Content-based event retrieval using semantic scene interpretation for automated traffic surveillance</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transport. Syst</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="151" to="163" />
			<date type="published" when="2001-09">Sep. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Models for motion-based video indexing and retrieval</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dagtas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Khatib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghafoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Kashyap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="88" to="101" />
			<date type="published" when="2000-01">Jan. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Traffic monitoring and accident detection at intersections</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kamijo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ikeuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sakauchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transport. Syst</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="108" to="118" />
			<date type="published" when="2000-06">Jun. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tracking non-rigid, moving objects based on color cluster flow</title>
		<author>
			<persName><forename type="first">B</forename><surname>Heisele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Kressel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1997-06">Jun. 1997</date>
			<biblScope unit="page" from="257" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">From cluster tracking to people counting</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E C</forename><surname>Pece</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Workshop Performance Evaluation of Tracking and Surveillance</title>
		<meeting>IEEE Workshop Performance Evaluation of Tracking and Surveillance<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-06">Jun. 2002</date>
			<biblScope unit="page" from="9" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">View-based interpretation of real-time optical flow for gesture recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conf. Automatic Face and Gesture Recognition</title>
		<meeting>IEEE International Conf. Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="416" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Monitoring crowded traffic scenes</title>
		<author>
			<persName><forename type="first">B</forename><surname>Maurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Masoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papanikolopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Intelligent Transportation Systems</title>
		<meeting>IEEE Int. Conf. Intelligent Transportation Systems<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="19" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Reducing the time complexity of the fuzzy c-means algorithm</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Colen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hutcheson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Fuzzy Syst</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="267" />
			<date type="published" when="2002-04">Apr. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Complexity reduction for large image processing</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Bezdek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. B, Cybern</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="598" to="611" />
			<date type="published" when="2002-10">Oct. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast accurate fuzzy clustering through data reduction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Eschrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">O</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Goldgof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Fuzzy Syst</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="262" to="270" />
			<date type="published" when="2003-04">Apr. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Double exponential smoothing: An alternative to Kalman filter-based predictive tracking</title>
		<author>
			<persName><forename type="first">J</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Laviola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Immersive Projection Technology and Virtual Environments</title>
		<meeting>Immersive Projection Technology and Virtual Environments</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="199" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A system for learning statistical motion patterns</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1450" to="1464" />
			<date type="published" when="2006-09">Sep. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi feature path modeling for video surveillance</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">N</forename><surname>Junejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Pattern Recognition</title>
		<meeting>Int. Conf. Pattern Recognition<address><addrLine>Cambridge, U.K.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">II</biblScope>
			<biblScope unit="page" from="716" to="719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning the distribution of object trajectories for event recognition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hogg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="609" to="615" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Application of the self-organizing map to trajectory classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hunter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Workshop Visual Surveillance</title>
		<meeting>IEEE Int. Workshop Visual Surveillance</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="77" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning patterns of activity using real-time tracking</title>
		<author>
			<persName><forename type="first">C</forename><surname>Stauffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E L</forename><surname>Grimson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="747" to="757" />
			<date type="published" when="2000-08">Aug. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning activity patterns using fuzzy self-organizing neural network</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. B, Cybern</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1618" to="1626" />
			<date type="published" when="2004-06">Jun. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Path detection in video surveillance</title>
		<author>
			<persName><forename type="first">D</forename><surname>Makris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="895" to="903" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Dietterich</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Becker</surname></persName>
		</editor>
		<editor>
			<persName><surname>Ghahramani</surname></persName>
		</editor>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Integrated browsing and querying for image databases</title>
		<author>
			<persName><forename type="first">S</forename><surname>Simone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ramesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Multimedia</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="26" to="39" />
			<date type="published" when="2000-07">Jul. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Querying video data by spatio-temporal relationships of moving object traces</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chikashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yoshihiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Katsumi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th IFIP 2.6 Working Conf. Visual Database Systems</title>
		<meeting>6th IFIP 2.6 Working Conf. Visual Database Systems<address><addrLine>Brisbane, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-05">May 2002</date>
			<biblScope unit="page" from="357" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rx for semantic video database retrieval</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dimitrova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Golshani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd ACM Int. Conf. Multimedia</title>
		<meeting>2nd ACM Int. Conf. Multimedia<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="219" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Incremental recognition of traffic situations from video image sequences</title>
		<author>
			<persName><forename type="first">M</forename><surname>Haag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-H</forename><surname>Nagel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="153" />
			<date type="published" when="2000-01">Jan. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On the use of motion concepts for top-down control in traffic scenes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mohnhaupt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Computer Vision</title>
		<meeting>Eur. Conf. Computer Vision<address><addrLine>Antibes, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="598" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Context knowledge and search control issues in object-oriented prolog-based image understanding</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Pau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="279" to="290" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Automatic symbolic traffic scene analysis using belief networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ogasawara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th Nat. Conf. Artificial Intelligence</title>
		<meeting>12th Nat. Conf. Artificial Intelligence<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="966" to="972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Visual surveillance in a dynamic and uncertain world</title>
		<author>
			<persName><forename type="first">H</forename><surname>Buxton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="431" to="459" />
			<date type="published" when="1995-10">Oct. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-agent visual surveillance of dynamic scenes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Remagnino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="529" to="532" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dynamic image sequence analysis using fuzzy measures</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Bruton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Bezdek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Bartley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern., B: Cybern</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="557" to="571" />
			<date type="published" when="2001-08">Aug. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Natural language description of human activities from video images based on concept hierarchy of actions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kojima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fukunaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="171" to="184" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Image understanding for visual surveillance applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Thonnat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rota</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Workshop Cooperative Distributed Vision</title>
		<meeting>Int. Workshop Cooperative Distributed Vision</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="51" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Monitoring human behavior from video taken in an office environment</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="833" to="846" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Motion prediction for moving objects: A statistical approach</title>
		<author>
			<persName><forename type="first">D</forename><surname>Vasquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fraichard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Robotics Automation</title>
		<meeting>IEEE Int. Conf. Robotics Automation</meeting>
		<imprint>
			<date type="published" when="2004-05-01">Apr. 26-May 1, 2004</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="3931" to="3936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Since April 2000, he has been with the National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing. Currently, he is a Professor and a Ph.D. Student Supervisor in the laboratory. His research interests are in visual surveillance, neural networks, filtering of Internet objectionable information, retrieval of multimedia, and understanding of Internet behaviors. He has published more than 80 papers in national and international journals and international conferences</title>
	</analytic>
	<monogr>
		<title level="m">Dan Xie received the B.S. degree in automatic control and the M.S. degree from the Beijing University of Aeronautics and Astronautics (BUAA)</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Sc</surname></persName>
		</editor>
		<meeting><address><addrLine>China; China; Beijing, China; Amherst; China; Beijing</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-04">April 1998 to March 2000. 2001 and 2004. 2001. 2004</date>
		</imprint>
		<respStmt>
			<orgName>Computer Science and Engineering, Zhejiang University ; Research Fellow with the Institute of Computer Science and Technology and Founder of the Research and Design Center, Peking University ; Computer Science Department, University of Massachusetts ; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences ; Research School of Information Sciences and Engineering, Australian National University</orgName>
		</respStmt>
	</monogr>
	<note>His research interests include pattern recognition and computer vision</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Her research interests include computer vision, pattern recognition, and artificial neural network. Steve Maybank (SM&apos;05) received the B.A. degree in mathematics from King&apos;s College, Cambridge, U.K., in 1976, and the Ph.D. degree in computer science from Birkbeck College</title>
	</analytic>
	<monogr>
		<title level="m">EPSRC Industrial Fellow in the Department of Engineering Science, University of Oxford</title>
		<title level="s">She is currently pursuing the M.S. degree at the Institute of Semiconductors, Chinese Academy of Sciences</title>
		<meeting><address><addrLine>China; Beijing; London, U.K.; Frimley, U.K.; Wembley, U.K.; Oxford, U.K. In; Reading, U.K., as; Birkbeck College</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1988">July 2006. 1988. 1989. 1993 to 1995. 1995. 2004</date>
		</imprint>
		<respStmt>
			<orgName>Department of Electronics, Peking University ; University of London ; Computer Science and Information Systems</orgName>
		</respStmt>
	</monogr>
	<note>a Lecturer in the Department of Computer Science. His research interests include the geometry of multiple images, camera calibration, visual surveillance, information geometry, and the applications of statistics to computer vision</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
