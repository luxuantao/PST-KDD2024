<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Simple Episodic Linear Probe Improves Visual Recognition in the Wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuanzhi</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Baidu Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">ReLER Lab</orgName>
								<orgName type="institution" key="instit1">AAII</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
							<email>linchao.zhu@uts.edu.au</email>
							<affiliation key="aff1">
								<orgName type="laboratory">ReLER Lab</orgName>
								<orgName type="institution" key="instit1">AAII</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaohan</forename><surname>Wang</surname></persName>
							<email>xiaohan.wang@zju.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<email>yangyics@zju.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Simple Episodic Linear Probe Improves Visual Recognition in the Wild</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Understanding network generalization and feature discrimination is an open research problem in visual recognition. Many studies have been conducted to assess the quality of feature representations. One of the simple strategies is to utilize a linear probing classifier to quantitatively evaluate the class accuracy under the obtained features. The typical linear probe is only applied as a proxy at the inference time, but its efficacy in measuring features' suitability for linear classification is largely neglected in training. In this paper, we propose an episodic linear probing (ELP) classifier to reflect the generalization of visual representations in an online manner. ELP is trained with detached features from the network and re-initialized episodically. It demonstrates the discriminability of the visual representations in training. Then, an ELP-suitable Regularization term (ELP-SR) is introduced to reflect the distances of probability distributions between the ELP classifier and the main classifier. ELP-SR leverages a re-scaling factor to regularize each sample in training, which modulates the loss function adaptively and encourages the features to be discriminative and generalized. We observe significant improvements in three real-world visual recognition tasks: fine-grained visual classification, long-tailed visual recognition, and generic object recognition. The performance gains show the effectiveness of our method in improving network generalization and feature discrimination.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep neural networks have achieved impressive improvements in visual recognition. The neural networks trained on large-scale visual recognition datasets, e.g., Ima-* This work was performed at Baidu Research. geNet <ref type="bibr" target="#b29">[30]</ref>, OpenImages <ref type="bibr" target="#b26">[27]</ref>, demonstrate remarkable generalization capabilities. The learned visual representations are compact and enjoy strong discriminability. Many works have been conducted to theoretically explain the rationale behind deep networks' generalization <ref type="bibr" target="#b59">[60]</ref>, but this problem is still largely unsolved and remains to be investigated.</p><p>There are a few analytical tools to probe deep neural networks' learning and generalization capabilities. Early works utilize visualization tools to understand the optimized parameters or employ dimensionality reduction techniques to visualize the quality of learned representations <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b58">59]</ref>. Though helpful, such visualization techniques only provide qualitative inspections on deep networks <ref type="bibr" target="#b7">[8]</ref>. Some works develop geometric probes to analyze the geometric properties of object manifold and connect object category manifolds' linear separability with the underlying geometric properties <ref type="bibr" target="#b45">[46]</ref>. These methods reveal the structure of memorization from different layers in deep networks but only probe layer capacity at the inference time, as shown in Fig. <ref type="figure" target="#fig_2">1 (a)</ref>.</p><p>Another simple strategy is to perform linear probing. One can use linear probes to evaluate the feature's quality quantitatively. Since the discrimination capability of linear classifiers is low, linear classifiers heavily rely on the quality of the input representation to obtain good classification accuracy <ref type="bibr" target="#b2">[3]</ref>. Alain et al. <ref type="bibr" target="#b0">[1]</ref> use linear probes to examine the dynamics of intermediate layers. The linear probe is a linear classifier taking layer activations as inputs and measuring the discriminability of the networks. This linear probe does not affect the training procedure of the model. Recently, linear probes <ref type="bibr" target="#b2">[3]</ref> have been used to evaluate feature generalization in self-supervised visual representation learning. After representation pre-training on pretext tasks <ref type="bibr" target="#b2">[3]</ref>, the learned feature extractor is kept fixed. The linear probe classifier is trained on top of the pre-trained feature representations. Though conceptually straightforward, linear probes are effective and have been widely used in measuring the discriminability of visual representations. Noticeably, the linear probing classifier is only used in testing. A natural question arises: can we utilize linear probes during training and bring the signal from the linear probes to regularize the model training?</p><p>In this paper, we introduce a simple strategy to regularize the network to be immediately plausible for an episodic linear probing classifier. Our simple framework (Fig. <ref type="figure" target="#fig_0">1</ref> (b)) consists of a main classifier, an episodic linear probing classifier, and a regularization term. The regularization term considers the relation between the main classifier and the episodic linear probing classifier, which effectively penalizes examples that are not immediately plausible for episodic linear probes.</p><p>First, we propose an episodic linear probing (ELP) classifier to estimate the discrimination of visual representation in an online way. Similar to the existing linear probes <ref type="bibr" target="#b0">[1]</ref>, ELP is applied on top of the last layer of a deep network. ELP classifier is trained to classify the detached features into the same label space as a regular classifier. Different from <ref type="bibr" target="#b0">[1]</ref>, ELP is applied during model training. It is episodically re-initialized at each epoch. This maintains its simplicity, avoids classifier overfitting, and prevents the classifier from memorizing features. ELP implicitly reflects the feature discriminability and separability <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref>. If the ELP classifier can quickly classify the feature points, it indicates that the given features are easily separable and would potentially be more generalizable.</p><p>Second, we introduce a penalization for less suitable examples for an episodic linear probe. Intuitively, given a training example, if the episodic linear probe and the main classifier contradict each other, e.g., the episodic linear probe receives a low prediction score while the main clas-sifier produces a high prediction score, it indicates that the main network exhibits overfitting on the given instance and a larger penalty should be enforced for proper regularization. Thus we design an ELP-suitable Regularization term (ELP-SR) to mitigate the intrinsic model bias and improve the linear separability of the learned features. ELP-SR sets a re-scaling factor to each instance and adaptively modulates the cross-entropy loss to avoid overfitting. The re-scaling factor considers the deviation between an example's predictive score from the main classifier and ELP classifier, which, to a certain extent, assesses the example's suitability for linear classification.</p><p>Without bells and whistles, our method achieves significant improvements for visual recognition tasks in the wild, providing consistent gains for fine-grained, long-tailed, and generic visual recognition. The fine-grained visual recognition datasets often contain high inter-class similarities. The long-tailed visual recognition datasets exhibit long-tailed data distribution, which is realistic in real-world recognition problems. We extensively evaluate the generalization performance on six standard datasets. The results indicate that our strategy empowers various deep networks with better discrimination and mitigates the model bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Various works have been proposed to learn visual representation based on deep learning. In diverse recognition tasks in the wild, deep neural networks possess the powerful ability to learn and represent images to high-dimensional features. With the high-quality features, some simple classifiers <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b55">56]</ref> are components to recognize the samples. Further, the quality of features is influenced by many factors. We roughly divided the factors into three aspects: data processing, network design, and training manner. Though the exact effect of representation learning <ref type="bibr" target="#b59">[60]</ref> remains to be investigated, numerous researchers keep exploring and propose many valuable solutions.</p><p>For data processing, large-scale datasets provide considerable network samples and are the most straightforward way to improve representation. Benefiting from the powerful ability of networks, taking large-scale datasets as inputs lead the network to learn various samples and memorize plenty of properties for discriminating. Some diverse and hard examples may be difficult in a limited data scale <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b34">35]</ref>. Under the view of larger scales of collections, it is always possible for the network to mine particular patterns. Besides directly collecting real data, pre-processing <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b63">64]</ref> or generating data <ref type="bibr" target="#b62">[63]</ref> are also equivalent. Various augmentations <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b49">50]</ref> enforce the networks to solve problems with higher requirements and urge the network to be generalized to different conditions.</p><p>Moreover, well-designed network structures also dramatically boost representation and become the hottest di-rection in recent years. Diverse methods constantly emerge like skip-connection <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22]</ref>, fusing channels <ref type="bibr" target="#b47">[48]</ref>, attention strategies <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b36">37]</ref>, architecture searching <ref type="bibr" target="#b4">[5]</ref>, transformers <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b53">54]</ref>, etc. With the same inputs, these methods explore different directions to boost the network's capacity. Meanwhile, almost all kinds of visual tasks <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b32">33]</ref> develop further with better networks.</p><p>Furthermore, besides data processing and network designs, the training manner is also crucial for visual representation. It contains various aspects like the optimizer <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b38">39]</ref>, regularization <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>, learning manner <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b43">44]</ref>, etc. In this direction, regularization plays an important role. It can be reflected in the loss function <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b31">32]</ref>, training strategies <ref type="bibr" target="#b17">[18]</ref>, etc., and is general to various networks and datasets. A proper regularization can leverage the network to learn better visual representation, for example, avoiding overfitting <ref type="bibr" target="#b31">[32]</ref>, explicit attention to the target <ref type="bibr" target="#b8">[9]</ref>, better diversity <ref type="bibr" target="#b13">[14]</ref>, etc. Vikash et al. <ref type="bibr" target="#b40">[41]</ref> propose an interesting margin to describe the separability of features. Rather than focusing on the accuracy of the classifier, the quality of features can be reflected through immediate suitability. The more discriminative features are considered more than memorable by the classifier.</p><p>In our work, going further with the immediate suitability, we propose an episodic linear probing (ELP) classifier to reflect the generalization of visual representation online. ELP can be applied as a novel regularization to encourage the network to produce more discriminative features. Rather than re-weighting according to samples' easiness <ref type="bibr" target="#b24">[25]</ref> or a meta set with iterative learning <ref type="bibr" target="#b43">[44]</ref>, we design an ELPsuitable regularization (ELP-SR) and leverage the ELP-SR to the regular loss function. Experimental results show that ELP-SR generally improves the performances of networks in three different benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this work, we introduce an auxiliary episodic linear probing classifier to provide additional regularization for better representation learning. As illustrated in Fig. <ref type="figure">2</ref>, our framework consists of three components, i.e., a deep neural network, a main linear classifier, and an episodic linear probing classifier. We illustrate our episodic linear probing classifier in Section 3.1. The details of the ELP-suitable regularization are introduced in Section 3.2. In Section 3.3, we describe the training and inference strategies of the model. between p and the ground-truth distribution y. Formally, we denote the typical training procedure below:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Episodic Linear Probing Classifier</head><formula xml:id="formula_0">h = F (x),<label>(1)</label></formula><formula xml:id="formula_1">p = Cls(h),<label>(2)</label></formula><formula xml:id="formula_2">? ce (p, y) = - C j=1 y j log(p j ),<label>(3)</label></formula><p>where C is the number of categories. y j = 1 if j is the ground-truth label. Otherwise, y j = 0. p j is the prediction score of class j. The feature extractor and the classifier are jointly optimized end-to-end using back-propagation.</p><p>Test-time Linear Probing. Linear probing is usually built to assess the quality of deep representations after the neural network is sufficiently trained <ref type="bibr" target="#b0">[1]</ref>. That amounts to training an auxiliary linear classifier on top of the pre-trained features. The parameters of the linear probe are randomly initialized, while the original classifier layer is neglected. The pre-trained backbone is frozen and not trained during linear probing. Since the complexity of the auxiliary classifier is not sufficient to provide additional discrimination, the classification performance heavily depends on the quality of the feature representations. Thus, predictive scores of the auxiliary linear classifier can probe the discrimination of the input features. During implementation, a linear probe can be extended to a Multi-Layer Perceptron (MLP) probe where the linear layer is replaced with a MLP <ref type="bibr" target="#b20">[21]</ref>.</p><p>The existing probes are mainly used during inference time, either providing quantitative evaluation on pre-trained features or interpreting intermediate layers <ref type="bibr" target="#b14">[15]</ref>. This drives us to incorporate a linear probe during training and borrow the simple nature of the linear probe for network regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Episodic Linear Probing Classifier</head><p>Motivated by the efficacy of test-time linear probe in assessing representation quality, we aim to design a linear probing classifier in training to measure the discrimination of a neural network and further leverage the probing signal to empower representation learning. We introduce an episodic linear probing (ELP) classifier and discuss its weight update scheme in training. Detached Linear Probing Classifier in Training. When incorporating a linear probing classifier in training, we need to maintain its independence from the main classifier. While keeping the main classifier and the backbone network unchanged, we build a new episodic linear probing classifier on top of the feature extractor. We stop the linear probe classifier's gradient to back-propagate to the backbone network. This helps the linear probe not be biased by the main classifier and produce a neutral evaluation of the discrimination of the feature representations.</p><p>Formally, the episodic linear probing classifier is trained to classify the features into C categories using the same labels assigned to the main classifier, p = Cls main (h), (4) q = Cls elp (stop-grad(h)),</p><p>(5)</p><formula xml:id="formula_3">? main (x, y) = ? ce (p, y), (6) ? elp (x, y) = ? ce (q, y). (<label>7</label></formula><formula xml:id="formula_4">)</formula><p>Cls main is the main classifier, and it produces a probability prediction of p. Cls elp is the linear probe classifier, and it generates a probability prediction of q. Cls elp is trained in an online manner, but its optimization is detached from the main branch. "stop-grad'" indicates that feature h is detached to train Cls elp . The gradients from the ELP classifier are unavailable to the backbone and main classifier, and vice versa. The main difference between the detached linear classifier and the test-time linear probe is that the features of the detached linear classifier are adaptively changed by the network, while the features of the test-time linear classifier are always fixed. Episodic weight re-initialization overcomes overfitting.</p><p>Training the detached linear classifier with the same number of epochs as the main classifier would lead to the detached linear classifier overfitting the features. This overfitting should be avoided because the simple linear probe is supposed to reflect the discrimination of the features. If the ELP classifier memorizes all samples, it would not be competent to evaluate the features effectively. To prevent the ELP classifier from overfitting the training data, we reinitialize its parameters episodically every I epochs where I indicates episodic re-initialization interval. Specifically, given a linear classifier parameterized with W and b, where W is the projection matrix, and b is the bias, both W and b are randomly re-initialized at the interval of I epochs. The episodic linear probe enables us to measure and understand the feature discriminability throughout the training process. A larger value of I enforces the ELP classifier to be better trained, but it makes the ELP classifier more likely to be overfitted. In contrast, the ELP classifier is underfitted, if I is too small. An under-fitted ELP classifier may not well describe the generalization capabilities of the features. In practice, we set I as a hyper-parameter. Empirically, I = 2 achieves consistent good probing performances across datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The ELP-Suitable Regularization</head><p>ELP-Suitable Regularization through loss modulation. ELP assesses the features' separability in an online way. The standalone ELP is detached from the backbone and does not influence the main network. In this paper, we aim to utilize the prediction from the auxiliary ELP classifier to effectively improve the discriminability of the main branch. However, the design of this regularization is not straightforward. Considering the episodic nature of the ELP classifier, ELP's prediction is periodic and not as confident as the main classifier. If the regularization is not well constructed, the performance of the main branch would be severely impaired.</p><p>In this paper, we introduce a simple formulation that modulates the cross-entropy loss with an adaptive factor ?,</p><formula xml:id="formula_5">L ELP -SR = B i=1 stop-grad(? i ) * ? ce (p i , y i ),<label>(8)</label></formula><p>where p i is the prediction probability from the main classifier, B is the batch size. The scalar factor ? i is assigned to each instance to modulate its cross-entropy loss adaptively. ? measures the main network's suitability for an ELP classifier. If an instance is not suitable for the ELP classifier, e.g., the instance may be not discriminative, or an out-ofdistribution data point, ? imposes a relatively large value so that the network would pay more attention to this instance. Our ELP-Suitable Regularization (ELP-SR) effectively mitigates the intrinsic model bias and regularizes the network towards better linear separability. We detach the gradients from ? so that the factor only influences the magnitude of the loss gradients, but the gradient orientation is not altered. This makes the optimization progress relatively easy and stable. The strategy works surprisingly well in practice. The instantiation of the ELP-SR factor. As aforementioned, ? aims to measure the main network's suitability for an ELP classifier. Given an instance x with the label c, we instantiate the ELP-SR factor by considering the prediction score of the main classifier (p c ) and the prediction of the ELP classifier (q c ). We utilize two elements when we construct the regularization factor ?.</p><p>First, the distance metric (D) between the prediction of the ELP classifier and the prediction of the main classifier should be concerned. The distance should reflect the main classifier's confidence gap compared to the ELP classifier.</p><p>If the distance is minimized, the main classifier is pushed to act like a less-trained linear classifier. Relatively, The features would be remarkably discriminative if a less-trained classifier is already sufficient for recognizing, Therefore, this metric encourages the main classifier to become simpler, promoting the features to be more discriminative. We instantiate D by simply computing the ? 1 distance between p c and q c , i.e., D = |p c -q c |.</p><p>Second, we incorporate a normalization metric (R) to reveal the discriminability of both the ELP classifier and the main classifier. The distance metric (D) measures the relative confidence gap, but we should also consider the absolute values of the confidence scores. If the distance between p c and q c is small, but both absolute scores are low, the network has not been well optimized to classify the instance. Thus, we should normalize the distance with a normalization metric. For simplicity, we set R as the average of p c and q c , i.e., R = (p c + q c )/2.</p><p>We formulate the ELP-SR factor ? as,</p><formula xml:id="formula_6">? = ( D R ) ? = ( 2|p c -q c | p c + q c ) ? ,<label>(9)</label></formula><p>where ? smoothly adjusts the rate between D and R. We empirically study other ELP-SR factor variants in the experiment section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training and inference</head><p>In the training phase, we calculate the softmax crossentropy loss for both the main classifier and the ELP classifier. Our ELP-SR loss is summed with these losses. The overall training objective is below,</p><formula xml:id="formula_7">L = B i=1 ? main (p i , y i ) + ? elp (q i , y i ) + ? i * ? ce (p i , y i )<label>(10)</label></formula><p>In the test phase, we remove the auxiliary ELP classifier and only keep the main classifier. The final prediction is obtained only from the main classifier. Our framework does not introduce any additional overhead during testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In the challenges of diverse objects of images in the wild, our method shows significant superiority for generalization. We evaluate three classification tasks, i.e., fine-grained visual recognition, long-tailed recognition, and generic object recognition. First, since the classes in fine-grained recognition are similar, and samples are difficult to be recognized even by humans, the fine-grained recognition task brings extra challenges to learning discriminative features. Second, long-tailed recognition involves the extremely imbalanced distributions of data samples. This requests the method to possess generalization ability and recognize the tailed classes with limited samples. The evaluations of these tasks reveal the advantages of our method in improving visual representations.</p><p>We further evaluate our method on ImageNet-1K to study the generalization ability of ELP-SR. Besides the classification accuracy metric, we also report the results of a k-nearest-neighbor (KNN) classifier on the test set. This further manifests the effectiveness of our method in improving the discriminability of feature representations. Moreover, we provide ablation studies to compare different ?, I, and formulations of the ELP-SR factor. To further demonstrate the ability of the ELP classifier, we present a comparison of the linear classifier's accuracy. The results reflect that the network with ELP-SR produces more discriminative and generalized features.</p><p>To be noticed, for all the tasks, we did NOT introduce any additional annotations nor incorporate extra parameters at the inference time. During testing, only the backbone networks are used to produce predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Fine-grained Visual Recognition</head><p>Classes in fine-grained recognition are similar. They are difficult to distinguish, even for a human. Meanwhile, samples in every class are diverse <ref type="bibr" target="#b1">[2]</ref>. Objects may be shown in various angles, illuminations, occlusions, backgrounds, etc. These induce fine-grained categories to show large intraclass variances, but small inter-class variances <ref type="bibr" target="#b1">[2]</ref>. Samples in fine-grained classification are hard to be generalized and discriminated, which brings difficulties for learning discriminative features by networks. Dataset and Implementation Details. To show the efficacy, we compare the performances on three standard benchmarks: CUB-200-2011 (CUB) <ref type="bibr" target="#b52">[53]</ref>, Stanford Cars (CAR) <ref type="bibr" target="#b27">[28]</ref>, and FGVC-Aircarft (AIR) <ref type="bibr" target="#b35">[36]</ref>.</p><p>Following the same training procedure in <ref type="bibr" target="#b9">[10]</ref>, we adapt ResNet-50 <ref type="bibr" target="#b18">[19]</ref> pre-trained by ImageNet <ref type="bibr" target="#b29">[30]</ref> as the backbone model. As the regular augmentations <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b64">65]</ref> in this task, resizing, random crops, rotations, and horizontal flips are applied. After operating these standard transformations, the final inputs become 448?448 resolutions. Similar to the ResNet50 baseline <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b64">65]</ref>, we train our method for 240 epochs and optimize the loss function by SGD. In our method, we report the results of ? = 3 for all three datasets with D = p c -q c and R = (p c + q c )/2. For CUB, CAR, and AIR, we set I = 2, 2, and 1, respectively. These are the best settings for parameters and will be discussed in the ablation section 4.4. Experimental Results. As in Table <ref type="table" target="#tab_0">1</ref>, our method achieves significant improvements based on the ResNet50 baseline. Without bells and whistles, our results are competitive or even outperform many recent methods with complicated network designs <ref type="bibr" target="#b23">[24]</ref>, additional augmentations <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16]</ref>, or Method Dataset CUB CAR AIR B-CNN <ref type="bibr" target="#b33">[34]</ref> 84.1 91.3 84.1 HIHCA <ref type="bibr" target="#b5">[6]</ref> 85.3 91.7 88.3 RA-CNN <ref type="bibr" target="#b16">[17]</ref> 85.3 92.5 88.2 OPAM <ref type="bibr" target="#b37">[38]</ref> 85.8 92.2 -Kernel-Pooling <ref type="bibr" target="#b12">[13]</ref> 84.7 91.1 85.7 MA-CNN <ref type="bibr" target="#b61">[62]</ref> 86.5 92.8 89.9 MAMC <ref type="bibr" target="#b46">[47]</ref> 86.5 93.0 -HBP <ref type="bibr" target="#b57">[58]</ref> 87.1 93.7 90.3 DFL-CNN <ref type="bibr" target="#b54">[55]</ref> 87.4 93.1 91.7 NTS-Net <ref type="bibr" target="#b56">[57]</ref> 87.5 93.9 91.4 DCL <ref type="bibr" target="#b9">[10]</ref> 87.8 94.5 93.0 PMG <ref type="bibr" target="#b15">[16]</ref> 88.9 95.0 92.8 ACNet <ref type="bibr" target="#b23">[24]</ref> 88.1 94.6 92.5 LIO <ref type="bibr" target="#b64">[65]</ref> 88.0 94.5 92. multi-scale features <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b64">65]</ref>. Merely utilizing naive backbone with ELP-SR in training, the simple backbone networks boost 3.3%, 1.5%, and 2.4% respectively in three datasets which are significant improvements in this task. Boosts in this task reveal that our method effectively improves the networks' ability to discriminate and generalize samples. To further manifest the superiority of our method, more discussions will be presented in 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Long-tailed Visual Recognition</head><p>In long-tail recognition, the data distributions of different classes show extreme imbalance. As the long-tailed distribution, a handful of 'head' classes contain considerable samples, but a large number of 'tail' classes only include limited samples. The networks are biased toward 'head' classes, and the samples in 'tail' classes are hard to be generalized. In this section, we also evaluate the performances of our method under the challenging long-tailed distribution.</p><p>Dataset and Implementation Details. The experiments are operated based on long-tailed CIFAR-10 and CIFAR-100 datasets <ref type="bibr" target="#b28">[29]</ref>. We first produce several versions of longtailed datasets following <ref type="bibr" target="#b6">[7]</ref> under different imbalance ratios, which denotes the ratio between the largest and smallest numbers of samples in classes. We report the results in three kinds of imbalance ratios which are 100, 50, and 10, respectively. To perform fair comparisons, we evaluate our method based on the ResNet-32 baseline from <ref type="bibr" target="#b6">[7]</ref>. in CIFAR-10 of imbalance ratio 100 and 50 are even larger than LDAM <ref type="bibr" target="#b6">[7]</ref>. Moreover, after adapting the normalization from <ref type="bibr" target="#b25">[26]</ref>, the results of our method show more competitiveness in this task. All results in different settings outperform LDAM. Besides, we further investigate our method based on the LDAM <ref type="bibr" target="#b6">[7]</ref>. By minimizing the margin-based boundary considering the generalization <ref type="bibr" target="#b6">[7]</ref>, LDAM is well-designed for long-tailed recognition and boosts the performances dramatically. Meanwhile, our method can achieve higher performances on the foundation of LDAM. Though without specific consideration for the long-tailed distribution, ELP-SR offers general improvements to this task. These results demonstrate that our method helps the network generalize and produce discriminative features against the challenging distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results. As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Generic Visual Recognition on ImageNet</head><p>To reveal the generalization of ELP-SR, we further investigate our method in generic object recognition on the standard benchmark for visual representation. Dataset and Implementation Details. We evaluate ELP-SR on ImageNet-1K <ref type="bibr" target="#b29">[30]</ref>, containing 1.28 million images with 1000 categories. To show the effectiveness and generalization, we apply ELP-SR on different backbone networks, which are ResNet-50 <ref type="bibr" target="#b18">[19]</ref>, ResNet-101 <ref type="bibr" target="#b18">[19]</ref>, ResNet-152 <ref type="bibr" target="#b18">[19]</ref>, BN-Inception <ref type="bibr" target="#b22">[23]</ref>, Inception-V3 <ref type="bibr" target="#b48">[49]</ref>, and Inception-ResNet-V2 <ref type="bibr" target="#b47">[48]</ref>. According to the standard implementations of these works, we adapt SGD with momentum 0.9 as the optimizer. All the networks are trained with the augmentations of random crops and horizontal flips. For ResNet-50, ResNet-101, ResNet-152, and BNinception, we first resize the images to 256?256 resolutions and then randomly crop them to 224 ? 224. For Inception-V3 and Inception-ResNet-V2, we resize to 320 ? 320 and randomly crop to 299 ? 299 as the corresponding implementations in their works <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49]</ref>. As in Table <ref type="table" target="#tab_3">3</ref>, we report top-1 and top-5 accuracy respectively and compare all the backbones with ELP-SR.</p><p>Experimental Results. As in Table <ref type="table" target="#tab_3">3</ref>, with ELP-SR, all backbone networks achieve performance gains. The results reveal that our method is valuable to various backbone models and generally ameliorates the representations of networks. Almost all the backbones obtain about a 0.5% percent increase in top-1 accuracy. Furthermore, to verify the general improvements introduced by our method, we explore the performances of our method with SE-block <ref type="bibr" target="#b21">[22]</ref>. As shown in Table <ref type="table" target="#tab_3">3</ref>, though SE-block already promotes the performances, our method leads to further boosts on the fundamental of SE-block <ref type="bibr" target="#b21">[22]</ref>. k-nearest neighbors accuracy. To reveal the effectiveness of our method, we provide an additional evaluation with the KNN classifier <ref type="bibr" target="#b55">[56]</ref>. For feature vector h, we select the top k nearest neighbors by the weights exp(h ? h ? /t) corresponding to the labels, where h ? indicates features from the training set and t is a temperature term. We apply t = 0.1 in our experiments.</p><p>As shown in Table <ref type="table" target="#tab_4">4</ref>, the results with 20 and 200 nearest neighbors are displayed. With the KNN classifier, our method outperforms the backbone network. This reflects that the features after training with ELP-SR become more discriminative.</p><p>In all, the general improvements in all the backbones, methods, and tasks reflect that ELP-SR is not sensitive to particular networks, designs, or visual challenges. It provides a valuable regularization for visual representation learning. iment with the different values of I in the CUB dataset. As shown in Table <ref type="table" target="#tab_5">5</ref>, the performances are influenced by I. The larger I induces the degradation of performances. With plenty of training iterations, the ELP classifier tends to be overfitting and cannot measure generalization effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Studies</head><p>Besides, we also operate comparisons on the ImageNet dataset. The model achieves 76.13, 76.82, and 76.30 when I equals to 1, 2, and 3, respectively. The proper value of I can better empower the advantages of ELP. Minor I may not be sufficient for the construction of ELP. The more significant I may induce degradation of the ability of the ELP classifier to indicate features' discrimination. Thus, we apply I = 2 in our experiments as this condition generally shows improvements in several datasets. ? in the SR Factor. The parameter ? is responsible for adjusting the intensity of regularization. Since D R is always lower than 1, the larger ? leverages smaller regularization for the inputs. As shown in Table <ref type="table" target="#tab_5">5</ref>, we compare multiple conditions of ? in fine-grained classification. The variances of ? slightly influence the performances. A proper ? leads to better performances but is not deterministic for fine-grained classification. Moreover, we evaluate different ? values under the condition of I = 2 on ImangeNet-1K. The recognition accuracies are 76.23, 76.82, and 76.30 when ? is set to 1, 2, and 3, respectively. The Variations of SR Factor. We further investigate our ELP-SR in different forms, as shown in Table <ref type="table" target="#tab_6">6</ref>. First, for regularization, the confidences of the ELP classifier reflect the discriminability of features. Since the main classifier tends to be overfitting, p c is relatively higher and close to 1. Thus, a similar effect may occur for 1 -q c and p c -q c . As shown in Table <ref type="table" target="#tab_6">6</ref>, both formulations enable regularizing the networks to perform better while the model with p c -q c achieves a higher result. This is because p c -q c provides a more precise measurement of the deviation between the main classifier and the ELP classifier.</p><p>Second, to formulate the normalization term, we require both confidences of the ELP classifier and the main classifier to become higher. The higher confidence of the main classifier indicates that the sample can be correctly recognized. This is a primary requirement for better representation of the feature. If the features are hard to recognize even for the main classifier, this may indicate that the visual representation quality is relatively low. It is a primary criterion that the network should provide at least recognizable features. As shown in Table <ref type="table" target="#tab_6">6</ref>, higher performances are shown if applying the normalization terms. Both p c +q c and p c * q c are valid to normalize our ELP-SR. Third, only the regularization of higher q c can also boost the performances. Without the normalization term, the impact of ELP-SR also guides the networks to be more generalized. However, lacking normalization, the improvements are relatively lower. Besides, simple normalization is also valuable. Since 2 p c +q c and 2 p c * q c also expect higher confidences of ELP, a similar influence may occur through leveraging the normalization term only. These results demonstrate that regularization and normalization are valuable in ELP-SR. Simultaneously, the combinations of both sides introduce a further increase in performances.</p><p>Finally, we also operate ablations for the distillation of the probability of two classifiers. Remarkable decreases are shown in Table <ref type="table" target="#tab_6">6</ref> of both conditions for L1 and L2 regressions. The network should not be optimized to solve features' discriminability directly. Distilling can lead the main classifier to perform similarly to the ELP classifier but does not encourage the network to be more generalized. If the main classifier is optimal according to the ELP classifier, the network can 'pretend' to achieve discriminative features. However, in testing, this 'cheating' is useless. Additionally, we replace the ELP classifier with a memory bank and update the memory by a momentum-based moving average. When the momentum is 0.9 and 0.1, the results are 86.1%and 86.5%, respectively. The results show that the moving average operation helps fine-grained recognition, but it provides a weaker regularization than the episodically initialized ELP classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Visualization</head><p>To demonstrate the efficacy of our ELP, we present a visualization for the testing accuracy of our ELP based on CUB. In detail, we train the baseline method, take the features from the backbone to train ELP, but do not leverage ELP-SR for network training. Meanwhile, we take our method training with ELP-SR as the comparison. This is similar to applying linear probing for every epoch. Since ELP is reinitialized every two epochs for CUB, to better reveal the capacity of ELP under different conditions, we plot the accuracy every two epochs. As shown in Fig. <ref type="figure">3</ref>, unseen features in the testing set are remarkably more recognizable. This indicates that the network with ELP-SR is more generalized and produces more discriminative features. Even for the simple classifier, the unseen samples represented by the network are easier to be classified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose episodic linear probing (ELP) to estimate the generalization and discriminability of features online. By ELP, we propose an ELP-suitable Reg-  p c -q c p c + q c 76.82 p c -q c p c * q c 76.75 1 -q c p c + q c 76.78 1 -q c p c * q c 76.70 D p c -q c -76.71 1 -q c -76.60 ularization term (ELP-SR) to regularize the models. Our insights are two-fold. 1). Since the main classifier may be overfitting and its confidence may not indicate the discrimination of features, the ELP classifier provides additional regularization for more discriminative features. 2). Immediate suitability is effective in measuring the discrimination of features. An intuitive hypothesis is that if the features are highly discriminative, they should be recognizable by an easily learned linear classifier. Our ELP is episodically re-initialized, effectively mitigating overfitting and regularizing the network towards better linear separability.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The typical linear probe in testing (a) and our ELP in training (b). Our ELP is episodically re-initialized to maintain simplicity. It effectively measures the discrimination of visual representations in an online manner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>3. 1 . 1 Figure 2 .</head><label>112</label><figDesc>Figure 2. The training flow of our framework. Black lines indicate that the gradient can be back-propagated, while the blue dotted lines indicate that the gradient back-propagation is stopped.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Parameter I = 1 I</head><label>1</label><figDesc>= 2 I = 3 I = 4 I = 5 ? = 1 88.0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of three benchmarks of fine-grained classification. Without additional augmentations or network designs, our method achieves significant improvements.</figDesc><table><row><cell>7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 ,</head><label>2</label><figDesc>ELP-SR dramatically improves the performances of the baseline method in all the settings and datasets. The improvements</figDesc><table><row><cell>Method</cell><cell></cell><cell>CIFAR-10</cell><cell></cell><cell cols="2">CIFAR-100</cell><cell></cell></row><row><cell>Imbalance ratio</cell><cell>100</cell><cell>50</cell><cell>10</cell><cell>100</cell><cell>50</cell><cell>10</cell></row><row><cell>Focal Loss [32]</cell><cell cols="6">70.4 76.7 86.7 38.3 43.9 55.7</cell></row><row><cell>CB Focal [12]</cell><cell cols="6">74.6 79.3 87.1 39.6 45.2 58.0</cell></row><row><cell>Meta-weight [44]</cell><cell cols="6">75.2 80.0 87.8 42.0 46.7 58.4</cell></row><row><cell>CDB-CE [45]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">42.5 46.7 58.7</cell></row><row><cell>Mixup [61]</cell><cell cols="6">73.1 77.8 88.3 39.6 45.0 58.2</cell></row><row><cell>ERM [7]</cell><cell cols="6">70.4 74.8 86.4 38.3 43.9 55.7</cell></row><row><cell>ERM [7] + ELP-SR</cell><cell cols="6">77.4 81.2 87.9 39.1 44.7 57.9</cell></row><row><cell cols="7">ERM [7] + ELP-SR (? = 1) 77.5 81.5 88.4 42.4 48.3 58.9</cell></row><row><cell>ERM [7] + ELP-SR (?  * )</cell><cell cols="6">78.0 81.5 88.7 42.4 48.3 59.1</cell></row><row><cell>LDAM [7]</cell><cell cols="6">77.0 81.0 88.2 42.0 46.6 58.7</cell></row><row><cell>LDAM [7] + ELP-SR</cell><cell cols="6">78.2 82.3 88.1 43.9 48.2 59.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison of top-1 validation accuracy of different methods on imbalanced CIFAR-10 and CIFAR-100 datasets. All results are implemented based on ResNet-32. ? = 1 indicates applying ? -normalization<ref type="bibr" target="#b25">[26]</ref> with ? = 1. ? * stands for results with the best settings of ? .</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparison of single-crop accuracy (%) on the ImageNet-1K validation set. Different backbones with our method show significant improvements. To perform a fair comparison, ? indicates the results implemented and re-trained by ours.</figDesc><table><row><cell>Backbone</cell><cell cols="4">Top-1 Accuracy Baseline ELP-SR Baseline ELP-SR Top-5 Accuracy</cell></row><row><cell>ResNet50</cell><cell>76.13</cell><cell>76.82</cell><cell>92.86</cell><cell>93.32</cell></row><row><cell>ResNet101</cell><cell>77.37</cell><cell>77.86</cell><cell>93.54</cell><cell>94.06</cell></row><row><cell>ResNet152</cell><cell>78.31</cell><cell>78.77</cell><cell>94.04</cell><cell>94.42</cell></row><row><cell>BN-Inception</cell><cell>73.52  ?</cell><cell>74.05</cell><cell>91.56  ?</cell><cell>91.74</cell></row><row><cell>Inception-V3</cell><cell>77.45</cell><cell>78.12</cell><cell>93.56</cell><cell>94.04</cell></row><row><cell>Inception-ResNet-V2</cell><cell>79.63  ?</cell><cell>80.22</cell><cell>94.79  ?</cell><cell>95.24</cell></row><row><cell>SE-ResNet50</cell><cell>77.05</cell><cell>77.45</cell><cell>93.48</cell><cell>93.88</cell></row><row><cell>SE-ResNet101</cell><cell>77.62</cell><cell>77.94</cell><cell>93.93</cell><cell>94.38</cell></row><row><cell>SE-ResNet152</cell><cell>78.43</cell><cell>78.61</cell><cell>94.27</cell><cell>94.53</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>4.4.1 Ablation on Hyper-parametersEpisodic interval I. The number of periodical intervals prevents the ELP from overfitting the features. We exper-KNN accuracy on ImageNet-1K. Results of accuracy with 20 and 200 nearest neighbors are presented.</figDesc><table><row><cell>Method</cell><cell>20</cell><cell>200</cell></row><row><cell>ResNet50</cell><cell cols="2">75.04 73.21</cell></row><row><cell cols="3">ResNet50 + ELP-SR 75.48 73.88</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Results for different values of I and ? on CUB. I prevents the ELP from overfitting, and ? adjusts the intensity of regularization.</figDesc><table><row><cell></cell><cell></cell><cell>88.2</cell><cell>88.2</cell><cell>88.0</cell><cell>87.8</cell></row><row><cell>? = 2</cell><cell>88.0</cell><cell>88.5</cell><cell>88.2</cell><cell>88.0</cell><cell>87.8</cell></row><row><cell>? = 3</cell><cell>87.6</cell><cell>88.8</cell><cell>88.0</cell><cell>88.0</cell><cell>87.6</cell></row><row><cell>? = 4</cell><cell>87.5</cell><cell>88.0</cell><cell>87.8</cell><cell>87.8</cell><cell>87.5</cell></row><row><cell>Formulation</cell><cell>D</cell><cell></cell><cell>R</cell><cell cols="2">Top-1 Accuracy</cell></row><row><cell>D</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>R</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Comparison for variations of SR Factor on ImageNet-1K. Various conditions are presented, including different formulations of D and R, with or without D and R, and direct distillation of the main and ELP classifier.Figure 3. Curves of testing accuracy only with ELP classifier on CUB. Compared with our method, We utilize the baseline method that extracts the features from the backbone, trains ELP with features individually but does not leverage ELP-SR for the backbone training. Features trained with ELP-SR are more discriminative than the baseline and easier to be classified by simple ELP.</figDesc><table><row><cell></cell><cell></cell><cell>1</cell><cell>-</cell><cell>p c + q c</cell><cell>76.25</cell></row><row><cell></cell><cell></cell><cell>R</cell><cell>-</cell><cell>p c  *  q c</cell><cell>76.23</cell></row><row><cell></cell><cell></cell><cell>Distillation</cell><cell>L1 L2</cell><cell>76.12 76.18</cell></row><row><cell></cell><cell>0.88</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.85</cell><cell></cell><cell></cell></row><row><cell>Accuracy</cell><cell>0.76 0.79 0.82</cell><cell></cell><cell></cell><cell>ELP accuracy of baseline</cell></row><row><cell></cell><cell>0.73</cell><cell></cell><cell></cell><cell>ELP accuracy of ours</cell></row><row><cell></cell><cell>0.7</cell><cell cols="3">1 6 11 16 21 26 31 36 41 46 51 56 61 66 71 76 81 86 91 96 101 106 111 116 121 126 131 136 141 146 151 156 161 166 171 176 1 60 120 180 240</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Epoch</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Understanding intermediate layers using linear classifier probes</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.01644</idno>
		<imprint>
			<date type="published" when="2016">2016. 1, 2, 3</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Facing the hard problems in fgvc</title>
		<author>
			<persName><forename type="first">Connor</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gwilliam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Teuscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Merrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Farrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.13190</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A critical analysis of self-supervision, or what we can learn from a single image</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Yuki M Asano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.13132</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">State-of-the-art in visual attention modeling</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="185" to="207" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00332</idno>
		<title level="m">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Higher-order integration of hierarchical convolutional activations for finegrained visual categorization</title>
		<author>
			<persName><forename type="first">Sijia</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="511" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning imbalanced datasets with labeldistribution-aware margin loss</title>
		<author>
			<persName><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Arechiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero-Soriano</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.04027</idno>
		<title level="m">Generating unseen complex scenes: are we there yet?</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The devil is in the channels: Mutual-channel loss for fine-grained image classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Bhunia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="4683" to="4695" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Destruction and construction learning for fine-grained image recognition</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yalong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006-05">June 2019. 5, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Barret</forename><surname>Ekin D Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Autoaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<title level="m">Learning augmentation policies from data</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9268" to="9277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Kernel pooling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2921" to="2930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Towards diverse and natural image descriptions via a conditional gan</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2970" to="2979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Improving simple models with confidence profiles</title>
		<author>
			<persName><forename type="first">Amit</forename><surname>Dhurandhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthikeyan</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronny</forename><surname>Luss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peder</forename><surname>Olsen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.07506</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fine-grained visual classification via progressive multi-granularity training of jigsaw patches</title>
		<author>
			<persName><forename type="first">Ruoyi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongliang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayan</forename><surname>Kumar Bhunia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiyang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4438" to="4446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2016. June 27-30, 2016. 2016. 3, 5, 6</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Theory of the backpropagation neural network</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Hecht-Nielsen</surname></persName>
		</author>
		<idno>1992. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Neural networks for perception</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<biblScope unit="page" from="65" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The low-dimensional linear geometry of contextualized word representations</title>
		<author>
			<persName><forename type="first">Evan</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.07109,2021.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention convolutional binary neural tree for fine-grained visual categorization</title>
		<author>
			<persName><forename type="first">Ruyi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Libo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Self-paced learning with diversity</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoou-I</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2078" to="2086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Decoupling representation and classifier for long-tailed recognition</title>
		<author>
			<persName><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Openimages: A public dataset for large-scale multi-label and multi-class image classification</title>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheyun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhyanesh</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<ptr target="https://github.com/openimages" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<pubPlace>Sydney, Australia</pubPlace>
		</imprint>
	</monogr>
	<note>Recognition (3dRR-13</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2006">2017. 1, 3, 5, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Gradient harmonized single-stage detector</title>
		<author>
			<persName><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8577" to="8584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitnick</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bilinear cnn models for fine-grained visual recognition</title>
		<author>
			<persName><forename type="first">Tsung-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aruni</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1449" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Large-scale long-tailed recognition in an open world</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongqi</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Object-part attention model for fine-grained image classification</title>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangteng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1487" to="1500" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Backpropagation: The basic theory. Backpropagation: Theory, architectures and applications</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>David E Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Durbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yves</forename><surname>Golden</surname></persName>
		</author>
		<author>
			<persName><surname>Chauvin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Self-supervised representation learning via neighborhood-relational encoding</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Khalooei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8010" to="8019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">On separability of self-supervised representations</title>
		<author>
			<persName><forename type="first">Vikash</forename><surname>Sehwag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mung</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prateek</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML workshop on Uncertainty and Robustness in Deep Learning (UDL)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Ramprasaath R Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A survey on image data augmentation for deep learning</title>
		<author>
			<persName><forename type="first">Connor</forename><surname>Shorten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Taghi</surname></persName>
		</author>
		<author>
			<persName><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="48" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Jun</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lixuan</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanping</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongben</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07379</idno>
		<title level="m">Meta-weight-net: Learning an explicit mapping for sample weighting</title>
		<imprint>
			<date type="published" when="2006">2019. 3, 6</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Class-wise difficulty-balanced loss for solving classimbalance</title>
		<author>
			<persName><forename type="first">Saptarshi</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroki</forename><surname>Ohashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katsuyuki</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">On the geometry of generalization and memorization in deep neural networks</title>
		<author>
			<persName><forename type="first">Cory</forename><surname>Stephenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchismita</forename><surname>Padhy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanlin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sueyeon</forename><surname>Chung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.14602,2021.1</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multiattention multi-class constraint for fine-grained image recognition</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="805" to="821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<idno>CoRR, abs/1602.07261</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The calculation of posterior distributions by data augmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><surname>Tanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung</forename><surname>Wing</surname></persName>
		</author>
		<author>
			<persName><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American statistical Association</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">398</biblScope>
			<biblScope unit="page" from="528" to="540" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds-200-2011 dataset</title>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><surname>Bevt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.01529</idno>
		<title level="m">Bert pretraining of video transformers</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning a discriminative filter bank within a cnn for fine-grained recognition</title>
		<author>
			<persName><forename type="first">Yaming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vlad</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4148" to="4157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning to navigate for fine-grained classification</title>
		<author>
			<persName><forename type="first">Ze</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiange</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="420" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Hierarchical bilinear pooling for fine-grained visual recognition</title>
		<author>
			<persName><forename type="first">Chaojian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinge</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="574" to="589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03530</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning multi-attention convolutional neural network for finegrained image recognition</title>
		<author>
			<persName><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5209" to="5217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3754" to="3762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13001" to="13008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Look-into-object: Self-supervised structure modeling for object recognition</title>
		<author>
			<persName><forename type="first">Mohan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yalong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06">June 2020</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
