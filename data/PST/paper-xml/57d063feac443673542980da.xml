<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Predicting Semantically Linkable Knowledge in Developer Online Forums via Convolutional Neural Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bowen</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Deheng</forename><surname>Ye</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhenchang</forename><surname>Xing</surname></persName>
							<email>zcxing@ntu.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Xia</surname></persName>
							<email>xxia@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guibin</forename><surname>Chen</surname></persName>
							<email>gbchen@ntu.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shanping</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Predicting Semantically Linkable Knowledge in Developer Online Forums via Convolutional Neural Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C602B28286A5627B41D79496F87FEFC2</idno>
					<idno type="DOI">10.1145/2970276.2970357</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Link prediction</term>
					<term>Semantic relatedness</term>
					<term>Multiclass classification</term>
					<term>Deep learning</term>
					<term>Mining software repositories</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Consider a question and its answers in Stack Overflow as a knowledge unit. Knowledge units often contain semantically relevant knowledge, and thus linkable for different purposes, such as duplicate questions, directly linkable for problem solving, indirectly linkable for related information. Recognising different classes of linkable knowledge would support more targeted information needs when users search or explore the knowledge base. Existing methods focus on binary relatedness (i.e., related or not), and are not robust to recognize different classes of semantic relatedness when linkable knowledge units share few words in common (i.e., have lexical gap). In this paper, we formulate the problem of predicting semantically linkable knowledge units as a multiclass classification problem, and solve the problem using deep learning techniques. To overcome the lexical gap issue, we adopt neural language model (word embeddings) and convolutional neural network (CNN) to capture wordand document-level semantics of knowledge units. Instead of using human-engineered classifier features which are hard to design for informal user-generated content, we exploit large amounts of different types of user-created knowledge-unit links to train the CNN to learn the most informative wordlevel and document-level features for the multiclass classification task. Our evaluation shows that our deep-learning based approach significantly and consistently outperforms traditional methods using traditional word representations and human-engineered classifier features.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In Stack Overflow, computer programming knowledge has been shared through millions of questions and answers. We consider a Stack Overflow question with its entire set of answers as a knowledge unit regarding some programmingspecific issues. The knowledge contained in one unit is likely to be related to knowledge in other units. When asking a question or providing an answer in Stack Overflow, users reference existing questions and answers that contain relevant knowledge by URL sharing <ref type="bibr" target="#b45">[46]</ref>, which is strongly encouraged by Stack Overflow <ref type="bibr">[2]</ref>. Through URL sharing, a network of linkable knowledge units has been formed over time <ref type="bibr" target="#b45">[46]</ref>.</p><p>Unlike linked pages on Wikipedia that follows the underlying knowledge structure, questions and answers are specific to individual's programming issues, and URL sharing in Q&amp;As is opportunistic, because it is based on the community awareness of the presence of relevant questions and answers. A recent study by Ye et al. <ref type="bibr" target="#b45">[46]</ref> shows that the structure of the knowledge network that URL sharing activities create is scale free. A scale free network follows a power law degree distribution, which can be explained using preferential attachment theory <ref type="bibr" target="#b3">[4]</ref>, i.e., "the rich get richer". On one hand, this means that a small proportion of the knowledge units is attracting a large proportion of users' attention. On the other hand, this means that large amounts of knowledge units in the long tail of the power law distribution are mostly under linked.</p><p>To mitigate this issue, Stack Overflow recommends related questions when users are viewing a question. Stack Overflow's recommendation of related questions is essentially based on lexical similarity of word overlap between questions <ref type="bibr" target="#b1">[1]</ref>. However, linkable knowledge units often share few words in common (i.e., lexical gap) due to two main reasons. First, users could formulate the same question in many different ways. For example, Table <ref type="table" target="#tab_0">1</ref> shows two duplicate questions from Stack Overflow. The question (Id 510357) has been recognized as a duplicate to another question <ref type="bibr">(Id 19477465</ref>) by the Stack Overflow users, because they can be answered by the same answer. However, literally, we can hardly see common words and phrases between these two questions. Second, two questions could discuss different but Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. Question Id 19477465 Title: Python read a single character from the user Title: Get python program to end by pressing anykey and not enter Body: Is there a way of reading one single character from the Body: How can I get my Python program to end by pressing any user input? For instance, they press one key at the terminal key without pressing enter. So if the user types "c", the program and it is returned (sort of like getch()). I know there's a should automatically end without pressing enter. My code so far: function in Windows for it, but I'd like something that is print("Hi everyone! This is just a quick sample code I made") cross-platform.</p><p>print("Press anykey to end the program.") Question Id 4547310 Question Id 53513 Title: Iterating over a stack (reverse list), is there an isempty() method?</p><p>Title: Best way to check if a list is empty Body: What's the best way to iterate over a stack in Python? I couldn't find Body: For example, if passed the following: an isempty method, and checking the length each time seems wrong somehow. a = [] How do I check to see if a is empty? built-ins? (list/tuple/string/dictionary) relevant knowledge. For example, Table <ref type="table" target="#tab_1">2</ref> shows two directly linkable questions, one asks "iterating over a stack (reverse list)" while the other asks "best way to check if a list is empty". Again, the two questions share few common words, but they embody strongly relevant knowledge (stack iteration and list empty checking). Furthermore, questions may discuss relevant knowledge but not directly for solving the questions, as the examples in Table <ref type="table" target="#tab_2">3</ref> shows. As such questions discuss indirectly linkable knowledge, they are unlikely to share many common words. Traditional word representations (e.g., BM25 <ref type="bibr" target="#b25">[26]</ref>, LDA <ref type="bibr" target="#b4">[5]</ref>) cannot reliably recognize many cases of potentially linkable knowledge units when lexical gaps exist between them.</p><p>As shown in the above examples, knowledge units can be linkable for different purposes. Ye et al.'s empirical study <ref type="bibr" target="#b45">[46]</ref> on the knowledge network in Stack Overflow confirms this phenomenon. Being able to classifying different classes of linkable knowledge units would support more targeted information needs when users search or explore the linkable knowledge. For example, duplicate questions allow users to understand a problem from different aspects, directlylinkable questions help explain concepts or sub-steps of a complex problem, while indirectly-linkable questions provide extended knowledge. However, such multiclass semantic relatedness in software engineering knowledge is not considered in existing work <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b5">[6]</ref>, which focuses on only binary classification, i.e., related or not. Furthermore, existing work heavily relies on human-engineered word-and document-level features to train the classifier, which are hard to design for informal user-generated content in Stack Overflow.</p><p>In this paper, we propose a novel approach for predicting multiclass semantically linkable knowledge units in Stack Overflow. Inspired by Ye et al. <ref type="bibr" target="#b45">[46]</ref>, we consider four classes of semantic relatedness: duplicate, direct linkable, indirectly linkable, and isolated. To overcome the lexical gap issue, Our approach recognizes and quantifies semantic relatedness between knowledge units using word embeddings <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b19">[20]</ref> and Convolutional Neural Network (CNN) <ref type="bibr" target="#b12">[13]</ref>, which are the state-of-the-art deep learning techniques for capturing word-level and document-level semantics for Natural Language Processing (NLP) tasks. Furthermore, our approach does not rely on human-engineered features to classify semantic relatedness. Instead, we collect large amounts of different types of user-created knowledge-unit links (duplicate, direct-linked, indirect-linked) from Stack Overflow to train the CNN to automatically learn the most informative wordand document-level features to classify semantic relatedness between knowledge units.</p><p>We conduct extensive experiments to compare our deeplearning based approach with the baseline methods using traditional word representations (TF-IDF) and the humanengineered features for determining semantic similarity. Our results show that: 1) our approach significantly outperforms the baseline methods, and performs much more consistently for different classes of semantic relatedness; 2) both word embeddings and CNN help improve the performance of multiclass classification of linkable knowledge units. CNN plays a more important role for predicting duplicate, directly linkable, and isolated knowledge unit, due to its capability of capturing document-level semantic features, while word embeddings are good at predicting indirectly linkable knowledge units; 3) domain-specific training corpus help improve the performance of deep-learning techniques for specific tasks.</p><p>Our contributions are as follows:</p><p>• We formulate the research problem of predicting multiclass linkable knowledge units in Stack Overflow;</p><p>• We propose a deep learning based approach to tackle the problem;</p><p>• We evaluate the effectiveness of our approach against the traditional methods for predicting relevant software engineering knowledge <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Many studies have been carried out on Stack Overflow <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b40">41]</ref>. Our work predicts semantically linkable knowledge in developers' discussions in Stack Overflow. It is related to two lines of research: link prediction in complex networks and semantic relatedness in software data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Link Prediction in Complex Networks</head><p>Link Prediction focuses on detecting potentially linkable objects in an observed network that is complex and evolves dynamically. Link prediction in complex networks has attracted enormous attention from physics, biochemical, and computer science communities <ref type="bibr">[7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b7">8]</ref>. A significant proportion of the related research falls into link prediction in social networks, many of which have been found to be complex networks, e.g., the user-user network in online Q&amp;A forums <ref type="bibr" target="#b48">[49]</ref>, in Twitter <ref type="bibr" target="#b13">[14]</ref>. These social network research work aims to understand the network evolution patterns and identify potential social relations between users <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>Ye et al. <ref type="bibr" target="#b45">[46]</ref> report that the knowledge network formed by developers' URL sharing activities in Stack Overflow is also a complex network, i.e., the network structure is scale-free. They show that "the rich get richer" effect <ref type="bibr" target="#b3">[4]</ref> influences the knowledge sharing activities and the growth of the knowledge network in Stack Overflow. As a result, a small portion of questions and answers attracts a large portion of developers' attention, while large amounts of relevant questions and answers in the long tail of the power law distribution are under linked. This finding motivates our work to predict potentially linkable knowledge units, which could enhance the knowledge sharing and search in Stack Overflow.</p><p>Unlike link prediction in social network which predicts only whether the two persons are linkable or not (i.e., binary classification), our work predicts different types of relatedness (duplicate, direct link, or indirect link) between potentially linkable knowledge units (i.e., multiclass classification). This milticlass link prediction is again inspired by the study of Ye et al. <ref type="bibr" target="#b45">[46]</ref>, which reveals that users link relevant questions and answers for different purposes. They suggest that mixing different types of linkable knowledge units together could hinder the knowledge search as users often need different types of relevant information in different situations. Therefore, we treat the linkable knowledge prediction as a multiclass classification problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Semantic Relatedness in Software Data</head><p>Measuring the relatedness (or similarity) between two pieces of textual contents has long been studied. The underlying mathematic model of textual contents has evolved from Vector-Space-Model (e.g., TF-IDF), n-gram language model, topic modeling (e.g., LDA <ref type="bibr" target="#b4">[5]</ref>), to the recent development of neural language model (e.g., distributed word representations (or word embeddings) <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b19">20]</ref>). The trend is towards semantically richer similarity measures. In 2013, Mikolov et al. <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20]</ref> propose two neural language models (referred to as word2vec in the literature) (continuous bagof-words and continuous skip-gram) and efficient negative sampling method for learning word embeddings from large amounts of text. In this work, we adopt continuous skipgram model for learning domain-specific word embeddings from Stack Overflow text.</p><p>In the software engineering community, researchers utilize the textual similarity between two software artifacts to solve various software engineering tasks, such as duplicate or similar bug report detection <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>, relevant software tweets detection <ref type="bibr" target="#b27">[28]</ref>, duplicate online question detection <ref type="bibr" target="#b49">[50]</ref>. For example, Sun et al. use a Support Vector Machine (SVM) classifier in <ref type="bibr" target="#b29">[30]</ref> and an IR based similarity measure (BM25F <ref type="bibr" target="#b25">[26]</ref>) in <ref type="bibr" target="#b28">[29]</ref>, respectively, to detect duplicate bug reports. Zhang et al. <ref type="bibr" target="#b49">[50]</ref> detect duplicate Stack Overflow questions by comparing their titles and question descriptions using topic model (LDA). These existing work are formulated as a binary prediction problem, e.g., duplicate or non-duplicate.</p><p>In the NLP community, the development of CNN architectures for sentence-level and document-level text processing is under intensive research. Some recent work utilizes CNN to learn the semantic relations between two pieces of texts. For example, Kim <ref type="bibr" target="#b15">[16]</ref> proposes a simple CNN trained on top of Mikolov's word embeddings <ref type="bibr" target="#b19">[20]</ref>, and then apply the CNN to sentence classification. Bogdanova et al. <ref type="bibr" target="#b5">[6]</ref> apply CNN with word embeddings to duplicate question detection in online Q&amp;A sites. These CNN-based methods outperform traditional NLP methods for sentence classification and duplication question detection.</p><p>Comparing with these existing works on semantic relatedness between two pieces of text, we leverage CNN with domain-specific word embeddings for the problem of multiclass classification of potentially linkable knowledge units in Stack Overflow, beyond the binary prediction of duplicate content or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">THE APPROACH</head><p>This section formulates our research problem and describes the technical details of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>We consider a question together with its entire set of answers in Stack Overflow as a knowledge unit regarding some programming-specific issues. If two knowledge units are semantically related, we consider them as linkable. We further predict the types of the relatedness between the two knowledge units. Inspired by Ye et al.'s study <ref type="bibr" target="#b45">[46]</ref> on the purposes of URL sharing, we define four types of the relatedness between the two knowledge units:</p><p>• Duplicate: Two knowledge units discuss the same question in different ways, and can be answered by the same answer. • Direct link: One knowledge unit can help solve the problem in the other knowledge unit, for example, by explaining certain concepts, providing examples, or covering a sub-step for solving a complex problem. • Indirect link: One knowledge unit provides related information, but it does not directly answer the question in the other knowledge unit. • Isolated: The two knowledge units are not semantically related. We formulate our task as a multiclass classification problem. Specially, given two knowledge units, our task is to predict whether the two knowledge units have one of the above four types of relatedness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Overview of Main Steps</head><p>In NLP tasks, words are usually represented as vectors. In this work, we use word embeddings (distributed representations of words in a vector space) <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b20">21]</ref> as word representations, because word embeddings require only large amounts of unlabeled text to train, and have been shown to be able to capture rich semantic and syntactic features of words. We use continuous skip-gram model <ref type="bibr" target="#b19">[20]</ref> (the Python implementation in Gensim <ref type="bibr" target="#b24">[25]</ref>) to learn domain-specific word embeddings from large amounts of software engineering text from Stack Overflow questions and answers. The output is a dictionary of word embeddings for each unique word. Word embeddings encode word-level semantics. To predict semantic relatedness between knowledge units, we need to capture semantics at sentence and knowledge unit level. To this end, we resort to convolutional neural network <ref type="bibr" target="#b12">[13]</ref>. Given two knowledge units, they are first converted into word vector representations by looking up the word embeddings dictionary, which are then fed into a CNN to produce a feature vector for each knowledge unit. The similarity of the two knowledge units are measured by the cosine similarity of their feature vectors. Based on the similarity score, the relatedness of the two knowledge units are classified as one of the four classes defined in Section 3.1.</p><p>To train the CNN for the prediction task, we collect a set of user-linked knowledge-unit pairs from Stack Overflow for each type of relatedness: duplicate, direct link, indirect link, and isolated. This set of training data is fed into the CNN and the training is guided by the cosine loss of the relatedness of the knowledge-unit pairs against the user-linked relatedness. During the training process, the CNN automatically learns the most informative word and sentence features for predicting multiclass linkable knowledge units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning Word Representations</head><p>Distributed word representations assume that words appear in similar context tend to have similar meanings <ref type="bibr" target="#b10">[11]</ref>. Therefore, individual words are no longer treated as unique symbols, but mapped to a dense real-valued low-dimensional vector space. Each dimension represents a latent semantic or syntactic feature of the word. Semantically similar words are close in the embedding space. Figure <ref type="figure" target="#fig_1">2</ref> shows some examples of domain-specific word embeddings we learn from Stack Overflow java text, visualized in a two-dimensional space using the t-SNE dimensionality reduction technique <ref type="bibr" target="#b32">[33]</ref>. Semantically close words, such as JPanel, JButton, JFrame and JLabel which belong to GUI component are close in the vector space.</p><p>Word embeddings are unsupervised word representations, which requires only large amounts of unlabeled text to learn. In this work, we collect body content of questions and answers from Stack Overflow as software engineering text. We preprocess the body content by removing large code snippets in &lt; pre &gt; HTML tag and cleaning HTML tags. Short code elements in &lt; code &gt; HTML tag in natural language sentences are kept. We use the software-specific tokenizer developed in <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b46">47]</ref> to tokenize the sentences. This tokenizer preserves the integrity of code tokens. For example, it tokenizes dataf rame.apply() as a single token, instead of a sequence of 5 tokens: dataframe . apply ( ). This supports more accurate domain-specific word embeddings learning because code tokens will not be mixed with normal words.</p><p>Word embeddings are typically learned using neural language models. In this work, we use continuous skip-gram model, a popular word to vector (word2vec) model proposed by Mikolov et al. <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b19">[20]</ref>. As shown in Figure <ref type="figure" target="#fig_2">3</ref>, continuous skip-gram model learns the word embedding of a center word (i.e., wi) that is good at predicting the surrounding words in a context window of 2k + 1 words (k = 2 in this example). The objective function of skip-gram model is to maximize the sum of log probabilities of the surrounding context words conditioned on the center word:</p><formula xml:id="formula_0">n i=1 -k≤j≤k,j =0 log p (wi+j|wi)</formula><p>where wi and wi+j denote the center word and the context word in a context window of length 2k + 1. n denotes the length of the word sequence.  The log p (wi+j|wi) is the conditional probability defined using the softmax function:</p><formula xml:id="formula_1">p (wi+j|wi) = exp(v T w i+j vw i )</formula><p>w∈W exp(v T w vw i ) where vw and v w are respectively the input and output vectors of a word w in the underlying neural model, and W is the vocabulary of all words. Intuitively, p (wi+j|wi) estimates the normalized probability of a word wi+j appearing in the context of a center word wi over all words in the vocabulary. Mikolov et al. <ref type="bibr" target="#b20">[21]</ref> propose an efficient negative sampling method to compute this probability.</p><p>The output of the model is a dictionary of words, each of which is associated with a vector representation. Given a knowledge unit, its title, description and answers content (after proper preprocessing) will be converted into a knowledge-unit vector by looking up the dictionary of word embeddings and concatenating the word embeddings of words comprising the body content. Let wvi ∈ R d be the ddimensional word vector corresponding to the i-th word in a knowledge unit. The knowledge unit of n words is represented as, knv = wv1 ⊕ wv2 ⊕ ... ⊕ wvn, which is used as input to the CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">The CNN Architecture</head><p>Figure <ref type="figure" target="#fig_2">3</ref> presents the architecture of our CNN. Our CNN takes as input a knowledge-unit vector knv = wv1 ⊕ wv2 ⊕ ... ⊕ wvn, and captures the most informative word and sentence features in the knowledge unit for determining semantic relatedness. Treating the knowledge-unit vector as an "image", we perform convolution on it via linear filters. Because each word is represented as a d-dimensional vector, we use filters with widths k equal to the dimensionality of the word vectors (i.e., k = d). Let wv i:i+h-1 refer to the concatenation vector of h adjacent words wi, wi+1, ..., w i+h-1 . A convolution operation involves a filter w ∈ R hk (a vector of h×k dimensions) and a bias term b ∈ R h , which is applied to h words to produce a new value oi ∈ R:</p><formula xml:id="formula_2">oi = w T • wv i:i+h-1 + b<label>(1)</label></formula><p>where i = 1...n -h + 1, and • is the dot product between the filter vector and the word vector. This filter is applied repeatedly to each possible window of h words in the sentences (zero padding where necessary) in the knowledge unit (i.e., wv 1:h , wv 2:h+1 , ..., wv n-h+1:n ) to produce an output sequence o ∈ R n-h+1 , i.e., o = [o1, o2, ..., o n-h+1 ]. We apply the nonlinear activation function ReLu <ref type="bibr" target="#b9">[10]</ref> to each oi to produce a feature map c ∈ R n-h+1 where ci = ReLu(oi) = max(0, oi).</p><p>To capture features of phrases of different length, we vary the window size h of the filter, i.e., the number of adja-cent words considered jointly. In our CNN, we use filters of 5 different window size <ref type="bibr" target="#b1">(1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr">7,</ref><ref type="bibr" target="#b8">9)</ref>, which can capture features of different n-grams respectively. For each window size, we use 128 filters to learn complementary features from the same word windows. The dimensionality of the feature map generated by each filter will vary as a function of the knowledge-unit length and the filter's window size. Thus, a pooling function should be applied to each feature map to induce a fixed-length vector. In this work, we use 1max pooling, which extracts a scalar (i.e., a feature vector of length 1) with the maximum value in the feature map for each filter. Together, the outputs from each filter can be concatenated into a top-level feature vector, denoted as f v ku . This feature vector would capture the most informative 1, 3, 5, 7 and 9-grams in the input knowledge unit.</p><p>Given two knowledge units kux and kuy, the CNN outputs their respective feature vectors f vx and f vy. For this pair of feature vectors f vx and f vy, the CNN computes a similarity score between f vx and f vy in the last layer, using the cosine similarity, i.e., relatedness(kux, kuy) = f vx • f vy f vx f vy</p><p>In the training phase, the computed similarity score will be used to compute the mean square error against the groundtruth similarity score for the given pair of knowledge units (see Section 3.5). In the prediction phase, the computed similar score will be used to determine the class of semantic relatedness of the given two knowledge units. Because the computed similarity score is a continuous value, we need to bin the similarity score as four discrete classes of semantic relatedness between the two knowledge units: [0, 0.25) as isolated, [0.25, 0.5) as indirect link, [0.5, 0.75) as direct link, and [0.75, 1] as duplicate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Training the CNN</head><p>Training of the proposed CNN follows supervised learning paradigm. According to our task objective, the training data must consist of sufficient examples of four types of related knowledge-unit pairs, i.e., duplicate, direct link, indirect link and isolated, so that the CNN can learn to capture informative word and sentence features for classifying knowledge-unit relatedness.</p><p>Fortunately, URL sharing activities by Stack Overflow users create all types of the needed relatedness. User-created links between knowledge units are stored in the post links table. We parse the post links to generate the training dataset as follows. First, we randomly select pairs of user-linked knowledge-unit pairs. If the LinkTypeId is 3 (i.e., duplicate posts), we mark the selected pair as an instance of duplicate knowledge units. Otherwise, the selected pair is an instance of direct-link. Second, we randomly select pairs of knowledge units that are only transitively linked (i.e., the shortest path between the knowledge units is at least 2). These pairs of knowledge units are instances of indirect-link knowledge units. Finally, we randomly select pairs of knowledge units that do not have links in the post links table. These pairs are instances of isolated knowledge units. We select equal number of instances for different types of relatedness.</p><p>Let the tuple &lt; kux, kuy, simV alue &gt; be a pair of knowledge units in the training dataset T D, and simV alue is the ground-truth similarity score between kux and kuy. In this work, we set 0.125, 0.375, 0.625 and 0.875 as the ground-truth similarity score for the four classes: isolated, indirect link, direct link and duplicate, respectively. The CNN with the current parameter set θ computes a similarity score between the knowledge units, i.e., relatedness(kux, kuy). The training objective is to minimize the mean-squared error of the computed similarity score and the ground-truth similarity score (i.e., cosine loss) with respect to θ:</p><formula xml:id="formula_3">θ → 1 |T D| T D</formula><p>(simV alue -relatedness(kux, kuy)) 2</p><p>We add the l2 norm regularization loss to the mean-squared error data loss. The l2 norm constraint penalizes large weight parameters. It helps avoid model overfitting, because no input dimension can have a very large influence on the predicted probabilities all by itself. The parameters to be estimated include: the convolution filters w and the bias terms b (see Eq. 1). We solve the optimization problem using Adam update algorithm <ref type="bibr" target="#b16">[17]</ref>. Once we learn the CNN parameters, the CNN can be used to predict the semantic relatedness of an unseen pair of knowledge units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENT</head><p>We conduct a set of experiments to evaluate the effectiveness of our approach, compare it against a well-designed baseline, and investigate the impact of domain-specific word embeddings. As our approach relies on deep learning techniques, we also report the training cost of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Baseline Building</head><p>As our task is essentially to predict relatedness between two pieces of text, we design two baselines, each of which is a multiclass SVM classifier with different textual features, i.e., traditional Term Frequency (TF) and Inverse Document Frequency (IDF) versus word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Feature Extraction with TF and IDF</head><p>TF and IDF are widely used in predicting textual similarity with cosine distance <ref type="bibr" target="#b29">[30]</ref>. In this baseline, we define in total 36 features based on the TF and IDF values of the words in a pair of knowledge units. That is, the pair of the knowledge units is represented as a 36-dimensional feature vector to be used as input to the SVM classifier.</p><p>Given a knowledge unit KUx, we split its text into three sub-documents, denoted as C i</p><p>x (1 ≤ i ≤ 3), which correspond to question title (i = 1), question body (i = 2), and question title plus question body plus body of all answers (i = 3). Let &lt; KUx, KUy &gt; be a pair of knowledge units.</p><p>For TF-based features, we first obtain a vocabulary V i,j</p><p>x,y with all words in a combination of different sub-documents of the two knowledge units, i.e., V i,j x,y = {w | w ∈ C i x C j y (1 ≤ i, j ≤ 3)}. The TF value of each word in the vocabulary V i,j</p><p>x,y is then computed. These word TF values are used to convert the sub-document C i x of the knowledge unit KUx into a vector representation v i</p><p>x . The dimensionality of the v i</p><p>x is the size of the vocabulary, and each dimension is the TF value of the word in the sub-document C i x , or 0 otherwise. We compute the cosine similarity of the two vectors v i</p><p>x and v j y to measure the similarity of the sub-document C i x and C j y of the knowledge unit KUx and KUy, respectively. As each knowledge unit has three sub-documents, i.e., 1 ≤ i, j ≤ 3, we obtain 3×3 (9) TF-based features for a pair of knowledge units.</p><p>For IDF-based features, we first combine the respective sub-documents C i x of all knowledge units in a dataset as a corpus C i . The IDF value of a word w in the corpus C i is then computed, denoted as idfi(w). We measure the IDF-based similarity between the sub-documents C i x of the knowledge unit KUx and the sub-document C j y of the knowledge unit KUy as:</p><formula xml:id="formula_4">SimIDF k (C i x , C j y ) = w∈C i x C j y idf k (w)</formula><p>i.e., the sum of the IDF value of the common words of C i x and C j y in the corpus C k . As 1 ≤ i, j, k ≤ 3, we obtain 3 × 3 × 3 (27) IDF-based features for a pair of knowledge units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Feature Extraction with Word Embeddings</head><p>The semantic information of a piece of text can be captured by taking the mean of the word embeddings of the words comprising the text <ref type="bibr" target="#b14">[15]</ref>. Following this treatment, we compute the mean of the word embeddings of the words in a knowledge unit KUx (including question title plus question body plus body of all answers) as the word embedding of the knowledge unit, i.e.,</p><formula xml:id="formula_5">wv(KUx) = 1 n w∈Bx wv(w)</formula><p>where Bx is the bag of words of the KUx and n is the size of Bx. A pair of knowledge units KUx and KUy is then represented as the mean of the two knowledge-unit word embeddings, i.e., wv(KUx, KUy) = 1 2 (wv(KUx) + wv(KUy))</p><p>which will be used as input to the SVM classifier. In this work, we use 200-dimensional word vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Multiclass SVM Classifier</head><p>We develop a multiclass SVM classifier to predict the relatedness of the two knowledge units. Based on the feature vector used (TF-IDF based or word embedding based as described above), we have two baselines: Baseline1 (TF-IDF+SVM) and Baseline2 (WordEmbed+SVM). The two baseline SVM classifiers are trained using the same dataset as that for training our approach. As our task is a multiclass classification problem, we use RBF kernel <ref type="bibr" target="#b33">[34]</ref> in the SVM model. We set the γ parameter of the SVM to 1/k, k denotes the dimensionality of the feature vector, i.e., 1/36 for the TF-IDF+SVM baseline and 1/200 for the WordEm-bed+SVM baseline. We use grid search <ref type="bibr" target="#b11">[12]</ref> to optimize the SVM parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Dataset</head><p>Our experimental data is from Stack Overflow data dump of March 2016 <ref type="foot" target="#foot_0">1</ref> .</p><p>Word Embedding Corpora. From the posts table, we randomly select 100,000 knowledge units tagged with "java"as the word embedding corpora. Note that our corpora contains not only questions but also question answers.</p><p>Training and test linkable knowledge units. From the postlinks table, we randomly select in total 6,400 pairs of knowledge units that are tagged with "java", i.e., 1,600 pairs for each type of relatedness defined in Section 3.1 (Duplicate, Direct Link, Indirect Link and Isolated ). These 6,400 pairs of knowledge units are used as training data. For test data, we select in total 1,600 pairs of knowledge units that are tagged with "java", i.e., 400 pairs for each type of relatedness. User-created links are considered as ground truth label for the semantic relatedness of the selected pairs of knowledge units. The knowledge units in the test data does not overlap with those in the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Metrics</head><p>We represent the multiclass classification results as a K × K matrix A, where K is the number of classes (in our work K = 4). The rows represent the ground truth labels and the columns represent the predicted labels. The value of the Aij is the number of times that a pair of knowledge units with the ground truth label Li that is classified as the label Lj. Therefore, the value of Aii is the number of correct classification for the label Li, 1≤j≤K Aij is the number of the ground truth label Li in a dataset, 1≤i≤K Aij is the number of predicted label Lj in a dataset, and i j Aij is the number of knowledge-unit pairs in a dataset.</p><p>Precision, recall, and F1-scores as the evaluation metrics are standard and widely used to evaluate the effectiveness of a prediction technique <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b50">51]</ref>. Thus, we use them to compare our approach with the two baselines:</p><p>Accuracy is defined as the proportion of numbers of correct classification in a dataset, i.e.,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accuracy =</head><p>i Aii i j Aij Precision for a class j is the proportion of knowledgeunit pairs correctly classified as the class j among all pairs classified as the class j. Precision for all classes is the mean of the precision for each class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P recisionj = Ajj</head><p>1≤i≤K Aij Recall for a class i is the percentage of knowledge-unit pairs correctly classified as the class i compared with the number of ground truth label Li in the dataset. Recall for all classes is the mean of the recall for each class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recalli = Aii</head><p>1≤j≤K Aij F1-score for a class i is a harmonic mean of precision and recall for that class. F1-score for all classes is the mean of the F1-score for each class.</p><formula xml:id="formula_6">F 1i = 2 × P recisioni × Recalli P recisioni + Recalli</formula><p>F1-score evaluates if an increase in precision (or recall) outweighs a loss in recall (or precision). As there is often a trade off between precision and recall, F1-score is usually used as the main evaluation metric in many software engineering papers <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b47">48]</ref>. In this paper, we also choose F1-score as the main evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Research Questions and Findings</head><p>In our experiment, we are interested in the following two research questions: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Overall Comparison</head><p>RQ1: How much improvement can our approach achieve over the baseline approaches? Motivation. Our approach uses deep learning techniques (word embeddings and CNN-derived features) to quantify semantic relatedness between two pieces of software engineering text, which is very different from the baseline approaches using traditional TF-IDF word representations and human-engineered features. Answer to this research question will shed light on whether and to what extent deep learning techniques can improve the results of the multiclass classification task on software engineering text.</p><p>Approach. We apply our approach and the baseline approaches to the test data, i.e., 1,600 pairs of linkable knowledge units. We compare the accuracy, precision, recall, and F1-score metrics of different approaches.</p><p>Result. Table <ref type="table" target="#tab_4">4</ref> and Table <ref type="table" target="#tab_5">5</ref> present the results. We can see that the accuracy of our approach outperforms the Baseline1 and Baseline2 by 34.6% and 25.7%, respectively. Overall (see the last column of Table <ref type="table" target="#tab_5">5</ref>), our approach outperforms the Baseline1 and Baseline2 in terms of F1-score by 36.5% and 28.2%, respectively. Similar improvement on overall precision and recall can be observed. Overall, our approach achieves the best performance in all the evaluated metrics by a substantial margin, compared with the two baseline approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Comparison by Different Classes</head><p>RQ2: How effective is our approach in predicting different classes of semantic relatedness, compared with the baseline approaches?</p><p>Motivation. Different from existing work on binary text classification, our approach is for multiclass text classification. Different classes of semantic relatedness may exhibit different word and sentence features, which may affect the effectiveness of different semantic similarity measures. We would like to investigate the advantages and disadvantages of our approach and the baseline approaches for predicting different classes of semantic relatedness. Approach. We compare the precision, recall and F1-score for each class of relatedness (i.e., duplicate, direct link, indirect link, and isolated) by our approach and the two baseline approaches (see Table <ref type="table" target="#tab_5">5</ref>).</p><p>Result. The performance of our approach and the baseline approaches do vary for different classes of relatedness. Our approach performs the best for three of the four classes (i.e., duplicate, direct link and isolated) on all the evaluation metrics, while the Baseline2 performs the best for the indirect link class only on recall and F1-score. In fact, the Baseline2 achieves the extremely high recall (0.980) but low precision (lower than our approach) for the indirect link class. The performance variance of our approach for different classes is small (0.05-0.09 in terms of F1-score), compared with the performance variance of the two baseline methods for different classes (in terms of F1-score, 0.1-0.27 for the Baseline1 and 0.27-0.39 for the Baseline2). Our approach performs better on more classes of relatedness than the baseline methods (3:1). Our approach also performs more consistently for different classes of relatedness. The Baseline2 is extremely good at recalling indirectly linkable knowledge units with a sacrifice of precision, but its performance varies greatly for different classes of relatedness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Effects of Word Embeddings and CNN</head><p>RQ3: What is the impact of word embeddings and CNN on the performance improvement respectively? Motivation. Word embeddings and CNN are two deep learning techniques our approach relies on. They help capture semantics at word-level and sentence/document-level respectively. Answer to this research question helps us understand the importance of different levels of semantics for the multiclass text classification task. Approach. To understand the importance of word embedding, we compare the performance of the Baseline1 (i.e., TF-IDF+SVM) and the Baseline2 (i.e., WordEmbedding+SVM).</p><p>To understand the importance of CNN, we compare the performance of the Baseline2 (i.e., WordEmbedding+SVM) and our approach (i.e., WordEmbedding+CNN).</p><p>Result. Overall, the Baseline2 outperforms the Baseline1 by a small margin (see Accuracy in Table <ref type="table" target="#tab_4">4</ref> and the Overall Column in Table <ref type="table" target="#tab_5">5</ref>). This suggests that word embeddings can moderately improve the text classification performance compared with the traditional TF-IDF word representation. Overall, our approach outperforms the Baseline2 by a substantial margin. This suggests that CNN plays a more important role than word embeddings for improving the multiclass classification performance. The performance of the Baseline2 is not always better than that of the Baseline1 for difference classes of relatedness. For some classes, the Baseline2 has better precision but worse recall than the Baseline1, and vice versa for other classes. In terms of F1-score, the two baselines are almost the same for duplicate and direct link classes, while the Baseline2 is significantly better than the Base-line1 for indirect link class, but significantly worse for isolated classes. This suggests that word-level semantics encoded in word embeddings may not be appropriate for determining all classes of relatedness. For isolated knowledge units, traditional TF-IDF representation which is sensitive to lexical gap performs better than word embeddings. Taking the mean of word embeddings as knowledge-unit representations may blur the semantic distinction between the knowledge units, which helps the Baseline2 recall indirectly linkable knowledge units, but degrades the performance of the Baseline2 for isolated knowledge units.</p><p>In contrast, our approach consistently outperforms the Baseline2 on all the evaluation metrics for different classes of relatedness, except the recall and F1-score for indirectly linkable knowledge units. This suggests that word-level semantics are especially useful for determining semantic similarity of indirectly linkable knowledge units. As indirectly linkable knowledge units may not exhibit semantic similarity at sentence/document level, considering sentence/documentlevel semantics by the CNN could rule out false-negative links, which degrades its recall. Both word embeddings and CNN help improve the performance of multiclass text classification, but CNN has a bigger impact than word embeddings. Word-level semantics are especially useful for predicting indirectly linkable knowledge units, while sentence/document-level semantics plays more significant role for predicting duplicate, directly linkable and isolated knowledge units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.4">Impact of Domain-Specific Word Embeddings</head><p>RQ4: How sensitive is our approach to word embeddings learned from different corpus? Motivation. In this work, we predict linkable knowledge in developers' online forum. Therefore, we learn word embeddings from Stack Overflow text which is representative of the ways people ask/answer questions and the vocabulary people use. We would like to investigate whether and to what extent word embeddings learned from a different corpora affect the performance of our approach. This will help us understand the importance of suitable corpus for a software-specific machine learning task. Approach. We collect a corpus of Wikipedia web pages from the Wikipedia data dump<ref type="foot" target="#foot_1">2</ref> . The number of sentences and the size of the vocabulary of the Wikepedia corpus is comparable to that of the corpus of the 100,000 knowledge units from Stack Overflow. We learn word embeddings from the Wikipedia corpus and use it to train the CNN subsequently. We compare the performance of the two CNNs, one trained with Stack Overflow word embeddings, and the other trained with Wikipedia word embeddings. Result. Table <ref type="table" target="#tab_6">6</ref> presents the accuracy, and the overall precision, recall and F1-score of our approach with domainspecific word embeddings (i.e., Stack Overflow) versus general word embeddings (i.e., Wikipedia). General word embeddings degrade the performance of our approach, com-pared with domain-specific word embeddings. However, the degrade is moderate, in terms of accuracy, precision, recall and F1-score by 7%, 5%, 7% and 7%, respectively. Furthermore, although the performance of our approach degrades with general word embeddings, it still outperforms the two baseline methods (at least 11% on all the evaluation metrics).</p><p>Our approach demonstrates certain level of reliability even with word embeddings learned from a general corpus that is completely different from Stack Overflow discussions. However, suitable domain-specific word embeddings leads to better performance. For training the CNN model, it takes about 14 hours for the CNN model to achieve the loss convergence (&lt; e -<ref type="foot" target="#foot_2">3</ref> ).</p><p>Training of the word embeddings model and the CNN model can be done efficiently offline and only need to be done once.</p><p>Our approach is practical for large dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">DISCUSSION</head><p>This section presents the qualitative analysis of some examples to illustrate the capability of our approach and discusses threads to validity of our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Qualitative Analysis</head><p>Table <ref type="table">7</ref> presents one example for each class of linkable knowledge unit and the classification results by our approach and the two baselines. The first example is a pair of duplicate questions. However, the two questions do not have many words in common. The two baseline methods classify them as indirectly linkable knowledge units. In contrast, our approach can capture semantic similarity between terms like "standard input / output" and "System.out.println()" 3 . This helps our approach classify the two questions as duplicate.</p><p>In the second example, the two knowledge units contain directly relevant knowledge, i.e., the "GUI repaint() problem" is directly relevant to "text not displaying correctly" problem. Unfortunately, the two knowledge units share few words in common, which makes the baseline methods classify them as isolated. Our approach can capture the semantic relatedness between the two technical problems, and thus correctly classify the two knowledge units as direct link.</p><p>In the third example, the two indirectly linked questions share some common words. Based on these overlap words, the Baseline1 which essentially relies on lexical similarity of overlapping words classifies the two questions as duplicate. The Baseline2 which considers word-level semantics by word embeddings makes a better judgment, classifying them as isolated. Our approach makes the most accurate prediction by taking into account not only word-level but also document-level semantics. Similarly, in the fourth example, the Baseline1 makes the least appropriate prediction based on overlapping words, the Baseline2 makes a better judgment based on word-level semantics, and our approach makes the most accurate prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Error Analysis</head><p>We also analyze the cases in which our approach makes the wrong prediction. We find that many cases that fail our approach involve knowledge units whose essential information is presented as a code snippet<ref type="foot" target="#foot_4">4</ref> or an image 5 . In this work, we remove code snippets and images during preprocessing. In the future, we could incorporate code snippets and images into our approach. Incorporating image semantics into our approach would be relatively easy because CNN is originally invented for image classification. Incorporating code semantics into our approach could be a challenging task. A recent work by Mou et al. <ref type="bibr" target="#b21">[22]</ref> proposes to use CNN to encode the program ASTs for program analysis tasks. However, their approach is not directly applicable to code snippets in Q&amp;A discussion, which is usually incomplete and cannot be complied. We plan to tackle this challenge as our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Threats to Validity</head><p>There are several threats that may potentially affect the validity of our experiments. Threats to internal validity relate to errors in our experimental data and tool implementation. We have double checked our experimental data and tool implementation. We have also manually checked the selected knowledge units in our dataset to ensure that they are really tagged with "java" and have the right types of knowledge-units links. Threats to external validity relate to the generalizability of our results. In this study, we use a medium-size training and test dataset (100,000 knowledge units for word embedding learning, 6,400 pairs of knowledge units for CNN training, and 1,600 pairs of knowledge units for testing). This allows us to perform some manual analysis to understand the capability and limitations of our approach. In the future, we will reduce this threat by extending our approach to larger word embeddings corpus and more knowledge-unit pairs for training and testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION AND FUTURE WORK</head><p>In this paper, we propose a novel deep-learning based approach for predicting multiclass semantically linkable knowledge units in Stack Overflow. Our approach can predict four types of semantic relatedness, duplicate link, direct link, indirect link and isolated. At word level, our approach adopts</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of the Main Steps</figDesc><graphic coords="4,292.09,253.86,272.02,168.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example of Word Embedding</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Architecture of the Proposed CNN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>An Example of Duplicate Knowledge Units from Stack Overflow Question Id 510357 (marked as duplicate to 19477465)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>An Example of Directly Linked Knowledge Units from Stack Overflow</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>An Example of Indirectly Linked Knowledge Units from Stack Overflow (both have an link to 53513) How to check if a nested list has a value Title: Cost of len() function Body: I have a nested list and I want to check if an item has a value or not. Body: What is the cost of len() function for Python Not really sure how to describe it, so basically, how do I get this to work?</figDesc><table><row><cell>Question Id 32831543</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Accuracy Comparison</figDesc><table><row><cell>Accuracy</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Precision, Recall and F1-Score of Our Approach and the Baseline Approaches</figDesc><table><row><cell>Duplicate</cell><cell>Direct Link</cell><cell>Indirect Link</cell><cell>Isolated Overall</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Performance of Our Approach with Word Embeddings Learned from Different Corpus</figDesc><table><row><cell></cell><cell>Accuracy</cell><cell>Overall Precision</cell><cell>Overall Recall</cell><cell>Overall F1-Score</cell></row><row><cell>General Corpus From Wikipedia</cell><cell>0.770</cell><cell>0.790</cell><cell>0.776</cell><cell>0.777</cell></row><row><cell>Domain-specific Corpus From Stack Overflow</cell><cell>0.841</cell><cell>0.847</cell><cell>0.842</cell><cell>0.841</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>What is the time cost for training the underlying deep learning models? Motivation. The underlying word embeddings and CNN models need to be trained before they can be used for prediction task. Training of word embeddings and the CNN model is done only once offline. After model training, using the model to predict the relatedness of a pair of knowledge units takes negligible time. Understanding the training time cost helps us understand the practicality of our approach. Approach. We record the start time and the end time of the program execution to obtain the training time cost of the word embeddings and the CNN model. The experimental environment is an Intel(R) Core(TM) i7 2.5 GHz PC with 16GB RAM running Windows7 OS (64-bit).Result. For learning word embeddings model, the 100,000 knowledge units contain 23,759,119 words (as bags of words), and the vocabulary size (i.e., the number of unique words) is 434,836. It takes about 15 minutes to analyze the text of these 100,000 knowledge units to learn the word embeddings.</figDesc><table><row><cell>4.4.5 Training Cost</cell></row></table><note><p>RQ5:</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://archive.org/download/stackexchange</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://dumps.wikimedia.org/enwiki/latest/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Note that we use software-specific tokenizer[45,  </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p><ref type="bibr" target="#b46">47]</ref> which can preserve the integrity of code tokens.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>For example, http://stackoverflow.com/questions/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_5"><p>5985912/simpledateformat-bug<ref type="bibr" target="#b4">5</ref> For example, http://stackoverflow.com/questions/ 4382178/android-sdk-installation-doesnt-find-jdk</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This work was partially supported by Singapore MOE AcRF Tier-1 grant M4011165.020 and the NSFC Program (No.61572426), and National Key Technology R&amp;D Program of the Ministry of Science and Technology of China under grant 2015BAH17F01.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>the state-of-the-art distributed word representations (i.e., word embedding) to encode word semantics in dense lowdimensional real-valued vectors. At document level, we train a CNN to automatically learn the most informative word and sentence features for classifying the semantic relatedness between two knowledge units. The training of the CNN is guided by sufficient examples of different types of semantically linkable knowledge units. Due to the adoption of word-level and document-level semantics, our approach is robust to the lexical gap (i.e., sharing few words in common) between linkable knowledge units. Our experiments confirm the effectiveness and consistency of our approach for predicting multiclass semantically linkable knowl-edge units, compared with the well-designed baselines using TF-IDF word representations and human-crafted word and sentence/document features. In the future, we will enhance our approach by incorporating image and code-snippet semantics into our framework. We will also develop automated tool to help developers search and explore different types of linkable knowledge in Stack Overflow.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">How are related questions selected?</title>
		<ptr target="http://meta.stackexchange.com/questions/20473" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A survey of link prediction in social networks</title>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Al</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Zaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Social network data analytics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="243" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Emergence of scaling in random networks</title>
		<author>
			<persName><forename type="first">A.-L</forename><surname>Barabási</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Albert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">286</biblScope>
			<biblScope unit="issue">5439</biblScope>
			<biblScope unit="page" from="509" to="512" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">the Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detecting semantically equivalent questions in online user forums</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bogdanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Barbosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th Conference on Computational Natural Language Learning</title>
		<meeting>the 19th Conference on Computational Natural Language Learning<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07-30">2015. July 30-31, 2015. 2015</date>
			<biblScope unit="page" from="123" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hierarchical structure and the prediction of missing links in networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Clauset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">453</biblScope>
			<biblScope unit="issue">7191</biblScope>
			<biblScope unit="page" from="98" to="101" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Temporal link prediction using matrix and tensor factorizations</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Dunlavy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Kolda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Acar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data (TKDD)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Predicting tie strength with social media</title>
		<author>
			<persName><forename type="first">E</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Karahalios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI conference on human factors in computing systems</title>
		<meeting>the SIGCHI conference on human factors in computing systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="211" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distributional structure</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">S</forename><surname>Harris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Word</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="146" to="162" />
			<date type="published" when="1954">1954</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A practical guide to support vector classification</title>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Receptive fields of single neurones in the cat&apos;s striate cortex</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Hubel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Wiesel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of physiology</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="574" to="591" />
			<date type="published" when="1959">1959</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Why we twitter: understanding microblogging usage and communities</title>
		<author>
			<persName><forename type="first">A</forename><surname>Java</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Finin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th WebKDD and 1st SNA-KDD 2007 workshop on Web mining and social network analysis</title>
		<meeting>the 9th WebKDD and 1st SNA-KDD 2007 workshop on Web mining and social network analysis</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="56" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Short text similarity with word embeddings</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 24th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1411" to="1420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<title level="m">Convolutional neural networks for sentence classification</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The link-prediction problem for social networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liben-Nowell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American society for information science and technology</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1019" to="1031" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Link prediction in complex networks: A survey</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lü</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica A: Statistical Mechanics and its Applications</title>
		<imprint>
			<biblScope unit="volume">390</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1150" to="1170" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<title level="m">Efficient estimation of word representations in vector space</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Convolutional neural networks over tree structures for programming language processing</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Duplicate bug report detection with a combination of information retrieval and topic modeling</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Automated Software Engineering</title>
		<imprint>
			<biblScope unit="page" from="70" to="79" />
			<date type="published" when="2012">2012. 2012</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recalling the imprecision of cross-project defect prediction</title>
		<author>
			<persName><forename type="first">F</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Posnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Devanbu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering</title>
		<meeting>the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page">61</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Software Framework for Topic Modelling with Large Corpora</title>
		<author>
			<persName><forename type="first">R</forename><surname>Řehůřek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sojka</surname></persName>
		</author>
		<ptr target="http://is.muni.cz/publication/884893/en" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks</title>
		<meeting>the LREC 2010 Workshop on New Challenges for NLP Frameworks<address><addrLine>Valletta, Malta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-05">May 2010</date>
			<biblScope unit="page" from="45" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The probabilistic relevance framework: BM25 and beyond</title>
		<author>
			<persName><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Now Publishers Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An empirical study on recommendations of similar bugs</title>
		<author>
			<persName><forename type="first">H</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Tulio</forename><surname>Valente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Marques-Neto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SANER</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Nirmal: Automatic identification of software relevant tweets leveraging language model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="449" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Towards more accurate retrieval of duplicate bug reports</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Khoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 26th IEEE/ACM International Conference on Automated Software Engineering</title>
		<meeting>the 2011 26th IEEE/ACM International Conference on Automated Software Engineering</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A discriminative model approach for accurate duplicate bug report retrieval</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Khoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering</title>
		<meeting>the 32nd ACM/IEEE International Conference on Software Engineering</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="45" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improved duplicate bug report identification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Software Maintenance and Reengineering (CSMR), 2012 16th European Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="385" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th annual meeting of the association for computational linguistics</title>
		<meeting>the 48th annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">85</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A primer on kernel methods</title>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Vert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tsuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Kernel Methods in Computational Biology</title>
		<imprint>
			<biblScope unit="page" from="35" to="70" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Tagcombine: Recommending tags to contents in software information sites</title>
		<author>
			<persName><forename type="first">X.-Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Science and Technology</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1017" to="1035" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">It takes two to tango: Deleted stack overflow question prediction with text and meta features</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Correa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sureka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shihab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 40th Annual International Computers, Software &amp; Applications Conference (COMPSAC)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Massively compositional model for cross-project defect prediction</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nagappan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Hydra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Automated bug report field reassignment and refinement prediction</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shihab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Reliability. IEEE</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Elblocker: Predicting blocking bugs with ensemble imbalance learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shihab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Software Technology</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="93" to="106" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Automatic, high accuracy prediction of reopened bugs</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shihab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automated Software Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="75" to="109" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Tag recommendation in software information sites</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Working Conference on Mining Software Repositories</title>
		<meeting>the 10th Working Conference on Mining Software Repositories</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="287" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Predicting crashing releases of mobile applications</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shihab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kamei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement</title>
		<imprint>
			<publisher>ESEM</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Efspredictor: Predicting configuration bugs with ensemble feature selection</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sureka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 Asia-Pacific Software Engineering Conference (APSEC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="206" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Domain-specific cross-language relevant question retrieval</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Workshop on Mining Software Repositories</title>
		<meeting>the 13th International Workshop on Mining Software Repositories</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="413" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Software-specific named entity recognition in software engineering social content</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Foo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Q</forename><surname>Ang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kapre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 23rd IEEE International Conference on Software Analysis, Evolution and Reengineering</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The structure and dynamics of knowledge network in domain-specific q&amp;a sites: a case study of stack overflow</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kapre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Empirical Software Engineering</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Software-specific part-of-speech tagging: An experimental study on stack overflow</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kapre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Annual ACM Symposium on Applied Computing, SAC &apos;16</title>
		<meeting>the 31st Annual ACM Symposium on Applied Computing, SAC &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1378" to="1385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Predicting bug-fixing time: an empirical study of commercial software projects</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Versteeg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 International Conference on Software Engineering</title>
		<meeting>the 2013 International Conference on Software Engineering</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1042" to="1051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Expertise networks in online communities: structure and algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Ackerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Adamic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th international conference on World Wide Web</title>
		<meeting>the 16th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="221" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multi-factor duplicate question detection in stack overflow</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Science and Technology</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="981" to="997" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Towards more accurate content categorization of api discussions</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Program Comprehension</title>
		<meeting>the 22nd International Conference on Program Comprehension</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="95" to="105" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
