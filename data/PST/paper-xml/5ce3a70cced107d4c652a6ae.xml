<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ISPRS Journal of Photogrammetry and Remote Sensing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jianfeng</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Geography and Planning</orgName>
								<orgName type="institution">Sun Yat-Sen University</orgName>
								<address>
									<postCode>510275</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xinchang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Geographical Sciences</orgName>
								<orgName type="institution">Guangzhou University</orgName>
								<address>
									<postCode>510006</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">The College of Environment and Planning of Henan University</orgName>
								<orgName type="institution" key="instit2">Henan University</orgName>
								<address>
									<postCode>475000</postCode>
									<settlement>Kaifeng</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Qinchuan</forename><surname>Xin</surname></persName>
							<email>xinqinchuan@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Geography and Planning</orgName>
								<orgName type="institution">Sun Yat-Sen University</orgName>
								<address>
									<postCode>510275</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Guangdong Key Laboratory of Urbanization and Geo-simulation</orgName>
								<address>
									<postCode>510275</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ying</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Geography and Planning</orgName>
								<orgName type="institution">Sun Yat-Sen University</orgName>
								<address>
									<postCode>510275</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Guangdong Key Laboratory of Urbanization and Geo-simulation</orgName>
								<address>
									<postCode>510275</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pengcheng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Guangzhou Urban Planning and Design Survey Research Institute</orgName>
								<address>
									<postCode>510060</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">School of Geography and Planning</orgName>
								<orgName type="institution">Sun Yat-Sen University</orgName>
								<address>
									<postCode>510275</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ISPRS Journal of Photogrammetry and Remote Sensing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1016/j.isprsjprs.2019.02.019</idno>
					<note type="submission">Received 17 September 2018; Received in revised form 15 January 2019; Accepted 25 February 2019</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Building extraction Deep learning Convolutional neural networks Image classification Semantic segmentation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automated extraction of buildings from remotely sensed data is important for a wide range of applications but challenging due to difficulties in extracting semantic features from complex scenes like urban areas. The recently developed fully convolutional neural networks (FCNs) have shown to perform well on urban object extraction because of the outstanding feature learning and end-to-end pixel labeling abilities. The commonly used feature fusion or skip-connection refine modules of FCNs often overlook the problem of feature selection and could reduce the learning efficiency of the networks. In this paper, we develop an end-to-end trainable gated residual refinement network (GRRNet) that fuses high-resolution aerial images and LiDAR point clouds for building extraction. The modified residual learning network is applied as the encoder part of GRRNet to learn multi-level features from the fusion data and a gated feature labeling (GFL) unit is introduced to reduce unnecessary feature transmission and refine classification results. The proposed model -GRRNet is tested in a publicly available dataset with urban and suburban scenes. Comparison results illustrated that GRRNet has competitive building extraction performance in comparison with other approaches. The source code of the developed GRRNet is made publicly available for studies.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Extracting building information from remotely sensed data is important for a wide range of geographic and environmental applications. Geographic information of buildings is valuable to traditional applications such as cartography and image interpretation <ref type="bibr" target="#b32">(Noronha and Nevatia, 2001</ref>) that greatly relies on manual interpretation and building vectorization and advanced applications like three-dimensional city modeling, urban expansion analysis, and environment surveying <ref type="bibr" target="#b16">(Huang and Zhang, 2011)</ref>. Developing automatic and robust algorithms of building extraction is therefore a research frontier in the field of remote sensing.</p><p>Despite that tremendous efforts have been made in the last few decades to develop various building extraction methods, accurate and automatic building extraction is still challenging for both remote sensing and computer vision communities. There are mainly two difficult issues, including that (1) buildings in most of the scenes, especially the urban areas, have varied sizes and band reflectance and are often obscured by trees and their shadow <ref type="bibr" target="#b23">(Liu et al., 2017a)</ref>, and (2) the high intra-class and low inter-class variation of building objects in high-resolution remote sensing images makes it complex to extract the spectral and geometrical features of buildings <ref type="bibr" target="#b0">(Alshehhi et al., 2017)</ref>.</p><p>Based on the used data, the building extraction methods <ref type="bibr" target="#b21">(Lee et al., 2008)</ref> mainly include image-based <ref type="bibr" target="#b9">(Ghanea et al., 2016;</ref><ref type="bibr" target="#b15">Huang and Zhang, 2012)</ref>, LiDAR-based <ref type="bibr" target="#b6">(Du et al., 2017;</ref><ref type="bibr" target="#b29">Mongus et al., 2014;</ref><ref type="bibr" target="#b30">Niemeyer et al., 2014;</ref><ref type="bibr" target="#b37">Sampath and Shan, 2010;</ref><ref type="bibr" target="#b41">Wang et al., 2016)</ref> and data fusion-based methods <ref type="bibr" target="#b27">(Meng et al., 2012;</ref><ref type="bibr" target="#b35">Rottensteiner et al., 2005;</ref><ref type="bibr" target="#b48">Zarea and Mohammadzadeh, 2016)</ref>. High-resolution aerial and/ or satellite images provide valuable spectral, texture, and geometric information to distinguish buildings from non-building objects (e.g., roads, water, shadows, and vegetation) <ref type="bibr" target="#b9">(Ghanea et al., 2016)</ref>. The fastdeveloping technology of airborne LiDAR offers three-dimensional information of the land surface and allows for retrieval of high-precision digital terrain model (DTM) that is useful to extract aboveground objects <ref type="bibr" target="#b45">(Yan et al., 2015;</ref><ref type="bibr" target="#b49">Zhang, 2010)</ref>. As using multi-source data could provide complementary information on building objects <ref type="bibr" target="#b2">(Awrangjeb et al., 2010)</ref>, the methods that use both high-resolution images and LiDAR data (i.e., the data fusion-based methods) have shown to improve the building extraction results effectively rather than using data from single sources alone <ref type="bibr" target="#b49">(Zhang, 2010)</ref>.</p><p>Among the data fusion-based methods, pixel-based and object-oriented image classification are two commonly used approaches for building extraction. <ref type="bibr" target="#b11">Haala and Brenner (1999)</ref> conducted building extraction by using a pixel-based classification framework based on the combination of the normalized digital surface model (nDSM) and multispectral images. <ref type="bibr" target="#b10">Gilani et al. (2016)</ref> proposed a data-driven building extraction and regularization method using the detected candidate building regions and line segments in an image. <ref type="bibr" target="#b20">Khoshelham et al. (2010)</ref> conducted a comprehensive evaluation of five different data fusion-based methods that extract features at the pixel or object levels. Based on the ISPRS WG II/4 challenging dataset (http://www2. isprs.org/commissions/comm3/wg4/detection-and-reconstruction. html) that consists of both airborne high-resolution images and LiDAR point clouds, <ref type="bibr" target="#b34">Rottensteiner et al. (2014)</ref> made comparisons on the stateof-the-art methods of building extractions. They found that most building extraction algorithms can produce satisfactory results for buildings larger than 50 m 2 , but there is still room for improvement in the accurate extraction of small buildings and building boundaries. There remain limitations in the current data fusion-based methods. First, many methods <ref type="bibr" target="#b2">(Awrangjeb et al., 2010;</ref><ref type="bibr" target="#b27">Meng et al., 2012)</ref> use low-or mid-level features to distinguish building objects from nonbuilding objects and it often needs to apply certain threshold settings or empirical decisive rules when using low-or mid-level features. Second, many algorithms <ref type="bibr" target="#b14">(Hermosilla et al., 2011)</ref> take image segmentation as a prerequisite step, of which the results are highly dependent on the segmentation parameter settings and are often easily affected by factors such as solar radiation, shadows and even random noise in remote sensing images <ref type="bibr" target="#b8">(Fu et al., 2017)</ref>.</p><p>Recent studies have demonstrated that deep Convolutional Neural Networks (CNNs) could achieve impressive performance on processing remote sensing images, such as scene classification and object detection. CNNs could automatically learn not only low-and middle-level features but also high-level semantic features from the raw images <ref type="bibr" target="#b51">(Zhao et al., 2017)</ref>. Prior methods have used CNNs for semantic segmentation of remotely sensed data <ref type="bibr" target="#b33">(Paisitkriangkrai et al., 2016;</ref><ref type="bibr" target="#b36">Saito and Aoki, 2015)</ref>, in which each pixel is labeled with the category of its enclosing region. However, these methods in the frame for category classification often generate lower resolution feature maps than the input images and shows coarse results in pixel-wise labeling <ref type="bibr" target="#b3">(Badrinarayanan et al., 2017)</ref>. <ref type="bibr" target="#b25">Long et al. (2015)</ref> proposed a fully convolutional neural network (FCN) that performs both image segmentation and pixel-wise labeling synchronously via an end-to-end encoder-decoder framework. Benefitting from the ability to get fullresolution classification maps, FCN has now become a common framework for some state-of-the-art semantic segmentation methods <ref type="bibr" target="#b26">(Marmanis et al., 2018)</ref>. CNNs and FCNs have been widely used in urban objects extraction such as buildings, roads, and trees <ref type="bibr" target="#b5">(Cheng et al., 2017;</ref><ref type="bibr" target="#b19">Kaiser et al., 2017;</ref><ref type="bibr" target="#b23">Liu et al., 2017a;</ref><ref type="bibr" target="#b39">Sun et al., 2018)</ref>. <ref type="bibr" target="#b28">Mnih and Hinton (2010)</ref> established a deep neural network for large-scale road and building detection using aerial images in Massachusetts. <ref type="bibr" target="#b33">Paisitkriangkrai et al. (2016)</ref> proposed a new method that uses both deep features and hand-crafted features to perform semantic labeling of aerial images. <ref type="bibr" target="#b47">Yuan (2017)</ref> designed a deep convolutional network that integrates hierarchical layer activations for pixel-wise prediction of buildings. <ref type="bibr" target="#b44">Xu et al. (2018)</ref> designed a new FCN model for building extraction and employed both hand-crafted features and the guided filtering technique to improve the classification results. <ref type="bibr" target="#b42">Wu et al. (2018)</ref> proposed an end-to-end segmentation network that synthesizes the multi-stage supervision technique. The above-mentioned studies not only develop and validate new CNNs and FCNs models on building extractions but also generously offer public available remote sensing datasets for comparative scientific studies.</p><p>There are still issues to be properly addressed in the current studies based on FCNs. First, FCNs perform pixel-wise classification by using the high-level but coarse-resolution semantic features from CNNs. Because the rich low-level image features such as building edges and building corners are largely neglected, FCNs often produce "blobby" extraction results <ref type="bibr" target="#b4">(Bischke et al., 2017)</ref>. An effective solution is to transmit the low-level features into the decoder part of FCN by skipping connections <ref type="bibr" target="#b22">(Lin et al., 2017;</ref><ref type="bibr" target="#b24">Liu et al., 2017b)</ref> or reusing the maximum indices of the pooling layers <ref type="bibr" target="#b3">(Badrinarayanan et al., 2017;</ref><ref type="bibr" target="#b31">Noh et al., 2015)</ref>. Many existing methods do not consider feature selection during the process of transmission <ref type="bibr" target="#b41">(Wang et al., 2017)</ref>, such that redundant features are generated and the learning efficiency of the network is reduced. Second, the transmitted features often contain categorical ambiguity <ref type="bibr" target="#b17">(Islam et al., 2017)</ref> or non-boundary related information that has no effect on the refinement of the classification results. <ref type="bibr" target="#b26">Marmanis et al. (2018)</ref> fused the boundary detection result from HED network <ref type="bibr" target="#b43">(Xie and Tu, 2017)</ref> with the deep encoder-decoder network to compensate for the edge loss during the down-sampling process. <ref type="bibr" target="#b41">Wang et al. (2017)</ref> proposed a gated segmentation neural network (referred to as GSN) that can adaptively select the effective information for feature fusion and obtained competitive segmentation results on the ISPRS labeling benchmark. <ref type="bibr" target="#b17">Islam et al. (2017)</ref> introduced a gate unit to the decoder part of FCN models (referred to as G-FRNet) that could filter out the features of categorical ambiguity. Inspired by the gate mechanisms proposed in GSN and G-FRNet, we intend to design a new gated network to optimize FCN in building extraction as the useful features can be selected via gate units. Third, most FCN models focus on extracting buildings from aerial images and there is still a need to understand their performance on the dataset fused from both high-resolution optical images and LiDAR point clouds. Adding LiDAR data to FCN models could potentially improve the accuracy of building extraction but also increase the learning difficulties of the networks because the initialization parameters of many FCN models are learned from natural images. Comprehensive evaluation of different FCN models using the fused data for building extraction would also be valuable.</p><p>The main contributions of this study are: (1) proposing a new gated semantic segmentation neural network that could be used to extract buildings from high-resolution aerial images and LiDAR data, (2) analyzing the effect of gated feature labeling unit for the refinement of the coarse classification results, and (3) comparing the performance of the state-of-the-art deep models using a large dataset from different city scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methodology</head><p>Here we develop an end-to-end trainable gated residual refinement network (GRRNet) for building extraction using both high-resolution aerial images and LiDAR data. The developed network is based on a modified residual learning network <ref type="bibr" target="#b13">(He et al., 2016)</ref> that extracts robust low/mid/high-level features from remotely sensed data. A new gated feature labeling (GFL) unit is introduced to reduce the unnecessary feature transmission and refine the coarse classification maps in each decoder stage of the network. We first introduce background that is relevant to our proposed model briefly and then describes GRRNet in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Deep residual network</head><p>A typical CNN consists of three types of layers, including the convolutional layer, the rectified linear unit (ReLU) layer, and the pooling layer. The convolutional layer convolves the input image with a set of filters and each filter generates a feature map in the output image. The ReLU layer generates an output of 0 if the value in the feature map is less than 0 and otherwise generates an output that is equal to the input feature. The pooling layer obtains abstract features by compressing the input feature maps and simplifies the computational complexity of the network. By combining these layers in an orderly manner, CNN learns the low/mid/high-level features that are more robust than traditional hand-crafted features in the input images <ref type="bibr" target="#b50">(Zhang et al., 2016)</ref>. Previous studies <ref type="bibr" target="#b13">(He et al., 2016)</ref> have found that increasing the depth of neural networks do not necessarily improve the performance of CNN. Instead, the accuracy may degrade after a saturation. This phenomenon is often referred to as the degradation problem. <ref type="bibr" target="#b13">He et al. (2016)</ref> recently proposed a Residual Network (ResNet) that is considerably deeper than previous networks like VGG-16 <ref type="bibr" target="#b38">(Simonyan and Zisserman, 2014)</ref> and solved the degradation problem effectively. Our study chooses ResNet-50, the well pre-trained ResNet model (https://github.com/ KaimingHe/deep-residual-networks) for the ImageNet challenge, as our basic block for building extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">Encoder-decoder network architecture</head><p>FCN models, such as FCN8s <ref type="bibr" target="#b25">(Long et al., 2015)</ref>, SegNet <ref type="bibr" target="#b3">(Badrinarayanan et al., 2017)</ref>, DeconvNet <ref type="bibr" target="#b31">(Noh et al., 2015)</ref>, and CNN-FPL <ref type="bibr" target="#b40">(Volpi and Tuia, 2017)</ref>, often have an encoder-decoder architecture. The encoder-decoder architecture could fully utilize CNNs to extract image features and effectively solve the end-to-end learning problem of semantic segmentation <ref type="bibr" target="#b25">(Long et al., 2015)</ref>. The encoder part could be a deep CNN (e.g., VGG-16, ResNet-50) that consists of a series of convolutional operations, non-linear operations, and pooling operations and the encoder part obtains high-level semantic features with spatial dimensions smaller than the input images. Different from the encoder part, the decoder part enlarges the features obtained by the encoder using upsampling layers or max unpooling layers and produces the final prediction result with spatial dimensions the same as the input images. The max unpooling layer enlarges the features by reusing the locations of maxima within each max-pooling layer <ref type="bibr" target="#b3">(Badrinarayanan et al., 2017)</ref> and the upsampling layer is often a learnable deconvolutional layer or bilinear interpolation layer <ref type="bibr" target="#b31">(Noh et al., 2015)</ref>. The proposed model has the encoder-decoder architecture and employs the upsampling layer of bilinear interpolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3.">FCNs for building extraction</head><p>The FCN model allows for predicting the probability that each pixel belongs to the classes of building or non-building in an image. The final classification map for a given image can be obtained by calculating the category corresponding to the maximum probability of each pixel. The function to extract buildings can be described as follows:</p><formula xml:id="formula_0">k p x n N argmax ( ), {1, 2, , } k k n {0,1} = (1)</formula><p>where x n denotes the n-th pixel and N is the total number of pixels in the given image; k is a binary value, where 0 and 1 represent the non- building category and building category, respectively; p x ( )</p><formula xml:id="formula_1">k n</formula><p>is the posterior probability of</p><p>x n belonging to the category k, and it is esti- mated by an FCN model with parameters .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Network architecture</head><p>Our proposed GRRNet consists of an encoder network and a decoder network (Fig. <ref type="figure" target="#fig_2">1</ref>). The encoder network can receive multiple input images from both remote sensing images and LiDAR data. This study uses the red (R), green (G) and near-infrared (NIR) bands from multispectral images and the LiDAR-derived nDSM as the input data for tests. The output of the encoder network is a binary classification map, where 0 and 1 represent non-building and building, respectively. The spatial dimensions of each input image and output classification map are set as 480 × 480 pixels.</p><p>The encoder network (Fig. <ref type="figure" target="#fig_2">1</ref>) is largely based on ResNet-50 and the layers are grouped into blocks according to the size of output features (the blocks in different sizes are displayed in different colors in Fig. <ref type="figure" target="#fig_2">1</ref>).</p><p>A more detailed description of ResNet-50 can be found in <ref type="bibr" target="#b13">(He et al., 2016)</ref> and here ResNet-50 is modified as follows to improve the model performance. First, a convolutional layer is added at the beginning of ResNet-50, followed by a batch-normalized (BN) layer and a ReLU layer. The newly added convolutional layer allows for receiving multiple input image bands and produces 64 features in the same size as the input data. The idea is to break the limitations of three-band input of ResNet-50 and provide the same size feature maps for subsequent upsampling operations. As a result, the band number for the next convolutional layer in ResNet-50 is changed from 3 to 64. Second, the last three layers of ResNet-50 are replaced with a dropout layer to avoid overfitting. In each block of ResNet-50, identity shortcuts are repeatedly used two or more times to optimize the training of the network while maintaining the feature size in a block unchanging. The projection shortcuts are used between every two different size blocks to increase the number of output bands and reduce the size of features. Within the encoder network, we could obtain 6 feature blocks with different sizes that range from 15 × 15 to 480 × 480 pixels.</p><p>The output features of the encoder (with 2048 bands) are passed to the decoder and are then convolved into coarse labeling maps with only two bands (2 × 15 × 15 pixels). A standard 2 × upsampling operation and a convolutional operation are repeatedly used in each coarse labeling map for five times to obtain the final prediction map (480 × 480 pixels). Note that two issues need to be solved explicitly here. First, the decoder network does not make good use of rich low/mid-level features obtained from the encoder, making it easily produce blobby extraction results <ref type="bibr" target="#b4">(Bischke et al., 2017)</ref>, especially for small building objects. Second, transmitting the encoder features into the decoder network without any feature selection has no effect on the refinement of coarse labeling maps, because these features usually contain a large amount of non-boundary related information and are of categorical ambiguity. A new component, named as the gated feature labeling (GFL) unit, is therefore introduced to solve the issues related to feature selection and feature transmission. Details on the GFL are illustrated in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Gated feature labeling unit</head><p>Fig. <ref type="figure" target="#fig_3">2</ref> illustrates an example for different refined modules of CNNs that transmit the rich low/mid-level features into the upsampling stages. SegNet <ref type="bibr" target="#b3">(Badrinarayanan et al., 2017)</ref> upsamples the decoder features using the max pooling indices followed by a trainable decoder filter bank (Fig. <ref type="figure" target="#fig_3">2a</ref>). Res-U-Net <ref type="bibr" target="#b44">(Xu et al., 2018)</ref> upsamples the decoder features and the corresponding encoder features separately and concatenates them for the next upsampling stage (Fig. <ref type="figure" target="#fig_3">2b</ref>). TD-CEDN <ref type="bibr" target="#b24">(Liu et al., 2017b)</ref> first applies a deconvolutional layer to the decoder features and then concatenated the upsampled results with the lower encoder features followed by convolutional, batch-normalized, ReLU and dropout layers (Fig. <ref type="figure" target="#fig_3">2c</ref>). In the upsampling stages of the above-mentioned modules, all encoder features (or max pooling indices) are transmitted into the decoder without selection such that the number of decoder features is often numerous. Different from these modules, the proposed GFL unit integrates the upper encoder features into the upsampling stages such that the decoder features in GRRNet are then restricted.</p><p>The GFL unit is illustrated in Fig. <ref type="figure" target="#fig_3">2e</ref> and described as follows. For a GFL unit G c , its inputs include the lower encoder features f e l and upper encoder features f e u coming from a specific encoder stage, respectively. f e l have large spatial dimensions and small receptive fields and f e u have small spatial dimensions and large receptive fields. The way that the GFL unit integrates the input features can be described as follows: </p><formula xml:id="formula_2">M BN BN f w UP f w ( ([ ]) ([ ]))</formula><formula xml:id="formula_3">f M w d u d c 2 3 3 2 = + × (4)</formula><p>One difference between the Gate unit as described in <ref type="bibr" target="#b17">(Islam et al., 2017)</ref> (Fig. <ref type="figure" target="#fig_3">2d</ref>) and the GFL unit is that the selected encoder features   transmitted by the GFL unit varies with the encoder level at which the GFL unit is located. The lower the encoder stage, the more the features are transmitted. Because lower-level features are considered to contain more detailed geometry, color, and texture characteristics of building objects than the upper-level ones, the GFL unit allows for preserving useful low-level features for building object identification. As shown in Fig. <ref type="figure" target="#fig_2">1</ref>, five GFL units in total are applied in GRRNet, and the numbers of features that are transmitted by the GFL unit are 20, 16, 12, 8 and 4, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Class balancing</head><p>The objective function for training GRRNet is the cross-entropy loss function as calculated by summing up all pixels in a mini-batch. Let x n b ( , ) be the n-th pixel in the b-th patch image and y n b ( , ) be the category label. The loss calculation is defined as follows:</p><formula xml:id="formula_4">p m m n N b B exp( ) exp( ) , {1, 2, , }, {1, 2, , } k n b k n b k k n b ( , )<label>( , )</label></formula><p>{0,1}</p><p>( , )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>=</head><p>(5)</p><formula xml:id="formula_5">Loss N B I y p I y p 1 ( { 0}log { 1}log ) n N b B n b n b n b n b 1 1 ( , ) 0 ( , ) ( , ) 1 ( , ) = × = + = = = (6)</formula><p>where k is binary where 0 represents non-building and 1 represents building; N is the total number of pixels in a patch image; B is the mini- batch size; and the values of 1 for other cases. The loss weights of both building and non-building categories are the same in Eq. ( <ref type="formula">6</ref>) but in most scenes, non-building pixels are more than building pixels. Class imbalance could result in an unbalanced distribution of features in the training datasets, making the classifiers tend to classify a pixel into a majority class. To solve the problem of class imbalance, we apply the median frequency balancing method <ref type="bibr" target="#b7">(Eigen and Fergus, 2015)</ref> to calculate the loss weights of different categories as described as follows:</p><formula xml:id="formula_6">w f f f pix num img num W H _ ( _ ) k k k k k median = = × × (7)</formula><p>where w k denotes the loss weight of category k; W and H denote the width and height of a single image; pix num _ k denotes the pixel numbers in category k; img num _ k denotes the number of images where the pixel in category k is present; f k denotes the pixel frequency in category k; and f median denotes the median of all f k . Finally, the class-weighted loss function is modified as follows:</p><formula xml:id="formula_7">Loss N B I y p w I y p w 1 ( { 0}log { 1}log ) n N b B n b n b n b n b 1 1 ( , ) 0 ( , ) 0 ( , ) 1 ( , ) 1 = × = + = = = (8)</formula><p>where w 0 and w 1 denote loss weights for non-building and building categories, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiment design</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Study materials</head><p>A publicly available dataset (https://dx.doi.org/10.6084/m9. figshare.3504413) compiled by the 2016 Data Plus Energy Analytics Group (hereinafter referred to as the DataPlus dataset) is used. The used dataset contains (1) high-resolution aerial orthoimages with the spatial resolutions ranging from 0.15 m to 0.3 m as obtained from the United States Geological Survey (USGS), (2) LiDAR point clouds obtained from the LiDAR surveys conducted by state and federal agencies, and (3) building footprints corresponding to the images as downloaded from the Open Street Maps (OSM).</p><p>14 orthoimages that have the size of 5000 × 5000 pixels that cover five different cities were chosen as the experimental dataset. All highresolution images have four spectral bands, including near-infrared (NIR), red (R), green (G) and blue (B). The LiDAR point clouds were used to derive the nDSM data as inputs for the networks. To mitigate the sparse noise effects, the noise points were first filtered from the raw point clouds using LasTools. The bare-earth and first-return LiDAR data were then interpolated using the natural neighbor interpolation method into the digital elevation model (DEM) and DSM. nDSM was simply generated as the difference between DSM and DEM. All nDSM data derived from LiDAR were processed to have the same resolution as the corresponding orthoimages. The OSM polygons were processed to label the ground reference images into the building and non-building categories. Because the used remote sensing images and OSM vector data were acquired at different time, the OSM polygons, if incorrect or missing, were manually corrected. The OSM polygons were rasterized to obtain building footprints for network training and evaluation. In total, 9 out of the 14 annotated images were used for network training and the remaining 5 images (each for one individual city) were used for network testing. As shown in Table <ref type="table" target="#tab_0">1</ref>, the DataPlus dataset consists of images with varied spatial resolutions, surface elevations, and building densities in urban and suburban scenes, representing varied environmental conditions that are ideal for understanding the robustness of the developed and tested methods.</p><p>Fig. <ref type="figure" target="#fig_5">3</ref> shows the examples of the training set images and the corresponding close-ups in the marked areas. The urban areas (e.g., Fig. <ref type="figure" target="#fig_5">3e</ref> in San Francisco) have higher building densities than the suburban areas (e.g., Fig. <ref type="figure" target="#fig_5">3a</ref> in Arlington). Buildings in the suburban areas often have relatively low heights and are easily overshadowed by surrounding trees (e.g., Fig. <ref type="figure" target="#fig_5">3d</ref> in Norfolk), whereas buildings in the urban areas are obviously higher than adjacent objects (e.g., Fig. <ref type="figure" target="#fig_5">3c</ref> in New York). These scenes with contrasting building densities, building sizes, and surrounding circumstances make it challenging for the task of automatic building extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Comparative studies using different networks</head><p>Five state-of-the-art FCN models, including SegNet <ref type="bibr" target="#b3">(Badrinarayanan et al., 2017)</ref>, DeconvNet <ref type="bibr" target="#b31">(Noh et al., 2015)</ref>, CNN-FPL <ref type="bibr" target="#b40">(Volpi and Tuia, 2017)</ref>, V-FuseNet <ref type="bibr" target="#b1">(Audebert et al., 2017)</ref>, and Res-U-Net <ref type="bibr" target="#b44">(Xu et al., 2018)</ref>, were used for comparisons. These methods were selected because all of them have already proven effective in semantic labeling and/or building extraction for remote sensing images and all of them are open source with easy implementation and accept multiple input images including the height information data. Details for each network can be found in the corresponding publication and here we only provide a brief summary to highlight the network characteristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">SegNet</head><p>SegNet is a classic deep learning method and is often used as a baseline for evaluating the performance of semantic segmentation methods because it has elegant encoder-decoder architecture and has high efficiency. The pooling indices in the encoder part in SegNet are reused in the decoder part <ref type="bibr" target="#b3">(Badrinarayanan et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">DeconvNet</head><p>Noh et al. ( <ref type="formula">2015</ref>) proposed a deconvolution network where the encoder part is based on VGG-16 and the decoder part consists of a series of deconvolutional and unpooling layers. This model achieved the state-of-the-art performance on the PASCAL VOC 2012 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">CNN-FPLaaa</head><p>Volpi and Tuia (2017) designed a full patch labeling CNN model where deconvolutions are used to upsample the spatially coarse feature maps back to the initial resolution. CNN-FPL achieved the results aligned with state-of-the-art models on ISPRS Vaihingen and Potsdam challenging datasets without any postprocessing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4.">V-FuseNet</head><p>Audebert et al. ( <ref type="formula">2017</ref>) proposed a novel Fuse-Net based architecture for early fusion of the LiDAR data and multispectral images. A "virtual" encoder that fuses activations from the other encoders with convolution and summation operations is embedded in the encoder network. V-FuseNet obtained results that are competitive with the state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5.">Res-U-Net</head><p>Xu et al. ( <ref type="formula">2018</ref>) designed a fully convolutional network for building extraction, where the deep residual network acts as the encoder part and a guided filter is used for postprocessing. The input images include four spectral bands (NIR-R-G-B) and additional hand-crafted features like NDVI and nDSM. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Comparative studies using different refined modules</head><p>To understand whether the proposed gated feature labeling unit gain an advantage over others, we take GRRNet with only 2× upsampling operations in the decoder part as the baseline and added four different refined modules to the baseline for comparisons, including (1) baseline with the feature fusion unit (Fig. <ref type="figure" target="#fig_3">2c</ref>) (referred to as Baseline + FF), (2) baseline with the gate unit (Fig. <ref type="figure" target="#fig_3">2d</ref>) (referred to as Baseline + GU), (3) baseline with the GFL unit (Fig. <ref type="figure" target="#fig_3">2e</ref>) but only data of two bands are transmitted (referred to as Baseline + GFL-2), and (4) the proposed GRRNet (referred to as Baseline + GFL).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Method implementation</head><p>Data augmentation techniques were applied to the training set images to avoid network overfitting and improve model efficiency. Each input image was cropped to create a sequence of 480 × 480 pixel patches with an overlap of 200 pixels. The patches were rotated every 90 degrees in the clockwise direction and were also mirrored in both the horizontal and vertical directions. As a result, the training dataset contains 15,606 patches in total. At the inference stage, we create patches with an overlap of 100 pixels from each test image without performing data augmentation. The final probability map was obtained by merging all the patch probability maps and the prediction values of the overlapping pixels were derived as the mean values of all the predictions.</p><p>The proposed network of GRRNet was implemented using Caffe <ref type="bibr" target="#b18">(Jia et al., 2014)</ref> on an NVIDIA GTX Titan X GPU. The network was trained with stochastic gradient descent (SGD) using the initial learning rate of 0.01, the weight decay of 0.0005, the momentum of 0.9 and the bath size of 4. The total iteration number was set as 40,000 and the learning rate was reduced to one-tenth of the original every 8000 iterations. The encoder part parameters in GRRNet were initialized with the pretrained ResNet-50 model. All the other parameters were initialized using the techniques introduced by <ref type="bibr" target="#b12">(He et al., 2015)</ref>. The NIR-R-G composite images and nDSMs were fed into GRRNet and other comparative networks. The ground reference data were used for supervised training.</p><p>Note that: (1) V-FuseNet was trained using a smaller patch size (128 × 128 pixels) than GRRNet as the GPU memory was limited to 12 GB, (2) compared to the literature, the input images of Res-U-Net in this study only include four bands (i.e., NIR-R-G and nDSM, the same as GRRNet used) other than seven bands, and no post-processing was performed for the classification results. All configurations for the other networks are the same as GRRNet, and all the model weights were initialized using the corresponding pre-trained models (e.g., VGG-16 and ResNet-101) for semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Accuracy assessment</head><p>Two commonly used metrics, namely the overall accuracy (OA) and the mean intersection over union (mean IoU), were used to assess the performance of different methods. OA is the ratio of the correctly classified pixel numbers to the total pixel numbers in an individual image or the entire image dataset. IoU, also known as the Jaccard similarity coefficient, provides statistical accuracies that penalize false positives (FPs). For each category, the IoU score is the ratio of the correctly classified pixel numbers to the total number of ground reference pixels and the detected pixels in the corresponding category as follows: </p><formula xml:id="formula_8">IoU TP TP FP FN k k K k k = + + (9)</formula><p>where TP k (true positive) denotes the number of pixels in the category k that present in both reference and prediction maps; FP k (false positive) denotes the number of pixels in the category k that only present in the prediction map but not in the reference map; FN k (false negative) de- notes the number of pixels in the category k that only present in reference map but not detected in the prediction map. In our experiments, the mean IoU score of all categories is calculated for both the individual image and the entire image dataset, respectively. The receiver operating characteristic (ROC) curve is plotted to compare the binary classification accuracy of different FCN models. The ROC curve illustrates the trade-off between the true positive rate (TPR) and the false positive rate (FPR) with varying probability thresholds. The larger area under the ROC curve (AUC) indicates better model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Deep learning model comparisons</head><p>Fig. <ref type="figure" target="#fig_6">4</ref> displays the test images and the classification results using different deep models across five city scenes. Visually, V-FuseNet (Fig. <ref type="figure" target="#fig_6">4e</ref>) and GRRNet (Fig. <ref type="figure" target="#fig_6">4g</ref>) obtained better classification results than other models in the urban and suburban scenes. SegNet (Fig. <ref type="figure" target="#fig_6">4b</ref>) and DeconvNet (Fig. <ref type="figure" target="#fig_6">4d</ref>) performed well in Norfolk than in the suburban areas of Arlington and New Haven, where water pixels and pixels with higher elevations were easily misclassified as buildings. In the urban area of San Francisco, there are many FNs and FPs in the classification results of both SegNet and DeconvNet, implying that the use of unpooling layers in SegNet and DeconvNet for feature transmission could not refine the upsampled results in this study. Although the unpooling layers are used in the decoder part of V-FuseNet, the corresponding encoder features are first convolved and transmitted by a third "virtual" encoder that performs feature selection and could refine the classification results. CNN-FPL (Fig. <ref type="figure" target="#fig_6">4c</ref>) generally performed better than SegNet and DeconvNet but still frequently misclassified water pixels as building pixels in Arlington and did not detect the building pixels in the central area of New York City. Res-U-Net (Fig. <ref type="figure" target="#fig_6">4f</ref>) performed better in the suburban areas than in the urban areas and correctly classified the water pixels in both Arlington and Norfolk. The FNs and FPs produced in the New York City and San Francisco indicate that Res-U-Net does not perform well enough in the urban scenes.</p><p>Fig. <ref type="figure" target="#fig_7">5</ref> shows the close-ups (as marked in yellow rectangles in Fig. <ref type="figure" target="#fig_6">4a</ref>) of the tested images and the classification results for detailed inspection. Most buildings were identified correctly and completely using both V-FuseNet and GRRNet, except that some FPs were generated in the classification result of V-FuseNet in New Haven (Fig. <ref type="figure" target="#fig_7">5e</ref>). The results for building boundaries demonstrated that V-FuseNet and GRRNet performed well on boundary refinement. The other four models however failed to detect the buildings completely in New Haven and New York City scenes as many FPs were found in the areas covered by roadside trees or shadows (Fig. <ref type="figure" target="#fig_7">5b, c, d, and f</ref>), indicating that these models did not fully utilize the features of the input data.</p><p>Table <ref type="table">2</ref> summarizes the quantitative results obtained using different methods. GRRNet generated the best result with the overall accuracy of 96.20% and the mean IoU score of 88.05% among all methods that have the encoder-decoder network architectures. GRRNet outperforms all other models in the test images of Arlington, New Haven, Norfolk, and San Francisco. V-FuseNet achieved the best performance for the data of the New York City. CNN-FPL achieved an overall accuracy of 92.44% but the resulted mean IoU score was nearly 9% lower than that of GRRNet. Res-U-Net generated comparable results with CNN-FPL in suburban areas of Arlington, New Haven, and Norfolk but did not perform well enough in urban scenes like San Francisco, where Res-U-Net only obtained the mean IoU score of 50.44% and the overall accuracy of 68.09%. The results imply that the strategy that Res-U-Net directly concatenates the encoder features with the decoder features without feature selection is not stable for building objects refinement.</p><p>Fig. <ref type="figure">6</ref> shows the ROC curves of all deep learning models for different scenes. Consistent with the statistical results in Table <ref type="table">2</ref>, GRRNet and V-FuseNet are shown to perform reasonably well. CNN-FPL has a stable classification performance for different scenes and is superior to Res-U-Net, SegNet, and DeconvNet. Res-U-Net has a competitive performance with respect to CNN-FPL in the Arlington scene, but it has low accuracies and is unstable in other scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Model comparisons using different refined modules</head><p>Fig. <ref type="figure">7</ref> exhibits the building extraction results of GRRNet (Baseline + GFL) and its variants. As shown in Fig. <ref type="figure">7c</ref>, Baseline + FF did not perform well as compared with other models. Due to the categorical ambiguity of the low-level encoder features, simply transmitting lowlevel features into a decoder network does not improve the baseline results. Both the gate unit and the GFL unit could improve the classification ability as compared to the baseline method.</p><p>Fig. <ref type="figure" target="#fig_9">8</ref> exhibits close-up views of the results. Baseline + FF frequently misclassified water pixels and building pixels (Fig. <ref type="figure" target="#fig_9">8c</ref>). The baseline model produced better classification results than Baseline + FF and the obtained results in New York city were competitive with the gate units applied models (Fig. <ref type="figure" target="#fig_9">8d-f</ref>). The gate units applied models obtain similar classification results in Arlington, New York City, and Norfolk. However, in the New Haven suburban area, an obvious FP patch appeared in Baseline + GU result (Fig. <ref type="figure" target="#fig_9">8d</ref>). Moreover, in the San Francisco urban scene, a football field is found misclassified as a building object. Both Baseline + GU and Baseline + GFL-2 cannot completely detect the white-roofed building, whereas Baseline + GFL extract this building successfully and can identify the areas covered by overpasses correctly (Fig. <ref type="figure" target="#fig_9">8f</ref>).</p><p>Table <ref type="table">3</ref> lists the quantitative results of GRRNet and its variants with different refined modules. The baseline model only achieves overall accuracy of 94.20% and mean IoU of 83.37%, but it still outperforms SegNet, DeconvNet, CNN-FPL, and Res-U-Net (Table <ref type="table">2</ref>), indicating that the basic network architecture of GRRNet is suitable for building extraction task. Baseline + FF achieves a better result in San Francisco but poor results in other city scenes. However, Baseline + GU significantly outperforms Baseline + FF, indicating that the feature selection operations are needed and join the upper encoder features to the transmission helps to improve the classification ability of the network. With the GFL unit, both Baseline + GFL-2 and Baseline + GFL further improve the baseline overall accuracy and mean IoU score by nearly 2% and 5%, respectively. The more mIoU scores increase, the better refinement in the edge of the classification results. Our proposed GFL units exhibit better performance than the original gate unit. Fig. <ref type="figure" target="#fig_11">9</ref> illustrates the gate units applied models (e.g. Baseline + GU, GRRNet) perform better in the task of building extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">The influence of data input strategies</head><p>Fig. <ref type="figure" target="#fig_12">10</ref> shows the statistical results for different data input strategies on GRRNet. The NIR-R-G and R-G-B composite images produced similar classification accuracies and the combinations of LiDAR-derived images and spectral images significantly improved the classification results as compared with using spectral images alone. When the nDSM image is included, the overall accuracies and the mean IoU scores of GRRNet could increase by approximately 2.5% and 6.0%, respectively. Using nDSM as the unique input can obtain a slightly lower accuracies than the "NIR-R-G-nDSM", which indicates that the relative elevation information of nDSM has positive effects on building extraction. On the flip side, the introduction of spectral information can further improve the performance of nDSM. Using DSM instead of nDSM could not improve the results greatly because the nDSM data remove most "bareearth noise" in the data preprocessing step. In our experiments, the NDVI data do not have obvious impacts on the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Model efficiency analysis</head><p>Table <ref type="table" target="#tab_3">4</ref> lists the computing statistics of the deep learning models and the variants of GRRNet. CNN-FPL requires fewer computing resources and less inference time than others because CNN-FPL has the custom and shallow encoder-decoder architecture. DeconvNet needs much larger computing resources and longer training time than other models except V-FuseNet. Because the FuseNet-based architecture has to deal with two feature branches from the optical image and the height image separately, V-FuseNet has the longest inference time. GRRNet (i.e., Baseline + GFL) requires less inference time of 81.44 ms and smaller model size of 91.57 MB than other models because the ResNetbased encoder and the gated features transmitted at each upsampling stage. Overall, GRRNet shows to be more efficient than most models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussions</head><p>The reasons for the excellent performance of our model are as follows. First, the encoder part of GRRNet is based on the modified version of ResNet-50, which can effectively solve the degradation problem and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>The statistical results obtained using the deep learning models. OA denotes the overall accuracy and mIoU denotes mean intersection over union. The bold values denote the best result and the underlined values denote the second best result achieved by models.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3</head><p>The statistical results obtained using GRRNet (Baseline + GFL) and its variants. OA denotes the overall accuracy and mIoU denotes the mean intersection over union. The bold values denote best result and the underlined values denote the second best result achieved by models.  The developed GRRNet could still be improved potentially. First, the registration between multi-source data is still a key factor affecting the accuracy of building extraction, and it also brings challenges for many existing studies <ref type="bibr" target="#b46">(Yang and Chen, 2015)</ref>. In the DataPlus dataset, we found that the ground truth annotations are more consistent with nDSM than with aerial images. This is mainly due to the geometric distortion of the images. We do not have accurate registration of images and point cloud data because it is still a challenging task and may cause more uncertainty in the experiments. Second, like many other methods,  LiDAR point clouds are first rasterized into 2D height images and then efficiently processed by GRRNet. However, this way may lead to loss of accurate 3D scene information. Therefore, it is promising to develop 3D CNN models for feature extraction and further fuse with image features learned from FCN models. Finally, in the experiments, we only focus on analyzing the performance of different FCN models. In other words, traditional methods, such as the thresholding-based <ref type="bibr" target="#b14">(Hermosilla et al., 2011)</ref> and object-based <ref type="bibr" target="#b20">(Khoshelham et al., 2010)</ref> methods, have not been compared, since the learning and inference procedures of FCN models are fully automated and quite different from traditional methods. In the future, it is needed to provide a more comprehensive comparison of these methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>Deep CNNs have now become increasingly important in semantic segmentation of remote sensing images and have found to be efficient in the extraction of urban objects. This study proposed a novel end-toend trainable gated residual refinement network (GRRNet) for building extraction using both high-resolution aerial images and airborne LiDAR data. The encoder part of GRRNet is based on a modified residual learning network and the decoder part uses the gated feature labeling unit to refine the upsampled classification results. The proposed GRRNet could learn multi-level image features and perform semantic pixel labeling of an entire image, thereby simplifying the extensive processes of both designing hand-crafted features and pre-segmentation of input images. The upsampling stage of FCN models is also designed to deal with the effect of gated feature transmission. The model is evaluated using a publicly available dataset that consists of images with different spatial resolutions, surface elevations, and building densities in urban and suburban scenes. Comparison results illustrated that GRRNet has competitive building extraction performance in comparison with other approaches. The model code is now publicly available at https://github.com/CHUANQIFENG/GRRNet for further studies. The Caffe time command was used to compute time requirement as averaged over 50 iterations with an image size of 480 × 480 pixels. The model size was obtained from the model file size. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>where c denotes the number of bands of the output features; and BN (•), UP (•),, and denote the batch normalization operator, the upsampling operator, the convolutional operator, and the element-wise product operator, respectively; w c 3 3 × denotes 3 × 3 convolutional kernels of and the outputs of contain c bands; M e c denotes the selected encoder features as derived from f e l and f e u . The selected encoder features M e c are then transmitted into the decoder network and fused with the coarse labeling maps f d l as follows: ReLU(•) and CONCAT a b ( , ) denotes the non-linear operator and the concatenation operator, respectively; M d c 2 + denotes the fused decoder features with (c + 2) bands. The fused decoder features M d c 2 + are convolved into the upper coarse labeling maps f d u as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>in the next gated stage in the GFL unit. In other words, the upper encoder features f e u ' of the next encoder stage are replaced by M e c. The idea is to connect the uppermost encoder features to the lowest encoder features, not only limited to interaction with the adjacent encoder features. Another difference is that the number of features</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An overview of the gated residual refinement network (GRRNet).</figDesc><graphic url="image-4.png" coords="4,56.75,57.03,481.82,239.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Schemes are illustrated for different refined modules in SegNet, Res-U-Net, TD-CEDN, G-FRNet and GRRNet.</figDesc><graphic url="image-5.png" coords="4,56.64,461.87,481.97,266.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>denote the response parameter obtained from uppermost decoder stage and the category probability of x n b ( , ) , respectively; and I y k { } n b ( , ) = is an indicator function that has the value of 0 when y k n b ( , )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Examples for the DataPlus training set images (top row) and the corresponding subset images (marked in yellow rectangles) for NIR-R-G false-color composite images (second row), nDSMs (third row), and ground references (bottom row), respectively. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)</figDesc><graphic url="image-6.png" coords="6,85.04,56.99,425.23,346.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Images are shown for the original true-color composite images and the classification results using the state-of-the-art deep learning methods across five cities. The true positive (TP), false positive (FP) and false negative (FN) are marked green, red, and blue, respectively. The yellow rectangles in the original images are enlarged for close-up inspection in Fig. 5. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)</figDesc><graphic url="image-7.png" coords="7,70.87,56.98,453.54,326.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Close-up views of images are shown for the original true-color composite images and the classification results using the state-of-the-art deep learning methods across five cities. The images are the subset from the yellow rectangles marked in Fig. 4a. The true positive (TP), false positive (FP) and false negative (FN) are marked in green, red, and blue, respectively. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)</figDesc><graphic url="image-8.png" coords="9,70.81,56.98,453.60,327.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Fig. 6. The receiver operating characteristic (ROC) curves of all deep learning methods on the DataPlus test set images.</figDesc><graphic url="image-9.png" coords="10,70.75,56.99,453.74,238.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Visual comparison with GRRNet (Baseline + GFL) and its variants on the close-ups of DataPlus test set images.</figDesc><graphic url="image-11.png" coords="11,70.70,57.00,453.89,374.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>randomly selected from the released annotated images and each test dataset contains 6 images. The training process and hyper-parameters used are the same as those of DataPlus dataset. Figs. 11 and 12 show the classification results of GRRNet on Vaihingen dataset and Potsdam dataset, respectively. The overall accuracy of 98.09% and the mIoU score of 95.38% were achieved by GRRNet on Potsdam dataset, and the overall accuracy of 96.52% and the mIoU score of 91.74% were obtained on Vaihingen dataset. In particular, the classification results on Potsdam are superior to that reported by<ref type="bibr" target="#b44">Xu et al. (2018)</ref>. Overall, the performances of GRRNet on Potsdam dataset and Vaihingen dataset are better than that of DataPlus dataset, which is probably due to the higher image resolution and more accurate image registration processing of the ISPRS datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. The receiver operating characteristic (ROC) curves of GRRNet (Baseline + GFL) and its variants on the DataPlus set images.</figDesc><graphic url="image-12.png" coords="12,70.75,57.02,453.74,236.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Accuracy assessment of different data input strategies as implemented with GRRNet.</figDesc><graphic url="image-13.png" coords="12,113.44,454.56,368.35,255.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Building extraction results of GRRNet on Vaihingen dataset. (a) NIR-R-G false-color composite images; (b) nDSMs; (c) ground truth images; (d) building extraction results (green: true positive, blue: false negative, red: false positive). (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)</figDesc><graphic url="image-14.png" coords="13,70.81,349.92,453.60,344.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-15.png" coords="14,70.81,57.01,453.60,237.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Information on all training set images and test set images for five cities. The size of each image is 5000 × 5000 pixels.</figDesc><table><row><cell>Type</cell><cell>Image name</cell><cell>Scene</cell><cell>Location</cell><cell>Year</cell><cell>Resolution</cell><cell>Mean elevation</cell><cell>Number of buildings</cell></row><row><cell>Training images</cell><cell>Arlington_02</cell><cell>Suburban</cell><cell>Massachusetts</cell><cell>2013</cell><cell>0.3 m</cell><cell>30.2 m</cell><cell>2139</cell></row><row><cell></cell><cell>Arlington_03</cell><cell>Suburban</cell><cell>Massachusetts</cell><cell>2013</cell><cell>0.3 m</cell><cell>70.4 m</cell><cell>1570</cell></row><row><cell></cell><cell>NewHaven_02</cell><cell>Suburban</cell><cell>Connecticut</cell><cell>2012</cell><cell>0.3 m</cell><cell>11.1 m</cell><cell>1640</cell></row><row><cell></cell><cell>NewYork_02</cell><cell>Urban</cell><cell>New York</cell><cell>2014</cell><cell>0.5 ft</cell><cell>11.3 m</cell><cell>1253</cell></row><row><cell></cell><cell>NewYork_03</cell><cell>Urban</cell><cell>New York</cell><cell>2014</cell><cell>0.5 ft</cell><cell>25.6 m</cell><cell>1287</cell></row><row><cell></cell><cell>Norfolk_02</cell><cell>Suburban</cell><cell>Virginia</cell><cell>2013</cell><cell>1 ft</cell><cell>2.5 m</cell><cell>2079</cell></row><row><cell></cell><cell>Norfolk_03</cell><cell>Suburban</cell><cell>Virginia</cell><cell>2013</cell><cell>1 ft</cell><cell>3.8 m</cell><cell>2158</cell></row><row><cell></cell><cell>SanFrancisco_02</cell><cell>Urban</cell><cell>California</cell><cell>2015</cell><cell>0.3 m</cell><cell>165.3 m</cell><cell>4186</cell></row><row><cell></cell><cell>SanFrancisco_03</cell><cell>Urban</cell><cell>California</cell><cell>2015</cell><cell>0.3 m</cell><cell>30.2 m</cell><cell>5305</cell></row><row><cell>Test images</cell><cell>Arlington_01</cell><cell>Suburban</cell><cell>Massachusetts</cell><cell>2013</cell><cell>0.3 m</cell><cell>9.7 m</cell><cell>2232</cell></row><row><cell></cell><cell>NewHaven_01</cell><cell>Suburban</cell><cell>Connecticut</cell><cell>2012</cell><cell>0.3 m</cell><cell>17.4 m</cell><cell>1174</cell></row><row><cell></cell><cell>NewYork_01</cell><cell>Urban</cell><cell>New York</cell><cell>2014</cell><cell>0.5 ft</cell><cell>9.6 m</cell><cell>871</cell></row><row><cell></cell><cell>Norfolk_01</cell><cell>Suburban</cell><cell>Virginia</cell><cell>2013</cell><cell>1 ft</cell><cell>2.9 m</cell><cell>2053</cell></row><row><cell></cell><cell>SanFrancisco_01</cell><cell>Urban</cell><cell>California</cell><cell>2015</cell><cell>0.3 m</cell><cell>92.4 m</cell><cell>4123</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>Comparisons of network efficiency among the tested deep learning models and the variants of GRRNet (Baseline + GFL).</figDesc><table><row><cell>Model</cell><cell>Forward pass (ms)</cell><cell>Backward pass (ms)</cell><cell>Model size (MB)</cell></row><row><cell>SegNet</cell><cell>121.69</cell><cell>279.08</cell><cell>112.33</cell></row><row><cell>CNN-FPL</cell><cell>70.72</cell><cell>154.74</cell><cell>13.87</cell></row><row><cell>DeconvNet</cell><cell>187.68</cell><cell>307.23</cell><cell>960.57</cell></row><row><cell>V-FuseNet</cell><cell>227.22</cell><cell>528.25</cell><cell>225.09</cell></row><row><cell>Res-U-Net</cell><cell>117.38</cell><cell>237.53</cell><cell>388.77</cell></row><row><cell>Baseline</cell><cell>53.66</cell><cell>132.94</cell><cell>90.78</cell></row><row><cell>Baseline + FF</cell><cell>87.73</cell><cell>197.57</cell><cell>209.07</cell></row><row><cell>Baseline + GU</cell><cell>82.69</cell><cell>192.80</cell><cell>91.07</cell></row><row><cell>Baseline + GFL-2</cell><cell>86.94</cell><cell>193.21</cell><cell>91.06</cell></row><row><cell>Baseline + GFL</cell><cell>81.44</cell><cell>207.17</cell><cell>91.57</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We acknowledge the Data+ and Bass Connections programs in Duke University for providing the experiment data set. We thank the ISPRS for making the Vaihingen and Potsdam dataset available for evaluating the proposed model. This research is supported by the National Natural Science Foundation of China (grant nos. 41431178, 41801351, 41671453 and 41875122), the Natural Science Foundation of Guangdong Province, China (grant no. 2016A030311016), the National Administration of Surveying, Mapping and Geoinformation of China (grant no. GZIT2016-A5-147), the Research Institute of Henan Spatio-Temporal Big Data Industrial Technology (grant no. 2017DJA001), the Key Projects for Young Teachers at Sun Yat-sen University (grant no. 17lgzd02). We thank anonymous reviewers for their constructive comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Simultaneous extraction of roads and buildings in remote sensing imagery with convolutional neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Alshehhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Marpu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Woon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dalla Mura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="139" to="149" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Beyond RGB: Very high resolution urban remote sensing with multimodal deep networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Audebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Le Saux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lefèvre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic detection of residential buildings using LIDAR data and multispectral imagery</title>
		<author>
			<persName><forename type="first">M</forename><surname>Awrangjeb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Fraser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="457" to="467" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SegNet: a deep convolutional encoderdecoder architecture for image segmentation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Multi-task learning for segmentation of building footprints with deep neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bischke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Helber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Folz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05932</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic road detection and centerline extraction via cascaded end-to-end convolutional neural network</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="3322" to="3337" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic building extraction from LiDAR data fusion of point and grid-based features</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">R</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="294" to="307" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Classification for high resolution remote sensing imagery using a fully convolutional network</title>
		<author>
			<persName><forename type="first">G</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">498</biblScope>
		</imprint>
	</monogr>
	<note>Remote Sens. 9</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Building extraction from high-resolution satellite images in urban areas: recent methods and strategies against significant challenges</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ghanea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Moallem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Momeni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="5234" to="5248" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">An automatic building extraction and regularisation technique using LiDAR point cloud data and orthoimage dagger</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A N</forename><surname>Gilani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Awrangjeb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Remote Sens. 8, 258</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Extraction of buildings and trees in urban environments</title>
		<author>
			<persName><forename type="first">N</forename><surname>Haala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Brenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Q</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (Cvpr)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Evaluation of automatic building detection approaches combining high resolution images and LiDAR data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hermosilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Recio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Estornell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>Remote Sens. 3, 1188-1210</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Morphological building/shadow index for building extraction from high-resolution imagery over urban areas</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="161" to="172" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A multidirectional and multiscale morphological index for automatic building extraction from multispectral GeoEye-1 imagery</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogramm. Eng. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="721" to="732" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gated feedback refinement network for dense image labeling</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D B</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="4877" to="4885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
				<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning aerial image segmentation from online maps</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="6054" to="6068" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Performance evaluation of automated approaches to building detection in multisource aerial data</title>
		<author>
			<persName><forename type="first">K</forename><surname>Khoshelham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nardinocchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frontoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zingaretti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="123" to="133" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fusion of lidar and imagery for reliable building extraction</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">U</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogramm. Eng. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="215" to="225" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">RefineNet: multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="5168" to="5177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semantic labeling in very high resolution images via a self-cascaded convolutional neural network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<date type="published" when="2017">2017a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.04456</idno>
		<title level="m">Learning to Refine Object Contours with a Top-Down Fully Convolutional Encoder-Decoder Network</title>
				<imprint>
			<date type="published" when="2017">2017b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Classification with an edge: Improving semantic image segmentation with boundary detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marmanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Stilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="page" from="158" to="172" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Detect residential buildings from lidar and aerial photographs through object-oriented land-use classification</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Currit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogramm. Eng. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="35" to="44" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to detect roads in high-resolution aerial images</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="210" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Ground and building extraction from LiDAR data based on differential morphological profiles and locally fitted surfaces</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mongus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lukac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zalik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="145" to="156" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Contextual classification of lidar data and building object detection in urban areas</title>
		<author>
			<persName><forename type="first">J</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rottensteiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Soergel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="152" to="165" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Detection and modeling of buildings from multiple aerial images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Noronha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="501" to="518" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semantic labeling of aerial and satellite imagery</title>
		<author>
			<persName><forename type="first">S</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sherrah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Janney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2868" to="2881" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Results of the ISPRS benchmark on urban object detection and 3D building reconstruction</title>
		<author>
			<persName><forename type="first">F</forename><surname>Rottensteiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gerke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Breitkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="256" to="271" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Using the Dempster-Shafer method for the fusion of LIDAR data and multi-spectral images for building detection</title>
		<author>
			<persName><forename type="first">F</forename><surname>Rottensteiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Trinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Clode</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kubik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. Fusion</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="283" to="300" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Building and road detection from large aerial imagery</title>
		<author>
			<persName><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Aoki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing: Machine Vision Applications VIII. International Society for Optics and Photonics</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">94050K</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Segmentation and reconstruction of polyhedral building roofs from aerial lidar point clouds</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sampath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1554" to="1567" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Developing a multi-filter convolutional neural network for semantic segmentation using high-resolution aerial imagery and LiDAR data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dense semantic labeling of subdecimeter resolution images with convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="881" to="893" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Gated convolutional neural network for semantic segmentation in high-resolution images. Remote Sens. 9, 446</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2016">2017. 2016</date>
		</imprint>
	</monogr>
	<note>Automatic extraction of building boundaries using aerial LiDAR data</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Automatic building segmentation of aerial imagery using multi-constraint fully convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shibasaki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">407</biblScope>
		</imprint>
	</monogr>
	<note>Remote Sens. 10</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="page" from="3" to="18" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Building extraction in very high resolution remote sensing imagery using deep learning and guided filters</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Remote Sens. 10, 144</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Urban land cover classification using airborne LiDAR data: a review</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>El-Ashmawy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens. Environ</title>
		<imprint>
			<biblScope unit="volume">158</biblScope>
			<biblScope unit="page" from="295" to="310" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Automatic registration of UAV-borne sequent images and LiDAR data</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="262" to="274" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning building extraction in aerial scenes with convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A novel building and tree detection method from LiDAR data and aerial images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zarea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohammadzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1864" to="1875" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multi-source remote sensing data fusion: status and trends</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Image Data Fusion</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5" to="24" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep learning for remote sensing data: A technical tutorial on the state of the art</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Mag</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="22" to="40" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Contextually guided very-high-resolution imagery classification with semantic segments</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Emery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="page" from="48" to="60" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
