<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mixture-of-Experts with Expert Choice Routing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
							<email>yanqiz@google.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Google, Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tao</forename><surname>Lei</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Google, Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
							<email>hanxiaol@google.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Google, Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
							<email>dunan@google.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Google, Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
							<email>huangyp@google.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Google, Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vincent</forename><surname>Zhao</surname></persName>
							<email>vzhao@google.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Google, Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><surname>Dai</surname></persName>
							<email>adai@google.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Google, Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
							<email>zhifengc@google.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Google, Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Google, Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">James</forename><surname>Laudon</surname></persName>
							<email>jlaudon@google.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Google, Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Mixture-of-Experts with Expert Choice Routing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sparsely-activated Mixture-of-experts (MoE) models allow the number of parameters to greatly increase while keeping the amount of computation for a given token or a given sample unchanged. However, a poor expert routing strategy (e.g. one resulting in load imbalance) can cause certain experts to be under-trained, leading to an expert being under or over-specialized. Prior work allocates a fixed number of experts to each token using a top-k function regardless of the relative importance of different tokens. To address this, we propose a heterogeneous mixture-of-experts employing an expert choice method. Instead of letting tokens select the top-k experts, we have experts selecting the top-k tokens. As a result, each token can be routed to a variable number of experts and each expert can have a fixed bucket size. We systematically study pre-training speedups using the same computational resources of the Switch Transformer top-1 and GShard top-2 gating of prior work and find that our method improves training convergence time by more than 2?. For the same computational cost, our method demonstrates higher performance in fine-tuning 11 selected tasks in the GLUE and SuperGLUE benchmarks. For a smaller activation cost, our method outperforms the T5 dense model in 7 out of the 11 tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Scaling up model capacity, dataset size, and training time has demonstrated huge success in enhancing the performance of computer vision architectures <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b3">4]</ref> as well as neural language models <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27]</ref>. The final model quality has been found to have a power-law relationship with the amount of data, model size, and compute time <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b19">20]</ref>. However, training efficiency, which is defined as the total amount of computation used to achieve superior model quality than the state of the art system <ref type="bibr" target="#b20">[21]</ref>, should receive greater attention as we increase our efforts towards green AI <ref type="bibr" target="#b28">[29]</ref>.</p><p>Sparsely gated mixture-of-experts <ref type="bibr" target="#b30">[31]</ref> provides an effective way to scale model capacity given a fixed computational cost, and has recently played an important role in increasing the training efficiency of large-scale language models <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b9">10]</ref>. A mixture-of-experts model operates by adopting a number of experts, each as a sub-network, and by activating only one or a few experts for each input token. A gating network must be chosen and optimized in order to route each token to the most suited expert(s). For example, recent work has implemented sparse routing via k-means clustering <ref type="bibr" target="#b11">[12]</ref>, linear assignment to maximize token-expert affinities <ref type="bibr" target="#b21">[22]</ref>, or hashing <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b7">8]</ref>. Many of the prior work use a routing strategy concerning the token choice, where each token selects the best one or two experts.</p><p>We argue that the independent token choice of prior work often leads to an imbalanced load of experts, which causes training inefficiency and sub-optimal training of the model. In order to mitigate this issue, previous sparsely gated networks introduce additional auxiliary losses as regularization to  prevent too many tokens being routed to a single expert, but the effectiveness is still limited. Recent approaches <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b7">8]</ref> explore alternative strategies for routing, but they focus on pre-training only and do not demonstrate performance gain on downstream tasks. Moreover, none of the previous methods consider allocating a variable number of experts to each token based on importance, which can be beneficial.</p><p>We propose a very simple yet effective routing method we are calling expert choice. Unlike conventional MoE where tokens select one or two top-scoring experts, our method lets each expert pick the top-k tokens. Our method guarantees perfect load balancing, allows a variable number of experts for each token, and achieves substantial gains in training efficiency and downstream performance as demonstrated in our experiments. Our major contributions include:</p><p>? We identify common pitfalls in conventional MoE such as load imbalance as described in Section 3.1. We then propose a heterogeneous, expert choice method to provide a fluid allocation of model parameters based on a learnt token-to-expert importance. This method intrinsically guarantees load balance without imposing an auxiliary loss.</p><p>? We show our method provides over 2? faster training convergence in a 8B/64E (8 billion activated parameters, 64 experts) model, compared to the top-1 and top-2 gating counterparts in Switch Transformer <ref type="bibr" target="#b9">[10]</ref> and GShard <ref type="bibr" target="#b20">[21]</ref>.</p><p>? We show our method demonstrates strong scaling when increasing the number of experts from 16 to 128, evaluated in training perplexity.</p><p>? We show our method demonstrates strong performance on downstream tasks selected from GLUE and SuperGLUE at all the evaluated scales. More specifically, our 8B/64E model outperforms a T5 11B dense model in 7 out of 11 tasks evaluated. Table <ref type="table" target="#tab_0">1</ref> highlights the average score comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Scaling: Various approaches have been proposed to scale up neural network capacity to improve performance. Recent works have successfully scaled models to billions of parameters via various forms of model parallelism <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b1">2]</ref>. Model parallelism <ref type="bibr" target="#b29">[30]</ref> splits weights and tensors across multiple cores while pipeline parallelism <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24]</ref> splits different layers across devices with micro-batches pipelined to the different layers. To enable continued scaling of neural networks, improving model training and serving efficiency has become a critical research area.</p><p>Conditional Computation: Computation decisions can be made dynamically based on the input <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b22">23]</ref>. Conditional computation has been proposed as a way to increase the capacity of a deep neural network without increasing the amount of computation, by activating certain parameters and computation on demand, on a per-example or per-token basis <ref type="bibr" target="#b2">[3]</ref>. Conditional convolution layers <ref type="bibr" target="#b0">[1]</ref> with task-specific gating has been used to combat catastrophic forgetting when a sequence of learning problems are optimized. The gating decisions may be binary or sparse and continuous, stochastic or deterministic.</p><p>Mixture of Experts: Sparsely-gated MoE <ref type="bibr" target="#b30">[31]</ref> is the first model to demonstrate massive improvements in model capacity, training time, or model quality with gating. Switch Transformer <ref type="bibr" target="#b9">[10]</ref> simplifies the gating by selecting only the top expert per token using a softmax over the hidden state and demonstrates better scaling than previous work. All the prior work requires an auxiliary loss to explicitly encourage balancing. This loss term has to be carefully weighted to not overwhelm the primary loss. However, auxiliary loss does not guarantee balancing and a hard capacity factor has to be imposed. As a result, many tokens can still be unprocessed by the MoE layer. Hard MoE <ref type="bibr" target="#b11">[12]</ref> with a single decoding layer can be efficiently trained to good effect on large scale hashtag prediction tasks.</p><p>Base Layers <ref type="bibr" target="#b21">[22]</ref> formulate a linear assignment that maximizes token-expert affinities while ensuring each expert receives an equal number of tokens. Hash layers <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b7">8]</ref> devise hashing techniques on input tokens. However, the evaluations are limited to pre-training perplexity. Different from these prior works, our method is a learnt method that enables heterogeneous MoE and effectively improves downstream fine-tuning performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We first identify a few pitfalls in the routing method of conventional mixture-of-experts (MoE) models and then present our method using expert choice to tackle these problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pitfalls of token-choice routing</head><p>Mixture-of-experts models enjoy increased modeling capacity while keeping the amount of computation fixed for a given token or a given sample. Although this can be computationally advantageous compared to a dense model, a routing strategy must be used to assign each token to the most-suited experts. Conventional MoE models employ token-choice routing which independently selects the top-k experts for each token <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b9">10]</ref>. We argue that this strategy has a few pitfalls that lead to sub-optimal training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Load Imbalance</head><p>Token-choice routing often lead to poor load balancing across experts. That is, some experts may be trained with most tokens, leaving the remaining experts under-utilized. This load balancing issue can result in sub-optimal training of the model since a lot of model capacity in the under-utilized experts are wasted. It also leads to training inefficiency because some tokens will not be processed, since over-utilized experts can only take a maximum number of tokens at each step in order to avoid running out of memory. Load imbalance can also hurt step latency, thus inference time, as the step latency can be determined by the most loaded expert.</p><p>Previous methods add an auxiliary loss on load balancing to mitigate the issue. However, this auxiliary loss does not guarantee a balanced load, especially during the important early stages of training. Indeed, we empirically observe that the over-capacity ratio can reach 20%-40% for some experts in token choice routing, indicating that a significant portion of the tokens routed to these experts will be dropped. Moreover, the auxiliary loss term has to be carefully weighted with the primary loss, which increases the complexity of model tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Under Specialization</head><p>Each MoE layer uses a gating network to learn token-to-expert affinity. Ideally, the learnt gating network should produce the affinity such that similar or relevant tokens are routed to the same expert. A sub-optimal strategy can produce redundant experts and/or experts that are not sufficiently specialized. Under specialization may result by imposing an large auxiliary loss which favors more load balanced but less effective routing. Finding the right balance on the auxiliary loss to promote both load balancing and specialization is one of the challenges for token-choice routing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Same Compute for every token</head><p>Finally, in a token-choice strategy each token receives exactly k experts and therefore occupies the same amount of compute. We hypothesize that this is not necessary nor desired. Instead, a MoE model should flexibly allocate its compute resource based on the complexity of the input.</p><p>Motivated by the aforementioned observations and our intuition, we next describe a simple yet effective method which produces load balanced assignments based on expert choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Heterogeneous MoE via Expert Choice</head><p>Different from conventional routing, an expert choice method independently selects top-k tokens for each expert, where k is a fixed expert capacity (i.e. the number of tokens each expert can take). Despite its simplicity, expert choice achieves perfect load balancing by design. It also enables more flexible allocation of model compute since tokens can be received by a variable number of experts.</p><p>In our experiments, we set k as</p><formula xml:id="formula_0">k = n ? c e (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where n is the total number of tokens in the input batch (such as batch size ? sequence length), c is the capacity factor, and e is the number of experts. The capacity factor c denotes on average how many experts are utilized by a token. Given input token representations X ? R n?d where d is the model hidden dimension, our method produces a token-to-expert assignment denoted by three output matrices I, G and P . The matrix I is an index matrix where I[i, j] specifies j-th selected token of the i-th expert. The gating matrix G ? R e?k denotes the weight of expert for the selected token, and P ? R e?k?n refers to an one-hot version of I that will be used to gather tokens for each expert. These matrices are computed using a gating function,</p><formula xml:id="formula_2">S = Softmax(X ? W g ), S ? R n?e G, I = TopK(S , k) P = Onehot(I)<label>(2)</label></formula><p>where S denotes the token-to-expert affinity scores, W g ? R d?e denotes the expert embeddings, and T opK() selects the k largest entries for each row of S .</p><p>Similar to the gating in Switch Transformer <ref type="bibr" target="#b9">[10]</ref> and GShard <ref type="bibr" target="#b20">[21]</ref>, we apply mixture of experts and the gating function in the dense feed-forward (FFN) layer, as it is the most computationally expensive part in a Transformer-based network. The input to the gated FFN, denoted by X in ? R e?k?d , is produced using the permutation matrix P . Here X in [i] ? R k?d denotes the input of the i-th expert.</p><p>Similarly, let W 1 and W 2 denote the parameters of gated FFN in which W 1 [i] and W 2 [i] ? R d?d denote the parameter matrices of the i-th expert. We compute the output of each expert X e [i] as follows,  We omit the bias terms here for brevity. The finally output of the gated FFN layer X out ? R n?d can be obtained given X e , the permutation and gating matrices P and G,</p><formula xml:id="formula_3">X in = P ? X ?i : X e [i] = GeLU(X in [i] ? W 1 [i]) ? W 2 [i]<label>(3)</label></formula><formula xml:id="formula_4">X out [l, d] = i,j P [i, j, l] G[i, j] X e [i, j, d]<label>(4)</label></formula><p>Both X e and X out can be efficiently computed using Einstein summation (einsum) operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Expert Choice with Additional Constraint</head><p>We also consider regularizing our expert choice routing by limiting the maximum number of experts for each token. We are interested in whether adding this constraint improves pre-training and finetuning results. More importantly, it helps analyzing to what degree using a variable number of experts per token affects the model performance.</p><p>Let A ? R e?n be a positive matrix where A[i, j] represents whether the i-th expert selects j-th token.</p><p>We solve the following entropy-regularized linear programming problem</p><formula xml:id="formula_5">max A S , A + ?H(A) s.t. ?i : j A[i, j ] = k ?j : i A[i , j] ? b ?i, j : 0 ? A[i, j] ? 1</formula><p>where &lt; S , A &gt; denotes the inner product, H(A) is the sum of element-wise entropy <ref type="foot" target="#foot_0">1</ref> , and b &gt; 0 is an integer that upper bounds the selection for each token. Adding a small entropy term gives a near-integer solution while enabling a fast iterative solver we can run on TPUs. Specifically, the solution space is the intersection of three convex sets each satisfying one of the linear constraints. We use Dykstra's algorithm <ref type="bibr" target="#b8">[9]</ref> that alternatively projects the intermediate solution onto one of the convex sets. <ref type="foot" target="#foot_1">2</ref> After A is computed, the routing indices I is selected using T opK(A, k) instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model Architecture</head><p>At the high level, we adopt the idea of sparsely activated Mixture-of-Experts (MoE) <ref type="bibr" target="#b30">[31]</ref>. We use a Transformer architecture and replace the feed-forward component of every other Transformer For example, we replace the standard positional embedding with per-layer relative positional bias <ref type="bibr" target="#b4">[5]</ref>.</p><p>In the non-MoE feed-forward sub-layers (only every other layers are MoE layers), we replace the first linear projection and the activation function with the Gated Linear Unit <ref type="bibr" target="#b5">[6]</ref>, which computes the component-wise product of two linear transformation of the input, followed by a Gaussian Error Linear Unit <ref type="bibr" target="#b14">[15]</ref> activation function.</p><p>As described earlier, each MoE layer consists of a group of independent feed-forward networks as denoted as "experts". The gating function in Equation 2 uses a softmax activation function to model a probability distribution over these experts. This distribution denotes the preference over experts of each incoming token, which is computed similarly in a conventional gating network <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>During training, each MoE layer's learnable gating network described in Equation 2 is trained to use the input to activate the best subset of experts using a top-k function along the token dimension. An "shuffle" stage and an "unshuffle" stage are inserted to the MoE layer, where the first stage gathers the tokens to their designated experts while the second stage permutes the tokens back to their original order in the input batch. This step is formulated in Equation 3 and 4.</p><p>Similar to conventional MoE method, there are more parameters in the MoE layer. However, the activated model size per token can be comparable to a dense layer because during training or inference, only a limited subset of experts is activated for any given token. For instance, Switch Transformer <ref type="bibr" target="#b9">[10]</ref> has only one activated expert while GShard <ref type="bibr" target="#b20">[21]</ref> uses two experts per token. In our method, the number of activated experts can vary for each token but the overall computation is kept the same as the baseline architectures by fixing the capacity factor c in Equation <ref type="formula" target="#formula_0">1</ref>. Unless otherwise specified, we set c = 2 such that our method can be directly compared to the top-2 token-choice gating in GShard.</p><p>We train several variants of our architecture at the 100M scale (i.e. 100M expert size) by increasing the number of experts to understand the scaling effects of our method. We also train a 8B scale mixture-of-experts model. The large MoE model is partitioned with a 2D sharding algorithm as presented in GSPMD <ref type="bibr" target="#b35">[36]</ref>, which fully exploits the 2D topology of the TPU cluster <ref type="bibr" target="#b18">[19]</ref>. Across different scales and setups, our method outperforms related work and demonstrates strong downstream task performance on selected tasks in GLUE and SuperGLUE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>4.1 Setup  architecture when each MoE layer only has one expert. While n params is the total number of trainable parameters, n act-params represents the number of activated parameters per token. L is the total number of Transformer layers, M is the model dimension, H is the hidden dimension after the projection in each transformer layer, n heads is the number of attention heads, and d head is the hidden dimension of each attention head.</p><p>Dataset: We use the high-quality dataset from GLaM <ref type="bibr" target="#b6">[7]</ref> of 1.6 trillion tokens that are representative of a wide range of natural language use cases. An in-house classifier is trained to classify between a collection of curated text and other webpages and estimate the content quality of a webpage. A high-quality filtered subset of webpages are combined with books, Wikipedia pages, conversations, forums, and news to create the final dataset. The data and mixture weights can be found in Table <ref type="table" target="#tab_2">3</ref> in the GLaM paper.</p><p>Model Training: Our model training follows the setups of GLaM <ref type="bibr" target="#b6">[7]</ref> where a maximum sequence length of 1024 tokens is adopted. We use an Adafactor optimizer <ref type="bibr" target="#b31">[32]</ref> with first-moment decay ? 1 = 0 and second-moment decay ? 2 = 0.99. We keep the learning rate constant for the first 10K training steps, and then decay it with an inverse square root schedule. Unlike most related works, we do not impose any auxiliary loss for load balance, such as described in Switch Transformer <ref type="bibr" target="#b9">[10]</ref> and GShard <ref type="bibr" target="#b20">[21]</ref>. We use the SentencePiece subword tokenizer with a vocabulary of size of 256K. The largest model (8B/64E) is trained on 512 TPU V4 chips. We use a dropout rate of 0 during training as the number of tokens in the training data corpus is much greater than the total number of tokens during training.</p><p>Model Evaluation: To demonstrate the effectiveness of our method, we mainly focus on evaluating the finetuning performance on the 11 selected tasks from GLUE and SuperGLUE benchmarks <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b33">34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>In the evaluation, we 1) compare training steps as a proxy of training efficiency of a 8B/64E model with GShard Top-2 gating which has been demonstrated to be stronger than Switch Transformer Top-1 gating (demonstrated in Table <ref type="table" target="#tab_5">4</ref>.2.1); 2) compare the scaling of our method by increasing the number of experts from 16 to 128 while keeping the expert size fixed at 100M parameters; 3) evaluate the finetuning performance on eleven downstream GLUE and SuperGlue tasks, comparing to Switch Transformer top-1 gating and GShard top-2 gating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Training Efficiency</head><p>We first study training efficiency and convergence. We use expert choice with a capacity factor of 2 (EC-CF2) to match the activated model size and computational cost on a per token basis in GShard top-2 gating and run both for a fixed number of steps. The results are shown in Figure <ref type="figure" target="#fig_3">3</ref>. Comparing to GShard top-2 gating, which showed stronger performance in both perplexity in the evaluation dataset and finetuning on downstream tasks compared to Switch Transformer top-  as GShard top-2 in less than half the steps, and with each GShard top-2 step being 20% slower than our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Scaling the Number of Experts</head><p>As presented in Table <ref type="table" target="#tab_2">3</ref>.3, increasing the number of experts effectively increases model capacity without increasing activated model size. We scale the number of experts while fixing the expert size to 100M parameters for both expert choice (EC) and GShard (Top-2) methods and find both methods scale well in terms of perplexity on the evaluation dataset during pre-training, as demonstrated in Figure <ref type="figure" target="#fig_4">4</ref>. Having more experts consistently improves training perplexity, which often translates directly to better performance in downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Fine-tuning on GLUE and SuperGLUE</head><p>We perform fine-tuning on 11 selected tasks from GLUE and SuperGLUE on the pretrained models of three MoE methods, including Switch Transformer top-1 gating (ST Top-1), GShard top-2 gating (GS Top-2) and our method with a capacity factor of 2 (EC-CF2) to match the activation memory size and thus computational cost of GS Top-2. Indicated by the results in   QQP (0.1%), RTE (1%), SST2 (0.3%), WiC (4.3%), WNLI (0.7%). Impressively, our 8B/64E model only has 9.8B activated parameters, and is more computationally efficient than the T5 11B model. Interestingly, we observe the 100M/32E model setting works the best for both GS Top-2 and EC-CF2, even though the effective model capacity is smaller than that of 100M/64E and 100M/128E. This result indicates that a good training perplexity does not always translate to better performance of downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Variable Experts per Token</head><p>We compute statistics on token-to-expert routing, particularly on the ratio of tokens that have been routed to a certain number of experts. According to Figure <ref type="figure" target="#fig_5">5</ref>, a majority of tokens have been routed to one or two experts while 23% have been routed to three or four experts and only about 3% tokens have been routed to more than 4 experts. This plot verifies our hypothesis that our method learns to allocate a variable number experts to tokens, which can be beneficial for important tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Capped Expert Choice</head><p>We also perform an ablation study by limiting the maximum number of experts for each token, using the method described in Section 3.3. Table <ref type="table" target="#tab_5">4</ref> reports the average accuracy on the 11 selected datasets. EC-CAP2 is the variant of our expert choice method by limiting the number of experts of each token to 2. This decreases the fine-tuning accuracy by 0.8 points on average. In addition, EC-CAP3 allows a maximum of 3 experts per token and achieves on par results compared to the vanilla expert choice method. This ablation study confirms that allowing variable number of experts per token is indeed helpful. Additional results can be found in Supplementary Section C. Figure <ref type="figure">6</ref>: Varying the capacity factor in our expert choice method: Decreasing the capacity factor from two to one degrades the perplexity but still outperforms the top-1 gating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Capacity Factor</head><p>We study the capacity factor in our expert choice method and compare the training perplexity with the baseline top-1 gating method used in Switch Transformer. As described in Equation <ref type="formula" target="#formula_0">1</ref>, the capacity factor determines how many experts on average each token can be routed to, thus the bucket size k of each expert. In all our previous experiments, we use a capacity factor of 2, which matches the computational footprint of the top-2 gating used in GShard method where each token is routed to two experts. To match the computation cost on a per-token basis fairly with top-1 gating used in Switch Transformer, we reduce the capacity factor to 1 and plot the training perplexity. Figure <ref type="figure">8</ref> demonstrates that our method with a capacity factor of 1 (EC_CF 1) still significantly outperforms top-1 gating. We further pushes the capacity factor further down to 0.5, and find it only marginally hurts training perplexity, outperforming top-1 gating. Not surprisingly, using a larger capacity factor in expert choice yields better performance. More details on downstream tasks are presented in Supplementary Section B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Comparison with Dense Models</head><p>We compare our method with dense models where no mixture-of-experts are used. As shown in Figure <ref type="figure" target="#fig_6">7</ref>, our method consistently outperforms the dense method in training perplexity and training convergence time. For a small expert size of 100M parameters, the benefit of sparse gating is even more significant. Orthogonal to results presented in Figure <ref type="figure" target="#fig_4">4</ref>, where scaling the number of experts improves model performance, Figure <ref type="figure" target="#fig_6">7</ref> shows that increasing expert capacity also significantly increases model performance. We conduct fine-tuning on the selected 11 tasks described in Table <ref type="table" target="#tab_5">4</ref>.2.1 on a dense 8B model and report the average score of 89.2, which is 3.4% lower than 8B/64E EC-CF2 in our method. More detailed comparison will be presented in Supplementary Section A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose a new routing method for sparsely activated mixture-of-experts (MoE) models. This method addresses load imbalance and under-utilization of experts in conventional MoE methods, and enables selecting different numbers of experts for each token.  Transformer models, and also achieves strong gains when finetuning on 11 datasets in the GLUE and SuperGLUE benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Limitations</head><p>The expert choice method might not immediately apply to auto-regressive text generation as our current implementation takes in the past and future tokens to perform the top-k selection. One possible solution is to collect a large batch of input sequences, dispatch tokens of the same sequence into separate groups, and perform expert choice routing for each group. Another scenario where the expert choice method does not immediately apply is when the batch size becomes very small during serving or inference. A global top-k can be selected instead and we can cap the number of times each expert or token gets selected. We leave these possible improvements for future work.</p><p>Another long-standing issue with MoE has been the large memory footprint. Even though computational cost can be reduced using sparsely gated networks, the total number of parameters increases linearly or sub-linearly with the number of experts. Increasing the number of experts requires reservation of a large number of hardware devices. Therefore, dynamic (used) power is saved while static (reserved) power is not. Power saving techniques such as the ability to put hardware devices into low power states while not in use <ref type="bibr" target="#b16">[17]</ref> can help with reducing the reserved power requirements.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Capacity Factor</head><p>We evaluate the downstream task fine-tuning performance by varying the capacity factors. Note that a capacity factor of n indicates on average how many experts each token can be received. EC-CF2 is our baseline expert choice, which matches GShard top-2 gating computational footprint. EC-CF1, however, matches Switch Transformer top-1 gating computational footprint. EC-CF0.5 further verifies that an aggressively lowered capacity factor can provide strong enough performance, that almost matches the top-2 gating baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Capped Expert Choice</head><p>As described in Section 3.3, the maximum number of experts each token is assigned can be capped by an entropy-regularized linear programming. Figure <ref type="figure">8</ref> compares the validation perplexity when training the 100M/64E models using the base expert choice method (EC-BASE), expert choice capped by two experts per token (EC-CAP2), expert choice capped by three experts per token (EC-CAP3), and GShard top-2 gating.</p><p>As shown in the figure, restricting the number of experts to 2 degrades the perplexity compared to the base expert choice method. This suggests that a more flexible allocation of experts (e.g. more than 2 experts for a token) can enhance model expressiveness. On the other hand, our EC-CAP2 and EC-CAP3 methods still outperform the top-2 gating method by a clear margin. We believe this confirms the effectiveness of a load balanced training, provided by our method. Finally, EC-CAP3 obtains comparable perplexity to EC-BASE. As indicated by Figure <ref type="figure" target="#fig_5">5</ref>, only a little fraction of tokens use more than 3 experts therefore we see little or no difference between EC-BASE and EC-CAP3 variants. We present the fine-tuning results of these methods in Table <ref type="table" target="#tab_10">6</ref>. </p><formula xml:id="formula_6">0</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Comparison with Hash Layer</head><p>In this section, we compare our method with Hash Layers <ref type="bibr" target="#b27">[28]</ref>. We use mod x to map a token ID to an expert ID. This in some way ensures load balance and generates specialized experts. The fine-tuning results are presented in the last row in Table <ref type="table" target="#tab_10">6</ref>. Hashing based routing performs much worse than expert choice in terms of average scores and variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Fine-tuning Details</head><p>We did a hyperparameter search for both baseline models and expert choice method. For fine-tuning of the 8B dense model, we use a constant learning rate of 0.0001 and a dropout rate of 0.1. We freeze the attention layer and feed-forward layer while leaving the embedding and layer normalization trainable. This setting has been found optimal for the 8B dense model. For MoE 8B/64E models including GShard top-2 gating and expert choice, we found continuing the learning rate from the pre-trained model while using a square root learning rate decay works better. In addition, we do not apply parameter freezing for fine-tuning MoE models. For models with 100M expert size, we use a constant learning rate of 0.0001 and no dropout is used.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: High-level Comparison Between Conventional MoE and expert choice MoE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: MoE Layer with expert choice.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>&gt;2xFigure 3 :</head><label>3</label><figDesc>Figure 3: Training convergence is more than 2x faster using our method compared to GShard top-2 gating.</figDesc><graphic url="image-1.png" coords="7,182.59,58.22,249.12,159.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Training perplexity scales strongly with the number of experts while keeping the expert size fixed. EC consistently outperforms GShard top-2 gating.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Distribution of the number of experts routed to per token in a 100M/64E model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Training perplexity comparison with dense models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>:2202.09368v1 [cs.LG] 18 Feb 2022 Highlighted comparison between Switch Transformer top-1 (ST Top-1), GShard top-2 (GS Top-2), and Expert Choice (EC) on 11 selected tasks in GLUE and SuperGLUE. n act-params is the activated parameter size, thus approximates the per-token computational cost. n params is the effective total capacity.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Model</cell><cell></cell><cell></cell><cell cols="6">n params n act-params Avg Score</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">ST Top-1 0.1B/64E</cell><cell>1.9B</cell><cell cols="2">130M</cell><cell></cell><cell>78.4</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">GS Top-2 0.1B/64E</cell><cell>1.9B</cell><cell cols="2">145M</cell><cell></cell><cell>83.0</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">EC 0.1B/64E</cell><cell></cell><cell>1.9B</cell><cell cols="2">145M</cell><cell></cell><cell>84.0</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">ST Top-8B/64E</cell><cell>143B</cell><cell cols="2">8.7B</cell><cell></cell><cell>88.9</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">GS Top-2 8B/64E</cell><cell>143B</cell><cell cols="2">9.8B</cell><cell></cell><cell>90.3</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">EC 8B/64E</cell><cell></cell><cell>143B</cell><cell cols="2">9.8B</cell><cell></cell><cell>92.6</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FFN 1</cell><cell></cell><cell></cell><cell></cell><cell>FFN 2</cell></row><row><cell>FFN 1</cell><cell>FFN 2</cell><cell>FFN 3</cell><cell>FFN 4</cell><cell>FFN 1</cell><cell>FFN 2</cell><cell>FFN 3</cell><cell>FFN 4</cell><cell>We</cell><cell>Like To</cell><cell>Play</cell><cell></cell><cell>We</cell><cell>Like Soccer Field</cell></row><row><cell>p = 0.65</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>p = 0.8</cell><cell></cell><cell></cell><cell>Top-k</cell><cell></cell><cell></cell><cell>Top-k</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Router</cell><cell></cell><cell></cell><cell></cell><cell>Router</cell></row><row><cell></cell><cell cols="2">Router</cell><cell></cell><cell></cell><cell cols="2">Router</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Token 1</cell><cell></cell><cell></cell><cell cols="2">Token 2</cell><cell></cell><cell>Token 1</cell><cell>Token 2</cell><cell>Token 3</cell><cell cols="2">Token 4 Token 5</cell><cell>Token 6 Token 7</cell><cell>Token 8</cell></row><row><cell></cell><cell>We</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Like</cell><cell></cell><cell>We</cell><cell>Like</cell><cell>To</cell><cell>Play</cell><cell>Soccer</cell><cell>In</cell><cell>The</cell><cell>Field</cell></row></table><note><p>Preprint. Under review. arXiv</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Sizes and architectures of both MoE and dense models that were trained in our experiments. Models are grouped by the number of activated parameters per token. All trained models share the same learning hyperparameters described in Session 4.1.layer with a MoE layer, following recent practice<ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b9">10]</ref>. Interleaving regular Transformer layers and MoE layers empirically improves model performance and training efficiency, probably because forcing some shared components in between MoE layers can mitigate the negative effects of skipping tokens. Several additional modifications adopted in recent work have been applied in our experiments.</figDesc><table><row><cell>Model</cell><cell>Type</cell><cell cols="2">nparams nact-params</cell><cell>L</cell><cell>M</cell><cell>H</cell><cell cols="3">nheads dhead E</cell></row><row><cell>0.1B</cell><cell cols="2">Dense 130M</cell><cell>130M</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell></row><row><cell>0.1B/16E</cell><cell>MoE</cell><cell>548M</cell><cell>145M</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>16</cell></row><row><cell>0.1B/32E 0.1B/64E</cell><cell>MoE MoE</cell><cell>1.0B 1.9B</cell><cell>145M 145M</cell><cell>12</cell><cell>768</cell><cell>3,072</cell><cell>12</cell><cell>64</cell><cell>32 64</cell></row><row><cell>0.1B/128E</cell><cell>MoE</cell><cell>3.7B</cell><cell>145M</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>64</cell></row><row><cell>8B 8B/64E</cell><cell>Dense MoE</cell><cell>8.7B 143B</cell><cell>8.7B 9.8B</cell><cell cols="3">32 4,096 16,384</cell><cell>32</cell><cell>128</cell><cell>-64</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>3 summarizes the hyperparameter settings of different MoE models. As a reference point,</figDesc><table /><note><p>we also include the respective dense model configurations with comparable numbers of activated parameters per-token during inference. To study of the effect of scaling the number of experts, we studied varying the number of experts but fixing the per expert size to 100M parameters. For example, 0.1B/64E represents the architecture of an approximately 100M parameter dense model with every other layer replaced by a 64-expert MoE layer. The MoE model degenerates into a dense transformer</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note><p>Expert choice with capacity factor of 2 (EC-CF2) outperforms Top-1 gating in Switch Transformer (ST) and top-2 gating in GShard (GS) on GLUE and SuperGLUE tasks. Note that with an expert size of 100M parameters, 100M/32E works best for our method and Ghard Top-2 while 100M/128E works better for Switch Transformer Top-1. Our method consistently outperforms the others across all the scales.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell>3.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>3.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>EC 128E</cell></row><row><cell>Eval Perplexity</cell><cell>2.8 2.9 3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Top-2 128E EC 64E Top-2 64E EC 32E</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Top-2 32E</cell></row><row><cell></cell><cell>2.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>EC 16E</cell></row><row><cell></cell><cell>2.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Top-2 16E</cell></row><row><cell></cell><cell>Step (K)</cell><cell>40</cell><cell>80</cell><cell>120</cell><cell>160</cell><cell>200</cell><cell>240</cell><cell>280</cell><cell>320</cell><cell>360</cell><cell>400</cell><cell>440</cell><cell>480</cell><cell>520</cell><cell>560</cell><cell>600</cell></row></table><note><p><p><p>.2.1, our EC-CF2 method consistently outperforms the related methods and yields more than 2% average accuracy increase in a large 8B/64E setting. Compared to the state-of-the-art T5 dense model</p><ref type="bibr" target="#b26">[27]</ref></p>, our 8B/64E model outperforms a T5 11B dense model in 7 out of the 11 tasks, including CB (1.8%), COLA (15.3%),</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Limiting the number of experts per token in expert choice method affects downstream accuracy.</figDesc><table><row><cell></cell><cell cols="3">Method</cell><cell></cell><cell cols="7">Max # of Experts</cell><cell></cell><cell cols="4">Avg acc.</cell><cell></cell></row><row><cell></cell><cell cols="4">EC-CAP2</cell><cell></cell><cell></cell><cell></cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">83.2 ? 0.4</cell><cell></cell></row><row><cell></cell><cell cols="4">EC-CAP3</cell><cell></cell><cell></cell><cell></cell><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">84.0 ? 0.4</cell><cell></cell></row><row><cell></cell><cell cols="3">EC-CF2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">84.0 ? 0.2</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="7">EC_CF2_100M64E</cell><cell></cell><cell cols="6">EC_CF1_100M64E</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="7">EC_CF0.5_100M64E</cell><cell></cell><cell cols="5">TOP1_100M64E</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Eval Perplexity</cell><cell>2.8 3 3.2 3.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Steps (K)</cell><cell>40</cell><cell>80</cell><cell>120</cell><cell>160</cell><cell>200</cell><cell>240</cell><cell>280</cell><cell>320</cell><cell>360</cell><cell>400</cell><cell>440</cell><cell>480</cell><cell>520</cell><cell>560</cell><cell>600</cell><cell>640</cell><cell>680</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Our model demonstrates more than 2x training efficiency improvements when compared to the state-of-the-art GShard and Switch</figDesc><table><row><cell></cell><cell cols="3">EC 100M64E</cell><cell cols="5">Dense 100M</cell><cell cols="3">EC 8B64E</cell><cell></cell><cell cols="3">Dense 8B</cell></row><row><cell></cell><cell>3.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Eval Perplexity</cell><cell>2.4 2.6 2.8 3 3.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Steps (K)</cell><cell>50</cell><cell>100</cell><cell>150</cell><cell>200</cell><cell>250</cell><cell>300</cell><cell>350</cell><cell>400</cell><cell>450</cell><cell>500</cell><cell>550</cell><cell>600</cell><cell>650</cell><cell>700</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Comparison between Dense 8B and Expert Choice (EC-CF2) 8B/64E models: Our method significantly outperforms the dense model in downstream tasks.</figDesc><table><row><cell>Model</cell><cell cols="7">BoolQ CB CoLA MNLI MRPC QNLI QQP RTE SST2 WiC WNLI</cell><cell>Avg</cell></row><row><cell>Top-2</cell><cell>78.1</cell><cell>87.0 88.3</cell><cell>85.0</cell><cell>82.6</cell><cell>90.1</cell><cell>90.7 81.6 94.7 68.2</cell><cell>67.2</cell><cell>83.0?0.3</cell></row><row><cell>EC-CAP2</cell><cell>78.2</cell><cell>88.0 88.5</cell><cell>85.7</cell><cell>83.0</cell><cell>90.8</cell><cell>91.1 80.0 95.4 70.4</cell><cell>64.1</cell><cell>83.2?0.4</cell></row><row><cell>EC-CAP3</cell><cell>78.5</cell><cell>91.7 89.3</cell><cell>86.3</cell><cell>83.5</cell><cell>90.9</cell><cell>91.1 81.8 94.9 70.0</cell><cell>65.6</cell><cell>84.0?0.4</cell></row><row><cell>EC-CF2</cell><cell>79.1</cell><cell>89.6 89.3</cell><cell>86.8</cell><cell>84.3</cell><cell>91.3</cell><cell>91.2 81.1 95.2 68.1</cell><cell>68.0</cell><cell>84.0?0.2</cell></row><row><cell>EC-CF1</cell><cell>77.4</cell><cell>90.6 88.0</cell><cell>85.5</cell><cell>83.6</cell><cell>90.3</cell><cell>91.2 79.8 95.3 66.5</cell><cell>64.9</cell><cell>83.0?0.2</cell></row><row><cell>EC-CF0.5</cell><cell>77.4</cell><cell>89.6 86.3</cell><cell>85.2</cell><cell>82.7</cell><cell>91.7</cell><cell>91.0 79.6 94.9 67.3</cell><cell>63.5</cell><cell>83.0 ?0.05</cell></row><row><cell cols="2">Hash Layers 76.1</cell><cell>85.2 86.7</cell><cell>83.4</cell><cell>82.5</cell><cell>90.0</cell><cell>90.3 75.7 94.0 67.4</cell><cell>63.3</cell><cell>81.3?1.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Comparison between different routing methods in fine-tuning of 100M/64E models. We perform 3 independent fine-tuning runs for each method and report the average results. This gives more accurate difference between the variants of expert choice method, since they achieve close fine-tuning results. We do not report averaged results in other experiments.Our 8B MoE model achieves stronger pre-training perplexity than its dense counterpart. However, a better perplexity does not always directly translate to downstream performance as demonstrated in Section 4.2.3. To this end, we compare fine-tuning performance of the 8B dense model and MoE model in Table5. As shown in the table, our MoE model using expert choice routing consistently outperforms the dense model across the 11 tasks in GLUE and SuperGLUE.</figDesc><table /><note><p>A Comparison on Fine-tuning with a Dense Model</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Validation perplexity during pre-training using various expert choice methods and top-2 gating.</figDesc><table><row><cell>Eval Perplexity</cell><cell>2.70 2.75 2.80 2.85</cell><cell>Comparison with Capped Expert Choice EC_BASE EC_CAP2 EC_CAP3 GS_TOP2</cell></row><row><cell></cell><cell>2.65</cell><cell></cell></row><row><cell></cell><cell></cell><cell>100 200 300 400 500 600 700 10K Steps</cell></row><row><cell>Figure 8:</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>H(A) = ij -A[i, j] log A[i, j]</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We use ? = 0.001 and a maximum of 100 iterations.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Conditional channel gated networks for task-aware continual learning</title>
		<author>
			<persName><forename type="first">Davide</forename><surname>Abati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tijmen</forename><surname>Blankevoort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><forename type="middle">Ehteshami</forename><surname>Bejnordi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3930" to="3939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ilya Sutskever, and Dario Amodei</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Exponentially increasing the capacity-to-computation ratio for conditional computation in deep learning</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">CoAtNet: Marrying convolution and attention for all data sizes</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Transformer-XL: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07">July 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Yann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="933" to="941" />
		</imprint>
	</monogr>
	<note>ICML&apos;17</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><forename type="middle">Emma</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kellie</forename><surname>Webster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie</forename><surname>Pellat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathy</forename><surname>Meier-Hellstern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toju</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Cui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Glam: Efficient scaling of language models with mixtureof-experts</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Tricks for training sparse translation models</title>
		<author>
			<persName><forename type="first">Dheeru</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedanuj</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">An iterative procedure for obtaining i-projections onto the intersection of convex sets. The annals of Probability</title>
		<author>
			<persName><surname>Richard L Dykstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">1985</date>
			<biblScope unit="page" from="975" to="984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity</title>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">NAS-FPN: learning scalable feature pyramid architecture for object detection</title>
		<author>
			<persName><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7036" to="7045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Hard mixtures of experts for large scale weakly supervised vision</title>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc'aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<editor>
			<persName><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<title level="m">Gaussian error linear units (GELUs)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Joel</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Newsha</forename><surname>Ardalani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Kianinejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Mostofa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Deep learning scaling is predictable, empirically</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A brief survey on power gating design</title>
		<author>
			<persName><forename type="first">Ping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuocheng</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guitao</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 10th IEEE International Conference on Solid-State and Integrated Circuit Technology</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="788" to="790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gpipe: Efficient training of giant neural networks using pipeline parallelism</title>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mia</forename><surname>Xu Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Hanna</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hugo</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alina</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Emily</forename><forename type="middle">B</forename><surname>Florence D'alch?-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Roman</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. December 8-14, 2019. 2019</date>
			<biblScope unit="page" from="103" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A domain-specific supercomputer for training deep neural networks</title>
		<author>
			<persName><forename type="first">Norman</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doe</forename><surname>Hyun Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Laudon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Scaling laws for neural language models</title>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">GShard: Scaling giant models with conditional computation and automatic sharding</title>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Base layers: Simplifying training of large, sparse models</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">Marina</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021-07">Jul 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="18" to="24" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Conditional computation for continual learning</title>
		<author>
			<persName><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Pipedream: Generalized pipeline parallelism for dnn training</title>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Harlap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amar</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><forename type="middle">R</forename><surname>Devanur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">R</forename><surname>Ganger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Association for Computing Machinery</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Scalable transfer learning with expert models</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><forename type="middle">Riquelme</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C?dric</forename><surname>Renggli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andr?</forename><surname>Susano Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>In ICLR. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">67</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Hash layers for large sparse models</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><forename type="middle">Etzioni</forename><surname>Green Ai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mesh-tensorflow: Deep learning for supercomputers</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Penporn</forename><surname>Koanantakool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingsheng</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><surname>Hechtman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS&apos;18</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems, NIPS&apos;18<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="10435" to="10444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Outrageously large neural networks: The sparsely-gated mixture-ofexperts layer</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster). OpenReview.net</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adafactor: Adaptive learning rates with sublinear memory cost</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">Jennifer</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</editor>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018-07">Jul 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Megatron-lm: Training multi-billion parameter language models using model parallelism</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Superglue: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alch?-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11">November 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">GSPMD: general and scalable parallelization for ML computation graphs</title>
		<author>
			<persName><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><forename type="middle">A</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Ly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcello</forename><surname>Maggioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shibo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<idno>CoRR, abs/2105.04663</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
