<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Uncertainty-Matching Graph Neural Networks to Defend Against Poisoning Attacks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Uday</forename><surname>Shankar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Arizona State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jayaraman</forename><forename type="middle">J</forename><surname>Thiagarajan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Lawrence Livermore National Labs</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Andreas</forename><surname>Spanias</surname></persName>
							<email>spanias@asu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Arizona State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Uncertainty-Matching Graph Neural Networks to Defend Against Poisoning Attacks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs), a generalization of neural networks to graph-structured data, are often implemented using message passes between entities of a graph. While GNNs are effective for node classification, link prediction and graph classification, they are vulnerable to adversarial attacks, i.e., a small perturbation to the structure can lead to a non-trivial performance degradation. In this work, we propose Uncertainty Matching GNN (UM-GNN), that is aimed at improving the robustness of GNN models, particularly against poisoning attacks to the graph structure, by leveraging epistemic uncertainties from the message passing framework. More specifically, we propose to build a surrogate predictor that does not directly access the graph structure, but systematically extracts reliable knowledge from a standard GNN through a novel uncertainty-matching strategy. Interestingly, this uncoupling makes UM-GNN immune to evasion attacks by design, and achieves significantly improved robustness against poisoning attacks. Using empirical studies with standard benchmarks and a suite of global and target attacks, we demonstrate the effectiveness of UM-GNN, when compared to existing baselines including the state-of-the-art robust GCN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Representation learning methods, in particular deep learning, have produced state-of-the-art results in image analysis, language modeling and more recently with graph-structured data (Torng and Altman 2019). In particular, graph neural networks (GNNs) <ref type="bibr" target="#b12">(Kipf and Welling 2017;</ref><ref type="bibr" target="#b10">Hamilton, Ying, and Leskovec 2017)</ref> have gained prominence due to their ability to effectively leverage the inherent structure to solve challenging tasks including node classification, link prediction and graph classification <ref type="bibr" target="#b23">(Wu et al. 2020)</ref>.</p><p>Despite their wide-spread use, GNNs are known to be vulnerable to a variety of adversarial attacks, similar to standard deep models. In other words, a small imperceptible perturbation intentionally designed in the graph structure can lead to a non-trivial performance degradation as seen in <ref type="bibr" target="#b26">(Zügner, Akbarnejad, and Günnemann 2018)</ref>. This limits their application to high-risk and safety critical domains. For example, the popular graph convolutional networks (GCN), which rely on aggregating message passes from a node's neighbor-hood, are not immune to poisoning attacks, wherein an attacker adds fictitious edges to the graph before the model is trained. Though there exists a vast literature on adversarial attacks on images <ref type="bibr" target="#b10">(Goodfellow, Shlens, and Szegedy 2014;</ref><ref type="bibr" target="#b18">Szegedy et al. 2013)</ref> and their countermeasures <ref type="bibr" target="#b13">(Ren et al. 2020;</ref><ref type="bibr" target="#b3">Chakraborty et al. 2018)</ref>, designing attack strategies for graphs is a more recent topic of research. In general, designing graph attacks poses a number of challenges: (i) the adversarial search space is discrete; (ii) nodes in the graphs are <ref type="bibr">non-i.i.d., and (iii)</ref> lack of effective metrics to measure structural perturbations. Following the progress in graph adversarial attacks, designing defense mechanisms or building robust variants of GNNs have become critical <ref type="bibr" target="#b25">(Zhu et al. 2019)</ref>.</p><p>In this paper, we propose a new approach UM-GNN aimed at improving the robustness of GNN models, particularly against challenging poisoning attacks to the graph structure. Our approach jointly trains a standard GNN model (implemented using GCN) and a surrogate predictor, which accesses only the features, using a novel uncertainty matching strategy. Through a systematic knowledge transfer from the GNN model, the surrogate demonstrates significantly improved robustness to challenging attacks. The key contributions of this work are summarized as follows:</p><p>• A novel architecture for semi-supervised learning, UM-GNN, that can be built upon any existing GNN model and is immune to evasion attacks by design;</p><p>• An uncertainty matching-based knowledge transfer strategy for achieving robustness to structural perturbations;</p><p>• Across a suite of global poisoning attacks, UM-GNN consistently outperforms existing methods including the recent Robust GCN <ref type="bibr" target="#b25">(Zhu et al. 2019</ref>);</p><p>• UM-GNN achieves significantly lower misclassification rate (&gt; 50% improvement) against targeted attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Setup</head><p>In this paper, we are interested in building graph neural networks that are robust to adversarial attacks on the graph structure. We represent an unweighted graph using the tuple G = (V, E), where</p><formula xml:id="formula_0">V = {v 1 , v 2 , • • • , v N } denotes</formula><p>the set of nodes with cardinality |V| = N , E denotes the set of edges and E ⊆ V × V. The edges in the graph may be alternately represented using an adjacency matrix A ∈ R N ×N . In addition, each node v i may be endowed with a d-dimensional node attribute vector x i ∈ R d . We use the matrix X ∈ R N ×d to denote the features from all nodes. We focus on a transductive learning setting, where the goal is to perform node classification. In particular, we assume that we have access to labels for a subset of nodes V L ⊂ V and we need to predict the labels for the remaining nodes</p><formula xml:id="formula_1">(v ∈ V \ V L ) in G. Each node v i is associated with a label y i ∈ Y = [1, • • • , K].</formula><p>While a variety of approaches currently exist to solve this semi-supervised learning problem, we restrict our study to the recently successful solutions based on graph neural networks (GNNs). A recurring idea in many existing GNN models is to utilize a message passing mechanism to aggregate and transform features from the neighboring nodes. Implementing a GNN hence involves designing a message function P and an update function U, i.e.,</p><formula xml:id="formula_2">m i = j∈Ni P(h i , h j , e ij ); h i = U(h i , m i ),<label>(1)</label></formula><p>where N i denotes the neighborhood of a node v i and h i its feature representation (in the input layer h i = x i ). For example, in a standard graph convolutional network (GCN),</p><formula xml:id="formula_3">h i = ψ   j∈Ni α ij h j W   .<label>(2)</label></formula><p>Here, the message computation is parameterized by α ij , which can be a symmetric normalization constant <ref type="bibr" target="#b12">(Kipf and Welling 2017)</ref> or a learnable attention weight <ref type="bibr" target="#b19">(Veličković et al. 2018</ref>). The update function U is parameterized using the learnable weights W and applies a non-linearity ψ.</p><p>As discussed earlier, our goal is to defend against adversarial attacks on the graph structure. Formally, we assume that an adversary induces structural perturbations to the graph, i.e., Ĝ = ( Â, X) such that A − Â 0 ≤ ∆.</p><p>Here, ∆ is used to ensure that the adversarial attack is imperceptible. Note that, one can optionally also consider the setting where the features X are also perturbed. While different classes of attacks currently exist (see Section 6), we focus on poisoning attacks, wherein the graph is corrupted even before the predictive model is trained. This is in contrast to evasion attacks, which assume that the model is trained on clean data and the perturbations are introduced at a later stage. We consider different popular poisoning attacks from the literature (see Section 4) and study the robustness of our newly proposed UM-GNN approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Approach</head><p>In this section, we present the proposed approach, Uncertainty Matching-GNN (UM-GNN), and provide details on the model training process.</p><p>While there exist very few GNN formulations for specifically defending against adversarial attacks, the recent robust GCN (RGCN) approach <ref type="bibr" target="#b25">(Zhu et al. 2019</ref>) has been the most effective, when compared to standard GCN and GAT models. At its core, RGCN relies on using the aleatoric uncertainties in the graph structure to weight the neighborhood. Since there exists no a priori knowledge about the structural uncertainties, in practice, simple priors such as the normal distribution (zero mean, unit variance) are placed on the node features and propagated through the network to estimate uncertainties at the output of each layer. Finally, a modified message passing is utilized, wherein neighboring nodes with low feature variance are emphasized during message computation to produce robust features. Despite its empirical benefits, this approach suffers from three main challenges: (i) the choice of the prior is critical to its success; (ii) since the estimated uncertainties are not calibrated, the fidelity of the uncertainty estimates themselves can be low, thus leading to only marginal improvements over GCN in practice; (iii) the model (epistemic) uncertainties are not considered, which can impact the generalization of the inferred parameters to the test nodes. In order to alleviate these challenges, we propose UM-GNN, a new GNN formulation that uses an uncertainty matching-based knowledge transfer strategy for achieving robustness to graph perturbations. In contrast to RGCN, UM-GNN utilizes epistemic uncertainties from the GNN, and does not require any modifications to the message passing module. As we will show in our empirical studies, our approach provides significant improvements in defending against well-known poisoning attacks.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> provides an illustration of UM-GNN, which jointly trains a GNN model M(Θ) and a surrogate model F(Φ) that is trained solely using the features X without any knowledge of the graph structure. Here Θ and Φ denote the learnable model parameters. Since we expect the graph structure to be potentially corrupted (though sever-ity or type of corruption is unknown), the predictions from the GNN model could be unreliable due to the presence of noisy edges. We reformulate the problem of making M robust into systematically transferring the most reliable knowledge to the surrogate F, so that F can make robust predictions. When compared to existing regularization strategies such as GraphMix <ref type="bibr" target="#b20">(Verma et al. 2019)</ref>, we neither use the (solely) feature-based model F to regularize the training of M nor are the weights shared between the networks. Instead, we build a surrogate predictor that selectively extracts the most reliable information from the "non-robust" M with the hope of being more robust to the noise in the graph structure. Interestingly, by design, the model F does not rely on the graph structure and hence is oblivious to evasion attacks. As showed in the figure, after training, we only use the surrogate F to obtain the predictions for unlabeled nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Bayesian Uncertainty Estimation</head><p>Quantifying the prediction uncertainties in the graph neural network M is at the core of UM-GNN. We propose to utilize Bayesian Neural Networks (BNNs) <ref type="bibr" target="#b1">(Blundell et al. 2015)</ref>, in particular its scalable variant based on Monte Carlo dropout <ref type="bibr" target="#b16">(Srivastava et al. 2014)</ref>. In general, dropout variational inference is used to estimate the epistemic uncertainties as follows: A deep network is trained with dropout and even at test time the dropout is used to generate samples from the approximate posterior through Monte Carlo sampling. Interestingly, it was showed in <ref type="bibr" target="#b9">(Gal and Ghahramani 2016)</ref> that the dropout inference minimizes the KL divergence between the approximated distribution and the posterior of a deep Gaussian process. The final prediction can then be obtained by marginalizing over the posterior, using Monte Carlo integration. In our formulation, the node classification task is transductive in nature and does not require test-time inferencing. Hence, we propose to leverage the prediction uncertainties in the training loop itself. More specifically, we obtain the prediction for each node v i as</p><formula xml:id="formula_4">p(y i = k; x i , A) = Softmax 1 T T t=1 M(x i , A; Θ) .</formula><p>Here we make T forward passes for x i with different masked weights Θ (using dropout inference) and compute the final prediction using a sample average. Note, we assume that the predictive model produces logits, i.e., no activation in the final prediction layer and hence compute the Softmax of the average predictions. We then use the entropy of the resulting prediction p(y i = k; x i , A) as an estimate of the model uncertainty for node v i .</p><formula xml:id="formula_5">Unc(v i ) = Entropy p(y i = k; x i , A) = − K k=1 p(y i = k) log p(y i = k) (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Algorithm</head><p>We now present the algorithm to train an UM-GNN model given a poisoned graph Ĝ = ( Â, X). As described earlier, our architecture is composed of a graph neural network M(Θ) and a surrogate model F(Φ) that takes only the features X as input. While we implement M using graph convolution layers as defined in eqn.</p><p>(2), it can be replaced using any other message passing strategy, e.g, graph attention layers <ref type="bibr" target="#b19">(Veličković et al. 2018)</ref>. Given that all datasets we consider in our study contain vector-values defined at the nodes, we implement F as a fully connected network. The optimization problem used to solve for the parameters Θ and Φ is given below: minimize</p><formula xml:id="formula_6">Θ,Φ L ce + λ m L m + λ s L s .<label>(4)</label></formula><p>Here, the first term L ce corresponds to the standard cross entropy loss over the set of labeled nodes computed using the predictions from the GNN model M.</p><p>The second term L m is used to align the predictions between the surrogate and GNN models so that the resulting classifiers are consistent. Directly distilling knowledge from the GNN model enables F to actually make meaningful predictions for the nodes, even without accessing the underlying graph structure. However, using a poisoned graph to build M can lead to predictions with high uncertainties. Such noisy examples may lead to unreliable gradients, thus making the knowledge transfer unstable. Hence, we propose to attenuate the influence of samples with high prediction uncertainty. We refer to this process as uncertainty matching and implement it using the KL divergence. However, this can be readily replaced using any general divergence or the Wasserstein metric. Mathematically,</p><formula xml:id="formula_7">L m = N i=1 β i KLDiv(M(x i , A; Θ), F(x i ; Φ)),<label>(5)</label></formula><p>where the weight β i s are computed as</p><formula xml:id="formula_8">β i = exp(−α i ) j exp(−α j )</formula><p>; where</p><formula xml:id="formula_9">α i = log 1 1 + Unc(v i ) .<label>(6)</label></formula><p>When the prediction uncertainty for a sample is low, it is given higher attention during matching. Note that, this loss is evaluated using both labeled and unlabeled nodes, since it does not need access to the true labels. Finally, the third term L s corresponds to a label smoothing regularization that attempts to match the predictions from F to an uniform distribution (KL divergence). This is included to safeguard the surrogate model from being misguided by the graph network, when the latter's confidences are not well-calibrated due to the poisoned graph. In all our experiments, we set λ m = 0.3 and λ s = 0.001. Figure <ref type="figure" target="#fig_1">2</ref> illustrates the behavior of UM-GNN for two different datasets under varying levels of poisoning. As the severity of the corruption increases, the surrogate model achieves significantly higher test performance when compared to the graph-based model M. In cases where no explicit node attributes are available, F may be implemented as a GNN and the uncertainty matching strategy will still be applicable and this is part of our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Poisoning Attacks used for Evaluation</head><p>While there exists a broad class of adversarial attacks that are designed to be applied during the testing phase of the In particular, it is well known that they are highly effective at degrading the performance of GNNs. More importantly, existing robust modeling variants such as RGCN provide only marginal improvements over the standard GNN models, when presented with poisoned graphs. Hence, we evaluate the proposed UM-GNN using several widely-adopted poisoning attacks. Here, we briefly describe those attacks and provide our implementation details.</p><p>Random Attack This is a purely black-box attack, where the attacker has no knowledge of ground truth labels or the model information. More specifically, in this attack, new edges are randomly introduced between two nodes that were not previously connected. Though being simple, this attack is known to be effective, particularly at higher noise ratios and sparse graphs. For our experiments, we varied the ratio of noisy edges between 10% and 100% of the total number of edges in the original graph.</p><p>DICE Attack <ref type="bibr">(Waniek et al. 2018</ref>) This is a gray-box attack where the attacker has information about the node labels but not the model parameters. This attack uses a modularity-based heuristic to Disconnect Internally (nodes from the same community) and Connect Externally (DICE) (nodes from different communities). For a given budget, an attacker randomly deletes edges that connect nodes from the same class; and adds edges between randomly chosen node pairs of samples from different classes. Similar to the random attack, we varied the perturbation ratio between 10% and 100% of the total number of existing edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meta-Gradient</head><p>Attack (Mettack) (Zügner and Günnemann 2019) This is a more challenging graybox attack where the attacker utilizes the graph structure and labels to construct a surrogate model, which is then utilized to generate the attacks. More specifically, Mettack formulates a bi-level optimization problem of maximizing the classification error on the labeled nodes after optimizing the model parameters on the poisoned graph. In other words, the graph structure is treated as the hyper-parameter to optimize, and this is solved using standard meta-learning strategies. Since the surrogate model is also designed based on GCNs (similar architectures as our predictive model) Projected-Gradient Attack (PGD) <ref type="bibr" target="#b24">(Xu et al. 2019</ref>) PGD is a first-order topology attack that attempts to determine the minimum edge perturbations in the global structure of the graph, such that the generalization can be maximally affected. Since PGD cannot access the true model parameters, we use a surrogate GNN model to generate the attacks. Similar to Mettack, we varied the perturbation ratio between 1% and 10% in this case as well.</p><p>Fast Gradient Attack (FGA) <ref type="bibr" target="#b4">(Chen et al. 2018)</ref> FGAs are created based on gradient information in GNNs and they belong to the category of targeted attacks. The goal of a targeted attack is to mislead the model into classifying a target node incorrectly. In FGA, the attacker adds an edge between node pairs that are characterized by largest absolute difference in their gradients. We choose FGA to show the superior performance of UM-GNN even against targeted attacks. The implementations for Mettack, PGD and FGA were based on the publicly available DeepRobust <ref type="bibr" target="#b11">(Jin et al. 2020)</ref> library. Due to the lack of computationally efficient implementations, we could not generate these attacks on largescale graphs such as Pubmed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Empirical Evaluation</head><p>In this section, we evaluate the robustness of UM-GNN against the graph poisoning methods discussed in the previous section. As mentioned in Section 4, non-targeted poisoning attacks are far more challenging and pose a more realistic threat to graph-based models. Datasets We consider three benchmark citation networks extensively used in similar studies: Cora, Citeseer, and Pubmed <ref type="bibr" target="#b14">(Sen et al. 2008)</ref>. The documents are represented by nodes, and citations among the documents are encoded as undirected edges. We follow the typical transductive node classification setup <ref type="bibr" target="#b12">(Kipf and Welling 2017;</ref><ref type="bibr" target="#b19">Veličković et al. 2018)</ref>, while using the standard train, test, and validation splits for our experiments (see Table <ref type="table" target="#tab_0">1</ref>).</p><p>Baselines We compare the proposed approach with three important baseline GNN models, which adopt different message passing formalisms and have been successfully used in semi-supervised node classification tasks. Note that the performance of a feature-only classifier (MLP) which ignores the graph structure produces trivial performances with the following accuracies: 55.1% for Cora, 46.5% for Citeseer, and 71.4% for Pubmed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GCN:</head><p>We use the GCN model, proposed by Kipf &amp; Welling, based on the message passing formulation in eqn. (2).</p><p>GAT <ref type="bibr" target="#b19">(Veličković et al. 2018</ref>): This model uses a multi-head attention mechanism to learn the hidden representations for each node through a weighted aggregation of features in a closed neighborhood where the weights are trainable. RGCN <ref type="bibr" target="#b25">(Zhu et al. 2019</ref>): This is a recently proposed ap-proach that explicitly enhances the robustness of GCNs. RGCN models node features as distributions as opposed to deterministic vectors in GCN and GAT models. It employs a variance-based attention mechanism to attenuate the influence of neighbors with large variance (potentially corrupted). Following <ref type="bibr" target="#b25">(Zhu et al. 2019)</ref>, we set hidden dimensions at 16 and assume a diagonal covariance for each node. For all baselines, we set the number of layers (2 layers) and other hyper-parameter settings as specified in their original papers. We set the number of hidden neurons to 16 for both GCN and GAT baselines. In addition, we set the number of attention heads to 8 for GAT. We implemented all the baselines and the proposed approach using the Pytorch Deep Graph Library (version 0.5.1) <ref type="bibr" target="#b21">(Wang et al. 2019</ref>). In our implementation of UM-GNN, the GNN model M was designed as a 2−layer GCN similar to the baseline and the surrogate F was a 3−layer FCN with configuration 32−16−K, where K is the total number of classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results</head><p>We evaluated the classification accuracy on the test nodes for the datasets against each of the attacks, under varying levels of perturbation. For random and DICE attacks, we varied the ratio of noisy edges to clean edges between 0.1 and 1. Since Mettack and PGD attacks are more powerful, we used noise ratios in the range (0.01, 0.1). For all the 4 global attacks, we repeated the experiment for 20 random trials (different corruption) for each noise ratio, and report the expected accuracies along with their standard deviations.</p><p>(i) Random Attack: The results for random attacks for all three datasets are shown in Figure <ref type="figure">3</ref>. As discussed earlier, RGCN provides only a marginal improvement over the vanilla GCN and GAT. However, UM-GNN consistently outperforms the baselines by a large margin even when the ratio of noisy edges to clean edges is high. In addition, UM-GNN has the least variance in performance compared to the baselines. In comparison, GAT appears to be the most sensitive to random structural perturbations and its low performance strongly corroborates with the findings in <ref type="bibr" target="#b25">(Zhu et al. 2019</ref>).</p><p>(ii) DICE Attack: In this challenging attack, where the attacker can both delete and add edges, all baseline methods suffer from severe performance degradation, when compared to random attacks. Surprisingly, UM-GNN is significantly more robust and achieves performance improvements as high as ≈ 15% (Figure <ref type="figure">4</ref>, Citeseer, noise ratio = 1.0). This clearly evidences the ability of UM-GNN to infer the true modular structure, even when the graph is poisoned.</p><p>(iii) Mettack Attack: Since mettack uses a surrogate model and its parameters to generate attacks, it is one of the more challenging attacks to defend. Nevertheless, UM-GNN consistently outperforms all the baselines by a good margin, as illustrated in Figure <ref type="figure" target="#fig_3">5</ref>. Interestingly, under this attack, both GCN and RGCN perform poorly when compared to the GAT model. However, the large variance makes GAT unreliable in practice, particularly when the attack is severe.</p><p>(iv) PGD Attack: This is comparatively the most severe, since the GCN model used to generate the attack has the same architecture as our model M, thus in actuality making it a white-box attack. From Figure <ref type="figure" target="#fig_4">6</ref>, we observe 1% − 2% improvements in mean performance over the baselines. More importantly, the lower variance of UM-GNN across trials makes it a suitable choice for practical scenraios.</p><p>(v) FGA Attack: For this targeted attack, we selected 100 test nodes with correct predictions in a baseline GCN as our tar- gets. Out of the 100 target nodes, 25 nodes were those with the highest margin of classification, 25 nodes were those with the lowest margin, and the remaining 50 were chosen randomly. Further, we set the number of perturbations allowed on each target node to be equal to its degree (so that it is imperceptible). The FGA attack was generated for each target node independently, and we checked if the targeted attack was defended successfully or not, i.e., whether the targeted node was classified correctly using the poisoned graph. The overall misclassification rates for the different models are shown in Table <ref type="table" target="#tab_1">2</ref>. We find that UM-GNN provides dramatic improvements in defending against FGA attacks, through its systematic knowledge transfer between the GNN M and the surrogate F. In Figure <ref type="figure">7</ref>, we plot the prediction probabilities for the true class (indicates a model's confidence) for all target nodes obtained using the original and poisoned graphs G and Ĝ respectively. As it can be observed, UM-GNN improves the confidences considerably for all samples, while the baseline methods demonstrate vulnerability to FGA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Semi-supervised learning based on graph neural networks (GNNs) enables representation learning using both the graph structure and node features <ref type="bibr" target="#b23">(Wu et al. 2020)</ref>. While GNNs based on spectral convolutional approaches <ref type="bibr" target="#b3">(Bruna et al. 2013;</ref><ref type="bibr" target="#b6">Defferrard, Bresson, and Vandergheynst 2016;</ref><ref type="bibr" target="#b12">Kipf and Welling 2017)</ref> have been widely adopted, there also exists models that implement convolutions directly using spatial neighborhoods <ref type="bibr" target="#b7">(Duvenaud et al. 2015;</ref><ref type="bibr" target="#b0">Atwood and Towsley 2016;</ref><ref type="bibr" target="#b10">Hamilton, Ying, and Leskovec 2017)</ref>. The vulnerability of GNNs to adversarial attacks was first studied in <ref type="bibr" target="#b26">(Zügner, Akbarnejad, and Günnemann 2018)</ref>. Since then, several graph adversarial attacks have been proposed <ref type="bibr" target="#b11">(Jin et al. 2020;</ref><ref type="bibr" target="#b17">Sun et al. 2018)</ref>. Adversarial attacks on graphs can be broadly categorized as follows:</p><p>(i) Attacker knowledge: based on the level of access an attacker has to the model internals, namely white-box <ref type="bibr" target="#b24">(Xu et al. 2019;</ref><ref type="bibr" target="#b22">Wu et al. 2019)</ref>, gray-box <ref type="bibr" target="#b26">(Zügner, Akbarnejad, and Günnemann 2018;</ref><ref type="bibr" target="#b27">Zügner and Günnemann 2019)</ref> Figure <ref type="figure">7</ref>: Results from FGA attacks on two benchmark datasets -On the x-axis, we plot the prediction probabilities for the true class obtained using GCN on the clean graph G. On the y-axis, we show the prediction probabilities obtained after the targeted attack. Note, for each method, we show the misclassified nodes in red and the correct predictions in green.  <ref type="bibr">2018)</ref> or targeting specific nodes either directly or indirectly for their misclassification <ref type="bibr" target="#b4">(Chen et al. 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Graph Adversarial Defense As graph adversarial attacks continue to be studied, efforts aimed at designing suitable defense strategies have emerged recently. For example, Feng et al. adapted the conventional adversarial training approach to the case of graphs in order to make GNNs more robust <ref type="bibr" target="#b10">(Goodfellow, Shlens, and Szegedy 2014;</ref><ref type="bibr" target="#b8">Feng et al. 2019</ref>). On the other hand, methods that rely on graph preprocessing have also been proposed -for example, in <ref type="bibr" target="#b22">(Wu et al. 2019)</ref>, edges with low Jaccard similarity between the constituent nodes were removed prior to training a GNN. Similarly, in <ref type="bibr" target="#b11">(Jin et al. 2019)</ref>, explicit graph smoothing was performed by training on a family of graphs to defend against evasion attacks. Entezari et al. obtained a low rank approximation of the given graph and showed that it can defend against specific types of graph attack <ref type="bibr" target="#b26">(Zügner, Akbarnejad, and Günnemann 2018)</ref>. Recently, Zhu et al. <ref type="bibr" target="#b25">(Zhu et al. 2019</ref>) introduced a robust variant of GCN based on a variance-weighted attention mechanism, and showed it to be effective against different types of attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In this work, we presented UM-GNN an uncertainty matching-based architecture to explicitly enhance the robustness of GNN models. UM-GNN utilizes epistemic uncertainties from a standard GNN M and does not require any modifications to the message passing module. Consequently, our architecture is agnostic to the choice of GNN to implement M. By design, the surrogate model F does not directly access the graph structure and hence is immune to evasionstyle attacks. Our empirical studies clearly evidenced the effectiveness of UM-GNN in defending against several graph poisoning attacks, thereby outperforming existing baselines. Furthermore, we showed dramatic improvements on defense against targeted attacks (FGA). Future work includes studying the performance bounds of UM-GNN and developing extensions for inductive learning settings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustration of the proposed UM-GNN, which constructs a surrogate model F and through an uncertainty matching strategy achieves robustness to poisoning attacks. After the model is trained, we use the surrogate model F to make predictions for the unlabeled nodes.</figDesc><graphic url="image-1.png" coords="2,316.17,54.00,240.63,228.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Illustration of the behavior of UM-GNN for two datasets under varying types and levels of poisoning attacks. In each case, we show the test accuracy curves across the training epochs from both the GNN and surrogate models. As the noise severity increases, the surrogate model F demonstrates improved robustness.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure3: Random attack: UM-GNN achieves robustness to random attacks, providing over 5 − 10% improvements in the test accuracy, even when the noise ratio is 1.0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: Mettack -This gray-box attack is known to be highly effective at causing performance degradation in GNNs. However, UM-GNN consistently provides 3 − 5% improvements in the test accuracy over the baselines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: PGD Attack -This is comparatively very severe, since it uses gradients from a GCN model (same architecture as M). While the accuracy improvements are still non-trivial (1% − 2%), the more interesting observation is the reduced variance of UM-GNN across trials.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of the three benchmark citation datasets used in our experments.</figDesc><table><row><cell cols="5">Dataset # Nodes # Edges # Features # Classes</cell></row><row><cell>Cora</cell><cell>2708</cell><cell>5278</cell><cell>1433</cell><cell>7</cell></row><row><cell>Citeseer</cell><cell>3327</cell><cell>4614</cell><cell>3703</cell><cell>6</cell></row><row><cell>Pubmed</cell><cell>19717</cell><cell>44325</cell><cell>500</cell><cell>3</cell></row></table><note>and trained with the entire graph (transductive setting), this gray-box attack is very powerful in practice. Hence we used lower noise ratios for our experiments, i.e., between 1% to 10% of the total existing edges, when compared to Random and DICE attacks.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Misclassification rates from 100 target nodes with FGA attack. A lower value implies improved robustness. and black-box attacks<ref type="bibr" target="#b2">(Bojchevski and Günnemann 2019)</ref>. (ii) Attacker capability: based on whether the attacker perturbs the graph before(Liu et al. 2019)  or after<ref type="bibr" target="#b5">(Dai et al. 2018)</ref> the model is trained. (iii) Attack strategy: based on whether the attacker corrupts the graph structure or node features. While structural perturbations can be induced by deleting, adding or re-wiring edges; new nodes could also be injected into the graph<ref type="bibr" target="#b15">(Shanthamallu, Thiagarajan, and Spanias 2020)</ref>. (iv)Attacker's goal: based on whether the attack is aimed at degrading the model's overall performance (Waniek et al.</figDesc><table><row><cell></cell><cell cols="2">Cora Citeseer</cell></row><row><cell>GCN</cell><cell>0.78</cell><cell>0.73</cell></row><row><cell>GAT</cell><cell>0.71</cell><cell>0.74</cell></row><row><cell>RGCN</cell><cell>0.73</cell><cell>0.76</cell></row><row><cell cols="2">UM-GNN 0.21</cell><cell>0.23</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was performed under the auspices of the U.S. Department of Energy by Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344. This work was also partially supported by the ASU SenSIP Center.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="1993">2016. 1993-2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.05424</idno>
		<title level="m">Weight uncertainty in neural networks</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adversarial attacks on node embeddings via graph poisoning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="695" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<idno>arXiv:1810.00069</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
				<imprint>
			<date type="published" when="2013">2013. 2018</date>
		</imprint>
	</monogr>
	<note>Adversarial attacks and defences: A survey</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02797</idno>
		<title level="m">Fast gradient attack on network embedding</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02371</idno>
		<title level="m">Adversarial attack on graph structured data</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Graph adversarial training: Dynamically regularizing based on graph structure</title>
		<author>
			<persName><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Knowledge and Data Engineering</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014. 2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
	<note>Explaining and harnessing adversarial examples</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Power up! robust graph convolutional network against evasion attacks based on graph powering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sojoudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10029</idno>
		<idno>arXiv:2003.00653</idno>
	</analytic>
	<monogr>
		<title level="m">Adversarial Attacks and Defenses on Graphs: A Review and Empirical Study</title>
				<imprint>
			<date type="published" when="2019">2019. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A unified framework for data poisoning attack to graph-based semisupervised learning</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><surname>Iclr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.14147</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017. 2019</date>
		</imprint>
	</monogr>
	<note>Semi-Supervised Classification with Graph Convolutional Networks</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adversarial attacks and defenses in deep learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Engineering</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A Regularized Attention Mechanism for Graph Attention Networks</title>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">S</forename><surname>Shanthamallu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Thiagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Spanias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3372" to="3376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.10528</idno>
		<title level="m">Adversarial attack and defense on graph data: A survey</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for predicting drug-target interactions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Torng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Altman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Modeling</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4131" to="4149" />
			<date type="published" when="2013">2013. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv pre</note>
	<note>Intriguing properties of neural networks</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJXMpikCZ" />
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Learning Representations URL</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Graphmix: Regularized training of graph neural networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11715</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hiding individuals and communities in a social network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Michalak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wooldridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rahwan</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1909.01315.Waniek" />
	</analytic>
	<monogr>
		<title level="m">Deep Graph Library: Towards Efficient and Scalable Deep Learning on Graphs. ICLR Workshop on Representation Learning on Graphs and Manifolds URL</title>
				<imprint>
			<date type="published" when="2018">2019. 2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="139" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tyshetskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Docherty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.01610</idno>
		<title level="m">Adversarial examples on graph data: Deep insights into attack and defense</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Neural Networks and Learning Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-W</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04214</idno>
		<title level="m">Topology attack and defense for graph neural networks: An optimization perspective</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Robust graph convolutional networks against adversarial attacks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1399" to="1407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adversarial attacks on neural networks for graph data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zügner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Akbarnejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2847" to="2856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Adversarial attacks on graph neural networks via meta learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zügner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.08412</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
