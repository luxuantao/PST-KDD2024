<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Submodular Optimization with Submodular Cover and Submodular Knapsack Constraints</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Rishabh</forename><surname>Iyer</surname></persName>
							<email>rkiyer@u.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
							<email>bilmes@u.washington.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Submodular Optimization with Submodular Cover and Submodular Knapsack Constraints</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">50B35F37590A7BE3E129DF7528AA479D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We investigate two new optimization problems -minimizing a submodular function subject to a submodular lower bound constraint (submodular cover) and maximizing a submodular function subject to a submodular upper bound constraint (submodular knapsack). We are motivated by a number of real-world applications in machine learning including sensor placement and data subset selection, which require maximizing a certain submodular function (like coverage or diversity) while simultaneously minimizing another (like cooperative cost). These problems are often posed as minimizing the difference between submodular functions <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b24">25]</ref> which is in the worst case inapproximable. We show, however, that by phrasing these problems as constrained optimization, which is more natural for many applications, we achieve a number of bounded approximation guarantees. We also show that both these problems are closely related and an approximation algorithm solving one can be used to obtain an approximation guarantee for the other. We provide hardness results for both problems thus showing that our approximation factors are tight up to log-factors. Finally, we empirically demonstrate the performance and good scalability properties of our algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A set function f : 2 V → R is said to be submodular <ref type="bibr" target="#b3">[4]</ref> if for all subsets S, T ⊆ V , it holds that f (S) + f (T ) ≥ f (S ∪ T ) + f (S ∩ T ). Defining f (j|S) f (S ∪ j) -f (S) as the gain of j ∈ V in the context of S ⊆ V , then f is submodular if and only if f (j|S) ≥ f (j|T ) for all S ⊆ T and j / ∈ T . The function f is monotone iff f (j|S) ≥ 0, ∀j / ∈ S, S ⊆ V . For convenience, we assume the ground set is V = {1, 2, • • • , n}. While general set function optimization is often intractable, many forms of submodular function optimization can be solved near optimally or even optimally in certain cases. Submodularity, moreover, is inherent in a large class of real-world applications, particularly in machine learning, therefore making them extremely useful in practice.</p><p>In this paper, we study a new class of discrete optimization problems that have the following form: Problem 1 (SCSC): min{f (X) | g(X) ≥ c}, and Problem 2 (SCSK): max{g(X) | f (X) ≤ b}, where f and g are monotone non-decreasing submodular functions that also, w.l.o.g., are normalized (f (∅) = g(∅) = 0) <ref type="foot" target="#foot_0">1</ref> , and where b and c refer to budget and cover parameters respectively. The corresponding constraints are called the submodular cover <ref type="bibr" target="#b28">[29]</ref> and submodular knapsack <ref type="bibr" target="#b0">[1]</ref> respectively and hence we refer to Problem 1 as Submodular Cost Submodular Cover (henceforth SCSC) and Problem 2 as Submodular Cost Submodular Knapsack (henceforth SCSK). Our motivation stems from an interesting class of problems that require minimizing a certain submodular function f while simultaneously maximizing another submodular function g. We shall see that these naturally occur in applications like sensor placement, data subset selection, and many other machine learning applications. A standard approach used in literature <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b14">15]</ref> has been to transform these problems into minimizing the difference between submodular functions (also called DS optimization): Problem 0: min X⊆V f (X) -g(X) .</p><p>(1)</p><p>While a number of heuristics are available for solving Problem 0, in the worst-case it is NP-hard and inapproximable <ref type="bibr" target="#b8">[9]</ref>, even when f and g are monotone. Although an exact branch and bound algorithm has been provided for this problem <ref type="bibr" target="#b14">[15]</ref>, its complexity can be exponential in the worst case.</p><p>On the other hand, in many applications, one of the submodular functions naturally serves as part of a constraint. For example, we might have a budget on a cooperative cost, in which case Problems 1 and 2 become applicable. The utility of Problems 1 and 2 become apparent when we consider how they occur in real-world applications and how they subsume a number of important optimization problems.</p><p>Sensor Placement and Feature Selection: Often, the problem of choosing sensor locations can be modeled <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b8">9]</ref> by maximizing the mutual information between the chosen variables A and the unchosen set V \A (i.e.,f (A) = I(X A ; X V \A )). Alternatively, we may wish to maximize the mutual information between a set of chosen sensors X A and a quantity of interest C (i.e., f (A) = I(X A ; C)) assuming that the set of features X A are conditionally independent given C <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b8">9]</ref>. Both these functions are submodular. Since there are costs involved, we want to simultaneously minimize the cost g(A). Often this cost is submodular <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b8">9]</ref>. For example, there is typically a discount when purchasing sensors in bulk (economies of scale). This then becomes a form of either Problem 1 or 2.</p><p>Data subset selection: A data subset selection problem in speech and NLP involves finding a limited vocabulary which simultaneously has a large coverage. This is particularly useful, for example in speech recognition and machine translation, where the complexity of the algorithm is determined by the vocabulary size. The motivation for this problem is to find the subset of training examples which will facilitate evaluation of prototype systems <ref type="bibr" target="#b22">[23]</ref>. Often the objective functions encouraging small vocabulary subsets and large acoustic spans are submodular <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b19">20]</ref> and hence this problem can naturally be cast as an instance of Problems 1 and 2.</p><p>Privacy Preserving Communication: Given a set of random variables X 1 , • • • , X n , denote I as an information source, and P as private information that should be filtered out. Then one way of formulating the problem of choosing a information containing but privacy preserving set of random variables can be posed as instances of Problems 1 and 2, with f (A) = H(X A |I) and g(A) = H(X A |P), where H(•|•) is the conditional entropy.</p><p>Machine Translation: Another application in machine translation is to choose a subset of training data that is optimized for given test data set, a problem previously addressed with modular functions <ref type="bibr" target="#b23">[24]</ref>. Defining a submodular function with ground set over the union of training and test sample inputs V = V tr ∪ V te , we can set f : 2 Vtr → R + to f (X) = f (X|V te ), and take g(X) = |X|, and b ≈ 0 in Problem 2 to address this problem. We call this the Submodular Span problem.</p><p>Apart from the real-world applications above, both Problems 1 and 2 generalize a number of wellstudied discrete optimization problems. For example the Submodular Set Cover problem (henceforth SSC) <ref type="bibr" target="#b28">[29]</ref> occurs as a special case of Problem 1, with f being modular and g is submodular. Similarly the Submodular Cost Knapsack problem (henceforth SK) <ref type="bibr" target="#b27">[28]</ref> is a special case of problem 2 again when f is modular and g submodular. Both these problems subsume the Set Cover and Max k-Cover problems <ref type="bibr" target="#b2">[3]</ref>. When both f and g are modular, Problems 1 and 2 are called knapsack problems <ref type="bibr" target="#b15">[16]</ref>.</p><p>The following are some of our contributions. We show that Problems 1 and 2 are intimately connected, in that any approximation algorithm for either problem can be used to provide guarantees for the other problem as well. We then provide a framework of combinatorial algorithms based on optimizing, sometimes iteratively, subproblems that are easy to solve. These subproblems are obtained by computing either upper or lower bound approximations of the cost functions or constraining functions. We also show that many combinatorial algorithms like the greedy algorithm for SK <ref type="bibr" target="#b27">[28]</ref> and SSC <ref type="bibr" target="#b28">[29]</ref> also belong to this framework and provide the first constant-factor bi-criterion approximation algorithm for SSC <ref type="bibr" target="#b28">[29]</ref> and hence the general set cover problem <ref type="bibr" target="#b2">[3]</ref>. We then show how with suitable choices of approximate functions, we can obtain a number of bounded approximation guarantees and show the hardness for Problems 1 and 2, which in fact match some of our approximation guarantees. Our guarantees and hardness results depend on the curvature of the submodular functions <ref type="bibr" target="#b1">[2]</ref>. We observe a strong asymmetry in the results that the factors change polynomially based on the curvature of f but only by a constant-factor with the curvature of g, hence making the SK and SSC much easier compared to SCSK and SCSC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Main Ideas</head><p>We first introduce several key concepts used throughout the paper. This paper includes only the main results and we defer all the proofs and additional discussions to the extended version <ref type="bibr" target="#b10">[11]</ref>. Given a submodular function f , we define the total curvature, κ f as<ref type="foot" target="#foot_1">2</ref> : κ f = 1 -min j∈V f (j|V \j) f (j) <ref type="bibr" target="#b1">[2]</ref>. Intuitively, the curvature 0 ≤ κ f ≤ 1 measures the distance of f from modularity and κ f = 0 if and only if f is modular (or additive, i.e., f (X) = j∈X f (j)). A number of approximation guarantees in the context of submodular optimization have been refined via the curvature of the submodular function <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>In this paper, we shall witness the role of curvature also in determining the approximations and the hardness of problems 1 and 2.</p><p>Algorithm 1: General algorithmic framework to address both Problems 1 and 2 1:</p><formula xml:id="formula_0">for t = 1, 2, • • • , T do 2:</formula><p>Choose surrogate functions ft and ĝt for f and g respectively, tight at X t-1 .</p><p>3:</p><p>Obtain X t as the optimizer of Problem 1 or 2 with ft and ĝt instead of f and g.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4: end for</head><p>The main idea of this paper is a framework of algorithms based on choosing appropriate surrogate functions for f and g to optimize over. This framework is represented in Algorithm 1. We would like to choose surrogate functions ft and ĝt such that using them, Problems 1 and 2 become easier. If the algorithm is just single stage (not iterative), we represent the surrogates as f and ĝ. The surrogate functions we consider in this paper are in the forms of bounds (upper or lower) and approximations. </p><formula xml:id="formula_1">π Y of ∂ f (Y ) with entries h π Y (π(i)) = f (S π i ) -f (S π i-1 ). Defined as above, h π Y forms a lower bound of f , tight at Y -i.e., h π Y (X) = j∈X h π Y (j) ≤ f (X), ∀X ⊆ V and h π Y (Y ) = f (Y ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modular upper bounds:</head><p>We can also define superdifferentials ∂ f (Y ) of a submodular function <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b9">10]</ref> at Y . It is possible, moreover, to provide specific supergradients <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13]</ref> that define the following two modular upper bounds (when referring either one, we use m f X ):</p><formula xml:id="formula_2">m f X,1 (Y ) f (X) - j∈X\Y f (j|X\j) + j∈Y \X f (j|∅), m f X,2 (Y ) f (X) - j∈X\Y f (j|V \j) + j∈Y \X f (j|X). Then m f X,1 (Y ) ≥ f (Y ) and m f X,2 (Y ) ≥ f (Y ), ∀Y ⊆ V and m f X,1 (X) = m f X,2 (X) = f (X)</formula><p>. MM algorithms using upper/lower bounds: Using the modular upper and lower bounds above in Algorithm 1, provide a class of Majorization-Minimization (MM) algorithms, akin to the algorithms proposed in <ref type="bibr" target="#b12">[13]</ref> for submodular optimization and in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b8">9]</ref> for DS optimization (Problem 0 above). An appropriate choice of the bounds ensures that the algorithm always improves the objective values for Problems 1 and 2. In particular, choosing ft as a modular upper bound of f tight at X t , or ĝt as a modular lower bound of g tight at X t , or both, ensures that the objective value of Problems 1 and 2 always improves at every iteration as long as the corresponding surrogate problem can be solved exactly. Unfortunately, Problems 1 and 2 are NP-hard even if f or g (or both) are modular <ref type="bibr" target="#b2">[3]</ref>, and therefore the surrogate problems themselves cannot be solved exactly. Fortunately, the surrogate problems are often much easier than the original ones and can admit log or constant-factor guarantees. In practice, moreover, these factors are almost 1. Furthermore, with a simple modification of the iterative procedure of Algorithm 1, we can guarantee improvement at every iteration <ref type="bibr" target="#b10">[11]</ref>. What is also fortunate and perhaps surprising, as we show in this paper below, is that unlike the case of DS optimization (where the problem is inapproximable in general <ref type="bibr" target="#b8">[9]</ref>), the constrained forms of optimization (Problems 1 and 2) do have approximation guarantees.</p><p>Ellipsoidal Approximation: We also consider ellipsoidal approximations (EA) of f . The main result of Goemans et. al <ref type="bibr" target="#b5">[6]</ref> is to provide an algorithm based on approximating the submodular polyhedron by an ellipsoid. They show that for any polymatroid function f , one can compute an approximation of the form w f (X) for a certain modular weight vector</p><formula xml:id="formula_3">w f ∈ R V , such that w f (X) ≤ f (X) ≤ O( √ n log n) w f (X), ∀X ⊆ V .</formula><p>A simple trick then provides a curvature-dependent approximation <ref type="bibr" target="#b11">[12]</ref> -we define the κ f -curve-normalized version of f as follows: <ref type="bibr" target="#b11">[12]</ref>:</p><formula xml:id="formula_4">f κ (X) f (X) -(1 -κ f ) j∈X f (j) /κ f . Then, the submodular function f ea (X) = κ f w f κ (X) + (1 -κ f ) j∈X f (j) satisfies</formula><formula xml:id="formula_5">f ea (X) ≤ f (X) ≤ O √ n log n 1 + ( √ n log n -1)(1 -κ f ) f ea (X), ∀X ⊆ V (<label>2</label></formula><formula xml:id="formula_6">)</formula><p>f ea is multiplicatively bounded by f by a factor depending on √ n and the curvature. We shall use the result above in providing approximation bounds for Problems 1 and 2. In particular, the surrogate functions f or ĝ in Algorithm 1 can be the ellipsoidal approximations above, and the multiplicative bounds transform into approximation guarantees for these problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Relation between SCSC and SCSK</head><p>In this section, we show a precise relationship between Problems 1 and 2. From the formulation of Problems 1 and 2, it is clear that these problems are duals of each other. Indeed, in this section we show that the problems are polynomially transformable into each other. Algorithm 2: Approx. algorithm for SCSK using an approximation algorithm for SCSC. Xc ← [σ, ρ] approx. for SCSC using c. Xb ← [ρ, σ] approx. for SCSK using b.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7: end while</head><p>We first introduce the notion of bicriteria algorithms. An algorithm is a [σ, ρ] bi-criterion algorithm for Problem 1 if it is guaranteed to obtain a set X such that f (X) ≤ σf (X * ) (approximate optimality) and g(X) ≥ c = ρc (approximate feasibility), where X * is an optimizer of Problem 1. Similarly, an algorithm is a [ρ, σ] bi-criterion algorithm for Problem 2 if it is guaranteed to obtain a set X such that g(X) ≥ ρg(X * ) and f (X) ≤ b = σb, where X * is the optimizer of Problem 2. In a bi-criterion algorithm for Problems 1 and 2, typically σ ≥ 1 and ρ ≤ 1. A non-bicriterion algorithm for Problem 1 is when ρ = 1 and a non-bicriterion algorithm for Problem 2 is when σ = 1. Algorithms 2 and 3 provide the schematics for using an approximation algorithm for one of the problems for solving the other. Theorem 3.1. Algorithm 2 is guaranteed to find a set Xc which is a [(1 -)ρ, σ] approximation of SCSK in at most log 1/(1-) [g(V )/ min j g(j)] calls to the [σ, ρ] approximate algorithm for SCSC. Similarly, Algorithm 3 is guaranteed to find a set Xb which is a [(1 + )σ, ρ] approximation of SCSC in log 1+ [f (V )/ min j f (j)] calls to a [ρ, σ] approximate algorithm for SCSK. Theorem 3.1 implies that the complexity of Problems 1 and 2 are identical, and a solution to one of them provides a solution to the other. Furthermore, as expected, the hardness of Problems 1 and 2 are also almost identical. When f and g are polymatroid functions, moreover, we can provide bounded approximation guarantees for both problems, as shown in the next section. Alternatively we can also do a binary search instead of a linear search to transform Problems 1 and 2. This essentially turns the factor of O(1/ ) into O(log 1/ ). Due to lack of space, we defer this discussion to the extended version <ref type="bibr" target="#b10">[11]</ref>.</p><p>From the above, it is clear that K g ≤ n. Notice also that H g is essentially a log-factor. We also see an interesting effect of the curvature κ f of f . When f is modular (κ f = 0), we recover the approximation guarantee of the submodular set cover problem. Similarly, when f has restricted curvature, the guarantees can be much better. Moreover, the approximation guarantee already holds after the first iteration, so additional iterations can only further improve the objective.</p><p>Ellipsoidal Approximation based Submodular Set Cover (EASSC): In this setting, we use the ellipsoidal approximation discussed in §2. We can compute the κ f -curve-normalized version of f (f κ , see §2), and then compute its ellipsoidal approximation √ w f κ . We then define the function f (X) = f ea (X) = κ f w f κ (X) + (1 -κ f ) j∈X f (j) and use this as the surrogate function f for f . We choose ĝ as g itself. The surrogate problem becomes:</p><formula xml:id="formula_7">min    κ f w f κ (X) + (1 -κ f ) j∈X f (j) g(X) ≥ c    .<label>(4)</label></formula><p>While function f (X) = f ea (X) is not modular, it is a weighted sum of a concave over modular function and a modular function. Fortunately, we can use the result from <ref type="bibr" target="#b25">[26]</ref>, where they show that any function of the form of w 1 (X) + w 2 (X) can be optimized over any polytope P with an approximation factor of β(1 + ) for any &gt; 0, where β is the approximation factor of optimizing a modular function over P. The complexity of this algorithm is polynomial in n and 1 . We use their algorithm to minimize f ea (X) over the submodular set cover constraint and hence we call this algorithm EASSC.</p><p>Theorem 4.4. EASSC obtains a guarantee of O(</p><formula xml:id="formula_8">√ n log nHg 1+( √ n log n-1)(1-κ f ) )</formula><p>, where H g is the approximation guarantee of the set cover problem.</p><p>If the function f has κ f = 1, we can use a much simpler algorithm. In particular, we can minimize (f ea (X)) 2 = w f (X) at every iteration, giving a surrogate problem of the form min{w f (X)|g(X) ≥ c}. This is directly an instance of SSC, and in contrast to EASSC, we just need to solve SSC once. We call this algorithm EASSCc. Corollary 4.5. EASSCc obtains an approximation guarantee of O( √ n log n H g ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Approximation Algorithms for SCSK</head><p>In this section, we describe our approximation algorithms for SCSK. We note the dual nature of the algorithms in this current section to those given in §4.1. We first investigate a special case, the submodular knapsack (SK), and then provide three algorithms, two of them (Gr and ISK) being practical with slightly weaker theoretical guarantee, and another one (EASK) which is not scalable but has the tightest guarantee.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Submodular Cost Knapsack (SK):</head><p>We start with a special case of SCSK (Problem 2), where f is a modular function and g is a submodular function. In this case, SCSK turns into the SK problem for which the greedy algorithm with partial enumeration provides a 1 -e -1 approximation <ref type="bibr" target="#b27">[28]</ref>. The greedy algorithm can be seen as an instance of Algorithm 1 with ĝ being the modular lower bound of g and f being f , which is already modular. In particular, define:</p><formula xml:id="formula_9">π(i) ∈ argmax g(j|S π i-1 ) f (j) j / ∈ S π i-1 , f (S π i-1 ∪ {j}) ≤ b ,<label>(5)</label></formula><p>where the remaining elements are chosen arbitrarily. The following is an informal description of the result described formally in <ref type="bibr" target="#b10">[11]</ref>.</p><p>Lemma 4.6. Choosing the surrogate function f as f and ĝ as h π (with π defined in eqn (5)) in Algorithm 1 with appropriate initialization obtains a guarantee of 1 -1/e for SK.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Greedy (Gr):</head><p>A similar greedy algorithm can provide approximation guarantees for the general SCSK problem, with submodular f and g. Unlike the knapsack case in (5), however, at iteration i we choose an element j / ∈ S i-1 : f (S π i-1 ∪ {j}) ≤ b which maximizes g(j|S i-1 ). In terms of Algorithm 1, this is analogous to choosing a permutation, π such that:</p><formula xml:id="formula_10">π(i) ∈ argmax{g(j|S π i-1 )|j / ∈ S π i-1 , f (S π i-1 ∪ {j}) ≤ b}.<label>(6)</label></formula><p>Theorem 4.7. The greedy algorithm for SCSK obtains an approx. factor of 1 κg (1 -(</p><formula xml:id="formula_11">K f -κg K f ) k f ) ≥ 1 K f , where K f = max{|X| : f (X) ≤ b} and k f = min{|X| : f (X) ≤ b &amp; ∀j ∈ X, f (X ∪j) &gt; b}.</formula><p>In the worst case, k f = 1 and K f = n, in which case the guarantee is 1/n. The bound above follows from a simple observation that the constraint {f (X) ≤ b} is down-monotone for a monotone function f . However, in this variant, we do not use any specific information about f . In particular it holds for maximizing a submodular function g over any down monotone constraint <ref type="bibr" target="#b1">[2]</ref>. Hence it is conceivable that an algorithm that uses both f and g to choose the next element could provide better bounds. We do not, however, currently have the analysis for this.</p><p>Iterated Submodular Cost Knapsack (ISK): Here, we choose ft (X) as a modular upper bound of f , tight at X t . Let ĝt = g. Then at every iteration, we solve max{g(X)|m f X t (X) ≤ b}, which is a submodular maximization problem subject to a knapsack constraint (SK). As mentioned above, greedy can solve this nearly optimally. We start with X 0 = ∅, choose f0 (X) = j∈X f (j) and then iteratively continue this process until convergence (note that this is an ascent algorithm). We have the following theoretical guarantee: Theorem 4.8. Algorithm ISK obtains a set X t such that g(X t ) ≥ (1-e -1 )g( X), where X is the opti-</p><formula xml:id="formula_12">mal solution of max g(X) | f (X) ≤ b(1+(K f -1)(1-κ f ) K f</formula><p>and where</p><formula xml:id="formula_13">K f = max{|X| : f (X) ≤ b}.</formula><p>It is worth pointing out that the above bound holds even after the first iteration of the algorithm. It is interesting to note the similarity between this approach and ISSC. Notice that the guarantee above is not a standard bi-criterion approximation. We show in the extended version <ref type="bibr" target="#b10">[11]</ref> that with a simple transformation, we can obtain a bicriterion guarantee.</p><p>Ellipsoidal Approximation based Submodular Cost Knapsack (EASK): Choosing the Ellipsoidal Approximation f ea of f as a surrogate function, we obtain a simpler problem:</p><formula xml:id="formula_14">max    g(X) κ f w f κ (X) + (1 -κ f ) j∈X f (j) ≤ b    .<label>(7)</label></formula><p>In order to solve this problem, we look at its dual problem (i.e., Eqn. ( <ref type="formula" target="#formula_7">4</ref>)) and use Algorithm 2 to convert the guarantees. We call this procedure EASK. We then obtain guarantees very similar to Theorem 4.4.</p><p>Lemma 4.9. EASK obtains a guarantee of 1 + , O(</p><formula xml:id="formula_15">√ n log nHg 1+( √ n log n-1)(1-κ f ) ) .</formula><p>In the case when the submodular function has a curvature κ f = 1, we can actually provide a simpler algorithm without needing to use the conversion algorithm (Algorithm 2). In this case, we can directly choose the ellipsoidal approximation of f as w f (X) and solve the surrogate problem: max{g(X) : w f (X) ≤ b 2 }. This surrogate problem is a submodular cost knapsack problem, which we can solve using the greedy algorithm. We call this algorithm EASKc. This guarantee is tight up to log factors if κ f = 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Extensions beyond SCSC and SCSK</head><p>SCSC and SCSK can in fact be extended to more flexible and complicated constraints which can arise naturally in many applications <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b7">8]</ref>. These include multiple covering and knapsack constraintsi.e., min{f (X</p><formula xml:id="formula_16">)|g i (X) ≥ c i , i = 1, 2, • • • k} and max{g(X)|f i (X) ≤ b i , i = 1, 2, • • • k},</formula><p>and robust optimization problems like max{min i g i (X)|f (X) ≤ b}, where the functions f, g, f i 's and g i 's are submodular. We also consider SCSC and SCSK with non-monotone submodular functions. Due to lack of space, we defer these discussions to the extended version of this paper <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Hardness</head><p>In this section, we provide the hardness for Problems 1 and 2. The lower bounds serve to show that the approximation factors above are almost tight.</p><p>Theorem 4.11. For any κ &gt; 0, there exists submodular functions with curvature κ such that no polynomial time algorithm for Problems 1 and 2 achieves a bi-criterion factor better than</p><formula xml:id="formula_17">σ ρ = n 1/2- 1+(n 1/2--1)(1-κ) for any &gt; 0.</formula><p>The above result shows that EASSC and EASK meet the bounds above to log factors. We see an interesting curvature-dependent influence on the hardness. We also see this phenomenon in the approximation guarantees of our algorithms. In particular, as soon as f becomes modular, the problem becomes easy, even when g is submodular. This is not surprising since the submodular set cover problem and the submodular cost knapsack problem both have constant factor guarantees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we empirically compare the performance of the various algorithms discussed in this paper. We are motivated by the speech data subset selection application <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">23]</ref> with the submodular function f encouraging limited vocabulary while g tries to achieve acoustic variability. A natural choice of the function f is a function of the form |Γ(X)|, where Γ(X) is the neighborhood function on a bipartite graph constructed between the utterances and the words <ref type="bibr" target="#b22">[23]</ref>. For the coverage function g, we use two types of coverage: one is a facility location function g 1 (X) = i∈V max j∈X s ij while the other is a saturated sum function g 2 (X) = i∈V min{ j∈X s ij , α j∈V s ij }. Both these functions are defined in terms of a similarity matrix S = {s ij } i,j∈V , which we define on the TIMIT corpus <ref type="bibr" target="#b4">[5]</ref>, using the string kernel metric <ref type="bibr" target="#b26">[27]</ref> for similarity. Since some of our algorithms, like the Ellipsoidal Approximations, are computationally intensive, we restrict ourselves to 50 utterances.  We compare our different algorithms on Problems 1 and 2 with f being the bipartite neighborhood and g being the facility location and saturated sum respectively. Furthermore, in our experiments, we observe that the neighborhood function f has a curvature κ f = 1. Thus, it suffices to use the simpler versions of algorithm EA (i.e., algorithm EASSCc and EASKc). The results are shown in Figure <ref type="figure" target="#fig_1">1</ref>. We observe that on the real-world instances, all our algorithms perform almost comparably. This implies, moreover, that the iterative variants, viz. Gr, ISSC and ISK, perform comparably to the more complicated EA-based ones, although EASSC and EASK have better theoretical guarantees. We also compare against a baseline of selecting random sets (of varying cardinality), and we see that our algorithms all perform much better. In terms of the running time, computing the Ellipsoidal Approximation for |Γ(X)| with |V | = 50 takes about 5 hours while all the iterative variants (i.e., Gr, ISSC and ISK) take less than a second. This difference is much more prominent on larger instances (for example |V | = 500).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussions</head><p>In this paper, we propose a unifying framework for problems 1 and 2 based on suitable surrogate functions. We provide a number of iterative algorithms which are very practical and scalable (like Gr, ISK and ISSC), and also algorithms like EASSC and EASK, which though more intensive, obtain tight approximation bounds. Finally, we empirically compare our algorithms, and show that the iterative algorithms compete empirically with the more complicated and theoretically better approximation algorithms. For future work, we would like to empirically evaluate our algorithms on many of the real world problems described above, particularly the limited vocabulary data subset selection application for speech corpora, and the machine translation application.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Modular lower bounds: Akin to convex functions, submodular functions have tight modular lower bounds. These bounds are related to the subdifferential ∂ f (Y ) of the submodular set function f at a set Y ⊆ V [4]. Denote a subgradient at Y by h Y ∈ ∂ f (Y ). The extreme points of ∂ f (Y ) may be computed via a greedy algorithm: Let π be a permutation of V that assigns the elements in Y to the first |Y | positions (π(i) ∈ Y if and only if i ≤ |Y |). Each such permutation defines a chain with elements S π 0 = ∅, S π i = {π(1), π(2), . . . , π(i)} and S π |Y | = Y . This chain defines an extreme point h</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 :</head><label>1</label><figDesc>Input: An SCSK instance with budget b, an [σ, ρ] approx. algo. for SCSC, &amp; ∈ [0, 1). 2: Output: [(1 -)ρ, σ] approx. for SCSK. 3: c ← g(V ), Xc ← V . 4: while f ( Xc ) &gt; σb do 5: c ← (1 -)c 6:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>7 : end while Algorithm 3 : 1 :</head><label>731</label><figDesc>Approx. algorithm for SCSC using an approximation algorithm for SCSK. Input: An SCSC instance with cover c, an [ρ, σ] approx. algo. for SCSK, &amp; &gt; 0. 2: Output: [(1 + )σ, ρ] approx. for SCSC. 3: b ← argmin j f (j), Xb ← ∅. 4: while g( Xb ) &lt; ρc do 5: b ← (1 + )b 6:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Corollary 4 . 10 .</head><label>410</label><figDesc>Algorithm EASKc obtains a bi-criterion guarantee of [1 -e -1 , O( √ n log n)].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparison of the algorithms in the text.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>A monotone non-decreasing normalized (f (∅) = 0) submodular function is called a polymatroid function.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We can assume, w.l.o.g that f (j) &gt; 0, g(j) &gt; 0, ∀j ∈ V</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: Special thanks to Kai Wei and Stefanie Jegelka for discussions, to Bethany Herwaldt for going through an early draft of this manuscript and to the anonymous reviewers for useful reviews. This material is based upon work supported by the National Science Foundation under Grant No. (IIS-1162606), a Google and a Microsoft award, and by the Intel Science and Technology Center for Pervasive Computing.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Approximation Algorithms</head><p>We consider several algorithms for Problems 1 and 2, which can all be characterized by the framework of Algorithm 1, using the surrogate functions of the form of upper/lower bounds or approximations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Approximation Algorithms for SCSC</head><p>We first describe our approximation algorithms designed specifically for SCSC, leaving to §4.2 the presentation of our algorithms slated for SCSK. We first investigate a special case, the submodular set cover (SSC), and then provide two algorithms, one of them (ISSC) is very practical with a weaker theoretical guarantee, and another one (EASSC) which is slow but has the tightest guarantee.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Submodular Set Cover (SSC):</head><p>We start by considering a classical special case of SCSC (Problem 1) where f is already a modular function and g is a submodular function. This problem occurs naturally in a number of problems related to active/online learning <ref type="bibr" target="#b6">[7]</ref> and summarization <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. This problem was first investigated by Wolsey <ref type="bibr" target="#b28">[29]</ref>, wherein he showed that a simple greedy algorithm achieves bounded (in fact, log-factor) approximation guarantees. We show that this greedy algorithm can naturally be viewed in the framework of our Algorithm 1 by choosing appropriate surrogate functions ft and ĝt . The idea is to use the modular function f as its own surrogate ft and choose the function ĝt as a modular lower bound of g. Akin to the framework of algorithms in <ref type="bibr" target="#b12">[13]</ref>, the crucial factor is the choice of the lower bound (or subgradient). Define the greedy subgradient as:</p><p>Once we reach an i where the constraint g(S π i-1 ∪ j) &lt; c can no longer be satisfied by any j / ∈ S π i-1 , we choose the remaining elements for π arbitrarily. Let the corresponding subgradient be referred to as h π . Then we have the following lemma, which is an extension of <ref type="bibr" target="#b28">[29]</ref>, and which is a simpler description of the result stated formally in <ref type="bibr" target="#b10">[11]</ref>. Lemma 4.1. The greedy algorithm for SSC <ref type="bibr" target="#b28">[29]</ref> can be seen as an instance of Algorithm 1 by choosing the surrogate function f as f and ĝ as h π (with π defined in Eqn. (3)).</p><p>When g is integral, the guarantee of the greedy algorithm is H g H(max j g(j)), where <ref type="bibr" target="#b28">[29]</ref> (henceforth we will use H g for this quantity). This factor is tight up to lowerorder terms <ref type="bibr" target="#b2">[3]</ref>. Furthermore, since this algorithm directly solves SSC, we call it the primal greedy. We could also solve SSC by looking at its dual, which is SK <ref type="bibr" target="#b27">[28]</ref>. Although SSC does not admit any constant-factor approximation algorithms <ref type="bibr" target="#b2">[3]</ref>, we can obtain a constant-factor bi-criterion guarantee: Lemma 4.2. Using the greedy algorithm for SK <ref type="bibr" target="#b27">[28]</ref> as the approximation oracle in Algorithm 3 provides a [1 + , 1 -e -1 ] bi-criterion approximation algorithm for SSC, for any &gt; 0.</p><p>We call this the dual greedy. This result follows immediately from the guarantee of the submodular cost knapsack problem <ref type="bibr" target="#b27">[28]</ref> and Theorem 3.1. We remark that we can also use a simpler version of the greedy iteration at every iteration <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b16">17]</ref> and we obtain a guarantee of (1 + , 1/2(1 -e -1 )). In practice, however, both these factors are almost 1 and hence the simple variant of the greedy algorithm suffices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Iterated Submodular Set Cover (ISSC):</head><p>We next investigate an algorithm for the general SCSC problem when both f and g are submodular. The idea here is to iteratively solve the submodular set cover problem which can be done by replacing f by a modular upper bound at every iteration. In particular, this can be seen as a variant of Algorithm 1, where we start with X 0 = ∅ and choose ft (X) = m f X t (X) at every iteration. The surrogate problem at each iteration becomes min{m f X t (X)|g(X) ≥ c}. Hence, each iteration is an instance of SSC and can be solved nearly optimally using the greedy algorithm. We can continue this algorithm for T iterations or until convergence. An analysis very similar to the ones in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref> will reveal polynomial time convergence. Since each iteration is only the greedy algorithm, this approach is also highly practical and scalable. Theorem 4.3. ISSC obtains an approximation factor of KgHg 1+(Kg-1)(1-κ f ) ≤ n 1+(n-1)(1-κ f ) H g where K g = 1 + max{|X| : g(X) &lt; c} and H g is the approximation factor of the submodular set cover using g.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The submodular knapsack polytope</title>
		<author>
			<persName><forename type="first">A</forename><surname>Atamtürk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete Optimization</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Submodular set functions, matroids and the greedy algorithm: tight worstcase bounds and some generalizations of the Rado-Edmonds theorem</title>
		<author>
			<persName><forename type="first">M</forename><surname>Conforti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cornuejols</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="251" to="274" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A threshold of ln n for approximating set cover</title>
		<author>
			<persName><forename type="first">U</forename><surname>Feige</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Submodular functions and optimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fujishige</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Elsevier Science</publisher>
			<biblScope unit="volume">58</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Timit, acoustic-phonetic continuous speech corpus</title>
		<author>
			<persName><forename type="first">J</forename><surname>Garofolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Fiscus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pallet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dahlgren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DARPA</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Approximating submodular functions everywhere</title>
		<author>
			<persName><forename type="first">M</forename><surname>Goemans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Iwata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mirrokni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SODA</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="535" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Interactive submodular set cover</title>
		<author>
			<persName><forename type="first">A</forename><surname>Guillory</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Simultaneous learning and covering with adversarial noise</title>
		<author>
			<persName><forename type="first">A</forename><surname>Guillory</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Algorithms for approximate minimization of the difference between submodular functions, with applications</title>
		<author>
			<persName><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The submodular Bregman and Lovász-Bregman divergences with applications</title>
		<author>
			<persName><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Submodular Optimization with Submodular Cover and Submodular Knapsack Constraints: Extended arxiv version</title>
		<author>
			<persName><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Curvature and Optimal Algorithms for Learning and Minimizing Submodular Functions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast semidifferential based submodular function optimization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Submodularity beyond submodular energies: coupling edges in graph cuts</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Prismatic algorithm for discrete dc programming problems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Washio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">H</forename><surname>Kellerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Pferschy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pisinger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Springer Verlag</publisher>
		</imprint>
	</monogr>
	<note>Knapsack problems</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A note on the budgeted maximization on submodular functions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<idno>CMU-CALD-05-103</idno>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Robust submodular observation selection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research (JMLR)</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2761" to="2801" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Near-optimal sensor placements in Gaussian processes: Theory, efficient algorithms and empirical studies</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="235" to="284" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">How to select a good training-data subset for transcription: Submodular active selection for sequences</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-document summarization via budgeted maximization of submodular functions</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A class of submodular functions for document summarization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL/HLT-2011)</title>
		<meeting><address><addrLine>Portland, OR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06">June 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Optimal selection of limited vocabulary speech corpora</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Intelligent selection of language model training data</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2010 Conference Short Papers</title>
		<meeting>the ACL 2010 Conference Short Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="220" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A submodular-supermodular procedure with applications to discriminative structure learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Approximation algorithms for offline risk-averse combinatorial optimization</title>
		<author>
			<persName><forename type="first">E</forename><surname>Nikolova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient computation of gapped substring kernels on large alphabets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rousu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1323</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A note on maximizing a submodular set function subject to a knapsack constraint</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sviridenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research Letters</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="43" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An analysis of the greedy algorithm for the submodular set covering problem</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Wolsey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combinatorica</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="385" to="393" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
