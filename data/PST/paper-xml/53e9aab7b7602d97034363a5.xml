<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Filtering: A Method for Solving Graph Problems in MapReduce</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Silvio</forename><surname>Lattanzi</surname></persName>
							<email>silviolat@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Benjamin</forename><surname>Moseley</surname></persName>
							<email>bmosele2@illinois.edu</email>
						</author>
						<author>
							<persName><forename type="first">Siddharth</forename><surname>Suri</surname></persName>
							<email>ssuri@yahoo-inc.com</email>
						</author>
						<author>
							<persName><forename type="first">Sergei</forename><surname>Vassilvitskii</surname></persName>
							<email>sergei@yahoo-inc.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Google, Inc</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois Urbana</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Yahoo! Research</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Yahoo! Research</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">SPAA&apos;11</orgName>
								<address>
									<addrLine>June 4-6</addrLine>
									<postCode>2011</postCode>
									<settlement>San Jose</settlement>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Filtering: A Method for Solving Graph Problems in MapReduce</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4D45CA798F92EA24C50B25E5F6F28047</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>F.2.2 [Analysis of Algorithms and Problem Complexity]: Nonnumerical Algorithms and Problems Algorithms</term>
					<term>Theory MapReduce</term>
					<term>Graph Algorithms</term>
					<term>Matchings</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The MapReduce framework is currently the de facto standard used throughout both industry and academia for petabyte scale data analysis. As the input to a typical MapReduce computation is large, one of the key requirements of the framework is that the input cannot be stored on a single machine and must be processed in parallel. In this paper we describe a general algorithmic design technique in the MapReduce framework called filtering. The main idea behind filtering is to reduce the size of the input in a distributed fashion so that the resulting, much smaller, problem instance can be solved on a single machine. Using this approach we give new algorithms in the MapReduce framework for a variety of fundamental graph problems for sufficiently dense graphs. Specifically, we present algorithms for minimum spanning trees, maximal matchings, approximate weighted matchings, approximate vertex and edge covers and minimum cuts. In all of these cases, we parameterize our algorithms by the amount of memory available on the machines allowing us to show tradeoffs between the memory available and the number of MapReduce rounds. For each setting we will show that even if the machines are only given substantially sublinear memory, our algorithms run in a constant number of MapReduce rounds. To demonstrate the practical viability of our algorithms we implement the maximal matching algorithm that lies at the core of our analysis and show that it achieves a significant speedup over the sequential version.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The amount of data available and requiring analysis has grown at an astonishing rate in recent years. For example, Yahoo! processes over 100 billion events, amounting to over 120 terabytes, daily <ref type="bibr" target="#b7">[7]</ref>. Similarly, Facebook processes over 80 terabytes of data per day <ref type="bibr" target="#b17">[17]</ref>. Although the amount of memory in commercially available servers has also grown at a remarkable pace in the past decade, and now exceeds a once unthinkable amount of 100 GB, it remains woefully inadequate to process such huge amounts of data. To cope with this deluge of information people have (again) turned to parallel algorithms for data processing. In recent years MapReduce <ref type="bibr" target="#b3">[3]</ref>, and its open source implementation, Hadoop <ref type="bibr" target="#b20">[20]</ref>, have emerged as the standard platform for large scale distributed computation. About 5 years ago, Google reported that it processes over 3 petabytes of data using MapReduce in one month <ref type="bibr" target="#b3">[3]</ref>. Yahoo! and Facebook use Hadoop as their primary method for analyzing massive data sets <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b17">17]</ref>. Moreover, over 100 companies and 10 universities are using Hadoop <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b21">21]</ref> for large scale data analysis.</p><p>Many different types of data have contributed to this growth. One particularly rich datatype that has captured the interest of both industry and academia is massive graphs. Graphs such as the World Wide Web can easily consist of billions of nodes and trillions of edges <ref type="bibr" target="#b16">[16]</ref>. Citation graphs, affiliation graphs, instant messenger graphs, and phone call graphs have recently been studied as part of social network analysis. Although it was previously thought that graphs of this nature are sparse, the work of Leskovec, Kleinberg and Faloutsos <ref type="bibr" target="#b14">[14]</ref> dispelled this notion. The authors analyzed the growth over time of 9 different massive graphs from 4 different domains and showed that graphs become denser. Specifically, if n(t) and e(t) denote the number of nodes and edges at time t, respectively, they show that e(t) ∝ n(t) 1+c , where 1 ≥ c &gt; 0. They lowest value of c they find is 0.08, but they observe three graphs with c &gt; 0.5. The algorithms we present are efficient for such dense graphs, as well as their sparser counterparts.</p><p>Previous approaches to graph algorithms on MapReduce attempt to shoehorn message passing style algorithms into the framework <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b15">15]</ref>. These algorithms often require O(d) rounds, where d is the diameter of the input graph, even for such simple tasks as computing connected components, minimum spanning trees, etc. A round in a MapReduce computation can be very expensive time-wise, because it often requires a massive amount of data (on the order of terabytes) to be transmitted from one set of machines to another. This is usually the dominant cost in a MapReduce computation. Therefore minimizing the number of rounds is essential for efficient MapReduce computations. In this work we show how many fundamental graph algorithms can be computed in a constant number of rounds. We use the previously defined model of computation for MapReduce <ref type="bibr" target="#b13">[13]</ref> to perform our analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Contributions</head><p>All of our algorithms take the same general approach, which we call filtering. They proceed in two stages. First, the algorithms use the parallelization of MapReduce to selectively drop, or filter, parts of the input with the goal of reducing the problem size so that the result is small enough to fit into a single machine's memory. In the second stage the algorithms compute the final answer on this reduced input. The technical challenge is to choose enough edges to drop but still be able to compute either an optimal or provably near optimal solution. The filtering step differs in complexity depending on the problem and takes a few slightly different forms. We exhibit the flexibility of this approach by showing how it can be used to solve a variety of graph problems.</p><p>In Section 2.4 we apply the filtering technique to computing the connected components and minimum spanning trees of dense graphs. The algorithm, which is much simpler, and more efficient algorithm than the one that appeared in <ref type="bibr" target="#b13">[13]</ref>, partitions the original input and solves a subproblem on each partition. The algorithm recurses until the data set is small enough to fit into the memory of a single machine.</p><p>In Section 3, we turn to the problem of matchings, and show how to compute a maximal matching in three MapReduce rounds in the model of <ref type="bibr" target="#b13">[13]</ref>. The algorithm begins by solving a subproblem on a small sample of the original input. We then use this interim solution to prune out the vast majority of edges of the original input, thus dramatically reducing the size of the remaining problem, and recursing if it is not small enough to fit onto a single machine. The algorithm allows for a tradeoff between the number of rounds and the available memory. Specifically, for graphs with at most n 1+c edges and machines with memory at least n 1+ our algorithm will require O(c/ ) rounds. If the machines have memory O(n) then our algorithm requires O(log n) rounds.</p><p>We then use this algorithm as a building block, and show algorithms for computing an 8-approximation for maximum weighted matching, a 2-approximation to Vertex Cover and a 3 /2approximation to Edge Cover. For all of these algorithms, the number of machines used will be at most O(N/η) where N is the size of the input and η is the memory available on each machine. That is, these algorithms require just enough machines to fit the entire input on all of the machines. Finally, in Section 4 we adapt the seminal work of Karger <ref type="bibr" target="#b10">[10]</ref> to the MapReduce setting. Here the filtering succeeds with a limited probability; however, we argue that we can replicate the algorithm enough times in parallel so that one of the runs succeeds without destroying the minimum cut.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Related Work</head><p>The authors of <ref type="bibr" target="#b13">[13]</ref> give a formal model of computation of MapReduce called MRC which we will briefly summarize in the next section. There are two models of computation that are similar to MRC. We describe these models and their relationship to MRC in turn. We also discuss how known algorithms in those models relate to the algorithms presented in this work.</p><p>The algorithms presented in this paper run in a constant number of rounds when the memory per machine is superlinear in the number of vertices (n 1+ for some &gt; 0). Although this requirement is reminiscent of the semi-streaming model <ref type="bibr" target="#b4">[4]</ref>, the similarities end there, as the two models are very different. One problem that is hard to solve in semi-streaming but can be solved in MRC is graph connectivity. As shown in <ref type="bibr" target="#b4">[4]</ref>, in the semi-streaming model, without a superlinear amount of memory it is impossible to answer connectivity queries. In MRC, however, previous work <ref type="bibr" target="#b13">[13]</ref> shows how to answer connectivity queries when the memory per machine is limited to n<ref type="foot" target="#foot_0">1</ref>-, albeit at the cost of a logarithmic number of rounds. Conversely, a problem that is trivial in the semi-streaming model but more complicated in MRC is finding a maximal matching. In the semi-streaming model one simply streams through the edges, and adds the edge to the current matching if it is feasible. As we show in Section 3, finding a maximal matching in MRC is a computable, but non-trivial endeavor. The technical challenge for this algorithm stems from the fact that no single machine can see all of the edges of the input graph, rather the model requires the algorithm designer to parallelize the processing 1 .</p><p>Although parallel algorithms are gaining a resurgence, this is an area that was widely studied previously under different models of parallel computation. The most popular model is the PRAM model, which allows for a polynomial number of processors with shared memory. There are hundreds of papers for solving problems in this model and previous work <ref type="bibr">[5,</ref><ref type="bibr" target="#b13">13]</ref> shows how to simulate certain types of PRAM algorithms in MRC. Most of these results yield MRC algorithms that require Ω(log n) rounds, whereas in this work we focus on algorithms that use O(1) rounds. Nonetheless, to compare with previous work, we next describe PRAM algorithms that either can be simulated in MRC, or could be directly implemented in MRC. Israel and Itai <ref type="bibr" target="#b8">[8]</ref> give an O(log n) round algorithm for computing maximal matchings on a PRAM. It could be implemented in MRC, but would require O(log n) rounds. Similarly, <ref type="bibr" target="#b19">[19]</ref> gives a distributed algorithm which yields constant factor approximation to the weighted matching problem. This algorithm, which could also be implemented in MRC, takes O(log 2 n) rounds. Finally, Karger's algorithm is in RN C but also requires O(log 2 n) rounds. We show how to implement it in MapReduce in a constant number of rounds in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PRELIMINARIES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">MapReduce Overview</head><p>We remind the reader about the salient features of the MapReduce computing paradigm (see <ref type="bibr" target="#b13">[13]</ref> for more details). The input, and all intermediate data, is stored in key; value pairs and the computation proceeds in rounds. Each round is split into three consecutive phases: map, shuffle and reduce. In the map phase the input is processed one tuple at a time. All key; value pairs emitted by the map phase which have the same key are then aggregated by the MapReduce system during the shuffle phase and sent to the same machine. Finally each key, along with all the values associated with it, are processed together during the reduce phase.</p><p>Since all the values with the same key end up on the same machine, one can view the map phase as a kind of routing step that determines which values end up together. The key acts as a (logi-cal) address of the machine, and the system makes sure all of the key; value pairs with the same key are collected on the same machine. To simplify our reasoning about the model, we can combine the reduce and the subsequent map phases. Looking at the computation through this lens, every round each machine performs some computation on the set of key; value pairs assigned to it (reduce phase), and then designates which machine each output value should be sent to in the next round (map phase). The shuffle ensures that the data is moved to the right machine, after which the next round of computation can begin. In this simpler model, we shall only use the term machines as opposed to mappers and reducers.</p><p>More formally, let ρj denote the reduce function for round j, and let µj+1 denote the map function for the following round of an MRC algorithm <ref type="bibr" target="#b13">[13]</ref> where j ≥ 1. Now let φj(x) = µj+1 ρj(x). Here ρj takes as input some set of key; value pairs denoted by x and outputs another set of key; value pairs. We define the operator to feed the output of ρj(x) to µj+1 one key; value pair at a time. Thus φj denotes the operation of first executing the reducer function, ρj, on the set of values in x and then executing the map function, µj+1, on each key; value pair output by ρj(x) individually. This syntactic change allows the algorithm designer to avoid defining mappers and reducers and instead define what each machine does during each round of computation and specify which machine each output key; value pair should go to.</p><p>We can now translate the restrictions on ρj and µj from the MRC model of <ref type="bibr" target="#b13">[13]</ref> to restrictions on φj. Since we are joining the reduce and the subsequent map phase, we combine the restrictions imposed on both of these computations. There are three sets of restrictions: those on the number of machines, the memory available on each machine and the total number of rounds taken by the computation. For an input of size N , and a sufficiently small &gt; 0, there are N 1-machines, each with N 1-memory available for computation. As a result, the total amount of memory available to the entire system is O(N<ref type="foot" target="#foot_1">2</ref>-2 ). See <ref type="bibr" target="#b13">[13]</ref> for a discussion and justification. An algorithm in MRC belongs to MRC i if it runs in worst case O(log i N ) rounds. Thus, when designing a MRC 0 algorithm there are three properties that need to be checked:</p><p>• Machine Memory: In each round the total memory used by a single machine is at most O(N 1-) bits.</p><p>• Total Memory: The total amount of data shuffled in any round is O(N 2-2 ) bits 2 .</p><p>• Rounds: The number of rounds is a constant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Total Work and Work Efficiency</head><p>Next we define the amount of work done by an MRC algorithm by taking the standard definition of work efficiency from the PRAM setting and adapting it to the MapReduce setting. Let w(N ) denote the amount of work done by an r-round, MRC algorithm on an input of size N . This is simply the sum of the amount of work done during each round of computation. The amount of work done during round i of a computation is the product of the number of machines used in that round, denoted pi(N ), and the worst case running time of each machine, denoted ti(N ). More specifically,</p><formula xml:id="formula_0">w(N ) = r X i=1 wi(N ) = r X i=1 pi(N )ti(N ).</formula><p>(1)</p><formula xml:id="formula_1">Algorithm: MST(V,E) 1: if |E| &lt; η then 2: Compute T * = M ST (E) 3:</formula><p>return T * 4: end if 5: ← Θ(|E|/η) 6: Partition E into E1, E2, . . . , E where |Ei| &lt; η using a universal hash function h : E → {1, 2, . . . , }. 7: In parallel: Compute Ti, the minimum spanning tree on G(V, Ei). 8: return MST(V, ∪iTi) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Notation</head><p>Let G = (V, E) be an undirected graph, and denote by n = |V | and m = |E|. We will call G, c-dense, if m = n 1+c where 0 &lt; c ≤ 1. In what follows we assume that the machines have some limited memory η. We will assume that the number of available machines is O(m/η). Notice that the number of machines is just the number required to fit the input on all of the machines simultaneously. All of our algorithms will consider the case where η = n 1+ for some &gt; 0. For a constant , the algorithms we define will take a constant number of rounds and lie in MRC 0 <ref type="bibr" target="#b13">[13]</ref>, beating the Ω(log n) running time provided by the PRAM simulation constructions (see Theorem 7.1 in <ref type="bibr" target="#b13">[13]</ref>). However, even when η = O(n) our algorithms will run in O(log n) rounds. This exposes the memory vs. rounds tradeoff since most of the algorithms presented take fewer rounds as the memory per machine increases. We now proceed to describe the individual algorithms, in order of progressively more complex filtering techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Warm Up: Connected Components and Minimum Spanning Trees</head><p>We present the formal algorithm for computing minimum spanning trees (the connected components algorithm is identical). The algorithm works by partitioning the edges of the input graph into subsets of size η and sending each subgraph to its own machine. Then, each machine throws out any edge that is guaranteed not to be a part of any MST because it is the heaviest edge on some cycle in that machine's subgraph. If the resulting graph fits into memory of a single machine, the algorithm terminates. Otherwise, the algorithm recurses on the smaller instance. We give the pseudocode in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>We assume the algorithm is given a c-dense graph; each machine has memory η = O(n 1+ ), and that the number of machines = Θ(n c-). Thus the algorithm only uses enough memory, across the entire system, to store the input. We show that every iteration reduces the input size by n c/ , and thus after c / iterations the algorithm terminates. LEMMA 2.1. Algorithm MST(V,E) terminates after c / iterations and returns the Minimum Spanning Tree. PROOF. To show correctness, note that any edge that is not part of the MST on a subgraph of G is also not part of the MST of G by the cycle property of minimum spanning trees.</p><p>It remains to show that (1) the memory constraints of each machine are never violated and (2) the total number of rounds is lim-ited. Since the partition is done randomly, an easy Chernoff argument shows that no machine gets assigned more than η edges with high probability. Finally, note that | S i Ti| ≤ (n -1) = O(n 1+c-). Therefore after c / -1 iterations the input is small enough to fit onto a single machine, and the overall algorithm terminates after c / rounds. LEMMA 2.2. The MST(V,E) algorithm does O( cm α(m, n)) total work. PROOF. During a specific iteration, randomly partitioning E into E1, E2, . . . , E requires a linear scan over the edges which is O(m) work. Computing the minimum spanning tree Mi of each part of the partition using the algorithm of <ref type="bibr" target="#b2">[2]</ref> takes O( m α(m, n)) work. Computing the MST of Gsparse on one machine using the same algorithm requires</p><formula xml:id="formula_2">(n -1)α(m, n) = O(mα(m, n)) work.</formula><p>For constant the MRC algorithm uses O(mα(m, n)) work. Since the best known sequential algorithm <ref type="bibr" target="#b11">[11]</ref> runs in time O(m) in expectation, the MRC algorithm is work efficient up to a factor of α(m, n).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MATCHINGS AND COVERS</head><p>The maximum matching problem and its variants play a central role in theoretical computer science, so it is natural to determine if is possible to efficiently compute a maximum matching, or, more simply, a maximal matching, in the MapReduce framework. The question is not trivial. Indeed, due to the constraints of the model, it is not possible to store (or even stream through) all of the edges of a graph on a single machine. Furthermore, it is easy to come up with examples where the partitioning technique similar to that used for MSTs (Section 2.4) yields an arbitrarily bad matching. Simply sampling the edges uniformly, or even using one of the sparsification approaches <ref type="bibr" target="#b18">[18]</ref> appears unfruitful because good sparsifiers do not necessarily preserve maximal matchings.</p><p>Despite these difficulties, we are able to show that by combining a simple sampling technique and a post-processing strategy it is possible to compute an unweighted maximal matching and thus a 2approximation to the unweighted maximum matching problem using only machines with memory of size O(n) and O(log n) rounds. More generally, we show that we can find a maximal matching on c-dense graphs in O( c / ) rounds using machines with Ω(n 1+ ) memory; only three rounds are necessary if = 2c /3. We extend this technique to obtain an 8-approximation algorithm for maximum weighted matching and use similar approaches to approximate the vertex and edge cover problems. This section is organized as follows: first we present the algorithm to solve the unweighted maximal matching, and then we explain how to use this algorithm to solve the weighted maximum matching problem. Finally, we show how the techniques can be adapted to solve the minimum vertex and the minimum edge cover problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Unweighted Maximal Matchings</head><p>The algorithm works by first sampling O(η) edges and finding a maximal matching M1 on the resulting subgraph. Given this matching, we can now safely remove edges that are in conflict (i.e. those incident on nodes in M1) from the original graph G. If the resulting filtered graph, H is small enough to fit onto a single machine, the algorithm augments M1 with a matching found on H. Otherwise, we augment M1 with the matching found by recursing on H. Note that since the size of the graph reduces from round to round, the effective sampling probability increases, resulting in a larger sample of the remaining graph.</p><p>Formally, let G(V, E) be a simple graph where n = |V | and |E| ≤ n 1+c for some c &gt; 0. We begin by assuming that each of the machines has at least η memory. We fix the exact value of η later, but require that η ≥ 40n. We give the pseudocode for the algorithm below:</p><p>1. Set M = ∅ and S = E.</p><p>2. Sample every edge (u, v) ∈ S uniformly at random with probability p = η 10|S| . Let E be the set of sampled edges. To proceed we need the following technical lemma, which shows that with high probability every induced subgraph with sufficiently many edges, has at least one edge in the sample. </p><formula xml:id="formula_3">were chosen to be in E is (1 -p) |E[I]| ≤ (1 -p) 2n/p ≤ e -2n .</formula><p>Since there are at most 2 n total possible induced subgraphs G[I], the probability that there exists one that does not have an edge in E is at most 2 n e -2n ≤ e -n .</p><p>Next we bound the number of iterations the algorithm takes. Note that, the term iteration refers to the number of times the algorithm is repeated. This does not refer to a MapReduce round. LEMMA 3.2. If η ≥ 40n then the algorithm runs for at most O(log n) iterations with high probability. Furthermore, if η = n 1+ , where 0 &lt; &lt; c is a fixed constant, then the algorithm runs in at most c/ iterations with high probability. PROOF. Fix an iteration i of the algorithm and let p be the sampling probability for this iteration. Let Ei be the set of edges at the beginning of this iteration, and denote by I be the set of unmatched vertices after this iteration. From Lemma 3.1, if |E[I]| ≥ 2n/p then an edge of E[I] will be sampled with high probability. Note that no edge in E[I] is incident on any edge in M . Thus, if an edge from E[I] is sampled then our algorithm would have chosen this edge to be in the matching. This contradicts the fact that no vertex in I is matched. Hence,</p><formula xml:id="formula_4">|E[I]| ≤ 2n/p ≤ 20n|E i | η with high probability.</formula><p>Now consider the first iteration of the algorithm, let G1(V1, E1) be the induced graph on the unmatched nodes after the first step of the algorithm. The above argument implies that</p><formula xml:id="formula_5">|E1| ≤ 20n|E 0 | η ≤ 20n|E| η ≤ |E| 2 . Similarly |E2| ≤ 20n|E 1 | η ≤ (20n) 2 |E 0 | η 2 ≤ |E| 2 2 . So after i iterations |Ei| ≤ |E| 2 i .</formula><p>The first part of the claim follows. To conclude the proof note that if η = n 1+ , we have that |Ei| ≤ |E| n i , and thus the algorithm terminates after c/ iterations. We continue by showing the correctness of the algorithm. THEOREM 3.1. The algorithm finds a maximal matching of G = (V, E) with high probability.</p><p>PROOF. First consider the case that the algorithm does not fail. Assume, for the sake of contradiction, that there exists an edge (u, v) ∈ E such that neither u nor v are matched in the final matching M that is output. Consider the last iteration of the algorithm. Since (u, v) ∈ E and u and v are not matched, (u, v) ∈ E[I]. Since this is the last run of the algorithm, a maximal matching M of G[I] is computed on one machine. Since M is maximal, either u or v or both must be matched in it. All of the edges of M get added to M in the last step, which gives our contradiction.</p><p>Next, consider the case that the algorithm failed. This occurs due to the set of edges E having size larger than η in some iteration of the algorithm. Note that E[|E |] = |S| • p = η /10 in a given iteration. By the Chernoff Bound it follows that |E | ≥ η with probability smaller than 2 -η ≤ 2 -40n (since η ≥ 40n). By Lemma 3.2 the algorithm completes in at most O(log n) rounds, thus the total failure probability is bounded by O(log n2 -40n ) using the union bound.</p><p>Finally we show how to implement this algorithm in MapReduce.</p><p>COROLLARY 3.1. The Maximal Matching algorithm can be implemented in three MapReduce rounds when η = n 1+2c/3 . Furthermore, when η = n 1+ then the algorithm runs for 3 c/ rounds and O(log n) rounds when η ≥ 40n. PROOF. By Lemma 3.2 the algorithm runs for one iteration with high probability when η = n 1+2c/3 , c/ iterations when η = n 1+ . Therefore it only remains to describe how to compute the graph G[I]. For this we appeal to Lemma 6.1 in <ref type="bibr" target="#b13">[13]</ref>, where the set Si are the edges incident on node i, and the function fi drops the edge i if it is matched and keeps it otherwise. Hence, each iteration of the algorithm requires 3 MapReduce rounds. Since the vertices in a maximal matching provide a 2approximation to the vertex cover problem, we get the following corollary. COROLLARY 3.2. A 2-approximation to the optimal vertex cover can be computed in three MapReduce rounds when η = n 1+2c/3 . Further, when η = n 1+ then the algorithm runs for 3 c/ rounds and O(log n) rounds when η ≥ 40n. This algorithm does O(m) total work when η = n 1+ for a constant &gt; 0. </p><formula xml:id="formula_6">!" #!" $!!" $#!" %!!" %#!" !&amp;!!$" !&amp;!!#" !&amp;!$" !&amp;!%" !&amp;!#" !&amp;$" $" !"#$</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Experimental Validation</head><p>In this Section we experimentally validate the above algorithm and demonstrate that it leads to significant runtime improvements in practice. Our data set consists of a sample of a graph of the twitter follower network, previously used in <ref type="bibr" target="#b1">[1]</ref>. The graph has 50,767,223 nodes, 2,669,502,959 edges, and takes about 44GB when stored on disk. We implemented the greedy streaming algorithm for maximum matching as well as the three phase MapReduce algorithm described above. The streaming algorithm remains I/O bounded and completes in 81 minutes. The total running times for the MapReduce algorithm with different values for the sampling probability p are given in Figure <ref type="figure" target="#fig_4">2</ref>.</p><p>The MapReduce algorithm achieves a significant speedup (over 10x) over a large number of values for p. The speed up is the result of the fact that a single machine never scans the whole input. Both the sampling in stage 1 and the filtering in stage 2 are performed in parallel. Note that the parallelization does not come for free, and the MapReduce system has non-trivial overhead over the straightforward streaming implementation. For example, when p = 1, the MapReduce algorithm essentially implements the streaming algorithm (since all of the edges are mapped onto a single machine), however the running time is almost 2.5 times slower. Overall these results show that the algorithms proposed are not only interesting from a theoretical viewpoint, but are viable and useful in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Maximum Weighted Matching</head><p>We present an algorithm that computes an approximation to the maximum weighted matching problem using a constant number of MapReduce rounds. Our algorithm takes advantage of both the sequential and parallel power of MapReduce. Indeed, it will compute several matchings in parallel and then combine them on a single machine to compute the final result. We assume that the maximum weight on an edge is polynomial in |E| and we prove an 8approximation algorithm. Our analysis is motivated by the work of Feigenbaum et al. <ref type="bibr" target="#b4">[4]</ref>, but is technically different since no single machines sees all of the edges.</p><p>The input of the algorithm is a simple graph G(V, E) and a weight function w : E → R. We assume that |V | = n and |E| = n 1+c for a constant c &gt; 0. Without loss of generality, assume that min{w(e) : e ∈ E} = 1 and W = max{w(e) : e ∈ E}. The algorithm works as follows:</p><formula xml:id="formula_7">1. Split the graph G into G1, G2, • • • , G log W ,</formula><p>where Gi is the graph on the set V of vertices and contains edges with weights in (2 i-1 , 2 i ].</p><p>2. For 1 ≤ i ≤ log W run the maximal matching algorithm on Gi. Let Mi be the maximal matching for the graph Gi.</p><p>3. Set M = ∅. Consider the edge sets sequentially, in descending order, M log W , . . . , M2, M1. When considering an edge e ∈ Mi, we add it to the matching if and only if M ∪ {e} is a valid matching. After all edges are considered, output M .</p><p>LEMMA 3.4. The above algorithm outputs an 8-approximation to the weighted maximum matching problem.</p><p>PROOF. Let OPT be the maximum weighted matching in G and denote by V (Mi) the set of vertices incident on the edges in Mi. Consider an edge (u, v) = e ∈ OPT, such that e ∈ Gj for some j. Let i * be the maximum i such that {u, v} ∩ V (Mi * ) = ∅. Note that i * must exist, and i * ≥ j else we could have added e to Mj. Therefore, w(e) ≤ 2 i * . Now for every such edge (u, v) ∈ OPT we select one vertex from {u, v} ∩ V (Mi * ). Without loss of generality, let that v be the selected vertex. We say that v is a blocking vertex for e. For each blocking vertex v, we associate its incident edge in Mi * and call it the blocking edge for e. Let V b (i) be the set of blocking vertices in V (Mi), we have that</p><formula xml:id="formula_8">log W X i=1 2 i |V b (i)| ≥ X e∈OPT w(e).</formula><p>This follows from the fact that every vertex can "block" at most one e ∈ OPT and that OPT is a valid matching. Note also that from the definition of blocking vertex if (u, v) ∈ M ∩ Mj then u, v / ∈ ∪ k&lt;j V b (k). Now suppose that an edge (x, y) ∈ M k is discarded by step 3 of the algorithm. This can happen if and only if there is an edge already present in the matching with a higher weight adjacent to x or y. Formally, there is a (u, v) ∈ M , (u, v) ∈ Mj with j &gt; k and {u, v} ∩ {x, y} = ∅. Without loss of generality assume that {u, v} ∩ {x, y} = x and consider such an edge (x, v). We say that (x, v) killed the edge (x, y) and the vertex y. Notice that an edge (u, v) ∈ M and (u, v) ∈ Mj kills at most two edges for every M k with k &lt; j and kills at most two nodes in V b (k). Finally we also define (u, v) b as the set of blocking vertices associated with the blocking edge (u, v). Now consider V b (k), each blocking vertex was either killed by one of the edges in the matching M , or is adjacent to one of the edges in M k . Furthermore, the total weight of the edges in OPT with that were blocked by a blocking vertex killed by (u, v) is at most</p><formula xml:id="formula_9">j-1 X i=1 2 i ˛{V b (i) killed by (u, v)} ˛≤ j-1 X i=1 2 i+1 ≤ 2 j+1 ≤ 4w((u, v)).</formula><p>(2) To conclude, note that each edge in OPT that is not in M was either blocked directly by an edge in M , or was blocked by a vertex that was killed by an edge in M . To bound the former, consider an edge (u, v) ∈ Mj ∩ M . Note that this edge can be incident on at most 2 edges in OPT, each of weight 2 j ≤ 2w((u, v)), and thus the weight in OPT incident on an edge (u, v) is 4w((u, v)).</p><p>Putting this together with Equation 2 we conclude:</p><formula xml:id="formula_10">8 X (u,v)∈M w((u, v)) ≥ X e∈OPT w(e).</formula><p>Furthermore we can show that the analysis of our algorithm is essentially tight. Indeed there exists a family of graphs where our algorithm finds a solution with weight w(OP T ) 8-o(1) with high probability. We prove the following lemma in Appendix A. LEMMA 3.5. There is a graph where our algorithm computes a solution that has value w(OP T ) 8-o(1) with high probability.</p><p>Finally, suppose that the weight function w : E → R is such that ∀e ∈ E, w(e) ∈ O(poly(|E|)) and that each machine has memory at least η ≥ max{2n log 2 n, |V | log 2 W } . Then we can run the above algorithm in MapReduce using only one more round than the maximal matching algorithm. In the first round we split G into G1, . . . , G log W ; then we run the maximal matching algorithm of the previous subsection in parallel on log W machines. In the last round, we run the last step on a single machine. The last step is always possible because we have at most |V | log W edges each with weights of size log W . THEOREM 3.2. There is an algorithm that finds a 8approximation to the maximum weighted matching problem on a c dense graph using machines with memory η = n 1+ in 3 c/ + 1 rounds with high probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>COROLLARY 3.3.</head><p>There is an algorithm that, with high probability, finds a 8-approximation to the maximum weighted matching problem that runs in four MapReduce rounds when η = n 1+2/3c .</p><p>To conclude the analysis of the algorithm we now study the work amount of the maximum matching algorithm. LEMMA 3.6. The amount of work performed by the maximum matching algorithm presented above is O(m) when η = n 1+ where 0 &lt; &lt; c is a fixed constant.</p><p>PROOF. The first step of the algorithm requires O(m) work as it can be done using a linear scan over the edges. In the second step, by Lemma 3.3 each machine performs work that is linear in the number of edges that are assigned to the machine. Since the edges are partitioned across the machines, the total work done in the second step is O(m). Finally we can perform the third step by a semi-streaming algorithm that greedily adds edges in the order M log W , . . . , M2, M1, requiring O(m) work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Minimum Edge Cover</head><p>Next we turn to the minimum edge cover problem. An edge cover of a graph G(V, E) is a set of edges E * ⊆ E such that each vertex of V has at least one endpoint in E * . The minimum edge cover is an edge cover E * of minimum size.</p><p>Let G(V, E) be a simple graph. The algorithm to compute a edge cover is as follows:</p><p>1. Find a maximal matching M of G using the procedure described in Section 3.1.</p><p>2. Let I be the set of uncovered vertices. For each uncovered vertex, take any edge incident on the vertex in I. Let this set of edges be U .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Output E</head><formula xml:id="formula_11">* = M ∪ U .</formula><p>Note that this procedure produces a feasible edge cover E * . To bound the size of E * let OPT denote the size of the minimum edge cover for the graph G and let OPTm denote the size of the maximum matching in G. It is known that the minimum edge cover of a graph is equal to |V | -OPTm. We also know that</p><formula xml:id="formula_12">|U | = |V |-2|M |. Therefore, |E * | = |V |-|M | ≤ |V |-1</formula><p>2 OPTm since a maximal matching has size at least 1  2 OPTm. Knowing that OPTm ≤ |V |/2 and using Corollary 3.1 to bound the number of rounds we have the following theorem. THEOREM 3.3. There is an algorithm that, with high probability, finds a 3  2 -approximation to the minimum edge cover in MapReduce. If each machine has memory η ≥ 40n then the algorithm runs in O(log n) rounds. Further, if η = n 1+ , where 0 &lt; &lt; c is a fixed constant, then the algorithm runs in 3 c/ + 1 rounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>COROLLARY 3.4.</head><p>There is an algorithm that, with high probability, finds a 3  2 -approximation to the minimum edge cover in four MapReduce rounds when η = n 1+2/3c . Now we prove that the amount of work performed by the edge cover algorithm is O(m). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">MINIMUM CUT</head><p>Whereas in the previous algorithms the filtering was done by dropping certain edges, this algorithm filters by contracting edges. Contracting an edge, will obviously reduce the number of edges and may either keep the number of vertices the same (in the case we contracted a self loop), or reduce it by one. To compute the minimum cut of a graph we appeal to the contraction algorithm introduced by Karger <ref type="bibr" target="#b10">[10]</ref>. The algorithm has a well known property that the random choices made in the early rounds succeed with high probability, whereas those made in the later rounds have a much lower probability of success. We exploit this property by showing how to filter the input in the first phase (by contracting edges) so that the remaining graph is guaranteed to be small enough to fit onto a single machine, yet large enough to ensure that the failure probability remains bounded. Once the filtering phase is complete, and the problem instance is small enough to fit onto a single machine, we can employ any one of the well known methods to find the minimum cut in the filtered graph. We then decrease the failure probability by running several executions of the algorithm in parallel, thus ensuring that in one of the copies the minimum cut survives this filtering phase.</p><p>The complicating factor in the scheme above is contracting the right number of edges so that the properties above hold. We proceed by labeling each edge with a random number between 0 and 1 and then searching for a threshold t so that contracting all of the edges with label less than t results in the desired number of vertices. Typically such a search would take logarithmic time, however, by doing the search in parallel across a large number of machines, we can reduce the depth of the recursion tree to be constant. Moreover, to compute the number of vertices remaining after the first t edges are contracted, we refer to the connected components algorithm in Section 2.4. Since the connected components algorithm uses a small number of machines, we can show that even with many parallel invocations we will not violate the machine budget. We present the algorithm and its analysis below. Also, the algorithm uses two subroutines, Findt and Contract which are defined in turn.</p><p>Algorithm 1 MinCut(E) 1: for i = 1 to n δ 1 (in parallel) do 2:</p><p>tag e ∈ E with a number re chosen uniformly at random from [0, 1] 3:</p><p>t ← Findt(E, 0, 1) 4:</p><p>Ei ← Contract(E, t) 5:</p><p>Ci ← min cut of Ei 6: end for 7: return minimum cut over all Ci</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Find Algorithm</head><p>The pseudocode for the algorithm to find the correct threshold is given below. The algorithm performs a parallel search on the value t so that contracting all edges with weight at most t results in a graph with n δ 3 vertices. The algorithm invokes n δ 2 copies of the connected components algorithm, each of which uses at most n c- machines, with n 1+ memory. ccj ← number of connected components in G = (V, Ej) 7: end for 8: if there exists a j such that ccj = n δ 3 then 9:</p><p>return j 10: else 11:</p><p>return Findt(E, τj, τj+1) where j is the smallest value s.t. ccj &lt; n δ 3 , ccj+1 &gt; n δ 3 12: end if</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Contraction Algorithm</head><p>We state the contraction algorithm and prove bounds on its performance. PROOF. Partition V into parts Pj = {v ∈ V | 2 j-1 &lt; deg(v) ≤ 2 j }. Since the degree of each node is bounded by n, there are at most log n parts in the partition. Define the volume of part j as Vj = |Pj| • 2 j . Parts having volume less than m 1-could all be mapped to one reducer without violating its space restriction. We now focus on parts with Vj &gt; m 1-, and so let Pj be such a part. Thus Pj contains between m 1-  + Θ " 3(6s) (1-)  " so even if we matched those nodes with the heavy edges in Gi we use at most (6s)</p><p>1+2 2 + Θ " 3(6s) (1-)  " of those edges. Thus we have that for every Gi, for every i &gt; 1 the maximal matching algorithm uses at most Θ " 6(6s) (1-) + (6s) " .</p><p>With the same reasoning, just with different constant, we notice that the same fact holds also for G1. So we have that for every Gi we use only o "</p><formula xml:id="formula_13">|V i | log 2 |V i | "</formula><p>heavy edges with probability " 1 -2e -Θ(</p><formula xml:id="formula_14">√ |V i |)</formula><p>" , further that every maximal matching that the algorithm computes it always matches the nodes in B1, because it is alway possible to use the edges that connect those nodes to the nodes of degree 1.</p><p>Knowing that we notice that the final matching that our algorithm outputs is composed by the maximal matching of G1 plus all the heavy edges in maximal matching of G2, • • • , GW . Thus we have that with probability Q  Thus the claim follows.</p><p>3 Note that every |Vi| ∈ Θ " n log W "</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Minimum spanning tree algorithm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>3 . 5 .</head><label>35</label><figDesc>If |E | &gt; η the algorithm fails. Otherwise give the graph G(V, E ) as input to a single machine and compute a maximal matching M on it. Set M = M ∪ M . 4. Let I be the set of unmatched vertices in G. Compute the subgraph of G induced by I, G[I], and let E[I] be the set of edges in G[I]. If |Ei| &gt; η, set S = E[I] and return to step 2. Otherwise continue to step 5. Compute a maximal matching M on G[I] and output M = M ∪ M</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>LEMMA 3 . 1 .</head><label>31</label><figDesc>Let E ⊆ E be a set of edges chosen independently with probability p. Then with probability at least 1 -e -n , for all I ⊆ V either |E[I]| &lt; 2n/p or E[I] ∩ E = ∅. PROOF. Fix one such subgraph, G[I] = (I, E[I]) with |E[I]| ≥ 2n/p. The probability that none of the edges in E[I]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>LEMMA 3 . 3 .</head><label>33</label><figDesc>The maximal matching algorithm presented above is work efficient when η = n 1+ where 0 &lt; &lt; c is a fixed constant.PROOF. ByLemma 3.2 when η = n 1+ there are at most a constant number of iterations of the algorithm. Thus it suffices to show that O(m) work is done in a single iteration. Sampling each edge with probability p requires a linear scan over the edges, which is O(m) work. Computing a maximal matching on one machine can be done using a straightforward, greedy semi-streaming algorithm requiring |E | ≤ η ≤ m work. Computing G[I] can be done as follows. Load M onto m machines where 0 &lt; &lt; c and partition E among those machines. Then, if an edge in E is incident on an edge in M the machines drop that edge, otherwise that edge is in G[I]. This results in O(m) work to load all of the data onto the machines and O(m) work to compute G[I]. Since G[I] has at most m edges, computing M on one machine using the best known greedy semi-streaming algorithm also requires O(m) work.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The running time of the MapReduce matching algorithm for different values of p, as well as the baseline provided by the streaming implementation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>LEMMA 3 . 7 .</head><label>37</label><figDesc>The amount of work performed by the edge cover algorithm presented above is O(m) when η = n 1+ where 0 &lt; &lt; c is a fixed constant. PROOF. By Lemma 3.3 when η = n 1+ the first step of the algorithm can be done performing O(m) operations. The second step can be performed by a semi streaming algorithm that requires O(m) work. Thus the claim follows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Algorithm 2 2 3:</head><label>22</label><figDesc>Findt(E, min, max)1: {Uses n δ 2 +c/ machines.} 2: γ ← max -min n δ for j = 1 to n δ 2 (in parallel) do 4: τj ← min +jγ 5: Ej ← {e ∈ E | re ≤ τj} 6:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Algorithm 3</head><label>3</label><figDesc>Contract(E, t)1: CC ← connected components in {e ∈ E | re ≤ t} 2: let h : [n] → [n δ 4 ]be a universal hash function 3: map each edge (u, v) to machine h(u) and h(v) 4: map the assignment of node u to its connected component CC(u), to machine h(u) 5: on each reducer rename all instances of u to CC(u) 6: map each edge (u, v) to machine h(u) + h(v) 7: Drop self loops (edges in same connected component) 8: Aggregate parallel edges LEMMA 4.1. The Contract algorithm uses n δ 4 machines with O( m n δ 4 ) space with high probability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>2 j</head><label>2</label><figDesc>and 2m 1-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>2 jor B E 2 i</head><label>22</label><figDesc>vertices. Let ρThus the maximum number of nodes left unmatched in B E 1 i after step 2 of the maximal matching algorithm is smaller then (6s) 1+2 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>1 C</head><label>1</label><figDesc>(6s) -log(6s))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>" = 1 -</head><label>1</label><figDesc>o(1) 3 the total weight of the computed solution is upper-bounded by`W 2 + 1 ´n 2 log W + W log W • o " W + o(W )and so the ratio between the optimum and the solution is</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>( 4 -</head><label>4</label><figDesc></figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In practice this requirement stems from the fact that even streaming through a terabyte of data requires a non-trivial amount of time as the machine remains IO bound.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>In other words, the total amount of data shuffled in any round must be less than the total amount of memory in the system.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Ashish Goel, Jake Hofman, John Langford, Ravi Kumar, Serge Plotkin and Cong Yu for many helpful discussions.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>* Work done while visiting Yahoo! Labs. † Work done while visiting Yahoo! Labs. Partially supported by NSF grants CCF-0728782 and CCF-1016684.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>be an arbitrary reducer. Since h is universal, the probability that any vertex v ∈ Pj maps to ρ is exactly n -δ 4 . Therefore, in expectation, the number of vertices of Pj mapping to ρ is at most 2m 1- 2 j n δ 4 . Since each of these vertices has degree at most 2 j , in expectation the number of edges that map to ρ is at most 2m 1- n δ 4 . Let the random variable Xj denote the number of vertices from Pj that map to ρ. Say that a bad event happens if more than 4m 1- 2 j vertices of Vj map to ρ. Chernoff bounds tell us that the probability of such an event happening is O(1/n 2δ 4 ),</p><p>Taking a union bound over all n δ 4 reducers and log n parts, we can conclude that the probability of any reducer being overloaded is bounded below by 1 -o(1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis of the MinCut Algorithm</head><p>We proceed to bound the total number of machines, maximum amount of memory, and the total number of rounds used by the MinCut algorithm. LEMMA 4.2. The total number of machines used by the MinCut algorithm is n δ 1 `nδ 2 +c-+ n δ 4 ´.</p><p>PROOF. The algorithm begins by running n δ 1 parallel copies of a simpler algorithm which first invokes Findt, to find a threshold t for each instance. This algorithm uses n δ 2 parallel copies of a connected component algorithm, which itself uses n c-machines (see Section 2.4). After finding the threshold, we invoke the Contract algorithm, which uses n δ 4 machines per instance. Together this gives the desired number of machines. LEMMA 4.3. The memory used by each machine during the execution of MinCut is bounded by max{n 2δ 3 , n 1+c-δ 4 , n 1+ }.</p><p>PROOF. There are three distinct steps where we must bound the memory. The first is the the searching phase of Findt. Since this algorithm executes instances of the connected components algorithm in parallel, the results of Section 2.4 ensure that each instance uses at most η = n 1+ memory. The second is the contraction algorithm. Lemma 4.1 assures us that the input to each machine is of size at most O( m n δ 4 ). Finally, the last step of MinCut requires that we load an instance with n δ 3 vertices, and hence at most n 2δ 3 edges onto a single machine. LEMMA 4.4. Suppose the amount of memory available per machine is η = n 1+ . MinCut runs in O( <ref type="formula">1</ref>δ 2 ) number of rounds. PROOF. The only variable part of the running time is the number of rounds necessary to found a threshold τ so that the number of connected components in Findt is exactly n δ 3 . Observe that after the k th recursive call, the number of edges with threshold between min and max is m n δ 2 k . Therefore the algorithm must terminate after at most 1+c δ 2 rounds, which is constant for constant δ2. We are now ready to prove the main theorem. THEOREM 4.1. Algorithm MinCut returns the minimum cut in G with probability at least 1-o(1), uses at most η = n 1+ memory per machine and completes in O( <ref type="formula">1</ref>2 ) rounds. PROOF. We first show that the success probability is at least</p><p>The algorithm invokes n δ 1 parallel copies of the following approach: (1) simulate Karger's contraction algorithm <ref type="bibr" target="#b10">[10]</ref> for the first n -n δ 3 steps resulting in a graph Gt and (2) Identify the minimum cut on Gt. By Corollary 2.2 of <ref type="bibr" target="#b12">[12]</ref> step (1) succeeds with probability at least p = Ω(n 2δ 3 -2 ). Since the second step can be made to fail with 0 probability, each of the parallel copies succeeds with probability at least p. By running n δ 1 independent copies of the algorithm, the probability that all of the copies fail in step ( <ref type="formula">1</ref>) is</p><p>To prove the theorem, we must find a setting of the parameters δ1, δ2, δ3, and δ4 so that the memory, machines, and correctness constraints are satisfied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A. WEIGHTED MATCHING LOWER BOUND</head><p>LEMMA A.1. There is a graph where our algorithm compute a solution that has value w(OP T ) 8-o(1) with high probability. PROOF. Let G(V, E) a graph on n nodes and m vertices, and fix a W = 2 log m . We say that a bipartite graph G(V, E1, E2) is balanced if |E1| = |E2|. Consider the following graph: there is a central balanced bipartite clique, B1, on n 2 log W nodes and all the edges of the clique have weight W 2 + 1. Every side of the central bipartite clique is also part of other log W -1 balanced bipartite cliques. We refer to those cliques has</p><p>we have that the weight of the edges in them have weight W 2 i + 1. Furthermore every node in B1 is also connected with an additional node of degree one with an edge of weight W , and every node in B E 1 i \B1 and B E 2 i \B1 is connected to a node of degree one with an edge of weight W 2 i-1 . Figure <ref type="figure">3</ref> shows the subgraph composed by B1 and the two graphs B E 1 i and B E 2 i . Note that the optimal weighted maximum matching for this graph is the one that is composed by all the edges incident to a node of degree one and its total value is</p><p>Now we will show an upper-bound on the performance of our algorithm that holds with high probability. Recall that in step one our algorithm splits the graph in G1, • • • , GW subgraph where the edges in Gi are in in (2 i-1 , 2 i ] then it computes a maximal matching on Gi using the technique shown in the previous subsection. In particular the algorithm works as follows: it samples the edges in Gi with probability 1 |E i | and then computes a maximal matching on it, finally it tries to match the unmatched nodes using the edges between the unmatched nodes in Gi.</p><p>To understand the value of the matching returned by the algorithm, consider the graph Gi note that this graph is composed only by the edges in</p><p>and the edges connected to nodes of degree one with weight W 2 i-1 . We refer to those last edges as the heavy edges in Gi. Note that the heavy edges are all connected to vertices in B E 1 i \B1 and B E 2 i \B1. Let s be the number of vertices in a side of B E 1 i , note that Gi, for i &gt; 1 has 6s nodes and s 2 + 2s edges.</p><p>Recall that we sample an edges with probability " = Θ `(2s) 1-´heavy edges, thus using the Chernoff bound we have that the probability that the number of sampled heavy edges is bigger or equal than Θ " 3(6s) (1-)  " is e -3(6s) (1-)  . Further notice that by lemma 3.1 for every set of node in B E 1 i or B E 2 i with (6s) 1+2 edges we have at least an edge in it with probability e - √ 6s( 1 C (6s) -log(6s)) .</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Everyone&apos;s an influencer: Quantifying influence on twitter</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bakshy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hofman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Watts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WSDM</title>
		<meeting>WSDM</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A minimum spanning tree algorithm with inverse-Ackerman type complexity</title>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Chazelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1028" to="1047" />
			<date type="published" when="2000-11">November 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">MapReduce: Simplified data processing on large clusters</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of OSDI</title>
		<meeting>OSDI</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="137" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On graph problems in a semi-streaming model</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Feigenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sampath</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mcgregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Suri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">348</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="207" to="216" />
			<date type="published" when="2005-12">December 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Simulating parallel algorithms in the mapreduce framework with applications to parallel computational geometry</title>
		<author>
			<persName><forename type="first">T</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><surname>Goodrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second Workshop on Massive Data Algorithmics (MASSIVE 2010)</title>
		<imprint>
			<date type="published" when="2010-06">June 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<ptr target="http://wiki.apache.org/hadoop/PoweredBy" />
		<title level="m">Hadoop Wiki -Powered By</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Big data and the power of hadoop. Yahoo! Hadoop Summit</title>
		<author>
			<persName><forename type="first">Blake</forename><surname>Irving</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010-06">June 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A fast and simple randomized parallel algorithm for maximal matching</title>
		<author>
			<persName><forename type="first">Amos</forename><surname>Israel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Itai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing Letters</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="77" to="80" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">HADI: Fast diameter estimation and mining in massive graphs with hadoop</title>
		<author>
			<persName><forename type="first">Charalampos</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><forename type="middle">Paula</forename><surname>Tsourakakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
		<idno>CMU-ML-08-117</idno>
		<imprint>
			<date type="published" when="2008-12">December 2008</date>
		</imprint>
		<respStmt>
			<orgName>CMU</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Global min-cuts in RN C and other ramifications of a simple mincut algorithm</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>Karger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SODA</title>
		<meeting>SODA</meeting>
		<imprint>
			<date type="published" when="1993-01">January 1993</date>
			<biblScope unit="page" from="21" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A randomized linear-time algorithm for finding minimum spanning trees</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>Karger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">N</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Tarjan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-sixth annual ACM symposium on Theory of computing, Proceedings of STOC</title>
		<meeting>the twenty-sixth annual ACM symposium on Theory of computing, STOC<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An Õ(n 2 ) algorithm for minimum cuts</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>Karger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clifford</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of STOC</title>
		<meeting>STOC</meeting>
		<imprint>
			<date type="published" when="1993-05">May 1993</date>
			<biblScope unit="page" from="757" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A model of computation for MapReduce</title>
		<author>
			<persName><forename type="first">Howard</forename><surname>Karloff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Suri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergei</forename><surname>Vassilvitskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SODA</title>
		<meeting>SODA</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="938" to="948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Graphs over time: Densification laws, shrinking daimeters and possible explanations</title>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>11th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Data-Intensive Text Processing with MapReduce. Number 7 in Synthesis Lectures on Human Language Technologies</title>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010-04">April 2010</date>
			<publisher>Morgan and Claypool</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pregel: A system for large-scale graph processing</title>
		<author>
			<persName><forename type="first">Grzegorz</forename><surname>Malewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">H</forename><surname>Austern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Aart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">C</forename><surname>Bik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilan</forename><surname>Dehnert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naty</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grzegorz</forename><surname>Leiser</surname></persName>
		</author>
		<author>
			<persName><surname>Czajkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGMOD</title>
		<meeting>SIGMOD<address><addrLine>Indianapolis, IN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010-06">June 2010</date>
			<biblScope unit="page" from="135" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Inside large-scale analytics at facebook. Yahoo! Hadoop Summit</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schroepfer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010-06">June 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Graph sparsification by effective resistances</title>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Spielman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of STOC</title>
		<meeting>STOC<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="563" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed weighted matching</title>
		<author>
			<persName><forename type="first">Mirjam</forename><surname>Wattenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Wattenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of DISC</title>
		<meeting>DISC</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="335" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Hadoop: The Definitive Guide</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>White</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<pubPlace>O&apos;Reilly Media</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<ptr target="http://research.yahoo.com/news/2743" />
		<title level="m">Yahoo! partners with four top universities to advance cloud computing and applications research</title>
		<imprint>
			<publisher>Yahoo! Inc Press Release</publisher>
			<date type="published" when="2009-04">April 2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
