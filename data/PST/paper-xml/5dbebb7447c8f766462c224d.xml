<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Job2Vec: Job Title Benchmarking with Collective Multi-View Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-09-16">16 Sep 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Denghui</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Magagement Science and Information Technology Department</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Magagement Science and Information Technology Department</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junming</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Magagement Science and Information Technology Department</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Magagement Science and Information Technology Department</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hengshu</forename><surname>Zhu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Baidu Talent Intelligent Center</orgName>
								<address>
									<settlement>Baidu Inc</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Baidu Talent Intelligent Center</orgName>
								<address>
									<settlement>Baidu Inc</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanchi</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Magagement Science and Information Technology Department</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Magagement Science and Information Technology Department</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lichen</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Electrical and Computer Engineering Department</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Electrical and Computer Engineering Department</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pengyang</forename><surname>Wang</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of Central Florida</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of Central Florida</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hui</forename><surname>Xiong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Magagement Science and Information Technology Department</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Baidu Talent Intelligent Center</orgName>
								<address>
									<settlement>Baidu Inc</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Magagement Science and Information Technology Department</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Baidu Talent Intelligent Center</orgName>
								<address>
									<settlement>Baidu Inc</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="middle">2019</forename><surname>Job2vec</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Baidu Talent Intelligent Center</orgName>
								<address>
									<settlement>Baidu Inc</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Job2Vec: Job Title Benchmarking with Collective Multi-View Representation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-09-16">16 Sep 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3357384.3357825</idno>
					<idno type="arXiv">arXiv:2009.07429v1[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>• Information systems → Information systems applications; Web mining Talent Intelligence</term>
					<term>Job Title Benchmarking</term>
					<term>Multi-view learning</term>
					<term>Auto-encoder</term>
					<term>Representation Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Job Title Benchmarking (JTB) aims at matching job titles with similar expertise levels across various companies. JTB could provide precise guidance and considerable convenience for both talent recruitment and job seekers for position and salary calibration/prediction. Traditional JTB approaches mainly rely on manual market surveys, which is expensive and labor intensive. Recently, the rapid development of Online Professional graph has accumulated a large number of talent career records, which provides a promising trend for datadriven solutions. However, it is still a challenging task since (1) the job title and job transition (job-hopping) data is messy which contains a lot of subjective and non-standard naming conventions for a same position (e.g., Programmer, Software Development Engineer, SDE, Implementation Engineer), (2) there is a large amount of missing title/transition information, and (3) one talent only seeks limited numbers of jobs which brings the incompleteness and randomness for modeling job transition patterns. To overcome these challenges, we aggregate all the records to construct a large-scale Job Title Benchmarking Graph (Job-Graph), where nodes denote job titles affiliated with specific companies and links denote the correlations between jobs. We reformulate the JTB as the task of link prediction over the Job-Graph that matched job titles should have links. Along this line, we propose a collective multi-view representation learning method (Job2Vec) by examining the Job-Graph jointly in (1) graph topology view (the structure of relationships among job titles), (2) semantic view (semantic meaning of job descriptions), (3) job transition balance view (the numbers of bidirectional transitions between two similar-level jobs are close), and (4) job transition duration view (the shorter the average duration of transitions is, the more similar the job titles are). We fuse the multi-view representations in the encode-decode paradigm to obtain an unified optimal representations for the task of link prediction. Finally, we conduct extensive experiments to validate the effectiveness of our proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recent years have witnessed the increasing popularity of using data mining techniques for addressing human resource management (HRM) tasks (e.g., intelligent job-person fit and intelligent interview assessment <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>). However, few research efforts have been made on intelligent Job Title Benchmarking (JTB), which aims at matching job titles with similar expertise levels across various companies. For both job seekers and employers, JTB is important for talent recruitment and salary calibration. With appropriate JTB insights, employers can recruit relevant talent with the right title and salary. While for job seekers, JTB can provide guidance for their career development. In this paper, we study the problem of JTB from the data mining persective.</p><p>Traditional JTB relies heavily on manual market surveys, which is expensive and labor intensive. Recently, the emergence of Online Professional graph (e.g., Linkedin) helps to accumulate a large number of career records, which provides an unparalleled opportunity for a data-driven solution. However, JTB is still a challenging task due to the following three aspects. First, the job title and job transition (job-hopping) data is messy which contains a lot of subjective and non-standard naming conventions for the same position. For example, as shown in Figure <ref type="figure" target="#fig_0">1</ref>, Software Engineer, SDE, Software Development Engineer, and Computer Programmer are the same-level jobs across different IT companies. Second, there is a large amount of missing title/transition information. Many users on Online Professional graph do not update their information in time. Too much missing information will hinder the applicability of data mining algorithms. Third, in individual career, one talent only seeks limited numbers of jobs compared to the total set of job titles on the job market, which brings the incompleteness and randomness for modeling job transition patterns.</p><p>To tackle these challenges, we propose to construct a Job Title Benchmarking Graph (Job-Graph), where nodes denote job titles affiliated with the specific companies and links denote the numbers of transition between job titles. We hold the assumption that the bechmarked job title pairs should have strong correlations that there exists links between the job titles. Along this line, we reformulate JTB as the task of the link prediction over the Job-Graph.</p><p>Representation learning methods achieve outstanding performances for the link prediction task <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b29">30]</ref>. However, due to the three unique properties of Job-Graph (the topology structure, rich semantic information of job titles, and job transition patterns), existing representation learning methods are unable to model these properties at the same time. Therefore, we propose a collective multi-view representation learning method to learn the representations of job titles for the task of link prediction.</p><p>Specifically, first, we model four views of representations: (1) Graph Structure View, which refers to the topology structure of the Job-Graph that encodes the graph structure and neighborhood information; (2) Semantic View, which refers to the semantic meaning of job titles; (3) Job Transition Balance View, which is compliant with the observation that the numbers of bidirectional transitions between two similar-level jobs are close; (4) Job Transition Duration View, which reveals the fact that the shorter the average duration of transitions is, the more similar the job titles are. Then, to obtain an unified representation, we design a representation fusion process based on the encode-decode paradigm. The multi-view representations are fed into the associated multi-layer perceptrons which are attached behind with a representation ensemble layer to work as an encoder. The ensembled representation is dispatched to the corresponding decoder by a representation dispatching layer to reconstruct the multi-view representations. The loss between the original and reconstructed multi-view representations will be minimized to guarantee the optimal unified representation. Moreover, we train the multi-view representation learning procedure and the representation fusion procedure in an alternative way. The loss from four views and the representation fusion procedure will be minimized simultaneously to generate high quality representation for job titles for the task of link prediction.  In summary, in this paper, we propose a data-driven solution for the problem of JTB. Specifically, we first construct the Job Title Benchmarking Graph based on the job transition records. Then, we reformulate the problem of JTB as the task of link prediction. We propose a collective multi-view representation learning method, by learning an unified job title representation from the graph structure view, semantic view, job transition balance view, and job transition duration view. Finally, we conduct extensive experiment to evaluate our proposed method over the real-world dataset. The promising results validate the effecitveness of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>In this section, we first briefly introduce the real-world dataset we collected for JTB task. Then, we introduce some essential definitions. Followed the definitions, we propose our problem statement. Finally, we present an overview of the propposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data Description</head><p>In this study, we analyze real-world talent job title transition data, collected from a major commercial Online Professional Network. The data includes two main categories, IT-related and Financerelated job titles. Table <ref type="table" target="#tab_1">2</ref> shows Job-Graphs constructed from these two datasets are very sparse. Table <ref type="table" target="#tab_0">1</ref> presents an example of job transition records from an individual talent. Each line consists of a job title, company name and the duration holding this position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Definition</head><p>Here, we introduce some essential definitions, which will be used throughout this paper. Definition 2.1. Job Title Bechmarking (JTB) JTB is a process that matches job titles with similar expertise levels across various companies. Formally, given two job title-company pair, (T itle i , Company i ) and (Title j , Company j ), the objective is determine whether the given paired job titles are on the same level. JTB could provide precise guidance and considerable convenience for both talent recruitment and job seekers for position and salary calibration/prediction.  and v j indicates that there exist job transitions from the (Title i , Company i ) to (T itle j , Company j ), and the weight of edge e i j represents the number of transitions observed from (Title i , Company i ) to (T itle j , Company j )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Problem Statement</head><p>In this paper, we study the problem of Job Title Benchmarking (JTB). We first construct job title transition graph to depict the job transition patterns. We formulate the JTB as the the task of link prediction over the Job-graph, based on the assumption that similar-level job titles should have strong correlations to enable a link between them. To enable the link prediction task, we push forward the problem formulation to the representation learning over the Job-Graph to learn unified and optimal representations for job titles. Formally, given the Job-Graph G = (V , E), we aim to find a mapping function f : v → z that takes node (job title) v as the input, and outputs the vectorized representation z of the job titles, while preserving properties of the Job-Graph and job transition patterns. The generated node (job title) representation v is then utilized to solve the problem of link prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Framework Overview</head><p>Figure <ref type="figure" target="#fig_1">2</ref> shows an overview of our proposed framework that includes the following essential tasks: (i) constructing the job title transition graph; (ii) developing a collective multi-view representation learning method for learning job title representations; (iii) applying learned job title representations for link prediction on Job-Graph. In the first task, given job transition records of talents, we construct a job title transition graph. In the second task, a collective multi-view representation learning method is developed for jointly modeling the graph structure, semantic meaning of job titles and job transition patterns. In the last task, we apply our proposed method to learn job title representations for link prediction on Job-Graph to benchmark job titles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">JOB-GRAPH CONSTRUCTION AND REFINEMENT</head><p>In this section, we show how to construct the Job-Graph. Intuitively, the Job-Graph can be directly constructed from the raw job tranistion record data. However, messy, noisy and non-standard name convention of job titles makes the Job-Graph extremely sparse and redundant, which hinders the further analysis. Therefore, we refine the Job-Graph into an applicable fashion in the following steps:</p><p>(1) Extract job transitions from the raw career records; (2) Map and aggregate all the transitions into the Job-Graph, where each node represents a job title, each edge represents the number of transitions between the nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Job Transition Extraction</head><p>As shown in Table <ref type="table" target="#tab_0">1</ref>, each transition consists of a source job and destination job, i.e., (job sr c , job des ). We first set all the job titles existed in the raw data as nodes. Then we sum the transition frequencies as the link from job sr c to job des . The constructed graph is set as the base graph for the further refinement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Job Title Aggregation</head><p>Generally speaking, job titles usually consist of three parts:</p><p>(1) Title level, such as Senior, Principle and Director.</p><p>(2) Title core function, such as Software Engineer, Product Manager.</p><p>(3) Unique additional information, such as Software Engineer in Bid Data, Sales Rep on Small and Medium businesses.</p><p>After we study the word frequencies of job titles, there is an interesting observation that the word frequency distribution subjects to power law distribution, as shown in Figure <ref type="figure" target="#fig_2">3</ref>. It also can be observed that noise words or those additional user-related words usually appear in the long tail. We also show the top 10 frequent words and bottom 10 frequent words. It is obvious that the most frequent words like manager and engineer usually describe the core function of job titles, while the less frequent words looks more like user's unique information. With this observation, we aggregate job titles by filtering out low frequency words in job titles and thus get a normalized and denser Job-Graph. Specifically, in this work, we filtered out the words that have a frequency lower than 30.</p><p>Table <ref type="table" target="#tab_4">3</ref> shows three real examples of aggregated job titles by filtering out low frequency words. Words in bold indicate the lowfrequency words. For example, "Tactical Sourcing Buyer (Unilever)" and "Sourcing Buyer, MARCOM &amp; FSOS" are originally thought to be different, after filtering low-frequency words, they are aggregated to the same title "Sourcing Buyer". To be noted, the reason we use filtering low-frequency words instead of using cluster algorithm to cluster the job titles is that there are no standard target classes for job titles, and it is hard to decide the number of clusters for job titles if cluster algorithm is applied.</p><p>With this aggregated Job-Graph, we can obtain some concise job title matching insights like: A Senior Software Engineer of LinkedIn can match a Software Engineer of Google since most Senior Software Engineers of LinkedIn obtained the title of Software Engineer when they just made a career transition to Google. However, the sparsity issue of Job-Graph still limits the performance of traditional representation learning method. Therefore, in next section, we introduce our collective multi-view representation learning method, Job2Vec, and show how to perform link prediction to enrich Job-Graph based on the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">JOB2VEC: COLLECTIVE MULTI-VIEW REPRESENTATION LEARNING FOR JOB TITLES</head><p>In this section, we introduce our collective multi-view representation learning methods fore job titles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model Intuition</head><p>We learn representations of job titles with the following intuitions. Intuition 1: Topology Structure Preservation. Job-Graph is built to depict job transitions on the job market. The topology structure of Job-Graph reveal the connectivity and neighbor information of job titles which can help to describe the latent structures among job titles. We should preserve the topology structure of job-graph in the representation learning.</p><p>Intuition 2: Semantics Preservation. Job titles contain rich semantic descriptions which can further enhance the quality of job title representation. Therefore, we should preserve the semantic meanings of job titles. Therefore, we model the job title representations in multi views. Specifically, we introduce graph topology view for Intuition 1, semantic view for Intuition 2, job transition balance view and job transition duration view for Intuition 3. In addition, we propose a collective method to fuse the multi-view representations into an unified representation. We introduce the details as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Graph Topology View</head><p>Job graph structure explicitly illustrates the similarity and correlations between different titles. It is the most crucial and effective information which provides comprehensive and accurate title connections. However, the topology information is hidden in the graph structure. Motivated by the success of graph representation learning methods in link prediction on social graph and knowledge graph <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29]</ref>, the first view we use in Job2vec is the Graph Topology view, which can encode the graph structure and neighborhood information into the representations. In Graph Topology view, we aim to learn a low-dimensional representation for each job title which can keep the neighborhood structure information, i.e., job titles that share similar neighbors in Job-Graph should be close to each other in the graph view representation space.</p><p>To achieve this, we first assign each job title v i into two representation vectors, "self representation", e i ∈ R N д , and "neighbor representation", e ′ i ∈ R N д , where N д is dimension of the representation vector of Graph Topology View. Both e i and e ′ i are randomly initialized. We utilize its "self representation" while "neighbor representation" will be used if v i is just the neighbor of the one we focus on. Then, in order to enforce embeddings to be close to each other if they share similar graph neighbors, we define the loss function O N as followed:</p><formula xml:id="formula_0">O N = − (i, j)∈E w i j loд(p(v j |v i )),<label>(1)</label></formula><p>where w i j is the weight between v i and v j , E is the set of all edges in Job-Graph, to incorporate high-order proximity, we extend E by adding edges of k-steps paths into the set. When k = 1, O N is the same as the second-order proximity in LINE. p(v j |v i ) is the probability of v j occured as neighbor given v i , defined as a softmax function followed:</p><formula xml:id="formula_1">p(v j |v i ) = exp(ì e ′T j • ì e i ) |V | k=1 exp(ì e ′T k •, ì e i ) ,<label>(2)</label></formula><p>where v i is the current job title we focus on, v j is the neighbors of v i , e i is the "self representation" of v i , e ′ j is the "neighbor representation" of v j , V is the set of all job titles, i.e., all nodes in Job-Graph, |V | is the cardinal number of V . Minimizing O N equals to maximizing the conditional probability of v j given v i . Since the conditional probability p(v j |v i ) is parameterized by e ′ j and e i , as a result, the "self representation" of those job titles that share similar neighbors will be similar, i.e., close in the graph view space. To be noted, in testing stage, we utilizes the "self representation" of each job title to calculate the similarity score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Semantic View</head><p>Normally, each job title is consisted of several key words which describe the basic function and duty of this job (e.g., Project Manager and Computer Engineer). Therefore, the semantic information contained in these keywords is crucial to be explored in the representation learning process for two reasons: (1) Talents tend to make transition between functionally similar jobs, and semantic information guides the model to learn a better representation to tackle the complex job transition pattern. (2) The shared key words in job titles could further connect them even though the Job Transition Graph is extremely sparse, thus can alleviate the sparsity issue and improve the prediction capability of the learned representation.</p><p>We consider this view as the semantic view of Job Transition Graph. In semantic view, we aim to learn a low-dimensional representation of each job title which can keep the semantic information, i.e., job titles that have similar key words should be close to each other in the semantic view representation space. To achieve this, we first assign each job title v i a vector s i ∈ R N s , and each word w j in the Job-Graph vocabulary a vector s ′ j ∈ R N s which are randomly initialized. N s is dimension of the representation vector of Semantic View. Then, we enforce s i to be close to each other if they share similar words, based on the loss function O S defined as followed:</p><formula xml:id="formula_2">O S = − w j ∈v i f i j loд(p(w j |v i )),<label>(3)</label></formula><p>where f i j is the frequency of the word w j occurred in v i , v i is a job title, w j is the words in v i , p(w j |v i ) is the probability of w j occurred in v i , defined as a softmax function:</p><formula xml:id="formula_3">p(w j |v i ) = exp(ì s ′T j • ì s i ) |W | k =1 exp(ì s ′T k • ì s i ) ,<label>(4)</label></formula><p>where W is the vocabulary set of Job-Graph, s i is the semantic representation of job title v i , s ′ j is the semantic representation of word w j . Minimizing O S equals to maximizing the conditional probability of w j given v i , as a result, job titles v i and v j will have similar representations s i and s j if they are similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Job Transition Balance View</head><p>The numbers of bidirectional transitions between two similar-level jobs are close. For example, software engineer in Apple is on the same level as the SDE in Facebook. The transition number from software engineer (Apple) to the SDE (Facebook), and the transition number from the SDE (Facebook) to software engineer (Apple) should be close. However, for two different-level jobs, like junior software engineer and senior software engineer, the transition usually is in one direction, from the junior to the senior one. In other words, the transition number for two-directions will be very different. To this end, we further consider Job Transition Balance as an important factors for JTB, which effectively indicates the matches of each pair of titles. The intuition of Transition Balance is that if comparable amounts of talent transitions can be found in both direction between two job titles, then these two job titles are highly likely to be in the same level. To model Transition Balance, we first assign each job title v i a vector b i ∈ R N b which is randomly initialized. N b is the dimension of the representation vector of Transition Balance View. Then given two job titles v i and v j , we define the Transition Balance (TB) between them as:</p><formula xml:id="formula_4">T B(v i , v j ) = exp(− |w i j − w ji | w i j w ji ),<label>(5)</label></formula><p>View Ensemble where w i j is the weight of the edge from v i to v j . Then, based on the loss function O B we enforce b i to be similar to each other if they have balanced transitions between each other. O B is defined as followed:</p><formula xml:id="formula_5">O B = − (i, j)∈v i T B(v i , v j )loд(p(v i , v j )).<label>(6)</label></formula><p>where p(v i , v j ) is the joint probability of v i and v j defined as:</p><formula xml:id="formula_6">p(v i , v j ) = 1 1 + exp(− ì b T i • ì b j ) .<label>(7)</label></formula><p>Minimizing O B will "drag" the representation vectors of those "balanced" job title pair to be close in the representation space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Job Transition Duration View</head><p>Most people require a relatively long time (e.g., one or several years) to get a promotion. In contrast, if a person can change his/her jobs quickly and frequently, then there are high possibilities that these jobs are similar titles requires similar expertise and working experience. Make a long story short, the shorter the average duration of transitions is, the more similar the job titles are. To this end, we define Job Transition Duration which is the average duration time between two job titles. To include Transition Duration property into our model, we first assign each job title v i a vector d i ∈ R N d which are randomly initialized. N d is dimension of the representation vector of Transition Duration View. Given two job titles v i and v j , we define the Transition Duration (TD) between them as:</p><formula xml:id="formula_7">T D(v i , v j ) = exp(−t i j ),<label>(8)</label></formula><p>where t i j is the average duration time from v i to v j . Then we designed a loss function O D , which enforces d i and d j to be closer to each other if the average transition time between them is short, i.e., it is easy to transit from v i to v j . O D is defined as:</p><formula xml:id="formula_8">O D = − (i, j)∈v i T D(v i , v j )loд(p(v i , v j )),<label>(9)</label></formula><p>where p(v i , v j ) is the joint probability of v i and v j defined as:</p><formula xml:id="formula_9">p(v i , v j ) = 1 1 + exp(− ì d ⊤ i • ì d j ) . (<label>10</label></formula><formula xml:id="formula_10">)</formula><p>Minimizing O D will "drag" the representation vectors of those "easily transited" job title pair to be close in the representation space, which hopefully further improve the learning performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Multi-view Representation Fusion</head><p>Each views provides a comprehensive and unique aspect across different titles, and there are more informative and sophisticated correlation knowledge residing across different views. While, naively combining all the views together cannot efficiently utilize the information, or even hurt the performance due to the dramatic different scales and formats of the views.</p><p>To this end, we propose a Collective Multi-View Auto-Encoder (CMVAE) framework to compress the multiple representations into a single denser representation. As shown in Figure <ref type="figure" target="#fig_4">4</ref>, the four different representations obtained by learning from the above mentioned objectives are feed into the CMVAE. To avoid losing information from all the representations, we directly concatenate the representations and feed them into the Fusion Encoder F (•). F (•) consists of two fully-connected layers and outputs a single and denser representation. Then this intermediate representation is feed to the Fusion Decoder G(•) which is also a two fully-connected layer neural network and outputs the restored representation. The objective function of CMVAE is shown below:</p><formula xml:id="formula_11">L = 1 N N i=1 ∥X i − G(F (X i ))∥ 2 2 , (<label>11</label></formula><formula xml:id="formula_12">)</formula><p>where N is the number of the training samples.</p><formula xml:id="formula_13">X i = [e i ; s i ; b i ; d i ] is the ensembled multi-view representation for a job title v i , G(F (X i ))</formula><p>is the restored representation. Minimizing the difference between the raw representations X i and restored representations G(F (X i )) will enforce the model to learn a denser and unified representation F (X i ). CMVAE hopefully captures the distinctive aspects from different views and further reveals the latent correlations across the views. We jointly optimize CMVAE associate with the other individual-view representation graphs. This jointly training strategy could let each graph assistant other graphs and further enhance the learning performance. Finally, we use F (X i ) as the fused multiview representation for subsequent link prediction task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENT RESULTS</head><p>This section details our empirical evaluation of the proposed method on real-world data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Data</head><p>Table <ref type="table" target="#tab_5">4</ref> presents the statistics of our data sets from Information Technology industry and Finance industry. Here we provide more details about our real-world data as follows: IT Data. To construct IT data, we randomly sampled one million user career records who have been working at several well-known IT companies in US. For ease of analysis, we chose 15 most famous and leading companies in IT and only keep transition records of these companies. The 15 companies include Facebook, Google, Amazon, Microsoft, Apple, IBM, LinkedIn, Cisco, Oracle, Airbnb, Uber, Yahoo, Nokia, Apple, Intel, HP. Then we used the methods mentioned and Section 3 to construct the Job-Graph and aggregated it. Finally </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baselines &amp; Evaluation Metrics</head><p>We compare our proposed method with the following representative baselines of representation learning: Deepwalk <ref type="bibr" target="#b12">[13]</ref>: DeepWalk adopted a truncated random walk on a graph to generate a set of walk sequences and train Skip-Gram on these sequences. It only considers graph topology. Node2Vec <ref type="bibr" target="#b1">[2]</ref>: Node2Vec generalizes DeepWalk by defining a more flexible notion of a node's graph neighborhood. It only considers graph topology. LINE(1st order) <ref type="bibr" target="#b16">[17]</ref>: In LINE, first-order and second-order proximity are modeled by the joint probability distribution between two nodes and the similarity between their neighborhood respectively. LINE(1st order) only keeps first-order proximity. It only considers graph topology. LINE(1st+2nd order): This is the full model of LINE. It keeps both first-order and second-order proximity. It only considers graph topology.</p><p>Word2Vec <ref type="bibr" target="#b8">[9]</ref>: Word2Vec only applies semantic view. Specifically, we treat each job title as a sentence, then train Word2Vec on all the job titles on Job-Graph. Finally we get the embedding vector of a job title by averaging the vectors of the words in it. Job2Vec: The model proposed in this paper which considers four crucial aspects (graph topology, semantic, transition balance, transition duration) of the Job-Graph. Metrics: We use two metrics, namely, MRR and MP@K, to evaluate the link prediction performance.</p><p>• For each test i, the correct answer job title is identified at position rank[i] for closest job titles. The Mean Reciprocal Rank (MRR) is Higher MRR means that correct answers appear more closely with the query job title. • Additionally, for test i consisting of a query job title and target job title pair, consider the closest K job titles to the query embedding.</p><formula xml:id="formula_14">MRR = 1 N N i=1 1 rank[i] .<label>(12)</label></formula><p>If the correct target job title to the query job title is among these K titles, then the Precision@K for test i (denoted P@K[i]) is 1; else, it is 0. Then the Mean Precision@K is defined as</p><formula xml:id="formula_15">MP@K = 1 N N i=1 (P@K[i]).<label>(13)</label></formula><p>Higher precision indicates a better ability to acquire correct answers using close embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Link Prediction Performance Comparison</head><p>We performed link prediction on both IT and Finance datasets, i.e., predicting missing links on Job-Graph. Since edges with a larger weight in Job-Graph indicates a better match between job titles, we kept edges that have weight larger than a threshold (here is 5) for training and also try to predict links that have weight larger than 5 (predicted links if have weights lower than 5 will be considered as wrong). Then we randomly split the Job-Graph links into 10 equal parts, 8 of them as training set, 1 as validation set and 1 as testing set, no data in validation set and testing set can be used for training the embeddings. To avoid "cold start", we only kept job titles that have occurred in training data. We trained all the baseline models and our Job2Vec on the training set to get the embeddings, to avoid overfitting we tuned parameters on the validation set, finally we predict links on the testing set using the learned embeddings. Given a job title job i , to predict which job may have links with it, we calculated the cosine similarity score between the embeddings of job i and the rest of other jobs, then ranked them based on the similarity score. Higher ranked jobs have a higher probability to be matched with job i . We obtained the best hyper parameters of our model on the validation set. The dimensions of the four views' representations are N д , N s , N b , N d = 128. In the Collective Multi-View Auto-Encoder (CMVAE), for the fusion encoder, we use a two-layer fully-connected network (512*512*248) with LeakyReLU activation (negative-slope=0.7) in the first layer and Tanh activation before output. For the fusion decoder, we use a two-layer fully-connected network (248*512*512) with LeakyReLU activation (negative-slope=0.7) in the first layer and no activation in the second layer.</p><p>From the results on the IT dataset, as shown in Table <ref type="table" target="#tab_6">5</ref>, we can tell that graph embedding models that only keep graph topology information get similar but poor performances, where DeepWalk achieves the best MRR and MP@K. Word2Vec(averaging word vectors in job title) achieves nearly 100% improvements on MRR and on  MP@K compared with DeepWalk, thus proves that semantic view is tremendously helpful for link prediction on Job-Graph. Job2Vec achieves further improvements on MRR compared with Word2Vec, which shows the effectiveness of preserving more views than only semantic view. Job2Vec achieves the best performance on MRR and MP@5, MP@10, MP@15, MP@20 among all the models. Specifically, it improves 200% over DeepWalk and 50% over Word2Vec. This confirms the superiority of the multi-view representation and the effectiveness of the encode-decode paradigm for fusing multiple views. From the results on the Finance dataset, similar conclusions can be drawn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Robustness Analysis</head><p>In this subsection, we explore the robustness of different models against the sparsity of job transition graph. Specifically, we make the original graph sparser by subsampling the training edges at different rates r = {0.9, 0.8, 0.7, 0.6}(only keep 90%, 80%, 70%, 60% edges). Then we retrain our model and baseline models on the subsampled graph and compare the link prediction performance degradation. Here we use IT dataset as an example. We compare our model Job2Vec with DeepWalk for conciseness. Job2Vec take four views (graph, semantic, transition balance and transition duration view) into account while DeepWalk only considers graph view. From Figure <ref type="figure" target="#fig_6">5</ref>, we can observe that the performance of Deep-Walk degrades sharply as r decreases while Job2Vec seems to hold steady. This again confirms the effectiveness of incorporating more views especially semantic view over the sparsity issue of job transition graph. This can also be well explained: when the graph is sparse, many nodes in graph have poor connectivity, existing graph embedding models can not learn sufficient representations from graph view. But in semantic view, shared key words can connect different job titles in Job-Graph even though when it is sparse. Learning feasible representations from semantic view does not rely on graph connectivity and thus can perform excellent performance over sparse graph. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Visualization</head><p>We visualize the learned representations in Figure <ref type="figure" target="#fig_7">6</ref> to show the promising benchmarking results of our proposed model. For convenience, we select four categories of job title representations, including engineer, sales, consultant and manager. We randomly sampled 1000 job titles for each categories. We utilize t-SNE <ref type="bibr" target="#b17">[18]</ref> to reduce the representation dimensions to do the visualization. Each color corresponds to one category of job titles. Figure <ref type="figure" target="#fig_7">6</ref> shows that our proposed Job2Vec achieves the best results. Each category of representations learned by our model can be clustered into four groups very well. In another word, job titles are benchmarked by our proposed model effectively. However, the representations learned by baselines are distributed randomly in the space which cannot reveal the becnmarking relations among job titles. The potential explanation is that our proposed method jointly model four views to preserve the topology structure, semantics and job transition patterns, which plays an essential role in the task of job title benchmarking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Job Title Benchmarking Case Studies</head><p>In this subsection, we show some Job Title Benchmarking (JTB) results extracted from existing Job Transition Graph as well as some link prediction JTB results generated by our model and baseline models.</p><p>Table <ref type="table" target="#tab_7">6</ref> shows eight JTB cases extracted from the aggregated Job-Graph, apparently job titles that have similar responsibilities and expertise level while also from companies of the same volume are matched. It is interesting that JTB can find some matching pairs that are not similar literally, such as (Software Engineer, Data Scientist), (SWE, Machine Learning Engineer). Table <ref type="table" target="#tab_8">7</ref> shows the Top 3 link prediction results of different models for "Software Engineer-Facebook". Titles in bold font are wrong predictions. It can be observed that DeepWalk tends to make predictions that have similar neighborhood structure in Job-Graph, while Word2Vec inclines to job titles that are semantically similar. With only graph topology view, DeepWalk may make anomalous predictions such as "Product Manager-Microsoft". With only semantic view, Word2Vec are likely to predict repetitive job titles and miss some interesting matching pairs that are not very similar literally, such as (Software </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>In this section, we review two categories of literatures that are related to this paper, namely research on data mining for career trajectory analysis, and research on graph embedding. Data Mining for Career Trajectory Analysis. With the rise of Online Professional graphs (OPNs), Career Trajectory Analysis have been proved useful in many human resource management (HRM) problems <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b27">28]</ref>. For example, Xu et al. build a organizational level job transition graph from OPN data and proposed a talent circle detection method to identify the right talent sources for recruitment <ref type="bibr" target="#b26">[27]</ref>. For better assessing the expertise level or rank of a job title, Xu et al. proposed a Gaussian Bayesian graph to extract the job title hierarchy of an organization from employees' career trajectory data <ref type="bibr" target="#b25">[26]</ref> . In <ref type="bibr" target="#b5">[6]</ref>, a contextual LSTM model is proposed to accurately predict an employee's next career move. However, few existing works pay attention to the problem of Job Title Benchmarking(JTB), which has broad application prospect in human resource management. To the best of our knowledge, we are the first to extract JTB insights from job transition graph.</p><p>Graph Representation learning graph representation learning assigns nodes in a graph to low-dimensional representations and effectively preserves the graph structure. Recently, a significant Project Manager Lead-IBM, Advisory Project Manager-IBM, Project Manager-Microsoft amount of progresses have been made toward this emerging graph analysis paradigm <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>. Inspired by the success of representation learning in natural language processing <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>, DeepWalk <ref type="bibr" target="#b12">[13]</ref> is the first extension of Word2Vec to graph analysis. It uses random walks to sample paths from a graph and treat paths as "sentences" to train a SkipGram model to keep graph Proximity into learned embeddings. Node2Vec <ref type="bibr" target="#b1">[2]</ref> generalizes DeepWalk by designing a more flexible random walk strategy. LINE <ref type="bibr" target="#b16">[17]</ref> proposes first-order and second-order proximity to keep graph properties and combines both by concatenating first-order and second-order vectors. To better model the asymmetric property of graphs, Ou et al. proposes asymmetric proximity preserving (APP) graph embedding <ref type="bibr" target="#b11">[12]</ref>. However, in our problem setting, we need to jointly model multi-view representations, and obtain an unified representation fused from multi-view. Current graph representation learning cannot be directly applied into the JTB scenario. To the best of our knowledge, our work is the first attempt to solve the JTB problem via multi-view graph representation learning.</p><p>Multi-View Representation learning has become attractive and urgent as the increasing multi-modal sensors are widely deployed in a great number of real-world applications <ref type="bibr" target="#b24">[25]</ref>. It explores the complementary information among different views, where the views refer to various feature representations, modalities or sensors. Most of the approaches focus on the multi-view data including features <ref type="bibr" target="#b10">[11]</ref>, images <ref type="bibr" target="#b9">[10]</ref>, and videos <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>, while our approach reveal the information from multiple graph structure data which is challenging.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Job transitions of different subjects across companies and titles. Our approach aims to explore the multiple clues for high performance on job title benchmarking task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of our job title benchmarking framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Intuition 3 :</head><label>3</label><figDesc>Job Transition Patterns Preservation. Job transitions have unique latent patterns. Transitions among different job title pairs are also different. Consequently, we should preserve the job transition patterns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Words frequency distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Collective Multi-View Representation Learning</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Robustness Comparison</figDesc><graphic url="image-41.png" coords="7,318.12,193.47,117.72,88.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Visualization of the learned representations Engineer-Facebook, SDE-Microsoft). Our model, Job2Vec, incorporating graph topology, semantic, transition balance and transition duration views, makes more reasonable predictions.</figDesc><graphic url="image-48.png" coords="8,446.29,298.78,108.11,81.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>An example of job transitions.</figDesc><table><row><cell>Job Title</cell><cell>Company</cell><cell>Duration</cell></row><row><cell>Production Engineer</cell><cell cols="2">Square Inc 2011/7-2016/10</cell></row><row><cell>Senior Site Reliability Engineer</cell><cell>Google</cell><cell>2010/10-2011/7</cell></row><row><cell>Architect</cell><cell>Yahoo!</cell><cell>2009/7-2010/6</cell></row><row><cell>Systems Engineer</cell><cell>Yahoo!</cell><cell>2006/6-2009/7</cell></row><row><cell>Systems Engineer</cell><cell>IBM</cell><cell>2006/2-2006/6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Statistics of Job-Graphs in IT and Finance.</figDesc><table><row><cell>Dataset</cell><cell>IT</cell><cell>Finance</cell></row><row><cell># Edges</cell><cell>39927</cell><cell>77118</cell></row><row><cell># Nodes</cell><cell>44030</cell><cell>89851</cell></row><row><cell>Average out degree</cell><cell>0.91</cell><cell>0.86</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>SWE Sourcing Buyer (a) Individual Career Profiles (b) Job titles aggregation (d) Collective Multi-view Representation Learning (c) Snapshot on Job-Graph</head><label></label><figDesc>Definition 2.2. Job Title Transition Graph (Job-Graph). Job-Graph is defined as a directed graph G = (V , E), where each node v i ∈ |V | represents a job title affiliated a company(i.e., v i = (Title i , Company i )), and each link e i j ∈ |E| between two nodes v i</figDesc><table><row><cell>SWE-Azure SWE-WinXP</cell><cell cols="2">Test Engineer</cell><cell>Software Engineer</cell><cell></cell><cell></cell><cell>Test Engineer</cell><cell>Software Engineer</cell></row><row><cell>SWE-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Android</cell><cell></cell><cell>Project</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Leader</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Tactical Sourcing Buyer Sourcing Buyer FSOS Sourcing Buyer Unilever</cell><cell>Staff Senior Technical</cell><cell cols="2">Technical Lead Engineer System Senior Staff</cell><cell>Semantic Transition Duration Transition Balance Network</cell><cell>Multi-view Fusion</cell><cell>Technical Lead Project Senior Leader Staff</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>e) Link prediction on Job-Graph Single-view Learning</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Examples of aggregating job titles.</figDesc><table><row><cell>Original Job Titles</cell><cell>Aggregated Job Title</cell></row><row><cell>Tactical Sourcing Buyer (Unilever) Sourcing Buyer, MARCOM &amp; FSOS</cell><cell>Sourcing Buyer</cell></row><row><cell>Software Design Engineer-</cell><cell></cell></row><row><cell>(Azure)</cell><cell></cell></row><row><cell>Software Design Engineer-WindowsXP</cell><cell>Software Design Engineer</cell></row><row><cell>Software Design Engineer-</cell><cell></cell></row><row><cell>(Contracting) Encarta</cell><cell></cell></row><row><cell>Cyber Security Architect Security Architect</cell><cell>Security Architect</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc>Graph shown in Table 4. The time span of the data is from 06/18/1998 to 12/01/2018. Finance Data. For finance data, we randomly sampled one million records according to the rule: whose records contains finance related keywords such as Finance, Asset Manager, Financial Research Analyst, Investment Banking Analyst, Equity Research Analyst, Trust Officer, Commercial Banker, etc. Then again we used the methods mentioned in Section 3 to construct the Job-Graph and aggregated it. Finally we got the Finance Job-Graph shown in Table 4. The time span of the data is from 03/20/2004 to 12/01/2018.</figDesc><table><row><cell></cell><cell cols="3">: Statistic Details of the Dataset</cell><cell></cell></row><row><cell></cell><cell>IT</cell><cell></cell><cell cols="2">Finance</cell></row><row><cell></cell><cell>train</cell><cell>test</cell><cell>train</cell><cell>test</cell></row><row><cell># Nodes</cell><cell>44,030</cell><cell>2,838</cell><cell>89,851</cell><cell>5,512</cell></row><row><cell># Edges</cell><cell>38,133</cell><cell>1,794</cell><cell>72,140</cell><cell>4,978</cell></row><row><cell># Transitions</cell><cell>45,095</cell><cell></cell><cell>92,085</cell><cell></cell></row><row><cell># Job titles</cell><cell>44,030</cell><cell></cell><cell>89,851</cell><cell></cell></row><row><cell># Companies</cell><cell>15</cell><cell></cell><cell>7,669</cell><cell></cell></row><row><cell># Time span</cell><cell cols="2">1998/06-2018/12</cell><cell cols="2">2004/03-2018/12</cell></row><row><cell>Job title examples</cell><cell cols="2">Software Engineer Google</cell><cell cols="2">Asset Manager Goldman Sachs</cell></row><row><cell>we got the IT Job-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Link Prediction Performance Comparison.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>IT Dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Finance Dataset</cell></row><row><cell></cell><cell>MRR</cell><cell cols="4">MP@5 MP@10 MP@15 MP@20</cell><cell>MRR</cell><cell cols="3">MP@5 MP@10 MP@15 MP@20</cell></row><row><cell>DeepWalk</cell><cell cols="2">0.0688 0.0858</cell><cell>0.1070</cell><cell>0.1198</cell><cell>0.1293</cell><cell cols="2">0.1044 0.1164</cell><cell>0.1444</cell><cell>0.1600</cell><cell>0.1675</cell></row><row><cell>Node2Vec</cell><cell cols="2">0.0645 0.0785</cell><cell>0.0925</cell><cell>0.1042</cell><cell>0.1153</cell><cell cols="2">0.0979 0.1065</cell><cell>0.1235</cell><cell>0.1335</cell><cell>0.1460</cell></row><row><cell>Line(1st order)</cell><cell cols="2">0.0644 0.0752</cell><cell>0.0947</cell><cell>0.1081</cell><cell>0.1198</cell><cell cols="2">0.0983 0.1071</cell><cell>0.1245</cell><cell>0.1341</cell><cell>0.1428</cell></row><row><cell cols="3">Line(1st+2nd order) 0.0651 0.0791</cell><cell>0.0958</cell><cell>0.1064</cell><cell>0.1125</cell><cell cols="2">0.0943 0.0994</cell><cell>0.1150</cell><cell>0.1274</cell><cell>0.1381</cell></row><row><cell>Word2Vec</cell><cell cols="2">0.1295 0.2135</cell><cell>0.2792</cell><cell>0.3194</cell><cell>0.3334</cell><cell cols="2">0.1110 0.1239</cell><cell>0.1452</cell><cell>0.1643</cell><cell>0.1775</cell></row><row><cell>Job2Vec</cell><cell cols="3">0.1734 0.2391 0.2976</cell><cell>0.3288</cell><cell cols="4">0.3511 0.1311 0.1378 0.1635</cell><cell>0.1815</cell><cell>0.1978</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Job Title Benchmarking Cases.</figDesc><table><row><cell>IT</cell><cell></cell></row><row><cell>Project Manager</cell><cell>Product Manager PC Accessories</cell></row><row><cell>-IBM</cell><cell>-Microsoft</cell></row><row><cell>IT Support Lead and System Trainer</cell><cell>IT Support Lead</cell></row><row><cell>-HP</cell><cell>-IBM</cell></row><row><cell>SWE -Google</cell><cell>Machine Learning Engineer -Airbnb</cell></row><row><cell>Software Engineer</cell><cell>Data Scientist</cell></row><row><cell>-Facebook</cell><cell>-Microsoft</cell></row><row><cell cols="2">Finance</cell></row><row><cell>Investment Banking Analyst</cell><cell>Investment Banking Analyst</cell></row><row><cell>-Citi</cell><cell>-J.P. Morgan</cell></row><row><cell>Equity Research Analyst</cell><cell>Equity Research Analyst</cell></row><row><cell>-Nomura</cell><cell>-Goldman Sachs</cell></row><row><cell>Financial Analyst</cell><cell>Financial Analyst</cell></row><row><cell>-Goldman Sachs</cell><cell>-Rushmark Properties</cell></row><row><cell>Portfolio Manager</cell><cell>Portfolio Manager</cell></row><row><cell>-WellsFargo</cell><cell>-ReMark Capital Group</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Top 3 Results of Link Prediction Comparison. Microsoft, Software Development Engineer-Amazon, Software Developer-Google DeepWalk Software Developer-Google, Product Manager-Microsoft, Software Test Engineer-Google Word2Vec Senior Software Engineer-Facebook, Software Engineer-IBM, Software Engineer-Apple</figDesc><table><row><cell>Top 3 results</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>In this paper, we propose a data-driven solution for the problem of job title benchmarking (JTB). We construct the job title transition graphs (Job-Graph) to represent job transitions, and reformulate the JTB problem as the task of link prediction over the Job-Graph. Specifically, we propose a collective multi-view representation learning method by jointly learning the four views of representations, including graph topology view, semantic view, job transition balance view, and job transition duration view. Besides, we also propose a encode-decode based fusion method to obtain an unified representation from the multi-view representations. We present intensive experimental results with IT and finance related job transition data to demonstrate the effectiveness of our method.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work is supported by NSFC 91746301. The code is available at: https://github.com/zdh2292390/Job2Vec-Job-Title-Benchmarkingwith-Collective-Multi-View-Representation-Learning</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Heterogeneous network embedding via deep architectures</title>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charu</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGKDD</title>
				<meeting>ACM SIGKDD</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="119" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGKDD</title>
				<meeting>ACM SIGKDD</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
				<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Signed networks in social media</title>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Huttenlocher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCHI. ACM</title>
				<meeting>ACM SIGCHI. ACM</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1361" to="1370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Prospecting the career development of talents: A survival analysis perspective</title>
		<author>
			<persName><forename type="first">Huayu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengshu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongke</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGKDD</title>
				<meeting>ACM SIGKDD</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="917" to="925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Nemo: Next career move prediction with contextual embedding</title>
		<author>
			<persName><forename type="first">Liangyue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">How</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewon</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bee-Chung</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on World Wide Web Companion</title>
				<meeting>International Conference on World Wide Web Companion</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="505" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Link Prediction in Knowledge Graphs: A Hierarchy-Constrained Approach</title>
		<author>
			<persName><forename type="first">Manling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denghui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yantao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Yuanzhuo None Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Big Data</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<title level="m">Efficient estimation of word representations in vector space</title>
				<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
				<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-view clustering and semisupervised classification with adaptive neighbours</title>
		<author>
			<persName><forename type="first">Feiping</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
				<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Parameter-Free Auto-Weighted Multiple Graph Learning: A Framework for Multiview Clustering and Semi-Supervised Classification</title>
		<author>
			<persName><forename type="first">Feiping</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCAI</title>
				<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1881" to="1887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Asymmetric transitivity preserving graph embedding</title>
		<author>
			<persName><forename type="first">Mingdong</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGKDD</title>
				<meeting>ACM SIGKDD</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1105" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGKDD</title>
				<meeting>ACM SIGKDD</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Enhancing person-job fit for talent recruitment: An abilityaware neural network approach</title>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengshu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGIR</title>
				<meeting>ACM SIGIR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="25" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A Joint Learning Approach to Intelligent Job Interview Assessment</title>
		<author>
			<persName><forename type="first">Dazhong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengshu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCAI</title>
				<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3542" to="3548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scalable proximity estimation and link prediction in online social networks</title>
		<author>
			<persName><forename type="first">Hee</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tae</forename><forename type="middle">Won</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vacha</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lili</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCOMM</title>
				<meeting>ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="322" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WWW. International World Wide Web Conferences Steering Committee</title>
				<meeting>WWW. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Accelerating t-SNE using tree-based algorithms</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="3221" to="3245" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Structural deep network embedding</title>
		<author>
			<persName><forename type="first">Daixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGKDD</title>
				<meeting>ACM SIGKDD</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1225" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning Transferable Subspace for Human Motion Segmentation</title>
		<author>
			<persName><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Low-Rank Transfer Human Motion Segmentation</title>
		<author>
			<persName><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1023" to="1034" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generative Multi-View Human Action Recognition</title>
		<author>
			<persName><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
				<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taotao</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12602</idno>
		<title level="m">EV-Action: Electromyography-Vision Multi-Modal Action Dataset</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Predictive network representation learning for link prediction</title>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengyao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGIR</title>
				<meeting>ACM SIGIR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="969" to="972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1304.5634</idno>
		<title level="m">A survey on multi-view learning</title>
				<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Extracting Job Title Hierarchy from Career Trajectories: A Bayesian Perspective</title>
		<author>
			<persName><forename type="first">Huang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingfei</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCAI</title>
				<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3599" to="3605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Talent circle detection in job transition networks</title>
		<author>
			<persName><forename type="first">Huang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengshu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGKDD</title>
				<meeting>ACM SIGKDD</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="655" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dynamic Talent Flow Analysis with Deep Sequence Prediction Modeling</title>
		<author>
			<persName><forename type="first">Huang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengshu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TKDE</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Efficient parallel translating embedding for knowledge graphs</title>
		<author>
			<persName><forename type="first">Denghui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yantao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10316</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">BL-MNE: emerging heterogeneous social network embedding through broad learning with aligned autoencoder</title>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congying</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDM</title>
				<meeting>ICDM</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="605" to="614" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
