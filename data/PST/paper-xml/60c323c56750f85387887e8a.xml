<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">OAG know : Self-supervised Learning for Linking Knowledge Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
							<email>liuxiao17@mails.tsinghua.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Li</forename><surname>Mian</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Fanjin</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
							<email>zhang-jing@ruc.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">Yuxiao Dong and Kuansan Wang are with Microsoft Research</orgName>
								<address>
									<postCode>98052</postCode>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Jibing Gong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
							<email>jietang@tsinghua.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Ieee</forename><surname>Fellow</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Peng</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jibing</forename><surname>Gong</surname></persName>
							<email>gongjibing@163.com</email>
						</author>
						<author>
							<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
							<email>kuansan.wang@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Evgeny</forename><surname>Kharlamov</surname></persName>
							<email>evgeny.kharlamov@de.bosch.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Department of Information Science and Engineer-ing</orgName>
								<orgName type="institution">Yanshan University</orgName>
								<address>
									<settlement>Qinhuangdao</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Robert Bosch GmbH</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="department">Deptment of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University and Tsinghua-Bosch Joint ML Center</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">OAG know : Self-supervised Learning for Linking Knowledge Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Concept Linking</term>
					<term>Self-supervised learning</term>
					<term>Contrastive Learning</term>
					<term>Knowledge Base</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a self-supervised embedding learning framework-SelfLinKG-to link concepts in heterogeneous knowledge graphs. Without any labeled data, SelfLinKG can achieve competitive performance against its supervised counterpart, and significantly outperforms state-of-the-art unsupervised methods by 26%-50% under linear classification protocol. The essential components of SelfLinKG are local attention-based encoding and momentum contrastive learning. The former aims to learn the graph representation using an attention network, while the latter is to learn a self-supervised model across knowledge graphs using contrastive learning. SelfLinKG has been deployed to build the the new version, called OAG know of Open Academic Graph (OAG). All data and codes are publicly available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Concept linking, with the goal of linking concepts of the same meaning, is critical for document-based data systems such as Academic Search (AMiner, Microsoft Academic Graph) and Question Answering Platform (Reddit, StackOverflow), where knowledge bases containing concepts and their relations are independently developed within each system to help complicated searching and reasoning. Usually, these knowledge bases are incomplete, and to complement each other via concept linking is important for advanced applications. For example, the topic classification of academic papers in AMiner depends on concept extraction in paper abstract. But the incompleteness of concept taxonomy and lack of concepts' text descriptions leads to bad performance. For instance, we cannot literally distinguish ambiguous concepts such as "entropy" in physics and "entropy" in information science. In the past decades, quite a few approaches have been proposed to address several related topics, such as entity linking <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b45">[46]</ref>, schema matching <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, Fig. <ref type="figure">1</ref>: An example of concept linking between the Open Academic Graph (OAG) and Wikipedia."Artificial intelligence" is a research concept in OAG, which is linked with the entry https://en.wikipedia.org/wiki/Artificial intelligence in Wikipedia.</p><p>entity resolution <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b44">[45]</ref>, and ontology alignment <ref type="bibr" target="#b27">[28]</ref>.</p><p>However, the problem of linking knowledge at the Webscale remains an open question. Most of the aforementioned methods can only tackle the concept linking problem at relatively small-scale. In <ref type="bibr" target="#b43">[44]</ref>, the authors present a deep learning method-LinKG-to generate links between two of the largest publicly available academic graphs. However, the method operates in a (semi-)supervised way and requires sufficiently labeled data, usually infeasible when dealing with concept linking in the open Web. Moreover, the ambiguity issue with noise makes the problem more severe.</p><p>Current supervised embedding-based alignment algorithms fail to solve these problems. To enable large-scale and label-efficient concept linking, we have to develop a new method and evaluate it on publicly available large-scale heterogeneous knowledge bases. In this work, we make an attempt with both research and deployment purposes, to link research concepts (a.k.a., fields of study) in the Open Academic Graph (OAG) and concept entries in (En-. OAG know data: https://aminer.cn/oag know/ . Code: https://github.com/Xiao9905/OAG know glish) Wikipedia. OAG <ref type="bibr" target="#b43">[44]</ref>, which consists of the Microsoft Academic Graph (MAG) <ref type="bibr" target="#b25">[26]</ref> and the AMiner Academic Graph <ref type="bibr" target="#b28">[29]</ref>, contains more than 700 million entities and 2 billion relationships, making it the largest publicly available academic entity graph to date. For the research purpose, we can have suitable large-scale datasets for evaluations. For deployment purpose, the OAG entity graph has several types of entities: papers, authors, institutions, conferences, journals, and research concepts. To tag the concepts to papers, e.g., to determine whether a paper is about "data mining", sufficient semantic information about the concept entities is desirable. Therefore, we propose to enrich OAG concepts' semantic information by linking them with the same concept entries in Wikipedia. Figure <ref type="figure">1</ref> illustrates an example of the "Artificial intelligence" concept in OAG. In OAG, concepts are crawled from the Internet and then organized into taxonomy by calculating their pairwise subsumption (a form of co-occurrence) from millions of academic papers <ref type="bibr" target="#b24">[25]</ref>. Thus the organization of this taxonomy is a bit different from Wikipedia. In addition, only 1/3 of the concepts are associated with Wikipedia entries (using the crawled URLs), leaving the rest of them with very little semantic information. From the figure, we can see at least two types of relations in OAG: hypernym and related. How to address and leverage the relation heterogeneity to improve concept linking is largely unexplored. In addition, there are many ambiguous and similar concepts. For the ambiguity issue, an example is that there are four different "entropy" entries in Wikipedia. For confusingly similar concepts, an example can be found between "artificial neural networks" in machine learning and "neural networks" in neuroscience in OAG. Therefore, the concept linking task must deal with these inherent issues. Finally, the OAG concept taxonomy covers over 679,921 concepts. To link OAG with Wikipedia, (semi-)supervised methods require massive label data, which is usually arduous and expensive to obtain.</p><p>In light of these issues and challenges, we propose a selfsupervised representation learning framework-SelfLinKGto link concepts between OAG and Wikipedia. The core idea is to leverage self-supervised contrastive learning <ref type="bibr" target="#b11">[12]</ref> to learn the intrinsic relations between different parts of the data. SelfLinKG compromises two key components: local attention-based encoding and global momentum contrastive learning.</p><p>The local attention-based encoding, which is based on graph attention networks <ref type="bibr" target="#b34">[35]</ref>, focuses on incorporating heterogeneous information within a single graph, including neighborhood context and hierarchy context. The global momentum contrastive learning aims at teaching the encoder to learn shared critical features across multiple graphs without labels. With the instance discrimination as pretraining task and contrastive loss, the designed encoder can learn features that distinguish ambiguous concepts from the target concept in an self-supervised manner. With the shared encoder mechanism, the representations are forced to be effective across the two graphs. With the momentum update, the fluctuating training of self-supervised is therefore stablized.</p><p>To summarize, our work makes the following contributions:</p><p>• First, we propose to study the concept linking problem across multiple large-scale knowledge bases, specifically between the large-scale OAG and Wikipedia. The challenges of this problem are identified: various relations, ambiguous concepts, scarce label data, and scalability. • Second, to address them, we present a self-supervised embedding learning framework for concept linking-SelfLinKG--which leverages state-of-the-art deep learning techniques for learning semantic and structural representations, even competitive against its supervised counterpart.</p><p>• Third, we conduct extensive experiments on OAG and Wikipedia, which suggest that SelfLinKG can achieve very high accuracy of 97.33% in the real application, significantly outperforming baseline models.</p><p>• Finally, together with the linking results, we make the Open Academic Graph with Knowledge (OAG know ) publicly available. OAG know consists of 93 million concepts, which can be used for various research problems such as text mining, question answering, and knowledge reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">THE CONCEPT LINKING PROBLEM</head><p>In this section, we formalize the problem of concept linking across knowledge bases and present the task of linking academic graphs with Wikipedia. Knowledge bases (KBs) comprise both structured and unstructured information. Broadly speaking, it could be a taxonomy, an encyclopedia, or a knowledge graph. Formally,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 1. Knowledge Base (KB)</head><p>A KB is defined as a graph G = {C, R, A}, where the concept c ∈ C contains semantic attributes and the relation r ∈ R is associated with its type mapping functions φ(r) : R → D with |D| &gt; 1, where D is the set of relation types (e.g. D = {hypernym, related}), and A = {c i , r ij , c j } is the adjacency set recording the connectivity between concepts.</p><p>For example, Wikipedia is a KB comprising of concepts (entries), and its relation set includes 1) the hypernym relations between concepts with a broad meaning and those with more specific meaning and 2) the related relations between concepts sharing weaker association. Compared with Wikipedia, taxonomies usually contain only the hypernym relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2. Concept Linking across KBs</head><formula xml:id="formula_0">Given m knowledge bases KB p (p = 1, • • • , m, with m &gt; 1), the goal is to generate concept linkings L = {(c (p) i , c (q) j )|c (p) i ∈ KB p , c (q) j ∈ KB q , p = q} such that c (p) i</formula><p>and c (q) j represent exactly the same concept in KB p and KB q .</p><p>In this work, we focus on the problem of concept linking between two public knowledge bases-the Open Academic Graph (OAG) <ref type="bibr" target="#b43">[44]</ref> and the English Wikipedia. OAG is to date the largest publicly available academic graph. It contains five types of entities, including papers, authors, affiliations, venues (journals and conferences), and research concepts (topics). The goal here is to link OAG's concepts with Wikipedia's concept entries by using 1) OAG's concept taxonomy and concept content, and 2) Wikipedia's content and entry taxonomy.</p><p>The problem is challenging, as it is difficult to acquire sufficiently labeled data to train an effective machine learning model. To deal with this issue, we desire to have a powerful unsupervised or semi-supervised model. Second, there is also the name disambiguation issue. For example, there are four different "entropy" entries in Wikipedia, and how to link different ones with those in academic graphs is challenging. Finally, the model needs to handle the scalability, as to train and deploy a model to deal with thousands of millions of concepts is not an easy task.</p><p>To deal with the aforementioned issues, especially the first one, we further clarify the supervised and unsupervised (or self-supervised) embedding learning for concept linking in the following definition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 3. Embedding Learning for Concept Linking</head><p>Given m knowledge bases represented as m graphs</p><formula xml:id="formula_1">G p = {C p , R p , A p }(p = 1, • • • , m), a embedding function f : c|G → R d is learned such that for each concept c (p) i ∈ C p , embedding v (p) i = f (c (p)</formula><p>i |G p ) could be efficiently utilized to recover the full concept linkings L = {(c</p><formula xml:id="formula_2">(p) i , c (q) j )|c (p) i ∈ C p , c (q) j ∈ C q , p = q} in:</formula><p>1) Supervised setting: part of L is provided as the training set for training f . 2) Unsupervised (Self-supervised) setting: none of L is provided for training f .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE SELFLINKG FRAMEWORK</head><p>In this section, we present the self-supervised embedding learning framework-SelfLinKG-for linking concepts across knowledge bases. We will first discuss the motivation of SelfLinKG and then introduce its two components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motivation</head><p>In related fields of concept linking, such as entity alignment, embedding-based methods are generally based on supervised learning. Supervised learning has achieved great success in the last decade, but it suffers from heavy dependency on manual labels and poor scalability on unseen data. These problems are especially fatal to large-scale concept linking and entity alignment. A large amount of manually labeled data is too expensive, and to make the linking system online, we need to make the algorithm scalable. Despite the drawbacks of supervised learning, however, previously people have few choices but to choose it because of two important reasons as shown in Figure <ref type="figure" target="#fig_0">2:</ref> 1) Lack of embedding consistency. For concepts in different KBs, their representations are located in different and inconsistent embedding spaces (just like two people using two languages). To make their embeddings consistent, we can either use a supervised classifier to bridge the gap (a translator <ref type="bibr" target="#b15">[16]</ref>) or let them fall into the same embedding space by anchor nodes (both turn to use the third language <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b44">[45]</ref>). Both methods require external supervision. 2) Lack of training objective. In supervised learning, labels serve as objectives for encoders to draw near positive samples and push away negative samples.</p><p>Without labels, such a goal seems to be impossible because we can not draw near positive pairs. Are there any means to cope with these problems, or part of them, without labels? Fortunately, recent breakthroughs in self-supervised learning shed light on this question.</p><p>In terms of embedding consistency, if KBs are in the same language, we can leverage the inherent embedding space of it. Instead of using word embeddings trained separately on different KBs, pre-trained language models such as BERT can provide a unified initial embedding space for concepts from different KBs. During the training, a shared encoder that yields embeddings for concepts from different KBs will further ensure the consistency.</p><p>In terms of training objectives, without labels, we cannot draw near positive sample pairs. However, there are always abundant negative samples. If we can push away negative samples from each other as much as possible, it equals we relatively draw near positive ones that share similarity to some extent. The instance discrimination pretext task with contrastive loss are born for that purpose.</p><p>To sum up, we propose SelfLinKG, a concept learning framework to deal with the large-scale heterogeneous concept linking problem without an arduously expensive process for producing massive labeled data. We propose to leverage self-supervised learning to learn the intrinsic relations between concepts across the two knowledge bases, which also help mitigate the scalability issue for handling large-scale data. In the following sections, we will introduce two components that SelfLinKG comprises of in details: 1) local attention-based encoding and 2) global momentum contrastive learning. Figure <ref type="figure" target="#fig_1">3</ref> illustrates the architecture of SelfLinKG.</p><p>Local Attention-based Encoding. The local attention-based encoding aims to tackle the data heterogeneity and map both data into the same latent space at both entity-level and graphlevel. For entity-level, both semantic information and structural information are involved. We design a heterogeneous graph-attention-based encoder to aggregate information from the taxonomy structures (both hierarchy and neighborhood). For graph-level, we formulate taxonomies, encyclopedias, and knowledge graphs into unified attributed graphs with two types of relations (hyponym and related) to simplify the problem.</p><p>Global Momentum Contrastive Learning. After encoding concepts' into vectors in the first step, we propose to use a self-supervised representation learning solution to link concepts across the two datasets. This solution requires a proper self-supervised training task and objective function. Note that under this setting, there is no existing link across the graphs. Inspired by recent progress on self-supervised contrastive learning in computer vision <ref type="bibr" target="#b11">[12]</ref>, we propose to use instance discrimination as the pre-trained task and leverage momentum contrastive learning as the objective. We use an online encoder (updated by direct gradient descends) and a target encoder (updated by momentum), which are trained across both graphs, to ensure the training stability and that embeddings are projected into the same embedding space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Local Attention-based Encoding</head><p>We introduce the local attention-based encoder, which aims to efficiently capture semantic and structural information by learning to aggregate information from concepts. The encoder comprises three components: semantic embedding generation, neighborhood aggregation, and hierarchy aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic embedding generation.</head><p>For a concept in the knowledge base, we embed its semantic information into the latent space. Traditionally, we can initialize embeddings randomly or train phrase-level embeddings on each KB using methods like doc2vec <ref type="bibr" target="#b12">[13]</ref>. However, when it comes to largescale applications across multiple knowledge bases, such methods are computationally expensive and can not easily scale up to concepts absent in training corpus. Additionally, semantic embeddings trained separately in each KB will spoil the embedding consistency we have discussed in Section 3.1 that is critical for a self-supervised setting.</p><p>Therefore, we adopt the recent pre-trained language model BERT <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b40">[41]</ref> as the basic encoder, which is pretrained over vast amounts of corpora (including whole Wikipedia) and has been proved to be effective for almost all the language tasks. With the help of BERT, concepts with similar semantics in different KBs will still be embedded into similar vectors and maintain the embedding consistency. What is more, since many concepts are professional terms, the conventional embedding methods such as GloVe fails to cover them. But BERT's sub-token technique, which split an unknown token into known sub-tokens, can preserve the semantics in these terms.</p><p>For most concepts, such as in many taxonomies, their names are the only semantic information we can utilize, and the semantic embedding h i is generated by: h i = encoder(name i ). When there are attributes available, we can extend the representation by using aggregators like pooling or soft attention mechanisms <ref type="bibr" target="#b30">[31]</ref>.</p><p>In detail, for a sequence to encode, we turn it into lowercase with the max sentence length as 25 with padding for shorter ones. Notice that we allow for special tokens in BERT tokenizer. Since BERT is a bidirectional transformer, the encoded output is also a sequence with the same length as the original sequence. To transform this sequence to a fix-length vector as sequence embedding, follow tradition, we apply the average pooling from the second output to the final output. The first output is the embedding of the special token [CLS] in BERT, meaning start of the sequence, which is usually used for natural language understanding task such as text classification. In our setting it is discarded because we want embeddings that represent token-level information rather than sentence-level. The BERT output units have a dimension of 768, to transform into a given dimension for other networks, we apply the max pooling and reduce the dimension to 150.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neighborhood aggregation.</head><p>In real-world applications, raw semantic information is usually minimal and does not contain sufficient information for high-quality links. Therefore, structural contexts are often utilized to generate high quality and distinguishable embeddings. To put correct weights on a concept's neighbors, we propose to use the multi-head graph attention networks <ref type="bibr" target="#b34">[35]</ref> with trainable unique embeddings.</p><p>Given a concept c (p) i in knowledge base KB p (for short, we use c i ), the goal of neighbor aggregation is to learn the attention coefficient attn(c i , c j ), which implies the aggregation weight of a concept c j 's influence on target concept c i . The attention coefficient is learned by the self-attention mechanism:</p><formula xml:id="formula_3">o ij = attn(W h i , W h j )<label>(1)</label></formula><p>where o ij indicates the importance of concept c j 's features to concept c i , h i is concept c i 's semantic embedding, and W is a shared projection matrix. By utilizing the neighborhood structure, the graph attention layer only needs to compute o ij for concepts c j that have the related relations with c i , i.e. concepts c j ∈ N i , where N i is the neighborhood of node c i .</p><p>Then o ij can be normalized across all possible c j by using the softmax function</p><formula xml:id="formula_4">α ij = exp(LeakyReLU(W h i + W h j )) k∈Ni exp(LeakyReLU(W h i + W h k ))<label>(2)</label></formula><p>Then we employ the multi-head attention to generate concept c i 's output embedding h i . Here we continue to use c (p) i and h (p) i to emphasize that they only involve information from KB p :</p><formula xml:id="formula_5">h (p) i = f c( K k=1 σ( j∈Ni α k ij W k h h (p) j ))<label>(3)</label></formula><p>where represents the concatenation operation, f c(•) is fullyconnected neural networks, σ is the activation function, W h denotes specified projection matrix for semantic embedding, N i is the neighborhood of node c i , and K is the head number. In practice, consider the average degree of 2.56 in MAG and 22.24 in EnWiki (see details in Section 4.1), we sampling a fixed number of 20 neighbor nodes as the N i for concept c i and employ a 2-hop multi-head attention networks to encode the embedding. Since the |N i | is fixed, when the 1-hop neighbors of c i are not enough, we will continue to sample 2-hop and even k-hop neighbors if needed.</p><p>The reason for using multi-head attentions are as follow. Analogous to different kernels and channels in convolutional neural networks, different attention heads in attention networks captures different patterns, while a single attention head could only captures one pattern and leads to unstable and poor performance, which has been proved in various work <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b36">[37]</ref>. Up until now, we follow the traditional setting of graph attention networks. However, this will lead to several problems:</p><p>1) Semantic embedding is static during the training, but it should be trainable to adapt to better representation. 2) Ambiguous concepts (i.e., with similar or the same name) could not be well distinguished. 3) Under heterogeneous multi-graph settings, synonym concepts should be linked, but they actually have very different semantic embedding.</p><p>To solve the tough problems above, beyond semantic embedding, we introduce unique embedding. For each concept c (p) i , we create a unique trainable embedding u i . In the self-supervised mode, every concept has its own unique embedding. Therefore, besides W h we create another set of projection matrix W u for aggregating the unique embedding as</p><formula xml:id="formula_6">u i = f c( K k=1 σ( j∈Ni α k ij W k u u j ))<label>(4)</label></formula><p>and the final output of neighborhood aggregation is to concatenate the aggregation results of semantic embedding and unique embedding and pass to a fully-connected layers</p><formula xml:id="formula_7">v (p) i = f c([h (p) i u i ])<label>(5)</label></formula><p>The auxiliary unique embedding successfully solves the problems above. First, unique embedding could now be trained to provide concepts with dynamic adaption. Second, ambiguous concepts could gain a distinct representation through differentiate unique embedding of themselves while not changing the original semantic embedding. Finally, synonym concepts could narrow the gap of their representation by having a shared or similar unique embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hierarchy aggregation.</head><p>In this part, we introduce the idea of extending the hierarchical context of a concept using breadthfirst-search (BFS) and applying a similar graph attention mechanism in neighborhood aggregation to perform the hierarchical aggregation.</p><p>First, we define the hierarchical context to be the hypernym (parent concepts) context rather than the hyponym (children concepts) because children concepts are usually noisy and excessive. In practice, we view children concepts as ordinary related concepts, and encode them in the neighborhood aggregation step.</p><p>The intuition for extending hierarchical context is that hierarchical structure in different knowledge bases is heterogeneous. For instance, in MAG the concept Machine learning has the parent concept Computer science. However, in EnWiki this structure is more elaborate and fine-grained: Machine learning has the parent Artificial Intelligence, and Artificial Intelligence has the parent Computer science. If we only consider direct hypernyms of the concept, we will not be able to deal with the heterogeneous problem that widely exists in real datasets.</p><p>A naive way is to use the top-down path from the root concept to the target concept. However, for most knowledge bases, the hierarchy is not organized as a tree, but a directed acyclic graph (DAG); and a concept could possibly have more than one direct parent. To solve the problem, we propose to extend the hierarchical context by bottom-up breadth-firstsearch (BFS).</p><p>Given d as number of samples, we perform BFS to sample d parent concepts from target concept c (p) i in KB p and generate the subgraph</p><formula xml:id="formula_8">G i = {E i , R i } where E i = {e j |j = 1, 2, ..., d} is the set of parent concepts and R i = {(c (p)</formula><p>i , e j )|j = 1, 2, ..., d} ∪ {(e j , e k )|e k is the parent of e j }. In this subgraph, we perform graph attention aggregation to generate the hierarchical representation as</p><formula xml:id="formula_9">α j = exp(LeakyReLU(W h(c (p) i ) + W h(e j ))</formula><p>k∈Ei exp(LeakyReLU(W h(c</p><formula xml:id="formula_10">(p) i ) + W h(e k )))<label>(6)</label></formula><formula xml:id="formula_11">m (p) i = f c( K k=1 σ( j∈Ei α k j W k m h(e j )))<label>(7)</label></formula><p>where m (p) i is the hierarchical representation, h(•) denotes semantic embedding, W m is the projection matrix for hierarchy aggregation. In practice, we sampling a fixed number of 5 hypernym nodes as the E i for concept c i and employ a 2hop multi-head attention networks to encode the embedding. Similarly, because hypernym relations are very sparse, 5 hypernym nodes could usually cover 2-hop and even 3-hop hypernym nodes for concept c i (see <ref type="bibr">Section 4.1)</ref>.</p><p>In all, we finally get the overall representation v</p><formula xml:id="formula_12">(p) i for concept c (p) i</formula><p>by integrating all these representation up as denotes the hierarchy aggregation result.</p><formula xml:id="formula_13">v (p) i = f c([h (p) i v (p) i m (p) i ])<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Global Momentum Contrastive Learning</head><p>In this section, we present the self-supervised global momentum contrastive learning framework with three key ideas: shared encoder, contrastive loss as instance discrimination, and momentum update.</p><p>Shared encoder As we have discussed in Section 3.1, during the shift from supervised embedding learning to selfsupervised embedding learning, lack of embedding consistency and training objectives are the main obstacles. For the consistency problem, BERT embeddings have provided us with a unified initial embedding space for concepts from different KBs. However, if we train encoders for each KB respectively, each of these encoders will learn their own parameters and therefore spoil the consistency.</p><p>For ensuring consistency during training, a natural idea is to "merge" KBs into one. This is what happens to the shared encoder, that it is jointly trained over KBs to maintain the unified embedding space because it only learns one set of parameters. Our ablation study shows that the shared encoder mechanism brings in a noticeable improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contrastive loss as instance discrimination</head><p>Recent studies show promising results on self-supervised representation learning <ref type="bibr" target="#b16">[17]</ref> using contrastive loss <ref type="bibr" target="#b8">[9]</ref> in computer vision <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b41">[42]</ref>, natural language processing <ref type="bibr" target="#b3">[4]</ref> and graph learning <ref type="bibr" target="#b35">[36]</ref>. Given a positive sample pair (x, y) and a set of negative samples Y − = y 1 , ..., y k , the contrastive loss is formulated as:</p><formula xml:id="formula_14">L contrast = E[− log e f T x fy/τ e f T x fy/τ + i e f T x f y − i /τ ] = E[−f T x f y /τ ] alignment + E[log(e f T x fy/τ + i e f T x f y − i /τ )] uniformity<label>(9)</label></formula><p>where f (•) refers to the encoder and τ is the temperature hyperparameter. The first term aims at "alignment" and the second aims at "uniformity" of sample vectors on a sphere given the normalization condition. The traditional supervised entity alignment algorithms can also be categorized into contrastive learning with manually labeled positive pairs. Based on the assumption that after encoder f converges, a concept x from KB 1 and its positive paired y from KB 2 should have similar embeddings (P(f x = f y ) = 1). If we always normalize the representation (||f x || = 1), according to <ref type="bibr" target="#b37">[38]</ref>, we have f T</p><p>x f y ≈ 1 and the contrastive loss can be further written as:</p><formula xml:id="formula_15">L contrast P(fx=fy )=1 ========= −1/τ + E[log(e 1/τ + i e f T x f y − i /τ )]<label>(10)</label></formula><p>which indicates that the main problem of embedding-based concept linking lies in uniformity rather than alignment given the prerequisite. This provides us with a solid theoretical foundation for applying self-supervised learning to concept linking problem, that if we can guarantee P(f x = f y ) = 1 during the training, we can still learn a good representation for concept linking without manually labeled positive samples.</p><p>This condition does approximately hold during the training of SelfLinKG. Because we have already unified the embedding space of different KBs, suppose x from KB 1 and y from KB 2 are identical concepts, they must share some semantic and structural similarities (otherwise no machine learning algorithm will be able to link them). From the perspective of representation, f x and f y are naturally close to each other in the embedding space. In SelfLinKG, we only push other noisy concepts away from them as far as possible to make their distance relatively small and do not try to draw them near. Of course, the performance will not be as good as supervised learning with abundant labels, but we will show in the experiments that our self-supervised SelfLinKG is competitive and even better than supervised ones in few-label situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Internal Deduplication Assumption (IDA) for negative sampling</head><p>In practice, we propose to leverage contrastive loss as instance discrimination to train our encoder. For each concept x, we use the initial BERT embeddings to select out a set of top-k similar concepts y − i as hard negative samples from the KB that contains x rather than the KB we want to link x to, and then maximize the similarity of f x = f (x) encoded by the online encoder f with f y = f t (x) encoded by the target encoder f t and minimize the similarity of f x with f y − i . This strategy works under the assumption that a KB is internally deduplicated, which we call "Internal Deduplication Assumption (IDA)". In fact, a KB usually does not contains two distinct concepts that have identical meaning. In that case, we can view any other concepts in the KB as negative ones to a given target x, and therefore would not select out true positive samples even without groundtruth because we are selecting negative samples from KB that the concept x comes from rather than the counter KB. Compared with Local Closed World Assumption <ref type="bibr" target="#b5">[6]</ref> which assumes triplets not in a set of KBs as negative samples, our assumption is made within a deduplicated KB and assumes all concepts beyond the target itself could play the role as negative samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Global Momentum Contrastive Learning</head><p>Input : Knowledge Base KB p (p = 0, 1, ..., P );</p><p>Online encoder f and params θ; Target encoder f t (•) and params θ t . Output : Pretrained embedding v for linking task. Initialize θ = θ t . while True do Randomly pick a KB p from all KBs and a batch X from KB p ; for concept c i ∈ X do There may be another remaining question about why we use hard negative samples. As indicated Equation ( <ref type="formula" target="#formula_15">10</ref>), the more negative samples rather than harder ones, the better uniformity. That is true for previous applications of contrastive learning that focus on image classification problems who leverage a memory bank or a queue to store a large amount of randomly selected negative samples ( <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b41">[42]</ref>). However, in concept linking, we find that since the semantic embeddings have been pre-trained and are quite distinctive in most cases, using random negative samples from memory banks or queue structures lead to trivial solutions in preliminary experiments. Thanks to the pre-training, trivial negative samples actually have been distributed far away from our target, and the main problem lies in those harder ones. Thus, we perform negative sampling based on semantic embedding similarity, i.e., we collect concepts that are similar to the target concept by name as negative sampled ones.</p><formula xml:id="formula_16">f x = f (c i ); / * 1. instance discrimination * / f y = f t (c i ); sample Y − = {y − 1 , ..., y − k } from KB p ; f y − i = f t (y − i ); L = −f T x f y /τ + log(e f T x fy/τ + i e f T x f y − i /τ ) / * 2. gradient update on f * / θ = θ − α • ∂ ∂θ L / * 3. momentum update on f t * / θ t = mθ t + (1 − m)θ</formula><p>Momentum Update. In this part, we will introduce the final critical idea that ensures the feasibility of SelfLinKGmomentum update. Recall Equation <ref type="bibr" target="#b9">(10)</ref>, given a concept x and a set of negative samples y − i , we will update the encoder f by minimizing L contrast . However, a question is that are we going to update f according to f x , f y − i , or both of them? Traditionally, in the end-to-end training fashion, we will update f from both f x and f y − i . However, in the contrastive learning scenario, we only update f according to f x and stop gradients being back-propagated to f from f y − i in a momentum update paradigm. A conceptual comparison between them is shown in Figure <ref type="figure" target="#fig_2">4</ref>. The reason is that f y − i here serves as a stable ground for f x to adjust its location in the embedding space. If we directly update them according to the direction of the instant gradient, so rapid are the fluctuations of f y − i at the beginning of the training that even x in adjacent batches may see drastically different embedding of the same negative sample y − i , leading to representation collapse <ref type="bibr" target="#b11">[12]</ref>. Instead, as many famous optimization algorithms such as Adam and Adagrad do, we can update them according to the direction of the gradient momentum, which is far more steady. In Section 4.4, our experiment also support this conclusion.</p><p>On the other hand, the f y − i should also be updated along with the learning of f . Thus, a compromise is to leverage momentum update with two encoders: the online encoder f and the target encoder f t . Such techniques are also common in fields such as reinforcement learning, where Double Qlearning <ref type="bibr" target="#b32">[33]</ref> with two encoders are proposed to deal with the learning collapse and instability. In this case, we can rewrite the contrastive loss as</p><formula xml:id="formula_17">L contrast = E[−f (x) T f t (x)/τ ] + E[e f (x) T ft(y)/τ + i e f (x) T ft(y − i )/τ ]<label>(11)</label></formula><p>where the online encoder f is directly updated by gradients as</p><formula xml:id="formula_18">θ ← θ − α • ∂ ∂θ L contrast (<label>12</label></formula><formula xml:id="formula_19">)</formula><p>where α is the learning rate. And the target encoder, by the momentum it is updated for every certain number of steps as</p><formula xml:id="formula_20">θ t ← mθ t + (1 − m)θ<label>(13)</label></formula><p>where m ∈ [0, 1) is the momentum coefficient that needs to be set as a large value like 0.999 to ensure smooth optimization. In this case, the representations of negative samples y − i are slowly updated following the rapidly updated online encoder. Our ablation study in Section 4.4 also shows a relatively bigger momentum value (i.e., a slower update to target encoder) could lead to better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">SelfLinKG in the Supervised Setting</head><p>To demonstrate SelfLinKG's characteristic of label efficiency, in this section, we will briefly describe SelfLinKG in the supervised setting, which is used for ablation study in the experiment. We use SelfLinKG s to stand for supervised SelfLinKG.</p><p>In the supervised setting, i.e., there are groudntruth cross links L train = {(x, y)} where x ∈ KB 1 , y ∈ KB 2 provided, according to Equation ( <ref type="formula" target="#formula_14">9</ref>), we leverage the most ordinary form of contrastive loss for the concept linking task with supervision. We still preserve the shared encoder mechanism and momentum update to make a fair comparison. For the local attention-based encoding, those linked pairs also share identical unique embeddings. In the experiment, we adjust the ratio |L train |/|L| of cross links for training SelfLinKG to compare with SelfLinKG with no L train provided.</p><p>We discuss the experiment results between SelfLinKG and SelfLinKG s in Section 4.4. The results show that SelfLinKG has a competitive and even much better performance in situations with fewer labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT</head><p>Because SelfLinKG is designed to cope with concepts in real-world large-scale heterogeneous knowledge bases, conventional entity alignment benchmarks like DBP15k <ref type="bibr" target="#b26">[27]</ref> are either too small, with little ambiguity, or cross-lingual (which must require external supervision), which fail to meet our requirements. Therefore, in this work, we decide to evaluate the SelfLinKG between two large heterogeneous knowledge bases: Microsoft Academic Graph (MAG) taxonomy ( <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>) in OAG <ref type="bibr" target="#b43">[44]</ref> and English Wikipedia (EnWiki).</p><p>Based on experiments, we further conduct concept linking between 14 different knowledge bases. The final linked concept graph OAG know is publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>MAG taxonomy. MAG taxonomy consists of 679,921 concepts collected from the Internet and 873,087 hypernym relations generated according to co-occurrence from 208,915,369 published papers. As some concepts in MAG are isolatedhave no relations with the other concepts, we filter them out. Finally, the resultant MAG graph consists of 490,885 concepts and 873,087 hypernym relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>English Wikipedia (EnWiki).</head><p>As for EnWiki, we use the snapshot of June 2019. After cleaning, we have 7,598,399 terms and 1,584,269 categories, with 7,029,440 related relations and 1,395,321 hypernym relations. We apply three rounds of cleaning to EnWiki: First, since there are too many entities rather than concepts in EnWiki, we use concepts in the MAG subgraph to search top-10 similar entries in EnWiki by name as seeds. And then, we extend all concepts that are 1) hypernyms of seeds, 2) has the same name with seeds into the subgraph. Finally, because some categories share the exact same name with concepts, we merge them into one. Notice that there are no original hypernym relations in Wikipedia, so we view pages-in-category and subcategory as hypernym relations.. Finally, the obtained EnWiki graph consists of 620,557 concepts, 5,503,012 related relations and 1,395,321 hypernym relations.</p><p>Groundtruth. As for evaluation, because MAG taxonomy is collected from the Internet, the original web page URLs for concepts are available. After examination, 233,010 concepts of the MAG taxonomy derive from EnWiki, in which some of the URLs are redirected to other new entries because the EnWiki is also evolving. We evaluate SelfLinKG and other baseline models based on these original links as groundtruth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Setup</head><p>Evaluation Tasks. To systematically evaluate the proposed methodology, we design the following four tasks:</p><p>• Synonym Linking: We utilize the redirect link in EnWiki to build a dataset consisting of 10,082 sample pairs, among which 5,041 are synonym concept pairs, and others are negative sample pairs by sampling similar terms. Following <ref type="bibr" target="#b30">[31]</ref>, we put the pair of embedding into a multi-layer neural network as a classifier to output the similarity score.</p><p>• Disambiguation: Disambiguation of concepts sharing the exact same name is extremely difficult for linking.</p><p>The ambiguous concepts are mainly from EnWiki, with a correct matching concept in MAG. We pick out ambiguous concepts by disambiguation pages in EnWiki and construct a dataset containing 730 unique terms and 3,548 concepts. Statistics of the disambiguation dataset are shown in Table <ref type="table" target="#tab_2">2</ref>. For each unique term, we re-rank concepts by L2 distance using trained representation. Because in the disambiguation task, the negative samples should only involve those ambiguous ones, Hit@K would be a more objective metric rather than Prec./Rec./F1 in which case negative samples are randomly selected and lead to a virtual-high result.</p><p>• General Linking: We build a challenging dataset contains both simple matching cases and synonym cases, altogether 20,100 samples, including 70% simple matching cases and 30% synonym concepts cases and harden it by sampling negative concepts that have similar semantic embedding with positive pairs. The dataset has 60% for training, 20% for validation, and 20% for testing. Following previous works, we feed pairs of embeddings into a multi-layer neural network as a classifier to output the similarity score. • General Ranking: We further construct a ranking dataset using positive pairs from the General Linking dataset. For each positive sample, we find the top-20 similar concepts by name in EnWiki as candidates, and re-ranking them by L2 distance using trained representation and faiss toolkit.</p><p>Comparison Methods. Though many embedding-based entity alignment algorithms have emerged these years, most of them focus on supervised learning and are not fair to serve as baselines for self-supervised SelfLinKG. Therefore, we select a series of state-of-the-art unsupervised knowledge graph embedding methods that are used as baselines in various entity alignment papers to compare. We manually tune their hyperparameters for a solid comparison.</p><p>• RESCAL [20]: This method is an approach to relational learning based on the factorization of a three-way tensor.</p><p>• TransE [1]: A method that models relationships by interpreting them as translations operating on the lowdimensional embeddings of the entities.</p><p>• ComplEx [32]: This is a simple approach to perform matrix and tensor factorization for link prediction data  we input the subgraph of a concept, and trained the shared encoder across MAG and EnWiki with only the instance discrimination pre-train task, which is an unsupervised method. The unique embedding has no sharing in these settings, i.e., every single concept has its unique embedding. We utilize OpenKE <ref type="bibr" target="#b9">[10]</ref>, an Open-source Framework for Knowledge Embedding organized by THUNLP based on the TensorFlow toolkit. The authors use C++ to implement some underlying operations, such as data preprocessing and negative sampling. For each specific model, it is implemented by TensorFlow with Python interfaces so that there is a convenient platform to run models on GPUs. For GAKE, we download the authors' source codes written in C++ to perform training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Environment and Settings.</head><p>In the experiment, the input dimension and hidden dimension for graph attention layers are 150; attention dropout is 0.3; the number of attention head is 4. The input dimension of the fully-connected-layer in the encoder is set to 600 and output a vector with a dimension of 150.</p><p>For the hyperparameters of global momentum contrastive learning, we set the momentum value m to 0.999, temperature τ to 10. The networks are optimized with Adam optimizer. All codes are implemented in Python3 and run by interpreter Python3.6. The experiments were conducted on a CentOS server with a 14 cores Intel(R) Xeon(R) Gold 5120 CPU @ 2.20GHz, 640G System RAM, and a Tesla V100-SXM2 32GB RAM GPU. An important technique in our method is negative sampling based on name similarity. Here we apply the Faiss, a library for efficient similarity search 1 . In the candidate searching period, we apply the IndexFlatL2 as an indexer based on L2 distance. The search was previously conducted respectively in the MAG subgraph and EnWiki subgraph. It is quite efficient because once the indices are built by KD-tree, the closest neighbor can be easily found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results of Unsupervised Linking and SelfLinKG</head><p>Table <ref type="table" target="#tab_1">1</ref> shows the overall linking performance on three tasks by embedding the training method: Synonym Linking, Disambiguation, and General Linking. The results of the General Ranking task are showed in Figure <ref type="figure">5</ref>. All the methods compared are unsupervised or self-supervised embedding learning methods. Results show that our method SelfLinKG consistently outperforms other alternatives (26%-33%) in every task. We will discuss and compare results on each of them. Synonym Linking. For Synonym Linking, the names of a positive concept pair are not identical, sometimes even very different from each other literally. SelfLinKG performs the best among all methods with high recall, F1, and AUROC, which means that SelfLinKG only omits a little proportion of positive pairs, even for those with entirely different names. Its comparatively low precision is probably because it assumes some negative concept pairs with similar names as correct. TransE performs comparatively well, while other methods generally have low F1 scores around 30%-40%.</p><p>Disambiguation. In the Disambiguation task, SelfLinKG also significantly performs better than other methods, which means SelfLinKG has learned a highly distinguishable representation that can discriminate ambiguous concepts. GAKE also achieves superior performance to other baseline methods, probably because context plays a vital role in its training. Moreover, it is also the structural and hierarchical context information that can help to disambiguate similar concepts. <ref type="bibr" target="#b30">[31]</ref> also observes such insight.</p><p>General Linking. For General Linking, compared with Synonym Linking, it is easier because we add in many simple cases. However, hardened negative samples can still cause trouble. SelfLinKG still performs the best, and other baselines' performance also increases. TransE is still the top method among baselines, but other methods also have competitive performance compared to TransE. SelfLinKG shows a more balanced precision and recall in this task, demonstrating its ability to cope with linking problems in most cases.</p><p>General Ranking. Figure <ref type="figure">5</ref> display the result on General Ranking, which takes the top-20 similar concepts by name as negative samples for each positive pair from General Linking and re-rank them using trained representation by L2 distance.</p><p>We only show the top-5 ranking results because the number of candidates for each concept is only 20. Noted that the SelfLinKG s here is trained under the training ratio 0.8 (see training ratio's detailed definition in Section 4.4).</p><p>The subfigure (a) compares the performance on variants of SelfLinKG. It is surprising for us to see that in this challenging setting, the SelfLinKG overwhelms other variants by 5%-9%, even using supervision. We speculate that this is because in the SelfLinKG s , we only train concepts in the |L train |, which takes up 80% of the whole |L| and approximately 25% of the whole MAG concepts, leading to an insufficient uniformity because only 25% of the concepts are trained. However, in SelfLinKG, because we have no limitation of the training set, every concepts are trained using the selfsupervised objective which results in a better uniformity over the whole dataset (as theoretically demonstrated in Eq. 9). This implies that our self-supervised objective could even be a complementary for the supervised setting to achieve better uniformity, but due to the limited passage we will leave it as an open problem for the following work.</p><p>For other unsupervised methods in subfigure (b), because their objectives do not focus enough on the discrimination task enough, they also fail to perform well in the General Ranking task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Detailed Ablation Study</head><p>In this section, we conduct ablation studies on three critical factors in SelfLinKG: self-supervision, shared encoder, and momentum update. Figure <ref type="figure" target="#fig_4">6</ref> shows the results of the different versions of SelfLinKG on these two tasks. For SelfLinKG and SelfLinKG (separate), their performance is static to the ratio of training labels provided because they are in the self-supervised setting. On the contrary, supervised SelfLinKG s 's performance benefits from the increasing ratio of provided training labels.</p><p>We observe from both (a) and (b) that the self-supervised SelfLinKG is significantly better than the supervised SelfLinKG s when the ratio of the training data is less than 0.5 (about 110k labels). Only when the training data ratio increases to nearly 0.6, the supervised version can take advantage of massive labeled information. Noted that in practice, collecting such a large number of training data is always expensive and infeasible.</p><p>Moreover, for multiple knowledge bases linking, this cost is growing quadratically with the number of knowledge bases involved. Suppose there is m KBs, under the supervised setting, we have to manually label cross-links for every two KBs, which results in m(m−1) 2 needed dataset. Our unsupervised model SelfLinKG suggests that the selfsupervised method could also perform ideally while cutting down the labeling costs. Shared encoder v.s. Separate Encoder. From both Figure <ref type="figure" target="#fig_4">6</ref> and Figure <ref type="figure">5</ref>, we also observe that the shared encoding is very helpful. With the shared encoding, SelfLinKG can obtain an improvement of 5.5% by Hit@1 rate in the Disambiguation task. In the General Linking task, we can also obtain an improvement of 2% by AUROC. This verifies the importance of sharing parameters in multi-graph learning and cross embedding space learning. A consistent embedding space is critical for the performance of concept linking and entity alignment.</p><p>Momentum Update v.s. End-to-end. In Table <ref type="table" target="#tab_3">3</ref> we discuss the gap between momentum update and end-to-end training. Besides, we also investigate how to choose momentum value.  As we discussed in Section 3.3, the single-encoderarchitecture (end-to-end) in contrastive learning leads to instability naturally. This is because, in training, the target encoder serves as the ground-truth, i.e., the distribution we want the online encoder to fit on. In the single-encoderarchitecture, model parameters are updated drastically at the beginning of training, leading to a rapid changing groundtruth and a failure. The momentum update can successfully deal with the problem by changing the target encoder slowly and smoothly, following the average direction of gradient optimization rather than the instant gradient.</p><p>For comparison between momentum update and endto-end training, the end-to-end training equals momentum update when m = 0. In this situation, we discover that endto-end training leads to a rapid collapse. This holds even when m is only a bit small such as 0.9. Our experiment shows that the training still fails very quickly.</p><p>Besides failed scenarios, we test on the other three typical values of m: 0.99, 0.999, 0.9999. The experiment indicates that the value between 0.999-0.9999 performs well. For value bigger than 0.9999, the target encoder is updated too slowly and causes the performance to drop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-head Attention</head><p>We conduct further experiments to show that why multi-head attention is necessary. As authors in Graph Attention Network (GAT) claimed <ref type="bibr" target="#b34">[35]</ref>, they found the multi-head attention to be beneficial similar to findings in transformers <ref type="bibr" target="#b33">[34]</ref>. The intuition is that a certain type of attention head usually captures a certain data pattern, like kernels in convolutional neural networks. With more heads, more parameters are engaged in the training which often yield better results.</p><p>In the Table <ref type="table" target="#tab_4">4</ref>, we compare the performance of traditional single-head attention and our multi-head attention (i.e. 4head attention) on Disambiguation and General Linking tasks. The results show that our 4-head attention outperforms single-head attention on every tasks. Although more heads may yield marginal benefits, but it also brings in a heavier model, so in this work we only choose 4 heads for our local attention network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">OAG know -LINKED CONCEPT GRAPH</head><p>Based on our proposed framework and public datasets, we have published Open Academic Graph Knowledge (OAG know ) 1 , which integrates concepts from 14 bilingual knowledge bases. Some datasets such as AMiner, AMiner-NSFC, and Termonline are published for the first time.</p><p>In practice, first, we use semantic embedding and fuzzy matching to generate a similar candidate pool. Then we feed the neighborhood and hierarchy of concepts into SelfLinKG, and self-supervisedly train their embeddings. Finally, we leverage the trained embeddings to do ambiguous ranking and classification on pairs from the candidate pool to get the linked KBs. The accuracy of links is 97.33% by random sampling a small subset of OAG know for evaluation.</p><p>Table <ref type="table" target="#tab_5">5</ref> shows the basic statistics of the OAG know . Concepts are linked to 0.7 billion OAG entities (authors, papers, venues, affiliations) through MAG and AMiner. Together with 93 million concepts from various KBs, OAG know is the largest public academic knowledge graph to date. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>Concept linking, closely related to entity linking, ontology alignment, schema matching and data integration etc., has long been studied for decades <ref type="bibr" target="#b6">[7]</ref>. Many approaches have been proposed to address this problem. For example, Li et al. <ref type="bibr" target="#b14">[15]</ref> argue for rule-based methods and develop a rule discovery algorithm. Tang et al. <ref type="bibr" target="#b27">[28]</ref> use machine learning and regard concept linking as minimizing Bayesian risk of decision making. As the size of KB increases, many semisupervised or unsupervised methods appear. For example, Rong et al. <ref type="bibr" target="#b22">[23]</ref> transfer the entity matching problem to a binary classification problem. Wang et al. <ref type="bibr" target="#b39">[40]</ref> present a factor graph model to learn the alignment across knowledge bases. For data integration across social networks, Zhang et al. <ref type="bibr" target="#b45">[46]</ref> propose COSNET, an energy-based model that considers global and local consistency. Pellissier et al. <ref type="bibr" target="#b20">[21]</ref> utilize existed hyper-links and build an online platform for tagging manually. Recently, network embedding and knowledge graph embedding have been proved to be efficient in many downstream tasks. Nickel et al. <ref type="bibr" target="#b19">[20]</ref> view the embedding training as tensor factorization. Bordes et al. <ref type="bibr" target="#b0">[1]</ref> propose TransE to interpret multi-relational data as translations operating. Trouillon et al. <ref type="bibr" target="#b31">[32]</ref> use complex embedding with composition to represent relation. Yang et al. <ref type="bibr" target="#b42">[43]</ref> put forward DistMult to jointly embed entities and relations by neural networks. Feng et al. <ref type="bibr" target="#b7">[8]</ref> formulate the knowledge base as a directed graph and learn representations leveraging the graph's structural information. These works are generally unsupervised learning on a single knowledge graph and pay little attention to cross knowledge bases situation.</p><p>In the more specific field of entity linking, many supervised embedding method appear these years. Chen et al. <ref type="bibr" target="#b1">[2]</ref> propose an embedding-based model for multilingual entity alignment based on TransE. Zhu et al. <ref type="bibr" target="#b46">[47]</ref> develop an iterative method for entity alignment via joint embeddings. Sun et al. <ref type="bibr" target="#b26">[27]</ref> propose a joint attribute-preserving embedding model for cross-lingual entity alignment. Trivedi et al. <ref type="bibr" target="#b30">[31]</ref> consider jointly combining link prediction and cross-linking tasks using attention. These methods rely on a large number of seed or anchor entities to ensure accuracy. However, few studies focus on the unsupervised embedding method and aim to find a unified solution for noisy and large-scale knowledge bases linking.</p><p>In this work, we propose a unified framework Self-LinKG to fill the gap. We first unify knowledge bases as heterogeneous information networks and employ attention to aggregate crucial structural information. To address the same embedding space problem, we propose to use a shared encoder mechanism and unique embedding. To solve the high-cost and ambiguous problem, we propose the unsupervised momentum contrastive learning. To learn more about self-supervised contrastive learning, please refer to <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION AND DISCUSSION</head><p>In this work, we propose an self-supervised model SelfLinKG for linking large-scale heterogeneous knowledge bases. Without labeled data, SelfLinKG uses global momentum contrastive learning to learn a shared representation among multiple knowledge bases. Our experiments on two largescale graphs show that the proposed unsupervised SelfLinKG can achieve a comparable performance with its supervised counterpart. We apply the model to automatically generate linkings among 14 different knowledge bases and make the linked graphs publicly available.</p><p>As the future work, it would be rather interesting to design a knowledge linking system to automatically harvest knowledge (linking new knowledge into existing bases) from the open Web. It would be also exciting to explore novel methods to make the model more robust, as the open data is always noisy.</p><p>There are also some open problems about leveraging contrastive objective in knowledge graph embedding. It requires further study on whether our SelfLinKG is adaptive to KBs that have fewer cross-links or from very different domains. Intuitively, we think the number of cross-links is not a problem, because the contrastive objective aims at scattering nodes' embedding uniformly in the sphere space and we can still efficiently narrow down distances of positive pairs. For KBs from very different domains, it is probably very hard to self-supervisedly link them up if very little common information is shared between target pairs' attributes and structure, in which case supervised labels may be still indispensable.</p><p>For questions about whether this contrastive objective could help the embedding learning within one knowledge base, we think it depends on the downstream task. Some very recent work <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b21">[22]</ref> demonstrate the effectiveness of it on node classification and graph classification in networks, but also show that it may not help tasks such as relation prediction and link prediction. We believe it is probably the same for the single knowledge graph embedding task. Limited to the passage, we do not study the problem in this work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Motivation of SelfLinKG from perspectives of embedding consistency and training objective.</figDesc><graphic url="image-2.png" coords="3,312.39,43.70,255.12,163.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: An illustrative architecture of SelfLinKG framework. Local Attention-based Encoding, i.e. the encoder, encodes neighbor and hierarchical information into vector embedding. Global Momentum Contrastive Learning leverages the output embedding to calculate contrastive loss using self-supervision rather than labeled data. Online encoder f uses momentum to update the target encoder f t .</figDesc><graphic url="image-3.png" coords="5,51.74,43.70,510.27,205.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Conceptual comparison of (a) end-to-end training and (b) momentum update. End-to-End use single encoder, which suffers from training instability and could not converge in the experiment. However, momentum update helps the target encoder to change steadily along the direction of gradient momentum, rather than that of the instant gradient.</figDesc><graphic url="image-4.png" coords="7,48.39,325.19,255.13,115.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1 .Fig. 5 :</head><label>15</label><figDesc>Fig. 5: Ablation study on General Ranking. The SelfLinKG s here is trained under the best training ratio 0.8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Ablation study on 1) SelfLinKG v.s. SelfLinKG s , 2) shared encoder v.s. separate encoder</figDesc><graphic url="image-7.png" coords="10,317.79,48.68,116.22,87.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 :</head><label>1</label><figDesc>Results of linking performances under unsupervised settings.</figDesc><table><row><cell cols="2">Task &amp; Metrics</cell><cell></cell><cell cols="2">RESCAL</cell><cell cols="2">TransE</cell><cell>ComplEx</cell><cell>HolE</cell><cell>DistMult</cell><cell>GAKE</cell><cell>SelfLinKG</cell></row><row><cell></cell><cell cols="2">Prec.</cell><cell>54.92</cell><cell></cell><cell>55.94</cell><cell></cell><cell>56.44</cell><cell>56.45</cell><cell>56.15</cell><cell>46.86</cell><cell>67.85</cell></row><row><cell>Synonym</cell><cell>Rec.</cell><cell></cell><cell>25.51</cell><cell></cell><cell>56.91</cell><cell></cell><cell>35.16</cell><cell>23.58</cell><cell>32.01</cell><cell>40.96</cell><cell>85.77</cell></row><row><cell>Linking</cell><cell>F1</cell><cell></cell><cell>34.84</cell><cell></cell><cell>56.42</cell><cell></cell><cell>43.33</cell><cell>33.26</cell><cell>40.78</cell><cell>43.71</cell><cell>75.76</cell></row><row><cell></cell><cell cols="2">AUROC</cell><cell>54.44</cell><cell></cell><cell>60.10</cell><cell></cell><cell>56.96</cell><cell>54.50</cell><cell>56.41</cell><cell>49.86</cell><cell>80.64</cell></row><row><cell></cell><cell cols="2">Hit@1</cell><cell>21.78</cell><cell></cell><cell>22.05</cell><cell></cell><cell>34.10</cell><cell>19.58</cell><cell>36.43</cell><cell>38.35</cell><cell>48.21</cell></row><row><cell>Disam-</cell><cell cols="2">Hit@2</cell><cell>58.90</cell><cell></cell><cell>57.94</cell><cell></cell><cell>63.69</cell><cell>56.98</cell><cell>67.26</cell><cell>64.65</cell><cell>70.00</cell></row><row><cell>biguation</cell><cell cols="2">Hit@3</cell><cell>75.20</cell><cell></cell><cell>76.71</cell><cell></cell><cell>80.00</cell><cell>76.30</cell><cell>80.41</cell><cell>80.41</cell><cell>82.73</cell></row><row><cell></cell><cell cols="2">Hit@5</cell><cell>91.09</cell><cell></cell><cell>92.19</cell><cell></cell><cell>92.32</cell><cell>93.15</cell><cell>91.64</cell><cell>92.32</cell><cell>93.28</cell></row><row><cell></cell><cell cols="2">Prec.</cell><cell>54.36</cell><cell></cell><cell>58.67</cell><cell></cell><cell>53.36</cell><cell>56.26</cell><cell>55.25</cell><cell>52.75</cell><cell>75.85</cell></row><row><cell>General</cell><cell>Rec.</cell><cell></cell><cell>51.81</cell><cell></cell><cell>59.53</cell><cell></cell><cell>62.95</cell><cell>35.58</cell><cell>53.27</cell><cell>57.72</cell><cell>76.15</cell></row><row><cell>Linking</cell><cell>F1</cell><cell></cell><cell>53.05</cell><cell></cell><cell>59.10</cell><cell></cell><cell>57.76</cell><cell>43.59</cell><cell>54.24</cell><cell>55.12</cell><cell>76.00</cell></row><row><cell></cell><cell cols="2">AUROC</cell><cell>54.09</cell><cell></cell><cell>59.79</cell><cell></cell><cell>54.55</cell><cell>54.86</cell><cell>56.47</cell><cell>53.04</cell><cell>80.75</cell></row><row><cell>#Candidates</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell></cell><cell></cell></row><row><cell cols="2">Portion(%) 31.4</cell><cell>20.0</cell><cell>16.0</cell><cell>10.4</cell><cell>9.7</cell><cell>12.5</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2 :</head><label>2</label><figDesc>Statistics for Disambiguation Task Dataset that uses vectors with complex values and retains the mathematical definition of the dot product. Holographic embeddings (HOLE) is a method to learn compositional vector space representations of entire knowledge graphs. It is related to holographic models of associative memory in that it employs the circular correlation to create compositional representations. This method focuses on the study of neural-embedding models, where the representations are learned using neural networks with energy-based objectives.</figDesc><table><row><cell>• GAKE [8]: This method formulates the knowledge base</cell></row><row><cell>as a directed graph, and learns representations for any</cell></row><row><cell>vertices or edges by leveraging the graph's structural</cell></row><row><cell>information. In this method, three types of graph context</cell></row><row><cell>for embedding are introduced: neighbor context, path</cell></row><row><cell>context, and edge context; each reflects properties of</cell></row><row><cell>knowledge from different perspectives.</cell></row></table><note>• HolE [19]: • DistMult [43]: • SelfLinKG: self-supervised SelfLinKG. In this method,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3 :</head><label>3</label><figDesc>Ablation study on momentum value m</figDesc><table><row><cell>Momentum</cell><cell>Hit@1</cell><cell>F1</cell><cell>AUROC</cell></row><row><cell>end-to-end (0.0)</cell><cell>failed</cell><cell>failed</cell><cell>failed</cell></row><row><cell>0.9</cell><cell>failed</cell><cell>failed</cell><cell>failed</cell></row><row><cell>0.99</cell><cell>46.57</cell><cell>74.74</cell><cell>81.43</cell></row><row><cell>0.999</cell><cell>48.21</cell><cell>76.00</cell><cell>80.75</cell></row><row><cell>0.9999</cell><cell>46.57</cell><cell>77.33</cell><cell>83.41</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 4 :</head><label>4</label><figDesc>Ablation study on multi-head attention</figDesc><table><row><cell>Heads</cell><cell>Hit@1</cell><cell>F1</cell><cell>AUROC</cell></row><row><cell>4</cell><cell>48.21</cell><cell>76.00</cell><cell>80.75</cell></row><row><cell>1</cell><cell>47.13</cell><cell>73.31</cell><cell>78.87</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 5 :</head><label>5</label><figDesc>Overall Statistics of OAG know . T Taxonomy ; E Encyclopedia; K Knowledge Graph</figDesc><table><row><cell>Name</cell><cell>Type</cell><cell>Language</cell><cell>#Concepts</cell><cell>#Cross-links</cell></row><row><cell>MAG [26]</cell><cell>T</cell><cell>En</cell><cell>679,921</cell><cell>963,579</cell></row><row><cell>AMiner [29]</cell><cell>T</cell><cell>En&amp;Zh</cell><cell>367,890</cell><cell>33,890</cell></row><row><cell>AMiner-NSFC</cell><cell>T</cell><cell>Zh</cell><cell>53,017</cell><cell>85,689</cell></row><row><cell>NSF</cell><cell>T</cell><cell>En</cell><cell>2,155</cell><cell>810</cell></row><row><cell>Termonline</cell><cell>T</cell><cell>En&amp;Zh</cell><cell>105,298</cell><cell>92,754</cell></row><row><cell>GB</cell><cell>T</cell><cell>Zh</cell><cell>3,543</cell><cell>2,697</cell></row><row><cell>Bpress</cell><cell>T</cell><cell>En</cell><cell>1,269</cell><cell>1,157</cell></row><row><cell>Xlore [39]</cell><cell>E</cell><cell>En&amp;Zh</cell><cell>508,768</cell><cell>252,144</cell></row><row><cell>EnWiki</cell><cell>E</cell><cell>En</cell><cell>7,598,399</cell><cell>17,552,975</cell></row><row><cell>ZhWiki</cell><cell>E</cell><cell>Zh</cell><cell>1,055,757</cell><cell>703,737</cell></row><row><cell>Baidu</cell><cell>E</cell><cell>Zh</cell><cell>10,423,650</cell><cell>229,137</cell></row><row><cell>HuDong</cell><cell>E</cell><cell>Zh</cell><cell>3,141,658</cell><cell>367,850</cell></row><row><cell>Wikidata</cell><cell>K</cell><cell>En</cell><cell>22,574,520</cell><cell>17,918,258</cell></row><row><cell>Freebase</cell><cell>K</cell><cell>En</cell><cell>47,294,433</cell><cell>4,021,644</cell></row><row><cell>Overall</cell><cell>-</cell><cell>-</cell><cell>93,810,278</cell><cell>42,226,321</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>The work is supported by the National Key R&amp;D Program of China (2018YFB1402600), NSFC for Distinguished Young Scholar (61825602), NSFC (61836013) and Tsinghua-Bosch Joint ML Center, Deptment of Computer Science and Technology, Tsinghua University.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Multilingual knowledge graph embeddings for cross-lingual knowledge alignment</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zaniolo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03954</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">What does bert look at? an analysis of bert&apos;s attention</title>
		<author>
			<persName><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04341</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Electra: Pretraining text encoders as discriminators rather than generators</title>
		<author>
			<persName><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Knowledge vault: A web-scale approach to probabilistic knowledge fusion</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Strohmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="601" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Duplicate record detection: A survey</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Elmagarmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Ipeirotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Verykios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gake: Graph aware knowledge embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2016: Technical Papers</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="641" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>IEEE</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
	<note>In (CVPR&apos;06</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Openke: An open toolkit for knowledge embedding</title>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="139" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Khasahmadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05582</idno>
		<title level="m">Contrastive multi-view representation learning on graphs</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05722</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rimom: A dynamic multistrategy ontology alignment framework</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TKDE</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1218" to="1232" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rule-based method for entity resolution</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="250" to="263" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Aligning users across social networks using network embedding</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">K</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ijcai</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1774" to="1780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Self-supervised learning: Generative or contrastive</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">2006</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Predict anchor links across social networks via an embedding approach</title>
		<author>
			<persName><forename type="first">T</forename><surname>Man</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ijcai</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1823" to="1829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Maximilian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lorenzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tomaso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.04935</idno>
		<title level="m">Holographic embeddings of knowledge graphs</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Icml</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">From freebase to wikidata: The great migration</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pellissier Tanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vrandečić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schaffert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pintscher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1419" to="1428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gcc: Graph contrastive coding for graph neural network pre-training</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1150" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A machine learning approach for instance matching based on similarity metrics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISWC&apos;12</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="460" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Linking named entities in tweets with knowledge base via user interest modeling</title>
		<author>
			<persName><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;13</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="68" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.12216</idno>
		<title level="m">A web-scale system for scientific knowledge exploration</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An overview of microsoft academic service (mas) and applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th WWW</title>
				<meeting>the 24th WWW</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="243" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cross-lingual entity alignment via joint attribute-preserving embedding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISWC</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="628" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Using bayesian decision for ontology mapping</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Web Semantics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="243" to="262" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Arnetminer: extraction and mining of academic social networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="990" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Linknbed: Multi-graph representation learning with entity linkage</title>
		<author>
			<persName><forename type="first">R</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="252" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">É</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bouchard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning with double q-learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Graph Attention Networks. ICLR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li Ò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.10341</idno>
		<title level="m">Deep graph infomax</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned</title>
		<author>
			<persName><forename type="first">E</forename><surname>Voita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Talbot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Moiseev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09418</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Understanding contrastive representation learning through alignment and uniformity on the hypersphere</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10242</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Xlore: A large-scale english-chinese bilingual knowledge graph</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISWC</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">1035</biblScope>
			<biblScope unit="page" from="121" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Cross-lingual knowledge linking across wiki knowledge bases</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW&apos;12</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="459" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brew</surname></persName>
		</author>
		<idno>ArXiv, abs/1910.03771</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>-T. Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6575</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Oag: Toward linking large-scale heterogeneous entity graphs</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2585" to="2595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Mego2vec: Embedding matched ego networks for user alignment across social networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="327" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Cosnet: Connecting heterogeneous social networks with local and global consistency</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;15</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1485" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Iterative entity alignment via joint knowledge embeddings</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4258" to="4264" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
