<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Zhenwei</forename><surname>Shi</surname></persName>
							<email>shizhenwei@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Image Processing Center</orgName>
								<orgName type="department" key="dep2">School of Astronautics</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing Key Labo-ratory of Digital Media</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Astronautics</orgName>
								<orgName type="laboratory">State Key Laboratory of Virtual Reality Technology and Systems</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7F4FF055D603C3F5081BAF451663B317</idno>
					<idno type="DOI">10.1109/TIP.2017.2773199</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-We propose a new paradigm for target detection in high resolution aerial remote sensing images under small target priors. Previous remote sensing target detection methods frame the detection as learning of detection model + inference of classlabel and bounding-box coordinates. Instead, we formulate it from a Bayesian view that at inference stage, the detection model is adaptively updated to maximize its posterior that is determined by both training and observation. We call this paradigm "random access memories (RAM)." In this paradigm, "Memories" can be interpreted as any model distribution learned from training data and "random access" means accessing memories and randomly adjusting the model at detection phase to obtain better adaptivity to any unseen distribution of test data. By leveraging some latest detection techniques e.g., deep Convolutional Neural Networks and multi-scale anchors, experimental results on a public remote sensing target detection data set show our method outperforms several other state of the art methods. We also introduce a new data set "LEarning, VIsion and Remote sensing laboratory (LEVIR)", which is one order of magnitude larger than other data sets of this field. LEVIR consists of a large set of Google Earth images, with over 22 k images and 10 k independently labeled targets. RAM gives noticeable upgrade of accuracy (an mean average precision improvement of 1% ∼ 4%) of our baseline detectors with acceptable computational overhead. Index Terms-High resolution aerial remote sensing image, target detection, convolutional neural networks, random access memories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T HE rapid development of remote sensing technologies has opened a door for people to observe the earth. Automatically detecting targets of remote sensing images, e.g. the airplane, oilpot and ship, is one of the core tasks in remote sensing applications, and have been drawing more and more attentions in recent years <ref type="bibr" target="#b0">[1]</ref>.</p><p>Most of the early attempts of remote sensing target detection <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b5">[6]</ref> are designed with the help of some specifically designed hand-crafted features and supervised classification algorithms. Recent advances in remote sensing target detection methods <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b12">[13]</ref> have been primarily focusing on deep learning based detection methods, especially the ones based on convolutional neural networks. Despite their great variations, all previous methods frame the detection as two stages: 1) learning a detection model H from training data D tr by finding local/global maximum of likelihood p(D tr |H), and 2) making inference of category labels and bounding-box coordinates from the observed image D ob . Arguably, once the learning process has stopped, the model will be fixed at testing time. In this paper, we will re-examine this problem from another perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Random Access Memories (RAM)</head><p>Here we re-think the detection paradigm by assuming that detection model can be changed adaptively as the model receives different observations. This idea is inspired by a large group of detection algorithms of signal process field called the Constant False Alarm Rate algorithms <ref type="bibr" target="#b13">[14]</ref> where the algorithm adapts the parameters of the model to the statistical characteristics of the observation at test time. Similar thoughts can also be found in quantum mechanics where the state of any quantum object not only depends on its states, but also on the measurement itself <ref type="bibr" target="#b14">[15]</ref>. Based on the above ideas, we formulate the detection model as any certain probability distribution p(H) in its hypothesis space at training phase, and at detection phase the model is further updated that is determined by both of the training and observation. We call this new paradigm "Random Access Memories". From a Bayesian perspective, "Memories" can be interpreted as any model distribution p(H) learned from training data, while "Random Access" may be interpreted as accessing to its memories and reaching its maximum of posterior after a random observation at detection phase. Fig. <ref type="figure" target="#fig_0">1</ref> shows the overview of our proposed new paradigm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Approximate Inference</head><p>Most of the CNN based detectors are built on basis of frequentist statistics, where their detection model can be explicitly determined by the local/global maximum of the likelihood during the training phase. Instead, the proposed paradigm is established based on a complete probability distribution of the detection model. In Bayesian machine learning, Laplace Approximation <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> acts as a simple but useful tool to find a Gaussian approximation of the posterior distribution near the MAP solution. In information theory, Fisher Information <ref type="bibr" target="#b17">[18]</ref> has the same essence but different interpretations. It describes the amount of information obtained by the Maximum Likelihood Estimation (MLE) of a set of training samples, namely, a measure of the flatness and sharpness of the the model's distribution at MLE solution Ĥ. By computing fisher information and using laplace approximation, we can bridge that gap between the mainstream CNN detectors and "Random Access Memories" paradigm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The Small Target Prior in Remote Sensing Image</head><p>For high resolution aerial remote sensing images, the targets of interest are usually sparse distributed and only occupy a very small number of pixels. In the case of sliding window based detectors, the imbalance between background and desired target could be as extreme as 10 7 background windows to every target window. This could be even true for wide-scale remote sensing images. The complex background distribution leads to a higher demand on the capacity of the model. Larger models are able to capture more complex background distributions, while it may suffer from a higher computation cost, which is especially infeasible for some on-orbit remote sensing applications. An advantage of our method lies that the small target prior can be very easily integrated to the proposed paradigm, as we are able to compress model's capacity to meet the speed requirement while maintaining the detection accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Contributions</head><p>The contributions of our work can be summarized as follows:</p><p>1) The key innovation of Random Access Memories lies that the detection model can be adaptively changed during detection phase that contemporarily determined by both training and the latest observations. Such paradigm can be easily integrated with the current CNN based detectors without complex changes. With RAM, a comparable or even higher detection accuracy of a larger model can be obtained with less parameters and a faster detection speed. 2) We introduce a new dataset "LEVIR"<ref type="foot" target="#foot_0">1</ref> for remote sensing target detection task, which is more challenging and one order of magnitude larger than existing datasets. We have made LEVIR open access at http://levir.buaa.edu.cn/Code.htm. The rest of this paper is organized as follows. In section II, we will review the recent advances in general object detection for natural images. In section III, we will give a detailed introduction to our proposed detection paradigm. In section IV, we will introduce LEVIR, a new dataset for remote sensing target detection. Some experimental results are given in section V, and the conclusions are drawn in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RECENT ADVANCES IN NATURAL IMAGE OBJECT DETECTION</head><p>In computer vision, there has been great progress of natural image object detection methods in recent years. Recent advances of deep CNN <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b20">[21]</ref> has made a great improvement on the general object detection tasks for natural images. While early approaches <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b23">[24]</ref> simply formulate the detection into an sliding window traversal + classification problem (background VS objects of interest), recent CNN based detection methods mainly focus on the following four aspects: 1) efficient algorithms for multi-scale detection, 2) accurate bounding-box prediction 3) training with imbalance data and 4) speedup strategy.</p><p>For multi-scale detection, the most straight forward way is to build feature map pyramids <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. Recent progress includes using external object proposals <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b28">[29]</ref>, multi-scale anchors <ref type="bibr" target="#b29">[30]</ref> and integrated multi-scale detection <ref type="bibr" target="#b30">[31]</ref>. For bounding-box prediction, efforts have been made to improve the accuracy by stepwise bounding-box correction <ref type="bibr" target="#b31">[32]</ref> and the probabilistic inference of location <ref type="bibr" target="#b32">[33]</ref>. For the problem of imbalance data, some useful strategies include cascaded training/detection <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b25">[26]</ref> and hard-negative mining <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b33">[34]</ref>. Time efficiency is another key factor in the object detection, especially for remote sensing images. Early approaches, which are designed based on sliding window techniques, usually search for objects exhaustively at different locations and scales <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b25">[26]</ref>. While it may be possible for fast detection of certain object categories (e.g. face and pedestrian <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b23">[24]</ref>) with extensive speed up strategies (e.g. integral image/channel <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b34">[35]</ref>, feature pyramid approximation <ref type="bibr" target="#b35">[36]</ref> and cascaded-detection <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>), it is still hard to extend such ideas to the fast detection of multiple categories. Some of the recent CNN based detection methods <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, with larger model capacity and stronger representation ability, apply a fixed set of filters with multiple bounding-box references on a fixed set of convolutional feature maps to speed up the detection process.</p><p>Despite that great efforts have been made, for some important remote sensing applications such as wide-scale remote sensing monitoring or even on-orbit target detection, these algorithms are still far from being practical at present due to the complex background features, drastically changes of the target scales <ref type="bibr" target="#b40">[41]</ref> and the computational cost requirements. Although some multi-scale techniques have largely improved the detection performance of small objects, such as detecting objects from the feature maps of different depth <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b41">[42]</ref>, or using multi-scale feature fusion <ref type="bibr" target="#b42">[43]</ref>- <ref type="bibr" target="#b44">[45]</ref> to improve the representation ability of small objects, in this paper, we try to study this problem from another point of view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RANDOM ACCESS MEMORIES</head><p>In this section, we will give a detailed description to our detection paradigm and explain how it works with a CNN based detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Fisher Information</head><p>The training process of any target detector is essentially a likelihood estimation process. Here we follow a classical learning paradigm that the optimal model Ĥ can be determined by the i.i.d. training data D tr and the model's hypothesis space F Ĥ = argmax H∈F p(D tr |H).</p><p>(</p><formula xml:id="formula_0">)<label>1</label></formula><p>For any early sliding window based detectors <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b25">[26]</ref>, D tr means the collection of image data and label within any sliding window region, while for the recent CNN based detectors <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, D tr corresponds to that of the respective field. Arguably, under the view of Bayesian statistics, Ĥ is not unique since for any perturbations of training data, the estimated model maybe different. To describe its potential variants as the data D tr changes, here we introduce fisher information as a basic metric. The fisher information is a way of measuring the amount of information that an observable training data D tr carries about an unknown model H. For a model with single parameter θ , the fisher information can be defined as follows:</p><formula xml:id="formula_1">I( θ) = -E{ ∂ 2 ∂θ 2 log( p(D tr |θ))},<label>(2)</label></formula><p>which is equivalent to the second derivative (if it exists) of the negative log likelihood function. I(θ ) can be viewed as a measurement of the "curvature" of the support curve near the MLE point θ , where a "blunt" support curve (one with a shallow maximum) could have a low negative expected second derivative, and thus low information, while a sharp one could have a high information.</p><p>For a detection model H with multiple parameters θ = [θ 1 , θ 2 , . . . , θ N ] T , the fisher information can be written as its matrix form I( Ĥ), where its element-wise representation is</p><formula xml:id="formula_2">I( Ĥ) i, j = -E{ ∂ 2 ∂θ i ∂θ j log( p(D tr |θ ))}. (<label>3</label></formula><formula xml:id="formula_3">)</formula><p>I( Ĥ) is equivalent to the Hessian matrix (if it exists) of the negative log likelihood function. It can be also understood as a metric of an appropriate changes of any variables induced from the Euclidean metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Laplace Approximation</head><p>Here we use a simple but widely used framework called the Laplace approximation, that aims to find a Gaussian approximation to a probability density defined over a set of continuous variables <ref type="bibr" target="#b15">[16]</ref>. Laplace Approximation can be implemented by fitting the model with a second order Taylor expansion of the log likelihood function log( p(H|D tr )) around the its maximum point Ĥ. The estimated distribution finally is conducted by Ĥ as the its mean and I( Ĥ) as its covariance matrix</p><formula xml:id="formula_4">H ∼ N (H| Ĥ, I( Ĥ) -1 ). (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>In this way, the probability value at any changes of the model can be represented as</p><formula xml:id="formula_6">p( Ĥ + δH) ∝ ex p(- 1 2 δH 2 I( Ĥ) ), (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>where</p><formula xml:id="formula_8">• 2 I( Ĥ)</formula><p>represents the I( Ĥ) norm metric of the model changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Updating Memories</head><p>The fisher information can be regarded as a very important prior that guides the update of the model. During detection phase, the detection model Ĥ will be changed according to both of the approximated distribution (4) and the observation D ob . The posterior distribution of H can be represented as</p><formula xml:id="formula_9">p(H|D ob ) = p(H) p(D ob |H) p(D ob ) ∼ p(H) p(D ob |H).<label>(6)</label></formula><p>By dividing the likelihood function p(D ob |H) into the positive part (target of interest D + ob ) and negative part Fig. <ref type="figure">2</ref>. Contour line of p(H|D tr ) on a group of toy data. in our method, the Laplace approximation acts like a regularization when updating the memories at detection phase. those parameters with small engine values of fisher information matrix turn out to be more likely to be updated, while those with large engine values are relatively stable.</p><p>(undesired background D - ob ), and substituting (5) into p(H), the posterior can be expanded as</p><formula xml:id="formula_10">p(H|D ob ) ∼ p(H) p(D + ob |H) p(D - ob |H) = ex p(- 1 2 H -Ĥ 2 I( Ĥ) ) p(D + ob |H) p(D - ob |H)<label>(7)</label></formula><p>For any latest observations, the model should be updated to access the maximum of posterior to best fit the training and testing data at the same time</p><formula xml:id="formula_11">Ĥnew = argmax H∈F p(H|D ob ). (<label>8</label></formula><formula xml:id="formula_12">)</formula><p>Clearly, for some directions of I( Ĥ), a slight change of the model δH would cause a rapid decline of the probability p(H|D ob ), while for some other directions the probability may keep stable. Fig. <ref type="figure">2</ref> shows the contour line of p(H|D tr ) on a group of toy data. If we move a step further and make an eigenvalue decomposition of the fisher information matrix I(H) = U U T , where = di ag(λ 1 , λ 2 , . . . , λ N ), an important conclusion can be easily derived that the direction of the eigenvector with the largest eigenvalue λ max will be the fastest dropping direction of p( Ĥ). We call those associated parameters permanent memories. The direction with the smallest eigenvalue λ min will be that of the slowest one. We call those associated parameters short time memories. The permanent memories acts as an important regularization at inference stage, while short time memories are more likely to be updated according to the observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Integrating the Small Target Priors</head><p>It is hard to optimize (8) directly since it contains latent variables D + ob and D - ob , where "+" and "-" represent the latent labels. Since the amount of the negatives are usually far greater than the positives for a remote sensing image: N -N + , we are reasonable to neglect the positive latent variables. In this way, (8) can be relaxed to its negative logarithmic likelihood , <ref type="bibr" target="#b8">(9)</ref> where H-Ĥ 2</p><formula xml:id="formula_13">I( Ĥ)</formula><p>can be seen as a constraint which controls the update range of the model. α &gt; 0 is a positive number controls the degree of the constraint.</p><p>Despite the latent variables has been eliminated, another problem lies that the high dimensional parameter space of a deep CNN makes it still very difficult to directly compute and store the fisher information matrix I( Ĥ). A further assumption can be made that only a small portion of the memories are updated during detection phase. Here we take a simple and straightforward manner, that only optimizing the parameters of the final output layer while keeping those of previous layers fixed as feature extractor. More specifically, when a testing image is fed into the network, passing through all convolutional layers and finally comes to the full-connected layer, the RAM operation should be performed. Since the regularization H -Ĥ 2</p><formula xml:id="formula_14">I( Ĥ)</formula><p>is always convex at any local/gobal maximum point, any types of the convex loss functions of -log( p(D|θ )), e.g. square loss, smoothed L1 loss <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b29">[30]</ref>, L2 loss or cross-entropy loss <ref type="bibr" target="#b45">[46]</ref> would lead to a final unique solution. In this way, ( <ref type="formula">9</ref>) can be efficiently optimized by any convex optimization algorithms, e.g. gradient decent or Newton method, and global optimal solution will always be guaranteed. For some special cases, saying that using square loss or smoothed L1 loss, ( <ref type="formula">9</ref>) would degenerate into a nonconstraint quadratic programming problem, thus a closed form solution can be simply obtained, as we will see it later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Implementation Details</head><p>Fig. <ref type="figure" target="#fig_3">3</ref> shows the backbone of our detector and the random access memories operation during detection phase. Our implementation details are given as follows.</p><p>1) Backbone: Current CNN based object detection methods can be divided into two important branches based on their processing flow. The first branch is cascaded detectors where the detection is performed from a coarse to fine manner <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b29">[30]</ref>, while the second branch is integrated ones where the detection is evaluated only once <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>. In this paper, we take the second one as the backbone of our detector which is faster and may have larger rooms of improvements in the future. Here we have designed three types of networks, a tiny one, a medium one and a large one. Their configurations are listed as follows ("Layer-name(Number of Filters, Size/Stride)"), where the ReLU layer between a convolutional layer and a maxpooling layer is omitted for simplicity: Large CNN: VGG-f <ref type="bibr" target="#b19">[20]</ref> (decision layer removed) + fullcnect.(4096 × k(C + 4)).  For tiny and medium size networks, similar to the VGG network, we use mostly 3 × 3 filters <ref type="bibr" target="#b19">[20]</ref> except for the last fully-connection layer (1 × 1 convolution). The output depth of the last layer is k(C + 4), which depends on the number of multi-scale anchors k, number of target categories C, and 4 coordinates of the bounding box. Our implementation of the network is based on matconvnet-1.0 beta23 <ref type="bibr" target="#b46">[47]</ref>.</p><p>2) Loss Layer Design: We follow the idea of pre-defined anchors for multi-scale detection <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>. The output dimensions are organized as in Fig. <ref type="figure" target="#fig_4">4</ref>. There are 3 predefined anchors: 0.9 × 0.9, 0.7 × 0. For bounding box prediction, we use the smoothed L1 loss <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b29">[30]</ref> that is less sensitive to outliers than the L2 loss used in <ref type="bibr" target="#b26">[27]</ref> </p><formula xml:id="formula_15">L reg. (t i , t * i ) = 5(t i -t * i ) 2 |t i -t * i | 0.1 |t i -t * i | -0.05 else. (<label>11</label></formula><formula xml:id="formula_16">)</formula><p>We use the parameterized coordinates as it was used in <ref type="bibr" target="#b29">[30]</ref>.</p><p>For category scoring, we also simply see it as a score regression problem with the smoothed L1 loss. Similar ideas have been used in <ref type="bibr" target="#b30">[31]</ref>. Since weights of the fully connected layer are learnt with independent loss. The memory update operation can be individual performed with corresponding anchor scale and target category. When we use the smoothed L1 loss, the negative log likelihood loss term in ( <ref type="formula">9</ref>) can be represented as</p><formula xml:id="formula_17">-log( p(D - ob |H)) = E (x,y)∼D ob {L reg (θ θ θ T x, y)}, (<label>12</label></formula><formula xml:id="formula_18">)</formula><p>where x refers to the features of the last convolutional layer, θ and y refer to the convolutional filters and ground-truth label of specific category and anchor-scale. Then the optimization  <ref type="formula">9</ref>) can be further written as</p><formula xml:id="formula_19">min θ θ θ L(θ θ θ |D ob ) = E (x,y)∼D ob {L reg (θ θ θ T x, y)} +αθ θ θ -θ θ θ 2 I( θ θ θ). (<label>13</label></formula><formula xml:id="formula_20">)</formula><p>The above optimization problem has an approximated closed form solution:</p><formula xml:id="formula_21">θ new = (C + αI( Ĥ)) -1 (E (x,y)∼D ob {yx} + αI( Ĥ) θ),<label>(14)</label></formula><p>where C and I( Ĥ) has the following expression:</p><formula xml:id="formula_22">I( Ĥ) = E (x,y)∼D tr {ξ(x, y)xx T } C = E (x,y)∼D ob {ξ(x, y)xx T }, ξ(x, y) = 1 | θ T x -y| &lt; 0.1 0 else.<label>(15)</label></formula><p>In this way, the model can be updated easily without any iterative operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. LEVIR: A NEW DATASET FOR REMOTE SENSING TARGET DETECTION</head><p>Large and challenging datasets are necessary for the progress in remote sensing applications. Our goal in introducing the LEVIR detection dataset is to provide a better benchmark and statistically meaningful evaluations for those current and future detection methods.</p><p>LEVIR consists of a large number of high resolution Google Earth images with over 22k images of 800 × 600 pixels and 0.2m ∼ 1.0m/pixel's resolution. LEVIR covers most types of ground features of human living environment, e.g. city, country, mountain area and ocean. Extreme land environments such as glacier, desert and gobi are not considered in our dataset. There are 3 types of targets in our dataset: airplane, ship (including both inshore ships and offshore ships) and oilpot. We label all the images for a total of 11k independent bounding boxes (BB) including 4,724 airplanes, 3,025 ships and 3,279 oilpots. The average number of targets per image is 0.5. For every image in which a given target of interest is visible, we draw a tight BB that indicates the full extent of the entire target. It should be noticed that for those targets that are partially outside the image boundary or occluded by other objects, this involves estimating the location of hidden parts. A summary of our dataset is given in Table . I. During the last decades, some efforts have been made to develop public dataset for target detection from aerial and satellite images. Here we list the detailed overview of three public available datasets:</p><p>NWPU-VHR-10 <ref type="bibr" target="#b4">[5]</ref> is a ten-class dataset which contains 800 images. There are a total of 757 airplanes, 302 ships, 655 storage tanks, 390 baseball diamonds, 524 tennis courts, 159 basketball courts, 163 ground track fields, 224 harbors, 124 bridges, and 477 vehicles manually annotated with BBs for ground truth.</p><p>TAS Aerial Car Detection Dataset (TACDD) <ref type="bibr" target="#b47">[48]</ref> consists of 30 images acquired from Google Earth with the size of 729 × 636 pixels. 1319 vehicles are manually labeled with BBs for groundtruth.</p><p>Overhead Imagery Research Dataset (OIRDS) <ref type="bibr" target="#b48">[49]</ref> is designed for vehicle detection with the collection of about 900 images captured by aircraft-mounted camera. The total number of labeled vehicles is about 1,800.</p><p>Table <ref type="table" target="#tab_2">II</ref> provides a detailed comparison of LEVIR and other existing remote sensing target detection datasets. NWPU-VHR-10 has helped drive recent advances <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b9">[10]</ref> in target detection of remote sensing images. TACDD and OIRDS remain the most widely used for vehicle dtection <ref type="bibr" target="#b47">[48]</ref>- <ref type="bibr" target="#b49">[50]</ref>. However, their defects are also obvious. Firstly, all these datasets are too limited to obtain statistically meaningful training and evaluation result for the most current deep learning based detection methods. Secondly, NWPU-VHR-10 also shows a strong bias toward large, unoccluded targets. Last but not least, these datasets also have the "center biased" problem, saying that the target tends to appear near the center of a image. In fact, unlike natural images where their contents are limited and the objects of interests are usually near the center of an image, remote sensing images, if not manually selected and cropped, usually have no specific region of interest. In Fig. <ref type="figure" target="#fig_6">5</ref>, we histogram the size of all BBs of LEVIR and NWPU-VHR-10. We can see LEVIR contains targets with a larger range of scales, especially for small target. Fig. <ref type="figure" target="#fig_7">6</ref> shows the heat-map of the BB location probability of LEVIR (left) and NWPU-VHR-10 (right). As we can see, LEVIR shows a more evenly distributed BB location than that of NWPU-VHR-10.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL RESULTS AND ANALYSIS</head><p>In this section, we have made an extensive evaluation and comparison with several variants of our model and other detection methods. Our experiments are performed on LEVIR and NWPU-VHR-10. For LEVIR, 70% images are used for training and rests are used for test. For NWPU-VHR-10, we use the same training-testing split criterion with other comparison methods as they originally used in their paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Training Details 1) Data Augmentation:</head><p>To detect targets with different directions, each individual target is randomly rotated for several times and then randomly resized and translated to make sure that there are sufficient information for each anchor-scale to learn. We use the similar criterion that is used in <ref type="bibr" target="#b29">[30]</ref> that we assign a target label to two kinds of anchors: 1) the anchor with the highest IoU overlap with a ground-truth box, or 2) an anchor whose IoU overlap higher than 0.5 with any groundtruth box.</p><p>2) Pre-Training: For our large-sized model, the initial weight is transferred from that of VGG-f which is trained on the ImageNet. For our tiny-sized and medium-sized models, the networks are first pre-trained with targets and randomly generated backgrounds. The training is performed for 20 epochs by back-propagation and stochastic gradient descent (SGD) <ref type="bibr" target="#b45">[46]</ref>.</p><p>3) Hard-Negative Mining: After pre-training, we fine-tune the model, where in each mining iteration the training set is augmented with hard negative examples. We assign a background label to any detected false negative anchor if its IoU overlap is lower than 0.2 for all ground-truth BBs. Anchors that are neither positive nor negative do not contribute to the training. In each mining iteration, the detector collects over 500 hard negatives and the SGD is performed for 2 epochs.</p><p>4) Multi-Octave Detection: Although recent advances <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref> advocate using single scale feature map to detection target of multiple scales since it offers trade-off between accuracy and speed, detection on pyramid feature maps still can be proved with better performance, especially for small sized targets <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b42">[43]</ref>. Since there is a large scale variation of remote sensing targets, we build a sparse image pyramid with octave stride os = 2 as the network's inputs. For scale variations within a single octave, the scale range can be well captured by the detector of multi-scale anchors. The regularization parameter α of ( <ref type="formula">9</ref>) is set to 100 for all experiments. All the important parameters we used are listed in Table <ref type="table" target="#tab_2">III</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Overall Results Statistics 1) Comparisons With Baselines:</head><p>For fair comparisons with our paradigm, we have designed three baseline methods:</p><p>Baseline 1: tiny-networks (TINY-BASE), • Baseline 2: medium-networks (MEDIUM-BASE),</p><p>• Baseline 3: VGG-f based networks (LARGE-BASE), and their improved variants with memories-updating:</p><p>• Proposed 1: tiny-networks+ram (TINY-RAM),</p><p>• Proposed 2: medium-networks+ram (MEDIUM-RAM),</p><p>• Proposed 3: VGG-f based networks+ram (LARGE-RAM). During evaluation, some ambiguous instances are excluded from our dataset. There are two kinds of situations that we identify it as an ambiguous target. The first type is that the targets bounding-box are partially clustered or outside (larger than 3/4 of its area) the image. The second type refers to the instance with very small size, whose length is smaller than 20 pixels. To detect smaller target, we suggest using higher resolution images. Any false detection or missing of these targets will not be taken into account neither as a false positive nor as a true positive. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE IV COMPARISONS OF THE PROPOSED METHODS AND THEIR BASELINES</head><p>All comparisons use the same settings including networks' hyperparameters, training parameters and detection parameters. Table <ref type="table">IV</ref> shows the three baseline methods and their variations on LEVIR dataset. <ref type="foot" target="#foot_1">2</ref> We can clearly observe RAM gives an overall improvement on all of the three classes and three varies sized models. We also observed that the performance enhancement of the small sized model is more remarkable than that of the medium and large sized model. This is simply because a smaller model tends to be saturated when training with background of complex distribution. RAM helps small model concentrate on the newly observed data and "forget" part of the useless memories previously learned from the training data. Fig. <ref type="figure" target="#fig_8">7</ref> and Fig. <ref type="figure">8</ref> shows some examples of detection results on LEVIR dataset and NWPU-VHR-10 dataset.</p><p>Fig. <ref type="figure">9</ref> shows the eigenvalues of the fisher information matrices accumulated on our training data (include pre-training data and hard negatives) at MLE point. As we can see, most of the eigenvalues stay almost close to zero (marked as dark blue bars) except for only a few large ones (marked as light blue bars). This phenomenon indicates that although our model is fully trained, their uncertainty is still large at these corresponding eigenvector directions and any disturbance on these directions will not make clear decrements of the training objective function. These eigenvectors serve as the main paths for updating the model.</p><p>2) Comparisons on NWPU-VHR-10: We also evaluate the performance of our method on NWPU-VHR-10 dataset and compare with other state-of-the-art remote sensing detection methods e.g. RICNN <ref type="bibr" target="#b9">[10]</ref>, COPD <ref type="bibr" target="#b4">[5]</ref> and FDDL <ref type="bibr" target="#b3">[4]</ref>. Among them, RICNN learns a rotation-invariant CNN model by introducing and learning a new Rotation-Invariant layer on the basis of the existing CNN architectures to detect remote sensing target with different orientations. COPD is designed based on the COllection of Part Detectors with a sliding window approach on Histograms of Oriented Gradients map and linear support vector machine classifier. FDDL is designed based on Fisher Discrimination Dictionary Learning method, where a set of target candidate regions are firstly generated by a saliency detection method and then a sparse representation based classifier is adopted on each candidate to perform multiclass detection.</p><p>We use the same training-testing split criterion as those was used in <ref type="bibr" target="#b9">[10]</ref> for a fair comparison.  from Google Earth images. The results of RICNN, COPD and FDDL are reported by <ref type="bibr" target="#b9">[10]</ref>. We can see our model (LARGE-RAM) obtains the best detection results.</p><p>3) How Important is Fisher Information?: The way to compute the fisher information matrix is a key point of our method that it describes how well the model distribution is approximated at its MLE point. In this experiment, we give two approximation forms to its original one: 1) the diagonal matrix approximation where only its diagonal elements are left while others are set zeros and 2) the identity matrix approximation where the matrix is further simplified as an identity matrix. Table <ref type="table">VI</ref> shows their comparisons based on the three proposed variations. For identity matrix approximation, updating memories under such priors means that the pretrained model has an equivalent probability of updating their parameters in any directions in the parameter space. The updating process can be simply viewed as a maximization of likelihood of observations under a Euclidean distance constraint. We can see there is a little drop of accuracy compared with their fisher-approximated models in this condition.  For diagonal matrix approximation, a similar explanation can be given while the only difference is that the dimensions are weighted by its diagonal elements when computing the Euclidean distance constraint. The accuracy also drops at this time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Hyper-Parameter Stability:</head><p>The regularization coefficient α of (9) serves as a very important hyper-parameter in our method. Fig. <ref type="figure" target="#fig_10">10</ref> and Fig. <ref type="figure" target="#fig_11">11</ref> show the performances of the above three approximation ways and their baselines respectively with different α. Performance enhancement can be observed with a wide range of the choice of parameter. When α is set too small, the accuracy is lower than the baseline method as we expected. This is because the constraint on the model near the MLE point is so weak that some targets are over-suppressed. When α is set too large, the model will be bounded at a very small feasible region near its MLE point thus the improvement is very little. When α → ∞, H new equals to Ĥ.</p><p>5) Speed Performance: We test our method on an Intel i7 PC with a Nvidia GTX 1080Ti graphics card. We use the GPU to accelerate the training and detection process. The training process takes a few hours to days on various sized models. For a 800 × 600 sized input image, a fast version of our method only takes about 0.1s (10fps) to finish the forward pass (0.02s) + memories updating process (0.10s). Table VII lists the total detection time and memory updating time of the baseline methods and the proposed four variants. For RICNN and FDDL, the running time is reported by their authors, and the testing image is about the same size. The authors of COPD did not report their time. All our models run much faster in spite that the memory updating process takes the most of the running time. The main time cost when updating memories is from the matrix inversion operation in <ref type="bibr" target="#b14">(15)</ref>, which has a cubic computational complexity of the number of weights in fully connected layer.</p><p>With random access the memories, we can obtain a comparable or even higher detection accuracy of a larger model with less parameters and a faster detection speed. For example, in Table <ref type="table" target="#tab_2">III</ref> and Table VI, a lighter model, MEDIUM-RAM achieves higher mAP than a VGG-based baseline model LARGE-BASE (mAP 58.0% VS 57.7%), meanwhile, the former one has faster detection speed (speed: 0.492s VS 0.710s). Notice that the vgg-based model takes quite long time to update memories, this is because the dimension of its full-connected layer is 4096, which is much higher than other two models. Nevertheless, as long as the parameter number of the fully connected layer is well configured, the calculation time can be controlled within an acceptable range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>We propose a new paradigm called "Random Access Memories" for target detection for high resolution aerial remote sensing image. We also provide a new challenging dataset for remote sensing target detection which is one order of magnitude larger than existing datasets. Experiments have confirmed the validity of the proposed paradigm where noticeable improvements over a CNN based detectors can be observed. The proposed method outperforms several other state-of-the-art remote sensing target detection methods. Besides, RAM may open some novel opportunities of investigation in other applications under small target priors, such as the fast detection of natural image objects, instance segmentation and even image retrieval tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An overview of our proposed new detection paradigm. Different from pervious approaches that the detection model is explicitly determined by the MLE point of training data, the proposed paradigm formulates the detection from a Bayesian view that at detection phase, the model is updated to maximize its posterior that contemporarily determined by both training and observation.</figDesc><graphic coords="2,76.07,55.97,467.78,186.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>form min H L(H|D ob ) = -log( p(D - ob |H)) + αH -Ĥ 2 I( Ĥ)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Fig.3shows the backbone of our detector and the random access memories operation during detection phase. Our implementation details are given as follows.1) Backbone: Current CNN based object detection methods can be divided into two important branches based on their processing flow. The first branch is cascaded detectors where the detection is performed from a coarse to fine manner<ref type="bibr" target="#b26">[27]</ref>-<ref type="bibr" target="#b29">[30]</ref>, while the second branch is integrated ones where the detection is evaluated only once<ref type="bibr" target="#b30">[31]</ref>,<ref type="bibr" target="#b38">[39]</ref>,<ref type="bibr" target="#b39">[40]</ref>. In this paper, we take the second one as the backbone of our detector which is faster and may have larger rooms of improvements in the future. Here we have designed three types of networks, a tiny one, a medium one and a large one. Their configurations are listed as follows ("Layer-name(Number of Filters, Size/Stride)"), where the ReLU layer between a convolutional layer and a maxpooling layer is omitted for simplicity: Tiny CNN: conv.(256, 3 × 3/1) + max pool.(-, 3 × 3/3) + conv.(256, 3 × 3/1) + max pool.(-, 3 × 3/3) + conv.(512, 3 × 3/1)+max pool.(-, 2 ×2/2)+ f ull -cnect.(512 ×k(C +4)). Medium CNN: conv.(256, 3×3/1) + max pool.(-, 2 × 2/2) + conv.(512, 3 × 3/1) + max pool.(-, 2 × 2/2) + conv.(512, 3×3/1)+max pool.(-, 2×2/2)+conv.(1024, 3× 3/1)+conv.(1024, 1 ×1/1)+ f ull -cnect.(1024 ×k(C +4)).Large CNN: VGG-f<ref type="bibr" target="#b19">[20]</ref> (decision layer removed) + fullcnect.(4096 × k(C + 4)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. A detailed overview of our proposed new detection paradigm: backbone of the detector and random access memories. the RAM contains three steps: 1) approximating the unknown distribution p(H) from Ĥ by Laplace approximation, 2) updating the memories to get the new detection model H new .</figDesc><graphic coords="5,80.15,54.89,453.62,259.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.4. An example of respective field and its multi-anchor boxes. For each anchor scale, the ground-truth label is arranged as two parts: category score and bounding box location coordinates. the final ground-truth label of the respective field can be formed by end-to-end connection of the label of each anchor scale.</figDesc><graphic coords="5,54.83,359.57,240.98,167.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>7 and 0.5 × 0.5 for each detection window. For each anchor scale, a multi-task loss is designed which consists of a category scoring loss and a bounding box prediction loss L(p, p * , t, t * ) = L scoring (p, p * ) + β I (p)L pred. (t, t * ) I (p) = 1 IoU{Anchor(p), t * } &gt; 0.5 0 else (10) where IoU{•} refers to the intersection over union overlap between two regions. Anchor(p) means the anchor box of a certain scale. I (p) is an indicator controls whether it is an target of interest under a certain anchor scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Distribution of BB size of LEVIR and NWPU-VHR-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Heat map of BB location for LEVIR (left) and NWPU-VHR-10 (right).</figDesc><graphic coords="7,68.03,179.33,212.54,76.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Selected examples of our detection results on LEVIR dataset (categories: airplanes, ships, oilpots).</figDesc><graphic coords="8,57.95,58.13,496.10,282.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .Fig. 9 .</head><label>89</label><figDesc>Fig. 8. Selected examples of our detection results on NWPU-VHR-10 dataset (categories: airplanes, ships, storage tanks, baseball diamonds, tennis courts, basketball courts, ground track fields, harbors, bridges and vehicles).</figDesc><graphic coords="9,57.47,58.61,496.10,165.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Performances of our method on LEVIR dataset with different values of regularization coefficient α.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Performances of our method on NWPU-VHR-10 dataset with different values of regularization coefficient α.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II A</head><label>II</label><figDesc>COMPARISON BETWEEN LEVIR AND OTHER REMOTE SENSING DETECTION DATASETS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III PARAMETER SETTINGS</head><label>IIISETTINGS</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VII TIME</head><label>VII</label><figDesc>PERFORMANCE OF DIFFERENT METHODS FOR A 600 × 800 IMAGE</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>LEVIR is the name of the authors' laboratory: LEarning, VIsion and Remote sensing laboratory.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Sine LARGE-RAM model takes too much time to in RAM process, we only sample a subset of our test data for this model. For other models, the full test set is used.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Natural Science Foundation of China under Grant 61671037, in part by the Beijing Natural Science Foundation under Grant 4152031, in part by the project of State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, under Grant BUAA-VR-16ZZ-03, and in part by the Excellence Foundation of BUAA for Ph.D. Students under Grant 2017056. The associate editor coordinating the review of this manuscript and approving it for publication was Prof.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A survey on object detection in optical remote sensing images</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="11" to="28" />
			<date type="published" when="2016-07">Jul. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ship detection in high-resolution optical imagery based on anomaly detector and local shape feature</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4511" to="4523" />
			<date type="published" when="2014-08">Aug. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Airplane detection based on rotation invariant and sparse coding in remote sensing images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optik-Int. J. Light Electron Opt</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="5327" to="5333" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient, simultaneous detection of multi-class geospatial targets based on visual saliency modeling and discriminative learning of sparse coding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="48" />
			<date type="published" when="2014-03">Mar. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-class geospatial object detection and geographic image classification based on collection of part detectors</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="132" />
			<date type="published" when="2014-12">Dec. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Vehicle detection in remote sensing imagery based on salient information and local shape feature</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optik-Int. J. Light Electron Opt</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="2485" to="2490" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Weakly supervised learning based on coupled convolutional neural networks for aircraft detection</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="5553" to="5563" />
			<date type="published" when="2016-09">Sep. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Object detection in optical remote sensing images based on weakly supervised learning and high-level feature learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3325" to="3337" />
			<date type="published" when="2015-06">Jun. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A hierarchical oil tank detector with deep surrounding features for high-resolution optical satellite imagery</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4895" to="4909" />
			<date type="published" when="2015-10">Oct. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning rotation-invariant convolutional neural networks for object detection in VHR optical remote sensing images</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="7405" to="7415" />
			<date type="published" when="2016-12">Dec. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ship detection in spaceborne optical image with SVD networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5832" to="5845" />
			<date type="published" when="2016-10">Oct. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Can a machine generate humanlike language descriptions for a remote sensing image?</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3623" to="3634" />
			<date type="published" when="2017-06">Jun. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Maritime semantic labeling of optical remote sensing images with multi-scale fully convolutional network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="480" to="500" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hyperspectral image processing for automatic target detection applications</title>
		<author>
			<persName><forename type="first">D</forename><surname>Manolakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lincoln Lab. J</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="116" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Feynman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Leighton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sands</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Lindsay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Feynman Lectures on Physics: Quantum Mechanics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="1966">1966</date>
			<publisher>AIP</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<title level="m">Gaussian Processes for Machine Learning</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<title level="m">Machine Learning: A Probabilistic Perspective</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Workshop Neural Inf. Process. Syst. (NIPS)</title>
		<meeting>Conf. Workshop Neural Inf. ess. Syst. (NIPS)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1409.1556" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2001-12">Dec. 2001</date>
			<biblScope unit="page" from="511" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005-06">Jun. 2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A discriminatively trained, multiscale, deformable part model</title>
		<author>
			<persName><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008-06">Jun. 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010-09">Sep. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014-06">Jun. 2014</date>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
			<date type="published" when="2015-09">Sep. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Workshop Neural Inf. Process. Syst. (NIPS)</title>
		<meeting>Conf. Workshop Neural Inf. ess. Syst. (NIPS)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">AttentionNet: Aggregating weak directions for accurate object detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Paek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2659" to="2667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">LocNet: Improving localization accuracy for object detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="789" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Training regionbased object detectors with online hard example mining</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Integral channel features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Brit. Mach. Vis. Conf. (BMVC)</title>
		<meeting>Brit. Mach. Vis. Conf. (BMVC)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="91" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fast feature pyramids for object detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1532" to="1545" />
			<date type="published" when="2014-08">Aug. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fast human detection using a cascade of histograms of oriented gradients</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-C</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006-06">Jun. 2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1491" to="1498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pedestrian detection at 100 frames per second</title>
		<author>
			<persName><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="2903" to="2910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">YOLO9000: Better, faster, stronger</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1612.08242" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep learning for remote sensing data: A technical tutorial on the state of the art</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Mag</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="22" to="40" />
			<date type="published" when="2016-06">Jun. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A unified multi-scale deep convolutional neural network for fast object detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="354" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1612.03144" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">HyperNet: Towards accurate region proposal generation and joint object detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="845" to="853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="2874" to="2883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deep</forename><surname>Learning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">MatConvNet: Convolutional neural networks for MATLAB</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 23rd ACM Int. Conf. Multimedia</title>
		<meeting>23rd ACM Int. Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="689" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning spatial context: Using stuff to find things</title>
		<author>
			<persName><forename type="first">G</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Comput. Vis</title>
		<meeting>Eur. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="30" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Overhead imagery research data set-An annotated data library &amp; tools to aid in the development of computer vision algorithms</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tanner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Appl. Imag. Pattern Workshop</title>
		<meeting>Appl. Imag. Pattern Workshop</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Vehicle detection using partial least squares</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1250" to="1265" />
			<date type="published" when="2011-06">Jun. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
