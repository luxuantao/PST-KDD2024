<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient and effective training of language and graph neural network models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-06-22">22 Jun 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Vassilis</forename><forename type="middle">N</forename><surname>Ioannidis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Amazon Web Services AI</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiang</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Amazon Web Services AI</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Da</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Amazon Web Services AI</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Amazon Web Services AI</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Houyu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Amazon Search AI</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Ma</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Amazon Search AI</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Amazon Search AI</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Belinda</forename><surname>Zeng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Amazon Search AI</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Trishul</forename><surname>Chilimbi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Amazon Search AI</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient and effective training of language and graph neural network models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-06-22">22 Jun 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2206.10781v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Can we combine heterogenous graph structure with text to learn high-quality semantic and behavioural representations? Graph neural networks (GNN)s encode numerical node attributes and graph structure to achieve impressive performance in a variety of supervised learning tasks. Current GNN approaches are challenged by textual features, which typically need to be encoded to a numerical vector before provided to the GNN that may incur some information loss. In this paper, we put forth an efficient and effective framework termed language model GNN (LM-GNN) to jointly train large-scale language models and graph neural networks. The effectiveness in our framework is achieved by applying stage-wise fine-tuning of the BERT model first with heterogenous graph information and then with a GNN model. Several system and design optimizations are proposed to enable scalable and efficient training. LM-GNN accommodates node and edge classification as well as link prediction tasks. We evaluate the LM-GNN framework in different datasets performance and showcase the effectiveness of the proposed approach. LM-GNN provides competitive results in an Amazon query-purchase-product application.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>GNNs rely on a layered processing architecture comprising trainable graph convolutional operations to linearly combine features per graph neighborhood, followed by pointwise nonlinear functions applied to the linearly transformed features <ref type="bibr" target="#b5">[6]</ref>. GNNs have shown remarkable success in a variety of graph machine learning tasks both in supervised and unsupervised learning settings <ref type="bibr" target="#b12">[13]</ref>. Typically, the graphs used for profiling GNN models have node features as numerical attributes. These numerical attributes may be the output of network that encodes a much richer original information that is in the form of text or picture. One could apply such a general pre-trained network to extract the represenations and use them as feature vectors in a GNN. However, as we detail in this work such an approach is not optimal. This raises the question of how can we train better GNN models with rich text features. This work presents a stage-wise fine-tuning framework termed LM-GNN for encoding text data with transformers and GNN models.</p><p>We implement a sequence of fine-tuning steps to gradually infuse the transformer model with graph structure information. Our study reveals the necessity of pre-fine-tuning the transformer for graphaware tasks. The graph-aware BERT is subsequently fine-tuned together with a GNN model for performing any downstream tasks, which allows the model to access multi-hop information. Our stagewise fine-tuning, besides achieving good performance, significantly reduces training time compared to end-to-end training without our stage-wise approach because the mode converges faster. Further, LM-GNN is a distributed framework which can scale to hundreds of millions nodes. Besides improv-ing the model performance we implement a series of system and design optimizations to speed up the overall training speed. Our LM-GNN framework improves compared to baseline performance in four public datasets and one query-purchase-product application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Node classification is typically formulated as a semi-supervised learning (SSL) task over graphs, where the labels for a subset of nodes are available for training <ref type="bibr" target="#b3">[4]</ref>. GNNs achieve state-of-the-art performance in SSL by utilizing regular graph convolution <ref type="bibr" target="#b16">[17]</ref> or graph attention <ref type="bibr" target="#b22">[23]</ref>, while these models have later been extended in the heterogeneous graph setting <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b24">25]</ref>. Similarly, another prominent graph machine learning task is link prediction with numerous applications in recommendation systems <ref type="bibr" target="#b23">[24]</ref> and drug discovery <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b14">15]</ref>. Knowledge-graph (KG) embedding models for link prediction rely on mapping the nodes and edges of the KG to a vector space by maximizing a score function for existing KG edges <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>. Relational graph convolutional network (RGCN) models <ref type="bibr" target="#b20">[21]</ref> have been successful in link prediction and contrary to KG embedding models can further utilize node features. The aforementioned graph machine learning tasks may also be addressed using unsupervised learning approaches. Graph representation learning approaches map nodes in an embedding space where the graph topological information and structure is preserved <ref type="bibr" target="#b12">[13]</ref>. Such unsupervised representations can be consumed by any downstream model for task-specific prediction. Unsupervised methods employ a GNN encoder that generates node embedding and is supervised by a task dependent decoder. Typically decoders perform matrix factorization <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b18">19]</ref>, random walks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20]</ref>, or a combination of learning tasks <ref type="bibr" target="#b15">[16]</ref>. Language models (LM)s are powerful in modeling text data <ref type="bibr" target="#b8">[9]</ref>. Harnessing the power of LMs with graph data is under-explored. This work details a framework for training large-scale LMs jointly with GNNs. Recent work <ref type="bibr" target="#b7">[8]</ref> also identifies that pre-training BERT models in graph data can be beneficial and exploits a neighborhood prediction objective to enrich the BERT model with graph information. However, this work <ref type="bibr" target="#b7">[8]</ref> did not explore to fine-tune the BERT and GNN model together. Another prominent work in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b31">32]</ref> trains GNN models for improving the search results in sponsored search. The work there can be seen as a special case of this framework, although <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b31">32]</ref> did not explore the stage-wise fine-tuning that we introduce in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Definitions and Problem formulation</head><p>A heterogeneous graph with T node types and R relation types is defined as</p><formula xml:id="formula_0">G := {{V t } T t=1 , {E r } R r=1 }.</formula><p>The node types represent the different entities and the relation types represent how these entities are semantically associated to each other. For example, in the query-product network of Fig. <ref type="figure" target="#fig_1">1a</ref>, the node types correspond to queries and products, whereas the relation types may correspond to whether a product was clicked based on a query and whether a product was purchased based on a query relations. The number of nodes of type t is denoted by N t and its associated node set by V t := {n t } Nt n=1 . The total number of nodes in G is N := </p><formula xml:id="formula_1">:= {(n t , r, n ? t ? ) ? V t ? V t ? }</formula><p>, holds all interactions of a certain type among V t and V t ? and may represent that a product is was-clicked-based on a query.</p><p>Each node n t is also associated with a short text. In the query-product network for example, each product is accompanied by a title and each query by the query text. Notice that different node types could have texts that are drawn from different distribution, e.g., the text for titles can be distinctly different from that for queries. Oftentimes, such text features are mapped to a F ? 1 embedding vector x nt by transformers in a task independent fashion. In this paper, we will explore different methods to implement this mapping, that directly relate to the downstream application of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Semantic encoder</head><p>We employ the BERT model <ref type="bibr" target="#b8">[9]</ref> as the transformer in the LM-GNN framework to encode the nodes textual semantics. Given a node's text BERT encodes the textual information to a F ? 1 embedding vector x nt . This embedding vector corresponds to the [CLS] token embedding of the BERT model and the mapping from the text to the embedding is defined as x nt := g BERT (n t ; W BERT ). The mapping is controlled by the learnable parameters W BERT . These parameters can be instantiated by any language model pre-training approach, e.g., masked language modeling (MLM). Pre-training BERT models on large unlabeled text data has shown significant benefit in different LM applications. However, transferring this benefit for graph ML applications is not straightforward. We employ the techniques in Sec. 4.3 to pre-train BERT models with graph data. For different nodetypes t ? {1, . . . , T } in the graph we may consider different semantic encoders; e.g. queries and products.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Graph Encoder</head><p>Although the LM-GNN framework can utilize any GNN model as an encoder <ref type="bibr" target="#b26">[27]</ref>, in this paper LM-GNN uses a modified RGCN encoder <ref type="bibr" target="#b20">[21]</ref>. RGCNs extend the graph convolution operation <ref type="bibr" target="#b16">[17]</ref> to heterogeneous graphs. The lth self-RGCN layer computes the nth node representation h (l+1) n as follows</p><formula xml:id="formula_2">h (l+1) n := ? ? ? W (l) self h (l) n + R r=1 n ? ?N r n W (l) r h (l) n ? ? ? ,<label>(1)</label></formula><p>where N r n is the neighborhood of node n under relation r, ? the rectified linear unit non linear function, W (l) r is a learnable matrix associated with the rth relation, and</p><formula xml:id="formula_3">W (l)</formula><p>self is a projection matrix for the nodes embedding in layer l. Our adaptation augments the messages at each layer with a projected transformation of the node embedding for that layer acting as a self loop with an appropriate weight matrix. This adaptation over the traditional RGCN the node's own representation based on the semantic encoder is essential for the downstream applications and needs to be treated separately by the model. The matrix H in this paper represents the embedding extracted in the final layer. The node features are the input of the first layer in the model i.e., h (0) n = x n , where x n is the node feature for node n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Supervision approaches</head><p>Structure prediction task. Link prediction is a specific type of structure prediction, which will be our focus in the following section. Consider the heterogeneous graph G in Sec. 3. Given the sets of links {E r } R r=1 , and the node features the goal of link prediction is to predict whether a new set of node pairs are linked or not.</p><p>Typically, structure prediction models utilize a contrastive loss function that requires the model to distinguish among positive and negative examples <ref type="bibr" target="#b28">[29]</ref>. In this context, positive examples are the set of existing links in the graph. The negative examples, which are links that the model should classify as nonexistent, are typically sampled from the missing links in the graph. For each positive triplet q = (n t , r, n ? t ? ) a number of negative links is generated by corrupting the head and tail entities at random (n t , r, ? ? t ? ) and (? t , r, n ? t ? ). The minimization function for link prediction can be defined as follows</p><formula xml:id="formula_4">(nt,r,n ? t ? )?D + ?D - log(1 + exp(-y ? c(n t , r, n ? t ? )),<label>(2)</label></formula><p>where c is a scoring function that return as scalar given the head, and tail nodes and the relation such as the DistMult model <ref type="bibr" target="#b27">[28]</ref>, D + and D -are the positive and negative sets of triplets and y = 1 if the triplet corresponds to a positive example and -1 otherwise.</p><p>Node prediction task. Each node n has a label y n ? {0, . . . , P -1}, which in the query-product network may represent the type of a product. In semi-supervised learning, we know labels only for a subset of nodes {y n } n?M , with M ? V. This partial availability may be attributed to privacy concerns (medical data); energy considerations (sensor networks); or unrated items (recommender systems). The N ?P matrix Y is the one-hot representation of the true node labels; that is, if y n = p then Y np = 1 and Y np ? = 0, ?p ? = p. The minimization objective in this task is the cross-entropy loss.</p><p>Edge prediction task. Each link l of a certain type r may also associated with a label of interest ? l ? {0, . . . , ? -1}. For example, consider the query to product graph in Fig. <ref type="figure" target="#fig_1">1a</ref>, where the task becomes to predict whether a pair of connected (query, product) is an exact search match or not.</p><p>Hence, here given an existing link we predict the class label, which is different from the structure prediction task in Sec. 4.3 where the objective is to predict the existence of a link.</p><p>The final predicted label for link l is the output of the following edge classification decoder</p><formula xml:id="formula_5">?l = W ec (h nt h n ? t ? )<label>(3)</label></formula><p>where W ec is a projection matrix of appropriate dimension and W ec denotes concatenation. Hence, the predicted label for l is a function of the entity embeddings for the nodes at the endpoints of the link n t , n ? t ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">LM-GNN: Training at scale using graph and text</head><p>A straightforward approach would directly use the LM encoder as a semantic encoder that feeds representations to the GNN encoder, and train such an architecture in an end-to-end fashion. However, training large scale language models and graph neural networks involves challenges relating to efficiency and effectiveness.</p><p>Effectiveness challenges stem from the fact that the pre-trained language model is well optimized in language tasks but has not trained before in graph tasks, which surfaces three main issues. ( <ref type="formula" target="#formula_2">1</ref>) Using such pre-trained transfomers may not be the most appropriate initialization and may trap the GNN to a sub-optimal local minimum. (2) Further, the well optimized transformer for the text tasks, may be more resistant in parameter updates. (3) Another hurdle stems from the random initialization of the GNN weights relative to the well-attuned transformer model, which may challenge the optimization of such an end-to-end framework.</p><p>Efficiency challenges relate to the large number of neighbors required by message passing in GNNs.</p><p>In mini-batch training of a k-layer GNN the k-hop ego-network of every target node is created and the target node embedding is computed as a function of all the node in the expanded ego-network (also known as source nodes). The number of source nodes in an ego-network may be very large even for shallow GNNs. Alleviating this issue, recent GNN approaches apply random sampling <ref type="bibr" target="#b11">[12]</ref> to reduce the number of neighbors. However, even with a shallow GNN (2 layers) and modest sampling (20 neighbors per layer), there are up to 400 source nodes. This remains a serious challenge in our unique setting where the transformer model needs to make 400 forward passes to calculate the embedding of a single target node. As a consequence the size of the required GPU is quite large even for small mini-batch sizes, which is a unique challenge in our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Addressing effectiveness</head><p>Consider the search graph among products and queries depicted in Fig. <ref type="figure" target="#fig_1">1a</ref>. Such a graph is typically encountered in catalog systems for query-product datasets. One could attempt to directly use the embedding generated by a transformer as an input to a GNN model for further fine-tuning. However, the transfomer embedding of such a model will only take into account the text information and may introduce noise at message passing. Indeed, Fig. <ref type="figure" target="#fig_1">1b</ref> shows that embeddings that are connected in the graph may be located in different regions of the embedding space. The poor performance of such a scheme is also detailed in the experiments; see Section 5. Our contribution in this context is to pre-fine-tune the transformer with graph information, which will endow the text embeddings with relational semantics and boost the performance when used as a semantic encoder.</p><p>Graph-aware pre-fine-tuning. We consider the structure prediction decoder that directly uses the scoring function c instantiated in Section 4.3. The graph-aware transformer model directly uses the structure prediction decoder as a supervision to predict whether an edge exists among two nodes or not. Specifically, the transformer generates the CLS token embeddings for the text associated with the nodes and the vectors are contributing to the loss (2). The resulting graph-aware transformer embeddings respect both the semantics introduced by the language as well as the relations imposed  The proposed LM-GNN captures the connectivity, higher order structure, as well as text semantics and provides a refined representations useful for retrieval tasks.</p><p>by the graph; see also Fig. <ref type="figure" target="#fig_1">1c</ref>. The graph-aware pre-fine-tuning also results LM that is more suitable for the end-to-end training with the GNN, which is also supported in Section 5. The top part of Fig. <ref type="figure">2</ref> showcases the graph-aware pre-fine-tuning pipeline.</p><p>Our proposed framework LM-GNN employs the graph-aware transformer as a semantic encoder that first embeds the text and then is fed to the GNN encoder. However, since the GNN model is typically initialized at random this may challenge the end-to-end fine-tuning method and get trapped in not desirable local minima. Hence, we warm-start the GNN weights by keeping fixed the transformer weights for a few iterations and optimize only the GNN encoder. This way we can have a good initialization for the GNN model weights before we attempt the joint training. Finally, we finetune end-to-end the semantic encoders and GNN models for the downstream task. Such a scheme will provide a good initial point for the GNN model. The resulting embeddings abide by the text semantics, graph relations and the multi-hop graph structure; see Fig. <ref type="figure" target="#fig_1">1d</ref>. Our overall stage-wise fine-tuning pipeline is depicted in the bottom part of Fig. <ref type="figure">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Addressing efficiency</head><p>The high computation overhead and memory consumption required by the LM-GNN framework limits the wide applicability of the approach. Addressing these issues, we adopt several optimizations to efficiently train LM-GNN. Cache BERT embeddings. To further reduce transformer computations, we cache text embeddings of some nodes in a mini-batch. During the training, whenever we compute new text embeddings, we save them in the cache. Whenever we need node BERT embeddings, we fetch them from the cache. Some cached text embeddings may be out-of-date in a large graph, which may lower the overall model accuracy.</p><p>Joint negative sampling. Link prediction task requires positive and negative samples to be trained as detailed in Sec. 4.3. Hence, we construct k negative edges for each positive edge in a mini-batch. By default, we sample k negative edges for each positive edge independently, which requires us to sample k ? n new nodes with n is the number of positive links. The optimization for joint negative sample samples n nodes and use them to construct k negative edges jointly. Specifically, we reuse these nodes and randomly pair them with nodes in our positive set to generate negative pairs. As a result, we can significantly reduce the number of nodes in a mini-batch and accelerate training. The default method generates 2 ? n + k ? n end-point nodes and their neighbor nodes, while the joint negative sampling generates 3 ? n end-point nodes.</p><p>Distributed GNN training. Finally, to allow scallability to billion node graphs we exploit and extend the distributed GNN training framework <ref type="bibr" target="#b29">[30]</ref> to accomodate our end-to-end fine-tuning setting.</p><p>To increase the training efficiency, we apply hierarchical graph partitioning in DGL's distributed training. When using this method, the target nodes/edges are sampled from the same partition. Therefore, when we sample their neighbors, it's more likely that different target nodes may sample the same neighbor nodes and thus, reduce the number of nodes in a mini-batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental setting</head><p>In the experiments we want to evaluate our techniques for improving the transformer representation with graph information to allow better joint fine-tuning with any GNN model. Hence, although LM-GNN can include any GNN model as an encoder here we will only evaluate the GraphSAGE <ref type="bibr" target="#b12">[13]</ref> for homogenous graphs and the RGCN <ref type="bibr" target="#b20">[21]</ref> for heterogenous graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets.</head><p>Public. Our unique setting requires graph datasets where the nodes are associated with text. We employ the arxiv, and products datesets from the OGB benchmark <ref type="bibr" target="#b13">[14]</ref> with N =169,343 and E =1,166,243 and N =2,449,029 E=61,859,140 respectively with the standard split ratios from <ref type="bibr" target="#b13">[14]</ref>.</p><p>We further augment the data with the original text features for each node; the data are collected in <ref type="bibr" target="#b7">[8]</ref>.</p><p>In the arxiv dataset the original title and abstract is used as text feature for the node. On the other hand the product dataset represents Amazon products and the product title was crawled from the web and used as the text feature. The benchmark in <ref type="bibr" target="#b13">[14]</ref> provides text embeddings of the original text as features for the nodes. The task here is to predict the labels on the nodes in a standard semisupervised setting. The labels are the type the paper and the category of product for arxiv and product respectively. For these datasets we also formulate a link prediction problem with splits 80% training, 10% validation and 10% testing and the tasks are predicting paper citation and product co-purchase links. Further we also construct the Yelp dataset augmented with the text using sources provided from <ref type="bibr" target="#b1">[2]</ref>. The following node types are included with corresponding number of nodes business Private. Additionally, we consider the dataset provided by the recent Amazon KDD22 challenge <ref type="bibr" target="#b0">[1]</ref>. The graph structure is indeed similar to the one depicted in Fig. <ref type="figure" target="#fig_1">1a</ref>. There are N 1 = 646, 640 product and N 2 = 33, 804 query nodes in the graph and E = 781, 744 edges that represent a match among the query and the product. Each edge in this dataset is associated with a label which corresponds to whether a match between the query and the product is an exact, substitute, complement or irrelevant. This problem is known as ESCI and is solved as an edge classification task. We create a custom split for this task by splitting the set of edges to 60% for training 10% for validation and 30% for testing. Finally, we also consider the private query-purchase-product dataset that is used to predict which product will be bought based on a query. Specifically, there are N 1 = 130, 191, 253 products and N 2 = 5, 878, 377 queries. Also there are the following behavioral data represented as edges among queries and products: Based on a query a product was added in the basked E 1 =15,163,234, a product was clicked E 2 =24,896,086, a product was returned as candidate E 3 =140,008,811, was purchased E 4 =13,138,944. The task here is given a query return the most probable product to be purchased. A natural formulation for this task is under the link prediction setting.</p><formula xml:id="formula_6">N 1 =160,585, category N 2 =1330, city N 3 =836, review N 4 =8,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baseline setting</head><p>Next we explain the different parameters that define the various approaches considered in this work.</p><p>Encoders. We consider the following possible semantic encoders in this work. BERT is the pretrained BERT model from <ref type="bibr" target="#b25">[26]</ref>. GRAPH-AWARE BERT is the pre-trained BERT model <ref type="bibr" target="#b25">[26]</ref> that we further fine-tune it for graph structure prediction as in equation <ref type="bibr" target="#b1">(2)</ref>. BERT-PR is a BERT model that is pre-trained using the MLM objective in the proprietary data of the company. Task encoders. We consider 2 candidate encoders for the experiments presented here. MLP is a single-layer MLP that projects the text embeding to an appropriate dimension for node classification or for link prediction and allows us to circumvent to directly compare with the language model. This is used as a baseline approach to directly use the semantic encoder model in the downstream tasks. GRAPHSAGE is the model presented in <ref type="bibr" target="#b11">[12]</ref> and is used as our baseline GNN model. Fine-tune. This parameter defines whether we will back-propagate the loss to the semantic encoder during learning or not. The training is orders of magnitude faster when the loss is not backpropagated.</p><p>Warm-start. This parameter defines whether we will warm-start the GNN model by keeping the semantic encoder parameters fixed for some iterations before end-to-end fine-tuning. Model configuration. We optimize the parameters such that the validation set performance is optimized. We select the number of GNN layers from 1, 2, 3, GNN hidden dimension from 128, 256, 512 and learning rate from 10 -3 , 10 -4 , 10 -5 . By comparing lines 2 and 3 we observe that fine-tuning the BERT directly for the downstream tasks and disregarding the graph structure achieves on-par performance as the one of keeping the BERT model fixed and using these representations as input to the GNN model. This suggests that the initial BERT embeddings are indeed not the most appropriate semantic embeddings. By comparing lines 3 and 4 we see a performance benefit of fine-tuning the BERT model through the GNN, since the multi-hop information is captured by the GNN. By comparing lines 3 and 5 we observe the clear advantage of the graph-aware BERT. The graph-aware pre-fine-tuning fuses the transformer with graph information and is the most suitable semantic encoder. Finally, line 6 coincides with the proposed LM-GNN framework. We observe that this configuration achieves the best overall performance and includes the proposed stage-wise fine-tuning approach. Hence, fine-tuning the BERT model for link prediction provides good performance in the node classification tasks. This result is very important since it allows to train a BERT model on link prediction and transfer the knowledge on different downstream tasks which may speed up the overall training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Link prediction</head><p>Table <ref type="table">2</ref> collects the link prediction performance of the various baselines measured using the MRR score. The first row applies the link prediction supervision in (2) directly on the node representation of the text encoding after mapped by a single layer MLP to an embedding and the whole architecture is fine-tuned end-to-end. This model corresponds to the graph-aware pre-fine-tuned model for link prediction and is the same as the used as a semantic encoder in lines 5 and 6.</p><p>By comparing lines 1 versus 2, 3, and 4 we observe that the original BERT model is indeed not appropriate as a semantic encoder used with the GNN. On the other hand, fine-tuning the BERT model for link prediction in row 1 achieves a very good MRR performance. Lines 5, 6, 7 relative to 2, 3, 4 showcase the advantage of using the graph-aware pre-fine-tuning as an essential step in our LM-GNN framework, where the former leads to a large performance boost. Furthermore, by comparing 5 with 6 and 7 we observe the necessity of warm-starting in certain cases of the GNN encoder to avoid non-desirable local minima. Since the GNN model is initialized at random and the graph-aware BERT is well-trained optimizing this model without warm-start is challenging. By comparing lines 3 and 4 we see a performance benefit of fine-tuning the BERT model through the GNN, since the multi-hop information is captured by the GNN.</p><p>Convergence improvement. The warm-starting option behinds performance gains in Table <ref type="table">2</ref> it provides significant training speed up. Specifically, for the ogbn-products dataset it takes 168 hours for the option without warm-start (row 6) to reach the maximum performance reported, whereas for the option with warm-start (row 7) it takes only 13 hours to reach the same MRR. Thus warm-start provides a 13x speed up in training speed. low for the BERT model to gradually adapt to the graph domain data. We prove with experiments in four public datasets and one query-purchase-product dataset the power of the LM-GNN framework.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) The underlying graph among products and queries where an edge signifies that a query leads to the purchase of a product. (b-d) The 2-D projected embeddings as generated by different pipelines. (b)The transformer maps entities solely on text and fails to capture semantic similarity, besides language based e.g., shoes are close to boots. (c) The graph-aware transformer maps connected entities close, however fails at capturing higher order relations and embeds the two running shoes in different regions. (d)The proposed LM-GNN captures the connectivity, higher order structure, as well as text semantics and provides a refined representations useful for retrieval tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The graph-aware transformer framework relies on the input text to predict whether two entities are connected in the heterogenous graph. (Bottom) The LM-GNN framework employs the graph-aware transformer as a semantic encoder that is further fine-tuned using the GNN encoder, for predicting links in the heterogeneous graph. Different than the graph-aware transformer framework the LM-GNN can access nodes in multi-hop neighborhood.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Nike ZoomX Vaporfly</cell></row><row><cell>Running shoes</cell><cell>Graph-aware transformer</cell><cell cols="2">Structure prediction decoder</cell><cell>?</cell></row><row><cell>Nike ZoomX Vaporfly</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Running shoes</cell></row><row><cell></cell><cell></cell><cell>LM-GNN</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Nike ZoomX Vaporfly</cell></row><row><cell>Running shoes Nike ZoomX Vaporfly</cell><cell>Graph-aware transformer</cell><cell>GNN encoder</cell><cell>Structure prediction decoder</cell><cell>?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Running shoes</cell></row><row><cell>Figure 2: (Top)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>Back-propagate on samples. Instead of back-propagating gradients to the transformer models on all nodes, we sub-sample a fixed-size number of nodes (train nodes) in a mini-batch where we backpropagate gradients to the BERT model. For the rest of the nodes (inference nodes), we just run BERT forward computation to generate BERT embeddings. To further reduce memory consumption and allow training in limited GPU machines, we split the inference nodes into multiple sub-batches of fixed size.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>635,403, user N 5 =2,189,457. The following edges are considered (user, friendship, user) E 1 =17,971,548, (business, in, city) E 2 =160585, (business, in category, category) E 3 =708968, (review, on, business) E 4 =8,635,403, (user, write, review) E 5 =8,635,403. In this dataset only the review nodes are associated with text. The task here is to predict the stars for a business and is formulated as a node prediction task</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Node classification results for the public datasets. Results measured in accuracy.Table1collects the results for the public datasets for different training configurations and encoder models in node classification. Notice that the first two rows apply the node prediction loss directly on the node representation of the text embeddings after it is appropriately mapped by a single layer MLP. Fine-tuning in this context means that the gradient updates the parameters of the semantic encoder otherwise it is not besides the MLP parameters. The target node for the Yelp dataset does not have any text hence, we can not evaluate the first two settings for that. For this experiment the warm-start did not give significant improvement and hence was not included.</figDesc><table><row><cell>Semantic encoder</cell><cell cols="4">Graph encoder Fine-tune arxiv products Yelp</cell></row><row><cell>1 BERT</cell><cell>MLP</cell><cell>No</cell><cell>62.91 61.83</cell><cell>-</cell></row><row><cell>2 BERT</cell><cell>MLP</cell><cell>Yes</cell><cell>72.98 77.64</cell><cell>-</cell></row><row><cell>3 BERT</cell><cell>GNN</cell><cell>No</cell><cell cols="2">71.39 79.10 65.81</cell></row><row><cell>4 BERT</cell><cell>GNN</cell><cell>Yes</cell><cell cols="2">73.42 81.24 73.06</cell></row><row><cell cols="2">5 graph-aware BERT GNN</cell><cell>No</cell><cell cols="2">73.79 80.53 66.88</cell></row><row><cell cols="2">6 graph-aware BERT GNN</cell><cell>Yes</cell><cell cols="2">74.97 82.35 76.47</cell></row><row><cell>6 Experiments</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>6.1 Node classification</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Macro recall at 100 for the query-purchase-product dataset. The converged model for row 2 is the graph-aware BERT in row 4.</figDesc><table><row><cell>Semantic encoder</cell><cell cols="3">Graph encoder Fine-tune Macro@100</cell></row><row><cell>1 BERT-PR</cell><cell>MLP</cell><cell>No</cell><cell>34.12</cell></row><row><cell cols="2">2 graph-aware BERT MLP</cell><cell>Yes</cell><cell>79.06</cell></row><row><cell>3 BERT-PR</cell><cell>GNN</cell><cell>No</cell><cell>77.26</cell></row><row><cell cols="2">4 graph-aware BERT GNN</cell><cell>No</cell><cell>86.53</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Preprint. Under review.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>LM-GNN Models : Adapt and fine-tuneIn this paper, our high-level goal is to investigate how to fuse transformer and GNN models to learn informative representations from graph and text data. Our LM-GNN framework achieves this by stage-wise fine-tuning that gradually fuses the transformer with graph information.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Table <ref type="table">2</ref>: Link prediction results for the public datasets. The performance is measured in MRR scores. Note that the converged model for row 1 is the graph-aware BERT used in row 5, 6, 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic encoder</head><p>Graph encoder Warm-start Fine-tune arxiv products </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Public ESCI: edge classification</head><p>The Table <ref type="table">3</ref> contains the edge classification results for the various baselines in ESCI. Note that here we also report the performance for each individual class since we are interested in predicting well also for the rare classes in our application.</p><p>By comparing rows 2 and 4 that both fine-tune the BERT-PR model we observe a strong boost of 320 bps in performance when the GNN is used. This indicates that the GNN can indeed help boosting the performance probably for the rare classes (S-C-I) by a large extend. By comparing rows 3 and 4 we observe that it is very important to fine-tune the BERT-PR model during GNN training. By comparing rows 3 and 6 we observe that the graph-aware pre-fine-tuning is giving a significant boost when the BERT embeddings are fixed. This benefit diminishes when the BERT model is fine-tuned. Finally, the performance in lines 5-8 is quite similar, but interesting the performance in the rare classes is maximized in rows 5 and 6. We plan to dive deeper into these results and analyze the performance for different sample sizes besides the current split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Query-purchase-product dataset</head><p>Table <ref type="table">4</ref> collects the results for predicting if the search query leads to the purchase of a product and is treated as a link prediction task. The evaluation metric is the Macro recall at 100, which is the percentage of true products that exist in the top 100 retrieved products by each method. The results for fine-tuning the BERT model via the GNN model are ongoing. By comparing rows 1 and 3 that do not fine-tune the BERT-PR model we observe a very large performance boost by the GNN model that considers the graph structure. An even larger performance boost is observed when fine-tuning the BERT model via graph information in row 2. The best performance is observed by using the graph-aware BERT model as a fixed semantic-encoder for the GNN model. Future steps will focus on end-to-end fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper we develop a framework termed LM-GNN that achieves high-quality representations for graph data with rich textual features. Our framework employs stage-wise fine-tuning steps that al-</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://www.aicrowd.com/challenges/esci-challenge-for-improving-product-search" />
		<title level="m">Amazon KDD 2022 challenge</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="https://www.yelp.com/dataset" />
		<title level="m">Yelp dataset</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Distributed large-scale natural graph factorization</title>
		<author>
			<persName><forename type="first">Amr</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shravan</forename><surname>Narayanamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vanja</forename><surname>Josifovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on World Wide Web</title>
		<meeting>the 22nd international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Regularization and semi-supervised learning on large graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matveeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annual Conf. Learning Theory</title>
		<meeting>Annual Conf. Learning Theory<address><addrLine>Banff, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004-07">Jul. 2004</date>
			<biblScope unit="volume">3120</biblScope>
			<biblScope unit="page" from="624" to="638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps and spectral techniques for embedding and clustering</title>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="585" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sig. Process. Mag</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Grarep: Learning graph representations with global structural information</title>
		<author>
			<persName><forename type="first">Shaosheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiongkai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM international on conference on information and knowledge management</title>
		<meeting>the 24th ACM international on conference on information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="891" to="900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Node feature extraction by self-supervised multi-scale neighborhood prediction</title>
		<author>
			<persName><forename type="first">Eli</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olgica</forename><surname>Milenkovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inderjit S</forename><surname>Dhillon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.00064</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Magnn: Metapath aggregated graph neural network for heterogeneous graph embedding</title>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiani</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqiao</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference 2020</title>
		<meeting>The Web Conference 2020</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2331" to="2341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS&apos;17</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems, NIPS&apos;17</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Representation learning on graphs</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05584</idno>
	</analytic>
	<monogr>
		<title level="m">Methods and applications</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22118" to="22133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Few-shot link prediction via graph neural networks for covid-19 drug-repurposing</title>
		<author>
			<persName><forename type="first">Da</forename><surname>Vassilis N Ioannidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2020; Graph Representation Learning and Beyond workshop</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Panrep: Graph neural networks for extracting universal node embeddings in heterogeneous graphs</title>
		<author>
			<persName><forename type="first">Da</forename><surname>Vassilis N Ioannidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD 2020; Workshop on Deep Learning on Graphs: Methods and Applications</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Learn. Represantions</title>
		<meeting>Int. Conf. on Learn. Represantions<address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04">Apr. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adsgnn: Behavior-graph augmented relevance modeling in sponsored search</title>
		<author>
			<persName><forename type="first">Chaozhuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bochen</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanling</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="223" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Asymmetric transitivity preserving graph embedding</title>
		<author>
			<persName><forename type="first">Mingdong</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1105" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Line: Largescale information network embedding</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on world wide web</title>
		<meeting>the 24th international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Learn. Represantions</title>
		<meeting>Int. Conf. on Learn. Represantions</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding: A survey of approaches and applications</title>
		<author>
			<persName><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2724" to="2743" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Heterogeneous graph attention network</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houye</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2022" to="2032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03771</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6575</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Da Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Karypis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08532</idno>
		<title level="m">Dgl-ke: Training knowledge graph embeddings at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Distributed hybrid cpu and gpu training for graph neural networks on billion-scale graphs</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Da Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengru</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominique</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qidong</forename><surname>Lasalle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><surname>Karypis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.15345</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Networkbased drug repurposing for novel coronavirus 2019-ncov/sars-cov-2</title>
		<author>
			<persName><forename type="first">Yadi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feixiong</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell discovery</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Textgnn: Improving text encoder via graph neural network in sponsored search</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanling</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Pelger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruofei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huasha</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2848" to="2857" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
