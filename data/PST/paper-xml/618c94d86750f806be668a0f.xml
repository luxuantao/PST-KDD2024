<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">xMoCo: Cross Momentum Contrastive Learning for Open-Domain Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
							<email>fuwei@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Binxing</forename><surname>Jiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
							<email>djiang@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Linjun</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">xMoCo: Cross Momentum Contrastive Learning for Open-Domain Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Dense passage retrieval has been shown to be an effective approach for information retrieval tasks such as open domain question answering. Under this paradigm, a dual-encoder model is learned to encode questions and passages separately into vector representations, and all the passage vectors are then pre-computed and indexed, which can be efficiently retrieved by vector space search during inference time. In this paper, we propose a new contrastive learning method called cross momentum contrastive learning (xMoCo), for learning a dualencoder model for query-passage matching. Our method efficiently maintains a large pool of negative samples like the original MoCo, and by jointly optimizing question-to-passage and passage-to-question matching, enables using separate encoders for questions and passages. We evaluate our method on various open domain QA datasets, and the experimental results show the effectiveness of the proposed approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Retrieving relevant passages given certain query from a large collection of documents is a crucial component in many information retrieval systems such as web search and open domain question answering (QA). Current QA systems often employ a two-stage pipeline: a retriever is firstly used to find relevant passages, and then a fine-grained reader tries to locate the answer in the retrieved passages. As recent advancement in machine reading comprehension (MRC) has demonstrated excellent results of finding answers given the correct passages <ref type="bibr" target="#b20">(Wang et al., 2017)</ref>, the performance of open-domain QA systems now relies heavily on the relevance of the selected passages of the retriever.</p><p>Traditionally the retrievers usually utilize sparse keywords matching such as TF-IDF or BM25 <ref type="bibr" target="#b18">(Robertson and Zaragoza, 2009)</ref>, which can be efficiently implemented with an inverted index. With the popularization of neural network in NLP, the dense passage retrieval approach has gained traction <ref type="bibr" target="#b12">(Karpukhin et al., 2020)</ref>. In this approach, a dual-encoder model is learned to encode questions and passages into a dense, low-dimensional vector space, where the relevance between questions and passages can be calculated by the inner product of their respective vectors. As the vectors of all passages can be pre-computed and indexed, dense passage retrieval can also be done efficiently with vector space search methods during inference time <ref type="bibr" target="#b19">(Shrivastava and Li, 2014)</ref>.</p><p>Dense retrieval models are usually trained with contrastive objectives between positive and negative question-passage pairs. As the positive pairs are often given by the training data, one challenge in contrastive learning is how to select negative examples to avoid mismatch between training and inference. During inference time, the model needs to find the correct passages from a very large set of pre-computed candidate vectors, but during training, both positive and negative examples need to be encoded from scratch, thus severely limiting the number of negative examples due to computational cost. One promising way to reduce the discrepancy is momentum constrastive learning (MoCo) proposed by <ref type="bibr" target="#b9">He et al. (2020)</ref>. In this method, a pair of fast/slow encoders are used to encode questions and passages, respectively. The slow encoder is updated as a slow moving average of the fast encoder, which reduces the inconsistency of encoded passage vectors between subsequent training steps, enabling the encoded passages to be stored in a large queue and reused in later steps as negative examples. Unfortunately, directly applying MoCo in question-passage matching is problematic. Unlike the image matching tasks in original MoCo paper, the questions and passages are distinct from each other and not interchangeable. Furthermore, the passages are only encoded by the slow encoder, but the slow encoder is only updated with momentum from the fast encoder, not directly affected by the gradients. As the fast encoder only sees the questions, the training becomes insensitive to the passage representations and fails to learn properly. To solve this problem, we propose a new contrastive learning method called Cross Momentum Contrastive Learning (xMoCo). xMoCo employs two sets of fast/slow encoders and jointly optimizes the question-passage and passage-question matching tasks. It can be applied to scenarios where the questions and passages require different encoders, while retaining the advantage of efficiently maintaining a large number of negative examples. We test our method on several open-domain QA tasks, and the experimental results show the effectiveness of the proposed approach.</p><p>To summarize, the main contributions of this work are as follows:</p><p>• We proposes a new momentum contrastive learning method, Cross Momentum Contrast (xMoCo), which can learn question-passage matching where questions and passages require different encoders.</p><p>• We demonstrate the effectiveness of xMoCo in learning a dense passage retrieval model for various open domain question answering datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There are mainly two threads of research work related to this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Passage Retrieval for QA</head><p>Retrieving relevance passages is usually the first step in the most QA pipelines. Traditional passage retriever utilizes the keyword-matching based methods such as TF-IDF and BM25 <ref type="bibr" target="#b4">(Chen et al., 2017)</ref>. Keyword-based approach enjoys its simplicity, but often suffers from term mismatch between questions and passages. Such term mismatch problem can be reduced by either query expansion <ref type="bibr" target="#b3">(Carpineto and Romano, 2012)</ref> or appending generated questions to the passages <ref type="bibr" target="#b16">(Nogueira et al., 2019)</ref>. Dense passage retrieval usually involves learning a dual-encoder to map both questions and passages into dense vectors, where their innerproduct denotes their relevance <ref type="bibr" target="#b14">(Lee et al., 2019)</ref>.</p><p>The challenge in training a dense retriever often lies in how to select negative question-passage pairs. As a small number of randomly generated negative pairs are considered too easy to differentiate, previous work has mainly focused on how to generate "hard" negatives. <ref type="bibr" target="#b12">Karpukhin et al. (2020)</ref>   <ref type="bibr" target="#b5">(Chen et al., 2020)</ref>. Different from the image counterpart, many NLP tasks has readily available positive pairs such question-passage pairs. Here the main benefit of momentum contrastive learning is to efficiently maintain a large set of negative samples, thus making the learning process more consistent with the inference. One example of applying momentum contrastive learning in NLP is <ref type="bibr" target="#b6">Chi et al. (2020)</ref>. In their work, momentum contrastive learning is employed to optimize the InfoNCE lower bound between parallel sentence pairs from different languages. Different from the above works, the questions and passages in our work are not interchangeable and require different encoders, which renders the original MoCo not directly applicable.</p><p>3 Background</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task description</head><p>In this paper, we deal with the task of retrieving relevant passages given certain natural language questions. Given a question q and a collection of N passages {q 1 , q 2 , . . . , q N }, a passage retriever aims to return a list of passages {q i 1 , q i 2 , . . . , q i M } ranked by their relevance to q. While the number of retrieved passages M is usually in the magnitude of hundreds or thousands, the number of total passages N is typically very large, possibly in millions or billions. Such practical concern places constraints in model choices of the passage retrievers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dual-encoder framework for dense passage retrieval</head><p>The de-facto "go-to" choice for dense passage retrieval is the dual-encoder approach. In this framework, a pair of encoders E q and E p , usually implemented as neural networks, are used to map the question q and the passage p into their lowdimensional vectors separately. The relevance or similarity score between q and p is calculated as the inner product of the two vectors:</p><formula xml:id="formula_0">s(q, p) = E q (q) • E p (p)</formula><p>The advantage of this approach is that the vectors of all passages can be pre-computed and stored. During inference, we only need to compute the vector for the question, and the maximum inner product search (MIPS) <ref type="bibr" target="#b19">(Shrivastava and Li, 2014</ref>) can be used to efficiently retrieve most relevant passages from a large collection of candidates. It is possible to train a more accurate matching model if the q and p are fused into one input sequence, or if a more sophisticated similarity model is used instead of the simple inner-product, but those changes would no longer permit efficient retrieval, thus can only be used in a later "re-ranking" stage.</p><p>The training data D for passage retrieval consists of a collection of positive question-passage pairs {(p 1 , q 1 ), (p 2 , q 2 ), . . . , (p n , q n )}, and an additional m passages {p n+1 , . . . , p n+m } without their corresponding questions. The encoders are trained to optimize the negative log-likelihood of all positive pairs:</p><formula xml:id="formula_1">L(D, E q , E p ) = − n i=1 log exp s(q i , p i ) n+m j=1 exp s(q i , p j )</formula><p>As the number of negative pairs (n + m − 1) is very large, it is infeasible to optimize the loss directly. Instead, only a subset of the negative samples will be selected to compute the denominator in the above equation. The selection of the negative samples is critical to the performance of trained model. Previous works such as <ref type="bibr" target="#b21">Xiong et al. (2020)</ref> and Ding et al. ( <ref type="formula">2020</ref>) mainly focus on selecting a few "hard" examples, which hve higher similarity scores with the question and thus contribute more to the sum in the denominator. In this work, we will explore how to use a large set of negative samples to better approximate the sum in the denominator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Momentum contrast for passage retrieval</head><p>We briefly review momentum contrast and explain why directly applying momentum contrast for passage retrieval is problematic.</p><p>Momentum contrast method employs a pair of encoders E q and E p . For each training step, the training pair of q i and p i is encoded as E q (q i ) and E p (p i ) respectively, which is identical to other training method. The key difference is that momentum contrast maintains a queue Q of passage vectors {E p (p i−k )} k encoded in previous training steps. The passage vectors in the queue serve as negative candidates for the current question q i . The process is computationally efficient since the vectors for negative samples are not re-computed, but it also brings the problem of staleness: the vectors in the queue are computed by the previous, not up-to-date models. To reduce the inconsistency, momentum contrast uses momentum update on the encoder E p , making E p a slow moving-average copy of the question encoder E q . The gradient from the loss function is only directly applied to the question encoder E q , not the passage encoder E p . After each training step, the newly encoded E p i is pushed into the queue and the oldest vector is discarded, keeping the queue size constant during training. Such formulation poses no problem for the original MoCo paper <ref type="bibr" target="#b9">(He et al., 2020)</ref>, because their "questions" and "passages" are both images and are interchangeable. Unfortunately, in our passage retrieval problem, the questions and passages are distinct, and it is desirable to use different encoders E q and E p . Even in scenarios where the parameters of the two encoders can be shared, the passages are only encoded by the passage encoder E p , but the gradient from the loss is not applied on the passage encoder. It makes the training process insensitive to the input passages, thus unable to learn reasonable representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">xMoCo: Cross momentum contrast</head><p>To solve the problems mentioned above, we propose a new momentum contrastive learning method, for passages. In addition, two separate queues Q q and Q p store previous encoded vectors for questions and passages, respectively. In one training step, given a positive pair q and p, the question encoders map q into E f ast q (q) and E slow q (q), while the passage encoders map p into E f ast p (p) and E slow p (p). The two vectors encoded by slow encoders are then pushed into their respective queues Q q and Q p . We jointly optimize the question-to-passage and passage-toquestion tasks by pitting q against all vectors in Q q and p against all vectors in Q p :</p><formula xml:id="formula_2">L qp = − log exp (E f ast q (q) • E slow p (p)) p ∈Qp exp E f ast q (q) • E slow p (p ) L pq = − log exp (E f ast p (p) • E slow q (q)) q ∈Qq exp E f ast p (p) • E slow q (q ) L = λL qp + (1 − λ)L pq</formula><p>where λ is a weight parameter and simply set to 0.5 in all experiments in this paper. Like the original MoCo, the gradient update from the loss is only applied to the fast encoders E f ast q and E f ast p , while the slow encoders E slow q and E slow p are updated with momentum from the fast encoders:</p><formula xml:id="formula_3">E slow p ← αE f ast p + (1 − α)E slow p E slow q ← αE f ast q + (1 − α)E slow q</formula><p>where α controls the update speed of the slow encoders and is typically set to a small positive value. When training is finished, both slow encoders are discarded, and only the fast encoders are used in inference. Hence, the number of parameters for xMoCo is comparable to other dual-encoder methods when employing similar-sized encoders.</p><p>In this framework, the two fast encoders E f ast q and E f ast p are not tightly coupled in the gradient update, but instead influence other through the slow encoders. E f ast p updates E slow p through momentum updates, which in turn influences E f ast q by gradient updates from optimizing the loss L qp . E f ast q can also influence E f ast p through similar path. See Fig. <ref type="figure">1</ref> for illustration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Adaption for Batch Training</head><p>Batch training is the standard training protocol for deep learning models due to efficiency and performance reasons. For xMoCo, we also expect our model to be trained in batches. Under the batch training setting, a batch of positive examples are processed together in one training step. The only adaption we need here is to push all vectors computed by slow encoders in one batch into the queues together. It effectively mimics the behavior of the "in-batch negative" strategy employed by previous works such as <ref type="bibr" target="#b12">Karpukhin et al. (2020)</ref>, where the passages in one batch will serve as negatives examples for their questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Encoders</head><p>We use pre-trained uncased BERT-base <ref type="bibr" target="#b7">(Devlin et al., 2019</ref>) models as our encoders following <ref type="bibr" target="#b12">Karpukhin et al. (2020)</ref>. The question and passage encoders utilize two sets of different parameters but are initialized from the same BERT-base model. For both question and passage, we use the vectors of the sequence start tokens in the last layer as their representations. Better pre-trained models such as <ref type="bibr" target="#b15">Liu et al. (2019)</ref> can lead to better retrieval performance, but we choose the uncased BERT-base model for easier comparison with previous work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Incorporating hard negative examples</head><p>Previous work has shown selecting hard examples can be helpful for training passage retrieval models. Our method can easily incorporate hard negative examples by simply adding an additional loss under the multitask framework:</p><formula xml:id="formula_4">L hard = − log exp (E f ast q (q) • E f ast p (p)) p ∈P − {p} exp E f ast q (q) • E f ast p (p )</formula><p>where P is a set of hard negative examples. The loss only involves the two fast encoders, not the slow encoders. We only add hard negatives for the question-to-passage matching tasks, not the passage-to-question matching tasks. In addition, we also encode these negative passages using the slow passage encoder E slow p and enqueue them to serve as negative passages in calculating loss L qp .</p><p>In this work, we only implement a simple method of generating hard examples following <ref type="bibr" target="#b12">Karpukhin et al. (2020)</ref>: for each positive pair, we add one hard negative example by randomly sampling from top retrieval results using a BM25 retriever. More elaborate methods of finding hard examples such as <ref type="bibr" target="#b21">Xiong et al. (2020)</ref> and Ding et al. ( <ref type="formula">2020</ref>) can also be included, but we leave it to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Removing false negative examples</head><p>False negative examples are passages that can match the given question but are falsely labeled as negative examples. In xMoCo formulation, false negatives can arise if a previous encoded passage p in the queue can answer current question q. It can happen if the some questions share the same passage as answer, or if the same question-passage pair is sampled another time when its previous encoded vector is still in the queue because the queue size can be quite large. This is especially important for datasets with small number of positive pairs. To fix the problem, we keep track of the passage ids in the queue and mask out those passages identical to the current passage when calculating the loss.</p><p>Labeling issues can also be the source of false negative examples as pointed out in <ref type="bibr" target="#b8">Ding et al. (2020)</ref>. In their work, an additional model with fused input is trained to reduce the false negatives. We plan to incorporate such model-based approach in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Wikipedia Data as Passage Retrieval Candidates</head><p>As many question answering datasets only provide positive pairs of questions and passages, we need to create a large collection of passages for passage retrieval tasks. Following <ref type="bibr" target="#b14">Lee et al. (2019)</ref>, we extract the passage candidate set from the English Wikipedia dump from Dec. 20, 2018. Following the pre-processing steps in <ref type="bibr" target="#b12">Karpukhin et al. (2020)</ref>, we first extract clean texts using pre-processing code from DrQA <ref type="bibr" target="#b4">(Chen et al., 2017)</ref>, and then split each article into non-overlapping chunks of 100 tokens as the passages for our retrieval task. After pre-processing, we get 20,914,125 passages in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Question Answering Datasets</head><p>We use the five QA datasets from <ref type="bibr" target="#b12">Karpukhin et al. (2020)</ref> and follow their training/dev/test splits.</p><p>Here is a brief description of the datasets. Natural Questions (NQ) <ref type="bibr" target="#b13">(Kwiatkowski et al., 2019)</ref> is a question answer dataset where the questions were real Google search queries and answers were text spans of Wikipedia articles manually selected by annotators.</p><p>TriviaQA <ref type="bibr" target="#b11">(Joshi et al., 2017)</ref> is a set of trivia questions with their answers. We use the unfiltered version of TriviaQA.</p><p>WebQuestions (WQ) <ref type="bibr" target="#b2">(Berant et al., 2013)</ref> is a collection of questions from Google Suggest API with answers from Freebase.</p><p>CuratedTREC (TREC) <ref type="bibr" target="#b0">(Baudiš and Šedivý, 2015)</ref> composes of questions from both TREC QA tracks and Web sources.</p><p>SQuAD v1.1 <ref type="bibr" target="#b17">(Rajpurkar et al., 2016)</ref> is original used as a benchmark for reading comprehension.</p><p>We follow the same procedure in <ref type="bibr" target="#b12">Karpukhin et al. (2020)</ref> to create positive passages for all datasets.</p><p>For TriviaQA, WQ and TREC, we use the highestranked passage from BM25 which contains the answer as positive passage, because these three datasets do not provide answer passages. We discard questions if answer cannot be found at the top 100 BM25 retrieval results. For NQ and SQuAD, we replace the gold passage with the matching passage in our passage candidate set and discard unmatched questions due to differences in processing. Table <ref type="table" target="#tab_2">1</ref> shows the number of questions in the original training/dev/test sets and the number of questions in training sets after discarding unmatched questions. Note that our numbers are slightly different from <ref type="bibr" target="#b12">Karpukhin et al. (2020)</ref> due to small differences in the candidate set or the filtering process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Settings</head><p>Following <ref type="bibr" target="#b12">Karpukhin et al. (2020)</ref>, we test our model on two settings: a "single" setting where each dataset is trained separately, and a "multi" setting where the training data is combined from NQ, TriviaQA, WQ and TREC (excluding SQuAD).</p><p>We compare our model against two baselines. The first baseline is the classic BM25 baseline. The second baseline is the Deep Passage Retrieval (DPR) model from <ref type="bibr" target="#b12">Karpukhin et al. (2020)</ref>. We also implement the setting where the candidates are re-ranked using a linear combination of BM25 and the model similarity score from either DPR or our xMoCo model.</p><p>The evaluation metric for passage retrieval is top-K retrieval accuracy. Here the top-K accuracy means the percentage of questions which have at least one passage containing the answer in the top K retrieved passages. In our experiments, we evaluate the results on both Top-20 and Top-100 retrieval accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Implementation details</head><p>For training, we used batch size of 128 for our models. For the two small datasets TREC and WQ, we trained the model for 100 epochs; for other datasets, we trained the model for 40 epochs. We used the dev set results to select the final checkpoint for testing. The dropout is 0.1 for all encoders. The queue size of negative examples in our model is 16, 384. The momentum co-efficient α in the momentum update is set to 0.001. We used Adam optimizer with a learning rate of 3e − 5, linear scheduling with 5% warm-up. We didn't do hyperparameter search. We follow their specification in <ref type="bibr" target="#b12">Karpukhin et al. (2020)</ref> when re-implementing DPR baselines. Training was done on 16 32GB Nvidia GPUs, and took less than 12 hours to train each model.</p><p>For inference, we use FAISS <ref type="bibr" target="#b10">(Johnson et al., 2017)</ref> for indexing and retrieving passage vectors. For BM25, we use Lucene implementation with b = 0.4 (length normalization) and k 1 = 0.9 (term frequency scaling) following <ref type="bibr" target="#b12">Karpukhin et al. (2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Main Results</head><p>We compare our xMoCo model with both BM25 and DPR baselines over the five QA datasets. As shown in Table <ref type="table" target="#tab_3">2</ref>, our model out-performs both BM25 and DPR baselins in most settings when evaluating on top-20 and top-100 accuracy, except SQuAD where xMoCo does slightly worse than BM25. The lower performance on SQuAD than BM25 is consistent with previous observation in <ref type="bibr" target="#b12">Karpukhin et al. (2020)</ref>. All the baseline numbers are our re-implementations and are comparable but slightly different from the numbers reported in <ref type="bibr" target="#b12">Karpukhin et al. (2020)</ref> due to the difference in the pre-processing and random variations in training. The results empirically demonstrate that using a large number of negative samples in xMoCo indeed leads to a better retrieval model. The improvement of top-20 accuracy is larger than that of top-100 accuracy, since top-100 accuracy is already reasonably high for the DPR baselines. Linearly adding BM25 and model scores does not bring consistent improvement, as xMoCo's performance is significantly better than BM25 except for SQuAD dataset. Furthermore, combining training data only brings improvement on smaller datasets and hurts results on larger datasets due to domain differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Ablation Study</head><p>We perform all ablation experiments on NQ dataset except for the end-to-end QA result evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.1">Size of the queue of negative samples</head><p>One main assumption of xMoCo is that using a larger size of negative samples will lead to a better model for passage retrieval. Here we empirically study the assumption by varying the size of the queues of negative samples. The queue size cannot be reduced to zero because we need at least one negative sample to compute the contrastive loss. Instead, we use the two times the batch size as the minimal queue size, when the strategy essentially reverses to "in-batch negatives" used in previous  works. As shown in Fig. <ref type="figure" target="#fig_0">2</ref>, the model performance increases as the queue size increases initially, but tapers off past 16k. This is different from previous work <ref type="bibr" target="#b6">Chi et al. (2020)</ref>, where they observe performance gains with queue size up to 130k. One possible explanation is that the number of training pairs is relatively small, thus limiting the effectiveness of the larger queue sizes. As for computational efficiency, the size of the queue has little impact on both training speed and memory cost, because both are dominated by the computation of the encoders. tions" and "passages" are drastically different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.3">End-to-end QA results</head><p>For some open domain QA tasks, after the relevant passages are fetched by the retriever, a "reader" is then applied to the retrieval results to extract finegrained answer spans. While improving retrieval accuracy is an important goal, it is interesting to see how the improvement would translate into the end-to-end QA results. Following <ref type="bibr" target="#b12">Karpukhin et al. (2020)</ref>, we implement a simple BERT based reader to predict the answer spans. Give a question Q and N retrieved passages {P 1 , . . . , P N }, the reader first concatenates the question Q to each passage P i and predicts the probability of span (P s i , P e i as the answer as: p(i, s, e|Q, P 1 , . . . , P N ) = p r (i|Q, P 1 , . . . , P N ) × p start (s|Q, P i ) × p end (e|Q, P i ) where p r is the probability of selecting the ith passage, and p start , p end are the probabilities of the sth and eth tokens being the answer start and end position respectively. p start and p end is computed by the standard formula in the original BERT paper <ref type="bibr" target="#b7">(Devlin et al., 2019)</ref>, and the p r is computed by applying softmax over a linear transformation over the vectors of the start tokens of all passages. We follow the training strategy of <ref type="bibr" target="#b12">Karpukhin et al. (2020)</ref>, and sample one positive passages and 23 negative passages from the top-100 retrieval results during training. Please refer to their paper for the details.</p><p>The results are shown in Table <ref type="table" target="#tab_5">4</ref>. While the results from xMoCo are generally better in most cases, the improvements are marginal compared to the results of DPR models. The reason might be that the improvement of xMoCo over DPR on top-100 accuracy is not very large, and it might require better reader to find out the answer spans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>How to select/create negative examples is an essential aspect of passage retrieval model training. xMoCo improves passage retrieval model by efficiently maintaining a large set of negative examples, while previous works mainly focus on finding a few hard examples. It is desirable to design a method to take the best from both worlds. As described in Section 4.5, we can combine the two approaches under a simple multitask framework. But this multitask framework also has its drawbacks. Firstly, it loses the computational efficiency of xMoCo, especially if the method of generating the hard examples is expensive. Secondly, the large set of negative examples in xMoCo and the set of hard examples are two separate sets, while ideally, we want to maintain a large set of hard negative examples. To this end, one possible direction is to employ curriculum learning <ref type="bibr" target="#b1">(Bengio et al., 2009)</ref>. Assuming the corresponding passages for similar questions can serve as hard examples for each other, we can schedule the order of training examples so that similar questions are trained in adjacent steps, resulting more hard examples to be kept in the queue. We plan to explore this possibility in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we propose cross momentum contrastive learning (xMoCo), for passage retrieval task in open domain QA. xMoCo jointly optimizes question-to-passage and passage-to-question matching, enabling using separate encoders for questions and passages, while efficiently maintains a large pool of negative samples like the original MoCo. We verify the effectiveness of the proposed method on various open domain QA datasets. For future work, we plan to investigate how to better integrate hard negative examples into xMoCo.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The effect of queue size of xMoCo. The results are top-20 accuracy on NaturalQuestions dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Illustration of MoCo and xMoCo. Compared with MoCo, xMoCo utilizes two pairs of fast/slow encoders, employs two separate queues for questions and passages, and jointly optimizes both question-to-passage and passage-to-question matching tasks.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>gradient</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>update</cell></row><row><cell>loss(p,q)</cell><cell></cell><cell></cell><cell></cell><cell>loss(q,p)</cell><cell>loss(p,q)</cell></row><row><cell>gradient</cell><cell></cell><cell></cell><cell></cell></row><row><cell>update</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Q Slow</cell><cell>P Slow</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Encoder</cell><cell>Encoder</cell></row><row><cell>Fast</cell><cell cols="2">momentum</cell><cell>Slow</cell></row><row><cell>Encoder</cell><cell></cell><cell>update</cell><cell>Encoder</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Q Fast</cell><cell>P Fast</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Encoder</cell><cell>momentum</cell><cell>Encoder</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>update</cell></row><row><cell>Question q</cell><cell></cell><cell></cell><cell>Passage p</cell><cell>Question q</cell><cell>Passage p</cell></row><row><cell></cell><cell cols="2">(a) MoCo</cell><cell></cell><cell>(b) xMoCo</cell></row><row><cell cols="5">Figure 1: called cross momentum contrast (xMoCo). xMoCo</cell></row><row><cell cols="4">employs two pairs of encoders: E f ast q</cell><cell>and E slow q</cell></row><row><cell cols="2">for questions; E f ast p</cell><cell cols="2">and E slow p</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Number of questions in the datasets. Numbers in the training sets are slightly different from the numbers reported in () due to difference in pre-processing.</figDesc><table><row><cell></cell><cell>Dataset</cell><cell></cell><cell cols="4">Train (Original) Train (Processed)</cell><cell>Dev</cell><cell>Test</cell><cell></cell></row><row><cell></cell><cell cols="2">Natural Questions</cell><cell></cell><cell>79,168</cell><cell></cell><cell cols="3">58,792 8,757 3,610</cell><cell></cell></row><row><cell></cell><cell cols="2">TriviaQA</cell><cell></cell><cell>78,785</cell><cell></cell><cell cols="3">60,404 8,837 11,313</cell><cell></cell></row><row><cell></cell><cell cols="2">WebQuestions</cell><cell></cell><cell>3,417</cell><cell></cell><cell>2,470</cell><cell cols="2">361 2,032</cell><cell></cell></row><row><cell></cell><cell cols="2">CuratedTREC</cell><cell></cell><cell>1,353</cell><cell></cell><cell>1,126</cell><cell>133</cell><cell>694</cell><cell></cell></row><row><cell></cell><cell>SQuAD</cell><cell></cell><cell></cell><cell>78,713</cell><cell></cell><cell cols="3">70,083 8,886 10,570</cell><cell></cell></row><row><cell cols="2">Training Retriever</cell><cell cols="8">Top-20 NQ TriviaQA WQ TREC SQuAD NQ TriviaQA WQ TREC SQuAD Top-100</cell></row><row><cell>None</cell><cell>BM25</cell><cell>59.0</cell><cell>66.9</cell><cell>54.2 70.9</cell><cell>68.9</cell><cell>73.9</cell><cell>76.6</cell><cell>71.1 84.5</cell><cell>80.3</cell></row><row><cell></cell><cell>DPR</cell><cell>78.6</cell><cell>79.0</cell><cell>72.2 80.1</cell><cell>64.3</cell><cell>85.3</cell><cell>85.1</cell><cell>81.2 88.9</cell><cell>77.1</cell></row><row><cell>Single</cell><cell>xMoCo DPR+BM25</cell><cell>82.3 76.0</cell><cell>80.2 79.7</cell><cell>76.5 80.7 72.3 85.2</cell><cell>65.1 72.3</cell><cell>86.0 83.7</cell><cell>85.9 84.3</cell><cell>83.1 89.4 80.1 92.4</cell><cell>77.5 81.5</cell></row><row><cell></cell><cell cols="2">xMoCo+BM25 79.2</cell><cell>80.1</cell><cell>76.6 85.8</cell><cell>73.0</cell><cell>85.2</cell><cell>85.2</cell><cell>83.0 93.1</cell><cell>81.2</cell></row><row><cell></cell><cell>DPR</cell><cell>79.4</cell><cell>78.5</cell><cell>74.8 89.2</cell><cell>52.8</cell><cell>85.7</cell><cell>84.8</cell><cell>82.9 93.7</cell><cell>68.1</cell></row><row><cell>Multi</cell><cell>xMoCo DPR+BM25</cell><cell>82.5 78.3</cell><cell>80.1 79.6</cell><cell>78.2 89.4 74.9 88.7</cell><cell>55.9 67.2</cell><cell>86.3 84.0</cell><cell>85.7 83.5</cell><cell>84.8 94.1 82.1 92.1</cell><cell>70.1 78.7</cell></row><row><cell></cell><cell cols="2">xMoCo+BM25 80.3</cell><cell>80.0</cell><cell>76.1 88.3</cell><cell>68.3</cell><cell>85.2</cell><cell>84.0</cell><cell>82.5 93.2</cell><cell>79.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Evaluation results on the five open domain test sets. Evaluation metric is Top-K accuracy which means the percentage of any passage in the top K retrieval results contain the answer. "Single" denotes the experiments where the training is performed on its own training data for each dataset, while "Multi" denotes the experiments where the training is performed on the combined training sets from NQ, TriviaQA, WQ and TREC. All DPR results are from our re-implementation, which are slightly different, but comparable to the numbers reported in the original paper.</figDesc><table><row><cell>83</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>82</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>81</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>79</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>78</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>77</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>76</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>74</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>73</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>256</cell><cell>512</cell><cell>1k</cell><cell>2k</cell><cell>4k</cell><cell>8k</cell><cell>16k</cell><cell>32k</cell><cell>64k</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation of tied encoders on NaturalQuestions dataset. Tying the parameters in the question and passage encoders decreases the performance of xMoCo.</figDesc><table><row><cell>Setting</cell><cell cols="2">Top-20 Top-100</cell></row><row><cell>xMoCo</cell><cell>82.3</cell><cell>86.0</cell></row><row><cell>+tied encoders</cell><cell>75.4</cell><cell>81.2</cell></row><row><cell cols="3">5.6.2 Effect of using two set of encoders</cell></row><row><cell cols="3">xMoCo formulation expands on the original mo-</cell></row><row><cell cols="3">mentum contrastive learning framework MoCo by</cell></row><row><cell cols="3">enabling two different set of encoders for questions</cell></row><row><cell cols="3">and passages respectively. For open-domain QA,</cell></row><row><cell cols="3">it is unclear whether it is beneficial to use two dif-</cell></row><row><cell cols="3">ferent encoders for questions and passages because</cell></row><row><cell cols="3">both questions and passages are texts. To empir-</cell></row><row><cell cols="3">ically answer this question, we perform another</cell></row><row><cell cols="3">ablation experiment where the parameters in the</cell></row><row><cell cols="3">question and passage encoders are tied. As can</cell></row><row><cell cols="3">be seen in Table 3, the model with tied encoders</cell></row><row><cell cols="3">gives reasonable results, but still under-performs</cell></row><row><cell cols="3">the model with two different encoders. Further-</cell></row><row><cell cols="3">more, the flexibility of xMoCo is necessary for</cell></row><row><cell cols="3">tasks such as text-to-image matching where "ques-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>End-to-end QA results.    </figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Modeling of the question answering task in the yodaqa system</title>
		<author>
			<persName><forename type="first">Petr</forename><surname>Baudiš</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Šedivý</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24027-5_20</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Experimental IR Meets Multilinguality, Multimodality</title>
				<meeting>the 6th International Conference on Experimental IR Meets Multilinguality, Multimodality<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9283</biblScope>
			<biblScope unit="page" from="222" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jérôme</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="DOI">10.1145/1553374.1553380</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning, ICML &apos;09</title>
				<meeting>the 26th Annual International Conference on Machine Learning, ICML &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic parsing on Freebase from question-answer pairs</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1533" to="1544" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A survey of automatic query expansion in information retrieval</title>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Carpineto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Romano</surname></persName>
		</author>
		<idno type="DOI">10.1145/2071389.2071390</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reading Wikipedia to answer opendomain questions</title>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1171</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1870" to="1879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Improved baselines with momentum contrastive learning</title>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Infoxlm: An information-theoretic framework for cross-lingual language model pre-training</title>
		<author>
			<persName><forename type="first">Zewen</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saksham</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian-Ling</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heyan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Rocketqa: An optimized training approach to dense passage retrieval for opendomain question answering</title>
		<author>
			<persName><forename type="first">Yingqi</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiyang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.00975</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9726" to="9735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Billion-scale similarity search with gpus</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dense passage retrieval for open-domain question answering</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.550</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6769" to="6781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Natural questions: A benchmark for question answering research</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00276</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Transactions of the Association for Computational Linguistics</publisher>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="453" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Latent retrieval for weakly supervised open domain question answering</title>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1612</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6086" to="6096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Roberta: A robustly optimized bert pretraining approach</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Document expansion by query prediction</title>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno>CoRR, abs/1904.08375</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1264</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The probabilistic relevance framework: Bm25 and beyond</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
		<idno type="DOI">10.1561/1500000019</idno>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="333" to="389" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Asymmetric lsh (alsh) for sublinear time maximum inner product search (mips)</title>
		<author>
			<persName><forename type="first">Anshumali</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2321" to="2329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gated self-matching networks for reading comprehension and question answering</title>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1018</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="189" to="198" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Approximate nearest neighbor negative contrastive learning for dense text retrieval</title>
		<author>
			<persName><forename type="first">Lee</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwok-Fung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junaid</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnold</forename><surname>Overwijk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
