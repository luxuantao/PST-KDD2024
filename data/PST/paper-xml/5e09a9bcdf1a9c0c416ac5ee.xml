<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving the Instruction Fetch Throughput with Dynamically Configuring the Fetch Pipeline</title>
				<funder ref="#_4hrMx4U">
					<orgName type="full">JSPS KAKENHI</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Reoma</forename><surname>Matsuo</surname></persName>
							<email>matsuo@ando.nuee.nagoya-u.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Nagoya University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shioya</forename><surname>Ryota</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nagoya University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hideki</forename><surname>Ando</surname></persName>
							<email>ando@nuee.nagoya-u.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Nagoya University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">?</forename><forename type="middle">R</forename><surname>Shioya</surname></persName>
							<email>shioya@ci.i.u-tokyo.ac.jp</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Tokyo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improving the Instruction Fetch Throughput with Dynamically Configuring the Fetch Pipeline</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/LCA.2019.2952592</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Instruction fetch, Pipeline implementation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Instruction cache misses are the critical performance bottleneck in the execution of recent workloads such as Web applications written in JavaScript and server applications. Although various instruction prefetchers have been proposed to reduce the misses, the requirements for both high miss coverage and small hardware cost are not satisfied. In this study, we propose a novel method that improves the instruction fetch throughput not by instruction prefetching but by dynamically configuring the fetch pipeline structure. Our scheme switches between the normal pipeline and newly introduced miss-assuming pipeline, which does not degrade the fetch throughput even when L1 instruction cache misses occur. Our method achieves high instruction fetch throughput with simple hardware and small cost unlike previously proposed prefetchers. Our evaluation results using Web and database workloads show that our method improves the performance by 16.6% and 8.6% on average, compared to that with noprefetching and the state-of-the-art instruction prefetcher, PIF, respectively, and achieves as much as 79.0% of the performance of the processor with a perfect instruction cache.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recent workloads such as Web applications written in JavaScript and server applications are known to have a huge instruction working set and thus cause many instruction cache (I-cache) misses <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. In these workloads, the instruction fetch (I-fetch) throughput is significantly decreased by the misses, and the performance of the processor is thereby severely degraded. A common approach to solve this problem is to improve instruction prefetchers. Although many prefetchers have been proposed, most of the state-of-the-art prefetcher leverage the characteristic that the streams of I-cache accesses are reproducible <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. However, while they achieve high miss coverage, they have a serious disadvantage that the storage cost for correlation tables is very large. Although prefetchers that need only a small amount of hardware are proposed, which commonly utilize an existing processor structure such as Branch Target Buffer (BTB) or Return Address Stack (RAS), they cannot capture complex cache miss patterns <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>.</p><p>This paper proposes a novel method to improve I-fetch throughput not by prefetching, but by dynamically configuring the fetch pipeline structure. This method consists of the following two parts: 1) Adding a fetch pipeline assuming misses and 2) dynamically configuring the fetch pipeline as the conventional pipeline or newly added pipeline. We explain these points in the following two paragraphs.</p><p>1) The conventional fetch pipeline is configured assuming that the I-cache is hit, and handles an I-cache miss as a special event by stalling the pipeline. In contrast, our newly added fetch pipeline is assumed that the I-cache is missed; we call this pipeline the miss-assuming pipeline (MAP). The MAP allows the fetch pipeline not to stall for both cases of I-cache hits and misses. Thus, the I-fetch throughput is not decreased, independently of hits or misses. 2) The downside of the MAP is that the fetch pipeline becomes long. This increases the branch misprediction penalty. To handle this problem while maximizing the benefit of the MAP, we propose dynamically configuring the fetch pipeline as the MAP or conventional fetch pipeline (hit-assuming pipeline (HAP)). We present an optimal algorithm for switching between the MAP and HAP.</p><p>Unlike previous methods, our method can be implemented with simple and small hardware, which is approximately only 0.39KB. Hence, our method can be applied to the wide range from server processors to embedded processors. Our evaluation results show that our method outperforms the state-of-the-art prefetcher, the PIF <ref type="bibr" target="#b3">[4]</ref> using more than 200KB, by 8.6% in terms of performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Since I-cache misses give a large negative impact on the performance for server workloads, various instruction prefetchers have been proposed. Ferdman et al. proposed a prefetcher called Proactive Instruction Fetch (PIF), which leverage the characteristic that streams of I-cache accesses are reproducible <ref type="bibr" target="#b3">[4]</ref>. Although the PIF achieves particularly high performance, it requires a large storage for the correlation table (200KB), which is much larger than a usual L1 I-cache (32KB-64KB).</p><p>More recent studies have attempted to reduce the hardware cost of an instruction prefetcher without degrading miss coverage. Kaynak et al. proposed SHIFT, which virtualizes the history into the LLC and shares it across cores executing the same workload <ref type="bibr" target="#b7">[8]</ref>. Although it does not need an additional storage for the correlation table, it consumes the LLC instead. Kolli et al. proposed return address stack (RAS) directed instruction prefetching (RDIP), which achieves performance close to that of PIF with smaller storage cost (63KB) by using the contents of RAS <ref type="bibr" target="#b4">[5]</ref>. <ref type="bibr">Reinman et al.</ref> proposed Fetch Directed Instruction Prefetching, which prefetches the predicted stream by decoupled branch predictor using idle cache ports. Although this base prefetcher has less hardware cost and has been introduced in recently proposed prefetchers (e.g. Boomerang <ref type="bibr" target="#b5">[6]</ref> and  Shotgun <ref type="bibr" target="#b6">[7]</ref>), it needs complicated prefetching mechanisms (i.e. filtering of prefetch requests, stream buffers).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DYNAMIC CONFIGURATION OF THE I-FETCH PIPELINE</head><p>In this section, we explain the behavior of the MAP in Sec. 3.1 and dynamic configuration of the fetch pipeline in Sec. 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Miss-Assuming Pipeline (MAP)</head><p>The MAP is the I-fetch pipeline that assumes that the (L1) I-cache is missed, as opposed to the HAP. Fig. <ref type="figure">1</ref> shows the structure of the MAP. The MAP has two I-fetch paths. One is the L2 cache access path, which is used when the I-cache is missed, and the other is the NOP-stage path, which is used when the I-cache is hit. The MUX selects instructions from the two paths according to the hit or miss. The NOP-stage path simply makes the I-fetch delayed by the L2 cache latency. This explicit delay using the NOP-stage path prevents the I-fetch throughput from reducing independently of I-cache hits or misses.</p><p>We clarify that the MAP does not reduce the I-fetch throughput using Fig. <ref type="figure" target="#fig_1">2</ref>, which compares the pipeline timing of the MAP with the HAP. In these figures, L1, L2, ID, EX, WB, and NOP represent an L1 I-cache access, L2 cache access, decode, execute, writeback, and NOP stage, respectively. The mark "Miss!" represents the occurrence of an I-cache miss. The latency of the L2 cache access is assumed two cycles.</p><p>As shown in Fig. <ref type="figure" target="#fig_1">2</ref>(a), the I-fetch stalls when an I-cache miss occurs in the HAP. In contrast, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>(b), the I-fetch does not stall in the MAP even if I-cache misses occur owing to starting the I-cache access (I2) immediately after the previous I-cache miss (I1). In these figures, the L2 cache is always hit, however, if it is missed, the MAP must be stalled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dynamic Switching between the Two Fetch Pipelines</head><p>Although I-fetch throughput is not decreased in the MAP, the I-fetch pipeline becomes long. This increases the branch misprediction penalty. To handle this problem while maximizing the benefit of the MAP, we propose dynamic configuration of the fetch pipeline between the MAP and HAP. We first explain the performance loss that arises from the structure of the I-fetch pipeline in Sec. 3.2.1. We then propose an optimal switching algorithm, which maximizes the performance of a processor with the proposed I-fetch pipeline, in Sec. 3.2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Performance Loss Analysis</head><p>Before proposing our optimal switching algorithm of the Ifetch pipeline, we discuss the performance loss in our I-fetch pipeline. Because only I-cache misses and branch mispredictions are the causes of performance loss associated with our I-fetch pipeline, we consider the following three cases.</p><p>(1) When an I-cache miss occurs in the HAP: As described in Sec. 3.1, the I-fetch stalls and the execution of instructions is delayed every I-cache miss in the HAP. If the latency of the L2 cache is C cycles, the execution cycle is increased by +C cycles.</p><p>Note that if the I-fetch pipeline is switched to the MAP simultaneously at the occurrence of the I-cache miss, the increase in the execution cycle is +C instead of +2C, because the I-cache miss is handled during the time between the dotted ? and ? of I2 in Fig. <ref type="figure" target="#fig_2">3</ref>.</p><p>(2) When a branch misprediction occurs in the MAP: Since the I-fetch pipeline of the MAP is long, the branch misprediction penalty becomes large, compared with that in the HAP. This increases the execution cycle by +C cycles for each branch misprediction. However, this performance loss can be avoided by switching to the HAP immediately after the branch misprediction because the I-fetch pipeline becomes shorter when restarting I-fetch. Note that instructions in the front-end need not be executed and are all squashed when a branch misprediction occurs.</p><p>(3) When switching from the HAP to the MAP: If the Ifetch pipeline is switched from the HAP to the MAP, the I-fetch pipeline becomes long. This results in the delay of the execution of the fetched instruction immediately after the switching and thus causes performance loss. We explain this using Fig. <ref type="figure" target="#fig_2">3</ref>. This figure is a simplified pipeline chart, where ?, ?, and ? represent the start of I-fetch, end of I-fetch, and completion of writeback, respectively. The pipeline chart with a blue or red background represent that the pipeline timing of the HAP or MAP, respectively. In this figure, after fetching I0 and I1, the I-fetch pipeline is switched from the HAP to the MAP and then the fetch of I2 is started. At this time, the completion of the fetch of I2 is delayed when compared with not-switched case, because the execution of the instruction immediately after switching is delayed due to increase of the pipeline stages. In this way, performance loss occurs when the processor switches from the HAP to the MAP. This results in an increase in the execution cycles by +C cycles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Optimal Switching Algorithm</head><p>In this section, we explain our switching algorithm of the I-fetch pipeline. Fig. <ref type="figure" target="#fig_3">4</ref> is the state machine diagram of our algorithm. In this algorithm, the initial state is the HAP and an I-cache miss or branch misprediction triggers transitions. Specifically, (a) the state is switched to the MAP when an I-cache miss occurs in the HAP, and (b) the state is switched to the HAP when a branch misprediction occurs in the MAP. (c) When a branch misprediction occurs in the HAP or (d) when an I-cache miss occurs in the MAP, the algorithm stays at the current state.</p><p>Our algorithm is optimal and can maximize the benefits of our I-fetch pipeline. In the following, we explain the reason why our algorithm is optimal.</p><p>First, we consider whether the state should be switched when a branch misprediction occurs in the HAP or an I-cache miss occurs in the MAP. In either case, there is no performance loss if the state is not switched; if switched, the performance is lost as described in ( <ref type="formula">1</ref>) and (2) of Sec. 3.2.1. Therefore, in these two cases, the switching is not to be triggered (the transitions of (c) and (d) in Fig. <ref type="figure" target="#fig_3">4</ref>).</p><p>Next, we consider whether the state should be switched when a branch misprediction occurs in the MAP. There are two strategies: (i) switching to the HAP and (ii) keeping staying at the MAP. Fig. <ref type="figure" target="#fig_4">5</ref> shows the amount of performance loss for each strategy according to possible state transition sequences. In the figure, the arrows indicate the occurrence of a branch misprediction or an I-cache miss. The events (I-cache miss (I$) or branch misprediction (Br)) and the amount of the associated performance loss are shown above and below the arrows, respectively. In this figure, branches are continuously mispredicted n (? 0) times after the first branch misprediction, and an I-cache miss occurs at the end. In the case of (i), performance loss due to the first branch misprediction is avoided by switching to the HAP, as described in (2) of Sec. 3.2.1. At the subsequent branch mispredictions, performance loss does not occur because of the short pipeline of the HAP. However, the last I-cache miss degrades performance by +C cycles. Therefore, the total amount of performance loss in this sequence using this strategy is +C cycles. In contrast, in the case of (ii), although the last I-cache miss does not degrade the performance, performance loss is caused at every branch misprediction. Consequently, the total amount of the performance loss using this strategy is +(n + 1)C cycles. Therefore, (i) causes less or equal performance loss than or to (ii), and it is thus better to switch to the HAP when a branch misprediction occurs in the MAP (transition of (b) in Fig. <ref type="figure" target="#fig_3">4</ref>).</p><p>Lastly, we consider whether the state should be switched when an I-cache miss occurs in the HAP. There are two strategies: (i) switching to the MAP or (ii) keeping staying at the Fig. <ref type="figure">6</ref>. Performance loss in the two strategies: (i) switching to the MAP and (ii) keeping staying at the HAP when an I-cache miss occurs in the HAP.</p><p>HAP. Fig. <ref type="figure">6</ref> shows performance loss associated with event occurrences. In this figure, I-cache misses continue n (? 0) times after the first I-cache miss, and a branch misprediction occurs at the end.</p><p>In the case of (i), although the first I-cache miss causes performance loss, the subsequent I-cache misses do not, because the I-fetch pipeline is configured as the MAP. Since the performance loss due to the last branch misprediction is avoided by switching to the HAP, the total amount of the performance loss in this sequence using this strategy is +C cycles. In contrast, in the case of (ii), although the last branch misprediction does not cause performance loss, performance loss is caused at every Icache miss. Therefore, the total amount of the performance loss using this strategy is +(n + 1)C cycles. Consequently, (i) causes less or equal performance loss than or to (ii), and it is thus better to switch to the MAP when an I-cache miss occurs in the HAP (transition of (a) in Fig. <ref type="figure" target="#fig_3">4</ref>).</p><p>As described above, the state transitions shown in Fig. <ref type="figure" target="#fig_3">4</ref> has been proven optimal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION</head><p>We built a simulator based on the gem5 <ref type="bibr" target="#b9">[10]</ref> to evaluate performance using a full-system simulation. Table <ref type="table" target="#tab_1">1</ref> summarizes the configuration of the base processor, which is based on Intel Skylake. We used five server workloads and two SPEC2006 benchmark programs as large and small instruction working  set workloads, respectively. Table <ref type="table">2</ref> lists the benchmark prowith the number of skip and simulated instructions. From SPEC2006 benchmark programs, we chose bzip2 and gobmk with the most and least I-cache misses in the SPEC2006, respectively. We evaluated the following six models.</p><p>1) HAP: no-prefetching (baseline) 2) FDIP: Fetch Directed Instruction Prefetching <ref type="bibr" target="#b10">[11]</ref> 3) PIF: Proactive Instruction Fetch <ref type="bibr" target="#b3">[4]</ref> 4) MAP: miss-assuming pipeline 5) DYN?dynamically configuring between HAP and MAP 6) PFCT: perfect I-cache (1) Opportunites: As described in Sec. 3.1, in an event sequence of I-cache misses and branch mispredictions, if I-cache misses occur consecutively, performance degradation caused by the latter I-cache miss can be avoided. We say that the I-cache miss is converted to be harmless; we call such the I-cache miss can-be-harmless in short. In contrast, if an I-cache miss occurs after a branch misprediction, it degrades the performance. We say that such the I-cache miss penalty is unavoidable. The amount of can-be-harmless misses in the entire misses delivers opportunities for performance improvement in DYN.</p><p>Fig. <ref type="figure">7</ref> shows the breakdown of the I-cache misses (MPKI: misses per kilo-instructions) into can-be-harmless and unavoidable for each program when we use the HAP. As shown in the figure, most of the I-cache misses are classified as can-beharmless, indicating that the DYN can improve the performance significantly.</p><p>(2) Speedup: Fig. <ref type="figure">8</ref> shows the speedup of each model over HAP. As shown in the figure, DYN outperforms the stateof-the-art prefetcher PIF in all workloads except for bzip2. The average speedup of the DYN with the large instruction working set is 16.6% and 8.6%, compared with HAP and PIF, respectively. In addition, the DYN achieved as much as 79.0% of the performance of PFCT. In particular, at MySQL in TPC-H, DYN achieved 93.6% performance of that of the PFCT.</p><p>The FDIP shows almost the same performance as the DYN. This is because the behavior of FDIP is almost the same as that of our method although the structure of FDIP is different from that of our method. There are two advantages of MAP over FDIP: complexity and power. In terms of complexity, the MAP has a significant advantage because it can achieve the same performance with less aggressive hardware compared to FDIP, which has a complex prefetch engine. And in terms of power, the MAP can reduce power consumption of L1 I-cache since our method only needs to fetch once per dynamic instruction while FDIP may perform prefetch in addition to demand access (the evaluation is a future work). However, the MAP cannot cover L2 miss while FDIP can tolerate L2 miss latency. This can be a problem for workloads with much larger instruction working sets (the L2I miss rate of our workloads is within 0.3%).</p><p>(3) Sensitivity to the I-cache size: Fig. <ref type="figure" target="#fig_5">9</ref> shows the average performance (IPC) of the HAP, PIF and DYN in server workloads with varying the I-cache size. While the performance of the HAP is very sensitive to the I-cache size, the DYN exhibits robustness to the size with keeping high performance. The performance of PIF is just in the middle of HAP and DYN. This result indicates we can make the I-cache smaller (less than 8KB) without little performance degradation when using the DYN, thus reducing the cost of a processor.</p><p>(4) Hardware Cost: The required cost for the DYN is estimated to only 0.39 KB. In contrast, the cost required for the PIF is huge (204KB) because of a large correlation table. In general, the instruction prefetchers that achieve particularly high performance commonly need a large correlation table (e.g. 63KB for RDIP <ref type="bibr" target="#b4">[5]</ref>). Therefore, our method requires a much less cost with achieving higher performance, compared with conventional prefetchers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this study, we proposed a novel method that configures the fetch pipeline dynamically for improving instruction fetch throughput. Our evaluation results using Web and database workloads with a large instruction working set show that our proposed method improves performance by 26.0% and 8.6% on average, compared with no-prefetching and the state-of-the-art prefetcher (PIF), respectively, with only 0.39KB cost.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Manuscript submitted: 11 -</head><label>11</label><figDesc>Jul-2019. Manuscript accepted: 04-Nov-2019. Final manuscript received: 06-Nov-2019</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Pipeline charts of the HAP and MAP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Penalty caused by switching to the MAP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Our pipeline switching algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Performance loss in the two strategies: (i) switching to the HAP and (ii) keeping staying at the MAP when a branch misprediction occurs in the MAP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Relative IPC for varying L1 I-cache size (baseline: HAP-32KB).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 Base</head><label>1</label><figDesc>Processor Configration.</figDesc><table><row><cell>Pipeline</cell><cell cols="2">8 instructions wide for each of</cell><cell></cell></row><row><cell></cell><cell cols="2">fetch, decode, issue, and commit</cell><cell></cell></row><row><cell>ROB</cell><cell>224 entries</cell><cell></cell><cell></cell></row><row><cell>Issue queue</cell><cell>97 entries</cell><cell></cell><cell></cell></row><row><cell cols="2">Branch prediction L-TAGE [9]</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">12-cycle misprediction penalty</cell><cell></cell></row><row><cell></cell><cell>4K-set 4-way BTB</cell><cell></cell><cell></cell></row><row><cell>Function unit</cell><cell cols="2">6 iALU, 2 iMULT/DIV, 2 Ld/St,</cell><cell></cell></row><row><cell></cell><cell cols="2">4 fpALU, 2 fpMULT/DIV/SQRT</cell><cell></cell></row><row><cell>L1 I-cache</cell><cell cols="3">32KB, 8-way, 64B line, 2-cycle hit latency, 8 MSHRs</cell></row><row><cell>L1 D-cache</cell><cell cols="3">32KB, 8-way, 64B line, 2-cycle hit latency</cell></row><row><cell>L2 cache</cell><cell cols="3">256KB, 2-way, 64B line, 12-cycle hit latency</cell></row><row><cell>L3 cache</cell><cell cols="3">8MB, 16-way, 64B line, 42-cycle hit latency</cell></row><row><cell>Main memory</cell><cell>300-cycle latency</cell><cell></cell><cell></cell></row><row><cell>ISA</cell><cell>ARMv8</cell><cell></cell><cell></cell></row><row><cell></cell><cell>TABLE 2</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Workloads.</cell><cell></cell><cell></cell></row><row><cell cols="2">W set size program</cell><cell cols="2">skip inst sim inst</cell></row><row><cell>small</cell><cell>SPEC2006 / bzip2</cell><cell>16B</cell><cell>100M</cell></row><row><cell>small</cell><cell>SPEC2006 / gobmk</cell><cell>16B</cell><cell>100M</cell></row><row><cell>large</cell><cell>TPC-H / MySQL</cell><cell>2B</cell><cell>100M</cell></row><row><cell>large</cell><cell>TPC-H / PostgreSQL</cell><cell>2B</cell><cell>100M</cell></row><row><cell>large</cell><cell>SPECweb99 / Apache</cell><cell>10B</cell><cell>100M</cell></row><row><cell>large</cell><cell>SPECweb99 / nginx</cell><cell>10B</cell><cell>100M</cell></row><row><cell>large</cell><cell>TPC-C / MySQL</cell><cell>10B</cell><cell>100M</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work was supported by <rs type="funder">JSPS KAKENHI</rs> Grant Number <rs type="grantNumber">JP16H05855</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_4hrMx4U">
					<idno type="grant-number">JP16H05855</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Microarchitectural implications of event-driven server-side web applications</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="762" to="774" />
		</imprint>
	</monogr>
	<note>in MICRO-48</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Performance of database workloads on shared-memory systems with out-of-order processors</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS-8</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="307" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Temporal instruction fetch streaming</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
	<note>in MICRO-41</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Proactive instruction fetch</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="152" to="162" />
		</imprint>
	</monogr>
	<note>in MICRO-44</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">RDIP: Return-address-stack directed instruction prefetching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kolli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="260" to="271" />
		</imprint>
	</monogr>
	<note>in MICRO-46</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Boomerang: A metadata-free architecture for control flow delivery</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA-23</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="493" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Blasting through the front-end bottleneck with shotgun</title>
	</analytic>
	<monogr>
		<title level="m">ASPLOS-23</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="30" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SHIFT: Shared history instruction fetch for leancore server processors</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kaynak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICRO-46</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="272" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A 256 Kbits L-TAGE branch predictor</title>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CBP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The gem5 simulator</title>
		<author>
			<persName><forename type="first">N</forename><surname>Binkert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fetch directed instruction prefetching</title>
		<author>
			<persName><forename type="first">G</forename><surname>Reinman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 32nd IEEE/ACM Int. Symp. Microarchitecture</title>
		<meeting>32nd IEEE/ACM Int. Symp. Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="16" to="27" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
