<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improved Multiscale Vision Transformers for Classification and Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-12-02">2 Dec 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yanghao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">equal technical contribution</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">equal technical contribution</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bo</forename><surname>Xiong</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">equal technical contribution</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improved Multiscale Vision Transformers for Classification and Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-12-02">2 Dec 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2112.01526v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>3. Revisiting Multiscale Vision Transformers</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we study Multiscale Vision Transformers (MViT) as a unified architecture for image and video classification, as well as object detection. We present an improved version of MViT that incorporates decomposed relative positional embeddings and residual pooling connections. We instantiate this architecture in five sizes and evaluate it for ImageNet classification, COCO detection and Kinetics video recognition where it outperforms prior work. We further compare MViTs' pooling attention to window attention mechanisms where it outperforms the latter in accuracy/compute. Without bells-and-whistles, MViT has state-of-the-art performance in 3 domains: 88.8% accuracy on ImageNet classification, 56.1 AP box on COCO object detection as well as 86.1% on Kinetics-400 video classification. Code and models will be made publicly available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Designing architectures for different visual recognition tasks has been historically difficult and the most widely adopted ones have been the ones that combine simplicity and efficacy, e.g. VGGNet <ref type="bibr" target="#b66">[67]</ref> and ResNet <ref type="bibr" target="#b36">[37]</ref>. More recently Vision Transformers (ViT) <ref type="bibr" target="#b17">[18]</ref> have shown promising performance and are rivaling convolutional neural networks (CNN) and a wide range of modifications have recently been proposed to apply them to different vision tasks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr">22,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b90">91]</ref>.</p><p>While ViT <ref type="bibr" target="#b17">[18]</ref> is popular in image classification, its usage for high-resolution object detection and space-time video understanding tasks remains challenging. The density of visual signals poses severe challenges in compute and memory requirements as these scale quadratically in complexity within the self-attention blocks of Transformerbased <ref type="bibr" target="#b76">[77]</ref> models. The community has approached this burden with different strategies: Two popular ones are <ref type="bibr" target="#b0">(1)</ref> local attention computation within a window <ref type="bibr" target="#b54">[55]</ref> for object detection and (2) pooling attention that locally aggregates features before computing self-attention in video tasks <ref type="bibr">[22]</ref>. The latter fuels Multiscale Vision Transformers (MViT) <ref type="bibr">[22]</ref>, an architecture that extends ViT in a simple way: instead of having a fixed resolution throughout the network, it has a feature hierarchy with multiple stages starting from high-resolution to low-resolution. MViT is designed for video tasks where it has state-of-the-art performance.</p><p>In this paper, we develop two simple technical improvements to further increase its performance and study MViT as a single model family for visual recognition across 3 tasks: image classification, object detection and video classification, in order to understand if it can serve as a general vision backbone for spatial as well as spatiotemporal recognition tasks (see Fig. <ref type="figure" target="#fig_0">1</ref>). Our empirical study leads to an improved MViT architecture and encompasses the following:</p><p>(i) We create strong baselines that improve pooling attention along two axes: (a) shift-invariant positional embeddings using decomposed location distances to inject position information in Transformer blocks; (b) a residual pooling connection to compensate the effect of pooling strides in attention computation. Our simple-yet-effective upgrades lead to significantly better results.</p><p>(ii) Using the improved structure of MViT, we employ a standard dense prediction framework: Mask R-CNN <ref type="bibr" target="#b35">[36]</ref> with Feature Pyramid Networks (FPN) <ref type="bibr" target="#b52">[53]</ref> and apply it to object detection and instance segmentation.</p><p>We study if MViT can process high-resolution visual input by using pooling attention to overcome the computation and memory cost involved. Our experiments suggest that pooling attention is more effective than local window attention mechanisms (e.g. Swin <ref type="bibr" target="#b54">[55]</ref>). We further develop a simple-yet-effective Hybrid window attention scheme that can complement pooling attention for better accuracy/compute tradeoff.</p><p>(iii) We instantiate our architecture in five sizes of increasing complexity (width, depth, resolution) and report a practical training recipe for large multiscale transformers. The MViT variants are applied to image classification, object detection and video classification, with minimal modification, to study its purpose as a generic vision architecture.</p><p>Experiments reveal that our MViTs achieve 88.8% accuracy for ImageNet-1K classification, with pretraining on ImageNet-21K (and 86.3% without), as well as 56.1 AP box on COCO object detection using only Cascade Mask R-CNN <ref type="bibr" target="#b5">[6]</ref>. For video classification tasks, MViT achieves unprecedented accuracies of 86.1% on Kinetics-400, 87.9% on Kinetics-600, 79.4% on Kinetics-700, and 73.3% on Something-Something-v2. Our video code will be open-sourced in PyTorchVideo 1,2  <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>CNNs serve as the primary backbones for computer vision tasks, including image recognition <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b71">72]</ref>, object detection <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b93">94]</ref> and video recognition <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b84">85,</ref><ref type="bibr" target="#b92">93]</ref>.</p><p>Vision transformers have generated a lot of recent enthusiasm since the work of ViT <ref type="bibr" target="#b17">[18]</ref>, which applies a Transformer architecture on image patches and shows very competitive results on image classification. Since then, different works have been developed to further improve ViT, including efficient training recipes <ref type="bibr" target="#b73">[74]</ref>, multi-scale transformer structures [22, <ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b78">79]</ref> and advanced self-attention mechanism design <ref type="bibr" target="#b11">[12,</ref><ref type="bibr">22,</ref><ref type="bibr" target="#b54">55]</ref>. In this work, we build upon the Multiscale Vision Transformers (MViT) and study it as a general backbone for different vision tasks.</p><p>Vision transformers for object detection tasks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b89">90]</ref> address the challenge of detection typically requiring high-resolution inputs and feature maps for accurate object localization. This significantly increases computation complexity due to the quadratic complexity of self-attention operators in transformers <ref type="bibr" target="#b76">[77]</ref>. Recent works develop technology to alleviate this cost, including shifted window attention <ref type="bibr" target="#b54">[55]</ref> and Longformer attention <ref type="bibr" target="#b89">[90]</ref>. Meanwhile, pooling attention in MViT is designed to compute self-attention efficiently using a different perspective <ref type="bibr">[22]</ref>. In this work, we study</p><p>The key idea of MViTv1 <ref type="bibr">[22]</ref> is to construct different stages for both low-and high-level visual modeling instead of single-scale blocks in ViT <ref type="bibr" target="#b17">[18]</ref>. MViT slowly expands the channel width D, while reducing the resolution L (i.e. sequence length), from input to output stages of the network.</p><p>To perform downsampling within a transformer block, MViT introduces Pooling Attention. Concretely, for an input sequence, X ∈ R L×D , it applies linear projections W Q , W K , W V ∈ R D×D followed by pooling operators (P) to query, key and value tensors, respectively:</p><formula xml:id="formula_0">Q = P Q (XW Q ) , K = P K (XW K ) , V = P V (XW V ) ,<label>(1)</label></formula><p>where the length L of Q ∈ R L×D can be reduced by P Q and K and V length can be reduced by P K and P V . Subsequently, pooled self-attention,</p><formula xml:id="formula_1">Z := Attn(Q, K, V ) = Softmax QK / √ D V,<label>(2)</label></formula><p>computes the output sequence Z ∈ R L×D with flexible length L. Note that the downsampling factors P K and P V for key and value tensors can be different from the ones applied to the query sequence, P Q .</p><p>Pooling attention enables resolution reduction between different stages of MViT by pooling the query tensor Q, and to significantly reduce compute and memory complexity by pooling the key, K, and value, V , tensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Improved Multiscale Vision Transformers</head><p>In this section, we first introduce an empirically powerful upgrade to pooling attention ( §4.1). Then we describe how to employ our generic MViT architecture for object detection ( §4.2) and video recognition ( §4.3). Finally, §4.4 shows five concrete instantiations for MViT in increasing complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Improved Pooling Attention</head><p>We start with re-examining two important implications of MViT for potential improvement and introduce techniques to understand and address them. Decomposed relative position embedding. While MViT has shown promises in their power to model interactions between tokens, they focus on content, rather than structure. The space-time structure modeling relies solely on the "absolute" positional embedding to offer location information. This ignores the fundamental principle of shift-invariance in vision <ref type="bibr" target="#b46">[47]</ref>. Namely, the way MViT models the interaction between two patches will change depending on their absolute position in images even if their relative positions stay unchanged. To address this issue, we incorporate relative positional embeddings <ref type="bibr" target="#b64">[65]</ref>, which only depend on the relative location distance between tokens into the pooled self-attention computation.</p><p>We encode the relative position between the two input elements, i and j, into positional embedding R p(i),p(j) ∈ R d , where p(i) and p(j) denote the spatial (or spatiotemporal) position of element i and j. <ref type="foot" target="#foot_0">3</ref> The pairwise encoding representation is then embedded into the self-attention module:</p><formula xml:id="formula_2">Attn(Q, K, V ) = Softmax (QK + E (rel) )/ √ d V,</formula><p>where</p><formula xml:id="formula_3">E (rel) ij = Q i • R p(i),p(j) .<label>(3)</label></formula><p>However, the number of possible embeddings R p(i),p(j) scale in O(T W H), which can be expensive to compute. To reduce complexity, we decompose the distance computation between element i and j along the spatiotemporal axes:</p><formula xml:id="formula_4">R p(i),p(j) = R h h(i),h(j) + R w w(i),w(j) + R t t(i),t(j) ,<label>(4)</label></formula><p>where R h , R w , R t are the positional embeddings along the height, width and temporal axes, and h(i), w(i), and t(i) denote the vertical, horizontal, and temporal position of token i, respectively. Note that R t is optional and only required to support temporal dimension in the video case. In comparison, our decomposed embeddings reduce the number of learned embeddings to O(T + W + H), which can have a large effect for early-stage, high-resolution feature maps.</p><p>Residual pooling connection. As demonstrated [22], pooling attention is very effective to reduce the computation complexity and memory requirements in attention blocks. MViTv1 has larger strides on K and V tensors than the stride of the Q tensors which is only downsampled if the resolution of the output sequence changes across stages. This motivates us to add the residual pooling connection with the (pooled) Q tensor to increase information flow and facilitate the training of pooling attention blocks in MViT.</p><p>We introduce a new residual pooling connection inside the attention blocks as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. Specifically, we add the pooled query tensor to the output sequence Z. So Eq. ( <ref type="formula" target="#formula_1">2</ref>) is reformulated as:</p><formula xml:id="formula_5">Z := Attn (Q, K, V ) + Q. (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>Note that the output sequence Z has the same length as the pooled query tensor Q.</p><p>The ablations in §6.2 and §5.3 shows that both the pooling operator (P Q ) for query Q and the residual path are necessary for the proposed residual pooling connection. This change still enjoys the low-complexity attention computation with large strides in key and value pooling as adding the pooled query sequence in Eq. ( <ref type="formula" target="#formula_5">5</ref>) comes at a low cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">MViT for Object Detection</head><p>In this section, we describe how to apply the MViT backbone for object detection and instance segmentation tasks.</p><p>FPN integration. The hierarchical structure of MViT produces multiscale feature maps in four stages, and therefore naturally integrates into Feature Pyramid Networks (FPN) <ref type="bibr" target="#b52">[53]</ref> for object detection tasks, as shown in Fig. <ref type="figure" target="#fig_2">3</ref>. The top-down pyramid with lateral connections in FPN constructs semantically strong feature maps for MViT at all scales. By using FPN with the MViT backbone, we apply it to different detection architectures (e.g. Mask R-CNN <ref type="bibr" target="#b35">[36]</ref>).</p><p>Hybrid window attention. The self-attention in Transformers has quadratic complexity w.r.t. the number of tokens. This issue is more exacerbated for object detection as it typically requires high-resolution inputs and feature maps. In this paper, we study two ways to significantly reduce this compute and memory complexity: First, the pooling attention designed in attention blocks of MViT. Second, window attention used as a technique to reduce computation for object detection in Swin <ref type="bibr" target="#b54">[55]</ref>.</p><p>Pooling attention and window attention both control the complexity of self-attention by reducing the size of query, key and value tensors when computing self-attention. Their intrinsic nature however is different: Pooling attention pools features by downsampling them via local aggregation, but keeps a global self-attention computation, while window attention keeps the resolution of tensors but performs selfattention locally by dividing the input (patchified tokens) into non-overlapping windows and then only compute local self-attention within each window. The intrinsic difference of the two approaches motivates us to study if they could perform complementary in object detection tasks.</p><p>Default window attention only performs local selfattention within windows, thus lacking connections across windows. Different from Swin <ref type="bibr" target="#b54">[55]</ref>, which uses shifted windows to mitigate this issue, we propose a simple Hybrid window attention (Hwin) design to add cross-window connections. Hwin computes local attention within a window in all but the last blocks of the last three stages that feed into FPN. In this way, the input feature maps to FPN contain global information. The ablation in §5.3 shows that this simple Hwin performs consistently better than Swin <ref type="bibr" target="#b54">[55]</ref> on image classification and object detection tasks. Further, we will show that combining pooling attention and Hwin achieves the best performance for object detection.</p><p>Positional embeddings in detection. Different from Ima-geNet classification where the input is a crop of fixed resolution (e.g. 224×224), object detection typically encompasses inputs of varying size in training. For the positional embeddings in MViT (either absolute or relative), we first initialize the parameters from the ImageNet pre-training weights corresponding to positional embeddings with 224×224 input size and then interpolate them to the respective sizes for object detection training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">MViT for Video Recognition</head><p>MViT can be easily adopted for video recognition tasks (e.g. the Kinetics dataset) similar to MViTv1 [22] as the upgraded modules in §4.1 generalize to the spatiotemporal domain. While MViTv1 only focuses on the training-fromscratch setting on Kinetics, in this work, we also study the (large) effect of pre-training from ImageNet datasets.</p><p>Initialization from pre-trained MViT. Compared to the image-based MViT, there are only three differences for video- based MViT: 1) the projection layer in the patchification stem needs to project the input into space-time cubes instead of 2D patches; 2) the pooling operators now pool spatiotemporal feature maps; 3) relative positional embeddings reference space-time locations.</p><formula xml:id="formula_7">Model #Channels #Blocks #Heads FLOPs Param MViT-T [96-192-384-768] [1-2-5-2] [1-2-4-8] 4.7 24 MViT-S [96-192-384-768] [1-2-11-2] [1-2-4-8] 7.0 35 MViT-B [96-192-384-768] [2-3-16-3] [1-2-4-</formula><p>As the projection layer and pooling operators in 1) and 2) are instantiated by convolutional layers by default <ref type="foot" target="#foot_1">4</ref> , we use an inflation initialization as for CNNs <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b24">25]</ref>. Specifically, we initialize the conv filters for the center frame with the weights from the 2D conv layers in pre-trained models and initialize other weights as zero. For 3), we capitalize on our decomposed relative positional embeddings in Eq. 4, and simply initialize the spatial embeddings from pre-trained weights and the temporal embedding as zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">MViT Architecture Variants</head><p>We build several MViT variants with different number of parameters and FLOPs as shown in Table <ref type="table" target="#tab_0">1</ref>, in order to have a fair comparison with other vision transformer works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b81">82]</ref>. Specifically, we design five variants (Tiny, Small, Base, Large and Huge) for MViT by changing the base channel dimension, the number of blocks in each stage and the number of heads in the blocks. Note that we use a smaller number of heads to improve runtime, as more heads lead to slower runtime but have no effect on FLOPs and Parameters.</p><p>Following the pooling attention design in MViT [22], we employ Key and Value pooling in all pooling attention blocks by default and the pooling stride is set to 4 in the first stage and adaptively decays stride w.r.t resolution across stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments: Image Recognition</head><p>We conduct experiments on ImageNet classification <ref type="bibr" target="#b14">[15]</ref> and COCO object detection <ref type="bibr" target="#b53">[54]</ref>. We first show state-of-theart comparisons and then perform comprehensive ablations. More results and discussions are in §A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Image Classification on ImageNet-1K</head><p>Settings. The ImageNet-1K <ref type="bibr" target="#b14">[15]</ref> (IN-1K) dataset has ∼1.28M images in 1000 classes. Our training recipe for MViT on ImageNet-1K is following MViTv1 <ref type="bibr">[22,</ref><ref type="bibr" target="#b72">73]</ref>. We train all MViT variants for 300 epochs without using EMA. Our MViT outperforms other Transformers, including DeiT <ref type="bibr" target="#b72">[73]</ref> and Swin <ref type="bibr" target="#b54">[55]</ref>, especially when scaling up models. For example, MViT-B achieves 84.4% top-1 accuracy, surpassing DeiT-B and Swin-B by 2.6% and 1.1% respectively. Note that MViT-B has over 33% fewer flops and parameters comparing DeiT-B and Swin-B. The trend is similar with 384×384 input and MViT-B has further +0.8% gain from the high-resolution fine-tuning under center crop testing.</p><p>In addition to center crop testing (with a 224/256=0.875 crop ratio), we report a testing protocol that has been adopted recently in the community <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b81">82]</ref>: This protocol takes a full-sized crop of the (resized) original validation images. We observe that full crop testing can increase our MViT-L ↑ 384 2 from 86.0 to 86.3%, which is the highest accuracy on IN-1K to date (without external data or distillation models).</p><p>Results using ImageNet-21K. Results for using the largescale IN-21K pre-training are shown in Table <ref type="table" target="#tab_2">3</ref>. The IN-21K data adds +2.2% accuracy to MViT-L.</p><p>Compared to other Transformers, our MViT-L achieves better results than Swin-L (+1.2%). We lastly finetune our MViT-L with 384 2 input to directly compare to prior models of size L: Our MViT-L achieves 88.4%, outperforming other large models. We further train a huge MViT-H with accurac 88.0%, 88.6% and 88.8% at 224 2 , 384 2 and 512 2 resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Object Detection on COCO</head><p>Settings. We conduct object detection experiments on the MS-COCO dataset <ref type="bibr" target="#b53">[54]</ref>. All the models are trained on 118K training images and evaluated on the 5K validation images. We use standard Mask R-CNN <ref type="bibr" target="#b35">[36]</ref> and Cascade Mask R-CNN <ref type="bibr" target="#b5">[6]</ref> detection frameworks implemented in Detec-tron2 <ref type="bibr" target="#b82">[83]</ref>. For a fair comparison, we follow the same recipe as in Swin <ref type="bibr" target="#b54">[55]</ref>. Specifically, we pre-train the backbones on IN and fine-tune on COCO using a 3×schedule (36 epochs) by default. Detailed training recipes are in §B.3.</p><p>For MViT, we take the backbone pre-trained from IN and add our Hybrid window attention (Hwin) by default. The window sizes are set as <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b6">7]</ref>  Main results. Table <ref type="table" target="#tab_17">5a</ref> shows the results on COCO using Mask R-CNN. Our MViT surpasses CNN (i.e. ResNet <ref type="bibr" target="#b37">[38]</ref> and ResNeXt <ref type="bibr" target="#b83">[84]</ref>) and Transformer backbones (e.g. Swin <ref type="bibr" target="#b54">[55]</ref>, ViL <ref type="bibr" target="#b89">[90]</ref>  Enhanced detectors such as HTC++ <ref type="bibr" target="#b9">[10]</ref> and inference strategies (e.g. SoftNMS <ref type="bibr" target="#b3">[4]</ref> or multi-scale testing) can boost this number further but are out of scope for this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablations on ImageNet and COCO</head><p>Different self-attention mechanism. We first study our pooling attention and Hwin self-attention mechanism in MViT by comparing with different self-attention mechanisms on ImageNet and COCO. For a fair comparison, we conduct the analysis on both ViT-B and MViT-S networks.</p><p>In Table <ref type="table" target="#tab_17">4a</ref> we compare different attention schemes on IN-1K. We compare 5 attention mechanisms: global (full), windowed, Shifted window (Swin), our Hybrid window (Hwin) and pooling. We observe the following:</p><p>(i) For ViT-B based models, default win reduces both FLOPs and Memory usage while the top-1 accuracy also drops by 2.0% due to the missing cross-window connection. Swin <ref type="bibr" target="#b54">[55]</ref> attention can recover 0.4% over default win. While our Hybrid window (Hwin) attention fully recovers the performance and outperforms Swin attention by +1.7%. Finally, pooling attention achieves the best accuracy/computation trade-off by getting similar accuracy for ViT-B with significant compute reduction (∼38% fewer FLOPs). 5  (ii) For MViT-S, pooling attention is used by default. We study if adding local window attention can improve MViT. We observe that adding Swin or Hwin both can reduce the model complexity with slight performance decay. However, directly increasing the pooling stride (from 4 to 8) achieves the best accuracy/compute tradeoff.</p><p>Table <ref type="table" target="#tab_5">4b</ref> shows the comparison of attention mechanisms on COCO detection task: (i) For ViT-B based models, pooling and pooling + Hwin achieves even better results (+0.6/0.3 AP box ) than standard full attention with ∼2× test speedup. (ii) For MViT-S, directly increasing the pooling stride (from 4 to 8) achieves better accuracy/computation tradeoff than adding Swin. This result suggests that simple pooling attention can be a strong baseline for object detection. Finally, combining our pooling and Hwin achieves the best tradeoff. Residual pooling connection. As shown in Table <ref type="table" target="#tab_10">9</ref>, FPN significantly improves performance for both backbones while MViT-S is consistently better than ViT-B. Note that the FPN gain for MViT-S (+2.9 AP box ) is much larger than those for ViT-B (+1.5 AP box ), which shows the effectiveness of a native hierarchical multiscale design for dense object detection tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments: Video Recognition</head><p>We apply our MViT on Kinetics-400 <ref type="bibr" target="#b43">[44]</ref> (K400), Kinetics-600 (K600) <ref type="bibr" target="#b7">[8]</ref>, and Kinetics-700 (K700) <ref type="bibr" target="#b6">[7]</ref> and Something-Something-v2 <ref type="bibr" target="#b30">[31]</ref> (SSv2) datasets.</p><p>Settings. By default, our MViT models are trained from scratch on Kinetics and fine-tuned from Kinetics models for SSv2. The training recipe and augmentations follow <ref type="bibr" target="#b19">[20,</ref><ref type="bibr">22]</ref>. When using IN-1K or IN-21K as pre-training, we adopt the initialization scheme introduced in §4.3 and shorter training.</p><p>For the temporal domain, we sample a T ×τ clip from the full-length video which contains T frames with a temporal stride of τ . For inference, we apply two testing strategies following <ref type="bibr">[22,</ref><ref type="bibr" target="#b23">24]</ref>: (i) Temporally, uniformly samples K clips (e.g. K=5) from a video. (ii) in spatial axis, scales the shorter spatial side to 256 pixels and takes a 224×224 center crop or 3 crops of 224×224 to cover the longer spatial axis. The final score is averaged over all predictions.</p><p>Implementation and training details are in §B. Table <ref type="table" target="#tab_1">12</ref>. Comparison with previous work on Kinetics-700.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Main Results</head><p>Kinetics-400. Prior ViT-based models require large-scale pre-training on IN-21K to produce best accuracy on K400. We fine-tune our MViT-L with large spatiotemporal input size 40×312 2 (time ×space 2 ) to reach 86.1% top-1 accuracy, showing the performance of our architecture in a large-scale setting.</p><p>Kinetics-600/-700. Table <ref type="table" target="#tab_12">11</ref> shows the results on K600. We train MViT-B, 32×3 from scratch and achieves 85.5% top-1 accuracy, which is better than the MViTv1 counterpart (+1.4%), and even better than other ViTs with IN-21K pre-training(e.g. +1.5% over Swin-B <ref type="bibr" target="#b55">[56]</ref>) while having ∼2.2×and ∼40% fewer FLOPs and parameters. The larger MViT-L 40×3 sets a new state-of-the-art at 87.9%. In Table <ref type="table" target="#tab_1">12</ref>, our MViT-L achieves 79.4% on K700 which greatly surpasses the previous best result by +7.1%.</p><p>Something-something-v2. Table <ref type="table" target="#tab_14">13</ref> compares methods on a more 'temporal modeling' dataset SSv2. Our MViT-S with 16 frames first improves over MViTv1 counterpart by a large gain (+3.5%), which verifies the effectiveness of our proposed pooling attention for temporal modeling. The deeper MViT-B achieves 70.5% top-1 accuracy, surpassing the previous best result Swin-B with IN-21K and K400 pre-training by +0.9% while using ∼30% and 40% fewer FLOPs and parameters and only K400. With IN-21K pretraining, MViT-B boosts accuracy by 1.6% and achieves 72.1%. MViT-L achieves 73.3% top-1 and 94.1% top-5 accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Ablations on Kinetics</head><p>In this section, we carry out MViT ablations on K400. The video ablation our technical improvements share trends with Table <ref type="table" target="#tab_6">6</ref>  Table <ref type="table" target="#tab_5">14</ref>.</p><p>Effect of pre-training on K400.</p><p>We use viewspace×viewtime = 1×10 crops for inference.</p><p>Effect of pre-training datasets. Table <ref type="table" target="#tab_5">14</ref> compares the effect different pre-training schemes on K400. We observe that: (i) For MViT-S and MViT-B models, using either IN1K or IN21k pre-training boosts accuracy compared to training from scratch, e.g.MViT-S gets +1.0% and 1.4% gains with IN1K and IN21K pre-training. (ii) For large models, Ima-geNet pre-training is necessary as they are heavily overfitting when trained from scratch (cf . Table <ref type="table" target="#tab_11">10</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We present an improved Multiscale Vision Transformer as a general hierarchical architecture for visual recognition. In empirical evaluation, MViT shows strong performance compared to other vision transformers and achieves state-ofthe-art accuracy on widely-used benchmarks across image classification, object detection, instance segmentation and video recognition. We hope that our architecture will be useful for further research in visual recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>This appendix provides further details for the main paper:</p><p>§A contains further results for AVA action detection ( §A.1) and ImageNet classification ( §A.2), as well as ablations for COCO object detection ( §A.3) and Kinetics action classification ( §A.4).</p><p>§B contains additional MViT upgrade details ( §B.1), and additional implementation details for: ImageNet classification ( §B.2), COCO object detection ( §B.3), Kinetics action classification ( §B.4), SSv2 action classification ( §B.5), and AVA action detection ( §B.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Results: AVA Action Detection</head><p>Results on AVA. Table <ref type="table" target="#tab_17">A</ref>.1 shows the results of our MViT models compared with prior state-of-the-art works on the AVA dataset <ref type="bibr" target="#b31">[32]</ref> which is a dataset for spatiotemporallocalization of human actions.</p><p>We observe that MViT consistently achieves better results compared to MViTv1 [22] counterparts. For example, MViT-S 16×4 (26.8 mAP) improves +2.3 over MViTv1-B 16×4 (24.5 mAP) with fewer flops and parameters (both with the same recipe and default K400 pre-training). For K600 pretraining, MViT-B 32×3 (29.9 mAP) improves +1.2 over MViTv1-B-24 32×3. This again validates the effectiveness of the proposed MViT improvements in §4.1 of the main paper. Using full-resolution testing (without cropping) can further improve MViT-B by +0.6 to achieve 30.5 mAP. Finally, the larger MViT-L 40×3 achieves the state-of-the-art results at 34.4 mAP using IN-21K and K700 pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Results: ImageNet Classification</head><p>Results of ImageNet-1K. Table <ref type="table" target="#tab_17">A</ref>.3 shows the comparison of our MViT with more prior work (without external data or distillation models) on ImageNet-1K. As shown in the Table, our MViT achieves better results than any previously published methods for a variety of model complexities. We note that our improvements to pooling attention bring significant gains over the MViTv1 <ref type="bibr">[22]</ref> counterparts which use exactly the same training recipes (for all datasets we compare on); therefore the gains over MViTv1 stem solely from our technical improvements in §4.1 of the main paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Ablations: COCO Object Detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Ablations: Kinetics Action Classification</head><p>In §5.3 of the main paper we ablated the impact of our improvements to pooling attention, i.e. decomposed relative positional embeddings &amp; residual pooling connections, for image classification and object detection. Here, we ablate the effect of our improvements for video classification.</p><p>Positional embeddings for video. Table <ref type="table" target="#tab_17">A</ref>.4 compares different positional embeddings for MViT on K400. Similar to image classification and object detection (Table <ref type="table" target="#tab_6">6</ref> of the main paper), relative positional embeddings surpass absolute positional embeddings by ∼0.6% comparing (2) and <ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b5">6)</ref>. Comparing ( <ref type="formula" target="#formula_5">5</ref>) to (6), our decomposed space/time rel. positional embeddings achieve nearly the same accuracy as the joint space rel. embeddings while being ∼2× faster in training. For joint space/time rel. (5 vs. 7), our decomposed space/time rel. is even ∼8×faster with ∼2×fewer parameters. This demonstrates the effectiveness of our decomposed design for relative positional embeddings.</p><p>Residual pooling connection for video. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Other Upgrades in MViT</head><p>Besides the technical improvements introduced in §4.1 of the main paper, MViT entails two further changes: (i) We conduct the channel dimension expansion in the attention computation of the first transformer block of each stage, instead of performing it in the last MLP block of the prior stage as in <ref type="bibr">MViTv1 [22]</ref>. This change has similar accuracy (±0.1%) to the original version, while reducing parameters and FLOPs. (ii) We remove the class token in MViT by default as this has no advantage for image classification tasks. Instead, we average the output tokens from the last transformer block and apply the final classification head upon it. In practice, we find this modification could reduce the training time by ∼8%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Details: ImageNet Classification IN-1K training.</head><p>We follow the training recipe of MViTv1 <ref type="bibr">[22,</ref><ref type="bibr" target="#b72">73]</ref> for IN-1K training. We train for 300 epochs with 64 GPUs. The batch size is 32 per GPU by default. We use truncated normal distribution initialization <ref type="bibr" target="#b34">[35]</ref> and adopt synchronized AdamW <ref type="bibr" target="#b57">[58]</ref> optimization with a base learning rate of 2 × 10 −3 for batch size of 2048. We use a linear warm-up strategy in the first 70 epochs and a decayed half-period cosine schedule <ref type="bibr" target="#b72">[73]</ref>.</p><p>For regularization, we set weight decay to 0.05 for MViT-T/S/B and 0.1 for MViT-L/H and label-smoothing <ref type="bibr" target="#b70">[71]</ref> to 0.1. Stochastic depth <ref type="bibr" target="#b40">[41]</ref> (i.e. drop-path or drop-connect) is also used with rate 0.1 for MViT-T &amp; MViT-S, rate 0.3 for MViT-B, rate 0.5 for MViT-L and rate 0.8 for MViT-H. Other data augmentations have the same (default) hyperparameters as in <ref type="bibr">[22,</ref><ref type="bibr" target="#b73">74]</ref>, including mixup <ref type="bibr" target="#b88">[89]</ref>, cutmix <ref type="bibr" target="#b87">[88]</ref>, random erasing <ref type="bibr" target="#b91">[92]</ref> and rand augment <ref type="bibr" target="#b12">[13]</ref>.</p><p>For 384×384 input resolution, we fine-tune the models trained on 224×224 resolution. We decrease the batch size to 8 per GPU and fine-tune 30 epochs with a base learning rate of 4 × 10 −5 per 256 batch-size samples. For MViT-L and MViT-H, we disable mixup and fine-tune with a learning rate of 5 × 10 −4 per batch of 64. We linearly scale learning rates with the number of overall GPUs (i.e. the overall batch-size). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Details: COCO Object Detection</head><p>For object detection experiments, we adopt two typical object detection framework: Mask R-CNN <ref type="bibr" target="#b35">[36]</ref> and Cascade Mask R-CNN <ref type="bibr" target="#b5">[6]</ref> in Detectron2 <ref type="bibr" target="#b82">[83]</ref>. We follow the same training settings from <ref type="bibr" target="#b54">[55]</ref>: multi-scale training (scale the shorter side in [480, 800] while longer side is smaller than 1333), AdamW optimizer <ref type="bibr" target="#b57">[58]</ref> (β 1 , β 2 = 0.9, 0.999, base learning rate 1.6 × 10 −4 for base size of 64, and weight decay of 0.1), and 3×schedule (36 epochs). The drop path rate is set as 0.1, 0.3, 0.4, 0.5 and 0.6 for MViT-T, MViT-S, MViT-B, MViT-L and MViT-H, respectively. We use PyTorch's automatic mixed precision during training.</p><p>For the stronger recipe for MViT-L and MViT-H in Table <ref type="table">.</ref> 5 of the main paper, we use large-scale jittering (1024×1024 resolution) as the training augmentation <ref type="bibr" target="#b26">[27]</ref> and a longer schedule (50 epochs) with IN-21K pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Details: Kinetics Action Classification</head><p>Training from scratch. We follow the training recipe and augmentations from <ref type="bibr" target="#b19">[20,</ref><ref type="bibr">22]</ref> when training from scratch for Kinetics datasets. We adopt synchronized AdamW <ref type="bibr" target="#b57">[58]</ref> and train for 200 epochs with 2 repeated augmentation <ref type="bibr" target="#b39">[40]</ref> on 128 GPUs. The mini-batch size is 4 clips per GPU. We adopt a half-period cosine schedule <ref type="bibr" target="#b56">[57]</ref> of learning rate decaying. The base learning rate is set as 1.6 × 10 −3 for 512 batch-size. We use weight decay of 0.05 and set drop path rate as 0.2 and 0.3 for MViT-S and MViT-B.</p><p>For the input clip, we randomly sample a clip (T frames with a temporal stride of τ ; denoted as T × τ <ref type="bibr" target="#b23">[24]</ref>) from the full-length video during training. For the spatial domain, we use Inception-style <ref type="bibr" target="#b69">[70]</ref> cropping (randomly resize the input area between a [min, max], scale of [0.08, 1.00], and jitter aspect ratio between 3/4 to 4/3). Then we take an H × W = 224×224 crop as the network input.</p><p>For the input clips, we perform the same data augmentations across all frames, including random horizontal flip, mixup <ref type="bibr" target="#b88">[89]</ref> and cutmix <ref type="bibr" target="#b87">[88]</ref>, random erasing <ref type="bibr" target="#b91">[92]</ref>, and rand augment <ref type="bibr" target="#b12">[13]</ref>.</p><p>For Kinetics-600 and Kinetics-700, all hyper-parameters are identical to K400. The SSv2 dataset <ref type="bibr" target="#b30">[31]</ref> contains 169k training, and 25k validation videos with 174 human-object interaction classes. We fine-tune the pre-trained Kinetics models and take the same recipe as in <ref type="bibr">[22]</ref>. Specifically, we train for 100 epochs (40 epochs for MViT-L) using 64 or 128 GPUs with 8 clips per GPU and a base learning rate of 0.02 (for batch size of 512) with half-period cosine decay <ref type="bibr" target="#b56">[57]</ref>. We adopt synchronized SGD and use weight decay of 10 −4 and drop path rate of 0.4. The training augmentation is the same as Kinetics in §B.4, except we disable random flipping and repeated augmentations in training.</p><p>We use the segment-based input frame sampling [22, 52] (split each video into segments, and sample one frame from each segment to form a clip). During inference, we take a single clip with 3 spatial crops to form predictions over a single video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6. Details: AVA Action Detection</head><p>The AVA action detection dataset <ref type="bibr" target="#b31">[32]</ref> assesses the spatiotemporal localization of human actions in videos. It has 211k training and 57k validation video segments. We evaluate methods on AVA v2.2 and use mean Average Precision (mAP) metric on 60 classes as is standard in prior work <ref type="bibr" target="#b23">[24]</ref>.</p><p>We use MViT as the backbone and follow the same detection architecture in <ref type="bibr">[22,</ref><ref type="bibr" target="#b23">24]</ref> that adapts Faster R-CNN <ref type="bibr" target="#b63">[64]</ref> for video action detection. Specifically, we extract region-ofinterest (RoI) features <ref type="bibr" target="#b28">[29]</ref> by frame-wise RoIAlign <ref type="bibr" target="#b35">[36]</ref> on the spatiotemporal feature maps from the last MViT layer. The RoI features are then max-pooled and fed to a per-class, sigmoid classifier for action prediction.</p><p>The training recipe is identical to [22] and summarized next. We pre-train our MViT models on Kinetics. The region proposals are identical to the ones used in <ref type="bibr">[22,</ref><ref type="bibr" target="#b23">24]</ref>. We use proposals that have overlaps with ground-truth boxes by IoU &gt; 0.9 for training. The models are trained with synchronized SGD training on 64 GPUs (8 clips per GPU). The base learning rate is set as 0.6 with a half-period cosine schedule of learning rate decaying. We train for 30 epochs with linear warm-up <ref type="bibr" target="#b29">[30]</ref> for the first 5 epochs and use a weight decay of 1 × 10 −8 and drop-path rate of 0.4.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Our Improved MViT is a multiscale transformer with state-of-the-art performance across three visual recognition tasks.</figDesc><graphic url="image-1.png" coords="1,310.11,220.61,65.37,135.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The improved Pooling Attention mechanism that incorporating decomposed relative position embedding, R p(i),p(j) , and residual pooling connection modules in the attention block.</figDesc><graphic url="image-4.png" coords="3,60.68,57.06,212.62,202.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. MViT backbone used with FPN for object detection. The multiscale transformer features naturally integrate with standard feature pyramid networks (FPN).</figDesc><graphic url="image-5.png" coords="3,308.86,62.04,236.23,102.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>IN-21K pre-training and fine-tuning on IN-1K. We download the latest winter-2021 version of IN-21K from the official website. The training recipe follows the IN-1K training introduced above except for some differences described next. We train the IN-21K models on the joint set of IN-21K and 1K for 90 epochs (60 epochs for MViT-H) with a 6.75×10 −5 base learning rate for MViT-S and MViT-B, and 10 −4 for MViT-L and MViT-H, per batch-size of 256. The weight decay is set as 0.01 for MViT-S and MViT-B, and 0.1 for MViT-L and MViT-H. When fine-tuning IN-21K MViT models on IN-1K for MViT-L and MViT-H, we disable mixup and fine-tune for 30 epochs with a learning rate of 7 × 10 −5 per batch of 64. We use a weight decay of 5 × 10 −2 . The MViT-H ↑ 512 2 model is initialized from the 384 2 variant and trained for 3 epochs with mixup enabled and weight decay of 10 −8 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fine-tuning</head><label></label><figDesc>from ImageNet. When using IN-1K or IN-21K as pre-training, we adopt the initialization scheme introduced in §4.3 of the main paper and shorter training schedules. For example, we train 100 epochs with base learning rate as 4.8 × 10 −4 for 512 batch-size when fine-tuning from IN-1K for MViT-S and MViT-B, and 75 epochs with base learning as 1.6 × 10 −4 when fine-tuning from IN-21K. For long-term models with 40×3 sampling, we initialize from the 16×4 counterparts, disable mixup, train for 30 epochs with learning rate of 1.6 × 10 −5 at batch-size of 128, and use a weight decay of 10 −8 . B.5. Details: Something-Something V2 (SSv2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>8] 10.2 52 MViT-L [144-288-576-1152] [2-6-36-4] [2-4-8-16] 39.6 218 MViT-H [192-384-768-1536] [4-8-60-8] [3-6-12-24] 120.6 667 Configuration for MViT variants. #Channels, #Blocks and #Heads specify the channel width, number of MViT blocks and heads in each block for the four stages, respectively. FLOPs are measured for image classification with 224 × 224 input. The stage resolutions are [56 2 , 28 2 , 14 2 , 7 2 ].</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison</figDesc><table><row><cell></cell><cell>Acc</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Acc</cell><cell></cell></row><row><cell>model</cell><cell cols="4">center resize FLOPs (G) Param (M)</cell><cell>model</cell><cell cols="3">center resize FLOPs (G) Param (M)</cell></row><row><cell>RegNetZ-4GF [16]</cell><cell>83.1</cell><cell></cell><cell>4.0</cell><cell>28</cell><cell>Swin-L [55]</cell><cell>86.3</cell><cell>34.5</cell><cell>197</cell></row><row><cell>EfficientNet-B4 ↑ 380 2 [72]</cell><cell>82.9</cell><cell></cell><cell>4.2</cell><cell>19</cell><cell>MViT-L</cell><cell>87.5</cell><cell>42.1</cell><cell>218</cell></row><row><cell>DeiT-S [73]</cell><cell>79.8</cell><cell></cell><cell>4.6</cell><cell>22</cell><cell>MViT-H</cell><cell>88.0</cell><cell>120.6</cell><cell>667</cell></row><row><cell>TNT-S [33]</cell><cell>81.5</cell><cell></cell><cell>5.2</cell><cell>24</cell><cell>ViT-L/16 ↑ 384 2 [18]</cell><cell>85.2</cell><cell>190.7</cell><cell>307</cell></row><row><cell>PVTv2-V2 [78]</cell><cell>82.0</cell><cell></cell><cell>4.0</cell><cell>25</cell><cell cols="2">ViL-B-RPB ↑ 384 2 [90] 86.2</cell><cell>43.7</cell><cell>56</cell></row><row><cell>CoAtNet-0 [14]</cell><cell>81.6</cell><cell></cell><cell>4.2</cell><cell>25</cell><cell>Swin-L ↑ 384 2 [55]</cell><cell>87.3</cell><cell>103.9</cell><cell>197</cell></row><row><cell>XCiT-S12 [19]</cell><cell>82.0</cell><cell></cell><cell>4.8</cell><cell>26</cell><cell>CSwin-L ↑ 384 2 [17]</cell><cell>87.5</cell><cell>96.8</cell><cell>173</cell></row><row><cell>Swin-T [55]</cell><cell>81.3</cell><cell></cell><cell>4.5</cell><cell>29</cell><cell>CvT-W24 ↑ 384 2 [82]</cell><cell>87.6</cell><cell>193.2</cell><cell>277</cell></row><row><cell>CSWin-T [17]</cell><cell>82.7</cell><cell></cell><cell>4.3</cell><cell>23</cell><cell>CoAtNet-4 [14] ↑ 512 2</cell><cell>88.4</cell><cell>360.9</cell><cell>275</cell></row><row><cell>MViT-T</cell><cell>82.3</cell><cell></cell><cell>4.7</cell><cell>24</cell><cell>MViT-L ↑ 384 2</cell><cell>88.2 88.4</cell><cell>140.7</cell><cell>218</cell></row><row><cell>RegNetY-8GF [62] EfficientNet-B5 ↑ 456 2 [72]</cell><cell>81.7 83.6</cell><cell></cell><cell>8.0 9.9</cell><cell>39 30</cell><cell>MViT-H ↑ 384 2 MViT-H ↑ 512 2</cell><cell>88.4 88.6 88.3 88.8</cell><cell>388.5 763.5</cell><cell>667 667</cell></row><row><cell>Twins-B [12]</cell><cell>83.2</cell><cell></cell><cell>8.6</cell><cell>56</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PVTv2-V2-B3 [78]</cell><cell>83.2</cell><cell></cell><cell>6.9</cell><cell>45</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Swin-S [55]</cell><cell>83.0</cell><cell></cell><cell>8.7</cell><cell>50</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CSWin-S [17]</cell><cell>83.6</cell><cell></cell><cell>6.9</cell><cell>35</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MViT-v1-B-16 [22]</cell><cell>83.0</cell><cell></cell><cell>7.8</cell><cell>37</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MViT-S</cell><cell>83.6</cell><cell></cell><cell>7.0</cell><cell>35</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RegNetZ-16GF [16]</cell><cell>84.1</cell><cell></cell><cell>15.9</cell><cell>95</cell><cell></cell><cell></cell><cell></cell></row><row><cell>EfficientNet-B6 ↑ 528 2 [72]</cell><cell>84.2</cell><cell></cell><cell>19</cell><cell>43</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DeiT-B [73]</cell><cell>81.8</cell><cell></cell><cell>17.6</cell><cell>87</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PVTv2-V2-B5 [78]</cell><cell>83.8</cell><cell></cell><cell>11.8</cell><cell>82</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CaiT-S36 [75]</cell><cell>83.3</cell><cell></cell><cell>13.9</cell><cell>68</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CoAtNet-2 [14]</cell><cell>84.1</cell><cell></cell><cell>15.7</cell><cell>75</cell><cell></cell><cell></cell><cell></cell></row><row><cell>XCiT-M24 [19]</cell><cell>82.7</cell><cell></cell><cell>16.2</cell><cell>84</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Swin-B [55]</cell><cell>83.3</cell><cell></cell><cell>15.4</cell><cell>88</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CSWin-B [17]</cell><cell>84.2</cell><cell></cell><cell>15.0</cell><cell>78</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MViTv1-B-24 [22]</cell><cell>83.4</cell><cell></cell><cell>10.9</cell><cell>54</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MViT-B</cell><cell>84.4</cell><cell></cell><cell>10.2</cell><cell>52</cell><cell></cell><cell></cell><cell></cell></row><row><cell>EfficientNet-B7 ↑ 600 2 [72]</cell><cell>84.3</cell><cell></cell><cell>37.0</cell><cell>66</cell><cell></cell><cell></cell><cell></cell></row><row><cell>NFNet-F1 ↑ 320 2 [5]</cell><cell>84.7</cell><cell></cell><cell>35.5</cell><cell>133</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DeiT-B ↑ 384 2 [73]</cell><cell>83.1</cell><cell></cell><cell>55.5</cell><cell>87</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CvT-32 ↑ 384 2 [82]</cell><cell></cell><cell>83.3</cell><cell>24.9</cell><cell>32</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CaiT-S36↑ 384 2 [75]</cell><cell></cell><cell>85.0</cell><cell>48</cell><cell>68</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Swin-B ↑ 384 2 [55]</cell><cell></cell><cell>84.2</cell><cell>47.0</cell><cell>88</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MViT-v1-B-24 ↑ 320 2 [22]</cell><cell>84.8</cell><cell></cell><cell>32.7</cell><cell>73</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MViT-B ↑ 384 2</cell><cell>85.2</cell><cell>85.6</cell><cell>36.7</cell><cell>52</cell><cell></cell><cell></cell><cell></cell></row><row><cell>NFNet-F2 ↑ 352 2 [5]</cell><cell>85.1</cell><cell></cell><cell>62.6</cell><cell>194</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CoAtNet-3 [14]</cell><cell>84.5</cell><cell></cell><cell>34.7</cell><cell>168</cell><cell></cell><cell></cell><cell></cell></row><row><cell>XCiT-M24 [19]</cell><cell>82.9</cell><cell></cell><cell>36.1</cell><cell>189</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MViT-L</cell><cell>85.3</cell><cell></cell><cell>42.1</cell><cell>218</cell><cell></cell><cell></cell><cell></cell></row><row><cell>NFNet-F4 ↑ 512 2 [5]</cell><cell>85.9</cell><cell></cell><cell>215.3</cell><cell>316</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CoAtNet-3 [14] ↑ 384 2</cell><cell></cell><cell>85.8</cell><cell>107.4</cell><cell>168</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MViT-L ↑ 384 2</cell><cell>86.0</cell><cell>86.3</cell><cell>140.2</cell><cell>218</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Results using ImageNet-1K. Table 2 shows our MViT and</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">state-of-the-art CNNs and Transformers (without external</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">data or distillation models [43, 75, 87]). The models are split</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">into groups based on computation and compared next.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Compared to MViTv1 [22], our improved MViT has</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">better accuracy with fewer flops and parameters. For ex-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">ample, MViT-S (83.6%) improves +0.6% over MViTv1-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">B-16 (83.0%) with 10% fewer flops. On the base model</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">size, MViT-B (84.4%) improves +1.0% over MViTv1-B-24</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">(83.4%) while even being lighter. This shows clear effective-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">ness of the MViT improvements in  §4.1.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>to published work on ImageNet-1K. Input images are 224×224 by default and ↑ denotes using different sizes. MViT is trained for 300 epochs without any external data or models. We report ↑ 384 2 MViT tested with center crop or a resized view of the original image, to compare to prior work. Full Table in A.<ref type="bibr" target="#b2">3</ref> We also explore pre-training on ImageNet-21K (IN-21K) with ∼14.2M images and ∼21K classes. See §B for details.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>ImageNet-1K fine-tunning results using IN-21K data. Fine-tuning is with 224 2 input size (default) or with ↑ 384 2 size. Center denotes testing with a center crop, while resize is scaling the full image to the inference resolution (including more context).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>for the four stages, which is consistent with the self-attention size used in IN pre-training which takes 224×224 as input. Results</figDesc><table><row><cell></cell><cell>(a) Mask R-CNN</cell><cell></cell><cell></cell></row><row><cell>model</cell><cell>AP box AP box 50 AP box 75 AP mask AP mask 50</cell><cell>AP mask 75</cell><cell cols="2">FLOPs Param</cell></row><row><cell>Res50 [38]</cell><cell cols="2">41.0 61.7 44.9 37.1 58.4 40.1</cell><cell>260</cell><cell>44</cell></row><row><cell>PVT-S [79]</cell><cell cols="2">43.0 65.3 46.9 39.9 62.5 42.8</cell><cell>245</cell><cell>44</cell></row><row><cell>Swin-T [55]</cell><cell cols="2">46.0 68.2 50.2 41.6 65.1 44.8</cell><cell>264</cell><cell>48</cell></row><row><cell cols="3">ViL-S-RPB [90] 47.1 68.7 51.5 42.7 65.9 46.2</cell><cell>277</cell><cell>45</cell></row><row><cell cols="3">MViTv1-T [22] 45.9 68.7 50.5 42.1 66.0 45.4</cell><cell>326</cell><cell>46</cell></row><row><cell>MViT-T</cell><cell cols="2">48.2 70.9 53.3 43.8 67.9 47.2</cell><cell>279</cell><cell>44</cell></row><row><cell>Res101 [38]</cell><cell cols="2">42.8 63.2 47.1 38.5 60.1 41.3</cell><cell>336</cell><cell>63</cell></row><row><cell>PVT-M [79]</cell><cell cols="2">44.2 66.0 48.2 40.5 63.1 43.5</cell><cell>302</cell><cell>64</cell></row><row><cell>Swin-S [55]</cell><cell cols="2">48.5 70.2 53.5 43.3 67.3 46.6</cell><cell>354</cell><cell>69</cell></row><row><cell cols="3">ViL-M-RPB [90] 48.9 70.3 54.0 44.2 67.9 47.7</cell><cell>352</cell><cell>60</cell></row><row><cell cols="3">MViTv1-S [22] 47.6 70.0 52.2 43.4 67.3 46.9</cell><cell>373</cell><cell>57</cell></row><row><cell>MViT-S</cell><cell cols="2">49.9 72.0 55.0 45.1 69.5 48.5</cell><cell>326</cell><cell>54</cell></row><row><cell>X101-64 [84]</cell><cell cols="2">44.4 64.9 48.8 39.7 61.9 42.6</cell><cell cols="2">493 101</cell></row><row><cell>PVT-L [79]</cell><cell cols="2">44.5 66.0 48.3 40.7 63.4 43.7</cell><cell>364</cell><cell>81</cell></row><row><cell>Swin-B [55]</cell><cell cols="2">48.5 69.8 53.2 43.4 66.8 46.9</cell><cell cols="2">496 107</cell></row><row><cell cols="3">ViL-B-RPB [90] 49.6 70.7 54.6 44.5 68.3 48.0</cell><cell>384</cell><cell>76</cell></row><row><cell cols="3">MViTv1-B [22] 48.8 71.2 53.5 44.2 68.4 47.6</cell><cell>438</cell><cell>73</cell></row><row><cell>MViT-B</cell><cell cols="2">51.0 72.7 56.3 45.7 69.9 49.6</cell><cell>392</cell><cell>71</cell></row><row><cell>MViT-L</cell><cell cols="4">51.8 72.8 56.8 46.2 70.4 50.0 1097 238</cell></row><row><cell>MViT-L †</cell><cell cols="4">52.7 73.7 57.6 46.8 71.4 50.8 1097 238</cell></row><row><cell></cell><cell>(b) Cascade Mask R-CNN</cell><cell></cell><cell></cell></row><row><cell>model</cell><cell>AP box AP box 50 AP box 75 AP mask AP mask 50</cell><cell>AP mask 75</cell><cell cols="2">FLOPs Param</cell></row><row><cell>R50 [38]</cell><cell cols="2">46.3 64.3 50.5 40.1 61.7 43.4</cell><cell>739</cell><cell>82</cell></row><row><cell>Swin-T [55]</cell><cell cols="2">50.5 69.3 54.9 43.7 66.6 47.1</cell><cell>745</cell><cell>86</cell></row><row><cell>MViT-T</cell><cell cols="2">52.2 71.1 56.6 45.0 68.3 48.9</cell><cell>701</cell><cell>76</cell></row><row><cell>X101-32 [84]</cell><cell cols="2">48.1 66.5 52.4 41.6 63.9 45.2</cell><cell cols="2">819 101</cell></row><row><cell>Swin-S [55]</cell><cell cols="2">51.8 70.4 56.3 44.7 67.9 48.5</cell><cell cols="2">838 107</cell></row><row><cell>MViT-S</cell><cell cols="2">53.2 72.4 58.0 46.0 69.6 50.1</cell><cell>748</cell><cell>87</cell></row><row><cell>X101-64 [84]</cell><cell cols="2">48.3 66.4 52.3 41.7 64.0 45.1</cell><cell cols="2">972 140</cell></row><row><cell>Swin-B [55]</cell><cell cols="2">51.9 70.9 56.5 45.0 68.4 48.7</cell><cell cols="2">982 145</cell></row><row><cell>MViT-B</cell><cell cols="2">54.1 72.9 58.5 46.8 70.6 50.8</cell><cell cols="2">814 103</cell></row><row><cell>MViT-B †</cell><cell cols="2">54.9 73.8 59.8 47.4 71.5 51.6</cell><cell cols="2">814 103</cell></row><row><cell>MViT-L</cell><cell cols="4">54.3 73.1 59.1 47.1 70.8 51.7 1519 270</cell></row><row><cell>MViT-L † †</cell><cell cols="4">55.8 74.3 60.9 48.3 71.9 53.2 1519 270</cell></row><row><cell>MViT-H † †</cell><cell cols="4">56.1 74.6 61.0 48.5 72.4 53.2 3084 718</cell></row></table><note>on COCO object detection with (a) Mask R-CNN<ref type="bibr" target="#b35">[36]</ref> and (b) Cascade Mask R-CNN<ref type="bibr" target="#b5">[6]</ref>. † indicates that the model is initialized from IN-21K pre-training. † † denotes using a stronger large-scale jittering training<ref type="bibr" target="#b26">[27]</ref> and longer schedule (50 epochs) with IN-21K pre-training without other bells-and-whistles. FLOPs and Params are in Giga(10 9 ) and Mega(10 6 ).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>and MViTv1 [22]5 ). E.g., MViT-B outperforms Swin-B by +2.5/+2.3 in AP box /AP mask , with lower compute and smaller model size. When scaling up, our deeper MViT-L improves over MViT-B by +0.8 AP box and using IN-21K pre-training further adds +0.9 to achieve 52.7 AP box with Mask R-CNN and a standard 3×schedule.In Table5bwe observe a similar trend among backbones for Cascade Mask R-CNN<ref type="bibr" target="#b5">[6]</ref> which lifts Mask R-CNN accuracy (5a). We also ablate the use of a longer training schedule with large-scale jitter that boosts our AP box to 55.8. MViT-H increases this to 56.1 AP box and 48.5 AP mask .</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>We adapt MViTv1 [22] as a detection baseline combined with Hwin. Comparison</figDesc><table><row><cell></cell><cell cols="3">(a) ImageNet-1K classification</cell><cell></cell><cell></cell><cell cols="3">(b) Mask R-CNN on COCO detection</cell><cell></cell><cell></cell></row><row><cell cols="2">variant attention</cell><cell>Acc</cell><cell cols="2">FLOPs (G) Mem (G)</cell><cell cols="2">variant attention</cell><cell cols="4">AP box Train(iter/s) Test(im/s) Mem(G)</cell></row><row><cell></cell><cell>full</cell><cell>82.0</cell><cell>17.5</cell><cell>12.4</cell><cell></cell><cell>full</cell><cell>46.6</cell><cell>2.3</cell><cell>4.6</cell><cell>24.7</cell></row><row><cell></cell><cell>fixed win</cell><cell>80.0</cell><cell>17.0</cell><cell>9.7</cell><cell></cell><cell>fixed win</cell><cell>43.4</cell><cell>3.3</cell><cell>7.8</cell><cell>5.6</cell></row><row><cell>ViT-B</cell><cell>Swin [55] Hwin</cell><cell>80.4 82.1</cell><cell>17.0 17.1</cell><cell>9.7 10.4</cell><cell>ViT-B</cell><cell>Swin [55] Hwin</cell><cell>45.1 46.1</cell><cell>3.1 3.1</cell><cell>7.5 6.8</cell><cell>5.7 11.0</cell></row><row><cell></cell><cell>pooling</cell><cell>81.9</cell><cell>10.9</cell><cell>8.3</cell><cell></cell><cell>pooling</cell><cell>47.2</cell><cell>2.9</cell><cell>7.9</cell><cell>8.8</cell></row><row><cell></cell><cell>pooling</cell><cell>83.6</cell><cell>7.0</cell><cell>6.8</cell><cell></cell><cell>pooling + Hwin</cell><cell>46.9</cell><cell>3.1</cell><cell>8.8</cell><cell>5.5</cell></row><row><cell>MViT-S</cell><cell cols="2">pooling (stride=8) pooling + Swin [55] 82.8 83.2 pooling + Hwin 83.0</cell><cell>6.3 6.4 6.5</cell><cell>5.5 6.0 6.2</cell><cell>MViT-S</cell><cell cols="2">pooling pooling (stride=8) pooling + Swin [55] 48.9 50.8 50.0</cell><cell>1.5 2.5 2.6</cell><cell>4.2 8.3 9.2</cell><cell>19.5 7.8 4.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>pooling + Hwin</cell><cell>49.9</cell><cell>2.7</cell><cell>9.4</cell><cell>5.2</cell></row></table><note>of attention mechanisms on ImageNet and COCO using ViT-B and MViT-S backbones. fixed win: non-overlapping window-attention in all Transformer blocks. Swin: shifted window attention<ref type="bibr" target="#b54">[55]</ref>. Hwin: our Hybrid window attention. Pooling: our pooling attention, the K, V pooling stride is 2 (ViT-B) and 4 on the first stage of MViT, or pooling (stride=8). Accuracy, FLOPs and peak training memory are measured on IN-1K. For COCO, we report AP box , average training iterations per-second, average frames per-second for test and peak training memory, which are measured in Detectron2<ref type="bibr" target="#b82">[83]</ref> with 8 V100 GPUs under the same settings. Default is in gray.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Ablation of positional embeddings on MViT-S.</figDesc><table><row><cell>positional embeddings</cell><cell cols="4">IN-1K Acc AP box Train(iter/s) Test(im/s) Mem(G) COCO</cell></row><row><cell>(1) no pos.</cell><cell>83.3 49.2</cell><cell>3.1</cell><cell>10.3</cell><cell>5.0</cell></row><row><cell>(2) abs. pos.</cell><cell>83.5 49.3</cell><cell>3.1</cell><cell>10.1</cell><cell>5.0</cell></row><row><cell>(3) joint rel. pos.</cell><cell>83.6 49.9</cell><cell cols="2">0.7 ↓4.4× 3.4 ↓3×</cell><cell>15.3</cell></row><row><cell cols="2">(4) decomposed rel. pos. 83.6 49.9</cell><cell>2.7</cell><cell>9.4</cell><cell>5.2</cell></row><row><cell>(5) abs. + dec. rel. pos.</cell><cell>83.7 49.8</cell><cell>2.7</cell><cell>9.5</cell><cell>5.2</cell></row><row><cell>residual pooling</cell><cell cols="4">IN-1K Acc AP box Train(iter/s) Test(im/s) Mem(G) COCO</cell></row><row><cell>(1) w/o</cell><cell>83.3 48.5</cell><cell>3.0</cell><cell>10.0</cell><cell>4.7</cell></row><row><cell>(2) residual</cell><cell>83.6 49.3</cell><cell>2.9</cell><cell>9.8</cell><cell>4.7</cell></row><row><cell cols="2">(3) full Q pooling + residual 83.6 49.9</cell><cell>2.7</cell><cell>9.4</cell><cell>5.2</cell></row><row><cell>(4) full Q pooling</cell><cell>83.1 48.5</cell><cell>2.8</cell><cell>9.5</cell><cell>5.1</cell></row></table><note>Positional embeddings. Table6compares different positional embeddings. We observe that: (i) Comparing (2) to (1), absolute position only slightly improves over no pos.. This is because the pooling operators (instantiated by conv layers) already model positional information. (ii) Comparing<ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b3">4)</ref> and (1, 2), relative positions can bring performance gain by introducing shift-invariance priors to pooling attention. Finally, our decomposed relative position embedding train 3.9× faster than joint relative position on COCO.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Ablation of residual pooling connections on MViT-S.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell>studies the impor-</cell></row><row><cell>tance of our residual pooling connection. We see that simply</cell></row><row><cell>adding the residual path (2) can improves results on both</cell></row><row><cell>IN-1K (+0.3%) and COCO (+0.8 for AP box ) with negligible</cell></row><row><cell>cost. (3) Using residual pooling and also adding Q pooling</cell></row><row><cell>to all other layers (with stride=1) leads to a significant boost,</cell></row><row><cell>especially on COCO (+1.4 AP box ). This suggests both Q</cell></row><row><cell>pooling blocks and residual paths are necessary in MViT. (4)</cell></row><row><cell>just adding (without residual) more Q pooling layers with</cell></row><row><cell>stride=1 does not help and even decays (4) vs. (1).</cell></row></table><note>Runtime comparison. We conduct a runtime comparison for MViT and Swin<ref type="bibr" target="#b54">[55]</ref> in Table8. We see that MViT-S surpasses Swin-B on both IN-1K (+0.3%) and COCO (+1.4%) while having a higher throughput (341 im/s vs. 276 im/s) on</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 .</head><label>8</label><figDesc>Runtime comparison on IN-1K and COCO. We report accuracy and throughput on IN-1K, measured with a V100 GPU as in<ref type="bibr" target="#b54">[55]</ref>. COCO models are measured similarly and also for training throughput and memory. Batch size for all measures is identical.</figDesc><table><row><cell cols="3">IN-1K and also trains faster (2.7iter/s vs. 2.5iter/s) on COCO</cell></row><row><cell cols="3">with less memory cost (5.2G vs. 6.3G). MViT-B is slightly</cell></row><row><cell cols="3">slower but significantly more accurate (+1.1% on IN-1K and</cell></row><row><cell>+2.5AP box on COCO).</cell><cell></cell><cell></cell></row><row><cell cols="3">Single-scale vs. multi-scale for detection. Table 9 com-</cell></row><row><cell cols="3">pares the default multi-scale (FPN) detector with the single-</cell></row><row><cell cols="3">scale detector for ViT-B and MViT-S. As ViT produces</cell></row><row><cell cols="3">feature maps at a single scale in the backbone, we adopt a</cell></row><row><cell cols="3">simple scheme [50] to up-/downsample features to integrate</cell></row><row><cell cols="3">with FPN. For single-scale, we directly apply the detection</cell></row><row><cell cols="2">heads to the last Transformers block.</cell><cell></cell></row><row><cell cols="3">variant FPN AP box AP mask FLOPs (G)</cell></row><row><cell>ViT-B no</cell><cell>45.1 40.6</cell><cell>725</cell></row><row><cell cols="2">ViT-B yes 46.6 42.3</cell><cell>879</cell></row><row><cell>MViT-S no</cell><cell>47.0 41.4</cell><cell>276</cell></row><row><cell cols="2">MViT-S yes 49.9 45.1</cell><cell>326</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 .</head><label>9</label><figDesc>Single</figDesc><table /><note>-scale vs. Multi-scale (FPN) on COCO. ViT-B and MViT-S models are equipped with or w/o a feature pyramid network (FPN). Both FPN models outperforms their single-scale variant while while MViT achieves even larger gains.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 .</head><label>10</label><figDesc>Comparison with previous work on Kinetics-400. We report the inference cost with a single "view" (temporal clip with spatial crop) × the number of views (FLOPs×viewspace×viewtime). Magnitudes are Giga(10 9 ) for FLOPs and Mega (10 6 ) for Param.</figDesc><table><row><cell>model</cell><cell cols="3">pre-train top-1 top-5 FLOPs×views Param</cell></row><row><cell>SlowFast 16×8 +NL [24]</cell><cell>-</cell><cell cols="2">79.8 93.9 234×3×10 59.9</cell></row><row><cell>X3D-XL [23]</cell><cell>-</cell><cell cols="2">79.1 93.9 48.4×3×10 11.0</cell></row><row><cell>MoViNet-A6 [45]</cell><cell>-</cell><cell>81.5 95.3</cell><cell>386×1×1 31.4</cell></row><row><cell>MViTv1, 16×4 [22]</cell><cell>-</cell><cell>78.4 93.5</cell><cell>70.3×1×5 36.6</cell></row><row><cell>MViTv1, 32×3 [22]</cell><cell>-</cell><cell>80.2 94.4</cell><cell>170×1×5 36.6</cell></row><row><cell>MViT-S, 16×4</cell><cell>-</cell><cell>81.0 94.6</cell><cell>64×1×5 34.5</cell></row><row><cell>MViT-B, 32×3</cell><cell>-</cell><cell>82.9 95.7</cell><cell>225×1×5 51.2</cell></row><row><cell>ViT-B-VTN [59]</cell><cell></cell><cell cols="2">78.6 93.7 4218×1×1 114.0</cell></row><row><cell>ViT-B-TimeSformer [3]</cell><cell></cell><cell cols="2">80.7 94.7 2380×3×1 121.4</cell></row><row><cell>ViT-L-ViViT [1] Swin-L↑ 384 2 [56]</cell><cell>IN-21K</cell><cell cols="2">81.3 94.7 3992×3×4 310.8 84.9 96.7 2107×5×10 200.0</cell></row><row><cell>MViT-L↑ 312 2 , 40×3</cell><cell></cell><cell cols="2">86.1 97.0 2828×3×5 217.6</cell></row><row><cell>model</cell><cell cols="3">pretrain top-1 top-5 FLOPs×views Param</cell></row><row><cell>SlowFast 16×8 +NL [24]</cell><cell>-</cell><cell>81.8 95.1</cell><cell>234×3×10 59.9</cell></row><row><cell>X3D-XL [23]</cell><cell>-</cell><cell>81.9 95.5</cell><cell>48.4×3×10 11.0</cell></row><row><cell>MoViNet-A6 [45]</cell><cell>-</cell><cell>84.8 96.5</cell><cell>386×1×1 31.4</cell></row><row><cell>MViTv1-B-24, 32×3 [22]</cell><cell>-</cell><cell>84.1 96.5</cell><cell>236×1×5 52.9</cell></row><row><cell>MViT-B, 32×3</cell><cell>-</cell><cell>85.5 97.2</cell><cell>206×1×5 51.4</cell></row><row><cell>ViT-L-ViViT [1]</cell><cell></cell><cell>83.0 95.7</cell><cell>3992×3×4 310.8</cell></row><row><cell>Swin-B [56]</cell><cell></cell><cell>84.0 96.5</cell><cell>282×3×4 88.1</cell></row><row><cell>Swin-L↑ 384 2 [56]</cell><cell>IN-21K</cell><cell>86.1 97.3</cell><cell>2107×5×10 200.0</cell></row><row><cell>MViT-L↑ 312 2 , 32×3</cell><cell></cell><cell>87.2 97.6</cell><cell>2063×3×4 217.6</cell></row><row><cell>MViT-L↑ 312 2 , 40×3</cell><cell></cell><cell>87.5 97.8</cell><cell>2828×3×4 217.6</cell></row><row><cell>MViT-L↑ 352 2 , 40×3</cell><cell></cell><cell>87.9 97.9</cell><cell>3790×3×4 217.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 .</head><label>11</label><figDesc>Comparison with previous work on Kinetics-600.</figDesc><table><row><cell>model</cell><cell cols="3">pretrain top-1 top-5 FLOPs×views Param</cell></row><row><cell cols="3">SlowFast 16×8 +NL [24] K600 71.0 89.6</cell><cell>234×3×10 59.9</cell></row><row><cell>MoViNet-A6 [45]</cell><cell cols="2">N/A 72.3 N/A</cell><cell>386×1×1 31.4</cell></row><row><cell>MViT-B, 32×3</cell><cell>-</cell><cell>76.6 93.2</cell><cell>206×3×3 51.4</cell></row><row><cell cols="3">MViT-L↑ 312 2 , 40×3 IN-21K 79.4 94.9</cell><cell>2828×3×3 217.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10</head><label>10</label><figDesc></figDesc><table><row><cell>compares MViT to prior work, in-</cell></row><row><cell>cluding state-of-the-art CNNs and ViTs.</cell></row><row><cell>When training from scratch, our MViT-S &amp; B models</cell></row><row><cell>produce 81.0% &amp; 82.9% top-1 accuracy which is +2.6% &amp;</cell></row><row><cell>+2.7% higher than their MViTv1 [22] counterparts. These</cell></row><row><cell>gains stem solely from the improvements in  §4.1, as the</cell></row><row><cell>training recipe is identical.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 13 .</head><label>13</label><figDesc>Comparison with previous work on SSv2.</figDesc><table><row><cell>model</cell><cell>pretrain</cell><cell cols="4">top-1 top-5 FLOPs×views Param</cell></row><row><cell>TEA [49]</cell><cell>IN-1K</cell><cell cols="2">65.1 89.9</cell><cell>70×3×10</cell><cell>-</cell></row><row><cell>MoViNet-A3 [45]</cell><cell>N/A</cell><cell cols="2">64.1 88.8</cell><cell>24×1×1</cell><cell>5.3</cell></row><row><cell>ViT-B-TimeSformer [3]</cell><cell>IN-21K</cell><cell>62.5</cell><cell>-</cell><cell cols="2">1703×3×1 121.4</cell></row><row><cell>MViTv1-B-24, 32×3</cell><cell>K600</cell><cell cols="2">68.7 91.5</cell><cell cols="2">236.0×3×1 53.2</cell></row><row><cell>SlowFast R101, 8×8 [24]</cell><cell></cell><cell cols="2">63.1 87.6</cell><cell cols="2">106×3×1 53.3</cell></row><row><cell>MViTv1-B, 16×4</cell><cell></cell><cell cols="2">64.7 89.2</cell><cell cols="2">70.5×3×1 36.6</cell></row><row><cell>MViTv1-B, 64×3</cell><cell>K400</cell><cell cols="2">67.7 90.9</cell><cell cols="2">454×3×1 36.6</cell></row><row><cell>MViT-S, 16×4</cell><cell></cell><cell cols="2">68.2 91.4</cell><cell cols="2">64.5×3×1 34.4</cell></row><row><cell>MViT-B, 32×3</cell><cell></cell><cell cols="2">70.5 92.7</cell><cell cols="2">225×3×1 51.1</cell></row><row><cell>Swin-B [56]</cell><cell cols="3">IN21K + K400 69.6 92.7</cell><cell cols="2">321×3×1 88.8</cell></row><row><cell>MViT-B, 32×3</cell><cell cols="3">IN21K + K400 72.1 93.4</cell><cell cols="2">225×3×1 51.1</cell></row><row><cell>MViT-L↑ 312 2 , 40×3</cell><cell cols="3">IN21K + K400 73.3 94.1</cell><cell cols="2">2828×3×1 213.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>&amp; 7 and are in §A.4.</figDesc><table><row><cell>model</cell><cell cols="6">T×τ scratch IN1k IN21k FLOPs Param</cell></row><row><cell>MViT-S</cell><cell>16×4</cell><cell>81.2</cell><cell>82.2</cell><cell>82.6</cell><cell>64</cell><cell>34.5</cell></row><row><cell>MViT-B</cell><cell>32×3</cell><cell>82.9</cell><cell>83.3</cell><cell>84.3</cell><cell>225</cell><cell>51.2</cell></row><row><cell>MViT-L</cell><cell>40×3</cell><cell>81.4</cell><cell>83.4</cell><cell>84.5</cell><cell cols="2">1127 217.6</cell></row><row><cell cols="2">MViT-L↑ 312 2 40×3</cell><cell>81.8</cell><cell>84.4</cell><cell>85.7</cell><cell cols="2">2828 217.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>Effect of pre-training datasets for detection. In §6.2 of the main paper we observe that ImageNet pre-training can have very different effects for different model sizes for video classification. Here, we are interested in the impact of pretraining on the larger IN-21K vs. IN-1K for COCO object detection tasks. Table A.2 shows our ablation: The large-Table A.1. Comparison with previvous work on AVA v2.2. We adopt two test strategies: 1) center (single center crop): we resize the shorter spatial side to 224 pixels and takes a 224 2 center crop for inference. 2) full (full-resolution): we resize the shorter spatial side to 224 pixels and take the full image for inference. We report inference cost with the center testing strategy (i.e. 224 2 input). Magnitudes are Giga (10 9 ) for FLOPs and Mega (10 6 ) for Param.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">val mAP</cell><cell></cell></row><row><cell>model</cell><cell></cell><cell>pretrain</cell><cell cols="4">center full FLOPs Param</cell></row><row><cell cols="2">SlowFast, 4×16, R50 [24]</cell><cell></cell><cell>21.9</cell><cell>-</cell><cell>52.6</cell><cell>33.7</cell></row><row><cell cols="2">SlowFast, 8×8, R101 [24]</cell><cell></cell><cell>23.8</cell><cell>-</cell><cell>137.7</cell><cell>53.0</cell></row><row><cell>MViTv1-B, 16×4 [22] MViTv1-B, 64×3 [22]</cell><cell></cell><cell>K400</cell><cell>24.5 27.3</cell><cell>--</cell><cell>70.5 454.7</cell><cell>36.4 36.4</cell></row><row><cell>MViT-S, 16×4</cell><cell></cell><cell></cell><cell cols="3">26.8 27.6 64.5</cell><cell>34.3</cell></row><row><cell>MViT-B, 32×3</cell><cell></cell><cell></cell><cell cols="3">28.1 29.0 225.2</cell><cell>51.0</cell></row><row><cell cols="2">SlowFast, 8×8 R101+NL [24]</cell><cell></cell><cell>27.1</cell><cell>-</cell><cell>146.6</cell><cell>59.2</cell></row><row><cell cols="2">SlowFast, 16×8 R101+NL [24]</cell><cell></cell><cell>27.5</cell><cell>-</cell><cell>296.3</cell><cell>59.2</cell></row><row><cell>X3D-XL [23]</cell><cell></cell><cell></cell><cell>27.4</cell><cell>-</cell><cell>48.4</cell><cell>11.0</cell></row><row><cell cols="2">Object Transformer [81] ACAR 8×8, R101-NL [60]</cell><cell>K600</cell><cell>31.0 -</cell><cell cols="2">-31.4 N/A 243.8</cell><cell>86.2 N/A</cell></row><row><cell>MViTv1-B, 16×4 [22]</cell><cell></cell><cell></cell><cell>26.1</cell><cell>-</cell><cell>70.4</cell><cell>36.3</cell></row><row><cell cols="2">MViTv1-B-24, 32×3 [22]</cell><cell></cell><cell>28.7</cell><cell></cell><cell>236.0</cell><cell>52.9</cell></row><row><cell>MViT-B, 32×3</cell><cell></cell><cell></cell><cell cols="3">29.9 30.5 225.2</cell><cell>51.0</cell></row><row><cell cols="2">ACAR 8×8, R101-NL [60]</cell><cell>K700</cell><cell>-</cell><cell cols="2">33.3 N/A</cell><cell>N/A</cell></row><row><cell>MViT-B, 32×3</cell><cell></cell><cell>K700</cell><cell cols="3">31.3 32.3 225.2</cell><cell>51.0</cell></row><row><cell>MViT-L↑ 312 2 , 40×3</cell><cell></cell><cell cols="5">IN21K+K700 33.5 34.4 2828 213.0</cell></row><row><cell>variant</cell><cell cols="4">AP box IN-1k IN-21k IN-1k IN-21k AP mask</cell><cell></cell></row><row><cell cols="3">MViT-S 49.9 50.2</cell><cell cols="2">45.1 45.1</cell><cell></cell></row><row><cell cols="3">MViT-B 51.0 51.5</cell><cell cols="2">45.7 46.4</cell><cell></cell></row><row><cell cols="3">MViT-L 51.8 52.7</cell><cell cols="2">46.2 46.8</cell><cell></cell></row><row><cell cols="7">Table A.2. Effect of pre-training datasets for COCO. Detection</cell></row><row><cell cols="7">methods are initialized from IN-1K or IN-21K pre-trained weights.</cell></row><row><cell cols="7">scale IN-21K pre-training is more helpful for larger models,</cell></row><row><cell cols="7">e.g. MViT-B and MViT-L have +0.5 and +0.9 gains in AP box .</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table A .</head><label>A</label><figDesc>TableA.5 studies the effect of residual pooling connections on K400. We observe similar results as for image classification and object 3. Comparison to previous work on ImageNet-1K. Input images are 224×224 by default and ↑ denotes using different sizes. MViT is trained for 300 epochs without any external data or models. We report our ↑ 384 2 models tested using a center crop or a resized full crop of the original image, to compare to prior work.</figDesc><table><row><cell></cell><cell>Acc</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>model</cell><cell cols="4">center resize FLOPs (G) Param (M)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RegNetY-4GF [62] RegNetZ-4GF [16] EfficientNet-B4 ↑ 380 2 [72] DeiT-S [73] PVT-S [79] TNT-S [33] T2T-ViTt-14 [86]</cell><cell>80.0 83.1 82.9 79.8 79.8 81.5 81.7</cell><cell></cell><cell>4.0 4.0 4.2 4.6 3.8 5.2 6.1</cell><cell>21 28 19 22 25 24 22</cell><cell>(1) no pos. (2) abs. pos. (3) time-only rel. (4) space-only rel.</cell><cell cols="2">rel. pos. abs. pos. space time dec.</cell><cell cols="2">Top-1 Train Param (%) (clip/s) (M) 80.1 91.5 34.4 80.4 91.0 34.7 80.8 80.5 34.4 80.6 76.2 34.5</cell></row><row><cell>CvT-13 [82]</cell><cell>81.6</cell><cell></cell><cell>4.5</cell><cell>20</cell><cell cols="2">(5) dec. space rel. + time rel. dec.</cell><cell></cell><cell cols="2">81.0 66.6 34.5</cell></row><row><cell>Twins-S [12]</cell><cell>81.7</cell><cell></cell><cell>2.9</cell><cell>24</cell><cell cols="2">(6) joint space rel. + time rel. joint</cell><cell></cell><cell cols="2">81.1 33.6 37.1</cell></row><row><cell>ViL-S-RPB [90]</cell><cell>82.4</cell><cell></cell><cell>4.9</cell><cell>25</cell><cell>(7) joint space/time rel.</cell><cell>joint</cell><cell></cell><cell>-</cell><cell>8.4</cell><cell>73.7</cell></row><row><cell>PVTv2-V2 [78]</cell><cell>82.0</cell><cell></cell><cell>4.0</cell><cell>25</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CrossViTc-15 [9]</cell><cell>82.3</cell><cell></cell><cell>6.1</cell><cell>28</cell><cell cols="5">Table A.4. Ablation of positional embeddings on K400 with</cell></row><row><cell>XCiT-S12 [19]</cell><cell>82.0</cell><cell></cell><cell>4.8</cell><cell>26</cell><cell cols="5">MViT-S 16×4. Training throughput is measured by average clips</cell></row><row><cell>Swin-T [55] MViT-T RegNetY-8GF [62] EfficientNet-B5 ↑ 456 2 [72] PVT-M [79] T2T-ViTt-19 [86]</cell><cell>81.3 82.3 81.7 83.6 81.2 82.4</cell><cell></cell><cell>4.5 4.7 8.0 9.9 6.7 9.8</cell><cell>29 24 39 30 44 39</cell><cell cols="5">per-second with 8 V100 GPUs. Our (5) decomposed space/time rel. positional embeddings are accurate and significantly faster than other joint versions. Note that we do not finish the full train-ing for (7) joint space/time rel. as the training speed is too slow (∼8× slower than ours) and (6) joint space rel. already shows large</cell></row><row><cell>CvT-21 [82]</cell><cell>82.5</cell><cell></cell><cell>7.1</cell><cell>32</cell><cell cols="5">drawbacks (∼2× slower) of joint rel. positional embeddings.</cell></row><row><cell>Twins-B [12]</cell><cell>83.2</cell><cell></cell><cell>8.6</cell><cell>56</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ViL-M-RPB [90] PVTv2-V2-B3 [78]</cell><cell>83.5 83.2</cell><cell></cell><cell>8.7 6.9</cell><cell>40 45</cell><cell cols="5">detection (Table 7 of the main paper), that: both Q pooling</cell></row><row><cell>CrossViTc-18 [9]</cell><cell>82.8</cell><cell></cell><cell>9.5</cell><cell>44</cell><cell cols="5">blocks and residual paths are essential in our improved MViT</cell></row><row><cell>XCiT-S24 [19] Swin-S [55] CSWin-S [17] MViT-v1-B-16 [22]</cell><cell>82.6 83.0 83.6 83.0</cell><cell></cell><cell>9.1 8.7 6.9 7.8</cell><cell>48 50 35 37</cell><cell cols="5">and combining them together leads to +1.7% accuracy on K400 while using them separately only improves slightly (+0.4%).</cell></row><row><cell>MViT-S</cell><cell>83.6</cell><cell></cell><cell>7.0</cell><cell>35</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RegNetY-16GF [62]</cell><cell>82.9</cell><cell></cell><cell>15.9</cell><cell>84</cell><cell cols="2">residual pooling</cell><cell cols="2">Top-1 FLOPs</cell></row><row><cell>RegNetZ-16GF [16] EfficientNet-B6 ↑ 528 2 [72] NFNet-F0 ↑ 256 2 [5]</cell><cell>84.1 84.2 83.6</cell><cell></cell><cell>15.9 19 12.4</cell><cell>95 43 72</cell><cell cols="2">(1) w/o (2) full Q pooling (3) residual</cell><cell>79.3 79.7 79.7</cell><cell>64 65 64</cell></row><row><cell>DeiT-B [73] PVT-L [79]</cell><cell>81.8 81.7</cell><cell></cell><cell>17.6 9.8</cell><cell>87 61</cell><cell cols="3">(4) full Q pooling + residual 81.0</cell><cell>65</cell></row><row><cell>T2T-ViTt-21 [86] TNT-B [33] Twins-L [12]</cell><cell>82.6 82.9 83.7</cell><cell></cell><cell>15.0 14.1 15.1</cell><cell>64 66 99</cell><cell cols="5">Table A.5. Ablation of residual pooling connections on K400 with MViT-S 16×4 architecture.</cell></row><row><cell>ViL-B-RPB [90]</cell><cell>83.7</cell><cell></cell><cell>13.4</cell><cell>56</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PVTv2-V2-B5 [78]</cell><cell>83.8</cell><cell></cell><cell>11.8</cell><cell>82</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CaiT-S36 [75]</cell><cell>83.3</cell><cell></cell><cell>13.9</cell><cell>68</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>XCiT-M24 [19]</cell><cell>82.7</cell><cell></cell><cell>16.2</cell><cell>84</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Swin-B [55]</cell><cell>83.3</cell><cell></cell><cell>15.4</cell><cell>88</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CSWin-B [17]</cell><cell>84.2</cell><cell></cell><cell>15.0</cell><cell>78</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MViTv1-B-24 [22]</cell><cell>83.4</cell><cell></cell><cell>10.9</cell><cell>54</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MViT-B</cell><cell>84.4</cell><cell></cell><cell>10.2</cell><cell>52</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>EfficientNet-B7 ↑ 600 2 [72]</cell><cell>84.3</cell><cell></cell><cell>37.0</cell><cell>66</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NFNet-F1 ↑ 320 2 [5]</cell><cell>84.7</cell><cell></cell><cell>35.5</cell><cell>133</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DeiT-B ↑ 384 2 [73]</cell><cell>83.1</cell><cell></cell><cell>55.5</cell><cell>87</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>TNT-B ↑ 384 2 [33]</cell><cell>83.9</cell><cell></cell><cell>N/A</cell><cell>66</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CvT-32 ↑ 384 2 [82]</cell><cell></cell><cell>83.3</cell><cell>24.9</cell><cell>32</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CaiT-S36↑ 384 2 [75]</cell><cell></cell><cell>85.0</cell><cell>48</cell><cell>68</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Swin-B ↑ 384 2 [55]</cell><cell></cell><cell>84.2</cell><cell>47.0</cell><cell>88</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MViT-v1-B-24 ↑ 320 2 [22]</cell><cell>84.8</cell><cell></cell><cell>32.7</cell><cell>73</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MViT-B ↑ 384 2</cell><cell>85.2</cell><cell>85.6</cell><cell>36.7</cell><cell>52</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NFNet-F2 ↑ 352 2 [5]</cell><cell>85.1</cell><cell></cell><cell>62.6</cell><cell>194</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>XCiT-M24 [19]</cell><cell>82.9</cell><cell></cell><cell>36.1</cell><cell>189</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CoAtNet-3 [14]</cell><cell>84.5</cell><cell></cell><cell>34.7</cell><cell>168</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MViT-L</cell><cell>85.3</cell><cell></cell><cell>42.1</cell><cell>218</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NFNet-F4 ↑ 512 2 [5]</cell><cell>85.9</cell><cell></cell><cell>215.3</cell><cell>316</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CoAtNet-3 [14] ↑ 384 2</cell><cell></cell><cell>85.8</cell><cell>107.4</cell><cell>168</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MViT-L ↑ 384 2</cell><cell>86.0</cell><cell>86.3</cell><cell>140.2</cell><cell>218</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0">Note that Q and (K, V ) can reside in different scales due to potentially different pooling. p maps the index of all of them into a shared scale.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1">Note that no initialization is needed if using max-pooling variants.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Vivit: A video vision transformer</title>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Lučić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15691</idno>
		<imprint>
			<date type="published" when="2008">2021. 1, 2, 8</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Toward transformer-based object detection</title>
		<author>
			<persName><forename type="first">Josh</forename><surname>Beal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Kislyuk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09958</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Is space-time attention all you need for video understanding?</title>
		<author>
			<persName><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05095</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Soft-nms-improving object detection with one line of code</title>
		<author>
			<persName><forename type="first">Navaneeth</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
				<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">High-performance large-scale image recognition without normalization</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06171</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2005">2018. 2, 5</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">João</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06987</idno>
		<title level="m">A short note on the kinetics-700 human action dataset</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2007">2017. 2, 4, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Crossvit: Cross-attention multi-scale vision transformer for image classification</title>
		<author>
			<persName><forename type="first">Chun-Fu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Quanfu Fan, and Rameswar Panda</title>
				<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>Proc. ICCV, 2021</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hybrid task cascade for instance segmentation</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05049</idno>
		<title level="m">Drop an octave: Reducing spatial redundancy in convolutional neural networks with octave convolution</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Twins: Revisiting the design of spatial attention in vision transformers</title>
		<author>
			<persName><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Ekin D Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Coatnet: Marrying convolution and attention for all data sizes</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04803</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast and accurate model scaling</title>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR, 2021</title>
				<meeting>CVPR, 2021</meeting>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Cswin transformer: A general vision transformer backbone with cross-shaped windows</title>
		<author>
			<persName><forename type="first">Xiaoyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00652</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09681</idno>
		<title level="m">Cross-covariance image transformers</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Yanghao</forename><surname>Haoqi Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wan-Yen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><surname>Pyslowfast</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/slowfast" />
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">PyTorchVideo: A deep learning library for video understanding</title>
		<author>
			<persName><forename type="first">Tullie</forename><surname>Haoqi Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Murrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghao</forename><surname>Vasudev Alwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhila</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haichuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Feiszli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wan-Yen</forename><surname>Adcock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><surname>Feichtenhofer</surname></persName>
		</author>
		<ptr target="https://pytorchvideo.org/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
				<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multiscale vision transformers</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Haoqi Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karttikeya</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghao</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
				<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2009">2021. 1, 2, 3, 4, 5, 6, 7, 8, 9</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">X3D: Expanding architectures for efficient video recognition</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SlowFast networks for video recognition</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
				<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2009">2019. 2, 7, 8, 9</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Simple copy-paste is a strong data augmentation method for instance segmentation</title>
		<author>
			<persName><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekin</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Video action transformer network</title>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
				<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training ImageNet in 1 hour</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The &quot;Something Something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ingo</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">AVA: A video dataset of spatiotemporally localized atomic visual actions</title>
		<author>
			<persName><forename type="first">Chunhui</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susanna</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Transformer in transformer</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Resnest: Split-attention networks</title>
		<author>
			<persName><forename type="first">Zhang</forename><surname>Hang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">How to start training: The effect of initialization and architecture</title>
		<author>
			<persName><forename type="first">Boris</forename><surname>Hanin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Rolnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01719</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask R-CNN</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
				<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2011">2017. 1, 3, 5, 6, 11</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
				<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Augment your batch: Improving generalization through instance repetition</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niv</forename><surname>Giladi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8129" to="8138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
				<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Stm: Spatiotemporal and motion encoding for action recognition</title>
		<author>
			<persName><forename type="first">Boyuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengmeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihao</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2000">2000-2009, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Token labeling: Training a 85.5% top-1 accuracy vision transformer with 56m parameters on imagenet</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10858</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">MoViNets: Mobile video networks for efficient video recognition</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Kondratyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangzhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yandong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Ima-geNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Handwritten digit recognition with a back-propagation network</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donnie</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donnie</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Tea: Temporal excitation and aggregation for action recognition</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="909" to="918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Benchmarking detection transfer learning with vision transformers</title>
		<author>
			<persName><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11429</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Kaiming He, and Ross Girshick</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">VideoLSTM convolves, attends and flows for action recognition</title>
		<author>
			<persName><forename type="first">Zhenyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kirill</forename><surname>Gavrilyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihir</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="41" to="50" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Temporal shift module for efficient video understanding</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
				<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
				<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<imprint>
			<date type="published" when="2007">2021. 1, 2, 4, 5, 6, 7</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13230</idno>
		<title level="m">Video swin transformer</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">SGDR: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Fixing weight decay regularization in adam</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Neimark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omri</forename><surname>Bar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.00719</idno>
		<title level="m">Maya Zohar, and Dotan Asselmann. Video transformer network</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Actor-context-actor relation network for spatio-temporal action localization</title>
		<author>
			<persName><forename type="first">Junting</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><forename type="middle">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal representation with pseudo-3d residual networks</title>
		<author>
			<persName><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
				<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Designing network design spaces</title>
		<author>
			<persName><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2005">June 2020. 2, 5</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Selfattention with relative position representations</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02155</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Segmenter: Transformer for semantic segmentation</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Strudel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05633</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.00567</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Hugo Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2005">2020. 4, 5</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Hugo Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><surname>Massa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<title level="m">Alexandre Sablayrolles, and Hervé Jégou. DeiT: Data-efficient image transformers</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Going deeper with image transformers</title>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Hugo Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.17239</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Video classification with channel-separated convolutional networks</title>
		<author>
			<persName><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
				<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<author>
			<persName><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13797</idno>
		<title level="m">Pvtv2: Improved baselines with pyramid vision transformer</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
				<imprint>
			<date type="published" when="2006">2021. 1, 2, 6</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Long-term feature banks for detailed video understanding</title>
		<author>
			<persName><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Towards long-form video understanding</title>
		<author>
			<persName><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Cvt: Introducing convolutions to vision transformers</title>
		<author>
			<persName><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15808</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><surname>Detectron2</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Rethinking spatiotemporal feature learning for video understanding</title>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.04851</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi-Hang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><forename type="middle">E H</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
				<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Volo: Vision outlooker for visual recognition</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seong</forename><surname>Joon Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjoon</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
				<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Mixup: Beyond empirical risk minimization</title>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Multi-scale vision longformer: A new vision transformer for high-resolution image encoding</title>
		<author>
			<persName><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
				<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2005">2021. 2, 5</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><surname>Philip Hs Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13001" to="13008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Objects as points. arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
