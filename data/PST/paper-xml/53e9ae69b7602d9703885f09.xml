<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">Ningbo University</orgName>
								<address>
									<postCode>315211</postCode>
									<settlement>Ningbo</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">J</forename><surname>Cao</surname></persName>
							<email>jdcao@seu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<postCode>210096</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">41E0EB00618C3DF4D481FAB9AF92FF83</idno>
					<idno type="DOI">10.1109/TNN.2010.2054108</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Introduction</head><p>A S IS WELL KNOWN, Cohen-Grossberg neural net- works (CGNNs) were originally proposed and investigated by Cohen and Grossberg in 1983 <ref type="bibr" target="#b3">[4]</ref>. Since then, the CGNNs model has been widely studied in the literature (see <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b37">[38]</ref>). Some other models, such as Hopfield neural networks, recurrent neural networks, cellular neural networks, and bidirectional associative memory neural networks, are special cases of this model. One of the most important problems is to discuss the stability of the equilibrium point of CGNNs. Therefore, it is important and necessary to investigate the stability of this class of CGNNs.</p><p>Recently, a class of neural networks with Markovian jump parameters has received a great deal of research attention because this class of neural networks has been recognized to be the best system to model the phenomenon of information latching, and the abrupt phenomena such as random failures or repairs of the components, sudden environmental changes, changing subsystem interconnections, etc. (see <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref> and references therein). In fact, this class of neural networks with Markovian jump parameters is a hybrid system with a state vector that has two components x(t) and r(t). Regarding the background of a hybrid system, we refer to the recent book <ref type="bibr" target="#b17">[18]</ref>. Generally speaking, the first component x(t) is referred to as the state, and the second component r(t) is a continuous-time Markov chain with a finite state space S = {1, 2, . . . , N}, which is usually regarded as the mode. In its operation, this class of neural networks will switch from one mode to another in a random way, which is determined by a continuous-time Markov chain r(t). It should be pointed out that a Markovian jump neural network will become a deterministic neural network when r(t) only takes a unique value, i.e., N = 1. That is to say, a deterministic neural network is a special case of a Markovian jump neural network. As a result, a Markovian jump neural network is more general and complex than a deterministic neural network. Thus, it is interesting and challenging to study the class of neural networks with Markovian jump parameters.</p><p>In practice, noise disturbances, time delays, unknown parameters, impulsive perturbations are probably four of the main sources for causing an instability and poor performances in neural networks. Actually, the synaptic transmission in real neural networks can be viewed as a noisy process introduced by random fluctuations from the release of neurotransmitters and other probabilistic causes. Moreover, a neural network can be stabilized or destabilized by certain stochastic inputs. On the other hand, time delays are often encountered in real neural networks because of the finite switching speed of amplifiers, and the existence of time delays may cause an oscillation or instability in neural networks, which is harmful to the applications of neural networks. Also, neural networks' parameters cannot be exactly known in prior in many applications. In addition, impulsive perturbations are likely to emerge in real neural networks due to the reason that the states of neural networks are changed abruptly at certain moments of time in the fields such as medicine and biology, economics, electronics, and telecommunications. Moreover, neural networks are often subject to impulsive perturbations that in turn affect dynamical behaviors of the system. Therefore, noise disturbances, time delays, unknown parameters, impulsive perturbations should be taken into account when studying the stability of neural networks.</p><p>In recent years, a large number of interesting results have been reported on the stability of Markovian jump neural networks with/without noise disturbances, time delays, unknown parameters and/or impulsive perturbations (see <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref> and the references therein). We now sum up these results as follows. Most references such as <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b39">[40]</ref>, and <ref type="bibr" target="#b40">[41]</ref> discussed Markovian jump Hopfield neural networks or Markovian jump recurrent neural networks, and only a few authors investigated Markovian jump CGNNs. In <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b16">[17]</ref>, and <ref type="bibr" target="#b32">[33]</ref>, noise disturbances, unknown parameters and impulsive perturbations were not considered and time delays proposed in <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b16">[17]</ref>, and <ref type="bibr" target="#b32">[33]</ref> were only constant delays or time-varying delays. The authors in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b37">[38]</ref>, and <ref type="bibr" target="#b40">[41]</ref> studied the stability but ignored the effects of unknown parameters and impulsive perturbations. In <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b14">[15]</ref>, and <ref type="bibr" target="#b21">[22]</ref>, only the asymptotical stability in the mean square was discussed and continuously distributed delays were not introduced, whereas impulsive perturbations were ignored in <ref type="bibr" target="#b14">[15]</ref>. In <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b14">[15]</ref>, and <ref type="bibr" target="#b39">[40]</ref>, the authors ignored impulsive perturbations, whereas time delays in <ref type="bibr" target="#b7">[8]</ref> were only time-varying delays. Therefore, the study of robust exponential stability of Markovian jump CGNNs with noise disturbances, continuously distributed delays, unknown parameters and impulsive perturbations has not been fully investigated. In fact, it has been pointed out in <ref type="bibr" target="#b40">[41]</ref> that investigating the exponential stability for a class of Markovian jump stochastic neural networks with continuously distributed delays is a challenging open issue.</p><p>Motivated by the above discussion, in this paper we study the problem of exponential stability for a class of Markovian jump impulsive stochastic CGNNs with mixed time delays and known or unknown parameters. The mixed time delays under consideration comprise both time-varying delays and continuously distributed delays. By constructing a novel Lyapunov-Krasovskii functional, and using some new approaches and techniques, several novel sufficient conditions are obtained to ensure the exponential stability of the trivial solution in the mean square. The results obtained in this paper generalize and improve some known results since many factors, such as noise disturbances, impulsive perturbations, unknown parameters, Markovian jump parameters, and continuously distributed delays are considered in this paper. It should be mentioned that the results and arguments in this paper give an answer to the open problem stated in <ref type="bibr" target="#b40">[41]</ref>, i.e., investigate the exponential stability for a class of Markovian jump stochastic neural networks with continuously distributed delays. Finally, two numerical examples and their simulations are given to show the effectiveness of the theoretical results.</p><p>The rest of this paper is organized as follows. In Section II, we introduce a new class of CGNNs, and give some necessary assumptions. By constructing a new Lyapunov-Krasovskii functional, and using some new approaches and techniques, we investigate the exponential stability for the considered model with known parameters in Section III, and with unknown parameters in Section IV, respectively. In Section V, two numerical examples and their simulations are given to show the effectiveness of the obtained results. Finally, in Section VI, the paper is concluded with some general remarks.</p><p>Notation: Throughout this paper, the following notations will be used. R n and R n×m denote the m-dimensional Euclidean space and the set of all n × m real matrices, respectively. The superscript "T" denotes the transpose of a matrix or vector, and the symbol " " denotes the symmetric term of the matrix. Trace (•) denotes the trace of the corresponding matrix and I denotes the identity matrix with compatible dimensions. For any matrix A, λ max (A) (respectively, λ min (A)) denotes the largest (respectively, smallest) eigenvalue of A. For square matrices M 1 and M 2 , the notation</p><formula xml:id="formula_0">M 1 &gt; (&lt;) M 2 denotes M 1 -M 2 is positive-definite (negative-definite) matrix. Let w(t) = (w 1 (t), • • • , w m (t))</formula><p>T be an m-dimensional Brownian motion defined on a complete probability space ( , F, P) with a natural filtration {F t } t≥0 . Also, let PC((-∞, 0]; R n ) denote the family of piecewise right continuous function φ from (-∞, 0] to R n with the uniform norm φ = sup θ≤0 |φ(θ)|.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Denote by</head><formula xml:id="formula_1">L 2 F t ((-∞, 0]; R n ) the family of all F t measurable, PC((-∞, 0]; R n )-valued stochastic variables ξ = {ξ(θ) : θ ≤ 0} such that 0 -∞ E|ξ(s)| 2 ds &lt; ∞,</formula><p>where E[•] stands for the correspondent expectation operator with respect to the given probability measure P.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. Model, Assumption, and Definition</head><p>In this paper, we consider a class of Markovian jump stochastic CGNNs with both impulsive perturbations and mixed time delays, which is described by the following integro-differential equation:</p><formula xml:id="formula_2">⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ dx(t) = -α(x(t), r(t))[β(x(t), r(t)) -A(r(t))f (x(t)) -B(r(t))g(x(t -τ(t))) -C(r(t)) t -∞ R(t -s)h(x(s))ds]dt+ σ(x(t), x(t -τ(t)), t -∞ R(t -s) h(x(s))ds, r(t))dw(t), t = t k x(t k ) = D k (r(t))x(t - k ), t = t k</formula><p>(1) for t &gt; 0 and k = 1, 2, . . . , where x(t) = [x 1 (t), x 2 (t), . . . , x n (t)] T is the state vector associated with the n neurons. The first part is the continuous part of (1), which describes the continuous evolution process of the neural network, where α(x(t), r(t)) = diag(α 1 (x 1 (t), r(t)), α 2 (x 2 (t), r(t)), . . . , α n (x n (t), r(t))) re-presents an amplification function, and β(x(t), r(t)) = [β 1 (x 1 (t), r(t)), β 2 (x 2 (t), r(t)), . . . , β n (x n (t), r(t))] T is the behaved function. The matrices A(r(t)) = (a ij (r(t))) n×n , B(r(t)) = (b ij (r(t))) n×n and C(r(t)) = (c ij (r(t))) n×n are the connection weight matrix, the time-varying delay connection weight matrix, and the distributed delay connection weight matrix, respectively.</p><p>f</p><formula xml:id="formula_3">(x(t)) = [f 1 (x 1 (t)), f 2 (x 2 (t)), . . . , f n (x n (t))] T , g(x(t)) = [g 1 (x 1 (t)), g 2 (x 2 (t)), . . . , g n (x n (t))] T and h(x(t)) = [h 1 (x 1 (t)), h 2 (x 2 (t)), . . . , h n (x n (t))] T are the neuron activation functions. The noise perturbation σ : R n ×R n ×R n ×S → R n×m is a Borel measurable function. τ(t) is the time-varying delay, R(t -s) = (R ij (t -s)) n×n and the delay kernel R ij (•) is a real value non-negative continuous function defined on [0, ∞) and such that ∞ 0 R ij (θ)dθ = 1 for i, j = 1, 2, . . . , n. {r(t), t ≥ 0}</formula><p>is a right-continuous Markov chain on a complete probability space ( , F, P) taking values in a finite state space S = {1, 2, . . . , N} with generator Q = (q ij ) N×N given by</p><formula xml:id="formula_4">P{r(t + t) = j|r(t) = i} = q ij t + o( t) ifi = j 1 + q ii t + o( t) ifi = j</formula><p>where t &gt; 0 and lim t→0 o( t) t = 0. Here, q ij ≥ 0 is the transition rate from i to</p><formula xml:id="formula_5">j if i = j while q ii = -j =i q ij .</formula><p>The second part is the discrete part of (1), which describes that the evolution process experiences abrupt changes of a state at the moment of time t k , where</p><formula xml:id="formula_6">x(t k ) = D k (r(t))x(t - k )</formula><p>is the impulse at the moment t k , and D k (r(t)) is the impulse gain matrix at the moment of time</p><formula xml:id="formula_7">t k . The discrete set {t k } satisfies 0 = t 0 &lt; t 1 &lt; • • • &lt; t k &lt; • • • , lim k→∞ t k = ∞. x(t - k</formula><p>) and x(t + k ) denote the left-hand and right-hand limits at t k , respectively. As in <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b21">[22]</ref>, we always assume that x(t + k ) = x(t k ). In the sequel, for simplicity, when r(t) = i, the matrices</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C(r(t)), A(r(t)), B(r(t)) and D k (r(t)) will be written as C</head><formula xml:id="formula_8">i , A i , B i and D i k , respectively. Also, let z 1 (t) = x(t -τ(t)), z 2 (t) = t -∞ R(t -s)h(x(s))ds.</formula><p>Therefore, (1) can be rewritten as the following:</p><formula xml:id="formula_9">⎧ ⎨ ⎩ dx(t) = -α(x(t), i)[β(x(t), i) -A i f (x(t)) -B i g(z 1 (t)) -C i z 2 (t)]dt + σ(x(t), z 1 (t), z 2 (t), i)dw(t), t = t k , x(t k ) = D i k x(t - k ), t = t k (2) for all r(t) = i, i ∈ S.</formula><p>Remark 1: The model (1) [or (2) equivalently] is new and more general than those investigated in <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b39">[40]</ref>, and <ref type="bibr" target="#b40">[41]</ref>. For instance, let</p><formula xml:id="formula_10">f = g = h, R ij (s) = 0(i = j) and R ii (s) = 0 s ≥ τ 1 s &lt; τ , τ &gt; 0, i = 1, 2, . . . , n</formula><p>then (1) is the same as in <ref type="bibr" target="#b21">[22]</ref>; If we do not consider continuously distributed delays, i.e, let C(r(t)) ≡ 0, then (1) is the same as in <ref type="bibr" target="#b4">[5]</ref>. Similarly, we can illustrate the systems in <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b39">[40]</ref>, and <ref type="bibr" target="#b40">[41]</ref> are a special case of (1) [or (2) equivalently] since noise disturbances, impulsive perturbations and continuously distributed delays are considered in <ref type="bibr" target="#b0">(1)</ref> [or (2) equivalently].</p><p>Throughout this paper, we make the following assumptions. Assumption 1: There exist positive constants</p><formula xml:id="formula_11">α 0 ij , α 1 ij (i = 1, 2, • • • , N, j = 1, 2, • • • , n) such that 0 &lt; α 0 ij ≤ α j (x j (t), i) ≤ α 1 ij for all x j (t) ∈ R, r(t) = i, i ∈ S and j = 1, 2, • • • , n.</formula><p>Assumption 2: There exist positive constants</p><formula xml:id="formula_12">µ ij (i = 1, 2, • • • , N, j = 1, 2, • • • , n) such that x j (t)β j (x j (t), i) ≥ µ ij x 2 j (t) for all x j (t) ∈ R, r(t) = i, i ∈ S and j = 1, 2, • • • , n. Assumption 3: There exist diagonal matrices U - i = diag (u - i1 , u - i2 , • • • , u - in ), U + i = diag(u + i1 , u + i2 , • • • , u + in ), i = 1, 2, 3 satisfying u - ij ≤ l i j (x 1 ) -l i j (x 2 ) x 1 -x 2 ≤ u + ij for all x 1 , x 2 ∈ R, x 2 = x 2 , j = 1, 2, • • • , n, where l 1 = f, l 2 = g, l 3 = h.</formula><p>Assumption 4: There exist positive constants τ and δ such that 0 ≤ τ(t) ≤ τ and τ(t) ≤ δ.</p><p>Assumption 5: There exist positive definite matrices 1i , 2i and 3i (i ∈ S) such that</p><formula xml:id="formula_13">trace[σ T (t)σ(t)] ≤ x T (t) 1i x(t) + z T 1 (t) 2i z 1 (t) + z T 2 (t) 3i z 2 (t)</formula><p>where</p><formula xml:id="formula_14">z 1 (t) = x(t -τ(t)), z 2 (t) = t -∞ R(t -s)h(x(s))ds, σ(t) = σ(x(t), z 1 (t), z 2 (t), i). Assumption 6: f (0) = g(0) = h(0) ≡ 0 and σ(0, 0, 0, r(t)) ≡ 0.</formula><p>Remark 2: Assumption 2 is weaker than the usual condition</p><formula xml:id="formula_15">β j (u, i) -β j (v, i) u -v ≥ µ ij or β j (u) -β j (v) u -v ≥ µ j .</formula><p>where u, v ∈ R. For example, we take β j (u, i) = ue |u| . Obviously, Assumption 2 holds, but the usual condition fails. Remark 3: The constants u - ij , u + ij in Assumption 3 are allowed to be positive, negative or zero, whereas the constant u - ij in <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b27">[28]</ref>, and <ref type="bibr" target="#b32">[33]</ref> is restricted to be zero and the constants u - ij , u + ij in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b37">[38]</ref>, and <ref type="bibr" target="#b39">[40]</ref> are required to satisfy u - ij ≡ -u + ij . Therefore, Assumption 3 of this paper is weaker than those given in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b37">[38]</ref>, and <ref type="bibr" target="#b39">[40]</ref>.</p><p>Remark 4: Assumption 4 is weaker than those given in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b39">[40]</ref>, and <ref type="bibr" target="#b40">[41]</ref> since the constant δ in Assumption 4 is allowed to take any value. Nevertheless, the constant δ in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b39">[40]</ref>, and <ref type="bibr" target="#b40">[41]</ref> is required to be zero or smaller than 1.</p><p>Remark 5: Assumption 5 is weaker than those given in <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b39">[40]</ref>, and <ref type="bibr" target="#b40">[41]</ref> since it depends on continuously distributed delays. Now, let x(t; ξ) denote the state trajectory from the initial data</p><formula xml:id="formula_16">x(θ) = ξ(θ) on -∞ &lt; θ ≤ 0 in L 2 F t ((-∞, 0]; R n ).</formula><p>It is obvious that under Assumption 6, (1) [or (2) equivalently] admits a trivial solution or zero solution x(t; 0) ≡ 0 corresponding to the initial data ξ = 0. For simplicity, we write x(t; ξ) = x(t).</p><p>Next, we present some concepts and technical preliminaries, which are important to develop our theories and results.</p><p>Definition 1: The trivial solution of (1) [or (2) equivalently] is said to be exponentially stable in the mean square if for every ξ ∈ L 2 F 0 ((-∞, 0]; R n ), there exist scalars α &gt; 0 and β &gt; 0 such that the following inequality holds:</p><formula xml:id="formula_17">E|x(t; ξ)| 2 ≤ αe -βt sup θ≤0 E|ξ(θ)| 2 .</formula><p>Definition 2 (Yang <ref type="bibr" target="#b29">[30]</ref>): <ref type="bibr">(Boyd et al. [3]</ref>): For any real matrices X and Y , the following matrix inequality holds:</p><formula xml:id="formula_18">The function V : [t 0 , ∞)×R n → R + belongs to class 0 if: 1) the function V is continuous on each of the sets [t k-1 , t k ) × R n and for all t ≥ t 0 , V (t, 0) ≡ 0; 2) V (t, x) is locally Lipschitzian in x ∈ R n ; 3) for each k = 1, 2, . . . , there exist finite limits lim (t,z)→(t - k ,x) V (t - k , x) and lim (t,z)→(t + k ,x) V (t + k , x) with V (t + k , x) = V (t k , x) satisfied. Lemma 1</formula><formula xml:id="formula_19">X T Y + Y T X ≤ X T X + Y T Y. Lemma 2 (Schur Complement): Given one positive definite matrix G 2 &gt; 0 and constant matrices G 1 , G 3 , where G 1 = G T 1 , then G 1 + G T 3 G -1 2 G 3 &lt; 0 if and only if G 1 G T 3 G 3 -G 2 &lt; 0 or -G 2 G 3 G T 3 G 1 &lt; 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. Exponential Stability of Neural Networks</head><p>With Known Parameters In this section, the exponential stability in the mean square of the trivial solution for (2) is investigated under Assumptions 1-6. Define</p><formula xml:id="formula_20">ϕ(δ) = e -ετ , if 0 &lt; δ ≤ 1 1, if δ &gt; 1.</formula><p>Theorem 1: Under Assumptions 1-6, the trivial solution of (2) [or (1) equivalently] is exponentially stable, if there exist positive scalars ε, λ i (i ∈ S), positive diagonal matrices L = diag{l 1 , l 2 , . . . , l n }, F, M 1 , M 2 , P i (i ∈ S), and any matrices V 1 , V 2 , V 3 with appropriate dimensions such that the following linear matrix inequalities (LMIs) hold:</p><formula xml:id="formula_21">P i ≤ λ i I (<label>3</label></formula><formula xml:id="formula_22">)</formula><formula xml:id="formula_23">D T i k P j D i k -P i ≤ 0, [herer(t k ) = j] (4) i X 1i -1 3(α 1 i ) 2 I &lt; 0 (5)</formula><p>where </p><formula xml:id="formula_24">X 1i = [P i 0 0 0 0 0 0] T i = ⎛ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝</formula><formula xml:id="formula_25">⎞ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎠ 11 = εP i -2α 0 i P i i + λ i 1i + U 1 M 1 U 1 + τU 2 FU 2 N j=1 q ij P j + U 3 trace(L)U 3 + V T 1 + V 1 , α 0 i = min 1≤j≤n α 0 ij 12 = -V 1 + V T 2 , 16 = -V 1 + V T 3 , 22 = λ i 2i +U 2 M 2 U 2 -(V T 2 + V 2 ), 26 = V 2 -V T 3 33 = -M 1 + A T i A i 44 = -M 2 -(1 -δ)ϕ(δ)F + B T i B i 55 = -trace(L) + C T i C i + λ i 3i 66 = -(V T 3 + V 3 ), i = diag(µ i1 , µ i2 , . . . , µ in ) α 1 i = max 1≤j≤n α 1 ij , U j = diag(u j1 , u j2 , . . . , u jn ) u jl = max{|u - jl |, |u + jl |} (j = 1, 2, 3, l = 1, 2, . . . , n). Proof: Let C 2 1 (R + × R n × S; R + ) denote the family of all nonnegative functions V (t, x, i) ∈ 0 on R + × R n × S which are continuously twice differentiable in x and differentiable in t. If V ∈ C 2 1 (R + × R n × S; R n ),</formula><p>then along the trajectory of (2) we define an operator</p><formula xml:id="formula_26">LV from R + × R n × S to R by LV (t, x(t), i) = V t (t, x(t), i) + V x (t, x(t), i){-α(x(t), i) [β(x(t), i) -A i f (x(t)) -B i g(z 1 (t)) -z 2 (t)]} + 1 2 trace[σ T (t)V xx (t, x(t), i)σ(t)] + N j=1 q ij V (t, x(t), j), t = t k<label>(6)</label></formula><p>where</p><formula xml:id="formula_27">V t (t, x(t), i) = ∂V (t, x(t), i) ∂t V x (t, x(t), i) = ∂V (t, x(t), i) ∂x 1 , . . . , ∂V (t, x(t), i) ∂x n V xx (t, x(t), i) = ∂ 2 V (t, x(t), i) ∂x j ∂x k n×n .</formula><p>Let us consider the following Lyapulov-Krasovskii functional:</p><formula xml:id="formula_28">V (t, x(t), i) = e εt x T (t)P i x(t) + t t-τ(t)</formula><p>e εs g T (x(s))Fg(x(s))ds</p><formula xml:id="formula_29">+ n m=1 n j=1 l m ∞ 0 R mj (θ) t t-θ e ε(s+θ) h 2 j (x j (s))dsdθ. For t ∈ [t k-1 , t k ), combining<label>(2)</label></formula><p>and ( <ref type="formula" target="#formula_26">6</ref>) we have</p><formula xml:id="formula_30">LV (t, x(t), i) = εe εt x T (t)P i x(t) -2e εt x T (t)P i α(x(t), i)[β(x(t), i) -A i f (x(t)) -B i g(z 1 (t)) -C i z 2 (t)] + e εt N j=1</formula><p>q ij x T (t)P j x(t)</p><formula xml:id="formula_31">+e εt trace[σ T (t)P i σ(t)] + e εt g T (x(t))Fg(x(t)) -(1 -τ(t))e ε(t-τ(t)) g T (z 1 (t))Fg(z 1 (t)) + n m=1 n j=1 l m ∞ 0 R mj (θ)e ε(t+θ) h 2 j (x j (t))dθ - n m=1 n j=1 l m ∞ 0 R mj (θ)e εt h 2 j (x j (t -θ))dθ ≤ εe εt x T (t)P i x(t) -2e εt x T (t)P i α(x(t), i)β(x(t), i) +2e εt x T (t)P i α(x(t), i)A i f (x(t)) + 2e εt x T (t)P i α(x(t), i)B i g(z 1 (t)) + 2e εt x T (t)P i α(x(t), i)C i z 2 (t) +e εt N j=1</formula><p>q ij x T (t)P j x(t) + e εt trace[σ T (t)P i σ(t)]</p><formula xml:id="formula_32">+e εt g T (x(t))Fg(x(t)) -e εt (1 -δ)ϕ(δ)g T (z 1 (t))Fg (z 1 (t)) + e εt h T (x(t))trace(L)h(x(t)) -e εt n m=1 n j=1 l m ∞ 0 R mj (θ)dθ ∞ 0 R mj (θ)h 2 j (x j (t -θ))dθ. (<label>7</label></formula><formula xml:id="formula_33">)</formula><p>By Assumptions 1 and 2 and Lemma 1, we get</p><formula xml:id="formula_34">-2e εt x T (t)P i α(x(t), i)β(x(t), i) = -2e εt n j=1</formula><p>x j (t)p ij α j (x j (t), i)β j (x j (t), i)</p><formula xml:id="formula_35">≤ -2α 0 i e εt n j=1 p ij µ ij x 2 j (t) = -2α 0 i e εt x T (t)P i i x(t) (8) 2e εt x T (t)P i α(x(t), i)A i f (x(t)) ≤ e εt [x T (t)P i α(x(t), i)α(x(t), i)P i x(t) +f T (x(t))A T i A i f (x(t))] = e εt [(α 1 i ) 2 x T (t)P 2 i x(t) +f T (x(t))A T i A i f (x(t))].<label>(9)</label></formula><p>As in the proof of (9), we have</p><formula xml:id="formula_36">2e εt x T (t)P i α(x(t), i)B i g(z 1 (t)) ≤ e εt [(α 1 i ) 2 x T (t)P 2 i x(t) +g T (z 1 (t))B T i B i g(z 1 (t))]<label>(10)</label></formula><p>2e εt x T (t</p><formula xml:id="formula_37">)P i α(x(t), i)z 2 (t) ≤ e εt [(α 1 i ) 2 x T (t)P 2 i x(t) + z T 2 (t)C T i C i z 2 (t)].<label>(11)</label></formula><p>From Assumption 5 and the condition (3), it follows that</p><formula xml:id="formula_38">trace[σ T (t)P i σ(t)] ≤ λ max (P i )trace[σ T (t)σ(t)] ≤ λ i trace[σ T (t)σ(t)] ≤ λ i x T (t) 1i x(t) + λ i z T 1 (t) 2i z 1 (t) +λ i z T 2 (t) 3i z 2 (t). (<label>12</label></formula><formula xml:id="formula_39">)</formula><p>Using the well-known Cauchy-Schwarz inequality, we have</p><formula xml:id="formula_40">n m=1 n j=1 l m ∞ 0 R mj (θ)dθ ∞ 0 R mj (θ)h 2 j (x j (t -θ))dθ ≥ n m=1 n j=1 l m ∞ 0 R mj (θ)h j (x j (t -θ))dθ 2 = ∞ 0 R(θ)h(x(t -θ))dθ T n m=1 l m ∞ 0 R(θ)h (x(t -θ))dθ = t -∞ R(t -s)h(x(s))ds T trace(L) t -∞ R(t -s)h(x(s))ds = z T 2 (t)trace(L)z 2 (t). (<label>13</label></formula><formula xml:id="formula_41">)</formula><p>By Assumption 3, we obtain</p><formula xml:id="formula_42">f T (x(t))M 1 f (x(t)) ≤ x T (t)U 1 M 1 U 1 x(t) (14) g T (x(t))Fg(x(t)) ≤ x T (t)U 2 FU 2 x(t) (15) h T (x(t))trace(L)h(x(t)) ≤ x T (t)U 3 trace(L)U 3 x(t) (16) g T (z 1 (t))M 2 g(z 1 (t)) ≤ z T 1 (t)U 2 M 2 U 2 z 1 (t). (<label>17</label></formula><formula xml:id="formula_43">)</formula><p>Take</p><formula xml:id="formula_44">y(t) = -α(x(t), i)[β(x(t), i) -A i f (x(t)) -B i g(z 1 (t)) -C i z 2 (t)]</formula><p>then it follows from (2) that</p><formula xml:id="formula_45">dx(t) = y(t)dt + σ(t)dw(t).</formula><p>Thus, we easily get</p><formula xml:id="formula_46">x(t) -x(t -τ(t)) - t t-τ(t) y(s)ds - t t-τ(t) σ(s)dw(s) = 0.</formula><p>For any matrices V 1 , V 2 , V 3 with appropriate dimensions, we obtain</p><formula xml:id="formula_47">e εt 2x T (t)V 1 + 2x T (t -τ(t))V 2 +2( t t-τ(t) y(s)ds) T V 3 x(t) -x(t -τ(t)) - t t-τ(t) y(s)ds - t t-τ(t) σ(s)dw(s) = 0. (<label>18</label></formula><formula xml:id="formula_48">)</formula><p>Noting Edw(t) = 0 and substituting ( <ref type="formula">8</ref>)-( <ref type="formula" target="#formula_47">18</ref>) into <ref type="bibr" target="#b6">(7)</ref>, it is easy to derive that for every t</p><formula xml:id="formula_49">∈ [t k-1 , t k ) ELV (t, x(t), i) ≤ e εt E[η T (t) i η(t) +3(α 1 i ) 2 x T (t)P 2 i x(t)] (<label>19</label></formula><formula xml:id="formula_50">)</formula><p>where </p><formula xml:id="formula_51">η T (t) = [x T (t) z T 1 (t) f T (x(t)) g T (z 1 (t)) z T 2 (t) z T 3 (t)] z 3 (t) = t t-</formula><formula xml:id="formula_52">EV (t, x(t), i) ≤ EV (t , x(t ), r(t )). (<label>21</label></formula><formula xml:id="formula_53">)</formula><p>On the other hand, for t = t k , by the condition (4) we have</p><formula xml:id="formula_54">V (t k , x(t k ), j) -V (t - k , x(t - k ), i) = e εt k x T (t k )P j x(t k ) -e εt - k x T (t - k )P i x(t - k ) + 0 -τ dθ t k t k +θ</formula><p>e εs g T (x(s))Fg(x(s))ds</p><formula xml:id="formula_55">- 0 -τ dθ t - k t - k +θ</formula><p>e εs g T (x(s))Fg(x(s))ds</p><formula xml:id="formula_56">+ n q=1 n m=1 l q ∞ 0 R qm (θ) t k t k -θ e ε(s+θ) h 2 m (x m (s))dsdθ - n q=1 n m=1 l q ∞ 0 R qm (θ) t - k t - k -θ e ε(s+θ) h 2 m (x m (s))dsdθ = e εt k x T (t k )P j x(t k ) -e εt - k x T (t - k )P i x(t - k ) = e εt k x T (t - k )D T i k P j D i k x(t - k ) -e εt k x T (t - k )P i x(t - k ) = e εt k x T (t - k )[D T i k P j D i k -P i ]x(t - k ) ≤ 0 and so V (t k , x(t k ), j) ≤ V (t - k , x(t - k ), i). (<label>22</label></formula><formula xml:id="formula_57">)</formula><p>Combining ( <ref type="formula" target="#formula_52">21</ref>) and ( <ref type="formula" target="#formula_56">22</ref>), we can derive by using the mathematical induction that for all i, j ∈ S and k ≥ 1</p><formula xml:id="formula_58">EV (t k , x(t k ), j) ≤ EV (t - k , x(t - k ), i) ≤ EV (t k-1 , x(t k-1 ), r(t k-1 )) ≤ EV (t - k-1 , x(t - k-1 ), r(t - k-1 )) ≤ • • • ≤ EV (t 0 , x(t 0 ), r(0)). (<label>23</label></formula><formula xml:id="formula_59">)</formula><p>Thus, from ( <ref type="formula" target="#formula_52">21</ref>) and ( <ref type="formula" target="#formula_58">23</ref>), the definition of V (t, x(t), i) and Assumption 3 we have e εt λ min (P i )E|x(t)| 2 ≤ EV (t, x(t), i) ≤ EV (0, x(0), r(0))</p><formula xml:id="formula_60">≤ Ex T (0)P r(0) x(0) + 0 -τ(0)</formula><p>e εs Eg T (x(s))Fg(x(s))ds</p><formula xml:id="formula_61">+ n k=1 n j=1 l k ∞ 0 R kj (θ) 0 -θ Ee ε(s+θ) h 2 j (x j (s))dsdθ ≤ max i∈S λ i sup θ≤0 E|ξ(θ)| 2 + τλ max (U 2 FU 2 ) 0 -τ Ex T (s) x(s)ds + λ max (U 3 trace(L)U 3 ) 0 -∞ Ex T (s)x(s)ds ≤ max i∈S λ i sup θ≤0 E|ξ(θ)| 2 + τ 2 λ max (U 2 FU 2 ) sup θ≤0 E|ξ(θ)| 2 +λ max (U 3 trace(L)U 3 ) sup θ≤0 E|ξ(θ)| 2 ≤ [max i∈S λ i + τ 2 λ max (U 2 FU 2 ) +λ max (U 3 trace(L)U 3 )] sup θ≤0 E|ξ(θ)| 2</formula><p>and so</p><formula xml:id="formula_62">E|x(t)| 2 ≤ e -εt min i∈S λ min (P i ) [max i∈S λ i + τ 2 λ max (U 2 FU 2 ) + λ max (U 3 trace(L)U 3 )] sup θ≤0 E|ξ(θ)| 2</formula><p>which together with Definition 1 implies that the trivial solution of (2) [or (1) equivalently] is exponentially stable in the mean square. This completes the proof of Theorem 1. P Remark 6: Theorem 1 is our first main result, which gives a new exponential stability condition for (2) based on a novel Lyapunov-Krasovskii functional and some new approaches and techniques. As discussed in Remarks 1-5, the generalized neural network ( <ref type="formula" target="#formula_29">2</ref>) is more general than those introduced in [2], <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b39">[40]</ref>, and <ref type="bibr" target="#b40">[41]</ref>, and our conditions are weaker than those given in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b39">[40]</ref>, and <ref type="bibr" target="#b40">[41]</ref>. Thus, the LMI criteria obtained in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b39">[40]</ref>, and <ref type="bibr" target="#b40">[41]</ref> fail in Theorem 1.</p><p>We now illustrate that some known results are as special cases of our result.</p><p>If the Markov chain {r(t), t ≥ 0} only takes a unique value 1, i.e., S = {1}, (2) will be reduced to the following impulsive stochastic CGNN without Markovian jump parameters:</p><formula xml:id="formula_63">⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ dx(t) = -α(x(t))[β(x(t)) -Af (x(t))- Bg(x(t -τ(t))) -C t -∞ R(t -s)h(x(s))ds]dt +σ(x(t), x(t -τ(t)), t -∞ R(t -s) h(x(s))ds)dw(t), t = t k x(t k ) = D k x(t - k ), t = t k (<label>24</label></formula><formula xml:id="formula_64">)</formula><p>where</p><formula xml:id="formula_65">A, B, C, D k , α(x(t)), β(x(t)), σ(x(t), x(t-τ(t)), t -∞</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R(t-s)h(x(s))ds)</head><p>denote</p><formula xml:id="formula_66">A 1 , B 1 , C 1 , D 1 k , α(x(t), 1</formula><p>), β(x(t), 1), σ(x(t), x(t -τ(t)), t -∞ R(t -s)h(x(s))ds, 1), respectively. Accordingly, in Assumptions 1, 2, and 5 we will use 1 , 2 , 3 , α 0 j , α 1 j , µ j , j = 1, 2, . . . , n to denote 11 , 21 , 31 , α 0 1j , α 1 1j , µ 1j , j = 1, 2, . . . , n. Taking D i k ≡ I, then (2) can be written as the following stochastic CGNN without impulses:</p><formula xml:id="formula_67">dx(t) = -α(x(t), i) β(x(t), i) -A i f (x(t)) -B i g(x(t -τ(t))) -C i t -∞ R(t -s)h(x(s))ds dt + σ(x(t), x(t -τ(t)), t -∞</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R(ts)h(x(s))ds, i)dw(t).</head><p>(</p><formula xml:id="formula_68">)<label>25</label></formula><p>Thus, from Theorem 1 we can easily obtain the following results.</p><p>Theorem 2: Under Assumptions 1-6, the trivial solution of ( <ref type="formula" target="#formula_63">24</ref>) is exponentially stable, if there exist positive scalars ε, λ, positive diagonal matrices P, L = diag{l 1 , l 2 , . . . , l n }, M 1 , M 2 , and any matrices V 1 , V 2 , V 3 with appropriate dimensions such that the following LMIs hold:</p><formula xml:id="formula_69">P ≤ λI (<label>26</label></formula><formula xml:id="formula_70">)</formula><formula xml:id="formula_71">D T k D k ≤ I (27) X 1 -1 3(α 1 ) 2 I &lt; 0 (<label>28</label></formula><formula xml:id="formula_72">)</formula><p>where </p><formula xml:id="formula_73">X 1 = [P 0 0 0 0 0 0] T = ⎛ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝<label>11</label></formula><formula xml:id="formula_74">⎞ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎠ 11 = εP -2α 0 P + λ 1 + U 1 M 1 U 1 + τU 2 FU 2 U 3 trace(L)U 3 + V T 1 + V 1 , α 0 = min 1≤j≤n α 0 j 12 = -V 1 + V T 2 , 16 = -V 1 + V T 3 , 22 = λ 2 +U 2 M 2 U 2 -(V T 2 + V 2 ), 26 = V 2 -V T 3 33 = -M 1 + A T A, 44 = -M 2 -(1 -δ)ϕ(δ)F +B T B, 55 = -trace(L) + C T C + λ 3 66 = -(V T 3 + V 3 ), = diag(µ 1 , µ 2 , . . . , µ n ) α 1 = max 1≤j≤n α 1 j , U j = diag(u j1 , u j2 , . . . , u jn ) u jl = max{|u - jl |, |u + jl |} (j = 1, 2, 3, l = 1, 2, . . . , n).</formula><p>Remark 7: Theorem 2 is more general than those given in <ref type="bibr" target="#b24">[25]</ref> since we remove the necessary condition in <ref type="bibr" target="#b24">[25]</ref>:</p><p>∞ 0 e -βs R ij (s)ds = r ij (β), where r ij (β) is a continuous function in [0, δ), δ &gt; 0 and r ij (0) = 1, i, j = 1, 2, . . . , n. Hence, the LMI criteria proposed in <ref type="bibr" target="#b24">[25]</ref> fail in Theorem 2.</p><p>Theorem 3: Under Assumptions 1-6, the trivial solution of (2) [or <ref type="bibr" target="#b0">(1)</ref> equivalently] is exponentially stable, if there exist positive scalars ε, λ i (i ∈ S), positive diagonal matrices L = diag{l 1 , l 2 , . . . , l n }, M 1 , M 2 , P i (i ∈ S), and any matrices V 1 , V 2 , V 3 with appropriate dimensions such that the LMIs (3) and ( <ref type="formula">5</ref>) in Theorem 1 hold.</p><p>Remark 8: Just as discussed in the introduction and Remarks 1-6, Theorem 3 extends and improves the LMI criteria obtained in <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b39">[40]</ref>, and <ref type="bibr" target="#b40">[41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. Extension to Neural Networks With Unknown</head><p>Parameters In this section, we investigate the exponential stability in the mean square of the following delayed Markovian jump impulsive stochastic CGNN with unknown parameters:</p><formula xml:id="formula_75">⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ dx(t) = -α(x(t), i)[β(x(t), i) -(A i + A(t))f (x(t)) -(B i + B(t))g(x(t -τ(t))) -(C i + C(t)) t -∞ R(t -s)h(x(s))ds]dt + σ(x(t), x(t -τ(t)), t -∞ R(t -s)h(x(s))ds, i)dw(t), t = t k x(t k ) = D i k x(t - k ), t = t k (<label>29</label></formula><formula xml:id="formula_76">)</formula><p>where A(t), B(t), and C(t) are unknown matrices denoting time-varying parameter uncertainties and such that the following condition:</p><formula xml:id="formula_77">[ A(t) B(t) C(t)] = MF (t)[N 1 , N 2 , N 3 ]<label>(30)</label></formula><p>where M and N k (k = 1, 2, 3) are known real constant matrices and F (t) is the unknown time-varying matrix-valued function satisfying</p><formula xml:id="formula_78">F T (t)F (t) ≤ I, ∀ t ≥ 0. (<label>31</label></formula><formula xml:id="formula_79">)</formula><p>Definition 3: The trivial solution of ( <ref type="formula" target="#formula_75">29</ref>) is said to be robustly exponentially stable in the mean square if the trivial solution of ( <ref type="formula" target="#formula_75">29</ref>) is exponentially stable in the mean square for all admissible unknown parameters.</p><p>Theorem 4: Under Assumptions 1-6, the trivial solution of ( <ref type="formula" target="#formula_75">29</ref>) is robustly exponentially stable, if there exist positive scalars ε, λ i (i ∈ S), positive diagonal matrices L = diag{l 1 , l 2 , . . . , l n }, M 1 , M 2 , P i (i ∈ S), and any matrices V 1 , V 2 , V 3 with appropriate dimensions such that the following LMIs hold:</p><formula xml:id="formula_80">P i ≤ λ i I (<label>32</label></formula><formula xml:id="formula_81">)</formula><formula xml:id="formula_82">D T i k P j D i k -P i ≤ 0, [here r(t k ) = j] (33) ⎛ ⎜ ⎝ * i X * 1i - 1 3(α 1 i ) 2 I ⎞ ⎟ ⎠ &lt; 0<label>(34)</label></formula><p>where</p><formula xml:id="formula_83">* i is i in Theorem 1 with * 33 = 33 + N T 1 N 1 , * 44 = 44 + N T 2 N 2 , * 55 = 55 + N T 3 N 3 , X * 1i = [P i (I + M) 0 0 0 0 0 0] T .</formula><p>Proof: Let us consider the same Lyapulov-Krasovskii functional as in Theorem 1. For convenience, we take</p><formula xml:id="formula_84">z 1 (t) = x(t -τ(t)), z 2 (t) = t -∞ R(t -s)h(x(s))ds.</formula><p>Then, we will use <ref type="bibr" target="#b28">(29)</ref> to compute LV (t, x(t), i). To end this, from Theorem 1 we only need to estimate the following equalities:</p><formula xml:id="formula_85">2e εt x T (t)P i α(x(t), i) A(t)f (x(t)) = 2e εt x T (t)P i α(x(t), i)MF (t)N 1 f (x(t)) 2e εt x T (t)P i α(x(t), i) B(t)g(z 1 (t)) = 2e εt x T (t)P i α(x(t), i)MF (t)N 2 g(z 1 (t)) 2e εt x T (t)P i α(x(t), i) C(t)z 2 (t) = 2e εt x T (t)P i α(x(t), i)MF (t)N 3 z 2 (t).</formula><p>Using Lemma 1 and Assumption 1, we have</p><formula xml:id="formula_86">2e εt x T (t)P i α(x(t), i)MF (t)N 1 f (x(t)) ≤ e εt [x T (t)P i α(x(t), i)MF (t)F T (t)M T α(x(t), i)P i x(t) + f T (x(t))N T 1 N 1 f (x(t))] ≤ e εt [(α 1 i ) 2 x T (t)P i MM T P i x(t) +f T (x(t))N T 1 N 1 f (x(t))] (35) 2e εt x T (t)P i α(x(t), i)MF (t)N 2 g(z 1 (t)) ≤ e εt [x T (t)P i α(x(t), i)MF (t)F T (t)M T α(x(t), i)P i x(t) + g T (z 1 (t))N T 2 N 2 g(z 1 (t))] ≤ e εt [(α 1 i ) 2 x T (t)P i MM T P i x(t) +g T (z 1 (t)))N T 2 N 2 g(z 1 (t))] (36) 2e εt x T (t)P i α(x(t), i)MF (t)N 3 z 2 (t) ≤ e εt [x T (t) P i α(x(t), i)MF (t)F T (t)M T α(x(t), i)P i x(t) +z T 2 (t)N T 3 N 3 z 2 (t)] ≤ e εt [(α 1 i ) 2 x T (t)P i MM T P i x(t) + z T 2 (t)N T 3 N 3 z 2 (t)].<label>(37)</label></formula><p>Then along the same line of Theorem 1, we can obtain the desired result by applying Lemma 2 and ( <ref type="formula">35</ref>)- <ref type="bibr" target="#b36">(37)</ref>. This completes the proof of Theorem 4. P Remark 9: Theorem 4 is our second main result, which presents a novel robust exponential stability condition for <ref type="bibr" target="#b28">(29)</ref>. As discussed in Remark 6, the generalized neural network ( <ref type="formula" target="#formula_75">29</ref>) is more general than those given in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b39">[40]</ref>, and <ref type="bibr" target="#b40">[41]</ref>, and so the LMI criteria which existed in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b39">[40]</ref>, and <ref type="bibr" target="#b40">[41]</ref> do not hold in Theorem 4.</p><p>We now investigate the robust exponential stability for some special cases of <ref type="bibr" target="#b28">(29)</ref>.</p><p>First of all, letting D i k ≡ I in (29), we can rewrite (29) as follows:</p><formula xml:id="formula_87">dx(t) = -α(x(t), i) β(x(t), i) -(A i + A(t))f (x(t)) -(B i + B(t))g(x(t -τ(t))) -(C i + C(t)) t -∞ R(t -s)h(x(s))ds dt + σ(x(t), x(t -τ(t)), t -∞</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R(ts)h(x(s))ds, i)dw(t).</head><p>(</p><formula xml:id="formula_88">)<label>38</label></formula><p>If we do not consider Markovian jump parameters, (29) will be reduced to the following impulsive stochastic CGNN:</p><formula xml:id="formula_89">⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ dx(t) = -α(x(t))[β(x(t)) -(A + A(t))f (x(t)) -(B + B(t))g(x(t -τ(t))) -(C + C(t)) t -∞ R(t -s)h(x(s))ds]dt + σ(x(t), x(t -τ(t)), t -∞ R(t -s)h(x(s))ds)dw(t), t = t k x(t k ) = D k x(t - k ), t = t k . (<label>39</label></formula><p>) Then, by Theorem 4 we have the following results. Theorem 5: Under Assumptions 1-6, the trivial solution of (2) [or <ref type="bibr" target="#b0">(1)</ref> equivalently] is exponentially stable, if there exist positive scalars ε, λ i (i ∈ S), positive diagonal matrices L = diag{l 1 , l 2 , . . . , l n }, M 1 , M 2 , P i (i ∈ S), and any matrices V 1 , V 2 , V 3 with appropriate dimensions such that the LMIs <ref type="bibr" target="#b31">(32)</ref> and <ref type="bibr" target="#b33">(34)</ref> in Theorem 4 hold.</p><p>Remark 10: As discussed in Remarks 1-6, Theorem 5 generalizes and improves the results obtained in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b39">[40]</ref>, and <ref type="bibr" target="#b40">[41]</ref>.</p><p>Theorem 6: Under Assumptions 1-6, the trivial solution of ( <ref type="formula" target="#formula_89">39</ref>) is robustly exponentially stable, if there exist positive scalars ε, λ, positive diagonal matrices P, L = diag{l 1 , l 2 , . . . , l n }, M 1 , M 2 , and any matrices V 1 , V 2 , V 3 with appropriate dimensions such that the following LMIs hold:</p><formula xml:id="formula_90">P ≤ λI (40) D T k D k ≤ I (41) * X * 1 -1 3(α 1 ) 2 I &lt; 0 (42)</formula><p>where * is in Theorem 2 with * 33 = 33 + N T 1 N 1 , * 44 = 44 + N T 2 N 2 , * 55 = 55 + N T 3 N 3 , X * 1 = [P(I + M) 0 0 0 0 0 0] T . Remark 11: Obviously, Theorem 6 extends and improves many known results such as in <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b22">[23]</ref>- <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b33">[34]</ref>- <ref type="bibr" target="#b36">[37]</ref>, and <ref type="bibr" target="#b38">[39]</ref>. Since many factors like noise disturbances, impulsive perturbations, unknown parameters and continuously distributed delays are considered in Theorem 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. Illustrative Examples</head><p>In this section, two numerical examples are given to illustrate the effectiveness of the obtained results.</p><p>Example 1: Consider a 2-D delayed Markovian jump impulsive stochastic CGNN Obviously, τ = 2.2 and δ = 1.1, which verifies that Assumption 4 holds. Let</p><formula xml:id="formula_91">⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ dx(t) = -α(x(t), i)[β(x(t), i) -A i f (x(t)) -B i g(x(t -τ(t))) -C i 6 0 e -s 1-e -6 h(x(t -s))ds]dt + σ(x(t), x(t -τ(t)), 6 0 e -s 1-e -6 h(x(t -s))ds)dw(t), t = t k , x(t k ) = D i k x(t - k ), t 0 = 0, t k = t k-1 + 0.1k, k = 1, 2, . . . (<label>43</label></formula><formula xml:id="formula_92">f i (x i ) = g i (x i ) = 0.01tanh(x i ), x i ≤ 0 0.02(|x i + 1| -|x i -1|), x i &gt; 0 h i (x i ) = 0.03(|x i + 1| -|x i -1|) (i = 1, 2) then (43) satisfies Assumption 3 with U - 1 = U - 2 = -0.01I, U - 3 = -0.06I, U + 1 = U + 2 = 0.04I, U + 3 = 0.06I. Take σ(x(t), x(t -τ(t)), 6 0 e -s 1 -e -6 h(x(t -s))ds, 1) = 0.2x 1 (t) 0 .3(x 2 (t) + x 1 (t -τ(t))) 0.1x 1 (t -τ(t)) 0.3 6 0 e -s 1-e -6 h 2 (x 2 (t -s))ds σ(x(t), x(t -τ(t)), 6 0 e -s 1 -e -6 h(x(t -s))ds, 2) = 0.3x 2 (t) 0 .2(x 1 (t) + x 2 (t -τ(t))) 0.1x 1 (t -τ(t)) 0.2 6 0 e -s</formula><p>1-e -6 h 1 (x 1 (ts))ds then (43) satisfies Assumptions 5 and 6 with 11 = 0.18I, 21 = 0.19I, 31 = 0.09I, 12 = 0.09I, 22 = 0.08I, 32 = 0.04I. Let α j (x j (t), i) = 3 + sin(t), β j (x j (t), i) = 2x j (t) (i, j = 1, 2). Then, Assumptions 1 and 2 hold with</p><formula xml:id="formula_93">α 0 ij = 2, α 1 ij = 4 and µ ij = 2.</formula><p>Other parameters of the network (43) are given as follows: Finally, by using the MATLAB LMI toolbox, we can obtain the maximum convergence rate 5.87. By using the Euler-Maruyama numerical scheme, simulation results are as follows: T = 8 and step size δt = 0.02. Fig. <ref type="figure" target="#fig_4">1</ref> is the state response of model 1 [i.e., the network (43) when r(t) = 1] with the initial condition [0.7, -0.5] T , for -6 ≤ t ≤ 0, and Fig. <ref type="figure" target="#fig_5">2</ref> is the state response of model 2 [i.e., the network (43) when r(t) = 2] with the initial condition [-0.4, 0.4] T , for -6 ≤ t ≤ 0.   It is clear that τ = 2.6 and δ = 1.3, which implies that Assumption 4 is true. Let</p><formula xml:id="formula_94">A 1 = 0.6 -1.2 0.4 0.5 , B 1 = 1.3 -0.7 0.3 0.8 C 1 = 0.5 0.4 -0.3 1.3 , D 1 = 0.1 0 0 0.1 A 2 = 0.3 0.2<label>-</label></formula><formula xml:id="formula_95">⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ dx(t) = -α(x(t), i)[β(x(t), i) -(A i + A(t))f (x(t)) -(B i + B(t))g(x(t -τ(t))) -(C i + C(t)) 6 0 e -s 1-e -6 h(x(t -s))ds]dt + σ(x(t), x(t -τ(t)), 6 0 e -s 1-e -6 h(x(t -s))ds)dw(t), t = t k , x(t k ) = D i k x(t - k ), t 0 = 0, t k = t k-1 + 0.1k, k = 1, 2, . . . (44) where x(t) = (x 1 (t), x 2 (t)) T , τ(t) = 1.3 sin t + 1.3, w(t)</formula><formula xml:id="formula_96">f i (x i ) = g i (x i ) = 0.01tanh(x i ), x i ≤ 0 0.02(|x i + 1| -|x i -1|), x i &gt; 0 h i (x i ) = 0.03(|x i + 1| -|x i -1|) (i = 1, 2) then (44) satisfies Assumption 3 with U - 1 = U - 2 = -0.01I, U - 3 = -0.06I, U + 1 = U + 2 = 0.04I, U + 3 = 0.06I. Take σ(x(t), x(t -τ(t)), 6 0 e -s 1 -e -6 h(x(t -s))ds, 1) = 0.5x 2 (t) 0 .2(x 2 (t) + x 1 (t -τ(t))) 0.3x 2 (t -τ(t)) 0.2 6 0 e -s 1-e -6 h 1 (x 1 (t -s))ds σ(x(t), x(t -τ(t)), 6 0 e -s 1 -e -6 h(x(t -s))ds, 2) = 0.4x 1 (t) 0 .3(x 2 (t) + x 2 (t -τ(t))) 0.3x 2 (t -τ(t)) 0.5 6 0 e -s</formula><p>1-e -6 h 2 (x 2 (ts))ds then (44) satisfies Assumptions 5 and 6 with 11 = 0.33I, 21 = 0.09I, 31 = 0.04I, 12 = 0.18I, 22 = 0.27I, 32 = 0.25I. Let α j (x j (t), i) = 3 + sin(t), β j (x j (t), i) = 2x j (t) (i, j = 1, 2). Then, Assumptions 1 and 2 hold with</p><formula xml:id="formula_97">α 0 ij = 2, α 1 ij = 4 and µ ij = 2.</formula><p>Other parameters of the network (44) are given as follows: </p><formula xml:id="formula_98">A(t) = B(t) = C(t) = M sint 0 0 cost N.</formula><p>By using the MATLAB LMI toolbox, we can obtain the following feasible solution for the LMIs ( <ref type="formula" target="#formula_80">32</ref>)-( <ref type="formula" target="#formula_82">34</ref>): Finally, by using the MATLAB LMI toolbox, it is easily computed that the maximum convergence rate of the network (44) is 1.94. By using the Euler-Maruyama numerical scheme, simulation results are as follows: T = 8 and step size δt = 0.02. Fig. <ref type="figure" target="#fig_4">1</ref> is the state response of model 1 [i.e., the network (44) when r(t) = 1] with the initial condition [0.8, -0.3] T , for -6 ≤ t ≤ 0, and Fig. <ref type="figure" target="#fig_5">2</ref> is the state response of model 2 [i.e., the network (44) when r(t) = 2] with the initial condition [-0.5, 0.6] T , for -6 ≤ t ≤ 0.</p><formula xml:id="formula_99">F =<label>27</label></formula><p>Remark 12: In Examples 1 and 2, the constant δ is over 1 and u - ij = -u + ij ; moreover, many factors such as noise disturbances, Markovian jump parameters, impulsive perturbations, unknown parameters, and continuously distributed delays are considered. Therefore, all of the criteria existing in the literature (see <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b30">[31]</ref>- <ref type="bibr" target="#b40">[41]</ref> and references therein) fail in Examples 1 and 2.</p><p>Remark 13: From Examples 1 and 2, we know that unknown parameters do have important effects on the convergence rate. Actually, if we add unknown parameters in Example 2 to Example 1, then we can easily obtain the maximum convergence rate 1.46 of the network (43) with unknown parameters, which is smaller than the maximum convergence rate 5.87 of the network (43) with known parameters. Similarly, we can get the maximum convergence rate 6.14 of the network (44) without unknown parameters, which is bigger  than the maximum convergence rate 1.94 of the network (44) with unknown parameters. Also, Figs. 1-4 further show that unknown parameters affect on the convergence rate.</p><p>Remark 14: Our stability criteria are less conservative than those criteria obtained in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b28">[29]</ref>, and <ref type="bibr" target="#b30">[31]</ref>- <ref type="bibr" target="#b40">[41]</ref>. For instance, it follows from Example 4.1 in <ref type="bibr" target="#b21">[22]</ref> that the maximum allowable delay τ = 4.6818. However, using the same parameters as in <ref type="bibr" target="#b21">[22]</ref> and by Theorem 2 we can obtain the maximum allowable delay τ = 106.6767. Due to the limited pages, we omit the details about comparing our criteria with the others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. Conclusion</head><p>By constructing a new Lyapunov-Krasovskii functional, and using some new approaches and techniques, in this paper we have investigated the robust exponential stability analysis problem for a class of Markovian jump stochastic CGNNs with mixed time delays and unknown parameters. The mixed time delays under consideration comprise both time-varying delays and continuously distributed delays, which make our results more effective and easily applicable as neural networks usually have a spatial nature due to the presence of a multitude of parallel pathways with a variety of axon sizes and lengths. Moreover, the derivative of time delays is not necessarily zero or smaller than 1. Therefore, the results obtained in this paper generalize and improve some known results since many factors such as noise disturbances, impulsive perturbations, unknown parameters, Markovian jump parameters, and continuously distributed delays are considered in this paper. It should be also mentioned that the traditional methods and techniques to investigate the deterministic system cannot handle our case as the latter involves the It o s calculus, which is more difficult to deal with than the usual Riemann-Lebesgue calculus. Finally, two numerical examples and their simulations are given to show the effectiveness of the theoretical results.</p><p>To conclude, we believe that the results presented in this paper give a satisfactory answer to the open question proposed in <ref type="bibr" target="#b40">[41]</ref> regarding the exponential stability for a class of Markovian jump stochastic neural networks with continuously distributed delays.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Robust</head><label></label><figDesc>Exponential Stability of Markovian Jump Impulsive Stochastic Cohen-Grossberg Neural Networks with Mixed Time Delays Quanxin Zhu and Jinde Cao, Senior Member, IEEE Abstract-This paper is concerned with the problem of exponential stability for a class of Markovian jump impulsive stochastic Cohen-Grossberg neural networks with mixed time delays and known or unknown parameters. The jumping parameters are determined by a continuous-time, discrete-state Markov chain, and the mixed time delays under consideration comprise both time-varying delays and continuously distributed delays. To the best of the authors' knowledge, till now, the exponential stability problem for this class of generalized neural networks has not yet been solved since continuously distributed delays are considered in this paper. The main objective of this paper is to fill this gap. By constructing a novel Lyapunov-Krasovskii functional, and using some new approaches and techniques, several novel sufficient conditions are obtained to ensure the exponential stability of the trivial solution in the mean square. The results presented in this paper generalize and improve many known results. Finally, two numerical examples and their simulations are given to show the effectiveness of the theoretical results. Index Terms-Continuously distributed delay, impulsive perturbation, Markovian jump parameter, robust exponential stability, stochastic Cohen-Grossberg neural network (CGNN), unknown parameter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>) where x(t) = (x 1 (t), x 2 (t)) T , τ(t) = 1.1 cos t + 1.1, w(t) is a 2-D Brownian motion, and r(t) is a right-continuous Markov chain taking values in S = {1, 2} with generator</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. State response of the model 1 in Example 1.</figDesc><graphic coords="9,75.50,53.89,201.60,177.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. State response of the model 2 in Example 1.</figDesc><graphic coords="9,338.52,54.31,202.08,178.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Example 2 :</head><label>2</label><figDesc>Consider the following 2-D delayed Markovian jump impulsive stochastic CGNN with unknown parameters:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>is a 2-D Brownian motion, and r(t) is a right-continuous Markov chain taking values in S = {1, 2} with generator</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. State response of the model 1 in Example 2.</figDesc><graphic coords="10,335.23,260.87,203.52,179.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. State response of the model 2 in Example 2.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>The authors would like to thank the editor and four anonymous reviewers for their helpful comments and valuable suggestions, which have greatly improved the quality of this paper.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>of current version August 6, 2010. This work was jointly supported by the National Natural Science Foundation of China under Grants 10801056 and 60874088, the Natural Science Foundation of Ningbo under Grant 2010A610094, the K. C. Wong Magna Fund in Ningbo University, and the Specialized Research Fund for the Doctoral Program of Higher Education under Grant 20070286003.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Delay-dependent robust stability analysis for Markovian jumping stochastic Cohen-Grossberg neural networks with discrete interval and distributed time-varying delays</title>
		<author>
			<persName><forename type="first">P</forename><surname>Balasubramaniam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rakkiyappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nonlinear Anal. Hybrid Syst</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="207" to="214" />
			<date type="published" when="2009-08">Aug. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Delay-range dependent stability criteria for neural networks with Markovian jumping parameters</title>
		<author>
			<persName><forename type="first">P</forename><surname>Balasubramaniam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lakshmanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nonlinear Anal. Hybrid syst</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="749" to="756" />
			<date type="published" when="2009-11">Nov. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ghaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Feron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Balakrishnan</surname></persName>
		</author>
		<title level="m">Linear Matrix Inequalities in System and Control Theory</title>
		<meeting><address><addrLine>Philadelphia, PA</addrLine></address></meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Absolute stability of global pattern formation and parallel memory storage by competitive neural networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grossberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="815" to="826" />
			<date type="published" when="1983-07">Jul. 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dynamics analysis of impulsive stochastic Cohen-Grossberg neural networks with Markovian jumping and mixed time delays</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="1999" to="2004" />
			<date type="published" when="2009-03">Mar. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust exponential stability of interval Cohen-Grossberg neural networks with time-varying delays</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chaos Solitons Fractals</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1914" to="1928" />
			<date type="published" when="2009-05">May 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On pth moment exponential stability of stochastic Cohen-Grossberg neural networks with time-varying delays</title>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="986" to="990" />
			<date type="published" when="2010-01">Jan. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust stability of stochastic delayed additive neural networks with Markovian switching</title>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W C</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="799" to="809" />
			<date type="published" when="2007-09">Sep. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exponential stability analysis of stochastic delayed cellular neural network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chaos Solitons Fractals</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1006" to="1010" />
			<date type="published" when="2006-02">Feb. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dynamical analysis of Cohen-Grossberg neural networks with time-delays and impulses</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<idno>nos. 10-12</idno>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="2303" to="2309" />
			<date type="published" when="2009-07">Jul. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stability analysis on Cohen-Grossberg neural networks with both time-varying and continuously distributed delays</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nonlinear Anal. RWA</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2600" to="2612" />
			<date type="published" when="2009-08">Aug. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exponential stability and instability of stochastic neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stoch. Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="165" to="185" />
			<date type="published" when="1996-03">Mar. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Stochastic stability of Markovian jumping Hopfield neural networks with constant and distributed delays</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="3669" to="3674" />
			<date type="published" when="2009-10">Oct. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On global exponential stability of generalized stochastic neural networks with mixed time-delays</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="314" to="326" />
			<date type="published" when="2006-12">Dec. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On delay-dependent robust exponential stability of stochastic neural networks with mixed time delays and Markovian switching</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nonlinear Dyn</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="199" to="212" />
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Delay-dependent stability analysis for impulsive neural networks with time varying delays</title>
		<author>
			<persName><forename type="first">S</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="1705" to="1713" />
			<date type="published" when="2008-03">Mar. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Delay-dependent stochastic stability of delayed Hopfield neural networks with Markovian jump parameters</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Math. Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">328</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="316" to="326" />
			<date type="published" when="2007-04">Apr. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<title level="m">Stochastic Differential Delay Equations With Markovian Switching</title>
		<meeting><address><addrLine>London, U.K.</addrLine></address></meeting>
		<imprint>
			<publisher>Imperial College Press</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On global stability criterion of neural networks with continuously distributed delays</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chaos Solitons Fractals</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="444" to="449" />
			<date type="published" when="2008-07">Jul. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Exponential stability of hybrid stochastic recurrent neural networks with time-varying delays</title>
		<author>
			<persName><forename type="first">G</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nonlinear Anal. Hybrid Syst</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1198" to="1204" />
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Global exponential stability of impulsive Cohen-Grossberg neural networks with continuously distributed delays</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chaos Solitons Fractals</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="164" to="174" />
			<date type="published" when="2009-07">Jul. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dynamic analysis of Markovian jumping impulsive stochastic Cohen-Grossberg neural networks with discrete interval and distributed time-varying delays</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rakkiyappan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Balasubramaniam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nonlinear Anal. Hybrid Syst</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="408" to="417" />
			<date type="published" when="2009-11">Nov. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Delay-dependent asymptotic stability for stochastic delayed recurrent neural networks with time varying delays</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rakkiyappan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Balasubramaniam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Math. Comput</title>
		<imprint>
			<biblScope unit="volume">198</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="526" to="533" />
			<date type="published" when="2008-05">May. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Novel global robust stability criterion for neural networks with delay</title>
		<author>
			<persName><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chaos Solitons Fractals</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="348" to="353" />
			<date type="published" when="2009-07">Jul. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Stability analysis of impulsive stochastic Cohen-Grossberg neural networks with mixed time delays</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. A</title>
		<imprint>
			<biblScope unit="volume">387</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="3314" to="3326" />
			<date type="published" when="2008-05">May 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Global exponential stability of impulsive Cohen-Grossberg neural network with time-varying delays</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nonlinear Anal. RWA</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="500" to="510" />
			<date type="published" when="2008-04">Apr. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Global robust stability criteria of stochastic Cohen-Grossberg neural networks with discrete and distributed time-varying delays</title>
		<author>
			<persName><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nonlinear Sci. Numer. Simul</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="520" to="528" />
			<date type="published" when="2009-02">Feb. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stabilization of stochastic delayed neural networks with Markovian switching</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Asian J. Control</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="327" to="340" />
			<date type="published" when="2008-08">Aug. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">New sufficient conditions for global asymptotic stability of Cohen-Grossberg neural networks with time-varying delays</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nonlinear Anal. RWA</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2139" to="2145" />
			<date type="published" when="2009-08">Aug. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">Impulsive Systems and Control: Theory and Applications</title>
		<meeting><address><addrLine>Huntington, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Nova</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Exponential stability in the mean square for stochastic neural networks with mixed time-delays and Markovian jumping parameters</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nonlinear Dyn</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="209" to="218" />
			<date type="published" when="2009-07">Jul. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Robust stability criteria for interval Cohen-Grossberg neural networks with time varying delay</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1105" to="1110" />
			<date type="published" when="2009-01">Jan. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Exponential stability of delayed recurrent neural networks with Markovian jumping parameters</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Lett. A</title>
		<imprint>
			<biblScope unit="volume">356</biblScope>
			<biblScope unit="page" from="346" to="352" />
			<date type="published" when="2006-08">Aug. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mean square exponential stability of stochastic delayed Hopfield neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Lett. A</title>
		<imprint>
			<biblScope unit="volume">343</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="306" to="1018" />
			<date type="published" when="2005-08">Aug. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">New results on robust exponential stability for discrete recurrent neural networks with time-varying delays</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="3337" to="3342" />
			<date type="published" when="2009-08">Aug. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Exponential p-stability of impulsive stochastic neural networks with mixed delays</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chaos Solitons Fractals</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="263" to="272" />
			<date type="published" when="2009-07">Jul. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Novel results for global robust stability of delayed neural networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Yucel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Arik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chaos Solitons Fractals</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1604" to="1614" />
			<date type="published" when="2009-02">Feb. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Stability analysis of Markovian jumping stochastic Cohen-Grossberg neural networks with mixed time delays</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="366" to="370" />
			<date type="published" when="2008-02">Feb. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Exponential stability of stochastic delayed Hopfield neural networks</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Math. Comput</title>
		<imprint>
			<biblScope unit="volume">199</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="84" to="89" />
			<date type="published" when="2008-05">May 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Exponential stability of hybrid stochastic neural networks with mixed time delays and nonlinearity</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="3357" to="3365" />
			<date type="published" when="2009-08">Aug. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">He is currently a TePin Professor and a Doctoral Advisor with the the Department of Mathematics, Southeast University. He is the author and co-author of more than 160 journal papers and five edited books. His current research interests include nonlinear systems, neural networks, complex systems and complex networks, stability theory, and applied mathematics. Dr. Cao is an Associate Editor of the IEEE Transactions on Neural Networks</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao ; Shatin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">respectively, all in mathematics/applied mathematics. From 1989 to 2000, he was with Yunnan University, where he was a Professor from 1996 to 2000</title>
		<title level="s">respectively, and the Ph.D. degree in probability and statistics from Sun Yat-sen</title>
		<meeting><address><addrLine>Changsha, China; Guangzhou, China; Guangzhou; Nanjing, China; Ningbo, China; Wuhu, China; Kunming, China; Chengdu, China; Nanjing, China; London, U.K.</addrLine></address></meeting>
		<imprint>
			<publisher>Discrete Dyn. Nat. Soc</publisher>
			<date type="published" when="1986">2009. Dec. 2009. 1999. 2002. 2005. 2005 to 2009. 2009. 1986. 1989. 1998. 2000. 2001 to 2002. 2006 to 2008</date>
			<biblScope unit="page">20</biblScope>
		</imprint>
		<respStmt>
			<orgName>Zhongshan) University ; South China Normal University ; Southeast University ; Ningbo University ; Yunnan University ; Sichuan University ; Department of Mathematics, Southeast University ; Doctoral Research Fellow with the Department of Automation and Computer-Aided Engineering, Chinese University of Hong Kong ; Brunel University</orgName>
		</respStmt>
	</monogr>
	<note>Mathematics and Computers in Simulation</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
