<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improve robustness of sparse PCA by L 1 -norm maximization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2011-07-19">19 July 2011</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Deyu</forename><surname>Meng</surname></persName>
							<email>dymeng@mail.xjtu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Qian</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zongben</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Institute for Information and System Sciences and Ministry of Education Key Lab for Intelligent Networks and Network Security</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<postCode>710049</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">D. Meng)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improve robustness of sparse PCA by L 1 -norm maximization</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2011-07-19">19 July 2011</date>
						</imprint>
					</monogr>
					<idno type="MD5">0D574FE0EEB0A07594593F1E712817DC</idno>
					<idno type="DOI">10.1016/j.patcog.2011.07.009</idno>
					<note type="submission">Received 9 March 2010 Received in revised form 2 June 2011 Accepted 7 July 2011</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Face recognition Noise Outlier Principal component analysis Robust Sparsity</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Various sparse principal component analysis (PCA) methods have recently been proposed to enhance the interpretability of the classical PCA technique by extracting principal components (PCs) of the given data with sparse non-zero loadings. However, the performance of these methods is prone to be adversely affected by the presence of outliers and noises. To alleviate this problem, a new sparse PCA method is proposed in this paper. Instead of maximizing the L 2 -norm variance of the input data as the conventional sparse PCA methods, the new method attempts to capture the maximal L 1 -norm variance of the data, which is intrinsically less sensitive to noises and outliers. A simple algorithm for the method is specifically designed, which is easy to be implemented and converges to a local optimum of the problem. The efficiency and the robustness of the proposed method are theoretically analyzed and empirically verified by a series of experiments implemented on multiple synthetic and face reconstruction problems, as compared with the classical PCA method and other typical sparse PCA methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Principal component analysis (PCA) is one of the most classical and popular techniques for data processing and dimensionality reduction, and has wide range of applications throughout science and engineering <ref type="bibr" target="#b0">[1]</ref>. In essence, PCA seeks the so-called principal components (PCs) along which the data variance can be maximally preserved. By projecting the data into the low-dimensional linear subspace constituted by the PCs so extracted, the data structure in the original input space can be effectively captured.</p><p>Despite its many advantages, the traditional PCA suffers from the fact that each component is generally a linear combination of all the original variables and all weights in the linear combination, also known as loadings, are typically non-zeroes. In many applications, however, the original variables have meaningful physical interpretations. In biology for example, each involved variable might correspond to a specific gene. In these cases, the interpretation of the PCs will be facilitated if the derived PCs involve fewer non-zero loadings.</p><p>Accordingly, sparse PCA has been an active research topic for more than a decade, and a variety of methods for this topic have been developed <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>. For example, good results have been achieved by the SPCA algorithm of Zou et al., which is developed based on iterative elastic net regression <ref type="bibr" target="#b1">[2]</ref>. D'Aspremont et al. proposed a method, called DSPCA, for finding sparse PCs by solving a sequence of semidefinite program relaxations of sparse PCA <ref type="bibr" target="#b2">[3]</ref>. Journe ´e et al. designed four algorithms ðGPower l 0 , GPower l 1 , GPower l 0 ,m , and GPower l 1 ,m Þ for sparse PCA by formulating the issue as non-concave maximization problems with L 0 -or L 1 -norm sparsity-inducing penalties and extracting single unit sparse PC sequentially or block units ones simultaneously <ref type="bibr" target="#b3">[4]</ref>. Based on expectation-maximization for probabilistic generative model of PCA, Sigg and Buhmann derived EMPCA for sparse and/or non-negative principal component analysis <ref type="bibr" target="#b4">[5]</ref>. Very recently, Lu and Zhang developed an augmented Lagrangian method (ALSPCA briefly) for sparse PCA by solving a class of non-smooth constrained optimization problems <ref type="bibr" target="#b5">[6]</ref>. Additionally, greedy methods were investigated for sparse PCA by Moghaddam et al. (GSPCA <ref type="bibr" target="#b6">[7]</ref>) and d <ref type="bibr">'Aspremont et al. (PathSPCA [8]</ref>). These methods have been successfully applied to many problems for extracting sparse and interpretable PCs from the given raw data.</p><p>However, the intrinsic principle underlying the current sparse PCA methods is to maximize the L 2 -norm variance of the input data under certain sparsity constraint (which is to be introduced in detail toward the next section). This naturally conducts the problem that the methods are prone to the presence of outliers or noises due to the fact that the influence of outliers or noises with a large norm tends to be considerably exaggerated by the use of the L 2 -norm. This robustness problem conducted by L 2 -norm variance has been emphasized by multiple traditional PCA researchers <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref>, while has not been noted in sparse PCA area.</p><p>In this paper, instead of maximizing variance with intrinsic L 2 -norm, a new optimization model that maximizes the L 1 -norm variance is presented to achieve robust sparse PCA. A simple algorithm for solving the proposed L 1 -norm optimization is correspondingly developed. The proposed algorithm is easy to be implemented, and especially, it is theoretically evaluated that the computational speed of the new algorithm surprisingly exceeds many of the current sparse PCA methods. The algorithm is also proved to be able to converge to a reasonable local optimum of the original optimization model. By a series of experiments, it is verified that the proposed algorithm has an efficient and robust performance on data with intrinsic outliers and noises.</p><p>In what follows, the robustness problem of the current sparse PCA methods is first formulated in Section 2. The new robust sparse PCA algorithm is then proposed in Section 3. Also in this section the local optimality of the algorithm is proved and the computational complexity of the algorithm is evaluated. To verify the effectiveness of the proposed algorithm, results obtained from a series of empirical studies, as compared with those of other conventional methods, are analyzed and interpreted in Section 4. The paper is then concluded with a summary and outlook for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Problem formulation</head><p>Denote the input data matrix as X ¼ ½x 1 , . . . ,x n A R dÂn , where d and n are the dimensionality and the size of the given data, respectively. After a location transformation, we can assume all fx i g n i ¼ 1 to have zero mean.</p><p>The classical PCA model tries to find an mð odÞ dimensional linear subspace where the variance of the input data X is maximized. Such a subspace can be achieved by solving the following optimization problem:</p><formula xml:id="formula_0">W n ¼ arg max W JW T XX T WJ 2 ¼ JW T XJ 2 2 , subject to W T W ¼ I m ,<label>ð1Þ</label></formula><p>where W ¼ ½w 1 ,w 2 , . . . ,w m A R dÂm , where each column w k of W corresponds to the k-th PC of the original data and J Á J 2 denotes the L 2 -norm of a matrix or a vector. Under the constraint that W T W ¼ I m , it is known that all fw k g m k ¼ 1 constitute the regular orthogonal bases of the m-dimensional linear subspace where the maximal L 2 -norm variance of X is captured.</p><p>Sparse PCA model aims at achieving sparse PCs on which maximal amount of data variance can be possibly obtained. This aim can be attained by solving the following optimization:</p><formula xml:id="formula_1">W n ¼ arg max W JW T XJ 2 2 , subject to W T W ¼ I m , JWJ 0 ok:<label>ð2Þ</label></formula><p>Note that the only difference between the optimizations (1) and</p><p>(2) for classical PCA and sparse PCA is that the latter involves an extra l 0 penalty, i.e., JWJ 0 o k, to enforce sparsity of the output PCs.</p><p>It is easy to see that the optimization ( <ref type="formula" target="#formula_1">2</ref>) is a hard combinatorial problem and very difficult to solve. Hence a more generally employed sparse PCA formulation is to relax the non-convex l 0 penalty to a weaker but convex l 1 penalty, i.e., JWJ 1 ot. This leads to the following amended optimization:</p><formula xml:id="formula_2">W n ¼ arg max W JW T XJ 2 2 , subject to W T W ¼ I m , JWJ 1 o t:<label>ð3Þ</label></formula><p>The formulation (3), as well as (2), constitutes the fundament of most of current sparse PCA methods <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>.</p><p>Note that the objective of both of the above optimizations for sparse PCA is to maximize the data variance with intrinsic L 2 -norm, i.e., JW T XJ <ref type="foot" target="#foot_1">2</ref>2 . Yet it is known that the L 2 -norm variance is sensitive to outliers and noises with large norms <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref>. This phenomenon is graphically depicted in Fig. <ref type="figure" target="#fig_1">1</ref>, which shows the L 2 -norm variance curve f ðxÞ ¼ JxJ 2  2 and L 1 -norm one f ðxÞ ¼ JxJ 1 , respectively. From the figure, the exaggerative effect of L 2 -norm variance at points with large norms, as compared with the L 1 -norm one, is evident. This on one hand clarifies the robust problem of the traditional sparse PCA methods on the data with heavy outlier or noise pollution, and on the other hand implies a meliorative strategy to this problem by substituting the L 1 -norm variance JW T XJ 1 for the L 2 -norm one JW T XJ 2  2 in optimization problems (2) or (3). <ref type="foot" target="#foot_0">1</ref>Motivated by the above analysis, we formulate the following optimization to realize the robust sparse PCA:</p><formula xml:id="formula_3">W n ¼ arg max W JW T XJ 1 , subject to W T W ¼ I m , JWJ 1 ot,<label>ð4Þ</label></formula><p>which is expected to be more robust to outliers and noises than the traditional sparse PCA techniques. One downside of (4) is that the optimal i-th PC w i yielded from (4) varies with different preset number m of PCs. Besides, finding a global solution of (4) for m4 1 is very difficult. To ameliorate the problems, we simplify the problem (4) into a sequence of m¼1 optimizations using a greedy search strategy. That is, (4) is simplified as the following optimization problem:</p><formula xml:id="formula_4">w n ¼ arg max w JX T wJ 1 , subject to w T w ¼ 1, JwJ 1 o t:<label>ð5Þ</label></formula><p>Although the successive greedy solutions of (5) may differ from the optimal solution of (4), it is expected to provide a good approximation for (4). In the following, an efficient algorithm to solve ( <ref type="formula" target="#formula_4">5</ref>) is first introduced and the greedy algorithm for searching m 4 1 PCs is then presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Robust sparse PCA</head><p>Even for the simplified problem (5), it is difficult to solve it by traditional optimization techniques due to its absolute value operations both on objective function and constraint. In this paper, a simple while efficient algorithm (called the robust sparse PCA algorithm, or simply RSPCA algorithm) is especially designed for <ref type="bibr" target="#b4">(5)</ref>, which is introduced in the following. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">RSPCA algorithm for one sparse PC</head><p>The new algorithm for solving the optimization (5) is listed as follows. The initialization of the algorithm is to be discussed toward the end of this section. In the algorithm, X ¼ ½x 1 , . . . ,x n A R dÂn denotes the data matrix, w(0) the initialized PC vector, w(t) the sparse PC vector in t-th iteration, and w n the sparse PC the algorithm finally converges to. Algorithm 1. RSPCA algorithm for one sparse PC.</p><p>Input: data matrix X, sparsity k.</p><p>(1) Initialize w(0); set wð0Þ ¼ wð0Þ Jwð0ÞJ 2 and t ¼0.</p><formula xml:id="formula_5">(2) Set v ¼ ðv 1 , . . . ,v d Þ T ¼ P n i ¼ 1 p i ðtÞx i , where p i ðtÞ ¼ 1 if w T ðtÞx i Z0 À1 if w T ðtÞx i o0</formula><p>( ; let g be the (kþ 1)-th largest element of jvj. The convergence of the above algorithm and the rationality of the obtained w n are theoretically substantiated by the following theorem.</p><formula xml:id="formula_6">(3) Let b ¼ ðb 1 , . . . ,b d Þ T , where b i ¼ sgnðv i Þðjv i jÀgÞ þ for i ¼ 1, . . . ,</formula><p>Theorem 1. By implementing Algorithm 1, w(t) converges to a k-sparse vector w n , which is a local maximum point of JX T wJ 1 in the k-dimensional subspace where the non-zero loadings of w n are located (denoted as the k-subspace of w n briefly in the following).</p><p>We first present a lemma which is necessary to prove the above theorem.</p><formula xml:id="formula_7">Lemma 1. Given the vector v ¼ ðv 1 , . . . ,v d Þ T , the solution of the following optimization problem max w w T v, subject to w T w ¼ 1, JwJ 1 o t<label>ð6Þ</label></formula><p>is of the following form</p><formula xml:id="formula_8">w n ¼ b JbJ 2<label>ð7Þ</label></formula><formula xml:id="formula_9">where b ¼ ðb 1 , . . . ,b d Þ T and b i ¼ sgnðv i Þðjv i jÀgÞ þ , i ¼ 1, . . . ,d:<label>ð8Þ</label></formula><p>Furthermore, if the sparsity of the solution w n is known to be k beforehand, then g ¼ y k þ 1 , where y k denotes the k-th largest element of jvj.</p><p>Proof of Lemma 1. We first prove that the solution of the optimization problem ( <ref type="formula" target="#formula_7">6</ref>) can be expressed as the ( <ref type="formula" target="#formula_8">7</ref>) form.</p><p>It is known that the Lagrangian formulation of ( <ref type="formula" target="#formula_7">6</ref>) is</p><formula xml:id="formula_10">LðwÞ ¼ w T vÀaðw T wÀ1ÞÀgðJwJ 1 ÀtÞ,</formula><p>where aAR and g40 are Lagrangian multipliers. Since @LðwÞ=@w ¼ vÀ2awÀg sgnðwÞ, it follows that the optimal solution w n of ( <ref type="formula" target="#formula_7">6</ref>) corresponds to</p><formula xml:id="formula_11">w n ¼ 1 2a b ¼ 1 2a ðb 1 , . . . ,b d Þ T , where b i ¼ sgnðv i Þðjv i jÀgÞ þ , 8i ¼ 1, . . . ,d.</formula><p>To further make the constraint w T w ¼ 1, we have that a ¼ JbJ 2 =2. That is, the optimal solution of ( <ref type="formula" target="#formula_7">6</ref>) is of the form w n ¼ b=JbJ 2 .</p><p>Then we prove that if the sparsity of the solution w n is known to be k, it holds that g in ( <ref type="formula" target="#formula_9">8</ref>) should be y k þ 1 .</p><p>Since the w n is known to be k-sparse, based on ( <ref type="formula" target="#formula_8">7</ref>) and ( <ref type="formula" target="#formula_9">8</ref>), it is evident that g should be evaluated in the interval</p><formula xml:id="formula_12">½y k þ 1 ,y k Þ. Let ṽ ¼ ð ṽ1 , . . . , ṽd Þ T , where ṽi ¼ v i if jv i jZ y k þ 1 , otherwise ṽi ¼ 0</formula><p>, and e ¼ sgnð ṽÞ. Based on ( <ref type="formula" target="#formula_8">7</ref>) and ( <ref type="formula" target="#formula_9">8</ref>), it is easy to obtain that when</p><formula xml:id="formula_13">gA½y k þ 1 ,y k Þ, it holds that w nT v ¼ ð ṽÀgeÞ T ṽ J ṽÀgeJ 2 :¼ f ðgÞ:</formula><p>Differentiate f ðgÞ w.r.t. g, we then get</p><formula xml:id="formula_14">f 0 ðgÞ ¼ À ṽT eð ṽÀgeÞ 2 þ ṽT ð ṽÀgeÞð ṽÀgeÞ T e J ṽÀgeJ 3 2 ¼ À g 1À ṽ J ṽJ 2 T e JeJ 2 2 ! J ṽJ 2 2 JeJ 2 2 J ṽÀgeJ 3 2 r 0:</formula><p>That is, f ðgÞ monotonically decreases w.r.t. g in the interval</p><formula xml:id="formula_15">½y k þ 1 ,y k Þ.</formula><p>Then it is easy to see that to maximize the objective</p><formula xml:id="formula_16">w nT v in optimization problem (6), g should be set as y k þ 1 . The proof is then completed. &amp;</formula><p>Based on Lemma 1, Theorem 1 is then proved as follows.</p><p>Proof of Theorem 1. Inspired by the idea presented in <ref type="bibr" target="#b14">[15]</ref>, the convergence of w(t) is proved by verifying the nondecreasing property of JX T wðtÞJ w.r.t. t as follows:</p><formula xml:id="formula_17">JX T wðtÞJ 1 ¼ X n i ¼ 1 jw T ðtÞx i j ¼ w T ðtÞ X n i ¼ 1 p i ðtÞx i Z w T ðtÞ X n i ¼ 1 p i ðtÀ1Þx i Zw T ðtÀ1Þ X n i ¼ 1 p i ðtÀ1Þx i ¼ X n i ¼ 1 jw T ðtÀ1Þx i j ¼ JX T wðtÀ1ÞJ 1 :</formula><p>Because the objective function JX T wðtÞJ 1 is obviously bounded and nondecreasing w.r.t. t, the convergence of Algorithm 1 is then naturally conducted. In the above deduction, due to the fact that p i ðtÞw T ðtÞx i Z 0 for all i, the first inequality is evident. The second inequality holds since for any t, w(t) is the k-sparse unit vector which maximizes the inner product of wðtÞ T vðtÀ1Þ ¼</p><formula xml:id="formula_18">P n i ¼ 1 wðtÞ T ðp i ðtÀ1Þx i Þ according to Lemma 1.</formula><p>Then we prove the local optimality of the k-sparse point w n yielded by the algorithm in its located k-subspace.</p><p>For all i ¼ 1,2, . . . ,n, let p i ¼ À1 if w nT x i o0; otherwise let p i ¼ 1. Due to the convergence condition (4.2) of the algorithm, it is evident that w nT p i x i 40 for any x i satisfying jsgnðw nT Þjjx i j a0. For such x i , it is easy to conduct that in a small neighborhood Nðw n Þ of w n in its k-subspace, it holds that for any wA Nðw n Þ, w T p i x i Z 0, i.e., w T p i x i ¼ jw T x i j. Besides, if x i satisfies jsgnðw nT Þjjx i j ¼ 0, it is easy to conduct that the k loadings of such x i in the k-subspace of w n are all zeroes. This implies that for any w in the k-subspace of w n , it holds that w T x i ¼ w T p i x i ¼ jw T x i j ¼ 0. Accordingly, it follows that for any w A Nðw n Þ, w T p i x i ¼ jw T x i j. Then according to Lemma 1 and the convergence of our algorithm as proved above, w n is the optimal k-sparse unit vector to maximize w T p i x i , and it naturally follows that JX T w n J</p><formula xml:id="formula_19">1 ¼ w nT P n i ¼ 1 p i x i Z w T P n i ¼ 1 p i x i ¼ JX T wJ 1 for all w A Nðw n Þ.</formula><p>Thus, w n yielded by Algorithm 1 corresponds to a local maximum of JX T wJ 1 in its k-subspace. &amp; So far, we have clarified that Algorithm 1 tends to attain a reasonable k-sparse PC of the given data. In the following we further extend this algorithm to a heuristic greedy strategy for finding an arbitrary number of k-sparse PCs for RSPCA model (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">RSPCA algorithm for m sparse PCs</head><p>The RSPCA algorithm for mð 4 1Þ sparse PCs is constructed by applying Algorithm 1 greedily to the remainder of the projected samples X j . The procedure is listed as follows.</p><p>Algorithm 2. RSPCA algorithm for m sparse PCs.</p><p>Input: data matrix X, sparsity k, desired PC number m 41;</p><p>(1) Set w 0 ¼ 0 A R d , where 0 is the all-zero vector; denote</p><formula xml:id="formula_20">X 0 ¼ fx 0 i ¼ x i g n i ¼ 1 . (2) For j ¼ 1, . . . ,m, do the following iteration: (2.1) Let X j ¼ fx j i ¼ x jÀ1 i Àw jÀ1 ðw T jÀ1 x jÀ1 i Þg n i ¼ 1 . (2.</formula><p>2) Apply Algorithm 1 to the projected data X j to get the k-sparse PC vector w j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>End for</head><formula xml:id="formula_21">Output: m k-sparse PCs fw i g m i ¼ 1 .</formula><p>It should be noted that the sparse PCs yielded by the proposed heuristic algorithm only offer an approximate solution to (4). On one hand, only a local maximum of the L 1 -norm optimization (4) can be achieved by Algorithm 1; and on the other hand, the orthonormality of the projection vectors fw i g m i ¼ 1 generated by Algorithm 2 cannot be theoretically guaranteed. However, since in each iteration of the algorithm, the projected samples X j are in fact located in the subspace orthogonal to the (jÀ 1)-dimensional space spanned by fw i g jÀ1 i ¼ 1 , i.e., w T k x j i ¼ 0 for all i ¼ 1, . . . ,n and k ¼ 1, . . . ,jÀ1, the k-sparse PC w j obtained from the projected data X j also inclines to be approximately orthogonal to all w i s (i ¼ 1, . . . ,jÀ1Þ. Besides, despite the heuristic approximation of Algorithm 1, the proposed algorithm is expected to provide good projections that can possibly capture a large L 1 dispersion of the original data, and hence offer good robust sparse PCs. All the aforementioned is further to be verified by experiments depicted in the next section.</p><p>An important issue still remains in the implementation of the proposed algorithm: the initial w 0 in Algorithm 1 needs to be properly specified to guarantee that the algorithm can converge to a good local optimal solution. Here two strategies are suggested. The first is to specify w 0 as the solution of the classical PCA. Since PCA precisely attains the global optimal solution where the L 2 -norm variance of the original data is maximized, it is expected that the proposed algorithm could also converge to a good sparse PC by starting the iteration from the PCA solution. Yet the downside is that the supplemental implementation of PCA in step (1) might materially increase the computational complexity of the proposed algorithm, especially for large data set. The second strategy is to run the proposed algorithm multiple times with different initial w 0 (which can be easily specified as the random vector or simple all-0 or all-1 vector) and output the solution that gives the maximal L 1 dispersion. This strategy is simple and easy to be implemented, and hence was employed for specification of initial w 0 in our experiments.</p><p>The computation of the proposed algorithm is mainly costed on its iterative process, i.e., steps (2)-( <ref type="formula" target="#formula_3">4</ref>) of Algorithm 1. Evidently, only simple vector computation is involved in these steps, and it is easy to obtain that the computational complexity of the whole algorithm is around Oðnd logdÞ Â n it , where n it is the number of the   iterations for convergence. As compared with the computational complexities of most of the current sparse PCA methods, such as</p><formula xml:id="formula_22">Oðnd 3 Þ Â n it of SPCA, Oðnd 4 logdÞ Â n it of DSPCA, Oðnd logdÞ Â n it of EMPCA, Oðnd 2 Þ Â n it of ALSPCA, OðndÞ Â n it of GPowerl 0 ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>and</head><p>Oðn 3 dÞ Â n it of PathSPCA (where n it is the iteration time of the corresponding method), the proposed algorithm does not substantially increase (for EMPCA and GPower l 0 methods), or even decrease (for other methods) the computational time for sparse PCA calculation. Besides, the iteration number n it of the proposed algorithm is generally very small, further conducting the efficiency of the proposed algorithm. All of the aforementioned will be further verified by the simulation results given in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment results</head><p>To evaluate the performance, especially the robustness, of the proposed method, it was applied to problems with intrinsic noises and outliers to different extents. For comparison, the classical PCA and nine of the current sparse PCA methods, including SPCA <ref type="bibr" target="#b1">[2]</ref>, DSPCA <ref type="bibr" target="#b2">[3]</ref>, PathSPCA <ref type="bibr" target="#b7">[8]</ref>, EMPCA <ref type="bibr" target="#b4">[5]</ref>, GPower l 1 , GPower l 0 , GPower l 1 ,m , GPower l 0 ,m <ref type="bibr" target="#b3">[4]</ref>, and ALSPCA <ref type="bibr" target="#b5">[6]</ref> methods, have also been utilized. The results are summarized in the following discussion. All programs were implemented under Matlab 7.0 platform. The implementation environment was the personal computer with Intel Core(TM)2 Q9300@2.50 G (CPU), 3.25 GB (memory), and Windows XP (OS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">A toy problem with two outliers</head><p>The performance of the proposed RSPCA method was first evaluated on the 2D toy data fx i ,y i g 50 i ¼ 1 as depicted in Fig. <ref type="figure" target="#fig_3">2</ref>(a). The data were generated by picking x i from À 2.4 to 2.5 with the similar interval 0.1, and yielding y i from the uniform distribution on [ À0.25,0.25], except that at x i ¼1.3 and x i ¼1.5, y i s were set values around 7. Evidently, the data contain two intrinsic outliers, and if we discard the outliers, the first principal component of the data should be the sparse vector (1,0). The first PC vectors obtained by applying the classical PCA method, nine existing sparse PCA methods, and the RSPCA method to the toy data are depicted in Fig. <ref type="figure" target="#fig_3">2</ref>(a) and also listed in Table <ref type="table" target="#tab_1">1</ref>. Fig. <ref type="figure" target="#fig_3">2(b)</ref> shows the residual error of each x i (i ¼ 1, . . . ,nÞ conducted by each of the 11 employed methods. Here the residual error of x i is calculated by e i ¼ jx i Àww T x i j, where w is the first PC vector obtained from the corresponding method. Furthermore, the average residual errors (denoted as ARSE in brief) of 11 utilized methods and the iteration times of these methods on calculating the PCs of the toy data are listed in Table <ref type="table" target="#tab_1">1</ref> for further comparison.</p><p>By observing Fig. <ref type="figure" target="#fig_3">2</ref> and Table <ref type="table" target="#tab_1">1</ref>, it is evident that the RSPCA outperforms the other methods in the toy problem. First, the RSPCA attains the accurate sparse PC (1,0) of the original data set, while all of the other 10 methods do not. Second, RSPCA achieves the smallest ARSE of all of the 11 utilized methods. These results show that the other methods are much influenced by the outliers than the proposed method. Besides, from Table <ref type="table" target="#tab_1">1</ref>, it is impressive that the RSPCA only needs four iterations to attain such a robust result, no larger than most of the other sparse PCA methods. This further verifies the efficiency of the proposed method in the outlier case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Tests on benchmark data with intrinsic noises and outliers</head><p>In this section we consider the data first proposed by <ref type="bibr" target="#b1">[2]</ref>. The data set contains a collection of 10D data points ðx 1 , . . . ,x 10 Þ T generated via the following two processes: first three hidden factors were created:</p><formula xml:id="formula_23">V 1 $ Nð0,290Þ, V 2 $ Nð0,300Þ, V 3 ¼ À0:3V 1 þ 0:925V 2 þ e,</formula><p>where e $ Nð0,1Þ, and V 1 ,V 2 and e are independent; afterwards, 10 observed variables were generated as</p><formula xml:id="formula_24">x i ¼ V j þ e j i , e j i $ Nð0,sÞ ð<label>9Þ</label></formula><p>with j ¼1 for i ¼ 1,2,3,4, j¼2 for i ¼ 5,6,7,8, j ¼3 for i¼9, 10, s ¼ 1 and all e j i s independent. It has been clarified that the data so generated are of intrinsic sparse PCs <ref type="bibr" target="#b1">[2]</ref>. In particular, the first PC should recover the factor V2 only using ðx 5 ,x 6 ,x 7 ,x 8 Þ, and the second should recover V1 only using ðx 1 ,x 2 ,x 3 ,x 4 Þ. This type of data is one of the most frequently utilized benchmark examples to evaluate the performance of the sparse PCA method <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6]</ref>, and hence employed here to verify the robustness of the proposed method. Specifically, two sequences of data were constructed by blending such benchmark data with noises and outliers respectively as follows:</p><p>Noise data sequence: Contain 2000 collections of 10D data sets, each with size 10,000. Each data set in the sequence was generated from the benchmark distribution as formulated in ( <ref type="formula" target="#formula_24">9</ref>) with noise extent s varying from 1 to 2000 at regular interval 1.</p><p>Outlier data sequence: Contain 3000 collections of 10D data sets, each with size 10,000. In each data, 9500 points were generated via the aforementioned benchmark process, and 500 ones were obtained by letting x i ¼0, for i ¼ 1, . . . ,8, and x i ¼ x i for i¼9, 10, where x i $ Nð0,sÞ (i¼ 9, 10) and x 9 and x 10 are independent. By varying the outlier extent s from 1 to 3000 with the fixed interval 1, the sequence of outlier data sets was then yielded. Evidently, 5% outliers are intrinsically mixed in each of the data set so generated.</p><p>For each data of the above cases, the classical PCA, the nine current sparse PCA methods, and the RSPCA method were, respectively, employed to calculate the first two PCs of the data. By virtue of the oracle information of the ideal sparse PCs (i.e., the positions of the intrinsic non-zero loadings of the first two PCs), the misidentification rate (MR in brief) of each method corresponding to the data can thus be obtained. By calculating the average of every 10 successive MR values so obtained, the tendency curves of MR w.r.t. the noise extent and the outlier extent were then attained (with lengths 200 and 300), as depicted in Figs. <ref type="figure" target="#fig_4">3</ref> and<ref type="figure" target="#fig_5">4</ref>, respectively. To make a clearer clarification, Tables <ref type="table" target="#tab_2">2</ref> and<ref type="table" target="#tab_3">3</ref> summarize performance of the 11 employed methods when they were applied to the data sets with the largest noise (s ¼ 2000Þ and outlier (s ¼ 3000Þ extents, respectively.</p><p>From Figs. <ref type="figure" target="#fig_4">3</ref> and<ref type="figure" target="#fig_5">4</ref>, it is easy to observe that both MR tendency curves of the RSPCA w.r.t. the noise and outlier extents are always located at or very close to 0. Combined with Tables <ref type="table" target="#tab_2">2</ref> and<ref type="table" target="#tab_3">3</ref>, it is apparent that the RSPCA method robustly delivers the ideal sparse representations of the first two PCs underlying the data, and has a stable performance w.r.t. different extents of noises and outliers. As compared with the tendency curves of the other 10 methods, it is evident that the proposed method significantly improves the robustness of the current sparse PCA methods. Furthermore, it is seen from Tables <ref type="table" target="#tab_2">2</ref> and<ref type="table" target="#tab_3">3</ref> that for each of the listed outlier and noise cases, the iteration time of the RSPCA method is the smallest of all of the utilized 10 sparse PCA methods. This further implies the efficiency of the proposed method. The proposed method was also applied to face reconstruction problems and the performance was compared with those of the other methods. The employed data set is the well-known Yale face database <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21]</ref>. The database contains 165 gray-scale images of 15 individuals, and there are 11 images per subject, one per different facial expression or configuration. The data can be downloaded from the website ''http://cvc.yale.edu/projects/yalefaces/yale-faces.html''.</p><p>The original pixel size of the face image is 320 Â 243. In our experiments, preprocessing was first performed to crop the original face images: the facial areas were cropped into the final images for matching, and the size of each cropped image is 171 Â 215. Some typical cropped faces are depicted in Figs. <ref type="figure" target="#fig_7">5</ref> and<ref type="figure">6</ref>. Each pixel was regarded as an input variable, and hence the input space of the database is of intrinsic 36,765 dimensionality.</p><p>Based on the database so generated, the occluded and dummy databases were then generated as follows, with intrinsic noises and outliers, respectively.</p><p>Occluded face data: Randomly pick two images from each of the 15 individuals' face images, and then occlude them with a rectangular noise consisting of random black and white dots whose size was 80 Â 160 or 150 Â 70, located at a random position of the image. Fig. <ref type="figure" target="#fig_7">5</ref> shows typical images so occluded.</p><p>Dummy face data: Add 30 dummy images which consist of random black and white dots to the original 165 cropped face images to constitute the dummy data set.</p><p>We have performed the classical PCA, nine existing sparse PCA methods, and the RSPCA method on the occluded and dummy face databases, respectively, while each of the SPCA, DSPCA, PathSPCA, GPower l 1 ,m , GPower l 0 ,m , ALSPCA methods encountered the ''out of memory'' problem and could not be executed. Consequently, only the performance of PCA, EMPCA, GPower l 1 , GPower l 0 , and RSPCA is involved for the following substantiation.</p><p>Our aim is to detect how well the images could be reconstructed by utilizing only a small number of PCs extracted from the employed methods. By taking the average reconstruction error (ARCE in brief <ref type="bibr" target="#b14">[15]</ref>) as the criterion, the quality of the reconstruction can be quantitatively assessed. The ARCE of each method with the first m PCs is calculated as <ref type="bibr" target="#b14">[15]</ref>: eðmÞ ¼ ð1=nÞ  <ref type="table" target="#tab_4">4</ref> and<ref type="table" target="#tab_5">5</ref> further list the summarizations of the performance of the five methods on the two utilized databases, respectively.</p><formula xml:id="formula_25">P n i ¼ 1 x org i À P m j ¼ 1 w j w T j x i k 2 ,</formula><p>For occluded case, it is seen from Fig. <ref type="figure">7</ref>(a) that when the number m of the extracted PCs is small, the ARCE value of the RSPCA is nonsubstantially larger than those of the other four methods. Yet from around m¼20, it is apparent that the RSPCA starts to be better than the other methods. From Fig. <ref type="figure" target="#fig_7">5</ref>, we can observe that when projected into the subspace constituted by the first 20 PCs yielded from different methods, the images reconstructed by the RSPCA eliminate the largest extent of noises of the original occluded images. Considering that ARCE curve of the RSPCA tends to be decreasing while those of the other methods incline to be increasing from m¼20, the advantage of the RSPCA is evident.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original face</head><p>Occuluded face PCA EMPCA GPowerl 1 GPowerl 0 RSPCA For dummy case, the similar phenomenon as the occluded case is observed from Fig. <ref type="figure">7</ref>(b): as m is small, the ARCE value of the RSPCA is a little larger than those of the other four methods, while from around m¼10, the better reconstruction capability of the RSPCA becomes apparent. This can be further substantiated by Fig. <ref type="figure">6</ref>: by projecting the images to the subspace spanned by the  first 30 PCs calculated by the employed methods, it is clear that the RSPCA best reconstructs the original images.</p><p>Furthermore, from Tables <ref type="table" target="#tab_4">4</ref> and<ref type="table" target="#tab_5">5</ref>, it is easy to see that in both cases, by extracting sparser PCs, RSPCA achieves smaller ARCE values and spends less computational cost (naturally conducted by less iteration steps) than the other utilized sparse PCA methods. These results further demonstrate the robustness and efficiency of the proposed RSPCA method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper we have proposed a new sparse PCA method, called RSPCA method, to enforce robustness of the sparse PCA calculation. The most distinguished characteristic of the new method is that it intends to find the PC directions of the feature space where the L 1 dispersion, instead of the L 2 -norm variance generally employed by the current sparse PCA methods, of the input data can be maximally captured. A simple algorithm to implement the RSPCA has also been developed, which has been proved to be able to converge to a local optimum of the problem. The robustness of the proposed method to outliers and noises has been supported by a series of experiments performed on the synthetic and face reconstruction problems. The efficiency of the method has also been theoretically and empirically substantiated.</p><p>There are, however, limitations of the proposed method. For example, the proposed RSPCA method only attains an approximation while not the rigorous solution to the original optimization problem (4), as aforementioned in Section 3.2. Endeavors still need to be made to design an effective and efficient algorithm to get the exact solution of (4) and hence to further enhance the performance of the robust sparse PCA. Besides, in our future research, the proposed method will be further evaluated by more practical applications.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Contents lists available at ScienceDirect journal homepage: www.elsevier.com/locate/pr Pattern Recognition 0031-3203/$ -see front matter &amp; 2011 Elsevier Ltd. All rights reserved. doi:10.1016/j.patcog.2011.07.009</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Graphical presentation of the exaggerative effect of the L 2 -norm variance curve, as compared with the L 1 -norm curve. In particular, at point x¼ 5, JxJ 2 2 is dominantly (five times) larger than JxJ 1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (a) The toy data points with two intrinsic outliers, and the first PCs yielded by applying the PCA, SPCA, DSPCA, PathSPCA, EMPCA, GPower l1 , GPower l0 , GPower l1 ,m , GPower l0 ,m , ALSPCA, and RSPCA methods to this data set. (b) Residual errors of the data by projecting them to the first PC calculated by the 11 methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. The tendency curves of the misidentification rate w.r.t. the noise extent s corresponding to the classical PCA method, nine current sparse PCA methods, and the RSPCA method, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.<ref type="bibr" target="#b3">4</ref>. The tendency curves of the misidentification rate w.r.t. the outlier extent s corresponding to the classical PCA method, nine current sparse PCA methods, and the RSPCA method, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>where n¼165 is the number of original face samples, x i org and x i are the i-th original image and the corresponding one in the occluded or dummy data sets, respectively, and m is the number of the involved PCs. Fig. 7(a) and (b) show the ARCE tendency curves of the employed five methods with various numbers of extracted PCs in occluded and dummy cases, respectively. Besides, Figs. 5 and 6 demonstrate some of the original and the reconstructed images by projecting the occluded and the dummy images into the subspaces constituted by the 20 and the 30 PCs calculated from the five employed methods, respectively. Tables</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The original face images, the corresponding images with occlusion, and the faces reconstructed by the classical PCA, EMPCA, GPower l1 , GPower l0 , and RSPCA methods with 20 corresponding projection PCs, respectively.</figDesc><graphic coords="8,78.80,400.73,428.09,308.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Fig. 6. Face images trained with dummy images and the faces reconstructed by the classical PCA, EMPCA, GPower l1 , GPower l0 , and RSPCA methods with 30 corresponding projection PCs, respectively.</figDesc><graphic coords="9,130.70,72.71,343.82,352.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Deyu</head><label></label><figDesc>Meng received his B.Sc., M.Sc., and Ph.D. degrees in 2001, 2004, and 2008, respectively, from Xi'an Jiaotong University, Xi'an, China. He is currently a lecturer with the Institute for Information and System Sciences, Faculty of Science, Xi'an Jiaotong University. His current research interests include principal component analysis, nonlinear dimensionality reduction, feature extraction and selection, compressed sensing, and sparse machine learning methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>If thereexists i such that w T ðtÞx i ¼ 0 and jsgnðw T ðtÞÞjjsgnðx i Þj a 0, then let w T ðtÞ þ Dw</figDesc><table><row><cell>8 &gt; &lt; &gt; :</cell><cell cols="2">d. Here ðxÞ þ ¼ 1,x 40 0,x ¼ 0 À1,x o 0 denote the thresholding and sign func-( x,x 4 0 and sgnðxÞ ¼ 0,x r 0</cell></row><row><cell cols="3">tions, respectively. Set wðt þ 1Þ ¼ b JbJ 2 , and t ¼ t þ1.</cell></row><row><cell cols="2">(4) Convergence check:</cell></row><row><cell cols="3">(4.1) If wðtÞ a wðtÀ1Þ, go to step (2); otherwise,</cell></row><row><cell></cell><cell>check (4.2).</cell></row><row><cell cols="2">(4.2) Jw T ðtÞ þ DwJ 2</cell><cell>and</cell></row><row><cell></cell><cell cols="2">go to step (2); otherwise, go to (4.3). Here Dw</cell></row><row><cell></cell><cell>is a small non-zero random vector.</cell></row><row><cell cols="2">(4.3) Set w n ¼ wðtÞ and stop iteration.</cell></row><row><cell cols="2">Output: The k sparse PC w n .</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Performance comparison of 11 methods, including the classical PCA method, nine current sparse PCA methods, and the proposed RSPCA method, by applying them to the toy data with intrinsic two outliers. The iteration time t 1 þ t 2 of a method denotes that it needs t 1 and t 2 iterations to compute PC1 and PC2, respectively.</figDesc><table><row><cell>Methods</cell><cell>PC1</cell><cell>PC2</cell><cell>ARSE</cell><cell>Iteration times</cell></row><row><cell>PCA</cell><cell cols="4">(0.6811,0.7322) ( À 0.7322,0.6811) 1.0155 0</cell></row><row><cell>SPCA</cell><cell>(0, À 1)</cell><cell>(1,0)</cell><cell cols="2">1.2500 4</cell></row><row><cell>DSPCA</cell><cell>(0,1)</cell><cell>( À 1,0)</cell><cell cols="2">1.2500 8 þ4</cell></row><row><cell>PathSPCA</cell><cell>(0,1)</cell><cell>(1,0)</cell><cell cols="2">1.2500 1 þ1</cell></row><row><cell>EMPCA</cell><cell>(0,1)</cell><cell>(1,0)</cell><cell cols="2">1.2500 2 þ1</cell></row><row><cell>GPower l1</cell><cell>(0,1)</cell><cell>(1,0)</cell><cell cols="2">1.2500 3 þ3</cell></row><row><cell>GPower l0</cell><cell>(0,1)</cell><cell>(1,0)</cell><cell cols="2">1.2500 3 þ3</cell></row><row><cell>GPower l1 ,m</cell><cell>(0,1)</cell><cell>(0.9945,0.1048)</cell><cell cols="2">1.2500 5</cell></row><row><cell>GPower l0 ,m</cell><cell cols="2">(0.6766,0.7363) (0,0)</cell><cell cols="2">1.0188 11</cell></row><row><cell>ALSPCA</cell><cell>(0,1)</cell><cell>(1, À 0.003)</cell><cell cols="2">1.2500 3</cell></row><row><cell>RSPCA</cell><cell>(1,0)</cell><cell>(0,1)</cell><cell cols="2">0.5720 2 þ2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Performance comparison of the classical PCA method, nine current sparse PCA methods, and the proposed RSPCA method by applying them to the benchmark data with noise extent s ¼ 2000. Here IT denotes the iteration time of the corresponding method on calculating the PCs.</figDesc><table><row><cell>PCA</cell><cell></cell><cell></cell><cell>SPCA</cell><cell></cell><cell>DSPCA</cell><cell></cell><cell>PathSPCA</cell><cell></cell><cell></cell><cell>EMPCA</cell><cell></cell></row><row><cell>PC1</cell><cell>PC2</cell><cell></cell><cell>PC1</cell><cell>PC2</cell><cell>PC1</cell><cell>PC2</cell><cell>PC1</cell><cell>PC2</cell><cell></cell><cell>PC1</cell><cell>PC2</cell></row><row><cell>À 0.1314</cell><cell cols="2">À 0.4858</cell><cell>0.5290</cell><cell>0</cell><cell>0</cell><cell>À 0.8513</cell><cell>0</cell><cell cols="2">À 0.5176</cell><cell>0</cell><cell>À 0.5013</cell></row><row><cell>À 0.0761</cell><cell cols="2">À 0.4850</cell><cell>0.4878</cell><cell>0</cell><cell>0</cell><cell>À 0.4320</cell><cell>0</cell><cell cols="2">À 0.4908</cell><cell>0</cell><cell>À 0.5081</cell></row><row><cell>À 0.0853</cell><cell cols="2">À 0.4824</cell><cell>0.5014</cell><cell>0</cell><cell>0</cell><cell>À 0.2979</cell><cell>0</cell><cell cols="2">À 0.5020</cell><cell>0</cell><cell>À 0.5174</cell></row><row><cell>À 0.0849</cell><cell cols="2">À 0.4676</cell><cell>0.4805</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell cols="2">À 0.4891</cell><cell>0</cell><cell>À 0.4720</cell></row><row><cell>0.4011</cell><cell cols="2">À 0.1450</cell><cell>0</cell><cell>0.5522</cell><cell>0.6121</cell><cell>0</cell><cell>À 0.5049</cell><cell>0</cell><cell></cell><cell>0</cell><cell>0</cell></row><row><cell>0.3968</cell><cell cols="2">À 0.1094</cell><cell>0</cell><cell>0.3149</cell><cell>0.1301</cell><cell>0</cell><cell>À 0.4889</cell><cell>0</cell><cell></cell><cell>À 0.5017</cell><cell>0</cell></row><row><cell>0.4146</cell><cell cols="2">À 0.1430</cell><cell>0</cell><cell>0.7386</cell><cell>0.7776</cell><cell>0</cell><cell>À 0.5253</cell><cell>0</cell><cell></cell><cell>À 0.5422</cell><cell>0</cell></row><row><cell>0.3809</cell><cell cols="2">À 0.1432</cell><cell>0</cell><cell>0.2242</cell><cell>0.0613</cell><cell>0</cell><cell>À 0.4797</cell><cell>0</cell><cell></cell><cell>À 0.4768</cell><cell>0</cell></row><row><cell>0.4070</cell><cell cols="2">0.0482</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell></cell><cell>0</cell><cell>0</cell></row><row><cell>0.4020</cell><cell cols="2">0.0351</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell></cell><cell>À 0.4764</cell><cell>0</cell></row><row><cell></cell><cell></cell><cell></cell><cell>IT: 110</cell><cell></cell><cell>IT: 15þ 12</cell><cell></cell><cell>IT: 4þ 4</cell><cell></cell><cell></cell><cell>IT: 341 þ48</cell><cell></cell></row><row><cell>GPower l1</cell><cell></cell><cell>GPower l0</cell><cell></cell><cell>GPower l1 ,m</cell><cell></cell><cell>GPower l0 ,m</cell><cell></cell><cell>ALSPCA</cell><cell></cell><cell>RSPCA</cell><cell></cell></row><row><cell>PC1</cell><cell>PC2</cell><cell>PC1</cell><cell>PC2</cell><cell>PC1</cell><cell>PC2</cell><cell>PC1</cell><cell>PC2</cell><cell>PC1</cell><cell>PC2</cell><cell>PC1</cell><cell>PC2</cell></row><row><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.1231</cell><cell>0.4883</cell><cell>0.0487</cell><cell>0.4854</cell><cell>0</cell><cell>0.9949</cell><cell>0</cell><cell>0.4952</cell></row><row><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.0679</cell><cell>0.4864</cell><cell>0.0539</cell><cell>0.4933</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.4662</cell></row><row><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.0771</cell><cell>0.4840</cell><cell>0.0641</cell><cell>0.4803</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.5383</cell></row><row><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.0769</cell><cell>0.4692</cell><cell>0.0289</cell><cell>0.5019</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.4977</cell></row><row><cell>0</cell><cell>1</cell><cell>0</cell><cell>1</cell><cell>À 0.4035</cell><cell>0.1368</cell><cell>À 0.4147</cell><cell>0.1098</cell><cell>0</cell><cell>0</cell><cell>0.5504</cell><cell>0</cell></row><row><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>À 0.3986</cell><cell>0.1013</cell><cell>À 0.3967</cell><cell>0.0899</cell><cell>0</cell><cell>0</cell><cell>0.3476</cell><cell>0</cell></row><row><cell>1</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>À 0.4169</cell><cell>0.1345</cell><cell>À 0.4165</cell><cell>0.0598</cell><cell>À 1</cell><cell>0</cell><cell>0.6621</cell><cell>0</cell></row><row><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>À 0.3832</cell><cell>0.1354</cell><cell>À 0.4023</cell><cell>0.0851</cell><cell>0</cell><cell>0.1008</cell><cell>0.3712</cell><cell>0</cell></row><row><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>À 0.4061</cell><cell>À 0.0564</cell><cell>À 0.4155</cell><cell>À 0.0527</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>À 0.4014</cell><cell>À 0.0433</cell><cell>À 0.3905</cell><cell>À 0.0680</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>IT: 3 þ3</cell><cell></cell><cell>IT: 3þ 3</cell><cell></cell><cell>IT: 17</cell><cell></cell><cell>IT: 16</cell><cell></cell><cell>IT: 7</cell><cell></cell><cell>IT: 2 þ2</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>Performance comparison of the classical PCA method, nine current sparse PCA methods, and the proposed RSPCA method by applying them to the benchmark data with outlier extent s ¼ 3000.</figDesc><table><row><cell>PCA</cell><cell></cell><cell></cell><cell>SPCA</cell><cell></cell><cell>DSPCA</cell><cell></cell><cell>PathSPCA</cell><cell></cell><cell></cell><cell>EMPCA</cell><cell></cell></row><row><cell>PC1</cell><cell>PC2</cell><cell></cell><cell>PC1</cell><cell>PC2</cell><cell>PC1</cell><cell>PC2</cell><cell>PC1</cell><cell>PC2</cell><cell></cell><cell>PC1</cell><cell>PC2</cell></row><row><cell>À 0.1120</cell><cell cols="2">À 0.4786</cell><cell>0.5003</cell><cell>0</cell><cell>0</cell><cell>À 0.6777</cell><cell>À 0.0311</cell><cell>0</cell><cell></cell><cell>0</cell><cell>À 0.5002</cell></row><row><cell>À 0.1122</cell><cell cols="2">À 0.4785</cell><cell>0.5003</cell><cell>0</cell><cell>0</cell><cell>À 0.7092</cell><cell>0</cell><cell>0</cell><cell></cell><cell>0</cell><cell>À 0.5002</cell></row><row><cell>À 0.1121</cell><cell cols="2">À 0.4783</cell><cell>0.4999</cell><cell>0</cell><cell>0</cell><cell>À 0.1942</cell><cell>0</cell><cell>0</cell><cell></cell><cell>0</cell><cell>À 0.4999</cell></row><row><cell>À 0.1120</cell><cell cols="2">À 0.4782</cell><cell>0.4995</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell cols="2">À 0.0267</cell><cell>0</cell><cell>À 0.4997</cell></row><row><cell>0.3841</cell><cell cols="2">À 0.1453</cell><cell>0</cell><cell>0.0155</cell><cell>0.0773</cell><cell>0</cell><cell>0.4574</cell><cell>0</cell><cell></cell><cell>À 0.4881</cell><cell>0</cell></row><row><cell>0.3843</cell><cell cols="2">À 0.1452</cell><cell>0</cell><cell>0.0223</cell><cell>0.0784</cell><cell>0</cell><cell>0.3574</cell><cell cols="2">0.5477</cell><cell>À 0.4883</cell><cell>0</cell></row><row><cell>0.3843</cell><cell cols="2">À 0.1448</cell><cell>0</cell><cell>0.0185</cell><cell>0.0785</cell><cell>0</cell><cell>0</cell><cell cols="2">À 0.7999</cell><cell>À 0.4882</cell><cell>0</cell></row><row><cell>0.3837</cell><cell cols="2">À 0.1455</cell><cell>0</cell><cell>0</cell><cell>0.0748</cell><cell>0</cell><cell>À 0.8137</cell><cell cols="2">0.2439</cell><cell>0</cell><cell>0</cell></row><row><cell>0.4254</cell><cell cols="2">0.0101</cell><cell>0</cell><cell>0.9995</cell><cell>0.9250</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell></cell><cell>À 0.5339</cell><cell>0</cell></row><row><cell>0.4227</cell><cell cols="2">0.0102</cell><cell>0</cell><cell>0</cell><cell>0.3471</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell></cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell></cell><cell></cell><cell>IT: 10</cell><cell></cell><cell>IT: 17þ 13</cell><cell></cell><cell>IT: 4þ 4</cell><cell></cell><cell></cell><cell>IT: 48þ 4</cell><cell></cell></row><row><cell>GPower l1</cell><cell></cell><cell>GPower l0</cell><cell></cell><cell>GPower l1 ,m</cell><cell></cell><cell>GPower l0 ,m</cell><cell></cell><cell>ALSPCA</cell><cell></cell><cell>RSPCA</cell><cell></cell></row><row><cell>PC1</cell><cell>PC2</cell><cell>PC1</cell><cell>PC2</cell><cell>PC1</cell><cell>PC2</cell><cell>PC1</cell><cell>PC2</cell><cell>PC1</cell><cell>PC2</cell><cell>PC1</cell><cell>PC2</cell></row><row><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>À 0.3717</cell><cell>À 0.3070</cell><cell>0</cell><cell>0.0001</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.4998</cell></row><row><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.2652</cell><cell>0.3032</cell><cell>0.0004</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.5006</cell></row><row><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>À 0.3988</cell><cell>À 0.4566</cell><cell>À 0.0005</cell><cell>0</cell><cell>0</cell><cell>0.8986</cell><cell>0</cell><cell>0.5000</cell></row><row><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.2663</cell><cell>0.3039</cell><cell>0.0001</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.4995</cell></row><row><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.2090</cell><cell>À 0.2508</cell><cell>À 0.0001</cell><cell>0.0001</cell><cell>0</cell><cell>0</cell><cell>0.5013</cell><cell>0</cell></row><row><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>À 0.1148</cell><cell>0.3606</cell><cell>0.0001</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.5036</cell><cell>0</cell></row><row><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.2083</cell><cell>À 0.2512</cell><cell>À 0.0001</cell><cell>À 0.0002</cell><cell>0</cell><cell>0</cell><cell>0.5016</cell><cell>0</cell></row><row><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.2085</cell><cell>À 0.2511</cell><cell>0.0001</cell><cell>0</cell><cell>0</cell><cell>0.4387</cell><cell>0.4934</cell><cell>0</cell></row><row><cell>1</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>À 0.6331</cell><cell>0.4264</cell><cell>0.7137</cell><cell>0.6985</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>0</cell><cell>1</cell><cell>0</cell><cell>1</cell><cell>À 0.1309</cell><cell>À 0.1097</cell><cell>0.7004</cell><cell>À 0.7156</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>IT: 3 þ3</cell><cell></cell><cell>IT: 3þ 3</cell><cell></cell><cell>IT: 4</cell><cell></cell><cell>IT: 4</cell><cell></cell><cell>IT: 5</cell><cell></cell><cell>IT: 2 þ2</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>Performance comparison of the classical PCA, EMPCA, GPower l1 , GPower l0 , and RSPCA methods by applying them to the occluded face database. The ARCE and computation time of each method are evaluated and recorded at m¼40.</figDesc><table><row><cell cols="2">Methods Average</cell><cell>ARCE</cell><cell>Computation</cell><cell>IT (the first</cell></row><row><cell></cell><cell>sparsity</cell><cell></cell><cell>time</cell><cell>three PCs)</cell></row><row><cell>PCA</cell><cell>36,765</cell><cell>6384.9971</cell><cell>0.417860</cell><cell>0</cell></row><row><cell>EMPCA</cell><cell>18,000</cell><cell cols="2">6726.0090 1415.837026</cell><cell>110 þ30 þ32 ¼172</cell></row><row><cell cols="2">GPower l1 22,836.125</cell><cell>6531.1172</cell><cell>58.448949</cell><cell>24 þ10 þ19 ¼53</cell></row><row><cell cols="2">GPower l0 22,781.85</cell><cell>6494.5514</cell><cell>56.339795</cell><cell>15 þ11 þ22 ¼ 48</cell></row><row><cell>RSPCA</cell><cell>18,000</cell><cell>5829.2719</cell><cell>34.795060</cell><cell>10þ 17 þ12 ¼39</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc>Performance comparison of the classical PCA, EMPCA, GPower l1 ; GPower l0 , and RSPCA methods by applying them to the dummy face database. The ARCE and computation time of each method are evaluated and recorded at m¼40.</figDesc><table><row><cell cols="2">Methods Average</cell><cell>ARCE</cell><cell>Computation</cell><cell>IT (the first</cell></row><row><cell></cell><cell>sparsity</cell><cell></cell><cell>time</cell><cell>three PCs)</cell></row><row><cell>PCA</cell><cell>36,765</cell><cell>6223.0231</cell><cell>0.488258</cell><cell>0</cell></row><row><cell>EMPCA</cell><cell>18,000</cell><cell cols="2">6665.7251 7114.826249</cell><cell>52þ 38 þ47 ¼137</cell></row><row><cell cols="2">GPower l1 24,961.775</cell><cell>6315.7237</cell><cell>82.327144</cell><cell>16þ 14 þ18 ¼48</cell></row><row><cell cols="2">GPower l0 24,946.675</cell><cell>6295.4676</cell><cell>60.607400</cell><cell>14þ 12 þ16 ¼42</cell></row><row><cell>RSPCA</cell><cell>18,000</cell><cell>5507.8358</cell><cell>39.602456</cell><cell>9þ 7þ 5¼ 21</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>It should be noted that the similar idea, i.e., substituting the L 1 -norm objective for the L</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>-norm one, has been employed by multiple machine learning algorithms to enhance the robustness of the related problems, such as the robust face recognition algorithm proposed in<ref type="bibr" target="#b21">[22]</ref> and the robust PCA algorithm proposed in<ref type="bibr" target="#b22">[23]</ref>. In this sense, these algorithms, including the proposed algorithm, are related to each other to a certain extent. D. Meng et al. / Pattern Recognition 45 (2012) 487-497</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>D. Meng et al. / Pattern Recognition 45 (2012) 487-497</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by the China NSFC project under Contract 60905003 and the National Grand Fundamental Research 973 Program of China under Grant no. 2007CB311002.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Supplementary data</head><p>Supplementary data associated with this article can be found in the online version at doi:10.1016/j.patcog.2011.07.009.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Principal Component Analysis</title>
		<author>
			<persName><forename type="first">I</forename><surname>Jollife</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<publisher>Springer Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sparse principal component analysis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="265" to="286" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A direct formulation for sparse PCA using semidefinite programming</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aspremont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">El</forename><surname>Ghaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Lanckriet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<date type="published" when="2005">2005</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generalized power method for sparse principal component analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Journe ´e</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Richta ´rik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sepulchre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="451" to="487" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Expectation maximization for sparse and nonnegative PCA</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Sigg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Buhmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning</title>
		<meeting>the 25th International Conference on Machine Learning<address><addrLine>Helsinki, Finland</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="960" to="967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An augmented lagrangian approach for sparse principal component analysis</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10107-011-0452-4</idno>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spectral bounds for sparse PCA: exact and greedy algorithms</title>
		<author>
			<persName><forename type="first">B</forename><surname>Moghaddam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="915" to="922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Optimal solutions for sparse principal component analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aspremont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Ghaoui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1269" to="1294" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A modified principal component technique based on the lasso</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">T</forename><surname>Jollife</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Uddin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="531" to="547" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rotation of principal components: choice of normalization constraints</title>
		<author>
			<persName><forename type="first">I</forename><surname>Jollife</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="29" to="35" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Loadings and correlations in the interpretation of principal components</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cadima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Jollife</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="203" to="214" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sparse principal component analysis via regularized low rank matrix approximation</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Multivariate Analysis</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="1015" to="1034" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sparse eigen methods by D.C. programming</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R G</forename><surname>Lanckriet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Machine learning</title>
		<meeting>the 24th International Conference on Machine learning<address><addrLine>Corvallis, OR, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="831" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Nonnegative sparse PCA</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1561" to="1568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Principal component analysis based on L1-norm maximization</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1672" to="1680" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A framework for robust subspace learning</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>La Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="117" to="142" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Robust factorization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Aanas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fisker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Astrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carstensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1215" to="1225" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">R1-PCA: rotational invariant L1-norm principal component analysis for robust subspace factorization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Machine Learning</title>
		<meeting>the 23rd International Conference on Machine Learning<address><addrLine>Pittsburgh, PA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="281" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A L1-norm PCA and a heuristic approach, Ordinal and Symbolic Data Analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Baccini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Besse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Falguerolles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="359" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Robust L1 norm factorization in the presence of outliers and missing data by alternative convex programming</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, IEEE</title>
		<meeting>the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, IEEE<address><addrLine>San Diego, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="739" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Eigenfaces versus Fisherfaces: recognition using class specific linear projection</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Hespanha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="711" to="720" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Robust face recognition via sparse representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="210" to="217" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Robust principal component analysis?</title>
		<author>
			<persName><forename type="first">E</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<idno type="DOI">10.1145/1970392.1970395</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">where he is currently working toward the M.Sc. degree. His current research interests include feature extraction and selection, dimensionality reduction, and compressed sensing</title>
	</analytic>
	<monogr>
		<title level="m">2009 from Xi&apos;an Jiaotong University</title>
		<meeting><address><addrLine>Xi&apos;an, China</addrLine></address></meeting>
		<imprint>
			<publisher>Qian Zhao received his B.Sc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">1981 and 1987, respectively. In 1988, he was a Postdoctoral Researcher with the Department of Mathematics, The University of Strathclyde, Glasgow, UK. He was a research fellow with the Information Engineering Department from February 1992 to March 1994, the center for environmental studies from April 1995 to August 1995, and the mechanical engineering and automation department from September</title>
		<author>
			<persName><forename type="first">Hong</forename><surname>Shatin</surname></persName>
		</author>
		<author>
			<persName><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Zongben Xu received his M.Sc. degree in mathematics and the Ph.D. degree in applied mathematics from Xi&apos;an Jiaotong University</title>
		<meeting><address><addrLine>Xi&apos;an, China; Kowloon, Hong Kong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-04">1996 to October 1996. January 1995 to April 1995</date>
		</imprint>
		<respStmt>
			<orgName>The Chinese University of Hong Kong ; The Hong Kong Polytechnic University</orgName>
		</respStmt>
	</monogr>
	<note>He is currently a professor with the Institute for Information and System Sciences, Faculty of Science, Xi&apos;an Jiaotong University. His current research interests include manifold learning. neural networks, evolutionary computation, and multiple-objective decision-making theory</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
