<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Positional Relevance Model for Pseudo-Relevance Feedback</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuanhua</forename><surname>Lv</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign Urbana</orgName>
								<address>
									<postCode>61801</postCode>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
							<email>czhai@cs.uiuc.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign Urbana</orgName>
								<address>
									<postCode>61801</postCode>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Positional Relevance Model for Pseudo-Relevance Feedback</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">80CF3F739D616F1826963663039A56A6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.3 [Information Search and Retrieval]: Retrieval models</term>
					<term>Relevance feedback</term>
					<term>Query formulation Positional relevance model</term>
					<term>pseudo-relevance feedback</term>
					<term>positional language model</term>
					<term>proximity</term>
					<term>passage-based feedback</term>
					<term>query expansion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pseudo-relevance feedback is an effective technique for improving retrieval results. Traditional feedback algorithms use a whole feedback document as a unit to extract words for query expansion, which is not optimal as a document may cover several different topics and thus contain much irrelevant information. In this paper, we study how to effectively select from feedback documents those words that are focused on the query topic based on positions of terms in feedback documents. We propose a positional relevance model (PRM) to address this problem in a unified probabilistic way. The proposed PRM is an extension of the relevance model to exploit term positions and proximity so as to assign more weights to words closer to query words based on the intuition that words closer to query words are more likely to be related to the query topic. We develop two methods to estimate PRM based on different sampling processes. Experiment results on two large retrieval datasets show that the proposed PRM is effective and robust for pseudo-relevance feedback, significantly outperforming the relevance model in both document-based feedback and passage-based feedback.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Pseudo-relevance feedback (or blind feedback) is an important general technique for improving retrieval accuracy <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b32">32]</ref>. The basic idea of pseudo-relevance feedback is to assume that a small number of top-ranked documents in the initial retrieval results are relevant and select from these documents related terms to the query to improve the query representation through query expansion, which generally leads to improvement of retrieval performance.</p><p>Most existing feedback algorithms (e.g., <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b32">32]</ref>) use a whole feedback document as a unit for selecting expansion terms, which, however, is non-optimal when the content of a document is incoherent (i.e., covering several different topics) and thus may contain much irrelevant information as often happens in Web search. The existence of multiple topics and irrelevant information would lead to a noisy feedback model as potentially harmful terms from non-relevant topics may be picked up to include in the feedback model. As a result, the use of pseudo feedback may not improve or even decrease the retrieval performance. Thus a critical challenge in improving all feedback methods is to effectively select from feedback documents those terms that are most likely relevant to the query topic.</p><p>In this paper, we solve this challenge by exploiting the position and proximity information of terms as cues to assess if a term is related to the query topic. Since topically related content is usually grouped together in text documents, terms closer to the occurrences of query words are, in general, more likely relevant to the query topic, thus a good feedback model should intuitively place higher weights on such terms.</p><p>Based on this intuition, we propose a novel positional relevance model (PRM) to incorporate the cues of term positions and term proximity in a probabilistic feedback model based on statistical language modeling. The key idea is to extend the relevance model <ref type="bibr" target="#b16">[16]</ref> to aggregate the associations between a term and query words at the position level via the positional language model (PLM) <ref type="bibr" target="#b19">[19]</ref>. An important advantage of estimating a relevance model based on PLM is that it can model the "relevant positions" in a feedback document with probabilistic models so as to assign more weights to terms at more relevant positions in a principled way, thus leading naturally to selection of expansion terms more likely relevant to the query topic.</p><p>Since PRM estimates a relevance model at the level of term positions, it incorporates individual term positions directly into a probabilistic model. This is in contrast with virtually all the existing pseudo feedback techniques which have only made use of term statistics at the document level <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b18">18]</ref>, or at the best, at the level of passages <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b21">21]</ref> without distinguishing every different position.</p><p>Analogously to the two methods proposed for estimating the relevance model <ref type="bibr" target="#b16">[16]</ref>, we also derive two methods for estimating PRM, leading to two different ways to aggregate term information based on positions. We evaluate the proposed PRM on two large TREC datasets. Experimental results demonstrate that PRM is effective in exploiting term proximity for pseudo feedback and significantly outperforms the relevance model in both document-based feedback and passage-based feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Pseudo-Relevance Feedback</head><p>Pseudo-relevance feedback has been shown to be effective with various retrieval models <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b18">18]</ref>.</p><p>In the vector space model, feedback is usually done by using the Rocchio algorithm, which forms a new query vector by maximizing its similarity to pseudo-relevant documents <ref type="bibr" target="#b26">[26]</ref>. The feedback method in classical probabilistic models is to select expansion terms primarily based on Robertson/Sparck-Jones weight <ref type="bibr" target="#b24">[24]</ref>.</p><p>Several query expansion techniques have been developed in the language modeling framework, including, e.g., the mixture-model feedback method <ref type="bibr" target="#b32">[32]</ref> and the relevance model <ref type="bibr" target="#b16">[16]</ref>. The basic idea is to use feedback documents to estimate a better query language model. Both the mixture model and relevance model have been shown to be very effective, but the relevance model appears to be more robust <ref type="bibr" target="#b18">[18]</ref>.</p><p>In the mixture-model feedback <ref type="bibr" target="#b32">[32]</ref>, the words in feedback documents are assumed to be drawn from two models:</p><p>(1) background model and (2) topic model. The mixturemodel feedback finds the topic model that best describes the feedback documents by separating the topic model from the background model. The topic model is then interpolated with the original query model to form the expanded query.</p><p>Much like mixture-model feedback, the relevance model also estimates an improved query language model. Given a query Q, a relevance model is a multinomial distribution P (w|Q) that encodes the likelihood of each term w given the query as evidence. To estimate the relevance model, the authors first compute the joint probability of observing a word together with the query words in each feedback document and then aggregate the evidence by summing over all the documents. It essentially uses the query likelihood P (Q|D) as the weight for document D and takes an average of the probability of word w given by each document language model. All these pseudo feedback algorithms use a whole feedback document as a unit, and thus term position and proximity evidences are largely ignored. Our work is an extension of the relevance model to estimate a feedback model based on individual term positions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Passage Feedback</head><p>There have been several studies to exploit passage-level evidence of documents for feedback, e.g., <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b17">17]</ref>, which can potentially address the heterogeneous topical structure of documents to some degree. However, these approaches usually take a traditional feedback model as a black box to handle sub-document units as if they were regular documents. For example, Liu and Croft's work <ref type="bibr" target="#b17">[17]</ref> estimates a relevance model based on the best matching passage of each feedback document, where fixed-length arbitrary passages that resemble overlapped windows but with an arbitrary starting point <ref type="bibr" target="#b12">[12]</ref> can often be used due to its effectiveness and efficiency <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b17">17]</ref>. A limitation of this approach is that term positions are not directly incorporated into the feedback model. As we will show later in the paper, the proposed PRM outperforms such a passage feedback approach. Some other approaches, e.g., <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b4">4]</ref>, make use of visual cues or eye tracker to improve passage feedback for web search: on the server side of a search engine, documents can be decomposed into topically different components via visual cues <ref type="bibr" target="#b31">[31]</ref>, while on the client side of users, gaze-based attention feedback <ref type="bibr" target="#b4">[4]</ref> can go down to the sub-document level by exploiting evidence about which document parts the user looks at. However, such approaches face the same problems as general passage feedback without being able to model each individual position.</p><p>In fact, these passage-based or sub-document level feedback models are orthogonal to the proposed PRM in the sense that PRM can be applied to passages to model proximity inside a passages in the same way as it can be applied to whole documents. Moreover, the underlying positional language model <ref type="bibr" target="#b19">[19]</ref>, which can capture passage-level evidence in a soft way in model estimation, has been shown to work better than imposing a "hard" boundary of passages.</p><p>Recently, Metzler and Croft's work on Latent Concept Expansion <ref type="bibr" target="#b21">[21]</ref> also indirectly captures term position and proximity evidence through the use of appropriate passages. Their work provides a more general model which is complementary with our ideas in that we can use PRM as an effective feature defined on their graph, so our PRM scores can then be combined with other features explored in <ref type="bibr" target="#b21">[21]</ref> to further improve its performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Term Proximity Heuristic in IR</head><p>The term proximity heuristic, which rewards a document where the matched query terms occur close to each other, has been previously studied in <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b34">34</ref>]. Keen's work <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b14">14]</ref> is among the earliest efforts, in which, a "NEAR" operator was introduced to address proximity in Boolean retrieval model. The shortest interval containing a match set was first used as a measure of proximity in <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b11">11]</ref>. Recent work has attempted to heuristically incorporate proximity into an existing retrieval model (often through score combinations) <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b28">28]</ref>. For example, a variety of proximity measures were proposed and explored in <ref type="bibr" target="#b28">[28]</ref>. Another work <ref type="bibr" target="#b10">[10]</ref> used a learning approach to combine various proximity measures to obtain an effective proximity-based retrieval function. Recently in our previous work <ref type="bibr" target="#b19">[19]</ref>, we proposed a positional language model (PLM) for information retrieval, which not only captured term proximity information but also covered passage retrieval in a unified language modeling approach. However, in all these studies, term proximity has been solely used for ranking documents in response to a given query rather than improving pseudo feedback.</p><p>There has been relatively little work done in the area of formally modeling term proximity heuristic in the context of pseudo feedback. However, there have been several attempts to simply combine term proximity with other feedback heuristics to select good expansion terms. In <ref type="bibr" target="#b29">[29]</ref>, several distance functions were evaluated for selecting query expansion terms from windows or passages surrounding query term occurrences; however, no improvement was observed as compared to existing feedback methods. Cao et al. <ref type="bibr">[7]</ref> used a supervised method to classify whether an individual expansion term is good or not, in which term proximity is one of their features. Their method only loosely combined term proximity with traditional feedback heuristics; in contrast, we incorporate term position and proximity into a probabilistic feedback model with more meaningful parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">POSITIONAL RELEVANCE MODEL</head><p>In this section, we describe the proposed positional relevance model (PRM) which incorporates term position information into the estimation of feedback models so that we can naturally reward terms close to query terms in the feedback documents and avoid including irrelevant terms in the feedback model.</p><p>The proposed PRM can be regarded as an extension to the relevance model (RM) <ref type="bibr" target="#b16">[16]</ref>. We thus first give a brief introduction to the relevance model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Relevance Models</head><p>As a pseudo feedback method, the relevance model <ref type="bibr" target="#b16">[16]</ref> has proven to be not only effective, but also robust in a recent study <ref type="bibr" target="#b18">[18]</ref>. The basic idea is to use the query likelihood score of a feedback document as the weight and estimate a query language model (for feedback) based on weighted aggregation of term counts in the feedback documents.</p><p>Formally, let Q = {q 1 , q 2 , • • • , q m } be a query and Θ represent the set of smoothed document models for the pseudo feedback documents. One of the most robust variants of the relevance model (RM1) <ref type="bibr" target="#b18">[18]</ref> is computed as follows <ref type="bibr" target="#b16">[16]</ref>:</p><formula xml:id="formula_0">P (w|Q) ∝ θ D ∈Θ P (w|θ D )P (θ D ) m i=1 P (q i |θ D )<label>(1)</label></formula><p>where p(θ D ) is a prior on documents and is often assumed to be uniform without any additional prior knowledge about document D. Thus, the estimated relevance model is essentially a weighted combination of individual feedback document model with the query likelihood score of a document as its weight.</p><p>After the relevance model is estimated, the estimated P (w|Q) can then be interpolated with the original query model θQ to improve performance <ref type="bibr" target="#b1">[1]</ref>.</p><formula xml:id="formula_1">P (w|θ Q ) = (1 -α)P (w|θ Q ) + αP (w|Q) (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where α is a parameter to control the amount of feedback.</p><p>In the rest of the paper, we will refer to this instantiation of relevance model as RM3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Positional Relevance Models</head><p>In the relevance model, the count of a term is computed over an entire feedback document. The main idea of the proposed positional relevance model (PRM) is to further distinguish different positions of a term and discount the occurrences of a term at positions that are far away from a query term in a feedback document.</p><p>Similarly to RM, a PRM is also a multinomial distribution P (w|Q) that attempts to capture the probability that term w is seen in a relevant document. However, PRM goes beyond RM to estimate the conditional probability P (w|Q) in terms of the joint probability of observing w with the query Q at every position in every feedback document. Formally,</p><formula xml:id="formula_3">P (w|Q) = P (w, Q) P (Q) ∝ P (w, Q) = D∈F |D| i=1 P (w, Q, D, i) (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>where i indicates a position in document D, and F is the set of feedback documents (assumed to be relevant).</p><p>The challenge now lies in estimating the joint probability P (w, Q, D, i). Inspired by the two estimation methods proposed in <ref type="bibr" target="#b16">[16]</ref> for estimating relevance models, we derive two methods similarly for estimating P (w, Q, D, i). The first method assumes that w is sampled in the same way as Q, while the second method assumes that w and Q are sampled using two different mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Method 1: i.i.d. sampling</head><p>In this method, we first compute the joint probability of observing a word together with the query words at each position and then aggregate the evidence by summing over all the possible positions. Specifically, we factor the joint probability P (w, Q, D, i) for each pseudo-relevant document D as follows:</p><formula xml:id="formula_5">P (w, Q, D, i) = P (D)P (i|D)P (w, Q|D, i) (4)</formula><p>Intuitively, we have assumed a generative model in which we would first pick a document according to P (D), then choose a position i in document D with probability P (i|D), and finally generate word w and query Q conditioned on D and i, with probability P (w, Q|D, i). P (D) can be interpreted as a document prior and set to a uniform distribution with no prior knowledge about document D. While it is possible to estimate P (i|D) based on document structures, here we assume that every position is equally likely, i.e., P (i|D) = 1  |D| . Improving the estimation of p(D) and P (i|D) would be an interesting future work. An illustration of the dependencies between the variables involved in the derivation is shown on Figure <ref type="figure" target="#fig_0">1</ref> (left side).</p><p>After making these assumptions and a further assumption that the generation of word w and that of query Q are independent, we have</p><formula xml:id="formula_6">P (w, Q, D, i) ∝ P (w, Q|D, i) |D| = P (Q|D, i)P (w|D, i) |D| (5)</formula><p>Plugging Equation <ref type="formula">5</ref>into Equation <ref type="formula" target="#formula_3">3</ref>, we obtain the following estimate of the PRM:</p><formula xml:id="formula_7">P (w|Q) ∝ P (w, Q) ∝ D∈F |D| i=1 P (Q|D, i)P (w|D, i) |D| (6)</formula><p>In the above equation, P (w|D, i) is the probability of sampling word w at position i in document D. To improve the efficiency of PRM, we simplify P (w|D, i) as:</p><formula xml:id="formula_8">P (w|D, i) = 1.0 if w occurs at position i in D 0.0 otherwise (7)</formula><p>The term P (Q|D, i) in Equation <ref type="formula">6</ref>is the key component in estimating the positional relevance model. It is the query likelihood at position i of document D, and we will discuss how to estimate it based on the positional language model <ref type="bibr" target="#b19">[19]</ref> in Section 3.2.3. Additionally, there is a third term |D| in the equation, which penalizes long documents to prevent them from dominating the feedback model (long documents naturally have more positions).</p><p>Thus, Equation <ref type="formula">6</ref>essentially combines all terms in feedback documents by assigning different weights to each term:</p><p>(1) P (Q|D, i) serves as a relevance-based weight for each position in each document so that a position with many query terms nearby would have a higher weight. Thus as an intra-document weight, P (Q|D, i) can measure the relative weights of positions within a document: a position closer to query words would more likely generate the query, and as a result a term that occurs at this position would naturally receive a higher weight. ( <ref type="formula" target="#formula_1">2</ref>) |D| comes into the formula because of the assumption about uniform distribution over all the positions in a document and can be interpreted as an inter-document weight: it penalizes a long document which is reasonable since a longer document by nature has more positions and more occurrences of terms to contribute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Method 2: conditional sampling</head><p>In this method, we consider the following different way to decompose the joint probability distribution:</p><formula xml:id="formula_9">P (w, Q, D, i) = P (Q)P (D|Q)P (i|Q, D)P (w|D, i) (8)</formula><p>The assumed generative model is as follows. We first pick a query according to some prior P (Q). We then generate a document D with probability P (D|Q). Finally, we select a position i in D with probability P (i|Q, D) and generate word w according to P (w|D, i). An illustration of this sampling process is given on the right side of Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>For the purpose of estimating P (w|Q), we can clearly ignore the term P (Q) as it is a query-specific constant. Using Bayes Rule and assuming both P (D) and P (i|D) to be uniform (as we have assumed in the first estimation method), we have</p><formula xml:id="formula_10">P (D|Q) = P (Q|D)P (D) D∈F P (Q|D)P (D) = P (Q|D) D∈F P (Q|D)<label>(9)</label></formula><formula xml:id="formula_11">P (i|D, Q) = P (Q|D, i)P (i|D) |D| i=1 P (Q|D, i)P (i|D) = P (Q|D, i) |D| i=1 P (Q|D, i) (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>Plugging Equations 8, 9, and 10 into Equation 3, we obtain the following estimate of PRM: <ref type="bibr" target="#b11">(11)</ref> where P (Q|D) is the query likelihood score of document D, which can be computed using either the positional language models <ref type="bibr" target="#b19">[19]</ref> or the standard document language model. In our experiments, we use the latter, i.e., P (Q|D) = m j=1 P (q j |D).</p><formula xml:id="formula_13">P (w|Q) ∝ D∈F |D| i=1 P (Q|D) D∈F P (Q|D) P (Q|D, i) |D| i=1 P (Q|D, i) P (w|D, i)</formula><p>As in the first estimation method, we compute P (w|D, i) using Equation <ref type="formula">7</ref>.</p><p>Similarly to the first estimation method, this second estimate of PRM also essentially combines all terms in feedback documents by assigning different weights to each term: the first weighting term in Equation 11 is seen to be the normalized query likelihood score of the document, which assigns more weights to documents that are more likely to be relevant, while the second weighting term is the normalized query likelihood of each positional language model, which assigns more weights to terms that are closer to query words.</p><p>Compared to the first estimation method, the document length normalizer |D| is missing, but a comparable effect is now achieved by normalizing the query likelihood of each positional language model P (Q|D, i). Indeed, the effect of intra-document weighting and inter-document weighting can now be seen even more clearly, i.e., the normalized P (Q|D) can be interpreted as the inter-document weight favoring a document matching the query well, while the normalized P (Q|D, i) clearly achieves intra-document weighting to place more weight on terms closer to query terms in document D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">More Estimation Details</head><p>This section provides the final estimation details for our positional relevance model (Equation 6 and 11), i.e., how to estimate P (Q|D, i). We adapt the positional language model <ref type="bibr" target="#b19">[19]</ref> to do that.</p><p>The key idea of PLM is to estimate a language model for each position of a document. Specifically, we let each word at each position of a document propagate the evidence of its occurrence to all other positions in the document so that positions closer to the word would get more share of the evidence than those far away. The PLM at each position can then be estimated based on all the propagated counts of all the words to the position as if all the words had appeared actually at the position with discounted counts. This new family of language models is intended to capture the content of the document at a position level, which is roughly like a "soft passage" centered at this position but can potentially cover all the words in the document with less weight on words far away from the position.</p><p>Formally, the PLM at position i of document D can be estimated as:</p><formula xml:id="formula_14">P (w|D, i) = c (w, i) w ∈V c (w , i)<label>(12)</label></formula><p>where c (w, i) is the total propagated count of term w at position i from the occurrences of w in all the positions. Following <ref type="bibr" target="#b19">[19]</ref>, we estimate c (w, i) using the Gaussian kernel function:</p><formula xml:id="formula_15">c (w, i) = |D| j=1 c(w, j) exp -(i -j) 2 2σ 2<label>(13)</label></formula><p>where i and j are absolute positions of the corresponding terms in the document, and |D| is the length of the document; c(w, j) is the real count of term w at position j. With the approximation method proposed in <ref type="bibr" target="#b19">[19]</ref>, the following estimation of P (w|D, i) is obtained:</p><formula xml:id="formula_16">P (w|D, i) = c (w, i) √ 2πσ 2 • Φ |D|-i σ -Φ 1-i σ (<label>14</label></formula><formula xml:id="formula_17">)</formula><p>where Φ(•) is the cumulative normal distribution and the denominator is essentially the length of the "soft" passage centered at position i.</p><p>However, there is one issue with the above estimation: the length of "soft" passages around the boundaries of a document would be smaller than that in the middle of the document; as a result, boundary positions tend to unfairly receive more weights. This may not raise problems in PLMs for retrieval <ref type="bibr" target="#b19">[19]</ref>, but it is a more serious concern for PRM, where the relative weights of terms are more important. So we decide to use a fixed length for all "soft" passages in feedback documents to estimate their corresponding positional language models as follows:</p><formula xml:id="formula_18">P (w|D, i) = c (w, i) √ 2πσ 2<label>(15)</label></formula><p>This strategy has shown to be better than the original implementation in <ref type="bibr" target="#b19">[19]</ref> for estimating PRM.</p><p>The distribution P (•|D, i) needs to be smoothed. Now that all "soft" passages have equal length, we use Jelinek-Mercer smoothing method to smooth PLM, which is shown to work as well as the Dirichlet prior smoothing method and is relatively insensitive to the setting of σ in our experiments.</p><formula xml:id="formula_19">P λ (w|D, i) = (1 -λ)P (w|D, i) + λP (w|C) (<label>16</label></formula><formula xml:id="formula_20">)</formula><p>where λ ∈ [0, 1] is a smoothing parameter and p(w|C) is the collection language model. Now we can compute the positional query likelihood score P (Q|D, i) for position i.</p><formula xml:id="formula_21">P (Q|D, i) = m j=1 P λ (q j |D, i)<label>(17)</label></formula><p>Plugging Equation <ref type="formula" target="#formula_21">17</ref>into Equations 6 and 11, we would be able to compute the two estimation methods directly. Interestingly, if we set λ = 1 or σ = ∞, Method 2 will degenerate to the general relevance model (see Equation <ref type="formula" target="#formula_0">1</ref>).</p><p>The computation of positional query likelihood is the most time-consuming part in estimating PRM. Fortunately, there is no serious efficiency concern even with an unoptimized implementation. The reason is because we only need to traverse each position of a document twice: during the first pass, the positions of query terms are recorded; in the second, we compute a positional query likelihood for each position directly based on the position information of query terms collected in the first pass. Therefore, the efficiency is comparable to the estimation of the relevance model.</p><p>Finally, the estimated positional relevance model P (w|Q) will also be interpolated with the original query model θ Q using Equation <ref type="formula" target="#formula_1">2</ref>to improve performance with a similar parameter α to that used in the mixture-model feedback <ref type="bibr" target="#b32">[32]</ref> and RM3 <ref type="bibr" target="#b1">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>We used two standard TREC datasets in our study: Terabyte (i.e., the Gov2 collection) and ClueWeb09 Category B. They represent two very large web text collections in English. Queries were taken from the title field of the TREC topics. We used the Lemur toolkit (version 4.10) and Indri search engine (version 2.10) 1 to implement our algorithms. For both datasets, the preprocessing of documents  <ref type="table" target="#tab_0">1</ref> shows some basic statistics about the datasets. We evaluated seven methods. ( <ref type="formula" target="#formula_0">1</ref>) The basic retrieval model is the KL-divergence retrieval model <ref type="bibr" target="#b15">[15]</ref>, and we chose the Dirichlet smoothing method <ref type="bibr" target="#b33">[33]</ref> for smoothing document language models, where the smoothing parameter µ was set empirically to 1500. This method was labeled as "NoFB".</p><p>(2) The baseline pseudo feedback method is the relevance model "RM3" described in Section 3.1 <ref type="bibr" target="#b1">[1]</ref>, which is one of the most effective and robust pseudo feedback methods under language modeling framework <ref type="bibr" target="#b18">[18]</ref>. (3) Another baseline pseudo feedback method is a standard passage-based feedback model, labeled as "RM3-p", which estimates the RM3 relevance model based on the best matching passage of each feedback document <ref type="bibr" target="#b17">[17]</ref>. <ref type="bibr" target="#b4">(4)</ref> We have two variations of PRM, i.e., "PRM1" and "PRM2", which are based on the two estimation methods described in Section 3.2, respectively. <ref type="bibr" target="#b5">(5)</ref> In addition, we also used PRM1 and PRM2 for passage feedback in a way as RM3-p does. Specifically, we first computed a PLM for each position of the document, and then we estimate a PRM based on a passage of size 2σ centered at the position with the maximum positional query likelihood score (see Equation <ref type="formula" target="#formula_21">17</ref>). These two runs are labeled as "PRM1-p" and "PRM2-p" respectively.</p><p>There are several parameters in these pseudo feedback algorithms. We fixed the number of feedback documents to 20 and the number of terms in feedback model to 30. Other parameters, including the feedback interpolation coefficient α, the two additional parameters σ and λ in PRM, the passage size, and the passage smoothing parameter in RM3-p, were all tuned on Terabyte05 dataset.</p><p>We used Terabyte06 and ClueWeb09 for testing. The topranked 1000 documents for all runs were compared in terms of their mean average precisions (MAP) (for Terabyte06) or eMAP <ref type="bibr" target="#b8">[8]</ref> (for ClueWeb09). In addition, other performance measures, such as Pr@10, Pr@30 and Pr@100 for Terabyte06 and eP@10, eP@30 and eP@100 for ClueWeb09, were also considered in our evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Feedback Effect</head><p>We first examine the overall retrieval precision of the pseudo feedback models for document-based feedback. The results are summarized in Table <ref type="table" target="#tab_1">2</ref>, where the best result for each row is highlighted. As we see, both PRM1 and PRM2 significantly outperform the basic KL-divergence retrieval model in terms of MAP. In addition, PRM1 and PRM2 are also significantly better than RM3 across data sets. For example, the relative improvements of PRM1 over NoFB are 9.0% on Terabyte06 and 13.5% on ClueWeb09 in terms of average precision, which are much larger than the corresponding improvements achieved by RM3 (only 2.8% and 5.9% respectively). RM3 improves Pr@10 over NoFB in neither dataset; however both PRM1 and PRM2 often improve Pr@10, though not significantly. Besides, comparing PRM1 and PRM2, we find that PRM1 is slightly more effective than PRM2. We are also interested in evaluating if a heuristic passagebased feedback (i.e., RM3-p) can work as well as PRM, since both PRM1 and PRM2 essentially can be regarded as achieving a soft effect of passage feedback. Moreover, we can also use PRM1 and PRM2 for "hard" passage feedback in a way as RM3-p does, which leads to PRM1-p and PRM2-p respectively. So we further compare the average precision of PRM1, PRM2, PRM1-p, PRM2-p, and RM3-p in Table <ref type="table">3</ref>. From the table, it is clear that PRM1, PRM2, PRM1-p and PRM2-p all outperform RM3-p significantly in most cases, suggesting that our model does not only have sound statistical foundation but also works effectively. In addition, we also observe that RM3-p behaves quite differently in two datasets: it beats RM3 on ClueWeb09 but loses to RM3 on Terabyte06. However, all the four variations of PRM perform better than RM3 consistently. Finally, it is also interesting to see that PRM1 and PRM2 work similarly to PRM1-p and PRM2-p respectively, which may mean that PRM1 and PRM2 have already achieved successfully an effect of passage-based feedback by assigning weights to different positions, so it does not bring too much additional benefit to apply PRM to passages explicitly.</p><p>Next we examine the robustness to the parameter setting in PRM on the Terabyte06 collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Robustness Analysis</head><p>In PRM1 and PRM2, there is a parameter σ inherited from the positional language model to control the propagation range, which would influence the effect of term position and term proximity. Specifically, if we increase σ to infinity, the effect of term position and proximity will be disabled. However, if we decrease this parameter to a finite value, term position and proximity will play an important role in PRM. We fix other parameters to their default values as trained on Terabyte05 and focus on understanding how σ affects the retrieval performance of PRM1 and PRM2. From Figure <ref type="figure">2</ref> (left), we can see that, as long as σ is in the range of [100, 1000], both PRM1 and PRM2 outperform RM3 clearly. Indeed, by setting σ around 200, we can often obtain the optimal performance for both PRM1 and PRM2. This result confirms the observation in previous work <ref type="bibr" target="#b19">[19]</ref>. In addition, comparing PRM1 and PRM2, PRM2 seems to be less sensitive to σ.</p><p>Next, the positional language model is smoothed using Jelinek-Mercer method to estimate PRM. The smoothing is controlled by a parameter λ. When λ = 0, we are using the pure positional language model, while if λ = 1, we completely ignore the position and proximity evidence so that every position will receive the same weight. Again, we fix other parameters and show in Figure <ref type="figure">2</ref> (right) how the average precision changes under different λ. The experiment results indicate that when λ is set to around 0.1, both PRM1 and PRM2 achieve their optimal performance. However, PRM1 and PRM2 always outperform RM3 with λ &lt; 1. Comparing PRM1 and PRM2, we see again that PRM2 seems to be more robust.</p><p>Recall that we interpolate the feedback model with the original query model. The interpolation is controlled by a coefficient α. When α = 0, we are only using the original query model (i.e., no feedback), while if α = 1.0, we completely ignore the original query model and use only the estimated feedback model. We fix other parameters and show in Figure <ref type="figure">3</ref> (left) how the average precision changes according to the value of α. We can see that both PRM1 and PRM2 are clearly better than RM3 with different α values. And the optimal α for all the methods seems to be in a range around 0.5. Besides, it is also interesting to observe that the pure feedback model results (α = 1.0) of PRM1 and PRM2 are much better than that of RM3, suggesting that the positional relevance model can lead to a more accurate query model. Finally, comparing PRM1 and PRM2, the former seems to be slightly more effective.</p><p>We further compare the robustness of different methods w.r.t. the number of feedback documents. We change the number of feedback documents from 1 to 200. The MAP results are shown in Figure <ref type="figure">3</ref> (middle). We notice that PRM1 and PRM2 are more robust to the number of feedback documents as compared to RM3. It is also interesting to see there is almost no performance decrease of PRM1 and PRM2 even when we set the parameter to 200, suggesting that the proposed positional relevance model works better in tolerating noisy information. Moreover, with only 1 feedback document, PRM1 and PRM2 have already been able to outperform RM3, no matter how many feedback documents RM3 uses, which may indicate that our methods can identify good feedback terms more accurately by assigning position-dependent weights.</p><p>Additionally, we also compare the sensitivity of different methods to the number of expansion terms in Figure <ref type="figure">3</ref> (right). We vary the number of terms from 5 to 100, and observe that both PRM1 and PRM2 can achieve a very effective performance with only 10 expansion terms, while  RM3 needs 70 terms, but even so, its performance is still not as good as our methods with 10 terms. This would be another advantage of our methods since fewer expansion terms mean higher efficiency, which is very important for retrieval systems.</p><p>To further see the robustness of our methods on individual queries, we plot the MAP of PRM1 versus RM3 and PRM2 versus RM3 on Terabyte06 in Figure <ref type="figure">4</ref>. It is interesting that the proposed methods, particularly PRM2, are quite robust; they improve most of the queries clearly with only a small number of queries decreased slightly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>We proposed a novel positional relevance model (PRM) for pseudo-relevance feedback. The PRM exploits term position and proximity evidence to assign more weights to words closer to query words based on the intuition that words closer to query words are more likely to be consistent with the query topic. Specifically, PRM generalizes the relevance model to aggregate the associations between a word and query words at the position-level in a probabilistic way. We also developed two methods to estimate the PRM based on different generative models.</p><p>Experiment results on two large web data sets show that the proposed PRM is quite effective and robust and performs significantly better than the state of the art relevance model in both document-based feedback and passage-based feedback. Compared to the relevance model, the proposed models are also less sensitive to the setting of various parameters, such as feedback coefficient, number of feedback documents, and number of expansion terms. Comparing the two estimation methods of PRM, the first method (PRM1) appears to be more effective, while the second (PRM2) tends to be more robust. Both methods achieve its optimal retrieval performance when setting the σ value in a range around 200 and λ to around 0.1.</p><p>There are many interesting future research directions to explore. One of the most interesting directions is to further study whether setting a term-specific and/or query-specific σ can further improve performance. Another interesting direction is to study how to optimize σ automatically based on the layout of web pages. Improving the estimate of other components in PRM (e.g., the probability of choosing a position in a document) would also be interesting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Dependence networks for two methods, i.e., method 1 (left) and method 2 (right), of estimating positional relevance models.</figDesc><graphic coords="3,341.13,52.05,191.20,78.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Sensitivity to the feedback interpolation coefficient α (left), the number of feedback documents (middle), and the number of expansion terms (right) of different pseudo feedback methods</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Document set characteristicand queries included stemming with the Porter stemmer and stopwords removing using a total of 418 stopwords from the standard InQuery stoplist. Table</figDesc><table><row><cell></cell><cell>Terabyte05</cell><cell>Terabyte06</cell><cell>ClueWeb09 Cat. B</cell></row><row><cell>queries</cell><cell>751-800</cell><cell>801-850</cell><cell>20001-21000</cell></row><row><cell>#qry(with qrel)</cell><cell>50</cell><cell>49</cell><cell>358</cell></row><row><cell>#documents</cell><cell cols="2">25, 205, 179</cell><cell>50, 220, 423</cell></row><row><cell>mean(dl)</cell><cell cols="2">931</cell><cell>875</cell></row></table><note><p>1 http://www.lemurproject.org/</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of different pseudo feedback models for document-based feedback. '*' and '+' mean the corresponding improvements over NoFB and RM3 are significant respectively.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Collection</cell><cell cols="2">Metric</cell><cell>NoFB</cell><cell>RM3</cell><cell>PRM1</cell><cell>PRM2</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Terabyte06</cell><cell cols="2">MAP Pr@10 Pr@30 Pr@100 0.3547 0.3576 0.3047 0.3131 0.3322  * + 0.5367 0.5041 0.5306 0.4653 0.4660 0.4884 + 0.3671  * +</cell><cell>0.3319  * + 0.5490 + 0.4871 + 0.3741  * +</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">ClueWeb09</cell><cell cols="2">eMAP eP@10 eP@30 eP@100 0.2216 0.2283 0.2356  * + 0.0713 0.0755 0.0809  * + 0.2371 0.2307 0.2418 + 0.2433 0.2486 0.2536  * +</cell><cell>0.0786  * + 0.2377 + 0.2525  * + 0.2325  * +</cell></row><row><cell>Collection Terabyte06 ClueWeb09</cell><cell>RM3-p 0.3077 0.0781</cell><cell>PRM1 0.3322  *  0.0809  *</cell><cell>PRM2 0.3319  *  0.0786</cell><cell cols="2">PRM1-p 0.3331  *  0.0800  *</cell><cell>PRM2-p 0.3290  *  0.0798  *</cell></row><row><cell cols="7">Table 3: MAP/eMAP comparison of passage-based</cell></row><row><cell cols="7">feedback methods. '*' means the corresponding im-</cell></row><row><cell cols="5">provement over RM3-p is significant.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Sensitivity to the propagation range σ (left) and the smoothing parameter λ (right) of PRM.</figDesc><table><row><cell></cell><cell>0.34</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.34</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.335</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.335</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.33</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.33</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.325</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.325</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MAP</cell><cell>0.32</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MAP</cell><cell>0.32</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.315</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.315</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.31</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.31</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.3 0.305</cell><cell>0</cell><cell>500</cell><cell>1000</cell><cell cols="2">1500 RM3 PRM1 PRM2</cell><cell cols="2">2000</cell><cell></cell><cell>0.3 0.305</cell><cell>0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8 RM3 PRM1 PRM2</cell><cell>1</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>σ</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>λ</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 Figure 2: 0.26 MAP RM3 PRM1 PRM2 NoFB</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MAP</cell><cell>0.3 0.305 0.31 0.315 0.32 0.325 0.33 0.335 0.34</cell><cell>RM3 PRM1 PRM2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MAP</cell><cell>0.3 0.305 0.31 0.315 0.32 0.325 0.33 0.335 0.34</cell><cell></cell><cell></cell><cell cols="2">RM3 PRM1 PRM2</cell></row><row><cell>0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell><cell></cell><cell>0</cell><cell>20</cell><cell>40</cell><cell>60</cell><cell>80</cell><cell>100</cell><cell></cell><cell>0</cell><cell>20</cell><cell>40</cell><cell>60</cell><cell>80</cell><cell>100</cell></row><row><cell></cell><cell cols="3">Feedback coefficient α</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Number of feedback documents</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Number of expansion terms</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">ACKNOWLEDGMENTS</head><p>We thank the anonymous reviewers for their useful comments. We also thank Wan Chen for helping improve the English in this paper. This material is based upon work supported by the National Science Foundation under Grant</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Umass at trec 2004: Novelty and hard</title>
		<author>
			<persName><forename type="first">Nasreen</forename><surname>Abdul-Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leah</forename><surname>Larkey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howard</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Courtney</forename><surname>Wade</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>In TREC &apos;04</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Relevance feedback with too much data</title>
		<author>
			<persName><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;95</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="337" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic query expansion using smart: Trec 3</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerard</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Singhal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC &apos;94</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="69" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Query expansion using gaze-based feedback on the subdocument level</title>
		<author>
			<persName><forename type="first">Georg</forename><surname>Buscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludger</forename><surname>Van Elst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;08</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="387" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Efficiency vs. effectiveness in terabyte-scale information retrieval</title>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Buttcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC &apos;05</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Term proximity scoring for ad-hoc retrieval on very large text collections</title>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Buttcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brad</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><surname>Lushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;06</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="621" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Selecting good expansion terms for pseudo-relevance feedback</title>
		<author>
			<persName><forename type="first">Guihong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="243" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Minimal test collections for retrieval evaluation</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Sitaraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;06</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="268" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Shortest substring ranking (multitext experiments for trec-4)</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Forbes</forename><forename type="middle">J</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName><surname>Burkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC &apos;95</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="295" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning in a pairwise term-term proximity framework for information retrieval</title>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Colm</surname></persName>
		</author>
		<author>
			<persName><surname>Riordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;09</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="251" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Proximity operators -so near and yet so far</title>
		<author>
			<persName><forename type="first">David</forename><surname>Hawking</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">B</forename><surname>Thistlewaite</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC &apos;95</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="500" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Effective ranking with arbitrary passages</title>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Kaszkiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="344" to="364" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The use of term position devices in ranked output experiments</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Michael</forename><surname>Keen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Some aspects of proximity searching in text retrieval systems</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Michael</forename><surname>Keen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Information Science</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="89" to="98" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Document language models, query models, and risk minimization for information retrieval</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;01</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="111" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Relevance-based language models</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;01</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Passage retrieval based on language models</title>
		<author>
			<persName><forename type="first">Xiaoyong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;02</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="375" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A comparative study of methods for estimating query language models with pseudo feedback</title>
		<author>
			<persName><forename type="first">Yuanhua</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;09</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1895" to="1898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Positional language models for information retrieval</title>
		<author>
			<persName><forename type="first">Yuanhua</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;09</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="299" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A markov random field model for term dependencies</title>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;05</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="472" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Latent concept expansion using markov random fields</title>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;07</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Minimal span weighting retrieval for question answering</title>
		<author>
			<persName><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR Workshop on Information Retrieval for Question Answering</title>
		<editor>
			<persName><forename type="first">Rob</forename><surname>Gaizauskas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mark</forename><surname>Greenwood</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mark</forename><surname>Hepple</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="23" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Term proximity scoring for keyword-based retrieval systems</title>
		<author>
			<persName><forename type="first">Yves</forename><surname>Rasolofo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacques</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECIR &apos;03</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="207" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Relevance weighting of search terms</title>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><forename type="middle">Sparck</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society of Information Science</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="129" to="146" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Okapi at trec-3</title>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Micheline</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Gatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC &apos;94</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="109" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Relevance feedback in information retrieval</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Rocchio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The SMART Retrieval System: Experiments in Automatic Document Processing</title>
		<imprint>
			<publisher>Prentice-Hall Inc</publisher>
			<date type="published" when="1971">1971</date>
			<biblScope unit="page" from="313" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improving retrieval performance by relevance feedback</title>
		<author>
			<persName><forename type="first">Gerard</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society of Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="288" to="297" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An exploration of proximity measures in information retrieval</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;07</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="295" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A study of the effect of term proximity on query expansion</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Information Science</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="324" to="333" />
			<date type="published" when="2006-08">August 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Query expansion using local and global document analysis</title>
		<author>
			<persName><forename type="first">Jinxi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;96</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="4" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improving pseudo-relevance feedback in web information retrieval using web page segmentation</title>
		<author>
			<persName><forename type="first">Shipeng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW &apos;03</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="11" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Model-based feedback in the language modeling approach to information retrieval</title>
		<author>
			<persName><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;01</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="403" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A study of smoothing methods for language models applied to ad hoc information retrieval</title>
		<author>
			<persName><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;01</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="334" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A proximity language model for information retrieval</title>
		<author>
			<persName><forename type="first">Jinglei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeogirl</forename><surname>Yun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;09</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="291" to="298" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
