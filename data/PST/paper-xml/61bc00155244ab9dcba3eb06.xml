<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Lifelong Generative Modelling Using Dynamic Expansion Graph Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Fei</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of York</orgName>
								<address>
									<postCode>YO10 5GH</postCode>
									<settlement>York</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Adrian</forename><forename type="middle">G</forename><surname>Bors</surname></persName>
							<email>adrian.bors@york.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of York</orgName>
								<address>
									<postCode>YO10 5GH</postCode>
									<settlement>York</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Lifelong Generative Modelling Using Dynamic Expansion Graph Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Variational Autoencoders (VAEs) suffer from degenerated performance, when learning several successive tasks. This is caused by catastrophic forgetting. In order to address the knowledge loss, VAEs are using either Generative Replay (GR) mechanisms or Expanding Network Architectures (ENA). In this paper we study the forgetting behaviour of VAEs using a joint GR and ENA methodology, by deriving an upper bound on the negative marginal log-likelihood. This theoretical analysis provides new insights into how VAEs forget the previously learnt knowledge during lifelong learning. The analysis indicates the best performance achieved when considering model mixtures, under the ENA framework, where there are no restrictions on the number of components. However, an ENA-based approach may require an excessive number of parameters. This motivates us to propose a novel Dynamic Expansion Graph Model (DEGM). DEGM expands its architecture, according to the novelty associated with each new database, when compared to the information already learnt by the network from previous tasks. DEGM training optimizes knowledge structuring, characterizing the joint probabilistic representations corresponding to the past and more recently learned tasks. We demonstrate that DEGM guarantees optimal performance for each task while also minimizing the required number of parameters.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The Variational Autoencoder (VAE) <ref type="bibr" target="#b18">(Kingma and Welling 2013)</ref> is a popular generative deep learning model with remarkable successes in learning unsupervised tasks by inferring probabilistic data representations <ref type="bibr" target="#b6">(Chen et al. 2018)</ref>, for disentangled representation learning <ref type="bibr" target="#b12">(Higgins et al. 2017;</ref><ref type="bibr" target="#b49">Ye and Bors 2021d)</ref> and for image reconstruction tasks <ref type="bibr">(Ye and</ref><ref type="bibr">Bors 2020c, 2021b,c,d)</ref>. Training a VAE model involves maximizing the marginal log-likelihood log p θ (x) = log p θ (x|z)p(z)dz which is intractable during optimization due to the integration over the latent space defined by the variables z. VAEs introduce using a variational distribution q ω (z|x) to approximate the posterior while trained by maximizing a lower bound, called Evidence Lower Bound (ELBO), <ref type="bibr" target="#b18">(Kingma and Welling 2013)</ref> :</p><formula xml:id="formula_0">log p θ (x) ≥ E qω(z|x) [log p θ (x | z)] − KL [q ω (z | x) || p (z)] := L ELBO (x; {θ, ω})<label>(1)</label></formula><p>where p θ (x | z) and p(z) = N (0, I) are the decoding and prior distribution, respectively, while KL[•] represents the Kullback-Leibler divergence. Defining a tighter ELBO to the marginal log-likelihood, achieved by using a more expressive posterior <ref type="bibr" target="#b16">(Kim and Pavlovic 2020;</ref><ref type="bibr" target="#b25">Maaløe et al. 2016)</ref>, importance sampling <ref type="bibr" target="#b3">(Burda, Grosse, and Salakhutdinov 2015;</ref><ref type="bibr" target="#b8">Domke and Sheldon 2018)</ref> or through hierarchical variational models <ref type="bibr" target="#b28">(Molchanov et al. 2019;</ref><ref type="bibr" target="#b40">Vahdat and Kautz 2020)</ref>, has been successful for improving the performance of VAEs. However, these approaches can only guarantee a tight ELBO for learning a single domain and do not work under the lifelong learning (LLL) setting, which involves learning sequentially several tasks associated with different databases. VAEs, similarly to other deep learning methods <ref type="bibr" target="#b11">(Guo et al. 2020)</ref>, suffer from catastrophic forgetting <ref type="bibr" target="#b9">(French 1999)</ref>, when learning new tasks, leading to degenerate performance on the previous tasks. One direct way to enable VAEs for LLL is by using Generative Replay (GR) processes <ref type="bibr" target="#b33">(Ramapuram, Gregorova, and Kalousis 2020)</ref>.</p><p>Let us consider a VAE model trained on a sequence of t tasks. After the i-th task learning is finished, the GR process allows the model to generate a pseudo dataset Xi which will be mixed with the incoming data set X new to form a joint dataset for the (i + 1)-th task learning. Usually, the distribution of { Xi , X new } does not match the real data distribution exactly and the optimal parameters {θ * , ω * } are estimated by maximizing ELBO, on samples x ′ drawn from { Xi , X new }. L ELBO (•) is not a tight ELBO in Eq. ( <ref type="formula" target="#formula_0">1</ref>) by using the model's parameters {θ * , ω * } which actually are not optimal for the real sample log-likelihood log p θ (x) (See Proposition 6 in Appendix-I from SM 1 ). In this paper, we aim to evaluate the tightness between log p θ (x) and L ELBO (x ′ ; θ * , ω * ), by developing an upper bound to the negative marginal log-likelihood, called Lifelong ELBO (LELBO). LELBO involves the discrepancy distance <ref type="bibr" target="#b26">(Mansour, Mohri, and Rostamizadeh 2009)</ref> between the target and the evolved source distributions, as well as the resulting accumulated errors when learning each new task. This analysis provides insights into how a VAE model is losing previously learnt knowledge during LLL. We also generalize the proposed theoretical analysis to ENA models, which leads to a novel dynamic expansion graph model (DEGM) enabled with generating graph structures linking the existing components and a newly created component, benefiting on the transfer learning and the model size reduction. We list our contributions as :</p><p>• This is the first research study to develop a novel theoretical framework for analyzing VAE's forgetting behaviour during LLL.</p><p>• We develop a novel generative latent variable model which guarantees the trade-off between the optimal performance for each task and the model's size during LLL.</p><p>• We propose a new benchmark for the probability density estimation task under the LLL setting. Supplementary materials (SM) and source code are available 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Works</head><p>Recent efforts in LLL would focus on regularization based methods <ref type="bibr" target="#b14">(Jung, Jung, and Kim 2016;</ref><ref type="bibr" target="#b23">Li and Hoiem 2017)</ref>, which typically penalize significant changes in the weights when learning new tasks. Other methods rely on memory systems such as using past learned data to guide the optimization <ref type="bibr" target="#b4">(Chaudhry et al. 2018;</ref><ref type="bibr" target="#b11">Guo et al. 2020;</ref><ref type="bibr" target="#b31">Pan et al. 2020)</ref>, using Generative Adversarial Nets (GANs) or VAEs <ref type="bibr" target="#b0">(Achille et al. 2018;</ref><ref type="bibr" target="#b33">Ramapuram, Gregorova, and Kalousis 2020;</ref><ref type="bibr" target="#b52">Ye and Bors 2021g;</ref><ref type="bibr" target="#b38">Shin et al. 2017;</ref><ref type="bibr">Ye and</ref><ref type="bibr">Bors 2020a,b, 2021a)</ref> aiming to reproduce previously learned data samples in order to attempt to overcome forgetting. However, most of these models focus on predictive tasks and the lifelong generative modelling remains an unexplored area.</p><p>Prior works for continuously learning VAEs are divided into two branches: Generative Replay (GR) and Expanding Network Architectures (ENA). GR was used in VAEs for the first time in <ref type="bibr" target="#b0">(Achille et al. 2018)</ref> while <ref type="bibr" target="#b33">(Ramapuram, Gregorova, and Kalousis 2020)</ref> extends the GR mechanism within a Teacher-Student framework, called the Lifelong Generative Modelling (LGM). A major limitation for GRbased models is their inability of learning a long sequence of data domain due to the fixed model capacity while having to retrain the generator frequently <ref type="bibr" target="#b44">(Ye and Bors 2020a)</ref>. This issue is relieved by using ENA <ref type="bibr" target="#b22">(Lee et al. 2020)</ref>, inspired by a network expansion mechanism <ref type="bibr" target="#b34">(Rao et al. 2019)</ref>, or by employing a combination between ENA and GR mechanisms <ref type="bibr">(Ye and Bors 2021f,e)</ref>. These methods significantly relieve forgetting but would suffer from informational interference when learning a new task <ref type="bibr">(Riemer et al. 2019)</ref>.</p><p>The tightness on ELBO is key to improving VAE's performance and one possible way is to use the Importance Weighted Autoencoder (IWELBO) <ref type="bibr" target="#b3">(Burda, Grosse, and Salakhutdinov 2015)</ref> in which the tightness is controlled by the number of weighted samples considered. Other approaches would consider as approximate posterior distributions either normalizing flows <ref type="bibr" target="#b17">(Kingma et al. 2016;</ref><ref type="bibr" target="#b35">Rezende and Mohamed 2015)</ref>, implicit distributions <ref type="bibr" target="#b27">(Mescheder, Nowozin, and Geiger 2017)</ref> or hierarchical variational inference <ref type="bibr" target="#b13">(Huang et al. 2019)</ref>. The IWELBO bound can be used with any of these approaches for further performance improvement <ref type="bibr" target="#b39">(Sobolev and Vetrov 2019)</ref>. Moreover, online variational inference <ref type="bibr" target="#b29">(Nguyen et al. 2017</ref>) has been used in 1 https://github.com/dtuzi123/Expansion-Graph-Model</p><p>VAEs, but it would require storing past samples for computing the approximate posterior, which is intractable when learning an infinite number of tasks. The tightness of ELBO under LLL was not studied in any of these works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preliminary</head><p>In this paper, we address a more general lifelong unsupervised learning problem, where task boundaries are provided only during the training. For a given sequence of tasks {T 1 , . . . , T N } we consider that each T i is associated with an unlabeled training set Q S i and an unlabeled testing set Q T i . The model only sees a sequence of training sets {Q S 1 , . . . , Q S N } while it is evaluated on {Q T 1 , . . . , Q T N }. Let us consider the input data space X ∈ R d of dimension d, and P i the probabilistic representation of the testing set Q T i . We desire to evaluate the quality of reconstructing data samples x ∈ X , by a model using the square loss (SL) function x − h(x) 2 , where h is a hypothesis function in a space of hypotheses {h ∈ H | H : X → X }. For the image space, the loss is represented by</p><formula xml:id="formula_1">d i=1 (x[i] − h(x)[i])</formula><p>2 , where [i] represents the entry for the i-th dimension.</p><p>Definition 1 (Single model.) Let M = {f ω , g θ } be a single model consisting of an encoder f ω : X → Z for representing q ω (z | x), and a decoder g θ : Z → X for modelling</p><formula xml:id="formula_2">p θ (x | z). The latent variable z = f µ ω (x) + f δ ω (x) ⊙ γ, γ ∼ N (0, I) is reparameterized by the mean f µ ω (x) and variance f δ ω (x)</formula><p>, implemented by a network f ω (x). {ω t , θ t } are the parameters of the model M t , where t represents the number of tasks considered for training the model. Let g θ (f ω ) : X → X be the encoding-decoding process for M.</p><p>Definition 2 (Discrepancy distance.) We implement h ∈ H by g θ (f ω ) evaluated on the error function L : X × X → R + which is bounded, ∀(x, x ′ ) ∈ X 2 , L(x, x ′ ) ≤ U for some U &gt; 0. We define the error function as the SL function L(x, x ′ ) = x − x ′ 2 , (x, x ′ ) ∈ X . A risk for h(•) on the target distribution P i of the i-th domain (task) is defined as</p><formula xml:id="formula_3">R Pi (h, f Pi ) = E x∼Pi L(h(x), f Pi (x))</formula><p>, where f Pi ∈ H is the true labeling function for P i . The discrepancy distance on two domains {P, P} over X , is defined as:</p><formula xml:id="formula_4">disc L (P, P) = sup (h,h ′ )∈H E x∼P [L (h ′ (x) , h (x))] −E x∼P [L (h ′ (x) , h (x))] .</formula><p>(2) Definition 3 (Empirical discrepancy distance.) In practice, we consider the sample populations of size m P and m P from the empirical distributions P and P, and the bounds U P and U P , corresponding to P and P, respectively. Then, the discrepancy can be estimated by using finite samples :</p><formula xml:id="formula_5">disc L P, P ≤ disc L P, P + 8 Re U P (H) + Re U P (H) + 3M log 4 δ 2m P + log 4 δ 2m P ,<label>(3)</label></formula><p>which holds with probability 1−δ, δ ∈ (0, 1), where M &gt; 0, and Re U P is the Rademacher complexity (See Appendix-H from SM 1 ). In the following we use disc ⋆ L (•) to represent the right-hand side (RHS) of Eq. (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Theoretical Framework Generalization Bounds for a Single Model</head><p>Let us consider P i the approximation distribution for the generated data by g θ i (•) of M i , which was trained on a sequence of domains {Q S 1 , . . . , Q S i } and Pi represents the probabilistic representation for Q S i . Training M i+1 using GR, for the (i + 1)-th task, requires the minimization of L ⋆ (implemented as the negative ELBO) :</p><formula xml:id="formula_6">M i+1 = arg min ω (i+1) ,θ (i+1) L ⋆ P i+1 , P i ⊗ Pi+1 ,<label>(4)</label></formula><p>where ⊗ represents the mixing distribution P i ⊗ Pi+1 , formed by samples uniformly drawn from both P i and Pi+1 , respectively. Eq. ( <ref type="formula" target="#formula_6">4</ref>) can be treated as a recursive optimization problem as i increases from 1 to t. The learning goal of M i+1 is to approximate the distribution P i+1 ≈ P i ⊗ Pi+1 by minimizing L ⋆ (•), when learning (i + 1)-th task. During the LLL, the errors when the model is tested on the initial tasks Q S i , i&lt;t, would increase, leading to a degenerated performance on its corresponding unseen domains, defined by its performance on the testing set Q T i . One indicator for the generalization ability of a model M is to predict its performance on a testing data set by achieving a certain error rate on a training data set <ref type="bibr" target="#b19">(Kuroki et al. 2019)</ref>. In this paper, we develop a new theoretical analysis that can measure the generalization of a model under LLL where the source distribution changes over time. Before we introduce the Generalization Bound (GB) for -ELBO, we firstly define the GB when considering a VAE learning a single task in Theorem 1 and then when learning several tasks in Theorem 2.</p><p>Theorem 1 Let P i and Pi be two domains over X . Then for h * Pi = arg min h∈H R Pi (h, f Pi ) and h * Pi = arg min h∈H R Pi (h, f Pi ) where f Pi ∈ H is the ground truth function (identity function under the encoder-decoding process) for Pi , we can define the GB between P i and Pi :</p><formula xml:id="formula_7">R Pi h, f Pi ≤ R Pi h, h * Pi + disc ⋆ L P i , Pi + R Pi h * Pi , f Pi + R Pi h * Pi , h * Pi ,<label>(5)</label></formula><p>where the last two terms represent the optimal combined risk denoted by ε(P i , Pi ), and we have :</p><formula xml:id="formula_8">R Pi h, h * Pi = E x∼ Pi L h(x), h * Pi (x) .<label>(6)</label></formula><p>See the proof in Appendix-A from SM 1 . We use R A (P i , Pi ) to represent disc ⋆ L (P i , Pi )+ε(P i , Pi ). Theorem 1 explicitly defines the generalization error of M trained on the source distribution Pi . With Theorem 1, we can extend this GB to ELBO and the marginal log-likelihood evaluation when the source distribution evolves over time. Theorem 2 For a given sequence of tasks {T 1 , . . . , T t }, we derive a GB between the target distribution and the evolved source distribution during the t-th task learning :</p><formula xml:id="formula_9">1 t t i=1 R Pi h, f Pi ≤ R P t−1 ⊗ Pt h, h * P t−1 ⊗ Pt + R A P (1:t) , P t−1 ⊗ Pt , (<label>7</label></formula><formula xml:id="formula_10">)</formula><p>where P (1:t) is the mixture distribution {P 1 ⊗P 2 , . . . , ⊗P t }.</p><p>See the proof in Appendix-B from SM 1 . Remark. Theorem 2 has the following observations:</p><p>• The performance on the target domain depends mainly on the discrepancy term even if M minimizes the source risk, from the first term of RHS of Eq. ( <ref type="formula" target="#formula_9">7</ref>).</p><p>• In the GR process, P t−1 is gradually degenerated as t increases due to the repeated retraining <ref type="bibr" target="#b44">(Ye and Bors 2020a)</ref>, which leads to a large discrepancy distance term. We also extend the idea from Theorem 2 to derive GBs for GANs, which demonstrates that the discrepancy distance between the target and the generator's distribution plays an important role for the generalization performance of GANs under the LLL setting, exhibiting similar forgetting behaviour as VAEs (See details in Appendix-G from SM 1 ). In the following, we extend this GB to L ⋆ .</p><p>Lemma 1 Let us consider the random samples x T i ∼ P i , for i = 1, . . . , t. The sample log-likelihood and its ELBO for all {P 1 , . . . , P t } can be represented by</p><formula xml:id="formula_11">t i=1 log p θ (x T i ) and t i=1 L ELBO (h, x T i ). Let xt repre- sent the random sample drawn from P t−1 ⊗ Pt . We know that KL(q ω t (z | x T i ) || p(z)) = KL(q ω t (z | xt ) || p(z)) if q ω t (z | x T i ) = q ω t (z | xt</formula><p>), and we have :</p><formula xml:id="formula_12">1 t t i=1 E Pi KL q ω t (z | x T i ) || p(z) ≤ E P t−1 ⊗ Pt KL q ω t (z | xt ) || p(z) + KL 1 − KL 2 ,<label>(8)</label></formula><p>where q ω t (•) represents the inference model for M t . KL 1 and KL 2 represent the left-hand side term (LHS) and the first term of the RHS of Eq. (8), respectively. Since ELBO consists of a negative reconstruction error term, a KL divergence term and a constant (− 1 2 log π) <ref type="bibr" target="#b7">(Doersch 2016)</ref>, when the decoder models a Gaussian distribution with a diagonal covariance matrix (the diagonal element is 1/ √ 2), we derive a GB on -ELBO by combining (7) and (8) :</p><formula xml:id="formula_13">1 t t i=1 E Pi − L ELBO x T i ; h ≤ R A P (1:t) , P t−1 ⊗ Pt + E P t−1 ⊗ Pt − L ELBO xt ; h + KL 1 − KL 2 , (9)</formula><p>where x T i ∼ P i and xt ∼ P t−1 ⊗ Pt . See the proof in Appendix-C from SM 1 . We call the RHS of Eq. ( <ref type="formula">9</ref>) as Lifelong ELBO (LELBO), denoted as L LELBO which is a bound for an infinite number of tasks (t → ∞). This bound shows the behaviour of M when minimizing -ELBO when learning each task. L LELBO is also an upper bound to − t i=1 E Pi log p(x T i ) /t, estimated by M t . The generalization of LELBO. From Eq. ( <ref type="formula">9</ref>), we can generalize LELBO to other VAE variants under LLL, including the auxiliary deep generative models <ref type="bibr" target="#b25">(Maaløe et al. 2016)</ref> and hierarchical variational inference <ref type="bibr" target="#b39">(Sobolev and Vetrov 2019)</ref> (See details in Appendix-F from SM 1 ). IWELBO bound <ref type="bibr" target="#b3">(Burda, Grosse, and Salakhutdinov 2015)</ref> is an extension of ELBO by generating multiple weighted samples under the importance sampling <ref type="bibr" target="#b8">(Domke and Sheldon 2018)</ref>. We generalize the IWELBO bounds to the LLL setting as:</p><formula xml:id="formula_14">1 t t i=1 E x T i ∼Pi −log p x T i ≤ E xt ∼P t−1 ⊗ Pt   −E z1...,z K ′ ∼q(z|x)   log 1 K ′ K ′ i=1 p (x t , z i ) q (z i | x)     + |KL 1 − KL 2 | + R A P (1:t) , P t−1 ⊗ Pt . (<label>10</label></formula><formula xml:id="formula_15">)</formula><p>See the derivation in Appendix-F.1 from SM 1 . We consider K ′ weighted importance samples {z 1 , . . . , z K ′ } (Domke and Sheldon 2018), and omit the subscript for q(•). We call RHS of Eq. ( <ref type="formula" target="#formula_14">10</ref>) as L LELBO K ′ , and</p><formula xml:id="formula_16">L LELBO K ′ =1 = L LELBO .</formula><p>Remark. We have several conclusions from Eq. ( <ref type="formula" target="#formula_14">10</ref>) :</p><p>• Based on the assumption that P t−1 is fixed and</p><formula xml:id="formula_17">|KL 1 − KL 2 | = 0, we have L LELBO K ′ +1 ≤ L LELBO K ′ .</formula><p>• The tightness of ELBO on P t−1 ⊗ Pt (the second term of RHS of Eq. ( <ref type="formula" target="#formula_14">10</ref>)) can not guarantee a tight bound on the testing data log-likelihood since the RHS of Eq. ( <ref type="formula" target="#formula_14">10</ref>) contains the discrepancy distance term and other error terms.</p><p>A tight GB can be achieved by reducing the discrepancy distance term by training a powerful generator that approximates the target distributions well, for example by using the Autoencoding VAE <ref type="bibr" target="#b3">(Cemgil et al. 2020)</ref> or adversarial learning <ref type="bibr" target="#b10">(Goodfellow et al. 2014)</ref>, which would fail when learning several entirely different domains due to the fixed model's capacity and the mode collapse <ref type="bibr" target="#b39">(Srivastava et al. 2017</ref>). In the following section, we show how we can achieve a tight GB by increasing the model's capacity through an expansion mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalization Bounds for ENA</head><p>For a given mixture model M = {M 1 , . . . , M K }, each component M i can be trained with GR. In order to assess the trade-off between performance and complexity, we assume that P (i,s) is the generator distribution of the s-th component which was trained on a number of i tasks. Suppose that the j-th task was learnt by the s-th component of the mixture and its approximation distribution P (m,s) j is formed by the sampling process x ∼ P (i,s) if I T (x) = j, where I T : X → T is the function that returns the true task label for the sample x, and m represents the number of times M s was used with GR for the j-th task. We omit the component index s for P (m,s) j for the sake of simplification and let P 0 t represent Pt . In the following, we derive a GB for a mixture model M with K components. Let C = {c(i, 1), . . . , c(i, n)} be a set where c(i, j) denotes the number of times M c ′ i was used for a(i, j)-th task. We have </p><formula xml:id="formula_18">|C| + |C ′ | = K, |A ′ c ′ i | &gt; 1,</formula><formula xml:id="formula_19">′ i -th component ãi = |A ′ c ′ i |.</formula><p>We derive the bound for M during the t-th task learning :</p><formula xml:id="formula_20">1 t |C ′ | i=1 ãi j=1 R P a(i,j) h c ′ i , f P a(i,j) + 1 t |C| i=1 R Pa i h ci , f Pa i ≤ 1 t R C + 1 t R A ′ (11)</formula><p>where each h ci ∈ H and h c ′ i ∈ H represent the hypothesis of the c i -th and c ′ i -th component in the mixture, respectively. R C is the error evaluated by the components that are trained only once :</p><formula xml:id="formula_21">R C = |C| i=1 R Pa i h ci , h * Pa i + R A P ai , Pai ,<label>(12</label></formula><p>) and R A ′ is the accumulated error evaluated by the components that are trained more than once :</p><formula xml:id="formula_22">R A ′ = |C ′ | i=1 ãi j=1 R P c(i,j) a(i,j) h c ′ i , h * P c(i,j) a(i,j) + R A P a(i,j) , P c(i,j) a(i,j) ,<label>(13)</label></formula><p>and after decomposing the last term it becomes</p><formula xml:id="formula_23">R A ′ = |C ′ | i=1 ãi j=1 R P c(i,j) a(i,j) h c ′ i , h * P c(i,j) a(i,j) + c(i,j)−1 k=−1 R A P k a(i,j) , P k+1 a(i,j) . (<label>14</label></formula><formula xml:id="formula_24">)</formula><p>The proof is provided in Appendix-D from SM 1 . Remark. We have several observations from Theorem 3 :</p><p>• If |C ′ | = 1 and |C| = 0, then the term R C in Eq. ( <ref type="formula">11</ref>) would disappear while R ′ A would accumulate additional error terms, according to Eq. ( <ref type="formula" target="#formula_23">14</ref>).</p><p>• In contrast, if |C| = t, then the GB from Eq. ( <ref type="formula">11</ref>) is reduced to R C , where the number of components K is equal to the number of tasks and there are no accumulated error terms, leading to a small gap on GB.</p><p>• When |C| increases, the gap on GB tends to be small and the model's complexity tends to be large because the accumulated error term will be reduced (|C ′ | = K − |C| in Eq. ( <ref type="formula" target="#formula_23">14</ref>)) while K increases.</p><p>• If a single component learns multiple tasks (|C ′ | = 1), then GB on the initial tasks (a(i, j) is small), tends to have more accumulated error terms compared to the GB on the latest given tasks (a(i, j) is large), shown by the number of accumulated error terms R A (•, •) in Eq. ( <ref type="formula" target="#formula_23">14</ref>), controlled by c(i, j) = t − a(i, j). In the following, we extend GB from L to L ⋆ .</p><p>Lemma 2 We derive a GB for the marginal log-likelihood during the t-th task learning for M:</p><formula xml:id="formula_25">1 t t i=1 E Pi − log p x T i ≤ 1 t R II A ′ + R II C + D ⋆ dif f + 1 t |C ′ | i=1 ãi j=1 E P c(i,j) a(i,j) − L ELBO x t a(i,j) ; h c ′ i + |C| i=1 E Pa i − L ELBO x S ai ; h ci , (<label>15</label></formula><formula xml:id="formula_26">)</formula><p>where we omit the component's index for each log p(x T i ) for the sake of simplification, we use R II C and R II A ′ to represent the second terms in the RHS's from Eq. ( <ref type="formula" target="#formula_21">12</ref>) and (13), respectively, and D ⋆ dif f represents the absolute difference on the KL divergence (details in Appendix-E from SM 1 ). Each x S ai is drawn from Pai and each x t a(i,j) is drawn from P c(i,j) a(i,j)</p><formula xml:id="formula_27">modelled by the c ′ i -th component in M. L ELBO (x S ai ; h ci ) is the ELBO estimated by the c i -th component.</formula><p>Lemma 2 provides an explicit way to measure the gap between ELBO and the model likelihood for all tasks using the mixture model. When |C ′ | = 0, D ⋆ dif f = 0 and the discrepancy disc ⋆ L (P ai , P 0 ai ) is very small, this bound is tight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamic Expansion Graph Model (DEGM)</head><p>According to Theorem 3, achieving an optimal GB requires each mixture component to model a unique task. However, adding dynamically a new component whenever learning a new task, leads to ever-increasing memory and computation requirements. For addressing the trade-off between task learning effectiveness and memory efficiency, we propose a novel mixture expansion mechanism. This would selectively allow the newly created component to reuse some of the parameters and thus transfer information from existing components, according to a knowledge similarity criterion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Basic and Specific Nodes in DEGM</head><p>A component trained during LLL, with independent parameters, is called a basic node and can be transferred to be used in other tasks. Therefore, a basic node can be seen as a knowledge source for other processing nodes in DEGM. Meanwhile, we also have specific nodes associated with the novel information acquired from a new task T (t+1) , after also considering reusing the information from the basic nodes. Let q ωi (z | x) and p θi (x | z) represent the encoding and decoding distributions, respectively, as in Eq. ( <ref type="formula" target="#formula_0">1</ref>). We implement the basic node using paired sub-models, for encoding and decoding information. We consider two sub-inference models, f ωi : X → Z and f ω ′ i : Z → Z for modelling q ωi (z | x), expressed by f ωi ⊙ f ω ′ i : X → Z, where Z is an intermediate latent representation space with the dimension larger than Z, | Z| &gt; |Z|. We use two networks, g θi : X → X and g θ ′ i : X → X , for modelling p θi (x | z) which is expressed by g θi ⊙ g θ ′ i : Z → X , where X is an intermediate representation space, | X | &lt; |X |. Since a basic node has two connectable sub-models {f ωi , g θi }, building a specific j-th node only requires two separate sub-models {f ω ′ j , g θ ′ j } which would be connected with the sub-models {f ωi , g θi } from all basic nodes, i = 1, . . . , K to form a graph structure in DEGM. In the following section, we describe how DEGM expands its architecture during LLL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Sub-Graph Structures in DEGM</head><p>Let us assume that we have trained t nodes after learning t tasks, where K nodes, K &lt; t, represent basic nodes G = {B 1 , . . . , B K }, and (t − K) nodes belong to specific nodes S = {S 1 , . . . , S (t−K) }. Let GI(•) and SI(•) be the functions that return the node index for G and S. Each B i ∈ G has four sub-models {f ωi * , f ω ′ i * , g θi * , g θ ′ i * } where i * = GI(i), and each S i ∈ S has only two sub-models</p><formula xml:id="formula_28">{f ω ′ i ′ , g θ ′ i ′ }, where i ′ = SI(i).</formula><p>Let us consider V ∈ R t×t an adjacency matrix representing the directed graph edges from S to G. V (i, j) is the directed edge from nodes i to j, and V is used for expanding the architecture whenever necessary. After learning t-th task, we set a new task T t+1 for training the mixture model with Q S t+1 . We evaluate the efficiency of using each element of B i ∈ G, i = 1, . . . , K by calculating L ELBO (x j ; B i ) on x j ∼ Q S t+1 , j = 1, . . . , n, (n = 1000 in experiments). For assessing the novelty of a given task T t+1 , with respect to the knowledge already acquired, we consider the following criterion :</p><formula xml:id="formula_29">ks i = L ELBO (B i ) − E x∼Q S (t+1) L ELBO (x; B i ) ,<label>(16)</label></formula><p>where i = 1, . . . , K and L ELBO (B i ) is the best loglikelihood estimated by B i on its previously assigned task and we form K = {ks 1 , . . . , ks K }. Similar log-likelihood evaluations were used for selecting components in <ref type="bibr" target="#b22">(Lee et al. 2020;</ref><ref type="bibr" target="#b34">Rao et al. 2019)</ref>. However, in our approach we develop a graph-based structure by defining Basic and Specific nodes based on analyzing K, as explained in the following.</p><p>Building a Basic node. A Basic node is added to the DEGM model when the incoming task is assessed as completely novel. If min(K) &gt; τ , where τ is a threshold, then we set V (t + 1, GI(i)) = 0, i = 1, . . . , K and DEGM builds a basic node which is added to G. During the (t + 1)-th task learning, we only optimize the parameters of the (t + 1)-th component by using the loss function from Eq. ( <ref type="formula" target="#formula_0">1</ref>) with the given task' dataset.</p><p>Building a Specific node. A Specific node is built when the incoming task is related to the already learned knowledge, encoded by the basic nodes. If min(K) ≤ τ , then we update V by calculating the importance weight</p><formula xml:id="formula_30">V (t + 1, GI(i)) = (w * − ks i )/ K j=1 (w * − ks j ), w * = K j=1 ks j , i = 1, . . . , K, where we denote π i = V (t + 1, GI(i)) for sim- plification. According to the updated V, we built a new sub-inference model f ω ′ (t+1) , based on a set of sub-models {f ωi * | i * = GI(i), i = 1, . . . , K}, as K i=1 π i f ωi * ⊙ f ω ′ (t+1) (x), which represents z = K i=1 π i z i , where each z i = f ωi * ⊙ f ω ′ (t+1) (x)</formula><p>is weighted by π i . In Fig. <ref type="figure">1</ref>, we show the structure of the decoder, where an identity function implemented by the input layer distributes the latent variable z to each g θi * (z), i * = GI(1), . . . , GI(K), lead- information from G is weighted by π i . We then build a new sub-decoder g θ ′ (t+1) (x), that takes x as input and outputs the reconstruction of x. We enlarge S by adding this specific node into its group. The procedure for building the graph and how a new node connects to the elements from G, as a sub-graph in DEGM, is shown in Fig. <ref type="figure">1</ref>. The importance of processing modules during LLL was considered in <ref type="bibr" target="#b1">(Aljundi, Kelchtermans, and Tuytelaars 2019;</ref><ref type="bibr" target="#b15">Jung et al. 2020</ref>). However, DEGM is the first model where this mechanism is used for the dynamic expansion of a graph model. Additionally, different from existing methods, the importance weighting approach proposed in this paper regularizes the transferable information during both the inference and generation processes. In the following, we propose a new objective function for training a Specific node, which also guarantees a lower bound to the marginal log-likelihood. Theorem 4 A Specific node is built for learning the (t + 1)-th task, which forms a sub-graph structure and can be trained by using a valid lower bound (ELBO) (See details in Appendix-J.1 from SM 1 ) :</p><formula xml:id="formula_31">ing to x = K i=1 π i g θi * (z),</formula><formula xml:id="formula_32">L M ELBO (x; M (t+1) ) =: E Q(z) log p θ ′ (t+1) ⊙{ θGI(1) ,..., θGI(K) } (x | z) − K i=1 π i KL Q ωGI(i) ⊙ ω ′ (t+1) (z | x) || p (z i ) , (<label>17</label></formula><formula xml:id="formula_33">)</formula><formula xml:id="formula_34">where q ωGI(i) ⊙ q ω ′ (t+1) (z | x) is the density function form of Q ωGI(i) ⊙ ω ′ (t+1) (z | x).</formula><p>We implement the variational distribution Q(z) by</p><formula xml:id="formula_35">K i=1 π i Q ωGI(i) ⊙ ω ′ (t+1) (z | x)</formula><p>, which is a mixture inference model. The difference in Eq. ( <ref type="formula" target="#formula_32">17</ref>) from q ω (z | x) in Eq. ( <ref type="formula" target="#formula_0">1</ref>) is that Q(z) is much more knowledge expressive by reusing the transferable information from previously learnt knowledge while also reducing the computational cost, by only updating the components {ω ′ (t+1) , θ ′ (t+1) } when learning the (t + 1)-th task. The first term in the RHS of Eq. ( <ref type="formula" target="#formula_32">17</ref>) is the negative reconstruction error evaluated by a single decoder. The second term consists of the sum of all KL terms weighted by their corresponding edge values π i . Model selection. We evaluate the negative marginal loglikelihood, (Eq. ( <ref type="formula" target="#formula_0">1</ref>) for Basic nodes, and Eq. ( <ref type="formula" target="#formula_32">17</ref>) for Specific</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>x</head><p>(1) </p><formula xml:id="formula_36">f   GI ( ) K f   GI ( 1 ) t f    1 K i i i     z z 1 z K z (1 ) g   G I ( ) K g   G I 1 x  K x  1 K i i i     x x  <label>(</label></formula><formula xml:id="formula_37">f    Input layer z 1  K  1  K  i  i  Figure 1:</formula><p>The graph structure in DEGM where an image is firstly processed by K Basic nodes to which the newly created Specific node connects during the inference process. This procedure is also performed at the decoding process.</p><p>nodes) for testing data samples after LLL. Then we choose the node with the highest likelihood for the evaluation. This mechanism can allow DEGM to infer an appropriate node without task labels (See details in Appendix-J.3 from SM 1 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results</head><p>Unsupervised Lifelong Learning Benchmark</p><p>Setting. We define a novel benchmark for the log-likelihood estimation under LLL, explained in Appendix-K from SM 1 . We consider learning multiple tasks defined within a single domain, such as <ref type="bibr">MNIST (LeCun et al. 1998) and</ref><ref type="bibr">Fashion (Xiao, Rasul, and</ref><ref type="bibr" target="#b41">Vollgraf 2017)</ref>. Following from <ref type="bibr" target="#b3">(Burda, Grosse, and Salakhutdinov 2015)</ref> we divide MNIST and Fashion into five tasks <ref type="bibr" target="#b54">(Zenke, Poole, and Ganguli 2017)</ref>, called Split MNIST (S-M) and Split Fashion (S-F). We use the cross-domain setting where we aim to learn a sequence of domains, called COFMI, consisting of Caltech 101 <ref type="bibr" target="#b8">(Fei-Fei, Fergus, and Perona 2007)</ref>, OMNIGLOT <ref type="bibr">(Lake, Salakhutdinov, and Tenenbaum 2015), Fashion, MNIST, In-verseFashion (IFashion)</ref> where each task is associated with a distinct dataset. All databases' images are binarized. Baselines. Our method is DEGM with ELBO and IWELBO bounds as DEGM-ELBO and DEGM-IWELBO-K ′ , respectively. We adapt the network architecture from <ref type="bibr" target="#b3">(Burda, Grosse, and Salakhutdinov 2015)</ref> and consider several baselines. A single VAE with GR is called ELBO-GR and when considering IWELBO bounds it becomes IWELBO-GR-K ′ where K ′ represent the number of weighted samples. We also compare with LIMix (Ye and Bors 2021e) and implement CN-DPM <ref type="bibr" target="#b22">(Lee et al. 2020</ref>) with the optimal setting, namely CN-DPM* (See details in Appendix-L.1 from SM 1 ).</p><p>Results. The testing data log-likelihood is estimated by the IWELBO bounds <ref type="bibr" target="#b3">(Burda, Grosse, and Salakhutdinov 2015)</ref> with K ′ = 5000. We perform five independent runs for S-M/S-F and COFMI data. We train various models under CCCOSCZC lifelong learning setting, where each task is associated with one of the datasets: CelebA <ref type="bibr" target="#b24">(Liu et al. 2015)</ref>, CACD (Chen, Chen, and Hsu 2014), 3D-Chair <ref type="bibr" target="#b2">(Aubry et al. 2014)</ref>, Ommiglot <ref type="bibr" target="#b20">(Lake, Salakhutdinov, and Tenenbaum 2015)</ref>, ImageNet* (Krizhevsky, Sutskever, and Hinton 2012), Car <ref type="bibr" target="#b42">(Yang et al. 2015)</ref>, Zappos <ref type="bibr" target="#b53">(Yu and Grauman 2017)</ref>, CUB <ref type="bibr" target="#b41">(Wah et al. 2010</ref>) (detailed dataset setting is provided in Appendix-L.1 from SM 1 ). The square loss (SL) is used to evaluate the reconstruction quality and the evaluation of other criteria is in Appendix-L.3 from SM 1 . The threshold for adding a new component to DEGM, as explained in Section , is τ = 600 on CCCOSCZC and the results are provided in Table <ref type="table" target="#tab_2">2</ref>. We can observe that the proposed DEGM outperforms other existing lifelong learning models and achieves a close result to DEGM-2 which trains individual VAEs for each task and requires more parameters. Visual results are shown in Appendix-L.6 from SM 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Empirical Results for Theoretical Analysis</head><p>We train a VAE on the binarized Caltech 101 database and use it to generate a set of images probabilistically consistent with the dataset. Then, ELBO-GR, IWELBO-GR-K ′ , where   K ′ ∈ {5, 50} corresponds to the number of importance samples used for training on the joint dataset consisting of the generated and a training set from a second task (Fashion). We evaluate the average target risk (LHS of Eq. ( <ref type="formula" target="#formula_14">10</ref>)) for these models in order to investigate the tightness between the negative log-likelihood (NLL) and LELBO, since NLL is a lower bound to LELBO. The IWELBO bounds considering 5000 weighted importance samples are shown in Fig. <ref type="figure" target="#fig_4">2a</ref>. Although, Lemma 1 considers the Gaussian decoder, VAEs with a Bernoulli decoder, corresponding to the IWELBO bound, indicates that IWELBO-GR-50 is a lower bound to IWELBO-GR-5 and ELBO-GR, which empirically proves L LELBO50 ≤ L LELBO5 when the generator distribution is fixed and |KL 1 − KL 2 | = 0, as discussed in Lemma 1.</p><p>We also train a VAE whose decoder outputs the mean vector of a Gaussian distribution with the Identity matrix as its covariance, under MNIST, Fashion and IFashion LLL, where images are greylevel with pixels in <ref type="bibr">[0,</ref><ref type="bibr">255]</ref>. The reconstruction error ELBO is normalized by dividing with the image size (28 × 28), as in <ref type="bibr" target="#b32">(Park, Kim, and Kim 2019)</ref>. We evaluate the risk and the discrepancy distance for each training epoch, according to Eq. ( <ref type="formula">9</ref>) from Lemma 1 with the results provided in Fig. <ref type="figure" target="#fig_4">2b</ref>, where the source risk (the first term in RHS of Eq. ( <ref type="formula">9</ref>)) keeps stable and the discrepancy distance disc L (•), Eq. ( <ref type="formula">2</ref>), represented within R A (•), increases while learning more tasks. The 'KL divergence,' calculated as |KL 1 − KL 2 |, shown in Fig. <ref type="figure" target="#fig_4">2b</ref> increases slowly. This demonstrates that the discrepancy distance plays an important role on shrinking the gap for the GB. An ablation study, demonstrating the effectiveness of the proposed expansion mechanism, is provided in Appendix-L.4 from SM 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper we analyze the forgetting behaviour of VAEs by finding an upper bound on the negative marginal loglikelihood, called LELBO. This provides insights into the generalization performance on the target distribution when the source distribution evolves continuously over time during lifelong learning (LLL). We further develop a Dynamic Expansion Graph Model (DEGM), which adds new Basic and Specific components to the network, depending on a knowledge novelty criterion during LLL. DEGM can significantly reduce the accumulated errors caused by the forgetting process. The empirical and theoretical results verify the effectiveness of the proposed DEGM methodology.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Theorem 3</head><label>3</label><figDesc>Let C = {c 1 , . . . , c m } represent a set, where each item c i indicates that the c i -th component (M 1 ci ) is only trained once during LLL. We use A = {a 1 , . . . , a m } to represent the task label set for C, where a i is associated to c i . Let C ′ = {c ′ 1 , . . . , c ′ k } represent a set where c ′ i indicates that the c ′ i -th component M c ′ i is trained more than once and is associated with a task label set A ′ c ′ i = {a(i, 1), . . . , a(i, n)}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>where K is the number of components in the mixture model and | • | is the cardinality of a set. Let Ã = {ã 1 , . . . , ãk } represent a set where each ãi denotes the number of tasks modelled by the probabilistic representations of the c</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Target risk as in Eq. (10).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The estimation of the target and source risks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>where the intermediate feature Results for Split MNIST, Split Fashion and COFMI.</figDesc><table><row><cell>Methods</cell><cell>S-M</cell><cell>S-F</cell><cell>COFMI</cell></row><row><cell>ELBO-GR</cell><cell>-98.23</cell><cell>-240.58</cell><cell>-177.47</cell></row><row><cell>IWELBO-GR-50</cell><cell>-93.57</cell><cell>-236.66</cell><cell>-172.10</cell></row><row><cell>IWELBO-GR-5</cell><cell>-95.80</cell><cell>-238.08</cell><cell>-176.21</cell></row><row><cell>ELBO-GR*</cell><cell>-98.36</cell><cell>-243.91</cell><cell>-180.50</cell></row><row><cell>IWELBO-GR*-50</cell><cell>-91.23</cell><cell>-236.90</cell><cell>-188.9</cell></row><row><cell>CN-DPM*-IWELBO-50</cell><cell>-95.91</cell><cell>-237.47</cell><cell>-184.19</cell></row><row><cell>LIMix-IWELBO-50</cell><cell>-95.74</cell><cell>-237.48</cell><cell>-184.32</cell></row><row><cell>DEGM-ELBO</cell><cell>-93.51</cell><cell>-238.54</cell><cell>-168.91</cell></row><row><cell>DEGM-IWELBO-50</cell><cell>-88.04</cell><cell>-233.76</cell><cell>-163.27</cell></row><row><cell>DEGM-IWELBO-5</cell><cell>-91.44</cell><cell>-235.93</cell><cell>-164.99</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The average results are reported in Table1where '*' denotes that the model uses two stochastic layers (See details in Appendix-K.1 from SM 1 ) which can further improve the performance with the IWELBO bound, according to IWELBO-GR*-50. The proposed DEGM-IWELBO-50 obtains the best results when Results under CCCOSCZC lifelong learning. using the IWELBO bound, for both S-M and S-F settings. The proposed DEGM also outperforms other baselines on COFMI, which represents a more challenging task than S-M/S-F. The detailed results for each task are reported in Appendix-K.2 from SM 1 , showing that ELBO-GR* and IWELBO-GR*-50 tend to degenerate the performance on the early tasks under the cross-domain learning setting when compared with VAEs that do not use two stochastic layers. Details, such as the number of Basic and Specific nodes used are provided in Appendix-K.2 from SM 1 .</figDesc><table><row><cell>Dataset</cell><cell cols="3">BE LIMix LGM DEGM DEGM-2 CN-DPM*</cell></row><row><cell>CelebA</cell><cell>213.9 214.2 535.6 229.2</cell><cell>217.0</cell><cell>215.4</cell></row><row><cell>CACD</cell><cell cols="2">414.9 353.5 814.3 368.3 281.95</cell><cell>347.3</cell></row><row><cell cols="3">3D-Chair 649.1 353.1 2705.9 324.0 291.46</cell><cell>513.8</cell></row><row><cell cols="2">Omniglot 875.1 351.1 5958.9 225.6</cell><cell>195.7</cell><cell>343.2</cell></row><row><cell cols="2">ImageNet* 758.4 778.5 683.1 689.6</cell><cell>652.8</cell><cell>769.1</cell></row><row><cell>Car</cell><cell>745.1 688.19 583.7 588.8</cell><cell>565.9</cell><cell>709.8</cell></row><row><cell>Zappos</cell><cell>451.1 283.4 431.2 263.4</cell><cell>275.8</cell><cell>280.7</cell></row><row><cell>CUB</cell><cell>492.0 400.7 330.2 461.3</cell><cell>569.6</cell><cell>638.6</cell></row><row><cell>Average</cell><cell>575.0 427.8 1505.4 393.8</cell><cell>381.3</cell><cell>477.2</cell></row><row><cell cols="3">Comparing to Lifelong Learning Models</cell><cell></cell></row><row><cell cols="4">Baselines. A very strong LLL baseline, called DEGM-2,</cell></row><row><cell cols="4">consists of dynamically creating a new VAE for each new</cell></row><row><cell cols="4">task, trained for achieving the best performance. Meanwhile,</cell></row><row><cell cols="4">Batch Ensemble (BE) (Wen, Tran, and Ba 2020) is designed</cell></row><row><cell cols="4">for classification tasks. We implement each component of</cell></row><row><cell cols="4">BE as a VAE. DEGM is trained by using ELBO without</cell></row><row><cell cols="4">considering the IWELBO bound, aiming for reducing com-</cell></row><row><cell cols="4">putational complexity. The number of parameters required</cell></row><row><cell cols="4">by various models are provided in Appendix-L.5 from SM 1 .</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Life-long disentangled representation learning with cross-domain latent homologies</title>
		<author>
			<persName><forename type="first">A</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eccles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Inf. Proc. Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9873" to="9883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Task-free continual learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kelchtermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11254" to="11263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Seeing 3D chairs: exemplar part-based 2D-3D alignment using a large dataset of CAD models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3762" to="3769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The Autoencoding Variational Autoencoder</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cemgil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghaisas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dvijotham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gowal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.00519</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Cont. of Learning Representations (ICLR)</title>
				<meeting>Int. Cont. of Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015">2015. 2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="15077" to="15087" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Efficient lifelong learning with A-GEM</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00420</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Learning Representations (ICLR)</title>
				<meeting>Int. Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cross-Age Reference Coding for Age-Invariant Face Recognition and Retrieval</title>
		<author>
			<persName><forename type="first">B.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf on Computer Vision (ECCV)</title>
				<meeting>European Conf on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8694</biblScope>
			<biblScope unit="page" from="768" to="783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Symmetric variational autoencoder and connections to adversarial learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carin</forename></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Artificial Intel. and Statistics (AISTATS) 2018</title>
				<meeting>Int. Conf. on Artificial Intel. and Statistics (AISTATS) 2018</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="661" to="669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05908</idno>
		<title level="m">Tutorial on variational autoencoders</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning generative visual models from few training examples: An incremental Bayesian approach tested on 101 object categories</title>
		<author>
			<persName><forename type="first">J</forename><surname>Domke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Sheldon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2007">2018. 2007</date>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="59" to="70" />
		</imprint>
	</monogr>
	<note>Importance weighting and variational inference</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Catastrophic forgetting in connectionist networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>French</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="128" to="135" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Inf. Proc. Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improved Schemes for Episodic Memory-based Lifelong Learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rosing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1023" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">β-VAE: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Learning Representations (ICLR)</title>
				<meeting>Int. Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hierarchical importance weighted autoencoders</title>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sankaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dhekane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno>PMLR 97</idno>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2869" to="2878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.00122</idno>
		<title level="m">Less-forgetting learning in deep neural networks</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Continual Learning with Node-Importance based Adaptive Group Sparse Regularization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3647" to="3658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recursive Inference for Variational Autoencoders</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="19632" to="19641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Proc. Systems (NIPS)</title>
				<meeting>Advances in Neural Inf. . Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4743" to="4751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Im-ageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Proc. Systems (NIPS)</title>
				<meeting>Advances in Neural Inf. . Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2012">2013. 2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Auto-encoding variational Bayes</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation based on source-guided discrepancy</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kuroki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Charoenphakdee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Honda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. on Artificial Intelligence</title>
				<meeting>AAAI Conf. on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4122" to="4129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE</title>
				<meeting>of the IEEE</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A Neural Dirichlet Process Mixture Model for Task-Free Continual Learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.00689</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Learning Representations (ICLR)</title>
				<meeting>Int. Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning without forgetting</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2935" to="2947" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Int. Conf. on Computer Vision (ICCV)</title>
				<meeting>of IEEE Int. Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Auxiliary deep generative models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Maaløe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Machine Learning (ICML)</title>
				<meeting>Int. Conf. on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1445" to="1453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Domain adaptation: Learning bounds and algorithms</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06715</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. on Learning Theory (COLT)</title>
				<meeting>Conf. on Learning Theory (COLT)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adversarial Variational Bayes: Unifying variational autoencoders and generative adversarial networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Machine Learning (ICML)</title>
				<meeting>Int. Conf. on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2391" to="2400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Doubly semi-implicit variational inference</title>
		<author>
			<persName><forename type="first">D</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kharitonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sobolev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Artificial Intelligence and Statistics (AISTATS)</title>
				<meeting>Int. Conf. on Artificial Intelligence and Statistics (AISTATS)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="2593" to="2602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Variational continual learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int</title>
				<meeting>Int</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<idno type="arXiv">arXiv:1710.10628</idno>
		<title level="m">Conf. on Learning Representations (ICLR)</title>
				<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Continual Deep Learning by Functional Regularisation of Memorable Past</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Swaroop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Immer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Eschenhagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E E</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4453" to="4464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Variational Laplace autoencoders</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<idno>PMLR 97</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Machine Learning (ICML)</title>
				<meeting>Int. Conf. on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5032" to="5041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Lifelong Generative Modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ramapuram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gregorova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kalousis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">404</biblScope>
			<biblScope unit="page" from="381" to="400" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Continual Unsupervised Representation Learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Visin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="7645" to="7655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Machine Learning (ICML)</title>
				<meeting>Int. Conf. on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1530" to="1538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Riemer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Cases</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ajemian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Rish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to Learn without Forgetting By Maximizing Transfer and Minimizing Interference</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tesauro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.11910</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Learning Representations (ICLR)</title>
				<meeting>Int. Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Continual learning with deep generative replay</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Proc. Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2990" to="2999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">VEEGAN: Reducing mode collapse in GANs using implicit variational learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sobolev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Valkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">U</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2017">2019. 2017</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="3308" to="3318" />
		</imprint>
	</monogr>
	<note>Advances in Neural Inf. Proc. Systems (NIPS)</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">NVAE: A Deep Hierarchical Variational Autoencoder</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="19667" to="19679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">BatchEnsemble: an Alternative Approach to Efficient Ensemble and Lifelong Learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06715</idno>
		<idno>arXiv:1708.07747</idno>
	</analytic>
	<monogr>
		<title level="m">Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms</title>
				<imprint>
			<date type="published" when="2010">2010. 2020. 2017</date>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proc. Int. Conf. on Learning Representations (ICLR)</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A large-scale car dataset for fine-grained categorization and verification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Change Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3973" to="3981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Lifelong Teacher-Student Network Learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bors</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning Latent Representations Across Multiple Data Domains Using Lifelong VAE-GAN</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Bors</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Conference on Computer Vision (ECCV)</title>
				<meeting>of European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020">2020a</date>
			<biblScope unit="volume">12365</biblScope>
			<biblScope unit="page" from="777" to="795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Lifelong learning of interpretable image representations</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Bors</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Image Processing Theory, Tools and Applications (IPTA)</title>
				<meeting>Int. Conf. on Image essing Theory, Tools and Applications (IPTA)</meeting>
		<imprint>
			<date type="published" when="2020">2020b</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Mixtures of variational autoencoders</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Bors</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Image Processing Theory, Tools and Applications (IPTA)</title>
				<meeting>Int. Conf. on Image essing Theory, Tools and Applications (IPTA)</meeting>
		<imprint>
			<date type="published" when="2020">2020c</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep Mixture Generative Autoencoders</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Bors</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">InfoVAEGAN: Learning Joint Interpretable Representations by Information Maximization and Maximum Likelihood</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Bors</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Image Processing (ICIP)</title>
				<meeting>IEEE Int. Conf. on Image essing (ICIP)</meeting>
		<imprint>
			<date type="published" when="2021">2021c</date>
			<biblScope unit="page" from="749" to="753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning joint latent representations based on information maximization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Bors</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">567</biblScope>
			<biblScope unit="page" from="216" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Lifelong Infinite Mixture Model Based on Knowledge-Driven Dirichlet Process</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Bors</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
				<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021">2021e</date>
			<biblScope unit="page" from="10695" to="10704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Lifelong Mixture of Variational Autoencoders</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Bors</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2021">2021f</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Lifelong Twin Generative Adversarial Networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Bors</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Image Processing (ICIP)</title>
				<meeting>IEEE Int. Conf. on Image essing (ICIP)</meeting>
		<imprint>
			<date type="published" when="2021">2021g</date>
			<biblScope unit="page" from="1289" to="1293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Semantic Jitter: Dense Supervision for Visual Comparisons via Synthetic Images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Computer Vision (ICCV)</title>
				<meeting>IEEE Int. Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5571" to="5580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Continual learning through synaptic intelligence</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zenke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Int. Conf. on Machine Learning (ICML)</title>
				<meeting>of Int. Conf. on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="3987" to="3995" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
