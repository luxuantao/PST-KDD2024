<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">E</forename><surname>Emary</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Hossam</forename><forename type="middle">M</forename><surname>Zawbaa</surname></persName>
							<email>hossam.zawbaa@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Crina</forename><surname>Grosan</surname></persName>
						</author>
						<author>
							<persName><surname>Emary</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computers and Information</orgName>
								<address>
									<addrLine>Cairo Univer-sity</addrLine>
									<postCode>12613</postCode>
									<settlement>Giza</settlement>
									<country key="EG">Egypt</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Computers and Information</orgName>
								<orgName type="institution">Beni-Suef University</orgName>
								<address>
									<postCode>62511</postCode>
									<settlement>Beni-Suef</settlement>
									<country key="EG">Egypt</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Babes-Bolyai University</orgName>
								<address>
									<postCode>400084</postCode>
									<settlement>Cluj-Napoca</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Babes</orgName>
								<orgName type="institution">Bolyai University</orgName>
								<address>
									<postCode>400084</postCode>
									<settlement>Cluj-Napoca</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">College of Engineering, Design and Physical Sciences</orgName>
								<orgName type="institution">Brunel University</orgName>
								<address>
									<postCode>UB8 3PH</postCode>
									<settlement>London</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FE88FEBBB86F69804F67CD4BD71CAE90</idno>
					<idno type="DOI">10.1109/TNNLS.2016.2634548</idno>
					<note type="submission">received May 26, 2016; revised September 25, 2016; accepted November 25, 2016.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Adaptive exploration rate</term>
					<term>artificial neural network (ANN)</term>
					<term>experienced gray wolf optimization (EGWO)</term>
					<term>gray wolf optimization (GWO)</term>
					<term>reinforcement learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, a variant of gray wolf optimization (GWO) that uses reinforcement learning principles combined with neural networks to enhance the performance is proposed. The aim is to overcome, by reinforced learning, the common challenge of setting the right parameters for the algorithm. In GWO, a single parameter is used to control the exploration/exploitation rate, which influences the performance of the algorithm. Rather than using a global way to change this parameter for all the agents, we use reinforcement learning to set it on an individual basis. The adaptation of the exploration rate for each agent depends on the agent's own experience and the current terrain of the search space. In order to achieve this, experience repository is built based on the neural network to map a set of agents' states to a set of corresponding actions that specifically influence the exploration rate. The experience repository is updated by all the search agents to reflect experience and to enhance the future actions continuously. The resulted algorithm is called experienced GWO (EGWO) and its performance is assessed on solving feature selection problems and on finding optimal weights for neural networks algorithm. We use a set of performance indicators to evaluate the efficiency of the method. Results over various data sets demonstrate an advance of the EGWO over the original GWO and over other metaheuristics, such as genetic algorithms and particle swarm optimization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>parameters has to be initialized. The parameter values are at most suboptimal, as there are never sufficient trials to get the optimal values.</p><p>According to <ref type="bibr" target="#b0">[1]</ref>, learning algorithms suffer from the following.</p><p>1) The Curse of Dimensionality: Many algorithms need to discretize the state space, which is impossible for control problems with high dimensionality, because the number of discrete states is enormous. 2) A Large Number of Learning Trials: Most algorithms need a large number of learning trials, specifically if size of the state space is high, so it is very difficult to apply reinforcement learning to real world tasks. 3) Finding Proper Parameters for the Algorithms: Many algorithms work well, but only with the right parameter setting. Searching for an appropriate parameter setting is, therefore, crucial, in particular, for time-consuming learning processes. Thus, algorithms which work with fewer parameters or allow a wider range of parameter setting are preferable. 4) The Need of a Skilled Learner: Since defining the reward function is not enough, we must also determine a good state-space representation or a proper function approximator, choose an appropriate algorithm and set the parameters of the algorithm. Consequently, additional knowledge and experience are needed when dealing with such learning. In this paper, we focus on the automatic setting of parameters used by gray wolf optimization (GWO) algorithm, in particular, on the setting of a parameter that plays a significant role in the final result: the exploration/exploitation rate. The metalearner employed for the automatic parameter setting uses reinforcement learning principles <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>.</p><p>In the conventional reinforcement learning model, an agent is connected to its environment via perception and action. On each step of interaction, the agent receives as input some indications of the current state of the environment and chooses an action that changes the state, and the value of this state transition is reached to the agent through a scalar reinforcement signal <ref type="bibr" target="#b3">[4]</ref>. When the search agent chooses an action, it obtains a feedback for that action and uses the feedback to update its data of state-action map. The goal is to determine the actions that tend to increase the long-run sum of values of the reinforcement signal <ref type="bibr" target="#b4">[5]</ref>.</p><p>In order to determine the right set of actions and, in particular, the right action at each time step in GWO, we propose to use neural networks. The input of the neural network is the set of all individual action history of all the agents, and the output is the action to be taken by a particular agent. Which means, the action of each agent is individually set in a reinforcement learning manner and a neural network learns it. The methodology is explained in detail in Section III, after a brief introduction of the original GWO algorithm in Section II. The validation of the proposed parameter learning methodology is achieved by considering two optimization problems: feature selection and weight training in neural networks. We perform two types of experiments, one related to feature selection and the other related to multilayer artificial neural networks (ANNs) weights optimization. Experiments on 21 data sets are performed in Section IV for feature selection and on 10 data sets for ANNs weight training and are validated using various metrics and statistical tests. The results indicate a very promising performance of the proposed methodology compared with the manual parameter setting in the original GWO. A similar approach could be either extended to other learning algorithms or, even more complex, generalized to the classes of similar algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PRELIMINARIES</head><p>Bioinspired optimization methods are becoming common among researchers due to their simplicity and extensibility. GWO is a relatively new optimization algorithm inspired by the social hierarchy and hunting behavior of the gray wolves in nature <ref type="bibr" target="#b5">[6]</ref>. A modified GWO is proposed which employed a good balance between the exploration and exploitation <ref type="bibr" target="#b6">[7]</ref>. A multiobjective variant of GWO is developed to optimize the multiobjective problems <ref type="bibr" target="#b7">[8]</ref>. A new binary version of GWO is proposed and applied to feature selection problem <ref type="bibr" target="#b8">[9]</ref>. In the next subsections (II-A and II-B), we briefly explain the general principles of GWO algorithm and the role of the parameters in the search process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Gray Wolf Optimization</head><p>GWO computationally simulates the hunting mechanism of gray wolves. Gray wolves live in a pack with a strict hierarchy: on the top are the alpha wolves, responsible for decision making, followed by beta and delta wolf. The rest of the pack is called omegas <ref type="bibr" target="#b5">[6]</ref>. Table <ref type="table" target="#tab_0">I</ref> shows the corresponding search Fig. <ref type="figure">1</ref>. GWO in the search space positioning (adapted from <ref type="bibr" target="#b5">[6]</ref>). and optimization stages of the GWO steps. Naturally, the prey location is the optimal solution and the wolves represent potential solutions in the search space. The wolves closer to the prey are the alpha wolves and they are the best solutions so far. Hierarchically, the beta wolves are the second best solutions and the delta wolves are the third best solutions. Their location in the search space is represented as X α , X β , and X δ . Omegas update their position in the search space based on their relative positions from alpha, beta, and delta wolves. Fig. <ref type="figure">1</ref> shows the positioning of the wolves and prey and the parameters involved in the equations used for updating the positions of the wolves in the search space. For hunting prey, a set of steps are to be applied as follows: prey encircling, hunting, attack, and search again.</p><p>1) Prey Encircling: The pack encircles a prey by repositioning individual agents according to the prey location, as follows:</p><p>-→</p><formula xml:id="formula_0">X (t + 1) = -→ X p (t) + -→ A • -→ D (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where t is the iteration, -→ X p is the prey position, -→ X is the gray wolf position, the . operator indicates vector entrywise multiplication, and -→ D is defined as follows:</p><formula xml:id="formula_2">-→ D = | -→ C • -→ X p (t) - -→ X (t)| (2)</formula><p>where -→ A and -→ C are coefficient vectors calculated as follows:</p><formula xml:id="formula_3">-→ A = 2a • -→ r 1 -a (3) -→ C = 2 -→ r 2 (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>where a is linearly diminished over the course of iterations controlling exploration and exploitation, and -→ r 1 and -→ r 2 are random vectors in the range of [0, 1]. The value of a is the same for all wolves. These equations indicate that a wolf can update its position in the search space around the prey in any random location.</p><p>2) Hunting: It is performed by the whole pack based on the information coming from the al pha, beta, and delta wolves, which are expected to know the prey location, as given in the following:</p><formula xml:id="formula_5">-→ X (t + 1) = -→ X 1 + -→ X 2 + -→ X 3 3 (5)</formula><p>where -→ X 1 , -→ X 2 , and -→ X 3 are defined as follows:</p><formula xml:id="formula_6">-→ X 1 = | -→ X α - -→ A 1 • -→ D α | (6) -→ X 2 = | -→ X β - -→ A 2 • -→ D β | (7) -→ X 3 = | -→ X δ - -→ A 3 • -→ D δ |<label>(8)</label></formula><p>where -→ X α , -→ X β , and -→ X δ are the first three best solutions at a given iteration t, -→ A 1 , -→ A 2 , and -→ A 3 are defined as in (3), and -→ D α , -→ D β , and -→ D δ are defined using the following:</p><formula xml:id="formula_7">-→ D α = | -→ C 1 • -→ X α - -→ X | (9) -→ D β = | -→ C 2 • -→ X β - -→ X | (10) -→ D δ = | -→ C 3 • -→ X δ - -→ X |<label>(11)</label></formula><p>where</p><formula xml:id="formula_8">-→ C 1 , -→ C 2</formula><p>, and -→ C 3 are defined as in ( <ref type="formula" target="#formula_3">4</ref>). This is interpreted by the fact that alpha, beta, and delta wolves know the best position of the prey and all the other wolves adapt their positions based on the position of these wolves.</p><p>3) Attacking Stage: The agents approach the prey, which is achieved by decrementing the exploration rate a. Parameter a is linearly updated in each iteration to range from 2 to 0 as follows:</p><formula xml:id="formula_9">a = 2 -t 2 Max Iter (<label>12</label></formula><formula xml:id="formula_10">)</formula><p>where t is the iteration number and Max Iter is the total number of iterations allowed for the optimization. According to <ref type="bibr" target="#b5">[6]</ref>, exploration and exploitation are guaranteed by the adaptive values of a allowing GWO to transit smoothly between exploration and exploitation, while half of the iterations are dedicated to the exploration and the other half is assigned to exploitation. This stage is interpreted as wolves moving or changing their position to any random position between their current position and the prey position. 4) Search for Prey: Wolves diverge from each other to search for prey. This behavior is modeled by setting large values for parameter a to allow for exploration of the search space. Hence, the wolves diverge from each other to better explore the search space and then converge again to attack when they find a better prey. Any wolf can find a better prey (optimum). If they get closer to the prey, they will become the new alphas and the other wolves will be split into beta, delta, and omega according to their distance from the prey. Parameter a gives random weights to the prey and shows the impact of the prey in characterizing the separation of wolves as in (1) and <ref type="bibr" target="#b1">(2)</ref>. That helps GWO to demonstrate a more random behavior, favoring exploration and local optima evasion. It is worth mentioning that a provides random values at all times keeping in mind the aim to accentuate exploration not only at the beginning of the optimization process but until its end. GWO is described in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Feed-Forward Artificial Neural Networks</head><p>ANNs are a family of models inspired by biological neural networks. They are represented as systems of interconnected neurons that exchange messages between each other. The neural connections have numeric weights that can be tuned</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 GWO</head><p>Input: Number of gray wolves (n), maximum iterations (Max I ter ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Result:</head><p>The optimal wolf position and its fitness. 1) Initialize a population of n gray wolves positions randomly. 2) Find α, β, and δ as the first three best solutions based on their fitness values. 3) t = 0.</p><p>while t ≤ Max I ter do foreach W ol f i ∈ pack do Update current wolf's position according to Eq. ( <ref type="formula">5</ref>). end -Update a, A, and C as in Eqs. ( <ref type="formula" target="#formula_9">12</ref>), ( <ref type="formula" target="#formula_3">4</ref>) and ( <ref type="formula">3</ref>).</p><p>-Evaluate the positions of individual wolves.</p><p>-Update α, β, and δ positions as the first best three solutions in the current population.</p><p>-t = t + 1. end 4) Select the optimal gray wolf position. based on experience, making neural nets adaptive to inputs and capable of learning <ref type="bibr" target="#b9">[10]</ref>. The primary building block of a neural network is the neuron which has a set of weights. ANN weights are attached to the inputs and an internal nonlinear function maps the input of the neuron to the output given the transfer function as follows:</p><formula xml:id="formula_11">O i = f w T i • x + b i (<label>13</label></formula><formula xml:id="formula_12">)</formula><p>where O i is the output of neuron i , f is the activation function attached to neuron i , w i is a vector of weights attached to neuron i , x is the input vector of neuron i , and b i is a bias scalar value. Each neural network possesses knowledge that is contained in the connections weights values. Changing the knowledge stored in the network as a function of experience implies a learning rule for modifying the values of the weights. Hence, the most challenging problem in using the ANN models is to choose the appropriate weight bias adaptation method <ref type="bibr" target="#b9">[10]</ref>. Usually, the gradient descent method is used to adapt neural network weights based on the following formula:</p><formula xml:id="formula_13">w t +1 kj = w t kj -η ∂ E ∂w kj (<label>14</label></formula><formula xml:id="formula_14">)</formula><p>where w t kj is the weight linking neuron k to neuron j at time t, and E is suitable error function that computes the deviation between the targeted and the actual output.</p><p>According to the availability of training data, neural models can be passive or online. In passive neural models, the neural model is trained using the whole data set at once, while in the case of online models, the data points are not presented as a whole, but it is given one at a time <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED EXPERIENCED GRAY WOLF OPTIMIZATION</head><p>Intensification and diversification are the two key components of any metaheuristic algorithm <ref type="bibr" target="#b10">[11]</ref>. Intensification or exploitation uses the available local information to generate better solutions. Diversification or exploration explores the search space to generate diverse solutions. The balance between exploration and exploitation controls the search behavior of a given optimizer. Excess exploration makes the optimization process to converge quickly, but it may lead to premature convergence. Excess exploitation increases the probability of finding the global optimum, but often slows down the process with a much lower convergence rate. Therefore, resolving the tradeoff between exploration and exploitation is a must to ensure fast global convergence of the optimization process <ref type="bibr" target="#b11">[12]</ref>. In almost all modern swarm intelligence optimizers, the exploration rate is adapted throughout the optimization iterations in some predetermined manner to allow for further exploration at some iterations, commonly, at the beginning of the optimization process, and to allow for extra exploitation at some iterations, commonly, at the end of the optimization process. Some optimizers use exploration rate that is linearly inverse proportional to the iteration number. The original GWO linearly decrements the exploration rate as optimization progresses</p><formula xml:id="formula_15">ExpRate = 2 -(t) * (2/T ) (<label>15</label></formula><formula xml:id="formula_16">)</formula><p>where t is the current iteration, T is the total number of iterations, and E x p Rate is the rate of exploration. Although this formula proves efficient for solving numerous optimization problems, it still possesses the following drawbacks. 1) Stagnation: Once the optimizer approaches the end of the optimization process, it becomes difficult to escape the local optima and find better solutions, because its exploration capability becomes very limited. 2) Suboptimal Selection: At the beginning of the optimization process, the optimizer has very high exploration capability, but with this enhanced explorative power, it may leave promising regions to less promising ones. 3) Uniform Behavior: It is not uncommon to have the same value for the exploration rate for all the search agents, which forces the whole set of wolves to search in the same manner. The above-mentioned remarks motivate this paper in finding a way to adapt the exploration and to model it individually for each search agent. We proposed to use experience as a guide for automatically adjusting the exploration rate. The primary goal is to find a mechanism that maps the different states of search agents to an action set so that the long run goal or fitness function is optimized. To reach such goal and to confront the considerations mentioned earlier regarding such learning, we defined the following components of the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. State-Action Map</head><p>The mapping between the states and actions is commonly nonlinear and can be modeled using any nonlinear mapping model, e.g., neural network model. Incremental neural networks rather than the batch version of the neural network are more adequate, as every time the agent obtains new data, it must use it to update the neural network model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Action Set</head><p>For each individual wolf, in order to adapt its exploration rate, we propose a set of actions as follows.</p><p>1) Increase the Exploration Rate: It takes place as a result of wolf's self-confidence and expertise. This action commonly happens when the wolf finds itself succeeding in some consecutive iterations. The success at a given iteration t is defined by the capability of the search agent to maintain a fitness at time t that is better than its fitness at time t -1. This increases its own confidence and hence increases its exploration rate. Another situation that may motivate such action is that when a successive failure occurs, the agent may need to scout in the search space hoping to find better prey. 2) Decrease Exploration Rate: Agent's oscillation of fitness may motivate such action and it reflects wrong decision taken by the agent, and hence, it should be cautious in its movements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Keep Exploration Rate:</head><p>The current exploration rate is kept static as there is no motivation for neither increase nor decrease it. The above-mentioned three action will directly affect the exploration rate at the next iteration as follows:</p><formula xml:id="formula_17">ExpRat t +1 i = ⎧ ⎪ ⎨ ⎪ ⎩ ExpRat t i * (1 + )(Increase action) ExpRat t i * (1 -)(Decrease action) ExpRat t i (Keep action)<label>(16)</label></formula><p>where ExpRat t i is the exploration rate for agent i at time t and is the change factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Agent State</head><p>The actions performed change the state of a search agent and the agent is repositioned in the search space and hence acquire a new fitness value. A history of fitnesses of a given agent is used to control its next action so that the agent can accumulate and make use of its past decisions. Formally, the state increases, decreases, or keeps constant the fitness value for a search agent over a time span of T past successive rounds</p><formula xml:id="formula_18">State i t = . . . , sign f i t -3 -f i t -4 , sign f i t -2 -f i t -3 , sign f i t -1 -f i t -2 , sign f i t -f i t -1<label>(17)</label></formula><p>where State i t is the state vector attached to a given agent i at time t, f (t) i is the fitness function value for agent i at time t, and sign(x) is defined as</p><formula xml:id="formula_19">sign(x) = ⎧ ⎪ ⎨ ⎪ ⎩ 1 if x &lt; 0 -1 if x &gt; 0 0 otherwise. (<label>18</label></formula><formula xml:id="formula_20">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Feedback</head><p>When an agent leaves a region with good fitness to a region with worse fitness, it receives a negative feedback. On the contrary, when the search agent leaves a bad region to a better one, it receives positive feedback. The agent's own fitness is an indicator of the search region around and decides whether the agent should receive positive or negative feedback. Such feedback can be formulated as</p><formula xml:id="formula_21">Feedback t i = +1 if f agent t +1 i &lt; f agent t i -1 if f agent t +1 i ≥ f agent t i (19)</formula><p>where Feedback t i is the feedback for agent i at time t, and f (agent t i ) is the fitness of agent i at time t. Models that map situations to actions to maximize some reward functions are commonly called reinforcement learning models <ref type="bibr" target="#b12">[13]</ref>. In such models, the learner is not told which actions to take but instead discovers which actions yield to higher reward by trying them <ref type="bibr" target="#b1">[2]</ref>. Two distinguishing features are commonly used to identify such models: 1) trial-anderror search and 2) delayed reward. According to our problem formulation, we consider a set of actions, state, feedback, and the state-action mapping model. The state action mapping model is a neural network with a single hidden layer. ANNs are commonly used to map an unknown (generally nonlinear) function and have well-established training methods <ref type="bibr" target="#b9">[10]</ref>. The set of previously mentioned actions increase, decrease, and keep exploration rate are encoded into neural network nodes in the output layer. Therefore, the network has three nodes in the output layer corresponding to the three applicable actions, with 1 on the node indicating the action that will be applied and a 0 value on the other two nodes. The state vector described in ( <ref type="formula" target="#formula_18">17</ref>) with length T is used as input to the neural model. So, the number of nodes in the input layer is exactly T nodes. The hidden layer has a 2 * T + 1 nodes <ref type="bibr" target="#b9">[10]</ref>.</p><p>Common to reinforcement learning, the feedback signal, computed as in <ref type="bibr" target="#b18">(19)</ref>, is used to adapt the experience of the model through training. The neural weights are adapted according to the action taken and the feedback received at time t. The weights of the neural model are adapted by rewarding or punishing the winning node. The winning node, representing the applied action, is rewarded by moving its current output to be closer to 1 in the case of receiving positive feedback through adapting its attached weights <ref type="bibr" target="#b13">[14]</ref>. In the case of receiving negative feedback, the node attached to the applied action is punished by moving its output to be closer to 0 through updating its assigned weights. Equation <ref type="bibr" target="#b19">(20)</ref> employs the gradient descent method to adapt the weights of the output layer attached to the winning node given a target value of 1 or 0 in the case of positive or negative feedback</p><formula xml:id="formula_22">w t +1 i = w t i + ηx(d i -y i )y i (1 -y i ) (<label>20</label></formula><formula xml:id="formula_23">)</formula><p>where w t i is the weight for output node i at time t, x is the input state, y i is the actual output on node i , and d i is the desired output on node i which is set either to 1 when receiving positive feedback or 0 when receiving negative feedback. The update is propagated to the hidden weights as follows:</p><formula xml:id="formula_24">w t +1 i = w t i + ηx y j (1 -y j ) o i w i j ((d i -y i )y i (1 -y i )) (<label>21</label></formula><formula xml:id="formula_25">)</formula><p>where η is the learning rate, o is the number of actions, y j is the output of the hidden node j , y i is the output of node i , and x is the input state. The wining node/action selection can  be formulated as follows:</p><formula xml:id="formula_26">Winner = 3 min i=1 |1 -o i | (<label>22</label></formula><formula xml:id="formula_27">)</formula><p>where o i is the value of the output node i .</p><p>The main assumption to propose such a training model is that the state-action mapping applies a static unknown function to be estimated in a trial and error manner, but if such mapping is not static or changes slightly, the system tends to take random actions and no experience is acquired. Such a training manner is very natural when the trainability is lost in case of fast changing environment <ref type="bibr" target="#b14">[15]</ref>. Fig. <ref type="figure" target="#fig_0">2</ref> shows the total number of punishments and rewards acquired by all search agents during the optimization process. We can remark from Fig. <ref type="figure" target="#fig_0">2</ref> that as optimization progresses, the experience of the search agents is enhanced, and hence, the number of correct actions (rewards) increases, while the number of wrong actions (punishments) decreases. This proves the capability of the proposed strategy to converge to optimal state-action map and, hence, to optimally timed parameters. The proposed algorithm is called EGWO, and is formally given in Algorithm 2. Fig. <ref type="figure" target="#fig_1">3</ref> outlines the flow of state change as a response to actions produced by the neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS AND DISCUSSION</head><p>This section summarizes the results from applying the proposed experienced GWO on two main applications, namely, feature selection and neural network weight adaptation. The next two subsections (IV-A and IV-B) contain the results and the analysis for each application. We use a set of qualitative  <ref type="formula" target="#formula_22">20</ref>) and ( <ref type="formula" target="#formula_24">21</ref>). -Update W ol f i state vector according to Eq. ( <ref type="formula" target="#formula_18">17</ref>).</p><p>-Update the exploration parameter a i attached to W ol f i as in Eq. ( <ref type="formula" target="#formula_17">16</ref>). end -Update A and C as in Eqs. ( <ref type="formula">3</ref>) and ( <ref type="formula" target="#formula_3">4</ref>).</p><p>-Evaluate the positions of individual wolves.</p><p>-Update α, β, and δ positions as the first best three solutions in the current population.</p><p>-t = t + 1. end 6) Select the optimal gray wolf position. measures in order to analyze the results obtained by the methods we apply. The first three metrics give a measure of the mean, best, and worst expected performance of the algorithms. The fourth measure is adopted to show the ability of the optimizer to converge to the same optimal solution. The fifth and sixth metrics show the accuracy of the classification. The seventh and eighth metrics are a measure of the size of the selected features set (we expect a low number of features and a high accuracy). The last three metrics are used for directly comparing two algorithms and show whether the difference between them is significant or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Mean Fitness: It is an average value of all the solutions</head><p>in the final sets obtained by an optimizer in a number of individual runs <ref type="bibr" target="#b15">[16]</ref>. 2) Best Fitness: It is the best solution found by an optimizer in all the final sets resulted from a number of individual runs <ref type="bibr" target="#b15">[16]</ref>. 3) Worst Fitness: It is the worst solution found by an optimizer in all the final sets resulted from a number of individual runs <ref type="bibr" target="#b15">[16]</ref>. 4) Standard Deviation: It is used to ensure that the optimizer convergences to the same optimal and ensures repeatability of the results. It is computed over all the sets of final solutions obtained by an optimizer in a number of individual runs <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Classifier Mean Square Error (CMSE):</head><p>It is a measure of classifier's average performance on the test data.</p><p>It is averaged over all final sets in all the independent runs <ref type="bibr" target="#b17">[18]</ref>. 6) Root-Mean-Square Error (RMSE): It measures the root average squared error of the difference between actual output and the predicted one. It is averaged over all final sets in all the independent runs <ref type="bibr" target="#b17">[18]</ref>. 7) Average Selected Feature: It represents the average size of the selected features subset. The average is computed for each final set of solutions in multiple individual runs. 8) Average Fisher Score: It evaluates a feature subset such that in the data space spanned by the selected features, the distances between data points in different classes are as large as possible, while the distances between data points in the same class are as small as possible <ref type="bibr" target="#b18">[19]</ref>. Fisher score in this paper is calculated for individual features given the class labels; as follows:</p><formula xml:id="formula_28">F j = c k=1 n k μ j k -μ j 2 (σ j ) 2<label>(23)</label></formula><p>where F j is the Fisher index for feature j , μ j and (σ j ) 2 are the mean and standard deviation (std) of the whole data set, n k is the size of class k, and μ j k is the mean of class k. The Fisher for a set of features is defined as</p><formula xml:id="formula_29">F tot = 1 S S i=1 F i (<label>24</label></formula><formula xml:id="formula_30">)</formula><p>where S is the number of selected features. The average Fisher score over a set of N runs is defined as</p><formula xml:id="formula_31">Fishr-score = 1 N N i=1 F i tot (<label>25</label></formula><formula xml:id="formula_32">)</formula><p>where F i tot is the Fisher score computed for selected feature set on run i . 9) Wilcoxon Rank Sum Test: It is a nonparametric test for significance assessment. The test assigns rank to all the scores considered as one group and, then, sums the ranks of each group <ref type="bibr" target="#b19">[20]</ref>. The test statistic relays on calculating W as follows:</p><formula xml:id="formula_33">W = N i=1 sgn(x 2,i -x 1,i ).R i ) (<label>26</label></formula><formula xml:id="formula_34">)</formula><p>where x 2,i and x 1,i are the best fitness obtained from the first and second optimizers on run i , R i is the rank of difference between x 2,i and x 1,i , and sgn(x) is the standard sign function. 10) T-Test: It measures the statistical significance and decides whether or not the difference between the average values of two sample groups reflects the real difference in the population (set) the groups were sampled from <ref type="bibr" target="#b20">[21]</ref>, as follows:</p><formula xml:id="formula_35">t = x -μ 0 S √ n (<label>27</label></formula><formula xml:id="formula_36">)</formula><p>where μ 0 is the mean of the t-distribution and (S/ √ n) is its std. 11) Average Run Time: It is the time (in seconds) required by an optimization algorithm for a number of different runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. EGWO Applied for Feature Selection</head><p>Finding a feature combination that maximizes a given classifier performance is a challenging problem. A data set with k features has 2 k different possible choices. The wrapper-based method for feature selection is a very common and reasonably efficient approach, but it comes with a huge processing cost as the classifier must be evaluated at each selected feature combination <ref type="bibr" target="#b21">[22]</ref>. The main characteristic of the wrapperbased method is the use of the classifier as a guide to feature selection procedure <ref type="bibr" target="#b22">[23]</ref>. The classifier adopted in this paper is the K -nearest neighbor (KNN) <ref type="bibr" target="#b23">[24]</ref>. KNN is a supervised learning algorithm that classifies an unknown sample instance based on the majority of the KNN category. According to the direct formulation of wrapper-based approach for feature selection, the evaluation criteria (the fitness to optimize) is formulated as</p><formula xml:id="formula_37">Fitness(D) = α E M (D) + β |M| |N|<label>(28)</label></formula><p>where E M (D) is the error rate for the classifier of condition feature set D, M is the size of selected feature subset, and N is the total number of features. α∈[0, 1] and β = 1 -α are constants which control the importance of classification accuracy and feature reduction. Any possible combination of features D can be evaluated, and hence, this function is continuously defined on the whole feature space with each component of the vector D in the range [0, 1] and such fitness function is generally nondifferentiable.</p><p>The search methods employed in this paper are the proposed EGWO, GWO, particle swarm optimization (PSO) <ref type="bibr" target="#b24">[25]</ref>, and genetic algorithms (GAs) <ref type="bibr" target="#b25">[26]</ref>.</p><p>1) Initialization: Four initialization methods are adopted in this paper differing from one another with respect to the following aspects.</p><p>1) Population Diversity: The ability of an optimizer to produce variants of the given initial population is a valuable property. 2) Closeness to Expected Optimal Solution: The capability to efficiently search the space for the optimal solution is must for a successful optimizer. Hence, it is intended to force the initial search agents to be apart from or close to the expected optimal solution. 3) Resemblance to Forward and Backward Selection: Each of these has its own strengths and weaknesses, and hence, we would like to assess the initialization impact on the feature selection process. Fig. <ref type="figure" target="#fig_2">4</ref> shows the initial wolves positions using the four initialization methods and the details about these techniques are as follows.</p><p>a) Small initialization: Search agents are initialized with a small number of randomly selected features. Therefore, if the number of agents is less than the number of features, we will see that each search agent will have a single dimension with value 1. Of course, the optimizer will search for feature(s) to be set to 1 to enhance the fitness function value as in the standard forward selection of features. This method is expected to test the global search ability of an optimizer as the initial search agents' positions are commonly away from the expected optimum. Therefore, the optimizer has to use global search operators to derive better solutions.</p><p>b) Mixed initialization: Half of the search agents are initialized using the small initialization and the other half are initialized using the large initialization method with more random features. Some search agents are close to the expected optimal solution and the other search agents are away from it. Hence, it provides much more diversity of the population as the search agents are expected to be far from each other. This method takes both the merits of small and large initialization <ref type="bibr" target="#b22">[23]</ref>.</p><p>c) Uniform initialization: Each feature has the same probability of being selected. This method is the most common initialization where the agents are randomly placed in the search space.</p><p>d) MRMR initialization: The minimum redundancy maximum relevance (MRMR) combines two criteria for feature selection <ref type="bibr" target="#b26">[27]</ref>, namely, relevance with the target class and redundancy to other features. In order to define MRMR, we will use the mutual dependence between two random variables X and Y given as</p><formula xml:id="formula_38">I (X; Y ) = x∈X y∈Y p(x; y)log p(x; y) p(x) p(y)<label>(29)</label></formula><p>where p(x; y) is joint probability distribution of x and Y , and p(x) and p(y) are the marginal probability distribution functions of X and Y .</p><p>For our feature selection problem, let F i and F j be two features from the set F of N features and c the class label. IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS</p><p>The maximum relevance method selects the top M best features based on their mutual dependence with class c</p><formula xml:id="formula_39">max F 1 N F i ∈F I (F i ; c). (<label>30</label></formula><formula xml:id="formula_40">)</formula><p>These top M features may not be the best M features as there might be correlations among them. Thus, the redundancy is removed using the minimum redundancy criterion min</p><formula xml:id="formula_41">F 1 N 2 F i ,F j ∈F I (F i ; F j ). (<label>31</label></formula><formula xml:id="formula_42">)</formula><p>The MRMR initialization combines both these objectives. Iteratively, if M -1 features are selected in the set S M-1 , the Mth feature is selected by maximizing the single variable relevance minus redundancy max</p><formula xml:id="formula_43">F i ∈F -S M-1 ⎛ ⎝ I (F i ; c) - 1 M -1 F j ∈S M-1 I (F i ; F j ) ⎞ ⎠ . (32)</formula><p>Using the concept of mutual information, the MRMR method selects variables that have the highest relevance with the target class and minimally redundant as well, i.e., dissimilar to each other. MRMR is normalized along the feature set as</p><formula xml:id="formula_44">P d i = MRMR i max d j =1 MRMR j (<label>33</label></formula><formula xml:id="formula_45">)</formula><p>where P i is the search agent position in dimension d, MRMR i is the MRMR value for feature i , and d is the problem dimension.</p><p>MRMR is used to initialize one search agent and the rest of search agents are set at random positions in the search space.</p><p>2) Data Sets: The 21 data sets in Table <ref type="table" target="#tab_0">II</ref> from the UCI machine learning repository <ref type="bibr" target="#b27">[28]</ref> are used for tests and comparisons. The data sets are selected to ensure a large variety regarding the number of features and the number of instances. Each data set is divided randomly into three different equal parts for validation, training, and testing using cross validation. Each experiment with each algorithm repeated 30 times to ensure the stability and the statistical significance of the results.</p><p>3) Parameter Settings: The global and optimizer-specific parameter settings are outlined in Table <ref type="table" target="#tab_0">III</ref>. All the parameters are set either according to domain-specific knowledge as the α and β parameters of the fitness function, or based on a trial and error methodology on small simulations or from previous experiments reported in the literature in the case of the rest parameters.</p><p>4) Results and Discussion: Fig. <ref type="figure" target="#fig_3">5</ref>(a) shows the statistical indicators computed for GWO, EGWO, PSO, and GA using the uniform initialization method. We can observe that the performance of EGWO is superior to that of GWO in the average performance. We can also see that the std of EGWO is comparable to that of GWO, which ensures repeatability of results and convergence to the same optimum. The enhanced performance of EGWO is due to the fact that the search agents learn the ill-promising regions of the search space and jump out of these regions to more promising regions. That possible all stages, not only at the beginning of search process due to the way in which the exploration rate parameter a i is controlled. Since every search agent has its own exploration parameter, the diversity of behavior is assured and hence tolerates for stagnation particularly in the end stages of optimization. Individual agents in the EGWO swarm can still increase the exploration rate even in the latter stages of the optimization process, allowing for escaping local minima and premature convergence. In the standard GWO, the exploration rate is linearly decreased during the search process, allowing for a large exploration at the beginning and more local search toward the end. The same conclusion can be derived from the results obtained using the small, mix, and MRMR initialization methods in Fig. <ref type="figure" target="#fig_3">5(b)-(d)</ref>, respectively, when EGWO performs better in average than the other methods. Small initialization forces the search agents to be initialized away from the expected optimal solution and the agents are forced to keep diversity. Hence, it adds difficulty to the optimizer to escape from such situation. EGWO has an advantage again here as it adapts its exploration rate based on (what has learned from) experience rather than using a formula that does it iteratively and ignoring previous performance and can quickly adapt its  exploration rate to approach the global optima while leaving the nonpromising regions. Fig. <ref type="figure" target="#fig_4">6</ref> outlines the snapshot of the exploration rates for five agents where we can see the difference in such rate between the different agents which motivates the global fitness function to reach the optimum.</p><p>When using a mixed initialization, all optimizers perform better than using small initialization and uniform initialization which can be interpreted by the variability of the initial position in the search space, some search agents being initialized close to the expected optimal solution. We can remark that the EGWO still performs better. Fig. <ref type="figure" target="#fig_5">7</ref> shows the behavior of a single search agent where we can see that the search agent adapts its own exploration in response to its own fitness and such behavior allows the search agent to keep its exploration capability even at the end stages of the optimization, trying to find better promising regions.  We can observe very good results using the MRMR initialization for all the algorithms. The good performance can be explained by the fact that the initial population already contains a nearly optimal solution and the optimizer is required only to enhance such solution. All these methods include a mechanism by which the solutions in the swarm tend to follow the best solution found so far. But, there are still situations in which the nearly optimal solution can be far away from the global optimum, requiring again very good explorations of the search space. This is why the exploration ability of EGWO brings it again in advantage compared with the other methods, not only regarding the accuracy of the convergence but also the size of the selected features, EGWO being able to detect the smallest number of relevant features for most of the test cases.</p><p>Tables IV-VII show the performance on the test data in terms of classification accuracy, the average size of the selected features set, and the average Fisher score. For the uniform initialization where the initial population is very random, EGWO still obtains the smallest features set for about 77% of the data sets, with the best classification accuracy for 72% of the data sets and the best Fisher score for over 66% of the data while for the remaining data sets still obtains very good results.</p><p>For the small initialization, EGWO still dominates the other methods in over 50% of the data sets. Results are better in the case of mixed initialization and MRMR initialization. EGWO obtains the smallest number of features for over 76% of the data sets, the best accuracy for over 57% of the data sets, and the best Fisher score for over 50% of the tests.</p><p>Regarding the running time (results in Table <ref type="table" target="#tab_6">VIII</ref>), EGWO and GWO perform slower than PSO and GA for the majority of the data sets, but the difference is not really high in all the cases. The extra time is consumed in the continual training of the neural network model and the retrieval of action decision. Such increase in time consumption in comparison to enhanced optimization seems to be tolerable in many applications. This could be possibly improved by applying only reinforcement  level of 5% when using the uniform and MRMR methods, while less importance value is observed when using the small and mixed initializations. This confirms that the learning of the adaptation rate in EGWO really helps the optimization process, regardless of the initial position of the agents, whether close to an optimum or random over the search space. We can also observe that the proposed EGWO has significant advance over PSO and GA using Wilcoxon and T-test at a significance level of 5% regardless of the used initialization method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multilayer ANNs Weight Adaptation</head><p>Feed-forward multilayer ANNs have been widely used as nonlinear function approximator that maps a given input to a given output. Commonly, the mapping function is either classification function, when the output is in discrete form, or regression when the output is in continuous form. The primary challenge is to estimate the weight set for such a network in order to achieve a proper mapping. Back propagation is commonly used to train the weights but, since it is gradientbased, it suffers from convergence to local optima <ref type="bibr" target="#b28">[29]</ref>. Recently, other algorithms were employed for such tasks, such as GA <ref type="bibr" target="#b29">[30]</ref> or PSO <ref type="bibr" target="#b30">[31]</ref>. A generic representation of such models selects a weight set that minimizes the total error over all training data min E</p><formula xml:id="formula_46">(W ) = 1 q * O q k=1 O i=1 y k i -C k i 2 (<label>34</label></formula><formula xml:id="formula_47">)</formula><p>where q is the number of training samples, O is the number of nodes in the output layer, y k i and C k i are the actual and the desired output of the training point k on the output node i , and W is the vector of all neural network weights and biases. The weight range is usually set to [0, 1] or [-1, 1] <ref type="bibr" target="#b28">[29]</ref>. The error function is used as a fitness function in the optimization problem of selecting appropriate weights that minimize the error between the predicted and the actual output. The network structure was assumed to be static and the challenge is to select the weight set in the range [-1, 1].</p><p>GWO has been successfully applied to train the multilayer ANNs for classification and regression problems <ref type="bibr" target="#b31">[32]</ref>. In the tests performed in <ref type="bibr" target="#b31">[32]</ref>, GWO dominates all the other methods used for comparison (GA, PSO, ant colony optimization, evolution strategies, and population-based incremental learning) for the regression tests and is only dominated by the GA for three of the classification tests. This motivates this paper to try to get even better results with our improved EGWO. In this paper, a two-layered feed-forward ANN is used for regression problems. Ten data sets given in Table X are used for experiments. Each individual data set is divided following a k-fold cross validation manner, with k=10 and the experiments repeated 30 times. In the k-fold cross validation, kth fold is used as testing data and the remaining data are employed for training.</p><p>1) Results and Discussion: Table <ref type="table" target="#tab_9">XI</ref> presents the results of the statistical measures for all the data sets. We can observe that the EGWO has better fitness function value, which proves the capability of the model to converge to better optima. The enhanced performance can be interpreted by the fact that the EGWO can exploit agents' own experience to adapt the exploration rate per search agent. The exploration rate control helps the optimizer to quickly jump to more promising regions especially in complex search space. The results in Table <ref type="table" target="#tab_9">XII</ref> show that both GWO and EGWO have comparable std, which ensures the repeatability of results regardless of the random factors used. From the performance on the test data, we can observe that EGWO has a better performance than GWO. The small difference in the running time is because EGWO employs a more complex methodology, but even so, the difference is not really sensitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION AND FUTURE WORK</head><p>In this paper, a variant of GWO that learns the exploration rate in an individual manner for each agent (wolf) is proposed. The experienced GWO (EGWO) uses reinforcement learning principles to learn the actions that should be taken at different states of the optimization and in various regions of the search space. A neural network model was used to hold the experience information. The proposed EGWO is compared with the original GWO, PSO, and GA on two main optimization applications: feature selection and ANN weight adaptation. Results were assessed in both applications using a set of performance indicators. We observe a significant improvement in the performance of EGWO while compared with the other methods. EGWO can adapt quickly to different search space terrains and can avoid premature convergence. Besides, the initialization of the search agents positions at the beginning of the optimization process plays a role in the performance of EGWO, with uniform and MRMR initialization providing more variability in the search agents, helping the experienced model to be easily trained. Our methodology is more of a proof of concept that an automatic rather than manual (via trial and error) setting of the parameters of learning algorithms is more efficient, and can be generalized to other similar algorithms, thus helping with the tedious task of always finding the right parameter configuration for a particular application.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Number of punishments and rewards received by all the agents during the optimization.</figDesc><graphic coords="5,329.51,58.49,216.02,93.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Flow chart describing the state change of ANN actions.</figDesc><graphic coords="5,325.55,198.17,223.58,144.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Sample initial wolves positions using different initializations with nine search agents and six dimensions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Box-plot of fitness values for the different initializations. (a) Uniform initialization. (b) Small initialization. (c) Mix initialization. (d) MRMR initialization.</figDesc><graphic coords="9,48.95,406.13,119.06,70.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Five agents with their exploration rates and the global fitness value.</figDesc><graphic coords="9,329.03,321.53,216.14,115.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Single agent with its exploration rate, its fitness, and the global fitness.</figDesc><graphic coords="9,329.03,474.29,216.14,115.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I CORRESPONDENCE</head><label>I</label><figDesc>BETWEEN GWO STAGES AND SPACE SEARCH</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Number of gray wolves (n), maximum iterations (Max I ter ), state vector (T ), exploration change factor . Result: The optimal wolf position and its fitness. 1) Randomly initialize a population of n wolves. 2) Initialize a set of n values a i representing the exploration rate attached to each wolf. 3) Find α, β, and δ solutions based on fitness values. Feedback t i according to Eq. 19. -Update the state-action neural network given Acti on t -1 , state t -1 and Feedback t according to Eqs. (</figDesc><table><row><cell>Algorithm 2 EGWO</cell></row><row><cell>Input: 4) Initialize the state-action neural network to random</cell></row><row><cell>values.</cell></row><row><cell>5) t = 0.</cell></row><row><cell>while t ≤ Max I ter do</cell></row><row><cell>foreach W ol f i ∈ pack do</cell></row><row><cell>-Update wolf's current position according to Eq.</cell></row><row><cell>(5).</cell></row><row><cell>-Calculate the</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV AVERAGE</head><label>IV</label><figDesc>CMSE, SELECTED FEATURE, AND FISHER SCORE USING UNIFORM INITIALIZATION</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V AVERAGE</head><label>V</label><figDesc>CMSE, SELECTED FEATURE, AND FISHER SCORE USING SMALL INITIALIZATION</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI AVERAGE</head><label>VI</label><figDesc>CMSE, SELECTED FEATURE, AND FISHER SCORE USING MIXED INITIALIZATION</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VII AVERAGE</head><label>VII</label><figDesc>CMSE, SELECTED FEATURE, SIZE AND FISHER SCORE USING MRMR INITIALIZATION</figDesc><table><row><cell>TABLE VIII</cell></row><row><cell>AVERAGE RUNNING TIME IN SECONDS FOR</cell></row><row><cell>ALL INITIALIZATION METHODS</cell></row><row><cell>learning at quantized optimization steps. Regarding the added</cell></row><row><cell>storage cost for the proposed algorithm, the ANN weights</cell></row><row><cell>could be stored and the short history per wolf could also be</cell></row><row><cell>kept. The neural model in the current implementation is stored</cell></row><row><cell>as two matrices with sizes T × (2T + 1) and (2T + 1) × 3.</cell></row><row><cell>The wolf's short history is a vector of length T .</cell></row><row><cell>Significance tests have the role of comparing two optimizers</cell></row><row><cell>in order to find a statistically significant difference between</cell></row><row><cell>them.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table</head><label></label><figDesc>IX shows the results of T-test and Wilcoxon's test calculated on the different initialization methods for all the four optimizers. We are interested in the performance of EGWO against the other algorithms; thus, we report all the comparisons with EGWO only. EGWO performs better than GWO, PSO, and GA as per T-test results at a significance</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE XI MEAN</head><label>XI</label><figDesc>, BEST, AND WORST OBTAINED FITNESS COMPUTED FOR ANN WEIGHTS ADAPTATION ON REGRESSION PROBLEMS TABLE XII AVERAGE STD, RMSE, AND RUN TIME COMPUTED FOR THE DATA REGRESSION DATA SETS</figDesc><table /></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the IPROCOM Marie Curie initial training network, funded through the People Programme (Marie Curie Actions) of the European Union's Seventh Framework Programme FP7/2007-2013/ under REA Grant 316555, and in part by the Romanian National Authority for Scientific Research, CNDI-UEFISCDI, under Project PN-II-PT-PCCA-2011-3.2-0917.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>He is currently a Lecturer with Information Technology Department, Faculty of Computers and Information, Beni-Suef University, Beni Suef, Egypt. He has authored or co-authored over 50 research publications in peer-reviewed reputed journals and international conference proceedings. His research interests include computational intelligence, machine learning, image processing, and data mining.</p><p>Dr. Zawbaa has served as the Technical Program Committee Member of various international conferences and a reviewer for a number of international journals.</p><p>Crina Grosan received the B.Sc. degree in mathematics and computer science, the M.Sc. degree in complex analysis and optimisation, and the Ph.D. degree in artificial intelligence from Babeş-Bolyai University, Cluj-Napoca, Romania.</p><p>He is currently a Lecturer with the Department of Computer Science, College of Engineering, Design and Physical Sciences, Brunel University, London, U.K. Her current research interests include the development of artificial intelligence/machine learning methods and algorithms for large scale optimisation, systems of equations, data analysis, graphs, and decision making, with applications in medicine, biology, pharmacy, economics, and engineering.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The reinforcement learning toolbox, reinforcement learning for optimal control tasks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inst. Theor. Comput. Sci., Univ. Technol</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<pubPlace>Graz, Austria</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">M.S. thesis</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Reinforcement Learning: An Introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Manifold-based reinforcement learning via locally linear reconstruction</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2016-01">Jan. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural basis of reinforcement learning and decision making</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Neurosci</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="287" to="308" />
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep direct reinforcement learning for financial signal representation and trading</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Systems</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2016-02">Feb. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Grey wolf optimizer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Eng. Softw</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="46" to="61" />
			<date type="published" when="2014-03">Mar. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Modified grey wolf optimizer for global engineering optimization</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sohi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Comput. Intell. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="page">7950348</biblScope>
			<date type="published" when="2016-04">Apr. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-objective grey wolf optimizer: A novel algorithm for multicriterion optimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saremi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D S</forename><surname>Coelho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="106" to="119" />
			<date type="published" when="2016-04">Apr. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Binary grey wolf optimization approaches for feature selection</title>
		<author>
			<persName><forename type="first">E</forename><surname>Emary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Zawbaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Hassanien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="371" to="381" />
			<date type="published" when="2016-01">Jan. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition</title>
		<meeting><address><addrLine>Oxford, U.K.</addrLine></address></meeting>
		<imprint>
			<publisher>Clarendon</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Swarm intelligence and bio-inspired computation</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Gandomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karamanoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Elsevier</publisher>
			<pubPlace>Amsterdam, The Netherlands</pubPlace>
		</imprint>
	</monogr>
	<note>Theory and Applications, 1st ed</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A framework for self-tuning optimization algorithm</title>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Loomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karamanoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput. Appl</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2051" to="2057" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Data-driven optimal consensus control for discrete-time multi-agent systems with unknown dynamics using reinforcement learning method</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ind. Electron</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2016-03">Mar. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adaptive fault-tolerant tracking control for mimo discrete-time systems via reinforcement learning algorithm with less learning parameters</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Autom. Sci. Eng</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2016-01">Jan. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An adaptive network-based reinforcement learning method for MPPT control of PMSG wind energy conversion systems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Power Electron</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="7837" to="7848" />
			<date type="published" when="2016-11">Nov. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Prey-predator algorithm: A new metaheuristic algorithm for optimization problems</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Tilahun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Ong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Inf. Technol. Decision Making</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1331" to="1352" />
			<date type="published" when="2015-11">Nov. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An improved self-adaptive control parameter of differential evolution for global optimization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Intell. Intell. Syst</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="215" to="224" />
			<date type="published" when="2009-10">Oct. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Theory of Point Estimation, 2nd ed</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Casella</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Springer</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stork</surname></persName>
		</author>
		<title level="m">Pattern Classification</title>
		<meeting><address><addrLine>Hoboken, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Individual comparisons by ranking methods</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wilcoxon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics Bull</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="80" to="83" />
			<date type="published" when="1945">1945</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Rice</surname></persName>
		</author>
		<title level="m">Mathematical Statistics and Data Analysis</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Cengage Learn</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note>rd ed</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improved binary particle swarm optimization using catfish effect for feature selection</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="12699" to="12707" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Particle swarm optimisation for feature selection in classification: Novel initialisation and updating mechanisms</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">N</forename><surname>Browne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="261" to="276" />
			<date type="published" when="2014-05">May 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improved binary PSO for feature selection using gene expression data</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Chem</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="29" to="38" />
			<date type="published" when="2008-02">Feb. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Particle swarm optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Eberhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICNN</title>
		<meeting>IEEE ICNN</meeting>
		<imprint>
			<date type="published" when="1995-12">Nov./Dec. 1995</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1942" to="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Genetic algorithms with multi-parent recombination</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Eiben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Raué</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">S</forename><surname>Ruttkay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf</title>
		<meeting>Int. Conf</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">866</biblScope>
			<biblScope unit="page" from="78" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A twostage gene selection scheme utilizing MRMR filter and GA wrapper</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Akadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Amine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Ouardighi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Aboutajdine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="487" to="500" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">UCI Machine Learning Repository</title>
		<author>
			<persName><forename type="first">A</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Asuncion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">School Inf. Comput. Sci., Univ. California</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<pubPlace>Irvine, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A hybrid particle swarm optimization-back-propagation algorithm for feedforward neural network training</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Lok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Math. Comput</title>
		<imprint>
			<biblScope unit="volume">185</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1026" to="1037" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A review of evolutionary artificial neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intell. Syst</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="539" to="567" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Particle swarm optimisation for evolving artificial neural network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Syst., Man</title>
		<meeting>Syst., Man</meeting>
		<imprint>
			<date type="published" when="2000-10">Oct. 2000</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="2487" to="2490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">He has authored or co-authored over 40 research publications in peer-reviewed reputed journals, book chapters, and conference proceedings. His current research interests include computer vision, pattern recognition, video and image processing, machine learning, data mining, and biometrics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Intell</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="150" to="161" />
			<date type="published" when="1979">2015. 1979. 2001. 2003, and 2010</date>
			<pubPlace>Giza, Egypt</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Faculty of Computers and Information, Cairo University ; Faculty of Computers and Information, Cairo University</orgName>
		</respStmt>
	</monogr>
	<note>He received the B.Sc., M.Sc., and Ph.D. degrees from Information Technology Department. Dr. Emary has served as the Technical Program Committee Member of various international conferences and a reviewer for various international journals</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
