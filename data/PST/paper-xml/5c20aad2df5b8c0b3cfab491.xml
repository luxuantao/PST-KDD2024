<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Remote Sensing Scene Classification Using Multilayer Stacked Covariance Pooling</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">College of Electrical and Information Engineering</orgName>
								<orgName type="institution">Hunan University</orgName>
								<address>
									<postCode>410082</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department of Technology of Computers and Commu-nications</orgName>
								<orgName type="department" key="dep2">Escuela Politécnica</orgName>
								<orgName type="laboratory">Hyperspectral Computing Laboratory</orgName>
								<orgName type="institution">University of Extremadura</orgName>
								<address>
									<postCode>E-10003</postCode>
									<settlement>Cáceres</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">College of Electrical and Informa-tion Engineering</orgName>
								<orgName type="institution">Hunan University</orgName>
								<address>
									<postCode>410082</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">Department of Technology of Computers and Communications</orgName>
								<orgName type="department" key="dep2">Escuela Politécnica</orgName>
								<orgName type="laboratory">Hyperspectral Computing Laboratory</orgName>
								<orgName type="institution">University of Extremadura</orgName>
								<address>
									<postCode>E-10003</postCode>
									<settlement>Cáceres</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4D49CA09F2DFD040CEDF87073CF224F0</idno>
					<idno type="DOI">10.1109/TGRS.2018.2845668</idno>
					<note type="submission">received consid- erable attention recently, as can be used in many practical Manuscript received March 15, 2018; revised May 8, 2018; accepted June 5, 2018.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Feature fusion</term>
					<term>multilayer feature maps</term>
					<term>pretrained convolutional neural networks (CNN)</term>
					<term>remote sensing scene classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes a new method, called multilayer stacked covariance pooling (MSCP), for remote sensing scene classification. The innovative contribution of the proposed method is that it is able to naturally combine multilayer feature maps, obtained by pretrained convolutional neural network (CNN) models. Specifically, the proposed MSCP-based classification framework consists of the following three steps. First, a pretrained CNN model is used to extract multilayer feature maps. Then, the feature maps are stacked together, and a covariance matrix is calculated for the stacked features. Each entry of the resulting covariance matrix stands for the covariance of two different feature maps, which provides a natural and innovative way to exploit the complementary information provided by feature maps coming from different layers. Finally, the extracted covariance matrices are used as features for classification by a support vector machine. The experimental results, conducted on three challenging data sets, demonstrate that the proposed MSCP method can not only consistently outperform the corresponding single-layer model but also achieve better classification performance than other pretrained CNN-based scene classification methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>applications, such as natural hazards detection, geographic image retrieval, urban planning, and so on <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>. Given a query remote sensing image, scene classification aims to assign a unique label (e.g., industrial area or airport) to the image, based on its contents. However, remote sensing scene classification is a challenging problem since the scene images often exhibit complex spatial structures with high intraclass and low interclass variabilities. To address this problem, many scene classification methods have been proposed over the past years <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b10">[11]</ref>. An extensive review of remote sensing scene classification methods can be found in <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b1">[2]</ref>.</p><p>Recently, inspired by the great success achieved by convolutional neural networks (CNNs) in the computer vision community <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b14">[15]</ref>, a considerable number of CNN-based models have been proposed for remote sensing scene classification <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b19">[20]</ref>. These models can achieve better classification performance than other traditional methods. The success of CNN-based scene classification methods is mainly due to the fact that pretrained CNN models (e.g., AlexNet <ref type="bibr" target="#b20">[21]</ref>, VGG-VD16 <ref type="bibr" target="#b21">[22]</ref>, and GoogleNet <ref type="bibr" target="#b22">[23]</ref>) on the ImageNet <ref type="bibr" target="#b23">[24]</ref> exhibit powerful generalization ability and can extract more representative features than the traditional feature extraction methods [e.g., scale-invariant feature transform (SIFT) <ref type="bibr" target="#b24">[25]</ref> or color histograms].</p><p>However, although these methods can obtain very good classification performance, the issue of how to utilize the pretrained CNN models effectively for remote sensing scene classification is still an open question. In this paper, we propose a new method, called multilayer stacked covariance pooling (MSCP), to combine the feature maps from different layers of a pretrained CNN for remote sensing scene classification. The proposed MSCP scene classification framework includes three main steps. In the first step, a pretrained CNN model (i.e., AlexNet or VGG-VD16) is used to extract multilayer feature maps. Then, the feature maps are stacked together and a covariance matrix is calculated. Each entry in the covariance matrix stands for the covariance between two different feature maps, which serves as a natural mechanism to fuse the feature maps from different layers. Finally, the obtained covariance matrices are used as features for classification using a support vector machine (SVM) classifier with linear kernel. We note that, in order to stack feature maps with different spatial dimensions together, downsampling is adopted. Moreover, channelwise average fusion is proposed and applied on each Fig. <ref type="figure">1</ref>. Example illustrating the reconstruction <ref type="bibr" target="#b25">[26]</ref> of feature maps from different convolutional layers (conv) of AlexNet <ref type="bibr" target="#b20">[21]</ref>. (Left to Right) We show the feature maps resulting from layers: conv1, conv2, conv3, conv4, and conv5, respectively. The maps at different levels are shown to convey complementary information that can be used to further improve the classification performance. convolutional layer to reduce computational complexity before stacking the maps together.</p><p>The main motivation of the proposed method is to be able to exploit the complementary information among different convolutional layers to further enhance classification performance. As pointed by Yann et al. <ref type="bibr" target="#b26">[27]</ref>, the basic idea behind the CNN model is to represent the image from raw to abstract using multilevel architectures. The sallower layers of the CNN model are more likely to reflect the low-level visual features (LLFs) (e.g., edges), while the deeper layers represent more abstract information contained in the images. Fig. <ref type="figure">1</ref> provides a graphical illustration of the feature maps in different layers of a typical CNN model (i.e., AlexNet <ref type="bibr" target="#b20">[21]</ref>). As can be observed in Fig. <ref type="figure">1</ref>, the AlexNet (with its hierarchical architecture) can extract various feature maps from the image, and these maps are expected to convey complementary information that can be used to further improve the classification performance. In addition, using the so-called shortcut connections to combine different layers, from shallow to deep, the recently proposed ResNet <ref type="bibr" target="#b27">[28]</ref> and DenseNet <ref type="bibr" target="#b28">[29]</ref> can achieve state-of-the-art performance in different computer vision tasks, which also suggests that the combination of different layers from the CNN can be very useful. In this regard, our proposed method uses a similar approach to exploit the complementary information contained by multiple layers. Our experiments demonstrate that the proposed MSCP method can indeed exploit such complementary information and achieve better classification performance than several state-of-the-art approaches.</p><p>The remainder of this paper is organized as follows. Section II gives an overview of related works and presents the main innovative contributions of our proposed approach. Section III details the proposed MSCP framework. In Section IV, comprehensive experimental results are reported on three data sets, and an exhaustive comparison to other state-of-the-art methods is also given. Section V concludes this paper with some remarks and hints at plausible future research lines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS AND CONTRIBUTIONS</head><p>Generally, the existing scene classification methods can be categorized into three classes: 1) LLF-oriented methods; 2) mid-LF (MLF)-oriented methods; and 3) high-LF-oriented methods. For the LLF-oriented methods, a local or global feature descriptor is first extracted to represent the test images. Then, the obtained features are sent to a supervised classifier such as the SVM for label assignment. Yang and Newsam <ref type="bibr" target="#b29">[30]</ref> combine Gabor features with the maximum a posteriori model for scene classification, where each test image is represented by a vector consisting of the mean and standard deviation of the corresponding Gabor feature. Moreover, the global color histograms are used to characterize the image and the SVM is then utilized to classify the obtained feature vectors <ref type="bibr" target="#b30">[31]</ref>. In <ref type="bibr" target="#b31">[32]</ref>, a sparse representation <ref type="bibr" target="#b32">[33]</ref>- <ref type="bibr" target="#b36">[37]</ref> is adopted to combine several LLFs (e.g., local binary patterns <ref type="bibr" target="#b37">[38]</ref> and histogram of oriented gradients <ref type="bibr" target="#b38">[39]</ref>) to enhance classification performance.</p><p>Bearing in mind that there may be semantic gaps between LLFs and the high-level semantic meaning of images, MLForiented methods are introduced to bridge these two levels <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>. Yang and Newsam <ref type="bibr" target="#b30">[31]</ref> use a bag of visual words (BoVW) model to encode the SIFT descriptor for MLF extraction. The MLFs are fed to an SVM with intersection kernel for classification. To further take into account the spatial contextual information, the spatial pyramid matching is used to extend the BoVW model in <ref type="bibr" target="#b41">[42]</ref> and <ref type="bibr" target="#b42">[43]</ref>. Zhu et al. <ref type="bibr" target="#b6">[7]</ref> utilize the BovW model to combine both the local and global features, extracted from the images, to enhance classification performance. With the introduction of partlets, which are a library of pretrained part detectors used for mid-level visual element discovery, an effective and efficient MLF method was proposed in <ref type="bibr" target="#b3">[4]</ref>. The probabilistic topic model is another popular technique to bridge the semantic gap between LLFs and the high-level semantic meaning <ref type="bibr" target="#b4">[5]</ref>, by means of which the input scene is represented as a probability distribution of the visual words. In <ref type="bibr" target="#b4">[5]</ref>, a multiple topic model was proposed to combine several different complementary features in order to achieve a discriminative MLF feature extraction. In addition, a sparse topic model was recently proposed to integrate homogeneous and heterogeneous features for scene classification <ref type="bibr" target="#b43">[44]</ref>. In <ref type="bibr" target="#b44">[45]</ref>, a multitask learning method is proposed to take both the multiresolutions analysis (MSA) and feature selection into account for scene classification. Du et al. <ref type="bibr" target="#b45">[46]</ref> propose a local structure learning framework to make use of the local topological construction of images for remote scene image retrieval. Instead of using handcrafted features such as SIFT, other unsupervised feature learning methods based on different concepts have also been recently proposed <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>.</p><p>Inspired by the recent success achieved by CNNs in the computer vision community, the CNN models have also been extended for remote sensing scene classification <ref type="bibr" target="#b16">[17]</ref>. However, training a deep CNN model from scratch generally needs a huge amount of training data, while the available offthe-shelf remote sensing scene image data sets are relatively small. For example, deep CNN models are usually trained on the ImageNet <ref type="bibr" target="#b23">[24]</ref>, which contains millions of images, while the NWPU-RESISC45 data set <ref type="bibr" target="#b0">[1]</ref> (one of the biggest data sets for remote sensing scene classification) contains less than 35 000 images. Moreover, the CNN models pretrained on Ima-geNet show a powerful generalization ability on different tasks (e.g., object detection and semantic segmentation <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>). Under this context, the perspective of using off-the-shelf pretrained CNN models such as AlexNet <ref type="bibr" target="#b20">[21]</ref>, VGG-VD16 <ref type="bibr" target="#b21">[22]</ref>, and GoogleNet <ref type="bibr" target="#b22">[23]</ref> as a universal feature extractors has become an interesting approach for remote sensing scene classification. In <ref type="bibr" target="#b50">[51]</ref>, the GoogleNet is used for remote sensing scene classification, showing that the pretrained CNN models can outperform the conventional handcrafted featurebased methods by a large margin. Hu et al. <ref type="bibr" target="#b15">[16]</ref> considered two different scenarios to utilize a pretrained CNN model (VGG-VD16). In the first scenario, the last few fully connected layers are regarded as final image features for scene classification. In the second scenario, a traditional feature encoding method such as the improved Fisher kernel (IFK) <ref type="bibr" target="#b51">[52]</ref> is used to encode the feature maps from the last convolutional layer for representing the input image. Both scenarios adopt the SVM as the final classifier. Cheng et al. <ref type="bibr" target="#b52">[53]</ref> use the BoVW model to encode a single convolutional layer. In <ref type="bibr" target="#b17">[18]</ref>, the last two fully connection layers of the CNN model are fused together to represent the image. In <ref type="bibr" target="#b18">[19]</ref>, a multiscale IFK coding method is proposed to combine the feature maps from different layers.</p><p>More recently, as a second-order pooling strategy, covariance pooling (CP) has been used in many computer vision tasks, such image segmentation <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref> and classification <ref type="bibr" target="#b55">[56]</ref>. There are two main advantages of the CP approach. First, different from conventional pooling methods, the CP takes the second-order statistics (i.e., covariance) into consideration and, therefore, obtains a more compact and discriminative representation. Second, each entry in the covariance matrix obtained by CP represents the covariance between two different feature maps. This offers a natural way to fuse complementary information coming from different feature maps.</p><p>The proposed method is different from existing pretrained CNN-based methods in the following two main innovative aspects. First, we utilize the different convolutional feature maps of the CNN (from shallow to deep layers) rather than the last one or two connection layers for representing the input image. As a result, the proposed approach can achieve better classification performance than the methods in <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b50">[51]</ref>, and <ref type="bibr" target="#b52">[53]</ref>. Second, we adopted a simple yet effective method (i.e., CP) to combine feature maps from different layers. Thus, the proposed method can run much faster than the method in <ref type="bibr" target="#b18">[19]</ref>, with very competitive classification performance, and is suitable to deal with relative large data sets such as the NWPU-RESISC45 data set in <ref type="bibr" target="#b0">[1]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>Fig. <ref type="figure" target="#fig_0">2</ref> illustrates the proposed MSCP-based classification framework, which consists of the following three steps: 1) multilayer feature extraction using a pretrained CNN model; 2) MSCP; and 3) SVM-based classification. In the following, we describe each one of the aforementioned steps in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Multilayer Feature Extraction</head><p>The CNN model can be thought as a composition of a number of functions as shown in <ref type="bibr" target="#b0">(1)</ref>, where each function f l takes the data samples X l and a filters bank w l as inputs and produces X l+1 , where</p><formula xml:id="formula_0">l = 1 • • • L and L is the number of layers f (X) = f L (• • • f 2 ( f 1 (X; w 1 ); w 2 ) • • • , w L ). (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>For a pretrained CNN model, the filters bank w l has been learned from some big data set (e.g., ImageNet <ref type="bibr" target="#b23">[24]</ref>). Given an input image X, the multilayer features are extracted as follows:</p><formula xml:id="formula_2">M 1 = f 1 (X; w 1 ), M 2 = f 2 (M 1 ; w 2 )</formula><p>, and so on. In this paper, the AlexNet and VGG-VD16 are used as pretrained CNN models. Specifically, three convolutional layers (i.e., "conv3," "conv4," and "conv5") of AlexNet are adopted, which are denoted by M 3 , M 4 ,andM 5 , respectively. Three convolutional layers (i.e., "conv3-3," "conv4-3," and "conv5-3") of VGG-VD16 are also used, denoted by M 3,3 , M 4,3 ,andM 5,3 , respectively. Note that the other layers (e.g., pooling layers) are omitted in (1) for simplicity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multilayer Stacked Covariance Pooling</head><p>Usually, different convolutional layers have different spatial dimensions, and thus, they cannot be stacked directly. If we take the VGG-VD16 as an example, we have</p><formula xml:id="formula_3">M 3,3 ∈ R 56×56×256 , M 4,3 ∈ R 28×28×512 ,andM 5,3 ∈ R 14×14×512 .</formula><p>To address this problem, downsampling with bilinear interpolation is adopted in this paper. Moreover, to reduce the computational complexity (the dimension of the covariance matrix is determined by the number of feature maps), a channelwise average fusion method is further proposed and adopted on each convolutional layer before stacking them together. For a given convolutional layer Y ∈ R H ×W ×L and a predefined number of fused feature maps d, the channelwise average fusion is conducted as follows. We first partition the L feature maps (each feature map is with size H × W ) into d subsets based on its original sequence. Then, the average feature maps of each subset are further calculated and are stacked together. Through the downsampling and channelwise average fusion operations, three preprocessed convolutional layers are obtained (i.e., M3,3 ∈ R s×s×d , M4,3 ∈ R s×s×d ,and M5,3 ∈ R s×s×d ) and the stacked feature set is obtained as follows: M = [ M3,3 ; M4,3 ; M5,3 ] ∈ R s×s×D , D=3d and s is the predefined downsampled spatial dimension. Finally, the CP of stacked feature M is expressed as follows:</p><formula xml:id="formula_4">C = 1 N -1 N i=1 (z i -μ)(z i -μ) T ∈ R D×D (2)</formula><p>where</p><formula xml:id="formula_5">[z 1 , z 2 , • • • , z N ] ∈ R D×N is the vectorization of M along the third dimension, N = s 2 and μ = (1/N) N i=1 z i ∈ R D .</formula><p>A graphical illustration of the CP is shown in Fig. <ref type="figure" target="#fig_1">3</ref>. The off-diagonal entries of covariance matrix C stands for the covariance between the two different feature maps and the diagonal entries represent the variance of each feature map. For example, in Fig. <ref type="figure" target="#fig_1">3</ref>, the blue square entry of covariance matrix C stands for the covariance of the last two feature maps (the red and the blue ones). From the definition of the CP [see <ref type="bibr" target="#b1">(2)</ref>], the following three advantages of CP can be concluded <ref type="bibr" target="#b56">[57]</ref>- <ref type="bibr" target="#b58">[59]</ref>. (Note that, in <ref type="bibr" target="#b56">[57]</ref> and <ref type="bibr" target="#b57">[58]</ref>, the CP is first proposed as a region feature descriptor, called region covariance descriptor, for texture classification and pedestrian detection.) First, CP provides a natural way to fuse different feature maps. As we have mentioned earlier, each off-diagonal entry of the covariance matrix stands for the covariance of two different feature maps, which can fuse different feature maps effectively. Second, there is an average operation during covariance computation [see <ref type="bibr" target="#b1">(2)</ref>], which can greatly filter the noise corrupting individual samples. Last but not least, the computation of covariance matrix is independent of the ordering information of the samples (i.e., z i , i = 1 • • • N), which indicates that CP is robust to the rotation. In summary, the CP can not only make use of the second-order information (i.e., covariance) to fuse different feature maps but also be robust to the noise and rotation. Meanwhile, psychophysics research shows that the second-order information plays an important role in the human visual recognition process <ref type="bibr" target="#b59">[60]</ref> The above-mentioned three distinctive advantages enable the CP that becomes a very effective feature coding method and thus it could be expected that more discriminative representation can be achieved by CP, when compared to the first-order pooling method (e.g., average pooling). More related works about CP and second-order pooling can be found in <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b60">[61]</ref>, and <ref type="bibr" target="#b61">[62]</ref>.</p><p>In addition, as pointed out by Arsigny et al. <ref type="bibr" target="#b62">[63]</ref>, the covariance matrices do not lie on the Euclidean space, but on the Riemannian manifold space. Thus, they cannot be processed by the SVM which is originally designed for data lying on the Euclidean space. Fortunately, with the matrix logarithm operation, the covariance matrix can be mapped into Euclidean space while preserving the intrinsic geometric relationships as defined on the manifold as follows <ref type="bibr" target="#b62">[63]</ref>:</p><formula xml:id="formula_6">Ĉ = logm(C) = Ulog()U T ∈ R D×D<label>(3)</label></formula><p>where C = UU T is the eigen decomposition of the covariance matrix C. More detailed explanation about matrix logarithm operation can be found in <ref type="bibr" target="#b62">[63]</ref> and <ref type="bibr" target="#b63">[64]</ref>. Note that Ĉ is a symmetric matrix and, therefore, only (D(D + 1)/2) entries need to be stored, which can further reduce the computational complexity. The (D(D + 1)/2) entries of Ĉ comprise the final set of output features to represent the input image X, denoted by v. The channelwise average fusion is an effective strategy to reduce the computational complexity. For example, the three original convolutional layers of AlexNet are with a size as follows: conv3 (13 </p><formula xml:id="formula_7">×</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. SVM Classification</head><p>The aforementioned operations are performed on both the training samples and test samples. As such, the training set (i.e., {v i , y i } i=1•••n ) and the testing set are now considered, where y i is the corresponding labels and n is the number of training samples. Then, {v i , </p><formula xml:id="formula_8">y i } i=1•••n is used to train an SVM</formula><formula xml:id="formula_9">1 2 α 2 + C i ξ i s.t. y i (φ(v i ), α + b) 1 -ξ i ξ i &gt; 0, ∀i = 1, • • • , n<label>(4)</label></formula><p>where α and b define a linear classifier, C is a regularization parameter that controls the generalization capacity of the classifier, ξ i are positive slack variables to cope with outliers in the training set, and φ(•) is the mapping function. A linear kernel is adopted in this paper, with K (v i , v j ) = v T i v j . Finally, the prediction label of each test sample v is determined by means of a decision function, as follows:</p><formula xml:id="formula_10">f (x) = sgn n i=1 y i λ i K (v i , v) + b (5)</formula><p>where λ i is the Lagrange multipliers. Note that the oneagainst-all strategy is adopted for solving the multiclass problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS AND DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Data Sets</head><p>To evaluate the performance of the proposed method, we conduct experiments on three challenging remote sensing scene image data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) UC Merced Land Use Data Set:</head><p>The UC Merced Land Use (UC) <ref type="bibr" target="#b30">[31]</ref> data set contains 2100 images divided into 21 scene classes. Each class consists of 100 images with size of 256 × 256 pixels in the RGB space. Each image has a pixel resolution of one foot. Fig. <ref type="figure" target="#fig_2">4</ref> shows some examples of the UC data set. As can be seen in Fig. <ref type="figure" target="#fig_2">4</ref>, some categories have very high interclass similarity (e.g., forest and sparse residential), which makes the UC data set a very challenging one. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE I CONVOLUTION LAYERS USED BY THE PROPOSED METHOD AND CORRESPONDING SIZE OF THESE LAYERS</head><p>2) AID30: The AID30 (AID) data set <ref type="bibr" target="#b1">[2]</ref> contains 10 000 images divided into 30 scene classes. Each class contain hundreds of images (ranging from 220 to 420) with size of 600 × 600 pixels in the RGB space. The spatial resolution changes from about 8 to 0.5 m. Fig. <ref type="figure" target="#fig_3">5</ref> shows some examples of the AID data set.</p><p>3) NWPU-RESISC45 Data Set: The NWPU-RESISC45 (NWPU) data set <ref type="bibr" target="#b0">[1]</ref> contains 31 500 images divided into 45 scene classes. Each class consists of 700 images, with size of 256 × 256 pixels in the RGB space. The spatial resolution changes from about 30 to 0.2 m/pixel for most of the scene classes. This data set is of the largest available in terms of both the number of scene classes and the total number of images. Thus, it contains richer image variations, larger within-diversity, and higher interclass similarity than the other considered data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Setup</head><p>In our implementation, two popular CNN pretrained models: AlexNet <ref type="bibr" target="#b20">[21]</ref> and VGG-VD16 <ref type="bibr" target="#b21">[22]</ref> are utilized to extract multilayer features. Specifically, the three convolutional layers (i.e., "conv3," "conv4," and "conv5") of AlexNet and the three convolutional layers (e.g., "conv3-3," "conv4-3," and "conv5-3") of VGG-VD16 are used, respectively. Detailed information about the used convolutional layers of the CNN models is summarized in Table <ref type="table">I</ref>. Before feeding the scene images into the pretrained CNN model for feature extraction, the images are resized to the predefined size as given in Table <ref type="table">I</ref> (i.e., 227 × 227 × 3 for AlexNet and 224 × 224 × 3 for VGG-VG16), which followed the experimental settings of <ref type="bibr" target="#b20">[21]</ref> and <ref type="bibr" target="#b21">[22]</ref>, respectively. Both two models are pretrained on ImageNet and downloaded from the homepage of MatConvNet <ref type="bibr" target="#b64">[65]</ref> (a MATLAB toolbox for CNN). 1 Since random sampling is adopted to generate the training and test sets <ref type="bibr" target="#b0">[1]</ref>, all experiments are run 10 times. The average and standard deviation of the obtained overall accuracies (OAs) are reported. Moreover, to avoid any possible experimental bias caused by random sampling, for each data set, we first randomly obtain 10 times splits and then apply the same 10 splits on all experiments. All our experiments are conducted on a laptop with MATLAB 2016, CPU (2.6 GHz) and 16-GB RAM, without any GPU acceleration. The LIBSVM library <ref type="bibr" target="#b65">[66]</ref> (with default parameters setting) is used for the linear kernel-based SVM. Our code is available online. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Parameters Setting</head><p>In the proposed method, there are two main parameters: the downsampled spatial dimension, s, and the number of feature maps after channelwise average fusion, d. To avoid an exhausting search, s is simply set to the minimum spatial dimension among the used convolutional layers (i.e., s is set to 13 for the AlexNet and set to 14 for VGG-VD16). We have empirically tested that this configuration can always get satisfactory classification performance in our experiments. The parameter d is then analyzed and the UC data set with 80% samples randomly selected for training is used in this experiment. Fig. <ref type="figure">6</ref> shows the effect of using different values of d with the proposed MSCP method on the two considered 1 http://www.vlfeat.org/matconvnet/ 2 https://sites.google.com/site/leyuanfang/home Fig. <ref type="figure">6</ref>.</p><p>Effect of parameter d on the proposed MSCP classification framework.</p><p>CNN models. As can be observed, on the AlexNet, when d grows from 50 to 80, there is an obvious improvement of OA, while further growing d will degrade the classification performance. Similar situation can be also observed on VGG-VD16 but with larger optimal d and the optimal d for VGG-VD16 is 130. The main reason maybe following two aspects. First, a relative small d means that more adjacent feature maps on the convolutional layer are fused together by the average operation, which could discard some useful information and thus decrease the classification performance. Second, a relative lager d will result in a covariance matrix with a larger dimension, which could weaken the discriminative ability and compactness of the covariance matrix, and therefore, lead to relative worse classification results. The above-mentioned parameters configuration are applied to all the test data sets and remained unchanged in the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Effect of Combination of Different Layers</head><p>In this experiment, to demonstrate that the proposed MSCP method can fuse multilayer feature maps effectively, a simplified version of MSCP are taken in consideration in this experiment, i.e., the single-layer feature maps of pretrained CNN (plus CP), with and without preprocessing, and the resulting features are classified by a linear SVM (as the proposed MSCP). The UC data set and AID data set are used here for illustration purposes, where 80% training samples are randomly selected for UC data set and 20% training samples are randomly selected for AID data set. Table <ref type="table" target="#tab_1">II</ref> illustrates the corresponding comparison results on the two data sets. As can be observed from Table <ref type="table" target="#tab_1">II</ref>, there is a clear improvement of OA with the combination of different layers by the proposed MSCP. For example, on UC data set, the OAs of the preprocessed single layer of VGG-VD16 are 96.93%, 97.29%, and 96.76%, respectively, while the use of MSCP to combine these three layers increases the accuracy significantly (OA = 98.36%). Similar results can also be observed on AID data sets. In addition, it can also be observed from Table <ref type="table" target="#tab_1">II</ref> that the proposed MSCP method can outperform the original single-layer feature map (plus CP) with smaller feature dimension on both the test data sets. Moreover, Fig. <ref type="figure" target="#fig_4">7</ref> shows the corresponding per-class accuracies obtained by MSCP and its simplified versions on the UC and AID data sets. As can be observed in Fig. <ref type="figure" target="#fig_4">7</ref>, using the proposed MSCP strategy, the classification accuracies achieved for most classes exhibit obvious improvements on both the data sets. For example, on UC data set with AlexNet (see the first graph in the first column of Fig. <ref type="figure" target="#fig_4">7</ref>), the accuracy obtained for the 14th class (mobile homepark) improves from 97% to 99.3%. The above-mentioned conducted experiments suggest that there is indeed complementary information among different layers in the considered CNN architecture, and that the proposed MSCP can exploit such information to further improve the classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Compared With Other Pretrained CNN-Based Methods</head><p>First, the proposed MSCP method is compared to several pretrained CNN model-based classification methods on the UC data set, including the two scenarios in <ref type="bibr" target="#b15">[16]</ref>, the method in <ref type="bibr" target="#b17">[18]</ref>, and the method in <ref type="bibr" target="#b18">[19]</ref>. Specifically, in the first scenario of <ref type="bibr" target="#b15">[16]</ref>, only the last fully connected layer is used to represent the input image, and then classified by a linear SVM.</p><p>In the second scenario of <ref type="bibr" target="#b15">[16]</ref>, only the last convolutional layer is encoded by some traditional coding methods (e.g., IFK) to represent the image, and then classified by a linear SVM. For the method in <ref type="bibr" target="#b17">[18]</ref>, the last two fully connected layers of the CNN are fused by means of a discriminant correlation analysis (DCA) to represent the image, and then classified by a linear SVM. For the method in <ref type="bibr" target="#b18">[19]</ref>, a multiscale IFK strategy is adopted to fuse different layers of the CNN model for classification purposes. Table <ref type="table" target="#tab_2">III</ref> shows the classification performance of the proposed method and the other four compared methods. As can be observed in Table <ref type="table" target="#tab_2">III</ref>, the MSCP (with pretrained AlexNet and VGG-VD16) exhibits better classification performance than the methods in <ref type="bibr" target="#b15">[16]</ref> and <ref type="bibr" target="#b17">[18]</ref>. Moreover, the MSCP with pretrained VGG-VD16 is slower than the MSCP with pretrained AlexNet. This is expected, since VGG-VG16 contains more layers than AlexNet and thus needs more time for forward propagation. Indeed, the method in <ref type="bibr" target="#b18">[19]</ref> shows better classification performance than MSCP.  However, the performances are similar and our method is almost 20 times faster. Fig. <ref type="figure" target="#fig_5">8</ref> shows the confusion matrix for one of the experiments conducted by the proposed MSCP (with pretrained VGG-VD16). As can be observed in Fig. <ref type="figure" target="#fig_5">8</ref>, the proposed method can get perfect classification performance on most classes except the following ones: 1#, 8#, 13#, 21#, and 21#. It is worth noting that some of the misclassified images in this case may also be difficult to distinguish for a human interpreter (see Fig. <ref type="figure" target="#fig_6">9</ref>). We then compared the proposed method with several benchmark methods on AID data set: 1) a technique that uses the last fully connected layers of two pretrained CNN models (i.e., VGG-VG16 and GoogleNet) as input to an SVM classifier <ref type="bibr" target="#b1">[2]</ref> and 2) the two fusion methods proposed in <ref type="bibr" target="#b17">[18]</ref> (i.e., fusion by concatenation and DCA) which only fuse the last two fully connected layers of the pretrained CNN model. In addition, following the experimental setup in <ref type="bibr" target="#b1">[2]</ref>, two kinds of training rates are used for comparison. The first one considers 20% of the samples for training and the rest for testing. The other one considers 50% of the samples for training and the rest  for testing. The corresponding results are given in Table <ref type="table" target="#tab_3">IV</ref>.</p><p>As can be seen in Table <ref type="table" target="#tab_3">IV</ref>, the proposed method (both with the pretrained AlexNet and VGG-VD16) can obviously outperform the other tested methods, which demonstrates the effectiveness of the proposed MSCP approach. Moreover, Fig. <ref type="figure">10</ref> shows the confusion matrix obtained by the MSCP method (with the pretrained VGG-VD16) in one of the experiments conducted using a 50% training rate. As can be seen in Fig. <ref type="figure">10</ref>, classes 6# (center), 23# (resort), and 25# (school) exhibit the lowest classification accuracies, which is mainly due to the fact that these classes usually share very similar objects (e.g., buildings) and thus making correct classification of these categories becomes very challenging. Some misclassified test images in this experiment are also given in Fig. <ref type="figure" target="#fig_7">11</ref> for illustrative purposes. Last but not least, we perform the proposed method on one of the largest and most challenging scene data sets (i.e., NWPU data set) and make comparison with other two pretrained CNN-based methods <ref type="bibr" target="#b0">[1]</ref>  <ref type="bibr" target="#b52">[53]</ref>. The experimental  setup is followed by the description in <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b52">[53]</ref>, two kinds of splits are used in this experiment. The first one considers 10% of samples for training and the rest for testing. The other one considers 20% of the samples for training and the rest for testing. Specifically, in <ref type="bibr" target="#b0">[1]</ref>, the last fully connected layer of three pretrained CNN models (using AlexNet, VGG-VG16, and GoogleNet) is used to represent the image, and then the SVM is used for classification. In <ref type="bibr" target="#b52">[53]</ref>, the BoVW model is used to encode the last convolution layer, and the encoded features are also fed to an SVM for classification purposes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. MSCP Plus Multiresolution Analysis to Further Improve Classification Accuracy</head><p>Here, we introduce a widely used method, i.e., MSA <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b44">[45]</ref>, into our method to further improve the classification performance. The basic motivation using The classification results achieved by our method with MRA and without MRA on the three test data sets (i.e., UC data set, AID data set, and NWPU data set) are reported in Table VI and are compared with two very recent related works <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr" target="#b43">[44]</ref>. Specifically, Zhu et al. <ref type="bibr" target="#b43">[44]</ref> propose a topical model-based handcrafted feature learning method, called sparse homogeneous-heterogeneous topic feature model (SHHTFM), for remote sensing scene image classification, which can simultaneously explore the homogeneous information (i.e., superpixels) and heterogeneous information (i.e., square window). In <ref type="bibr" target="#b16">[17]</ref>, by considering there are large variance within the same class and high similarity among different classes, the authors introduce a discriminative loss term into pretrained CNN models and then fine-tune the whole CNN model for end-to-end remote scene image classification, which is called discriminative CNN (DCNN) model. As can be observed from Table VI, our methods (VGG-VD16 + MSCP and VGG-VD16 + MSCP + MRA) are slightly better than the method in <ref type="bibr" target="#b43">[44]</ref>. The reason is because our methods use the deep CNN models for feature extraction, which can obtain more discriminative future than the handcrafted-based feature learning method. In addition, our methods (with MRA) show competitive or better classification performance to the DCNN. For example, on the AID data set with a training ratio 20%, our methods (i.e., VGG-VD16 + MSCP and VGG-VD16 + MSCP + MRA) can outperform the DCNN. However, in most cases, DCNN shows slightly better classification results. This is mainly due to that DCNN can fine-tune the neural units (i.e., parameters) in the CNN models to match different images in different data sets, and therefore, achieve better classification results. In our future work, we will modify our method to an end-to-end classification framework and then adopt the fine-turning strategy to further improve the classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we proposed a new method called MSCP to fuse the feature maps from different layers of a CNN architecture for remote sensing scene classification. The proposed MSCP-based classification framework first performs feature extraction, using a pretrained CNN model, and then performs feature fusion by CP. Since the proposed MSCP can take the second-order information into consideration, more compact features are extracted for classification purposes. Moreover, each feature represents the covariance of two different feature maps, which captures the complementary information among different layers in a natural way. Our comprehensive experiments using three publicly available remote sensing image scene classification data sets, and the conducted comparisons with the state-of-the-art approaches, verify the effectiveness of the proposed MSCP method. As a potential line of improvement, we realize that it can be useful to process the original layers with the MSCP approach. However, the dimension of features is difficult to manage taking into account the available off-the-shelf CNN models. In the future, we are planning to address this issue by designing a new end-to-end CNN model similar to the one presented in <ref type="bibr" target="#b28">[29]</ref> with CP, which uses fewer feature maps in each of the layers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Flowchart of the proposed MSCP classification framework. The proposed framework consists of three steps: 1) multilayer feature extraction using a pretrained CNN model; 2) MSCP; and 3) SVM classification. Dotted and colored lines: downsampling and channelwise average fusion, respectively.</figDesc><graphic coords="3,49.43,58.25,513.14,114.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Illustration of the concept of CP. Off-diagonal entries of C: covariance between two different feature maps. Diagonal entries: variance of each feature map. Blue square entry (i.e., the second entry in the first line) of covariance matrix C: covariance of the last two feature maps (i.e., the red one and the blue one).</figDesc><graphic coords="4,48.35,58.01,251.18,76.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Some examples of the UC data set.</figDesc><graphic coords="5,48.47,58.49,251.18,213.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Some examples of the AID data set.</figDesc><graphic coords="5,312.47,58.85,250.10,271.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Comparison of the per-class classification performance achieved by the proposed method and by a single-layer pretrained CNN architecture (plus CP) with various pretrained CNN models on different data sets. (a) UC data set (training ratio = 80%). (b) AID data set (training ratio = 20%). First line: AlexNet. Second line: VGG-VD16. Here, "_pre" means that the convolutional layer has been preprocessed by downsampling and channelwise average fusion.</figDesc><graphic coords="7,49.43,58.73,513.14,236.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Confusion matrix for the UC data set under a training rate of 80%, using the proposed MSCP method (with pretrained VGG-VD16), in one single experiment (with OA = 98.57%).</figDesc><graphic coords="8,310.79,187.01,251.18,135.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Misclassified test images in one single experiment using the proposed MSCP method (with pretrained VGG-VD16) on UC data set. (a) Agricultural misclassified into (→) forest. (b) Forest → sparse residential. (c) Medium residential → dense residential. (d) Medium residential → dense residential. (e) Storage tanks → intersection. (f) Tennis court → sparse residential.</figDesc><graphic coords="8,49.43,249.53,250.10,184.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Some examples of misclassified test images in one single experiment using the proposed MSCP method (with pretrained VGG-VD16) on AID data set. (a) Bridge misclassified into (→) resort. (b) Center → square. (c) Church → square. (d) Commercial → school. (e) Industrial → school. (f) Industrial → commercial. (g) Park → square. (h) Railway station → park. (i) Resort → park. (j) School → commercial. (k) School → square. (l) School → commercial.</figDesc><graphic coords="9,49.43,195.29,250.10,216.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Some examples with large scales variance in NWPU data set. First line: category airplane. Second line: category store tank. Third line: category tennis court. Last line: category bridge.</figDesc><graphic coords="9,312.47,195.41,250.10,253.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc><ref type="bibr" target="#b12">13</ref> × 384), conv4 (13 × 13 × 384), and conv5 (13 × 13 × 256). If we stack them together without channelwise average fusion and then perform CP, the dimension of the obtained feature v for one single image is beyond 500 K [(384 + 384 + 256) 2 /2], which is hard to manage. By contrast, the dimension of the obtained feature v can be reduced significantly by a channelwise average fusion strategy with a small d, e.g., the dimension of v can be reduced to 29 K [(80 + 80 + 80) 2 /2] with d = 80, which is much less than 500 K.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II COMPARISON</head><label>II</label><figDesc>BETWEEN THE CLASSIFICATION RESULTS (%) OBTAINED BY USING A SINGLE CONVOLUTIONAL LAYER PLUS CP AND THE PROPOSED MSCP METHOD ON UC (TRAINING RATIO = 80%) AND AID DATA SETS (TRAINING RATIO = 20%). "_PRE" MEANS THAT THE CONVOLUTIONAL LAYER HAS BEEN PREPROCESSED BY DOWNSAMPLING AND CHANNELWISE AVERAGE FUSION</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III COMPARISON</head><label>III</label><figDesc></figDesc><table /><note><p>OF THE CLASSIFICATION RESULTS (%) OBTAINED FOR THE UC DATA SET</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV COMPARISON</head><label>IV</label><figDesc>OF THE CLASSIFICATION RESULTS (%) OBTAINED FOR THE AID DATA SET Fig. 10. Confusion matrix for the AID data set under a training rate of 50%, using the proposed MSCP method (with pretrained VGG-VD16), in one of our experiments (with OA = 94.84%).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V COMPARISON</head><label>V</label><figDesc>OF THE CLASSIFICATION RESULTS (%) OBTAINED FOR THE NWPU DATA SET</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI COMPARISON</head><label>VI</label><figDesc>OF THE CLASSIFICATION RESULTS (%) ACHIEVE BY OUR METHOD WITH MRA AND WITHOUT MRA AS WELL AS TWO VERY RECENT REMOTE SCENE CLASSIFICATION METHODS<ref type="bibr" target="#b16">[17]</ref>,<ref type="bibr" target="#b43">[44]</ref>. TR, TRAINING RATIO</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Table V reports the classification results obtained by all tested methods. It is clear that the proposed MSCP (with pretrained VGG-VD16) can outperform the other methods with the two considered training rates.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank Prof. C. Gong and Prof. G. Xia for providing the NWPU and AID data sets on their homepages, respectively. They would also like to thank the editors and anonymous reviewers for their insightful comments and suggestions, which have significantly improved this paper.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Natural Science Fund of China for International Cooperation and Exchanges under Grant 61520106001, in part by the National Natural Science Foundation for Young Scientist of China under Grant 61501180, in part by the National Natural Science Foundation under Grant 61771192 and Grant 61471167, in part by the Fund of Hunan Province for Science and Technology Plan Project under Grant 2017RS3024, in part by the China Postdoctoral Science Foundation under Project 2017T100597, and in part by the China Scholarship Council.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Remote sensing image scene classification: Benchmark and state of the art</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="2017-10">Oct. 2017</date>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="1865" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">AID: A benchmark data set for performance evaluation of aerial scene classification</title>
		<author>
			<persName><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3965" to="3981" />
			<date type="published" when="2017-07">Jul. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Extinction profiles fusion for hyperspectral images classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ghamisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Benediktsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1803" to="1815" />
			<date type="published" when="2018-03">Mar. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Effective and efficient midlevel visual elements-oriented land-use classification using VHR remote sensing images</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4238" to="4249" />
			<date type="published" when="2015-08">Aug. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dirichlet-derived multiple topic scene classification model for high spatial resolution remote sensing imagery</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2108" to="2123" />
			<date type="published" when="2016-04">Apr. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Remote sensing scene classification by unsupervised representation learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="5148" to="5157" />
			<date type="published" when="2017-09">Sep. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bag-of-visualwords scene classifier with local and global features for high spatial resolution remote sensing imagery</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="747" to="751" />
			<date type="published" when="2016-06">Jun. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Remote sensing image scene classification using multi-scale completed local binary patterns and Fisher vectors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">483</biblScope>
			<date type="published" when="2016-06">Jun. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fusing local and global features for high-resolution scene classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2889" to="2901" />
			<date type="published" when="2017-06">Jun. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning coarse-to-fine sparselets for efficient object detection and scene classification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015-06">Jun. 2015</date>
			<biblScope unit="page" from="1173" to="1181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scene classification using local and global features with collaborative representation fusion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">348</biblScope>
			<biblScope unit="page" from="209" to="226" />
			<date type="published" when="2016-06">Jun. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning deep event models for crowd anomaly detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">219</biblScope>
			<biblScope unit="page" from="548" to="556" />
			<date type="published" when="2017-01">Jan. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploring models and data for remote sensing image caption generation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2183" to="2195" />
			<date type="published" when="2018-04">Apr. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A coarse-to-fine semi-supervised change detection for multispectral images</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3587" to="3599" />
			<date type="published" when="2018-06">Jun. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deformable convolutional neural networks for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ghamisi</surname></persName>
		</author>
		<idno type="DOI">10.1109/LGRS.2018.2830403</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett., to be published</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Transferring deep convolutional neural networks for the scene classification of high-resolution remote sensing imagery</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="14680" to="14707" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">When deep learning meets metric learning: Remote sensing image scene classification via learning discriminative CNNs</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2811" to="2821" />
			<date type="published" when="2018-05">May 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep feature fusion for VHR remote sensing scene classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chaib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4775" to="4784" />
			<date type="published" when="2017-08">Aug. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Integrating multilayer features of convolutional neural networks for remote sensing scene classification</title>
		<author>
			<persName><forename type="first">E</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Samat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5653" to="5665" />
			<date type="published" when="2017-10">Oct. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Towards better exploiting convolutional neural networks for remote sensing scene classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">A B</forename><surname>Penatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="539" to="556" />
			<date type="published" when="2017-01">Jan. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf</title>
		<meeting>Conf</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent</title>
		<meeting>Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015-06">Jun. 2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2009-06">Jun. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Understanding deep image representations by inverting them</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015-06">Jun. 2015</date>
			<biblScope unit="page" from="5188" to="5196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015-05">May 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017-07">Jul. 2017</date>
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Comparing SIFT descriptors and Gabor texture features for classification of remote sensed imagery</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process</title>
		<meeting>IEEE Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2008-10">Oct. 2008</date>
			<biblScope unit="page" from="1852" to="1855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bag-of-visual-words and spatial extensions for land-use classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf</title>
		<meeting>Int. Conf</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Land-use classification with compressive sensing multifeature fusion</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Mekhalfi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Melgani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Alajlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2155" to="2159" />
			<date type="published" when="2015-10">Oct. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A non-negative low-rank representation for hyperspectral band selection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="4590" to="4609" />
			<date type="published" when="2016-10">Oct. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Manifold regularized sparse NMF for hyperspectral unmixing</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2815" to="2826" />
			<date type="published" when="2013-05">May 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Joint dictionary learning for multispectral change detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="884" to="897" />
			<date type="published" when="2017-04">Apr. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Super-resolution of hyperspectral image via superpixel-based sparse representation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">273</biblScope>
			<biblScope unit="page" from="171" to="177" />
			<date type="published" when="2018-01">Jan. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Segmentation based sparse reconstruction of optical coherence tomography images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cunefare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Farsiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="407" to="421" />
			<date type="published" when="2017-02">Feb. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Face description with local binary patterns: Application to face recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2037" to="2041" />
			<date type="published" when="2006-12">Dec. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
			<date type="published" when="2005-06">Jun. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Land-use scene classification in high-resolution remote sensing images using improved correlatons</title>
		<author>
			<persName><forename type="first">K</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2403" to="2407" />
			<date type="published" when="2015-12">Dec. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A spectral-structural bag-of-features scene classifier for very high spatial resolution remote sensing imagery</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="73" to="85" />
			<date type="published" when="2016-06">Jun. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Spatial pyramid co-occurrence for image classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2011-11">Nov. 2011</date>
			<biblScope unit="page" from="1465" to="1472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pyramid of spatial relatons for scene-level land use classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1947" to="1957" />
			<date type="published" when="2015-04">Apr. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Scene classification based on the sparse homogeneous-heterogeneous topic feature model</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2689" to="2703" />
			<date type="published" when="2018-05">May 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Semi-supervised multitask learning for scene recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1967" to="1976" />
			<date type="published" when="2015-09">Sep. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Local structure learning in high resolution remote sensing image retrieval</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">207</biblScope>
			<biblScope unit="page" from="813" to="822" />
			<date type="published" when="2016-09">Sep. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via spectral clustering of multidimensional patches for remotely sensed scene classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2015" to="2030" />
			<date type="published" when="2015-05">May 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Saliency-guided unsupervised feature learning for scene classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2175" to="2184" />
			<date type="published" when="2015-04">Apr. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2014-06">Jun. 2014</date>
			<biblScope unit="page" from="1717" to="1724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2014-06">Jun. 2014</date>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Land use classification in remote sensing images by convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Castelluccio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sansone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Verdoliva</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1508.00092" />
		<imprint>
			<date type="published" when="2015-08">Aug. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Improving the Fisher kernel for large-scale image classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="143" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Remote sensing image scene classification using bag of convolutional features</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1735" to="1739" />
			<date type="published" when="2017-10">Oct. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Matrix backpropagation for deep networks with structured layers</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vantzos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015-12">Dec. 2015</date>
			<biblScope unit="page" from="2965" to="2973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Free-form region description with second-order pooling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1177" to="1189" />
			<date type="published" when="2015-06">Jun. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Is second-order information helpful for large-scale visual recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017-10">Oct. 2017</date>
			<biblScope unit="page" from="2089" to="2097" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Region covariance: A fast descriptor for detection and classification</title>
		<author>
			<persName><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="589" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Pedestrian detection via classification on Riemannian manifolds</title>
		<author>
			<persName><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1713" to="1727" />
			<date type="published" when="2008-10">Oct. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A new spatialspectral feature extraction method for hyperspectral images using local covariance matrix representation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Plaza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3534" to="3546" />
			<date type="published" when="2018-06">Jun. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Inability of humans to discriminate between visual textures that agree in second-order statistics-Revisited</title>
		<author>
			<persName><forename type="first">B</forename><surname>Julesz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shepp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Frisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="391" to="405" />
			<date type="published" when="1973-12">Dec. 1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Towards faster training of global covariance pooling networks by iterative matrix square root normalization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Bilinear CNN models for fine-grained visual recognition</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015-12">Dec. 2015</date>
			<biblScope unit="page" from="1449" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Geometric means in a novel vector space structure on symmetric positive-definite matrices</title>
		<author>
			<persName><forename type="first">V</forename><surname>Arsigny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Pennec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ayache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="328" to="347" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Log-Euclidean metrics for fast and simple calculus on diffusion tensors</title>
		<author>
			<persName><forename type="first">V</forename><surname>Arsigny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Pennec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ayache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magn. Reson. Med</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="411" to="421" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">MatConvNet: Convolutional neural networks for MATLAB</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia</title>
		<meeting>ACM Int. Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="689" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
