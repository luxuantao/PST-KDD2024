<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 1 R 2 -CNN: Fast Tiny Object Detection in Large-Scale Remote Sensing Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Cong</forename><surname>Li</surname></persName>
							<email>licong@sensetime.com</email>
						</author>
						<author>
							<persName><forename type="first">Jianping</forename><surname>Shi</surname></persName>
							<email>shijianping@sensetime.com</email>
						</author>
						<author>
							<persName><forename type="first">Zhihai</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Huajun</forename><surname>Feng</surname></persName>
							<email>fenghj@zju.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Modern Optical Instrumentation</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<postCode>310027</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">College of Optical Science and Engineering</orgName>
								<orgName type="institution">Zhejiang Univer-sity</orgName>
								<address>
									<postCode>310027</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">SenseTime Research</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 1 R 2 -CNN: Fast Tiny Object Detection in Large-Scale Remote Sensing Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">235A7EF0470E8FBF959B850B532C6704</idno>
					<idno type="DOI">10.1109/TGRS.2019.2899955</idno>
					<note type="submission">Manuscript received May 27, 2018; revised October 5, 2018 and December 12, 2018; accepted January 28, 2019.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Object detection</term>
					<term>remote sensing images</term>
					<term>remote sensing region-based convolutional neural network (R 2 -CNN)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, the convolutional neural network has brought impressive improvements for object detection. However, detecting tiny objects in large-scale remote sensing images still remains challenging. First, the extreme large input size makes the existing object detection solutions too slow for practical use. Second, the massive and complex backgrounds cause serious false alarms. Moreover, the ultratiny objects increase the difficulty of accurate detection. To tackle these problems, we propose a unified and self-reinforced network called remote sensing region-based convolutional neural network (R 2 -CNN), composing of backbone Tiny-Net, intermediate global attention block, and final classifier and detector. Tiny-Net is a lightweight residual structure, which enables fast and powerful features extraction from inputs. Global attention block is built upon Tiny-Net to inhibit false positives. Classifier is then used to predict the existence of target in each patch, and detector is followed to locate them accurately if available. The classifier and detector are mutually reinforced with end-to-end training, which further speed up the process and avoid false alarms. Effectiveness of R 2 -CNN is validated on hundreds of GF-1 images and GF-2 images that are 18 000 × 18 192 pixels, 2.0-m resolution, and 27 620 × 29 200 pixels, 0.8-m resolution, respectively. Specifically, we can process a GF-1 image in 29.4 s on Titian X just with single thread. According to our knowledge, no previous solution can detect the tiny object on such huge remote sensing images gracefully. We believe that it is a significant step toward practical real-time remote sensing systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>However, those methods mainly focus on small region segments comparing to the original large inputs, e.g., usually over than 20 000×20 000 pixels. Therefore, they cannot scale up to handle such huge images gracefully. Zhang et al. <ref type="bibr" target="#b3">[4]</ref> attempted to detect airports at first to speed up the process in largescale images, but the training and testing images are all from the regions near to airports, which escape from the complex backgrounds. According to our experiments, it is not robust enough for practical applications.</p><p>Object detection in large-scale remote sensing images is pretty challenging. First, the scale of the input image is too large to reach the practical application. The computation time and memory consumption are increased quadratically, making it too slow and not runnable on current hardware. Second, massive and complex backgrounds that appear in a real scenario may introduce more false positives, for instance, desert region with random texture or urban area with massive building structure. Moreover, the performance drops drastically with tiny objects (such as 8-32 pixels), especially in the lowresolution images, which further increases the difficulty of tiny object detection in remote sensing images.</p><p>To tackle these problems, we propose a unified and selfreinforced convolutional neural network called remote sensing region-based convolutional neural network (R 2 -CNN), which is composed of the backbone Tiny-Net, intermediate global attention block, and final classifier and detector, enabling the entire network efficient in both computation and memory consumption, robust to false positives, and strong to detect tiny objects. Pipeline is shown in Fig <ref type="figure" target="#fig_0">1</ref>.</p><p>First, as a unified and self-reinforced framework, R 2 -CNN first crops large-scale images with a much more smaller scale (such as 640×640 pixels) with 20% overlap to tackle the oversized input size. By processing the patches asynchronously, the limited memory is not a problem anymore. A convolutional backbone structure is then applied to inputs, which enables powerful features extraction. Based on the discriminative features, a classifier first predicts the existence of detection target in the current patch, and a detector is followed to locate them accurately if available. The classifier and detector are mutually reinforced each other under the end-to-end training framework. There are two advantages of this self-reinforced architecture as follows.</p><p>1) Since, in large-scale remote sensing images, most crops do not contain valid target so that about 99% of the total patches do not need to pass the heavy detector branch. The light classifier branch can filter out a blank patch without heavier detector cost. 2) As most false positives commonly occur with massive backgrounds, benefited from the self-reinforced framework, the classifier can identify the difficult situation even when there is only one tiny object in the patch given the fine-grained features from the detector. On the other hand, the detector receives less false positive candidates since most of them are filtered out by the classifier.</p><p>Even if the patches are distinguished incorrectly by a classifier, the detector can still rectify the results later.</p><p>Second, we specially designed a lightweight residual network called Tiny-Net to reduce the inference cost and preserve powerful features for object detection. Tiny-Net is motivated by <ref type="bibr" target="#b5">[6]</ref> but is much more lightweight. On the other hand, Tiny-Net can be trained from scratch with a cycle training schedule because of fewer parameters, making that the framework does not be influenced by the limited training samplers and the domain gap between natural images and remote sensing images.</p><p>Third, to further inhibit the false positives, we also use feature pyramid pooling as a global attention block on the top of Tiny-Net. The feature maps are first pooled in different pyramid levels, such as 1 × 1, 2 × 2, and 4 × 4. Then, we recover the pooled features to their original scale with bilinear interpolation. The feature maps are fused additionally next. Feature maps get more context information, and the receptive field is also enlarged to the whole image. The detector is more discriminative with the help of more context information. We can find that the confidence of false positives drops obviously with this module, proving its effectiveness.</p><p>Finally, to make the framework strong to detect tiny objects, we comprehensively analyzed why the detected performance drops drastically with tiny objects and proposed a scaleinvariant anchor strategy to tile anchors reasonably, especially for small objects based on the region proposal network (RPN) in <ref type="bibr" target="#b6">[7]</ref>. On the other hand, we insert an efficient zoom-out and zoom-in architecture in Tiny-Net to enlarge the feature maps, which improve the recall of tiny objects obviously. Position-sensitive region of interests (RoI) pooling <ref type="bibr" target="#b7">[8]</ref> is also used to share the computation from all detectors on the entire image and get more spatial information, which is faster and more accurate than the original RoI pooling in <ref type="bibr" target="#b6">[7]</ref>.</p><p>Our contributions can be summarized into four components.</p><p>1) We proposed a unified and self-reinforced framework called R 2 -CNN, which is efficient in computation and memory consumption, robust to false positives, and strong to detect tiny objects.</p><p>2) We proposed Tiny-Net, a lightweight residual network that can be trained from scratch and further improve the efficiency. 3) We insert a global attention block into R 2 -CNN to further inhibit the false positives. 4) We comprehensively analyze why the detected performance drops drastically with tiny objects and then make the framework strong to detect tiny objects. The remainder of this paper is organized as follows.</p><p>In Section II, we briefly introduce the state-of-the-art object detection methods and their applications on remote sensing systems. Then, we explain the details of our R 2 -CNN in Section III and show the experiments in Section IV. Finally, Section V concludes this paper with a discussion of the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>As a fundamental problem in a remote sensing area, object detection in remote sensing images has been extensively studied in recent years. Previous methods (such as scaleinvariant feature transform <ref type="bibr" target="#b8">[9]</ref> and histogram of oriented gradient (HoG) <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>) use low-level or middle-level feature representations to detect objects. Recently, impressive improvements have achieved with convolutional neural networks. Cheng and Han <ref type="bibr" target="#b11">[12]</ref> provide a review of the recent progress in those fields and propose two promising research directions, which are deep learning-based methods and weakly supervised learning-based methods.</p><p>Convolutional neural networks got a start from LeNet <ref type="bibr" target="#b12">[13]</ref> and became popular with AlexNet <ref type="bibr" target="#b13">[14]</ref>. Many impressive methods are proposed to promote the development of image recognition from then on, such as network-in-network <ref type="bibr" target="#b14">[15]</ref>, VGGNet <ref type="bibr" target="#b15">[16]</ref>, and GoogLeNet <ref type="bibr" target="#b16">[17]</ref>. ResNet <ref type="bibr" target="#b5">[6]</ref> is a milestone, which is using residual connections to train very deep convolutional models. It made a great improvement in image recognition. Object detectors, such as OverFeat <ref type="bibr" target="#b17">[18]</ref> and region convolutional neural network (R-CNN) <ref type="bibr" target="#b18">[19]</ref>, made dramatic improvements in accuracy with those deep learningbased feature representations. OverFeat adopted a Conv-Net as a sliding window detector on an image pyramid. R-CNN adopted a region proposal-based method based on selective search <ref type="bibr" target="#b19">[20]</ref> and then used a Conv-Net to classify the scalenormalized proposals. spatial pyramid pooling (SPP) <ref type="bibr" target="#b20">[21]</ref> adopted R-CNN on feature maps extracted on a single image scale, which demonstrated that such region-based detectors could be applied much more efficiently. Fast R-CNN <ref type="bibr" target="#b21">[22]</ref> and Faster R-CNN <ref type="bibr" target="#b6">[7]</ref> made a unified object detector in a multitask manner. Region proposal networks are proposed to replace selective search. Dai et al. <ref type="bibr" target="#b7">[8]</ref> proposed R-FCN, which uses position-sensitive RoI pooling to get a faster and better detector. While those region-based methods are too slow for practical use, a single-stage detector, such as YOLO <ref type="bibr" target="#b22">[23]</ref> and SSD <ref type="bibr" target="#b23">[24]</ref>, is proposed to accelerate the processing speed but with a performance drop, especially in small objects.</p><p>Along with the rapid development with those mechanisms, small object detection seems much more difficult, and thus, researchers proposed many frameworks for small object detection specifically. Those methods mainly focus on how to implement a multiscale framework elegantly or using hard mining method which let the network pay more attention to small objects. Lin et al. <ref type="bibr" target="#b24">[25]</ref> proposed feature pyramid networks that use the top-down architecture with lateral connections as an elegant multiscale feature warping method. Zhang et al. <ref type="bibr" target="#b25">[26]</ref> proposed a scale-equitable face detection framework to handle different scales of faces well. Hu and Ramanan <ref type="bibr" target="#b26">[27]</ref> showed that the context is crucial and defines the templates that make use of massively large receptive fields. Zhao et al. <ref type="bibr" target="#b27">[28]</ref> proposed a pyramid scene parsing network that employs the context reasonable. Shrivastava et al. <ref type="bibr" target="#b28">[29]</ref> proposed an online hard example mining method that can improve the performance of small objects obviously.</p><p>Many methods <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b34">[35]</ref> are proposed to improve the object detection accuracy in remote sensing images with convolutional neural networks. Those methods often use pretrained CNN models on large data sets to handle the limited remote sensing training data. Zhang et al. <ref type="bibr" target="#b30">[31]</ref> used the trained CNN models to extract surrounding features. Those features were combined with features from HoG to get final representations and then applied gradient orientation to generate region proposals. Zhu et al. <ref type="bibr" target="#b35">[36]</ref> used CNN features from multilevel layers for object detection, which handle the scale-invariance with single scale input. Jiang et al. <ref type="bibr" target="#b36">[37]</ref> used a graph-based superpixel segmentation to generate proposals and then trained a CNN to classify these proposals into different classes. Cheng et al. <ref type="bibr" target="#b34">[35]</ref> introduced a rotation-invariant operator to the existing CNN architectures and achieves a significant performance. Long et al. <ref type="bibr" target="#b1">[2]</ref> proposed an unsupervised scorebased bounding-box regression for accurate object localization in remote sensing images. Those methods mainly focus on small region segment compared to the original large remote sensing image input, usually over 10 000 × 10 000 pixels, and thus, they cannot scale up to handle such large input gracefully. Zhang et al. A. R 2 -CNN R 2 -CNN is a unified and self-reinforced framework working in an end-to-end manner. Considering that the large input image size increases the computation time and memory consumption quadratically, large-scale remote sensing images (such as 20 000 × 20 000 pixels) are cropped with a much more smaller scale (such as 640 × 640 pixels) with 20% overlap. By processing the patches asynchronously, the limited memory is not a problem anymore.</p><p>A convolutional backbone structure is then applied to the inputs, which enables powerful features extraction. Based on those discriminative features, the classifier first predicts the existence of detection target in the current patch, and the detector is followed to locate them accurately if available. The classifier and detector are mutually reinforced each other under the end-to-end training framework. There are two advantages of this self-reinforced architecture as follows.</p><p>First, the light classifier branch can filter out a blank patch without heavier detector cost. Classifier's architecture is in Fig. <ref type="figure" target="#fig_1">2(b</ref>), and we just use two CONV-BN-RELU blocks to extract features from the former features. Global average pooling and a 1 × 1 convolutional operator are then attached to it. Softmax loss is employed to guide the training of the classifier. Considering that most crops do not contain a valid target in remote sensing images, about 99% of the total patches do not need to pass the heavy detector branch.</p><p>Second, massive and complex backgrounds appear in a real scenario may introduce more false positive, for instance, desert region with random texture or urban area with massive building structure. The false positives are first inhibited by the mutual reinforcement from the classifier and the detector. On the one hand, the classifier can distinguish the difficult situation even when there is only one tiny object (such as 12 ×12 pixels) in the patch. We explain this promotion mainly given the fine-grained feature extracted from the detector. On the other hand, the detector receives less false positive candidates since most of them are filtered out by the classifier. Even if the patches are distinguished incorrectly by the classifier, the detector can still rectify the results later.</p><p>There are three outputs from our network. One output m from classifier represents the probability of whether there are target objects in corresponding patch or not. Two outputs from detector represent the discrete probability [ p = ( p 0 , . . . , p k )] distribution of each RoI over K + 1 categories and bounding-box regression offsets, t k = (t k x , t k y , t k w , t k h ), for each of the K object classes, indexed by k, in the corresponding patch. We use the parameterization for t k in <ref type="bibr" target="#b18">[19]</ref>, in which t k represents a scale-invariant translation and logspace height/width shift relative to an object proposal. Each of the training patches is labeled with a binary ground truth n, and each RoI in detector is labeled with a ground-truth class u and a ground-truth bounding-box regression target v. We use a unified multitask loss L on each patch to joint classifier and detector</p><formula xml:id="formula_0">L(m, n, p, u, t u , v) = L cls (m, n) + μ[n = 1](L cls ( p, u) +λ[u ≥ 1]L loc (t u , v)) (1)</formula><p>in which L cls ( p, u) and L cls (m, n) are softmax loss and L loc is smooth-L1 Loss in <ref type="bibr" target="#b6">[7]</ref>. The hyperparameter λ and μ in (1) controls the balance between the three task losses. All experiments use λ = 1 and μ = 1. We only backpropagate detector's loss when there are detection targets in the corresponding patch during training time. The entire network is efficient, robust, and strong.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Tiny-Net</head><p>Recently, CNN-based methods often use VGG <ref type="bibr" target="#b15">[16]</ref> or ResNets <ref type="bibr" target="#b5">[6]</ref> as feature extractors. Those models are pretrained on ImageNet <ref type="bibr" target="#b37">[38]</ref>, a large-scale hierarchical image database with millions of images, to deal with the limited training samples and get a much more quicker convergence. However, there are still many disadvantages when using those pretrained models. First, those models are too heavy to reach real-time efficiency. Second, those models are designed specifically for image classification, making that the feature resolution may be not enough for object detection. Finally, considering the heavy parameters, training scratch is pretty difficult, especially with limited training samplers. When applying the pretrained models to remote sensing frameworks, the domain gap between natural images and remote sensing images may make the models suboptimal.</p><p>The architecture of Tiny-Net is shown in Table <ref type="table" target="#tab_0">I</ref>. The 3 × 3 block is a residual block in ResNet <ref type="bibr" target="#b5">[6]</ref> except conv-1. We do not apply the downsample operator in conv-1 to enable the feature maps more discriminative for tiny object detection, which is different from ImageNet pretrain models such as VGG <ref type="bibr" target="#b15">[16]</ref> and ResNets <ref type="bibr" target="#b5">[6]</ref>. The parameters of Tiny-Net are far less than ResNets. Thanks to this lightweight architecture, Tiny-Net can be trained from scratch and converge well just with a cycle training schedule, which iteratively updates the step learning rate twice or more. Under this condition, Tiny-Net will not be influenced by the domain gap between the natural images and the remote sensing images. Benefited from those characters, Tiny-Net can reduce inference cost and preserve powerful features for tiny object detection in remote sensing images, which further improved the efficiency of R 2 -CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Global Attention</head><p>Thanks to the unified classifier and detector, numerous blank patches are filtered by classifier, making that false positives reduce obviously. However, the problem still exists because of the limited receptive field. When you catch sight of two objects with similar appearance, you may not be sure what they are without context information. For example, when you see the top image in Fig. <ref type="figure" target="#fig_3">3</ref>(a), you may confuse in this question: what exactly are they? However, when you see two images in the bottom image, you can easily distinguish them out. As discussed in <ref type="bibr" target="#b38">[39]</ref>, the CNN has two types of receptive fields: the theoretical receptive field and the effective receptive field. The theoretical receptive field indicates the input region that can affect the value of this unit theoretically. However, not every pixel in the theoretical receptive field contributes equally to the final output. Only a subset of the area has an effective influence on the output value, which is called effective receptive field. The effective receptive field is smaller than the theoretical receptive field, as shown in Fig. <ref type="figure" target="#fig_3">3(b)</ref>. The limited effective receptive field leads the final feature map to obtain little context information, thus leading to more false positives.</p><p>Inspired by this phenomenon, we use feature pyramid pooling as a global attention block on the top of Tiny-Net. The architecture is shown in Fig. <ref type="figure" target="#fig_3">3(c</ref>). The feature maps are first pooled in different pyramid levels, such as 1 × 1, 2 × 2, and 4 × 4. Then, we recover the pooled features to their original scale with bilinear interpolation. The feature maps are fused additionally next. Feature maps can get more context information, and the receptive field will also be enlarged to the whole image. The global attention module fuses the features from different pyramid scales and leads the detector to pay more attention to the whole image. The detector is more discriminative with the help of more context information. We can found that the confidence of false positives drops obviously with this module, as shown in Fig. <ref type="figure" target="#fig_3">3(d)</ref>, proving its effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Detector</head><p>Our R 2 -CNN is strong to detect tiny objects. The architecture of the detection branch is shown in Fig. <ref type="figure" target="#fig_1">2(c</ref>). The stateof-the-art object detectors are mainly based on the RPN in <ref type="bibr" target="#b6">[7]</ref>, which use anchors to generate object proposals. Anchors are a set of predefined boxes with multiple scales and aspect ratios tiled regularly on the image plane. However, anchorbased detectors drop the performance drastically on objects with tiny sizes, such as less than 16 × 16 pixels, and those tiny objects are the majority in remote sensing images, such as airplanes, ships, and cars. To tackle this problem, we first investigate why this is the case and propose a scale-invariant anchor strategy to tile anchors reasonably, especially for small objects. On the other hand, we insert an efficient zoom-out and zoom-in architecture in Tiny-Net to enlarge the feature map without margin cost, which improves the recall of tiny objects obviously. Position-sensitive RoI pooling is also used to get more spatial information. Through these ways, we get the excellent results on tiny object detection.</p><p>1) Why This Is the Case?: Like in Fig. <ref type="figure" target="#fig_4">4</ref>(a), the stride size of the lowest anchor-associated layer is too large (e.g., 16 pixels or 32 pixels), and the features loss along with the downsampling of pooling layer. Therefore, tiny objects have been highly squeezed on these layers and have a few features for detection. An airplane may be only 1 × 1 pixels in the final feature map. On the other hand, the anchor scales are discrete (i.e., 16, 32, 64, . . . 2 k ), but object scales are continuous. During training, an anchor will be assigned to a ground-truth box if its intersection over union (IoU) with this box is the highest than other anchors or its IoU is higher than a threshold T h . When the object's scale is near to anchor scales, they will be attached more anchors and thus easier to be located. The face detector single shot scale-invariant face detector (S 3 FD) <ref type="bibr" target="#b25">[26]</ref>, which uses SSD <ref type="bibr" target="#b23">[24]</ref> architecture, explained this phenomenon appropriately, and we infer their statistics in Fig. <ref type="figure" target="#fig_4">4(b)</ref>. It shows the number of matched anchors at different face scales under 16, 32, 64, . . . 2 k anchor scales. If an object has a scale over average line, it will be matched enough anchors. However, tiny objects are matched a few anchors, leading to performance drop drastically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Our Method:</head><p>To tile anchors more reasonably, we analyze the bounding boxes' scale distribution of our training data set, which is shown in Fig. <ref type="figure" target="#fig_4">4(d)</ref>. We can see that tiny objects are majority in it. Instead of choosing anchors by hand, we run k-means clustering on the training set to automatically find good priors. Our training set's bounding-box scales are X : (x1, x2, . . . , x n ). There are k anchor scales we want, and the center scales are (μ 1 , μ 2 , . . . , μ k ). Scales are clustered (2)</p><p>we can get the clustered anchor scales.</p><p>To extract beneficial features for tiny object detection, one way is to reduce the anchor stride by enlarging the feature map using a zoom-out and zoom-in architecture. Like shown in Fig. <ref type="figure" target="#fig_4">4</ref>(c), we first zoom out the feature map with a residual block, and thus, its anchor stride is 16 pixels. Then, we employed to recover the feature maps that are recovered to their original scale using a zoom-in operator. In addition, we use a skip connection between the stride-8 layer and the upsampled layer. We found that the stride-16 layer can extract more high-level features, which is beneficial to object detection. The skip connection can fuse low-level features and highlevel features, making final feature maps more discriminative.</p><p>Considering the complicated backgrounds of remote sensing images, we use position-sensitive RoI pooling <ref type="bibr" target="#b7">[8]</ref> in our detector instead of RoI pooling. RoI pooling applies costly per-region subnetwork hundreds of times. If there are 1000 proposals, the detector will be tested 1000 times wastefully. In contrast to this operator, position-sensitive RoI pooling is fully convolutional with almost all computation shared on the entire image. Much computation is saved with this operator. On the other hand, position-sensitive RoI pooling can address the dilemma between translation invariance in image classification and translation variance in object detection. More spatial information is extracted, thus leading to a better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we first present the implementations of our R 2 -CNN, such as data sets, evaluation metric, and parameter settings. The results of our network and comparative experiments are then discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementations</head><p>1) Data Sets: Due to the lack of standard data sets of large-scale remote sensing images, we collected 1169 GF-1 images and 318 GF-2 images that are 18 000 × 18 192 pixels, 2.0-m resolution, and 27 620 × 29 200 pixels, 0.8-m resolution, respectively. In addition, we collected 38 472 pieces of 640 × 640 images, which contains target objects from the publicly available Google Earth service to supplement the poor positive patches in GF images. All the images are cropped in the patches of 640×640 pixels with 20% overlap. In particular, if we have a maximum object scale d and a cropped scale D, we recommend an overlap d/D. For example, as the scales in Fig. <ref type="figure" target="#fig_4">4(d)</ref>, the maximum scale of the objects in our data set is about 128 and the cropped scale is 640; thus, a 20% overlap is applied to not only prevent the object from being truncated off but also augment the data sets. To help the convergence of the network, we control the proportion of positive patches and negative patches to be 1 : 3 to obtain a balanced training set. The negative patches are selected using hard example boosting to enhance the training process.</p><p>We collect 102 GF-1 images as GF1-test-dev and 40 GF-2 images as GF2-test-dev to evaluate the ability of R 2 -CNN. To evaluate our model more exhaustively, we collected 4633 images (640 × 640 pixels) from Google Earth as Rgb-test-dev and 1000 images (640 × 640 pixels) from GF-1 and GF-2 as Gray-test-dev, which help us evaluate the ability of the detector.</p><p>2) Evaluation Metric: Considering the few target objects in large-scale remote sensing images and the requirement for practical engineering applications, we evaluate our R 2 -CNN with recall and precision of different score thresholds. The correct number of detections is true positives T P, and the number of spurious detections of the same object is false positives F P. The number of ground-truth instances is N P. The precision and recall are given in the following:</p><formula xml:id="formula_1">Precision = T P T P + F P (3) Recall = T P N P . (<label>4</label></formula><formula xml:id="formula_2">)</formula><p>We also show the instance number in detail for more intuitional and convincing. Considering numerous negative scene of classifier, the accuracy is generally over 99.0%, making this metric meaningless. Therefore, we use recall, precision, and instance number to evaluate our classifier. We use mean average precision (mAP) and Max-Recall with a score threshold of 0.05 as in PASCAL-VOC <ref type="bibr" target="#b39">[40]</ref> to evaluate our detector, which are defined as</p><formula xml:id="formula_3">m AP = 1 Q Q q=1 A P(q) (5) A P = n k=1 (P(k) × r (k)) | R(q) | (<label>6</label></formula><formula xml:id="formula_4">)</formula><p>where Q is the number of categories, | R(q) | is the number of images relevant to the category q, k is the rank in the sequence of retrieved images, n is the total number of retrieved images, P(k) is the precision at cutoff k in the list, and r (k) is an indicator function whose value is 1 if the image at rank k is relevant and is 0 otherwise.</p><p>3) Parameter Settings: We adopt synchronized stochastic gradient descent training on 8 GPUs with synchronized batch normalization. A minibatch involves 1 image per GPU and 512 proposals per image for detector training. We use a momentum of 0.9 and a weight decay of 0.0005. We use a learning rate of 0.001 for 80k minibatches and 0.0001, 0.00001 for the next 80k and 40k minibatches. The learning rate and training epochs are iterated twice as a cycle schedule, because the network is trained from scratch. We randomly initialize all layers by drawing weights from a zeromean Gaussian distribution with a standard deviation of 0.01. The anchor's scale in RPN we used in our final model is <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b29">30]</ref> with a stride of 8, and the anchor's ratio is 1 considering the airplane's shape. Other implementation details are the same as in <ref type="bibr" target="#b5">[6]</ref> and <ref type="bibr" target="#b6">[7]</ref>. We also use multiscale training with scale [513, 609, 721, 801, 913] and online hard example mining <ref type="bibr" target="#b28">[29]</ref> for hard example boosting. We stretch remote sensing images from uint16 to uint8 which is divided by a factor of 4. We also stretch the images' histogram with a factor in [0, 0.02], which can not only inhibit the noise in remote sensing images but also as a data augmentation method. We augment the data online with rotation and flipping randomly. Our implementation uses Caffe <ref type="bibr" target="#b40">[41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation of R 2 -CNN</head><p>We comprehensively evaluate our method on the GF1-test-dev and GF2-test-dev. The results are shown in Tables <ref type="table" target="#tab_0">II</ref> and<ref type="table" target="#tab_0">III</ref>. It is efficient that we can process a 18 000 × 18 192 GF-1 image in 29.4 s on Titian X just with single thread. It is robust that with a score threshold of 0.85, there are only 10 false positives in GF1-test-dev and 5 false positives in GF2-test-dev. It is strong that we can get 95.78 recall and 98.33 precision on GF1-test-dev also with a score threshold of 0.85, showing its potential for practical application. We compared our R 2 -CNN with ununified (train and test separately) network and fully detection network. The results show that when we joint detector with classifier, they can promote each other to get the best results. When the network is ununified, the classifier cannot obtain enough discrimination without the feature extracted by the detector, leading recall and precision drop obviously. When detecting the objects all using detector, too many false positives appeared and the efficiency is lower considering the heavy detector. Our network achieves the superior results on performance and speed, showing its efficient, robust, and strong, corresponding visualized results are shown in Fig. <ref type="figure" target="#fig_6">6</ref>. 1) Efficient: Tiny-Net enables the inference time of network less than ResNet-50 or other large models and preserves powerful features for object detection, and the details are shown in Tables <ref type="table" target="#tab_0">IV</ref> and<ref type="table">V</ref>. The classifier with Tiny-Net costs 16.63 ms with 640 × 640 inputs on Titian X, and the detector costs 45.21 ms with the same setting. The detector is three times slower than classifier. In our GF1-test-dev, there are only 154 patches that have target objects. The unified architecture makes that 99.9% of the total patches do not need to pass the heavy detector branch. Total costs of our network and fully detection network are shown in Table <ref type="table" target="#tab_3">VI</ref>. We can process a 18 000 × 18 192 GF-1 image in 29.4 s on Titian X just with single thread. Though there is still a long way to build a real-time detection system on large-scale remote  2) Robust: The unified classifier can identify the difficult situation even when there is only one tiny objects in the patch, given the fine-grained feature extracted by the detector, and the detector receives less false positive candidates since most of them are filtered out by the classifier. The results are shown in Tables <ref type="table" target="#tab_0">IV</ref> and<ref type="table">V</ref>. Because of the large memory needed by ResNet-50 with an anchor stride of 8, we cannot evaluate our R 2 -CNN with ResNet-50. Thus, the setting of ResNet-50 is the same as in <ref type="bibr" target="#b5">[6]</ref> without detector. GF1-test-dev is cropped in 131 148 patches with 640×640 pixels and 20% overlap, and there are also 123 120 patches from GF2-test-dev. The recall and precision only consider the positive patches. The results of our R 2 -CNN get the best results compared with others. The classifier drops performance drastically without detector, and the recall of our R 2 -CNN is higher than ResNet-50, showing the effectiveness of the features extracted by the detector. Though there are more false positives in our model, the detector can rectify the results later. We have attempted to train ResNet-50 from scratch to break the domain gap between natural images and remote sensing images but get bad results. We argue that this is mainly because of the numerous parameters of ResNet-50 but with the small amount of training sets. Though there are hundreds of thousand remote sensing images for us, only a few of them can be used during training time to handle the positive-negative imbalance problem.  The performance of objects larger than 32 pixels is basically comparable to our method, but the results of small objects drop obviously without the architecture. In addition, we attempt to attach the global attention block to conv-4 directly. The results of No Conv-5 drop 2 points compared to our method. This modification shows that Conv-5 can extract more high-level features that are benefited for object detection, proving the effectiveness of this architecture.</p><p>How Important Is Our Anchor Strategy? The number of clustered points is a handcrafted parameter in k-means. We attempt to cluster anchor scales with a number of 4, 5, 6, and 8. The results with different anchor scales are shown in the following.  <ref type="table" target="#tab_5">VIII</ref> and<ref type="table" target="#tab_6">IX</ref>. The results show that the distribution of anchors can better fit the data set to reach better performance with this strategy. We found that Anchor-6 gets an excellent tradeoff between efficiency and performance so that it is the final parameter of our R 2 -CNN.</p><p>How Important Is Position-Sensitive RoI Pooling? Compared with Faster R-CNN <ref type="bibr" target="#b6">[7]</ref>, our R 2 -CNN improves mAP by 2.35 points in Gray-test-dev, particularly in tiny objects. The spatial information is better encoded via position-sensitive RoI pooling, which is beneficial to tiny object detection. Moreover, the time costs is 49.26 ms for Faster R-CNN but 45.21 ms for R 2 -CNN with position-sensitive RoI pooling. Through sharing all proposals' weights, we instead recalculate the feature maps of all proposals of voting from the final feature maps. This modification is also greatly helpful for an efficient process, especially when there are numerous patches.</p><p>Comparison Experiments With the State of the Art: To validate the effectiveness of our architecture, the comparison experiments are implemented with FPN Faster R-CNN and Mask R-CNN. Considering the lack of mask annotations, Mask R-CNN is only implemented with RoI Align. Both the methods are implemented with a ResNet-50 backbone. The results are shown in Table <ref type="table">X</ref>. The comparable results prove the effectiveness of R 2 -CNN. We also found that the RoI Align brings little improvement to the FPN Faster R-CNN baseline. Considering that the tiny objects are dominating our </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Discussion</head><p>In our experiments, we get pretty surprising results in Fig <ref type="figure" target="#fig_7">7</ref>, i.e., an image is under the heavy haze. We found that we can detect the aircrafts easily. The confidence of those objects is higher than 0.9 but lower than 0.95. It is not a high score in our results (a high confidence is larger than 0.99) but still enough for practical engineering applications. We must recognize that these gratifying results are not only coming from the reasonable architecture of our R 2 -CNN but also the precise annotation of similar situations in our training set. The backbone, classifier, and detector are specially designed to converge well while training from scratch. This is a meaningful result for us to understand the powerful generalization ability of deep convolutional neural networks. However, annotating all those terrible conditions very well is not a sensible selection. However, we can still explore why this is the case and how does CNN execute such well to push the meaningful research in those situations. There are numerous remote sensing resources to utilize and problems to solve. With more and more powerful operators and theories, hopefully, we can fast promote the development of real-time remote sensing systems in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We proposed R 2 -CNN, a unified and self-reinforced convolutional neural network under the end-to-end training framework, which joint the classifier and detector elegantly. The lightweight backbone Tiny-Net extracts the powerful features from inputs quickly, and the intermediate global attention block enlarges the receptive field to inhibit false positives. The classifier first predicts the existence of detection target in the current patch, and the specifically designed detector is followed to locate them accurately if available. The high recall and precision in GF-1 and GF-2 validate the effectiveness of our network. Specifically, we can process a GF-1 image in 29.4 s on Titian X just with single thread. All those experiments prove that our R 2 -CNN is efficient in both computation and memory consumption, robust to false positives, and strong to detect tiny objects.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. R 2 -CNN is a unified network working in a patchwise manner. Pipeline is shown on the right.</figDesc><graphic coords="1,313.07,166.85,249.02,85.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Architecture of our R 2 -CNN. (a) Tiny-Net, a lightweight residual structure which enables fast and powerful features extraction from inputs. (b) Classifier, which can speed up the unified network and avoid false alarm raised by massive backgrounds. (c) Detector, which can locate target objects accurately if available. The classifier and detector are mutually reinforced each other under the end-to-end training framework. In addition, global attention block is built on top of Tiny-Net to inhibit false positives.</figDesc><graphic coords="3,48.95,58.73,513.02,160.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc><ref type="bibr" target="#b3">[4]</ref> attempted to detect airports in large-scale images first to reduce overall airplane detection time, but the training and testing images are all from the region near airports without arbitrary massive backgrounds. For practical use, object detection in large-scale remote sensing images is very important and necessary. III. PROPOSED METHOD The R 2 -CNN, shown in Fig 2, consists of the backbone Tiny-Net, intermediate global attention block, and final classifier and detector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. (a) Whether they are airplanes or not? (b) Black area: theoretical receptive field. Green area: effective receptive field. Red area: bounding box. (c) Global attention block. (d) False positive's confidence drop obviously with global attention.</figDesc><graphic coords="5,81.47,252.53,205.70,114.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. (a) Few features: small objects have a few features at detection layer. (b) Anchor matching analysis: the figure is from S 3 FD [26], and tiny and outer objects match too little anchors. (c) Our skip-connection architecture for reducing the anchor stride by enlarging the feature map. (d) Data distribution: bounding-box scales of our training data set.</figDesc><graphic coords="6,328.43,241.01,206.30,154.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Max-Recall of different scales in Gray-test-dev. Besides, we evaluate the effectiveness of global attention block on GF1-test-dev. The results are shown in Table VII. When R 2 -CNN is implemented without global attention block, there are more false positives because of the limited receptive field. With the global attention block, the confidence of false positives drops obviously. We found that the recall drops a little with global attention block. Comparing to the enhancement of inhibiting false positives, this is a better model for practical engineering. 3) Strong: To validate the effectiveness of R 2 -CNN on tiny object detection, we evaluate our network on Rgb-test-dev and Gray-test-dev. The results are shown in Tables VIII and IX. How Important Is the Zoom-Out and Zoom-In Architecture? Considering the large memory needed by ResNet-50 with an anchor stride of 8 pixels, a detector with ResNet-50 is implemented with an anchor stride of 16 pixels. The results of stride-16 with R 2 -CNN only get 83.68 mAP in Gray-testdev. Besides, the mAP of different scales is shown in Fig 5.The performance of objects larger than 32 pixels is basically comparable to our method, but the results of small objects drop obviously without the architecture. In addition, we attempt to attach the global attention block to conv-4 directly. The results of No Conv-5 drop 2 points compared to our method. This</figDesc><graphic coords="9,48.47,252.89,252.02,189.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Results of our R 2 -CNN with a score threshold of 0.9. Two airplanes are flying back to airport in image a. Results of the airport are shown in image b. Two objects marked by the red circle in image b are false positives.</figDesc><graphic coords="10,49.43,58.85,513.14,435.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Results of our R 2 -CNN with a score threshold of 0.9. The area is under the heavy haze. Image a shows an airplane in the wild. Image b shows an airplane flying in sky. Image c shows an airport with some airplanes. Image d shows the results of airport. Image e shows the details of d. The ignored airplane marked by a red circle in image e has a confidence of 0.83.</figDesc><graphic coords="11,48.95,56.33,513.02,639.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I ARCHITECTURE</head><label>I</label><figDesc>OF OUR TINY-NET. EACH 3 × 3 BLOCK ARE RESIDUAL BLOCK IN RESNET [6] EXCEPT CONV-1</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II R 2 -</head><label>II2</label><figDesc>CNN's RESULTS IN GF1-Test-Dev. RESULTS ARE SHOWN IN UNIFIED/ UNUNIFIED/FULLY DETECTION TRAINING AND TESTING</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III R 2 -</head><label>III2</label><figDesc>CNN's RESULTS IN GF2-Test-Dev. RESULTS ARE SHOWN IN UNIFIED/UNUNIFIED/FULLY DETECTION TRAINING AND TESTING</figDesc><table><row><cell>TABLE IV</cell></row><row><cell>RESULTS OF CLASSIFIER ON GF1-Test-Dev. MEANS FINE-TUNING</cell></row><row><cell>FROM IMAGENET PRETRAINED MODEL. MEANS TRAINING</cell></row><row><cell>FROM STRETCH. -MEANS TRAINING</cell></row><row><cell>WITHOUT DETECTOR</cell></row><row><cell>TABLE V</cell></row><row><cell>RESULTS OF CLASSIFIER ON GF2-Test-Dev. MEANS FINE-TUNING</cell></row><row><cell>FROM IMAGENET PRETRAINED MODEL. MEANS TRAINING</cell></row><row><cell>FROM STRETCH. -MEANS TRAINING</cell></row><row><cell>WITHOUT DETECTOR</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE VI TIME</head><label>VI</label><figDesc>COSTS OF DIFFERENT METHODS WITH SINGLE THREAD</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE VII PERFORMANCE</head><label>VII</label><figDesc>WITH OR WITHOUT GLOBAL ATTENTION BLOCK sensing images, our network tackles the problem well with the proposed methods.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VIII RESULTS</head><label>VIII</label><figDesc>OF DETECTORS ON Rgb-test-dev. -MEANS TRAINING WITHOUT CLASSIFIER. + MEANS TRAINING WITH IMAGENET PRETRAINED MODEL</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IX RESULTS</head><label>IX</label><figDesc>OF DETECTORS ON Gray-Test-Dev. -MEANS TRAINING WITHOUT CLASSIFIER. + MEANS TRAINING WITH IMAGENET PRETRAINED MODEL</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>PANG et al.: R 2 -CNN: FAST TINY OBJECT DETECTION IN LARGE-SCALE REMOTE SENSING IMAGES</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by the Basic Research under Grant ID JCKY2018110C081.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>He is currently a Professor with the State Key Laboratory of Modern Optical Instrumentation, Zhejiang University. His research interests include imaging technique, image processing, precision testing technology, and optical system design.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Object detection in optical remote sensing images based on weakly supervised learning and high-level feature learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3325" to="3337" />
			<date type="published" when="2015-06">Jun. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Accurate object localization in remote sensing images based on convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2486" to="2498" />
			<date type="published" when="2017-05">May 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">VHR object detection based on structural feature extraction and query expansion</title>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="6508" to="6520" />
			<date type="published" when="2014-10">Oct. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weakly supervised learning based on coupled convolutional neural networks for aircraft detection</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="5553" to="5563" />
			<date type="published" when="2016-09">Sep. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rotation-invariant object detection of remotely sensed images based on texton forest and Hough voting</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1206" to="1217" />
			<date type="published" when="2012-04">Apr. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">R-FCN: Object detection via regionbased fully convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Elliptic Fourier transformationbased histograms of oriented gradients for rotationally invariant object detection in remote-sensing images</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="618" to="644" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A survey on object detection in optical remote sensing images</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="11" to="28" />
			<date type="published" when="2016-07">Jul. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1998-11">Nov. 1998</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Network in network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1312.4400" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1409.1556" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015-06">Jun. 2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">OverFeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1312.6229" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2014-06">Jun. 2014</date>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013-04">Apr. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Fast R-CNN</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1504.08083" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SSD: Single shot MultiBox detector</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017-07">Jul. 2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">S 3 FD: Single shot scale-invariant face detector</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1708.05237" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Finding tiny faces</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07">Jul. 2017</date>
			<biblScope unit="page" from="951" to="959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07">Jul. 2017</date>
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Transferring deep convolutional neural networks for the scene classification of high-resolution remote sensing imagery</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="14680" to="14707" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A hierarchical oil tank detector with deep surrounding features for high-resolution optical satellite imagery</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4895" to="4909" />
			<date type="published" when="2015-10">Oct. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Surface object recognition with CNN and SVM in Landsat 8 images</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nakada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mochizuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th IAPR Int. Conf. Mach. Vis. Appl. (MVA)</title>
		<meeting>14th IAPR Int. Conf. Mach. Vis. Appl. (MVA)</meeting>
		<imprint>
			<date type="published" when="2015-05">May 2015</date>
			<biblScope unit="page" from="341" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Convolutional neural network based automatic object detection on aerial images</title>
		<author>
			<persName><forename type="first">I</forename><surname>Ševo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Avramović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="740" to="744" />
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Detection of seals in remote sensing images using features extracted from deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A.-B</forename><surname>Salberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Geosci. Remote Sens. Symp. (IGARSS)</title>
		<meeting>IEEE Int. Geosci. Remote Sens. Symp. (IGARSS)</meeting>
		<imprint>
			<date type="published" when="2015-07">Jul. 2015</date>
			<biblScope unit="page" from="1893" to="1896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning rotation-invariant convolutional neural networks for object detection in VHR optical remote sensing images</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="7405" to="7415" />
			<date type="published" when="2016-12">Dec. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Orientation robust object detection in aerial images using deep convolutional neural network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process. (ICIP)</title>
		<meeting>IEEE Int. Conf. Image ess. (ICIP)</meeting>
		<imprint>
			<date type="published" when="2015-09">Sep. 2015</date>
			<biblScope unit="page" from="3735" to="3739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep neural networks-based vehicle detection in satellite images</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symp. Bioelectron. Bioinf. (ISBB)</title>
		<meeting>Int. Symp. Bioelectron. Bioinf. (ISBB)</meeting>
		<imprint>
			<date type="published" when="2015-10">Oct. 2015</date>
			<biblScope unit="page" from="184" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009-06">Jun. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Understanding the effective receptive field in deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4898" to="4906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The Pascal visual object classes (VOC) challenge</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2009-09">Sep. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd ACM Int. Conf. Multimedia</title>
		<meeting>22nd ACM Int. Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
