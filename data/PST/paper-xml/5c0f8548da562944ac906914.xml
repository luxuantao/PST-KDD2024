<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Real-Time Action Recognition with Deeply-Transferred Motion Vector CNNs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Limin</forename><surname>Wang</surname></persName>
							<email>07wanglimin@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Zhe</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Hanli</forename><surname>Wang</surname></persName>
							<email>hanliwang@tongji.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
							<email>yu.qiao@siat.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><surname>Wang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science &amp; Technology and Key Laboratory of Embedded System and Service Comput-ing</orgName>
								<orgName type="department" key="dep2">Ministry of Education</orgName>
								<orgName type="institution">Tongji University</orgName>
								<address>
									<postCode>200092</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<postCode>8092</postCode>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">Computational Vision Group</orgName>
								<orgName type="institution">University of California at Irvine</orgName>
								<address>
									<settlement>Irvine</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">Provincial Key Laboratory of Computer Vision and Virtual Reality Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Real-Time Action Recognition with Deeply-Transferred Motion Vector CNNs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">068612F4272843CAF26968CFD801216D</idno>
					<idno type="DOI">10.1109/TIP.2018.2791180</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2018.2791180, IEEE Transactions on Image Processing SUBMITTED TO IEEE TRANS. ON IMAGE PROCESSING 1 This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2018.2791180, IEEE Transactions on Image Processing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Action Recognition</term>
					<term>Motion Vector</term>
					<term>Knowledge Transfer</term>
					<term>Real-Time Processing</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The two-stream CNNs prove very successful for video based action recognition. However the classical two-stream CNNs are time costly, mainly due to the bottleneck of calculating optical flows. In this paper, we propose a two-stream based real-time action recognition approach by using motion vector to replace optical flow. Motion vectors are encoded in video stream and can be extracted directly without extra calculation. However directly training CNN with motion vectors degrades accuracy severely due to the noise and the lack of fine details in motion vectors. In order to relieve this problem, we propose four training strategies which leverage the knowledge learned from optical flow CNN to enhance the accuracy of motion vector CNN. Our insight is that motion vector and optical flow share inherent similar structures which allows us to transfer knowledge from one domain to another. To fully utilize the knowledge learned in optical flow domain, we develop deeply transferred motion vector CNN. Experimental results on various datasets show the effectiveness of our training strategies. Our approach is significantly faster than optical flow based approaches and achieves processing speed of 390.7 frames per second, surpassing real-time requirement. We release our model and code to facilitate further research. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Words framework <ref type="bibr" target="#b5">[6]</ref> and deep learning <ref type="bibr" target="#b0">[1]</ref>. Bag-of-Visual-Words framework (or its variants) is a stage-wise method, whose pipeline contains feature extraction, feature encoding, and classification. One popular method along this line is the improved-dense-trajectories (iDT) <ref type="bibr" target="#b1">[2]</ref> and Fisher Vector encoding <ref type="bibr" target="#b6">[7]</ref>. It uses trajectory-aligned hand-crafted features to represent action and achieves high accuracy on various datasets. Unlike these traditional methods, deep learning is an end-to-end framework. It takes a video as input and employs multiple-layer neural network as its architecture. The parameters of deep learning method are automatically tuned based on back propagation algorithm. Two-stream CNN is a successful architecture in this deep learning paradigm. Both RGB CNN and optical flow CNN are used to extract appearance and motion representation from videos, respectively. These representations are used to predict action classes from videos. Two-stream framework and its variants <ref type="bibr" target="#b7">[8]</ref> achieve the state-of-the-art accuracy on several large datasets like UCF101 <ref type="bibr" target="#b8">[9]</ref> and HMDB51 <ref type="bibr" target="#b9">[10]</ref>. The existing two-stream frameworks mainly rely on optical flow extraction to represent motion information. However the calculation of optical flow is time consuming, which prohibits the real-time preprocessing of two-stream based approaches even with GPU.</p><p>The main objective of this paper is to develop a realtime action recognition approach with high performance and accuracy. We utilize the successful two-stream framework <ref type="bibr" target="#b0">[1]</ref> as our basic architecture. It is non-trivial to speed up performance while still keep the recognition accuracy of twostream CNNs, because it requires optical flow as its input. Only performing prediction with RGB image leads to inferior recognition accuracy. However the calculation of optical flow is computationally expensive, for example it can only be conducted at the speed of 16.7 frames per second (fps) with K40 GPU <ref type="bibr" target="#b10">[11]</ref>, which is a bottleneck for real-time processing. To circumvent this problem, in this paper, motion vector as the input of CNN is introduced. Motion vector is encoded in the video stream during video compression phrase and can be directly extracted almost with no extra computational overhead.</p><p>Motion vector <ref type="bibr" target="#b11">[12]</ref> is originally proposed for video coding. It is designed to exploit the motion information of corresponding image blocks to reduce the bit rate of video. Research <ref type="bibr" target="#b12">[13]</ref> shows that motion vector can be used for action recognition. Similar to optical flow, motion vector contains local motion information. However, as it is not designed to reveal the motion as accurate as possible, motion vector contains noisy and imprecise movement information as shown in Fig. <ref type="figure">1</ref>. Directly using motion vector as input can degrade the accuracy Fig. <ref type="figure">1</ref>. Comparison of motion vector and optical flow in x and y components. We can see that motion vector contains lots of noisy movement information and it is much coarser than optical flow. It is clearly that the structures of bow and arrow are lost and the outline of human is blurred. severely.</p><p>To solve the possible issue of accuracy drop, our key idea in this paper is to transfer the knowledge from optical flow CNN to motion vector CNN to improve its generalization ability. Compared with optical flow images, motion vector images contain similar movement information. The main difference lies in the quality of motion. Motion vector contains coarse and noisy movement pattern, while optical flow contains more precise and clear one. Due to the high quality of optical flow images, optical flow CNN can learn elaborate and concise filters, while motion vector CNN only learn noisy filters which harms its recognition accuracy. This fact inspires us that the knowledge learned by optical flow CNN may be beneficial to motion vector CNN. To fully unleash the potential of motion vector CNN, we design an algorithm, called deeplyconnected transfer, to perform multi-layer knowledge transfer from optical flow CNN to motion vector CNN. It should be noted that optical flows are only used during training phrase for knowledge transfer. For testing, only motion vectors are used for real time action recognition. Experiments show that deeply-transferred motion vector CNN obtains a significant accuracy improvement over directly using motion vector as input, while still keeps the good merit for real-time processing speed of motion vectors.</p><p>The preliminary version is published in CVPR 2016 <ref type="bibr" target="#b13">[14]</ref> and we have extended it in two important ways. First, we propose a new training strategy to improve motion vector CNN's accuracy. Second, more extensive experiments are performed to verify the effectiveness of our new approach. In particular, we observe that the learning strategies proposed in <ref type="bibr" target="#b13">[14]</ref> are kinds of shallow supervision. During training, optical flow CNN's knowledge is only presented to the final layer of motion vector net. The knowledge that contained in middle layers of optical flow CNN is not directly exploited. In this paper, a new method called "Deeply Connected Transfer" is proposed to enable knowledge transfer between middle layers. Furthermore, extensive experiments are performed on the UCF101 <ref type="bibr" target="#b8">[9]</ref>, HMDB51 <ref type="bibr" target="#b9">[10]</ref>, and THUMOS14 <ref type="bibr" target="#b14">[15]</ref> datasets. These results verify that our newly proposed method can provide stronger supervision than <ref type="bibr" target="#b13">[14]</ref> and further improve the accuracy of motion vector CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Recent years have witnessed significant progress for action recognition. State-of-the-art methods can be roughly divided into two frameworks: the Bag-of-Visual-Words (BoVW) paradigm and deep learning approach.</p><p>BoVW framework is originally proposed for image classification <ref type="bibr" target="#b15">[16]</ref>. Further researches show its potential for video retrieval <ref type="bibr" target="#b5">[6]</ref> and action recognition <ref type="bibr" target="#b1">[2]</ref>. Typical BoVW framework consists of three steps: feature extraction, feature encoding, and classification. For feature extraction, Laptev et al. <ref type="bibr" target="#b16">[17]</ref> explored the spatial-temporal domain by extending harris corner detector to 3D. Wang et al. <ref type="bibr" target="#b17">[18]</ref> evaluated different combinations of detectors and descriptors, and showed that dense feature extraction can exhibit better accuracy than salient point based approach. Wang et al. <ref type="bibr" target="#b18">[19]</ref> further extended this idea by utilizing optical flow to track dense feature points in several continuous frames and employed several descriptors to extract feature along optical flow trajectories. Wang et al. <ref type="bibr" target="#b1">[2]</ref> discovered that camera motion can hamper accuracy for action recognition and exploited camera motion elimination method to further improve the accuracy. Popular descriptors for action recognition include HOG <ref type="bibr" target="#b19">[20]</ref>, HOF <ref type="bibr" target="#b16">[17]</ref>, MBH <ref type="bibr" target="#b20">[21]</ref>, and the recently proposed TDD <ref type="bibr" target="#b4">[5]</ref>. Unlike the previous hand-crafted features <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b20">[21]</ref>, TDD utilized deep neural network to extract features along trajectories and showed significant improvement on various datasets. For descriptor encoding, hard quantization <ref type="bibr" target="#b5">[6]</ref>, VLAD <ref type="bibr" target="#b21">[22]</ref> and Fisher Vector <ref type="bibr" target="#b6">[7]</ref> are among the popular methods for action recognition. Peng et al. <ref type="bibr" target="#b22">[23]</ref> conducted extensive experiments on various datasets to give practical advice on how to choose the optimal setup of feature encoding for action recognition.</p><p>Recent studies demonstrated that deep learning approaches can achieve superior accuracy on image classification <ref type="bibr" target="#b23">[24]</ref> and object detection <ref type="bibr" target="#b24">[25]</ref>, which inspires researchers to utilize CNN for action recognition task. Different from image classification, video based action recognition is in spatial-temporal domain, where motion information yields an important cue. One research line is to exploit contiguous frame relationship by stacking RGB images. Karpathy et al. <ref type="bibr" target="#b25">[26]</ref> first used CNN on stacked RGB images to learn motion patterns, and designed several temporal pooling method. Tran et al. <ref type="bibr" target="#b3">[4]</ref> proposed to use 3D convolution to directly extract motion relation in stacked RGB images, and showed good speed and accuracy on various datasets. Wang et al. <ref type="bibr" target="#b26">[27]</ref> proposed a new module, called as SMART, to directly model appearance and relation from RGB images in an explicit and separate way. Another research line is based on optical flow. Optical flow can directly unveil motion information by calculating the movement of corresponding points. One successful approach in this line is two-stream CNNs <ref type="bibr" target="#b0">[1]</ref>, where a two-stream net was developed to exploit appearance information and motion relation in RGB CNN and optical flow CNN, respectively. Two-stream CNN framework is used as a baseline for our study as it achieves high recognition accuracy. Wang et al. <ref type="bibr" target="#b7">[8]</ref> improved two stream framework by using temporal segments of video to train multiple snippet-level CNNs, which achieves state-ofthe-art accuracy on several datasets. Feichtenhofer et al. <ref type="bibr" target="#b27">[28]</ref> explored the fusion strategies of RGB CNN and optical flow CNN, which showed high accuracy on action recognition. Feichtenhofer et al. <ref type="bibr" target="#b28">[29]</ref> proposed ST-ResNet by combining ResNet <ref type="bibr" target="#b24">[25]</ref> with two-stream convnets. ST-ResNet achieved impressive results on UCF101 and HMDB51 datasets. Ng et al. <ref type="bibr" target="#b29">[30]</ref> introduced recurrent neural network (LSTM) to further exploit motion patterns in optical flow images. Wu et al. <ref type="bibr" target="#b30">[31]</ref> combined the merits of two-stream CNNs and LSTMs by fusing RGB net and optical flow net with a recurrent architecture.</p><p>Despite of improving accuracy, several approaches were proposed to accelerate the processing speed of deep neural networks. Knowledge distillation method <ref type="bibr" target="#b31">[32]</ref> was proposed to compress cumbersome net. Courbariaux et al. <ref type="bibr" target="#b32">[33]</ref> discovered that float computation in neural network is time consuming and binary number can be efficiently calculated by shift operations. <ref type="bibr" target="#b32">[33]</ref> proposed to binarize the activation of feed forward processing to accelerate neural network computation. Rastegari et al. <ref type="bibr" target="#b33">[34]</ref> further extended this idea by proposing XNOR-Net to achieve high accuracy on ImageNet datasets.</p><p>Several researches also aimed to improve processing speed of traditional methods on action recognition. For example, Kantorov et al. <ref type="bibr" target="#b12">[13]</ref> accelerated dense trajectory method <ref type="bibr" target="#b18">[19]</ref> by employing motion vector to replace optical flow. <ref type="bibr" target="#b12">[13]</ref> further used FLANN to substitute brute force search in Fisher Vector <ref type="bibr" target="#b6">[7]</ref> and VLAD <ref type="bibr" target="#b21">[22]</ref> to improves the speed.</p><p>Another thread of researches focused on leveraging privileged knowledge provided by teacher model to improve student model's performance. Vapnik et al. <ref type="bibr" target="#b34">[35]</ref> provided detailed investigations on two topics in learning using privileged information (LUPI): similarity control in LUPI paradigm and transfer knowledge from privileged knowledge space to decision space. They proposed SVM+ to implement LUPI algorithm. Lapin et al. <ref type="bibr" target="#b35">[36]</ref> showed the close relationship between weighted SVM and privileged information. Further, Lopez-Paz et al. <ref type="bibr" target="#b36">[37]</ref> unified two style of knowledge transfer, privileged information and knowledge distillation, into generalized distillation. They showed that distillation and privileged information can improve the results with respect to pure supervised learning. Li et al. <ref type="bibr" target="#b37">[38]</ref> proposed sparse multi-instance learning using privileged information (sMIL-PI) approach. sMIL-PI leverages textual features from tags and captions of web images as privileged knowledge to tackle noisy labeling problem in training data. Their work showed that using privileged information can improve the performance of image retrieval and image categorization tasks. Niu et al. <ref type="bibr" target="#b38">[39]</ref> showed that sMIL-PI can also achieve promising results for action recognition. The idea proposed in our work that using teacher network to improve student net's performance is similar to LUPI paradigm. However, unlike LUPI which is based on SVM framework, our approach implements knowledge transfer in deep learning domain.</p><p>Among these approaches, the most relevant works to us are <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b39">[40]</ref> and <ref type="bibr" target="#b31">[32]</ref>. Unlike <ref type="bibr" target="#b12">[13]</ref>, we use motion vector to improve speed in the deep learning framework. It is non-trivial to train a high accuracy network with motion vector. Our work is also related to knowledge distillation <ref type="bibr" target="#b31">[32]</ref> in spirit. Unlike <ref type="bibr" target="#b31">[32]</ref>, however, our aim is not to transfer knowledge in the same domain between two different structures but to transfer knowledge between two different domains with the same structure. We further design several strategies to enhance knowledge transfer from optical flow domain to motion vectors. Unlike <ref type="bibr" target="#b39">[40]</ref> that only using the output of one layer as supervision signal, we fully utilize the multiple middle representations to provide deeply knowledge transfer. The soft labels of teacher networks have been also employed in other computer vision tasks for different objectives, such as class disambiguation in large-scale scene recognition <ref type="bibr" target="#b40">[41]</ref> and knowledge transfer in CNN fine tuning for event recognition <ref type="bibr" target="#b41">[42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MOTION VECTOR FOR DEEP ACTION RECOGNITION</head><p>Two-stream CNNs <ref type="bibr" target="#b0">[1]</ref> consists of two parts, RGB CNN and optical flow CNN, to achieve state-of-the-art accuracy on various datasets. RGB CNN can be conducted in short time with GPU. However, the optical flow part is computational expensive and cannot satisfy real-time preprocessing requirement. Optical flow CNNs take frames of optical flow as input. It first needs to extract optical flow images from video. Then these images are processed with a CNN. Although the feed forward process of CNNs can be conducted at fast speed (around 300ms for 250 frames with center crop) with GPU, the extraction of optical flow is relatively slow. For example, with Farneback's method <ref type="bibr" target="#b42">[43]</ref>, calculating optical flow needs 360ms per frame on CPU with efficient implementation. Even with GPU, optical flow (Brox's flow <ref type="bibr" target="#b10">[11]</ref>) can only be extracted around 60ms per frame, which is still far from the requirement of real-time processing. Thus, the calculation of optical flow is one of the main bottlenecks that lower the processing speed of two-stream CNNs.</p><p>As the optical flow CNN is an important part and has a large contribution to the accuracy of two-stream CNNs. Directly processing video with only RGB CNN degrades the recognition accuracy severely. Here, we follow the two-stream architecture and propose motion vector CNNs to extract the motion pattern of videos, instead of using the computationally expensive optical flows.</p><p>Motion vector is similar to optical flow. Both are twodimension vectors to describe the movement information of corresponding pixels in two continuous frames. Unlike optical flow, motion vector is widely used in various video coding standards (H.264 <ref type="bibr" target="#b11">[12]</ref>, HEVC <ref type="bibr" target="#b43">[44]</ref> and etc.). It is available in compressed video stream and can be obtained directly with almost no computational cost. This property makes motion vector an attractive substitution for optical flow to achieve efficient action analysis. Early work <ref type="bibr" target="#b12">[13]</ref> had demonstrated the usefulness of motion vector for action recognition. They purposed to use motion vector to form trajectories and then used VLAD and Fisher vector with efficient implementation to describe videos for action classification. Different to this work, we explore motion vector in deep neural network framework. The main difficulty comes from the noise and the block-wise imprecise motion information exhibited by motion vector images. As shown in our experiments, directly training CNN with motion vectors from scratch will largely harm the recognition accuracy.</p><p>To tackle this problem, several training methods are proposed to transfer the knowledge learned by optical flow CNN to motion vector CNN. Here, our insight is that both motion vector and optical flow contain the movement information. Furthermore, the knowledge learned by optical flow CNN and motion vector CNN are correlated. Since optical flow is more precise and optical flow CNN can learn more elaborate filters, we may leverage optical flow CNN as a teacher net to improve the accuracy of motion vector CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Motion Vector</head><p>In this subsection, we give a brief description of motion vector and explain why it is hard to train motion vector CNN with high accuracy.</p><p>Motion vector is originally designed for video coding. In video coding, the main goal is to reduce the spatial and temporal redundancy within several continuous frames. Motion vectors describe the movement information of corresponding blocks in two frames, which yields an ideal cue to exploit the temporal redundancy in two frames. Thus motion vector is widely implemented in various video coding standards like H.264, MPEG, HEVC, and etc. For action recognition, as motion vector is already calculated in video coding phase and contains motion, it can provide effective motion information for action recognition with high efficiency.</p><p>However, training motion vector CNN with high accuracy is challenging. Firstly, motion vector only provides block wise movement information. In video coding, macro block is the basic coding unit, which has size ranging from 8×8 to 16×16. Different with the pixel level motion information provided by optical flow, motion vector only yields macro block level information. This property causes the coarse structure of motion vector, as Fig. <ref type="figure">1</ref>. It also poses difficulty for using motion vector CNN to learn fine-grained action information, which further degrades the accuracy of motion vector CNN severely.</p><p>Furthermore, motion vectors contain noisy motion information, which made it difficult to learn high accuracy motion vector CNN. As stated before, motion vector needs to meet the balance between the speed of encoding and the bit rate of video. It is calculated based on three or four steps of blockwise comparison. Thus, motion vectors fail to provide precise motion information. It can be clearly seen from Fig. <ref type="figure">1</ref>, unlike the clear background part of optical flow image, noise patterns exist in motion vector image. This property hampers motion vector CNN to achieve high accuracy.</p><p>Motion vectors may not exist in all frames. It is calculated based on reference frames. In order to achieve the balance of video quality and the compression rate, images in video coding are grouped into group-of-pictures (GOP). One typical GOP contains intra-coded frame (I-frame), predictive frame (P-frame), and bi-predictive frame (B-frame). As the name indicated, I-frame is coded based on itself which indicates that it contains no motion vector. P-frame and B-frame are coded based on other reference frames, which means that they both contain movement information. Empty I-frame poses difficulties in training high accuracy CNN. Our strategy to this is to use previous frame's motion vector to replace empty Iframe. We find this simple strategy works well in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Real-time Action Recognition Frameworks</head><p>The proposed real-time action recognition framework contains two components: video decoder and two-stream architecture CNNs. The video decoder takes compressed video as input and directly gets RGB and motion vector images during decoding phrase. Then RGB and motion vector images are fed into two-stream CNNs to get the action prediction of this video. The main difference between our proposed real-time action recognition framework with two-stream CNNs is that our method doesn't require optical flow computation during deploying. Motion vector CNN (MV-CNN) and RGB CNN are utilized in our framework to extract high level motion and appearance representations, respectively. Thus, the most timeconsuming part, optical flow calculation, is avoided in our real-time action recognition framework.</p><p>In training phase, RGB images and motion vector are firstly extracted from video. The video's label is assigned to each frame. Data augmentation is important for training CNN. Random cropping and random scale jitter in spatial domain are employed to get a patch from image and motion vector.</p><p>In testing phases, videos are firstly decomposed into raw images and motion vectors. RGB CNN and MV-CNN are then employed to take images and motion vectors as input. The final action prediction of video is determined by the weighted average of two CNN's prediction scores. Weights are set as 1 and 2 for spatial CNN and temporal CNN respectively.</p><p>For fair comparison with two stream CNNs <ref type="bibr" target="#b0">[1]</ref>, ClarifaiNet <ref type="bibr" target="#b0">[1]</ref> is used as the basic architecture for spatial and temporal CNN. Following <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b0">[1]</ref>, dropout ratios are set to 0.5 for spatial net to avoid over-fitting. For temporal net, dropout ratios are set to 0.9 and 0.8 for FC6 and FC7 respectively. Our spatial net is pre-trained on ImageNet ILSVRC-2012 dataset and then fine-tuned on action dataset. The learning rate for spatial net is firstly set to 10 -3 and then drop to 10 -4 after 14k iterations. The whole training process terminates at 20k steps. As HMDB51 dataset is relatively smaller than UCF101 and THUMOS14 dataset is larger than UCF101, we stops the training process at 10k steps and 50k steps, respectively.</p><p>For temporal net, we slightly modify the ReLU layers of original ClarifaiNet to PReLU layers, as it leads to better results and accelerates convergence. 10 frames of motion vectors is stacked as input for temporal net. Learning rate for temporal CNN starts from 10 -2 and then decreases to 10 -3 after 30k iterations. Learning rate further drops to 10 -4 at 70k steps. The training stops at 90k iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DEEPLY-TRANSFERRED MOTION VECTOR CNNS</head><p>As motion vectors only contain block level details, and suffer from noisy and imprecise motion information. It is challenging to train motion vector CNN with high accuracy. Experiments show that directly using motion vector to replace optical flow will lead to 7%, 10% and 26% accuracy degradation on UCF101 split1, HMDB split1 and THUMOS14 datasets, respectively. Our aim is to achieve the real-time processing merit of motion vector as well the high recognition accuracy as optical flow. Although motion vector and optical flow are calculated in different domains, they both contain similar motion information. Inspired by this fact, we design several methods to leverage the rich and fine-grained features that learned by optical flow CNN to improve motion vector CNN. These methods can be seen as transfer the knowledge learned in optical domain flow to that in motion vector domain.</p><p>For training, as in Fig. <ref type="figure" target="#fig_1">3</ref>(b), optical flow CNN is employed as teacher network to transfer knowledge to the student network: motion vector CNN. For the testing phrase, only motion vector CNN is used as temporal net to process the video. Optical flow images and optical flow CNN need not be calculated in testing. Thus, in testing, our proposed knowledge transfer strategy will not influence the testing speed of our action recognition system.</p><p>To implement the idea described above, four different strategies are proposed to transfer knowledge from OF-CNN to MV-CNN. We first introduce several notations. For teacher net in optical flow domain, the parameters are defined as T p = {T 1 p , T 2 p , ..., T n p }, where T n p stands for parameters of the n-th layer of teacher network and n represents the total number of layers. Parameters for student net in motion vector domain are denoted by S p = {S 1 p , S 2 p , ..., S n p }. For simplicity, we assume that the motion vector CNN has the same structure as optical flow CNN. Our method can also be extended to other structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Teacher Initialization</head><p>Extensive works <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b7">[8]</ref> in image and action classification show that initializing network with a pre-trained model on ImageNet can improve the accuracy and accelerate convergence. This fact inspires us to find an appropriate pretrained model for motion vector CNN. Although motion vector and optical flow are from different domains, they are inherently correlated as they both contain similar motion information of local patches. Thus, optical flow CNN is used as a pre-trained model for motion vector CNNs. More specifically, shown in Fig. <ref type="figure" target="#fig_1">3</ref>(a), we use optical flow CNN (OF-CNN) to initialize the parameters of motion vector CNN (MV-CNN),</p><formula xml:id="formula_0">S t p = T t p , t = 1, ..., n.<label>(1)</label></formula><p>Then, motion vector images are used to fine-tune motion vector CNN until convergence. Teacher initialization strategy directly provides MV-CNN a good start point for training with knowledge of detailed motion information. It can be seen as MV-CNN starts to train on fine optical flow features and then learns by itself. For implementation details, we first set the learning rate as 10 -3 , and then decrease it to 10 -4 and 10 -5 at 30k and 70k steps respectively. The training stops at 90k iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Supervision Transfer</head><p>Teacher initialization provides student network a good starting point for training with pre-learned network parameters. However, teacher network is not involved in training process. Student network MV-CNN is only trained with motion vector samples. As the inaccurate and coarse nature of motion vectors, the fine motion features provided by the teacher initialization may be diminished during the fine-tuning process. To tackle this problem, as in Fig. <ref type="figure" target="#fig_1">3 (b)</ref>, we propose to include additional supervision signal by using both ground truth label and teacher network to teach student net during training. Thus, MV-CNN can learn from OF-CNN during the whole training process. Here the last full connection layer of OF-CNN is employed as a new supervision signal for MV-CNN.</p><p>The technique in supervision transfer is similar to Hinton et al.'s work on dark knowledge <ref type="bibr" target="#b31">[32]</ref>. However our aims are different. Hinton's work mainly focuses on how to compress a large network to a small one with similar accuracy. Inputs of two networks are in the same domain. But the structures for cumbersome network and small network are different. In our problem, the input for teacher and student net are different (optical flow vs. motion vector). The main barrier needs to be tackled is how to improve the accuracy of student net with low quality input. Thus, we do not need to compress a network. Our aim is to use teacher network to improve the accuracy of the student with the same structure.</p><p>For a given frame I, the optical flow and motion vector are defined as o and v respectively. The output of the last fully connected (FC) layer of teacher CNN and student CNN are calculated as: T n (o) = softmax(T n-1 (o)), and S n (v) = softmax(S n-1 (v)), respectively, where 'softmax' function is employed to generate a probability score of multiple classes from the FC feature.</p><p>For transferring knowledge from OF-CNN to MV-CNN, the difference between student's and teacher's output need to be measured. Inspired by Hinton's work <ref type="bibr" target="#b31">[32]</ref>, a teacher supervision loss function is introduced. We utilize a temperature parameter T emp to soften both teacher's and student's output to ease the learning difficulty. The softmax output of teacher net is softened as P T = softmax(T n-1 /T emp), Similarly, the student net's softmax output is defined as P S = softmax(S n-1 /T emp), we use cross-entropy function to define the teacher supervision loss (TSL):</p><formula xml:id="formula_1">L T SL = - k i=1 P T (i) log P S (i),<label>(2)</label></formula><p>where k is the dimension of student and teacher's output (number of categories).</p><p>In spite of teacher supervision loss, the cross entropy between student's output S n and the ground truth Q is still needed to be minimized. Thus, the ground truth loss (GT) is defined by,</p><formula xml:id="formula_2">L GT = - i 1[Q = i] log S n (i),<label>(3)</label></formula><p>where S n and Q represent the unsoftened student net's softmax vectors and the ground truth label respectively. TSL loss (Eq.2) and GT loss (Eq.3) are combined to form the final loss:</p><formula xml:id="formula_3">L = L T SL + w • L GT (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>where w is a weight to balance these two terms. As suggested in <ref type="bibr" target="#b31">[32]</ref>, during training, the weight w for TSL and GT loss is set as T emp 2 to balance gradients of these two losses. The parameters of teacher network is frozen. Only parameters of student net is updated by the supervision of teacher net and ground truth label.</p><p>For implementation details, learning rate starts from 10 -3 and then decays to 10 -4 at 50k and 10 -5 at 70k steps. The whole training procedure terminates at 90k iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Combination</head><p>In third strategy, we want to combine the merits of both teacher initialization and supervision transfer. For the combination strategy, the teacher's parameters are firstly copied to student's net. Then both teacher supervision loss Eq.2 and ground truth loss Eq.3 are employed to transfer the knowledge from OF-CNN to MV-CNN. In this way, the pre-trained model on optical flow can provide MV-CNN a good start point for learning. Furthermore, MV-CNN can still receive supervision from OF-CNN during training by mimicking the output of OF-CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Deeply Connected Transfer</head><p>We have introduced three strategies in the above sections, namely Teacher Initialization, Supervision Transfer, and their combination. These training methods leverage OF-CNN's knowledge to improve MV-CNN's recognition accuracy. Here, we argue that these three strategies are still kinds of weak knowledge transfer from OF-CNN to MV-CNN. For teacher initialization, knowledge transfer only occurs in the start stage of training. Then MV-CNN will only rely on ground truth label to tune its parameters. For supervision transfer, only the final layer of OF-CNN and MV-CNN is connected. Knowledge that transferred from OF-CNN to MV-CNN is thus very limited, which may impede the performance of MV-CNN. To relieve this shortage, we propose a new strategy, coined as the Deeply Connected Transfer, to perform knowledge transfer in midlevel layers. In this deeply connected transfer approach, we force the feature representation of middle layers of OF-CNN and MV-CNN to be as similar as possible. The insight behind this is that in addition to the final FC layer, middle layers of OF-CNN may also contain useful knowledge that is worth transferring.</p><p>In spite of minimizing the difference between S n (v) and T n (o) as small as possible, we hope each layer output of student net S t (v) can approximate the output of the corresponding layer in teacher's net T t (o), where t represents the ordinal number of each layer. Unlike in the supervision transfer situation that the divergence between two softmax outputs (possibilities) can be measured by cross entropy, we utilize L2 distance to represent the variance between two feature maps. Thus, deeply connected transfer can be seen as a stronger knowledge-transfer strategy than Supervision Transfer. The structure of DTMV-CNN is shown in Figure <ref type="figure" target="#fig_2">4</ref>.</p><p>L2 loss is utilized to measure the deeply connected supervision loss (DCSL). Thus, DCSL can be defined as:</p><formula xml:id="formula_5">L DCSL = n-1 t=st L t 2 (S t (v), T t (o)),<label>(5)</label></formula><p>where st and n stand for ordinal number for one middle layer and the total number of layers respectively. L 2 is the L2 loss function,</p><formula xml:id="formula_6">L t 2 = 1 k k n=1 (S t k (v) -T t k (o)) 2 , (<label>6</label></formula><formula xml:id="formula_7">)</formula><p>where k is the dimension for layer t of student and teacher net and t is the ordinal number for one specific layer.</p><p>As each layer of teacher net provides supervision for student net MV-CNN, deeply connected supervision transfer strategy can fully unleash the supervision ability of teacher net OF-CNN.</p><p>For implementation details, we set learning rate to 10 -3 . The learning rate is reduced to 10 -4 and 10 -5 after 50k and 70k iterations respectively. We terminate the learning process at 90k iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>In experiment part, we firstly introduce the datasets used in our experiments and then analyze the experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets and Evaluation Protocol</head><p>Three datasets are employed to evaluate our proposed real-time action recognition algorithm: UCF101 <ref type="bibr" target="#b8">[9]</ref>, HMDB51 <ref type="bibr" target="#b9">[10]</ref>, and THUMOS14 <ref type="bibr" target="#b14">[15]</ref>. UCF101 contains 13,320 videos which are divided into three splits for training and testing. We follow the standard setup for three splits and report mean accuracy over three splits.</p><p>HMDB51 dataset is among the largest dataset for action recognition. It contains around 7,000 video clips. These clips are split into three sub-datasets. Each contains 3,570 and 1,530 clips for training and testing, respectively. The standard setup for HMDB51 is used and mean accuracies over three splits are reported.</p><p>THUMOS14 is a dataset with untrimmed videos. It is originally proposed for action recognition challenge 2014. 13,320 trimmed videos are for training, while another 1,010 and 1,574 untrimmed videos are for validation and testing. Untrimmed videos contain lots of irrelevant frames that make it more challenging in training and testing CNNs. Following <ref type="bibr" target="#b46">[47]</ref>, both training set and validation set are used for training. We use the official evaluation tool to evaluate performance. According to the standard setup of this dataset, mean Average Precision (mAP) is reported.</p><p>For the speed evaluation, the speed is reported as frames per second (fps) on a CPU (E5-2640 v3) and a K40 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>Three data augmentation strategies are used to learn robust CNN features. A 224 × 224 patch is randomly cropped from training image set. And then random horizontally flipping is implemented to augment training data. Furthermore, following <ref type="bibr" target="#b44">[45]</ref> <ref type="foot" target="#foot_0">2</ref> , a scale jittering strategy is implemented to help CNN to learn robust features. We set scale ratio for 1, 0.875, and 0.75 to yield a patch of size for 256, 224 and 192, respectively. Then these patches are resized to 224 × 224. In testing phase, one 224 × 224 patch cropped from the center of input image are used for evaluation. No data augmentation strategy is used in testing phase.</p><p>As HMDB51 is relatively small compared with UCF101 and THUMOS14, we use multi-task learning strategy to train temporal model on HMDB51. Following <ref type="bibr" target="#b0">[1]</ref>, a CNN is modified to have two separated input layers and two output layers. One is for UCF101 to calculate the loss with ground truth label, while the other is for HMDB51. Furthermore, as the motion vector for the original video in HMDB51 is relatively noisy, we follow <ref type="bibr" target="#b12">[13]</ref> to first use ffmpeg to re-encode videos and then extract motion vectors. For other datasets like UCF101 and THUMOS14, we directly extract motion  vectors from the original version of videos. Our teacher CNN is trained on TV-L1 optical flow <ref type="bibr" target="#b47">[48]</ref> with data augmentation that achieves 81.6% on UCF101 Split1, comparable with the accuracy in the original paper 81.2% <ref type="bibr" target="#b0">[1]</ref>. For HMDB51 split1, our teacher CNN achieves 60.0%, which is better than 55.4% on the original two-stream implementation <ref type="bibr" target="#b0">[1]</ref>. This significant improvement can be ascribed to the scale jittering strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Parameter Sensitivity</head><p>We first analyze the parameters for Supervision Transfer method. There are two important parameters existing in Supervision Transfer: temperature T emp and weight w for soft target. As suggested in <ref type="bibr" target="#b31">[32]</ref>, soft target weights are set as w = T emp 2 to balance the gradients between two targets. We set temperature T emp to 1, 2, and 3 and evaluate them on UCF101 split1. Thus the corresponding soft target weight w is set as 1, 4 and 9. As temperature goes up from 1 to 2, the corresponding accuracy grows up from 79.1% to 79.2%. The accuracy slightly degrade to 79.0% if we set temperature as 3. We can find that accuracies between different temperatures are relatively close, which implies Supervision Transfer strategy is robust for temperature setting. As T emp = 2 achieves the best accuracy, we set temperature and weight to 2 and 4 for the following experiment.</p><p>Second, we evaluate the accuracy of Deeply Connected Transfer methods and conduct experiments to analyze which layers should be connected for knowledge transfer. Greedy search method is employed to identify the best connection strategy for deeply-transferred motion vector (DTMV). For DTMV, Supervision Transfer and Teacher Initialization are always used. Thus FC8 is connected to TSL loss. Connection for DTMV starts from FC7 layer. The greedy search will be stopped if current connection strategy gets worse accuracy than previous strategy. From Table <ref type="table" target="#tab_0">I</ref>, we can observe that the accuracy goes up from 79.3% to 80.3% by connecting FC7 layer to Conv3 layer. As Conv2-FC8 performs slightly worse than Conv3-FC8, we set connection strategy to Conv3-FC8 for further experiments. For HMDB51 and THUMOS14, we use the same temperature and connection strategy setting as UCF101.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Evaluation of MV-CNNs, EMV-CNNs and DTMV-CNNs</head><p>This subsection compares and analyzes different knowledge transfer strategies through experiments on UCF101, HMDB51, and THUMOS14. The results are summarized in Table <ref type="table" target="#tab_1">II</ref>, Table <ref type="table" target="#tab_2">III</ref>, and Table <ref type="table" target="#tab_3">IV</ref>. We re-implement two-stream CNNs on this dataset, as <ref type="bibr" target="#b0">[1]</ref> did not provide results on THUMOS14.</p><p>First, comparing the accuracy of MV-CNN trained from scratch and OF-CNN, we can observe that directly replacing  optical flow with motion vector severely degrades the temporal net's accuracy by around 7%, 10% and 25% on UCF101 Split1, HDMB51 Split1 and THUMOS14, respectively. It verifies the fact that block wise motion structure, noisy motion blocks and inaccuracy movement information can severely harm the accuracy of temporal net. Furthermore, we observe that the accuracy gap between MV-CNN and OF-CNN is extremely large in THUMOS14 dataset. As lots of video in THUMOS14 is untrimmed, they contains lots of shots shift and a large number of irrelevant frames which aggravate the difficulties of training MV-CNN. Second, we can see a significant improvement from MV-CNN to DTMV-CNN for around 6%, 8%, and 14% on UCF101 Split1, HMDB51 Split1, and THUMOS14 respectively, which shows the effectiveness of our proposed deeply connected transfer method. Directly training MV-CNN with ground truth label from scratch lacks elaborate fine-level motion knowledge. Our proposed method shows that although motion vector and optical flow are from different domains, the knowledge of OF-CNN can still be helpful to MV-CNN.</p><p>In the next, we study the accuracy of our proposed strategies on UCF101 Split1. For fair comparison with the two stream ConvNets <ref type="bibr" target="#b0">[1]</ref>, we use data augmentation strategies for testing. Following <ref type="bibr" target="#b0">[1]</ref>, we use five image crops with the size of 224 × 224 from 4 corners and the center of a image. We feed forward these crops with their flipped version into CNN. We average these ten image crops to get the final result of the image. From Table <ref type="table" target="#tab_1">II</ref>, supervised transfer and teacher initialization outperforms MV-CNN for 3.1% and 3.8%, respectively. Similar to researches in image classification <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b0">[1]</ref>, teacher initialization can improve the accuracy of MV-CNN by providing finegrained knowledge through a pre-trained model. Fine-grained motion knowledge provided by OF-CNN is helpful for training MV-CNN. Furthermore, combining supervision transfer and teacher initialization can further improve the results of teacher initialization about 1.1% on UCF101 Split1, which shows that the softened softmax vector provided by supervision transfer can give richer information than ground truth label solely. As indicated in <ref type="bibr" target="#b31">[32]</ref>, the softened output of teacher net can be seen as a regulator for MV-CNN to prevent over-fitting. Thus supervision transfer can help to train better MV-CNN. We observe that deeply-transferred motion vector (DTMV-CNN) can further boost the results of EMV-CNN for 1%, 1.8% and 2% on UCF101, HMDB51, and THUMOS14 respectively. It indicates that knowledge contained in each layer of OF-CNN is useful for MV-CNN to enhance its generalization ability and only using the final fully connection layer (FC8) as supervision can not fully utilize the knowledge of OF-CNN. We noticed that DTMV with TI+DC and ST+TI+DC achieves similar performance. They all surpass the performance of EMV-CNN, which indicate that, for knowledge transfer, deeply connected transfer can provide much stronger supervision to MV-CNN than supervision transfer.</p><p>We also show the accuracy of DTMV-CNN with 10 crops and the one with center crop on UCF101 Split1 in Table <ref type="table" target="#tab_1">II</ref>. DTMV-CNN with center crop shows similar performance with the one with 10 crops. Thus in the HMDB51 and THUMOS14 dataset, only one 224 × 224 patch cropped from the center of input image for testing.</p><p>Furthermore, we can observe that combining temporal net DTMV-CNN and spatial net can outperform MV-CNN with RGB net. It indicates that the knowledge of DTMV-CNN is more complementary to spatial net than MV-CNN.</p><p>Finally, we analyze each category accuracy of MV-CNNs, EMV-CNNs and DTMV-CNNs on UCF101 Split1. According to the class category in <ref type="bibr" target="#b8">[9]</ref>, videos in UCF101 can be classified into 101 different classes. These classes can be categorized as 4 major categories: Human-Human action, Human-Object action, Human-Instrument action, and Human-Sports action. The comparison for accuracy on each category can be seen from Fig. <ref type="figure" target="#fig_4">6</ref>. EMV-CNN shows significant improvement over MV-CNN by around 4%, 6%, and 4% on categories of Human-Human action, Human-Object action and Human-Sports action, respectively. It indicates that the strategies of transferring knowledge from optical flow CNN to motion vector CNN can improve the motion vector CNN's performance. However for the Human-Instrument category, EMV-CNN shows similar accuracy with MV-CNN. As videos in Human-Instrument category require detail motion information, similar to MV-CNN, EMV-CNN may still lack knowledge for fine-grained motion. By using the proposed deeply connected transfer method, DTMV-CNN further improves the results of EMV-CNN on categories for Human-Human action and Human-Instrument action by 2% and 4%, respectively. It shows that the purposed method transfers more knowledge of detail motion to DTMV-CNN than EMV-CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Confusion Matrix for DTMV-CNN</head><p>We show the confusion matrix for DTMV-CNN in Fig.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Speed Evaluation</head><p>We analyze the speed of different components of our proposed approach. In our implementation, motion vector and RGB image are extracted with CPU, while GPU is utilized to process the feed forward calculation of RGB CNN and MV-CNN. As the I/O is related to hardware and operating system, the computation time reported doesn't include I/O. However time for video reading and decompression is still included in processing time. A volume with 10 frames of motion vector images and one RGB image is processed at each time. The speed is measured based on the time cost on each frame instead of each volume.</p><p>First, speeds of each component on UCF101, HMDB51, and THUMOS14 are shown in Table <ref type="table" target="#tab_5">V</ref>. We evaluate the speeds for different spatial resolutions. For UCF101 and HMDB51, the spatial resolution is 320 × 240, while THUMOS14's video has a resolution of 320 × 180. The calculation of our approach includes video reading and decoding, extraction for motion vector and RGB, and CNN processing. Around 57% time is used for motion vector and RGB image extraction with CPU. The rest of time is occupied by the feed forward computation of RGB-CNN and MV-CNN on GPU. As GPU is optimized for matrix multiplication, feed forward calculating of CNN is slightly faster than extracting motion vector and RGB images from video. We can observe that the total processing speed for UCF101, HMDB51, and THUMOS14 is 390.7, 390.7 and 403.2 fps, respectively, which is one order faster than real-time processing requirement (25 fps).</p><p>Second, speeds for motion vectors and optical flow (Brox's flow) extraction are compared in Table <ref type="table" target="#tab_5">VI</ref>. Although the processing speed of motion vector is slight different under Accuracy FPS MV+FV (CPU) (re-implement) <ref type="bibr" target="#b12">[13]</ref> 78.5% 132.8 C3D (1 net) (GPU) <ref type="bibr" target="#b3">[4]</ref> 82.3% 313.9 C3D (3 net) (GPU) <ref type="bibr" target="#b3">[4]</ref> 85.2% -iDT+FV (CPU) <ref type="bibr" target="#b1">[2]</ref> 85.9% 2.1 Two-stream CNNs (GPU) <ref type="bibr" target="#b0">[1]</ref> 88.0% 14.3 EMV+RGB-CNN (GPU) <ref type="bibr" target="#b13">[14]</ref> 86.4% 390.7 TSN (GPU) <ref type="bibr" target="#b7">[8]</ref> 94.0% &lt; 25 DTMV+RGB-CNN 87.5% 390.7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE VIII COMPARISON OF SPEED AND ACCURACY WITH STATE-OF-THE-ART ON</head><p>HMDB51.</p><p>Accuracy FPS MV+FV (CPU) <ref type="bibr" target="#b12">[13]</ref> 46.7% 101.0 MV+VLAD (CPU) <ref type="bibr" target="#b12">[13]</ref> 46.3% 227.8 iDT+FV (CPU) <ref type="bibr" target="#b1">[2]</ref> 57.2% 2.1 Two-stream CNNs (GPU) <ref type="bibr" target="#b0">[1]</ref> 59.4% 14.3 TSN (GPU) <ref type="bibr" target="#b7">[8]</ref> 68.5% </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Comparison with the State of the Art</head><p>We compare our proposed method with several state-of-theart methods in this subsection. SVM is employed in most stateof-the-art approaches <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b12">[13]</ref> for classification. Different to these step-wise methods, our proposed real-time action recognition method is an end-to-end approach.</p><p>First, speed and accuracy performance on UCF101 (3 Splits) are analyzed. As <ref type="bibr" target="#b12">[13]</ref> did not report accuracy on this dataset, we re-implement their methods using the public code offered by <ref type="bibr" target="#b12">[13]</ref>. From Table <ref type="table" target="#tab_6">VII</ref>, we can observe that DTMV+RGB-CNN is around 3 times faster than previous action recognition research on motion vector <ref type="bibr" target="#b12">[13]</ref>. Although it may not be a fair comparison as MV+FV only uses CPU, DTMV+RGB-CNN still outperforms MV+FV for 9%. Compared with optical flow based approaches <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b7">[8]</ref>, our approach is 180 times, 27 Accuracy FPS Objects (GPU) <ref type="bibr" target="#b46">[47]</ref> 44.7% -iDT+CNN (CPU+GPU) <ref type="bibr" target="#b48">[49]</ref> 62.0% &lt; 2.38 Motion (iDT+FV) (CPU) <ref type="bibr" target="#b46">[47]</ref> 63.1% 2.38 Objects+Motion (CPU+GPU) <ref type="bibr" target="#b46">[47]</ref> 71.6% &lt; 2.38 UntrimmedNet (GPU) <ref type="bibr" target="#b49">[50]</ref> 82.2% &lt; 25 TSN (GPU) <ref type="bibr" target="#b50">[51]</ref> 80.1% &lt; 25 EMV+RGB-CNN (GPU) <ref type="bibr" target="#b13">[14]</ref> 61.5% 403.2 DTMV+RGB-CNN 62.1% 403.2  times, and 16 times faster than iDT+FV, classical Two-stream CNNs, and TSN respectively. Our approach also achieves higher performance than iDT+FV and achieves similar performance with Two-stream CNNs. Although TSN achieves impressive results on UCF101 dataset, it is based on optical flow. Thus, TSN cannot be conducted in real time. We also compare our approach with RGB-based algorithm <ref type="bibr" target="#b3">[4]</ref>. Our approach achieves better accuracy and faster processing speed than C3D (1 net) and C3D (3 net).</p><p>Second, we compare the performance on HMDB51 (3 Splits). Our method significantly outperforms other motion vector based algorithm <ref type="bibr" target="#b12">[13]</ref> by around 9%. At the same time, our approach is 100 fps faster than MV+VLAD. Compared with optical flow performance, our approach shows slightly worse accuracy than iDT <ref type="bibr" target="#b1">[2]</ref> and classical two-stream CNNs <ref type="bibr" target="#b0">[1]</ref>. TSN achieves superior performance than us. However, as these methods requires optical flow as input, our method is around 300 fps much faster than iDT, two-stream CNNs, and TSN.</p><p>Finally, results on THUMOS14 dataset (Table <ref type="table" target="#tab_0">IX</ref>) are compared. Our approach achieves better performance than RGB based approach (Objects) <ref type="bibr" target="#b46">[47]</ref> by 18%. For optical flow based approach, DTMV+RGB-CNN is slightly better than iDT+CNN and shows comparable performance with iDT+FV, but exhibits worse result than Objects+Motion. However, as Objects+Motion and iDT+FV need to calculate optical flow, even with efficient implementation, optical flow based approach is one order slower than real-time requirement. Our method is around 16 times faster than real-time processing (25 fps) and exhibits 200 times quicker than optical flow based algorithm <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Visualization of Filters</head><p>To verify the effectiveness of our proposed training strategy, filters of both horizontal and vertical components of the first layer (Conv1) for MV-CNN, DTMV-CNN, and OF-CNN are visualized. As analyzed before, motion vector lacks finegrained motion information and contains imprecise motion blocks. We can clearly observe that filters for MV-CNN are much coarser than those of OF-CNN and exhibit more noisy part. On the other hand, filters of OF-CNN show clear and smooth boundaries which are more suitable for extracting effective features from videos. We hope that the knowledge of OF-CNN can be transferred to MV-CNN. However, due to the coarse nature of motion vector, the fine features learned by OF-CNN can not be directly applied into MV-CNN. Thus, at the same time, we also hope that MV-CNN can benefit the knowledge from both optical flow domain and motion vector domain. From the filters of DTMV-CNN, we can observe that the filters are smoother than those in MV-CNN but still contain certain coarse structures which are learned from motion vector. This indicates that our proposed method can transfer OF-CNN knowledge to DTMV-CNN. Our experimental results on various datasets also verify that our training strategies help to enhance motion vector CNN with better generalization abilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, motion vector CNN has been proposed to accelerate the processing speed of deep learning approach for action recognition. As motion vectors are already encoded in video stream, it can be directly extracted without extra computation. However, as motion vectors only contain block level and inaccurate motion information, directly training CNN from scratch with motion vector severally degrades the action recognition performance. To tackle this problem, DTMV-CNN is proposed to enable knowledge transfer from optical flow domain to motion vector domain. Performance of DTMV-CNN on three challenging datasets verify the effectiveness of our training approach which shows significantly better performance than MV-CNN trained from scratch. Furthermore, our proposed real-time action recognition approach is around 16 times faster than real-time requirement and achieves 391 fps, 391 fps, and 403 fps on UCF101, HMDB51 and THU-MOS2014 with high performance.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Structure for real-time action recognition system. In spatial and temporal CNN, F stands for kernel size and S means stride step. O represents for output number and P is pad size. Each time, MV-CNN processes an input by stacking 10 motion vectors, which contains 20 channels in total (10 channels for x axis and 10 channels for y axis). RGB-CNN processes one RGB image with 3 channels at one time.</figDesc><graphic coords="5,48.96,56.72,514.06,210.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Structure of Teacher Initialization, Supervision Transfer, and their combination. Blue dash lines represent copying the initial weights from teacher net to student net. Green lines are the backward propagation path. Blue full lines mean feed forward paths of teacher flow. Orange lines are feed forward paths of student net.</figDesc><graphic coords="6,86.60,59.96,123.38,119.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Structure for Deeply Connected Transfer. Blue lines represent the feed forward process of CNN, while the orange dash line means the back propagation for DTMV-CNN. It should be noticed that the OF-CNN is only utilized during training and the weight for OF-CNN is frozen.</figDesc><graphic coords="8,48.96,56.72,514.08,181.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Confusion matrix for DTMV-CNN of 101 classes on UCF101 (split1).</figDesc><graphic coords="11,48.96,56.73,514.04,391.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Performance of MV-CNN, EMV-CNN and DTMV-CNN on four categories of UCF101 Split1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a) Filters on horizontal components of Conv1 layer for MV-CNN, DTMV-CNN, and OF-CNN. From top to down: MV-CNN, DTMV-CNN, and OF-CNN. (b) Filters on vertical components of Conv1 layer for MV-CNN, DTMV-CNN, and OF-CNN. From top to down: MV-CNN, DTMV-CNN, and OF-CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Filters for Conv1 layer of MV-CNN, DTMV-CNN, and OF-CNN.</figDesc><graphic coords="13,48.96,243.69,514.07,163.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="2,118.34,56.73,375.30,180.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I ACCURACY</head><label>I</label><figDesc>OF DIFFERENT CONNECTION STRATEGY FOR DTMV-CNN ON UCF101 SPLIT1.</figDesc><table><row><cell>Connection strategy</cell><cell>Accuracy</cell></row><row><cell>FC7-FC8</cell><cell>79.3%</cell></row><row><cell>FC6-FC8</cell><cell>79.5%</cell></row><row><cell>Conv5-FC8</cell><cell>80.0%</cell></row><row><cell>Conv4-FC8</cell><cell>80.0%</cell></row><row><cell>Conv3-FC8</cell><cell>80.3%</cell></row><row><cell>Conv2-FC8</cell><cell>80.1%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II COMPARISON</head><label>II</label><figDesc>OF TEMPORAL CNN ACCURACY FOR OPTICAL FLOW BASED APPROACH AND MOTION VECTOR BASED METHOD ON UCF101 (SPLIT1). DC REPRESENTS DEEPLY CONNECTED SUPERVISION TRANSFER, ST STANDS FOR SUPERVISION TRANSFER AND TI MEANS TEACHER INITIALIZATION.</figDesc><table><row><cell>Temporal CNN</cell><cell>Accuracy</cell></row><row><cell>OF-CNN [1] (10 crops)</cell><cell>81.2%</cell></row><row><cell>MV-CNN trained from scratch (10 crops)</cell><cell>74.4%</cell></row><row><cell>EMV-CNN with ST (10 crops)</cell><cell>77.5%</cell></row><row><cell>EMV-CNN with TI (10 crops)</cell><cell>78.2%</cell></row><row><cell>EMV-CNN with ST+TI (10 crops)</cell><cell>79.3%</cell></row><row><cell>DTMV-CNN with TI+DC (10 crops)</cell><cell>81.0%</cell></row><row><cell>DTMV-CNN with ST+TI+DC (10 crops)</cell><cell>80.7%</cell></row><row><cell>DTMV-CNN with TI+DC (center crop)</cell><cell>80.3%</cell></row><row><cell>DTMV-CNN with ST+TI+DC (center crop)</cell><cell>80.3%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III COMPARISON</head><label>III</label><figDesc>OF TEMPORAL CNN ACCURACY FOR OPTICAL FLOW BASED APPROACH AND MOTION VECTOR BASED METHOD ON HMDB51 (SPLIT1).</figDesc><table><row><cell>Temporal CNN</cell><cell>Accuracy</cell></row><row><cell>OF-CNN [1]</cell><cell>55.4%</cell></row><row><cell>OF-CNN (Our reimplementation)</cell><cell>60.0%</cell></row><row><cell>MV-CNN trained from scratch</cell><cell>45.8%</cell></row><row><cell>EMV-CNN with ST+TI</cell><cell>51.2%</cell></row><row><cell>DTMV-CNN with ST+TI+DC</cell><cell>53.0%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV ACCURACY</head><label>IV</label><figDesc>OF DTMV-CNNS, EMV-CNNS AND MV-CNNS ON THUMOS 14 DATASET. WE ALSO REPORT THE RESULTS OF TWO-STREAM CNNS.</figDesc><table><row><cell>CNN</cell><cell>MAP</cell></row><row><cell>RGB CNN</cell><cell>57.7%</cell></row><row><cell>OF-CNN</cell><cell>55.3%</cell></row><row><cell>RGB CNN+OF-CNN</cell><cell>66.1%</cell></row><row><cell>MV-CNN</cell><cell>29.8%</cell></row><row><cell>EMV-CNN</cell><cell>41.6%</cell></row><row><cell>DTMV-CNN</cell><cell>43.6%</cell></row><row><cell>RGB CNN+MV-CNN</cell><cell>58.7%</cell></row><row><cell>RGB CNN+EMV-CNN</cell><cell>61.5%</cell></row><row><cell>RGB CNN+DTMV-CNN</cell><cell>62.1%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>5. It can be shown that DTMV-CNN performs well in most 1057-7149 (c) 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2018.2791180, IEEE Transactions on Image Processing</figDesc><table><row><cell>10</cell><cell>SUBMITTED TO IEEE TRANS. ON IMAGE PROCESSING</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V SPEED</head><label>V</label><figDesc>OF EACH COMPONENTS IN REAL-TIME ACTION RECOGNITION SYSTEM. MV AND RGB STANDS FOR MOTION VECTOR AND RGB IMAGE EXTRACTION, WHILE CNN MEANS CONVOLUTIONAL NEURAL NETWORK Human action category like BandMarching and HeadMassage. However, DTMV-CNN performs worse in class BrushingTeeth. For BrushingTeeth , DTMV-CNN miss-classifies majority of videos into ShavingBeards. It may be due to the action in BrushingTeeth is similar to the one in ShavingBeards.</figDesc><table><row><cell></cell><cell cols="2">PROCESSING.</cell><cell></cell></row><row><cell></cell><cell cols="2">MV and RGB</cell><cell>CNN</cell><cell>Total</cell></row><row><cell>Dataset</cell><cell></cell><cell>(fps)</cell><cell>(fps)</cell><cell>(fps)</cell></row><row><cell>UCF101</cell><cell></cell><cell>675.7</cell><cell>925.9</cell><cell>390.7</cell></row><row><cell cols="2">HMDB51</cell><cell>675.7</cell><cell>925.9</cell><cell>390.7</cell></row><row><cell cols="2">THUMOS14</cell><cell>757.6</cell><cell>925.9</cell><cell>403.2</cell></row><row><cell></cell><cell cols="2">TABLE VI</cell><cell></cell></row><row><cell cols="5">COMPARISON OF SPEED FOR OPTICAL FLOW FIELDS AND MOTION</cell></row><row><cell cols="5">VECTORS. MV MEANS MOTION VECTOR.</cell></row><row><cell></cell><cell>Spatial</cell><cell cols="2">Brox's Flow[11]</cell><cell>MV</cell></row><row><cell>Dataset</cell><cell>Resolution</cell><cell cols="2">(GPU) (fps)</cell><cell>(CPU) (fps)</cell></row><row><cell>UCF101</cell><cell>320 × 240</cell><cell cols="2">16.7</cell><cell>675.7</cell></row><row><cell>HMDB51</cell><cell>320 × 240</cell><cell cols="2">16.7</cell><cell>675.7</cell></row><row><cell cols="2">THUMOS14 320 × 180</cell><cell cols="2">17.5</cell><cell>757.6</cell></row><row><cell>videos for Human-</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VII COMPARISON</head><label>VII</label><figDesc>OF SPEED AND ACCURACY WITH STATE-OF-THE-ART ON UCF101.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://github.com/yjxiong/caffe</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_1"><p>SUBMITTED TO IEEE TRANS. ON IMAGE PROCESSING</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>REAL-TIME ACTION RECOGNITION WITH DEEPLY-TRANSFERRED MOTION VECTOR CNNS</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by National Natural Science Foundation of China (U1613211, 61633021, 61622115), Shenzhen Basic Research Program (JCYJ20150925163005055), External Cooperation Program of BIC Chinese Academy of Sciences (172644KYSB20150019,172644KYSB20160033), and Shanghai Engineering Research Center of Industrial Vision Perception &amp; Intelligent Computing (17DZ2251600). This work was mainly conducted when B. Zhang interned in Shenzhen Institutes of Advanced Technology.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<meeting><address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12">Dec. 2014</date>
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<meeting><address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-03">Mar. 2013</date>
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">MoFAP: A multi-level representation for action recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="254" to="271" />
			<date type="published" when="2016-09">Sept. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12">Dec. 2015</date>
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Action recognition with trajectorypooled deep-convolutional descriptors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06">Jun. 2015</date>
			<biblScope unit="page" from="4305" to="4314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Video google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<meeting><address><addrLine>Nice, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-04">Apr. 2003</date>
			<biblScope unit="page" from="1470" to="1477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image classification with the fisher vector: Theory and practice</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="222" to="245" />
			<date type="published" when="2013-12">Dec. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<meeting><address><addrLine>Amsterdam, the Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-10">Oct. 2016</date>
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>abs/1212.0402</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-01">Jan. 2011</date>
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">High accuracy optical flow estimation based on a theory for warping</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<meeting><address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-05">May 2004</date>
			<biblScope unit="page" from="25" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Overview of the h. 264/avc video coding standard</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bjontegaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Luthra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Techn</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="560" to="576" />
			<date type="published" when="2003-07">Jul. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient feature extraction, encoding, and classification for action recognition</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kantorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-09">Sept. 2014</date>
			<biblScope unit="page" from="2593" to="2600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Real-time action recognition with enhanced motion vector cnns</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="2718" to="2726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">THUMOS challenge: Action recognition with a large number of classes</title>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://crcv.ucf.edu/THUMOS14/" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Representing and recognizing the visual appearance of materials using three-dimensional textons</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="44" />
			<date type="published" when="2001-06">Jun. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On space-time interest points</title>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="107" to="123" />
			<date type="published" when="2005-09">Sept. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Evaluation of local spatio-temporal features for action recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Ullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<meeting><address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-09">Sept. 2009</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="124" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Action recognition by dense trajectories</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Colorado Springs, CO, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-08">Aug. 2011</date>
			<biblScope unit="page" from="3169" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-07">Jul. 2005</date>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Human detection using oriented histograms of flow and appearance</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<meeting><address><addrLine>Graz, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-05">May 2006</date>
			<biblScope unit="page" from="428" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Aggregating local descriptors into a compact image representation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-08">Aug. 2010</date>
			<biblScope unit="page" from="3304" to="3311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bag of visual words and fusion methods for action recognition: Comprehensive study and good practice</title>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">150</biblScope>
			<biblScope unit="page" from="109" to="125" />
			<date type="published" when="2016-09">Sept. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<meeting><address><addrLine>Lake Tahoe, Nevada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-12">Dec. 2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-09">Sept. 2014</date>
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Appearance-and-relation networks for video classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno>abs/1711.09125</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12">Dec. 2016</date>
			<biblScope unit="page" from="3468" to="3476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-H</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-10">Oct. 2015</date>
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Exploring interfeature and inter-class relationships with deep neural networks for video classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<meeting><address><addrLine>Orlando, Florida, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-11">Nov. 2014</date>
			<biblScope unit="page" from="167" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1503.02531</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Binarynet: Training deep neural networks with weights and activations constrained to+ 1 or-1</title>
		<author>
			<persName><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1602.02830</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Xnor-net: Imagenet classification using binary convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<meeting><address><addrLine>Amsterdam, the Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-10">Oct. 2016</date>
			<biblScope unit="page" from="525" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning using privileged information: similarity control and knowledge transfer</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Izmailov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="2023" to="2049" />
			<date type="published" when="2015-09">Sept. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning using privileged information: Svm+ and weighted svm</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lapin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="95" to="108" />
			<date type="published" when="2014-05">May 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unifying distillation and privileged information</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<idno>abs/1511.03643</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Exploiting privileged information from web data for image categorization</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-09">Sept. 2014</date>
			<biblScope unit="page" from="437" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Exploiting privileged information from web data for action and event recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="130" to="150" />
			<date type="published" when="2016-06">Jun. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Cross modal distillation for supervision transfer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="2827" to="2836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Knowledge guided disambiguation for large-scale scene classification with multi-resolution cnns</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2055" to="2068" />
			<date type="published" when="2017-04">Apr. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Transferring deep object and scene representations for event recognition in still images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2017-09">Sept. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Two-frame motion estimation based on polynomial expansion</title>
		<author>
			<persName><forename type="first">G</forename><surname>Farnebäck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SCIA</title>
		<meeting><address><addrLine>Halmstad, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-06">Jun. 2003</date>
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">High efficiency video coding (hevc) text specification draft 10</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-R</forename><surname>Ohm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wiegand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JCTVC</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Towards Good Practices for Very Deep Two-Stream ConvNets</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno>abs/1507.02159</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">What do 15,000 object categories tell us about classifying and localizing actions?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06">Jun. 2015</date>
			<biblScope unit="page" from="46" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">TV-L1 optical flow estimation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Meinhardt-Llopis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Facciolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IPOL Journal</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="137" to="150" />
			<date type="published" when="2013-07">Jul. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Action recognition and detection by combining motion and appearance features</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">THUMOS Action Recognition challenge</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Untrimmednets for weakly supervised action recognition and detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, Honolulu</title>
		<meeting><address><addrLine>HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07">Jul. 2017</date>
			<biblScope unit="page" from="4325" to="4334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Temporal segment networks for action recognition in videos</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno>abs/1705.02953</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
