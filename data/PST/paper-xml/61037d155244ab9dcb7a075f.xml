<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-07-28">28 Jul 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
							<email>weizhey@cs.cmu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
							<email>zhengbaj@cs.cmu.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
							<email>hiroakih@cs.cmu.edu</email>
							<affiliation key="aff4">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
							<email>gneubig@cs.cmu.edu</email>
							<affiliation key="aff5">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-07-28">28 Jul 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2107.13586v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper surveys and organizes research works in a new paradigm in natural language processing, which we dub "prompt-based learning". Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P (y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: it allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this paper we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g. the choice of pre-trained models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts, but also release other resources, e.g., a website NLPedia-Pretrain including constantly-updated survey, and paperlist.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1 Two Sea Changes in NLP Fully supervised learning, where a task-specific model is trained solely on a dataset of input-output examples for the target task, has long played a central role in many machine learning tasks <ref type="bibr" target="#b91">(Kotsiantis et al., 2007)</ref>, and natural language processing (NLP) was no exception. Because such fully supervised datasets are ever-insufficient for learning high-quality models, early NLP models relied heavily on feature engineering (Tab. 1 a.; e.g. <ref type="bibr">Lafferty</ref>   <ref type="formula">2017</ref>)). <ref type="foot" target="#foot_0">1</ref>However, from 2017-2019 there was a sea change in the learning of NLP models, and this fully supervised paradigm is now playing an ever-shrinking role. Specifically, the standard shifted to the pre-train and fine-tune paradigm (Tab. 1 c.; e.g. <ref type="bibr">Radford</ref>   <ref type="formula">2020a</ref>)). In this paradigm, a model with a fixed 2 architecture is pre-trained as a language model (LM), predicting the probability of observed textual data. Because the raw textual data necessary to train LMs is available in abundance, these LMs can be trained on large datasets, in the process learning robust general-purpose features of the language it is modeling. The above pre-trained LM will be then adapted to different downstream tasks by introducing additional parameters and fine-tuning them using task-specific objective functions. Within this paradigm, the focus turned mainly to objective engineering, designing the training objectives used at both the pre-training and fine-tuning stages. For example, <ref type="bibr" target="#b205">Zhang et al. (2020a)</ref> show that introducing a loss function of predicting salient sentences from a document will lead to a better pre-trained model for text summarization. Notably, the main body of the pre-trained LM is generally (but not always; Peters et al. ( <ref type="formula">2019</ref>)) fine-tuned as well to make it more suitable for solving the downstream task. Now, as of this writing in 2021, we are in the middle of a second sea change, in which the "pre-train, fine-tune" procedure is replaced by one in which we dub "pre-train, prompt, and predict". In this paradigm, instead of adapting pre-trained LMs to downstream tasks via objective engineering, downstream tasks are reformulated to look more like those solved during the original LM training with the help of a textual prompt. For example, when recognizing the emotion of a social media post, "I missed the bus today.", we may continue with a prompt "I felt so ", and ask the LM to fill the blank with an emotion-bearing word. Or if we choose the prompt "English: I missed the bus today. French: "), an LM may be able to fill in the blank with a French translation. In this way, by selecting the appropriate prompts we can manipulate the model behavior so that the pre-trained LM itself can be used to predict the desired output, sometimes even without any additional task-specific training (Tab.  <ref type="formula">2021</ref>)). The advantage of this method is that, given a suite of appropriate prompts, a single LM trained in an entirely unsupervised fashion can be used to solve a great number of tasks (Brown et al., 2020; <ref type="bibr" target="#b177">Sun et al., 2021)</ref>. However, as with most conceptually enticing prospects, there is a catch -this method introduces the necessity for prompt engineering, finding the most appropriate prompt to allow a LM to solve the task at hand. This survey attempts to organize the current state of knowledge in this rapidly developing field by providing an overview and formal definition of prompting methods ( §2), and an overview of the pre-trained language models that use these prompts ( §3). This is followed by in-depth discussion of prompting methods, from basics such as prompt engineering ( §4) and answer engineering ( §5) to more advanced concepts such as multi-prompt learning methods ( §6) and prompt-aware training methods ( §7). We then organize the various applications to which prompt-based learning methods have been applied, and discuss how they interact with the choice of prompting method ( §8). Finally, we attempt to situate the current state of prompting methods in the research ecosystem, making connections to other research fields ( §9), suggesting some current challenging problems that may be ripe for further research ( §10), and performing a meta-analysis of current research trends ( §11).</p><p>Finally, in order to help beginners who are interested in this field learn more effectively, we highlight some systematic resources about prompt learning (as well as pre-training) provided both within this survey and on companion websites:</p><p>• : A website of prompt-based learning that contains: frequent updates to this survey, related slides, etc. • Fig. <ref type="figure" target="#fig_7">1</ref>: A typology of important concepts for prompt-based learning.  1. Apply a template, which is a textual string that has two slots: an input slot [X] for input x and an answer slot [Z] for an intermediate generated answer text z that will later be mapped into y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Fill slot [X]</head><p>with the input text x.</p><p>In the case of sentiment analysis where x ="I love this movie.", the template may take a form such as "[X] Overall, it was a [Z] movie.". Then, x would become "I love this movie. Overall it was a [Z] movie." given the previous example. In the case of machine translation, the template may take a form such as "Finnish: [X] English:</p><p>[Z]", where the text of the input and answer are connected together with headers indicating the language. We show more examples in Tab. <ref type="bibr" target="#b13">3</ref> Notably, (1) the prompts above will have an empty slot to fill in for z, either in the middle of the prompt or at the end. In the following text, we will refer to the first variety of prompt with a slot to fill in the middle of the text as a cloze prompt, and the second variety of prompt where the input text comes entirely before z as a prefix prompt. <ref type="bibr" target="#b12">(2)</ref> In many cases these template words are not necessarily composed of natural language tokens; they could be virtual words (e.g. represented by numeric ids) which would be embedded in a continuous space later, and some prompting methods even generate continuous vectors directly (more in §4.3.2). (3) The number of [X] slots and the number of [Z] slots can be flexibly changed for the need of tasks at hand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Answer Search</head><p>Next, we search for the highest-scoring text ẑ that maximizes the score of the LM. We first define Z as a set of permissible values for z. Z could range from the entirety of the language in the case of generative tasks, or could be a small subset of the words in the language in the case of classification, such as defining Z = {"excellent", "good", "OK", "bad", "horrible"} to represent each of the classes in Y = {++, +, , -, --}.</p><p>We then define a function f fill (x , z) that fills in the location [Z] in prompt x with the potential answer z. We will call any prompt that has gone through this process as a filled prompt. Particularly, if the prompt is filled with a true answer, we will refer to it as an answered prompt (Tab. 2 shows an example). Finally, we search over the set of potential answers z by calculating the probability of their corresponding filled prompts using a pre-trained LM P (•; θ) ẑ = search z∈Z P (f fill (x , z); θ).</p><p>This search function could be an argmax search that searches for the highest-scoring output, or sampling that randomly generates outputs following the probability distribution of the LM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Answer Mapping</head><p>Finally, we would like to go from the highest-scoring answer ẑ to the highest-scoring output ŷ. This is trivial in some cases, where the answer itself is the output (as in language generation tasks such as translation), but there Table <ref type="table">3</ref>: Examples of input, template, and answer for different tasks. In the Type column, "CLS" is an abbreviation for "classification". In the Task column, "NLI" and "NER" are abbreviations for "natural language inference" (Bowman et al., 2015) and "named entity recognition" <ref type="bibr" target="#b182">(Tjong Kim Sang and De Meulder, 2003)</ref> respectively.</p><p>are also other cases where multiple answers could result in the same output. For example, one may use multiple different sentiment-bearing words (e.g. "excellent", "fabulous", "wonderful") to represent a single class (e.g. "++"), in which case it is necessary to have a mapping between the searched answer and the output value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Design Considerations for Prompting</head><p>Now that we have our basic mathematical formulation, we elaborate a few of the basic design considerations that go into a prompting method, which we will elaborate in the following sections:</p><p>• Pre-trained Model Choice: There are a wide variety of pre-trained LMs that could be used to calculate P (x; θ). In §3 we give a primer on pre-trained LMs, specifically from the dimensions that are important for interpreting their utility in prompting methods.</p><p>• Prompt Engineering: Given that the prompt specifies the task, choosing a proper prompt has a large effect not only on the accuracy, but also on which task the model performs in the first place. In §4 we discuss methods to choose which prompt we should use as f prompt (x).</p><p>• Answer Engineering: Depending on the task, we may want to design Z differently, possibly along with the mapping function. In §5 we discuss different ways to do so.</p><p>• Expanding the Paradigm: As stated above, the above equations represent only the simplest of the various underlying frameworks that have been proposed to do this variety of prompting. In §6 we discuss ways to expand this underlying paradigm to further improve results or applicability.</p><p>• Prompt-based Training Strategies: There are also methods to train parameters, either of the prompt, the LM, or both. In §7, we summarize different strategies and detail their relative advantages.</p><p>3 Pre-trained Language Models</p><p>Given the large impact that pre-trained LMs have had on NLP in the pre-train and fine-tune paradigm, there are already a number of high-quality surveys that interested readers where interested readers can learn more <ref type="bibr" target="#b151">(Raffel et al., 2020;</ref><ref type="bibr" target="#b148">Qiu et al., 2020;</ref><ref type="bibr" target="#b66">Xu et al., 2021;</ref><ref type="bibr" target="#b43">Doddapaneni et al., 2021)</ref>. Nonetheless, in this chapter we present a systematic view of various pre-trained LMs which (i) organizes them along various axes in a more systematic way, (ii) particularly focuses on aspects salient to prompting methods. Below, we will detail them through the lens of main training objective, type of text noising, auxiliary training objective, attention mask, typical architecture, and preferred application scenarios. We describe each of these objectives below, and also summarize a number of pre-trained LMs along each of these axes in Tab. 13 in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training Objectives</head><p>The main training objective of a pre-trained LM almost invariably consists of some sort of objective predicting the probability of text x.</p><p>Standard Language Model (SLM) objectives do precisely this, training the model to optimize the probability P (x) of text from a training corpus <ref type="bibr" target="#b150">(Radford et al., 2019)</ref>. In these cases, the text is generally predicted in an autoregressive fashion, predicting the tokens in the sequence one at a time. This is usually done from left to right (as detailed below), but can be done in other orders as well.</p><p>A popular alternative to standard LM objectives are denoising objectives, which apply some noising function x = f noise (x) to the input sentence (details in the following subsection), then try to predict the original input sentence given this noised text P (x| x). There are two common flavors of these objectives:</p><p>Corrupted Text Reconstruction (CTR) These objectives restore the processed text to its uncorrupted state by calculating loss over only the noised parts of the input sentence.</p><p>Full Text Reconstruction (FTR) These objectives reconstruct the text by calculating the loss over the entirety of the input texts whether it has been noised or not <ref type="bibr" target="#b104">(Lewis et al., 2020a)</ref>.</p><p>The main training objective of the pre-trained LMs plays an important role in determining its applicability to particular prompting tasks. For example, left-to-right autoregressive LMs may be particularly suitable for prefix prompts, whereas reconstruction objectives may be more suitable for cloze prompts. In addition, models trained with standard LM and FTR objectives may be more suitable for tasks regarding text generation, whereas other tasks such as classification can be formulated using models trained with any of these objectives.</p><p>In addition to the main training objectives above, a number of auxiliary objectives have been engineered to further improve models' ability to perform certain varieties of downstream tasks. We list some commonly-used auxiliary objectives in Appendix A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Noising Functions</head><p>In training objectives based on reconstruction, the specific type of corruption applied to obtain the noised text x has an effect on the efficacy of the learning algorithm. In addition, prior knowledge can be incorporated by controlling the type of noise, e.g. the noise could focus on entities of a sentence, which allows us to learn a pre-trained model with particularly high predictive performance for entities. In the following, we introduce several types of noising functions, and give detailed examples in Tab. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Directionality of Representations</head><p>Masking (e.g. <ref type="bibr" target="#b42">Devlin et al. (2019)</ref>) The text will be masked in different levels, replacing a token or multi-token span with a special token such as <ref type="bibr">[MASK]</ref>. Notably, masking can either be random from some distribution or specifically designed to introduce prior knowledge, such as the above-mentioned example of masking entities to encourage the model to be good at predicting entities.</p><p>Replacement (e.g. <ref type="bibr" target="#b151">Raffel et al. (2020)</ref>) Replacement is similar to masking, except that the token or multi-token span is not replaced with a [MASK] but rather another token or piece of information (e.g., an image region <ref type="bibr" target="#b174">(Su et al., 2020)</ref>).</p><p>Deletion (e.g. <ref type="bibr" target="#b104">Lewis et al. (2020a)</ref>) Tokens or multi-token spans will be deleted from a text without the addition of <ref type="bibr">[MASK]</ref> or any other token. This operation is usually used together with the FTR loss.</p><p>Permutation (e.g. <ref type="bibr" target="#b114">Liu et al. (2020a)</ref>) The text is first divided into different spans (tokens, sub-sentential spans, or sentences), and then these spans are be permuted into a new text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Directionality of Representations</head><p>A final important factor that should be considered in understanding pre-trained LMs and the difference between them is the directionality of the calculation of representations. In general, there are two widely used ways to calculate such representations:</p><p>Left-to-Right The representation of each word is calculated based on the word itself and all previous words in the sentence. For example, if we have a sentence "This is a good movie", the representation of the word "good" would be calculated based on previous words. This variety of factorization is particularly widely used when calculating standard LM objectives or when calculating the output side of an FTR objective, as we discuss in more detail below.</p><p>Bidirectional The representation of each word is calculated based on all words in the sentence, including words to the left of the current word. In the example above, "good" would be influenced by all words in the sentence, even the following "movie".</p><p>In addition to the two most common directionalities above, it is also possible to mix the two strategies together in a single model <ref type="bibr" target="#b45">(Dong et al., 2019;</ref><ref type="bibr" target="#b16">Bao et al., 2020)</ref>, or perform conditioning of the representations in a randomly permuted order <ref type="bibr" target="#b198">(Yang et al., 2019)</ref>, although these strategies are less widely used. Notably, when implementing these strategies within a neural model, this conditioning is generally implemented through attention masking, which masks out the values in an attentional model <ref type="bibr" target="#b14">(Bahdanau et al., 2014)</ref>, such as the popular Transformer architecture <ref type="bibr" target="#b186">(Vaswani et al., 2017)</ref>. Some examples of such attention masks are shown in Figure <ref type="figure" target="#fig_14">2</ref>.</p><p>x 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I E E G y U V A W x T a u u X 3 x + X f M m 7 U r O c = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J N N p D c 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h k E m H e e 1 Y C 0 s L i 2 v F F f X 1 j c 2 t 7 Z L O 7 u t L M k F 4 0 2 W h I n o + F 7 G w y D m T R n I k H d S w b 3 I D 3 n b H 5 + p e P u G i y x I 4 k s 5 S X k v 8 k Z x M A y Y J 4 m 6 u O u 7 / V L Z q T h 6 2 f P A N a A M s x p J 6 Q V X G C A B Q 4 4 I H D E k 4 R A e M n q 6 c O E g J a 6 H K X G C U K D j H P d Y I 2 1 O W Z w y P G L H 9 B 3 R r m v Y m P b K M 9 N q R q e E 9 A p S 2 j g k T U J 5 g r A 6 z d b x X D s r 9 j f v q f Z U d 5 v Q 3 z d e E b E S 1 8 T + p Z t l / l e n a p E Y 4 l T X E F B N q W Z U d c y 4 5 L o r 6 u b 2 l 6 o k O a T E K T y g u C D M t H L W Z 1 t r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q z 0 x u j n d 1 S x q w + 3 O c 8 6 B V r b h O x T 0 / L t e q Z t R F 7 O M A R z T P E 9 R Q R w N N 8 h 7 h E U 9 4 t u p W b O X W 7 W e q V T C a P X x b 1 s M H C T u Q E Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I E E G y U V A W x T a u u X 3 x + X f M m 7 U r O c = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J N N p D c 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h k E m H e e 1 Y C 0 s L i 2 v F F f X 1 j c 2 t 7 Z L O 7 u t L M k F 4 0 2 W h I n o + F 7 G w y D m T R n I k H d S w b 3 I D 3 n b H 5 + p e P u G i y x I 4 k s 5 S X k v 8 k Z x M A y Y J 4 m 6 u O u 7 / V L Z q T h 6 2 f P A N a A M s x p J 6 Q V X G C A B Q 4 4 I H D E k 4 R A e M n q 6 c O E g J a 6 H K X G C U K D j H P d Y I 2 1 O W Z w y P G L H 9 B 3 R r m v Y m P b K M 9 N q R q e E 9 A p S 2 j g k T U J 5 g r A 6 z d b x X D s r 9 j f v q f Z U d 5 v Q 3 z d e E b E S 1 8 T + p Z t l / l e n a p E Y 4 l T X E F B N q W Z U d c y 4 5 L o r 6 u b 2 l 6 o k O a T E K T y g u C D M t H L W Z 1 t r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q z 0 x u j n d 1 S x q w + 3 O c 8 6 B V r b h O x T 0 / L t e q Z t R F 7 O M A R z T P E 9 R Q R w N N 8 h 7 h E U 9 4 t u p W b O X W 7 W e q V T C a P X x b 1 s M H C T u Q E Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I E E G y U V A W x T a u u X 3 x + X f M m 7 U r O c = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J N N p D c 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h k E m H e e 1 Y C 0 s L i 2 v F F f X 1 j c 2 t 7 Z L O 7 u t L M k F 4 0 2 W h I n o + F 7 G w y D m T R n I k H d S w b 3 I D 3 n b H 5 + p e P u G i y x I 4 k s 5 S X k v 8 k Z x M A y Y J 4 m 6 u O u 7 / V L Z q T h 6 2 f P A N a A M s x p J 6 Q V X G C A B Q 4 4 I H D E k 4 R A e M n q 6 c O E g J a 6 H K X G C U K D j H P d Y I 2 1 O W Z w y P G L H 9 B 3 R r m v Y m P b K M 9 N q R q e E 9 A p S 2 j g k T U J 5 g r A 6 z d b x X D s r 9 j f v q f Z U d 5 v Q 3 z d e E b E S 1 8 T + p Z t l / l e n a p E Y 4 l T X E F B N q W Z U d c y 4 5 L o r 6 u b 2 l 6 o k O a T E K T y g u C D M t H L W Z 1 t r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q z 0 x u j n d 1 S x q w + 3 O c 8 6 B V r b h O x T 0 / L t e q Z t R F 7 O M A R z T P E 9 R Q R w N N 8 h 7 h E U 9 4 t u p W b O X W 7 W e q V T C a P X x b 1 s M H C T u Q E Q = = &lt; / l a t e x i t &gt; x 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w + O 2 A V A N 5 t G Y s 3 Q A W 4 f 1 x G 8 R q I 4 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J O m 0 D s 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h j y T j v N a s B Y W l 5 Z X i q t r 6 x u b W 9</p><formula xml:id="formula_1">u l n d 1 W l u Q i Y M 0 g C R P R 8 b 2 M h T x m T c l l y D q p Y F 7 k h 6 z t j 8 9 U v H 3 D R M a T + F J O U t a L v F H M h z z w J F E X d / 1 q v 1 R 2 K o 5 e 9 j x w D S j D r E Z S e s E V B k g Q I E c E h h i S c A g P G T 1 d u H C Q E t f D l D h B i O s 4 w z 3 W S J t T F q M M j 9 g x f U e 0 6 x o 2 p r 3 y z L Q 6 o F N C e g U p b R y S J q E 8 Q V i d Z u t 4 r p 0 V + 5 v 3 V H u q u 0 3 o 7 x u v i F i J a 2 L / 0 s 0 y / 6 t T t U g M c a p r 4 F R T q h l V X W B c c t 0 V d X P 7 S 1 W S H F L i F B 5 Q X B A O t H L W Z 1 t</formula><p>r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q H 5 j c H O / q l j R g 9 + c 4 5 0 G r W n G d i n t + X K 5 V z a i L 2 M c B j m i e J 6 i h j g a a 5 D 3 C I 5 7 w b N W t 2 M q t 2 8 9 U q 2 A 0 e / i 2 r I c P C 5 u Q E g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w</p><formula xml:id="formula_2">+ O 2 A V A N 5 t G Y s 3 Q A W 4 f 1 x G 8 R q I 4 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J O m 0 D s 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h j y T j v N a s B Y W l 5 Z X i q t r 6 x u b W 9 u l n d 1 W l u Q i Y M 0 g C R P R 8 b 2 M h T x m T c l l y D q p Y F 7 k h 6 z t j 8 9 U v H 3 D R M a T + F J O U t a L v F H M h z z w J F E X d / 1 q v 1 R 2 K o 5 e 9 j x w D S j D r E Z S e s E V B k g Q I E c E h h i S c A g P G T 1 d u H C Q E t f D l D h B i O s 4 w z 3 W S J t T F q M M j 9 g x f U e 0 6 x o 2 p r 3 y z L Q 6 o F N C e g U p b R y S J q E 8 Q V i d Z u t 4 r p 0 V + 5 v 3 V H u q u 0 3 o 7 x u v i F i J a 2 L / 0 s 0 y / 6 t T t U g M c a p r 4 F R T q h l V X W B c c t 0 V d X P 7 S 1 W S H F L i F B 5 Q X B A O t H L W Z 1 t</formula><p>r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q H 5 j c H O / q l j R g 9 + c 4 5 0 G r W n G d i n t + X K 5 V z a i L 2 M c B j m i e J 6 i h j g a a 5 D 3 C I 5 7 w b N W t 2 M q t 2 8 9 U q 2 A 0 e / i 2 r I c P C 5 u Q E g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w</p><formula xml:id="formula_3">+ O 2 A V A N 5 t G Y s 3 Q A W 4 f 1 x G 8 R q I 4 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J O m 0 D s 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h j y T j v N a s B Y W l 5 Z X i q t r 6 x u b W 9 u l n d 1 W l u Q i Y M 0 g C R P R 8 b 2 M h T x m T c l l y D q p Y F 7 k h 6 z t j 8 9 U v H 3 D R M a T + F J O U t a L v F H M h z z w J F E X d / 1 q v 1 R 2 K o 5 e 9 j x w D S j D r E Z S e s E V B k g Q I E c E h h i S c A g P G T 1 d u H C Q E t f D l D h B i O s 4 w z 3 W S J t T F q M M j 9 g x f U e 0 6 x o 2 p r 3 y z L Q 6 o F N C e g U p b R y S J q E 8 Q V i d Z u t 4 r p 0 V + 5 v 3 V H u q u 0 3 o 7 x u v i F i J a 2 L / 0 s 0 y / 6 t T t U g M c a p r 4 F R T q h l V X W B c c t 0 V d X P 7 S 1 W S H F L i F B 5 Q X B A O t H L W Z 1 t</formula><p>r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q H 5 j c H O / q l j R g 9 + c 4 5 0 G r W n G d i n t + X K 5 V z a i L 2 M c B j m i e J 6 i h j g a a 5 D 3 C I 5 7 w b N W t 2 M q t 2 8 9 U q 2 A 0 e / i 2 r I c P C 5 u Q E g = = &lt; / l a t e x i t &gt; <ref type="bibr" target="#b16">6</ref> 2 C 0 e z j 2 7 I e P g B M i 5 A n &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T o R A e S A j J X f q b p 5 c p o <ref type="bibr" target="#b16">6</ref> 2 C 0 e z j 2 7 I e P g B M i 5 A n &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T o R A e S A j J X f q b p 5 c p o</p><formula xml:id="formula_4">x 3 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C L X q B p c I l H 0 j f t D 1 W T P R S D D H 1 d 0 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I q 6 L L g p s u K 9 g G 1 l G Q 6 r U P T J E w m a i m C P + B W P 0 3 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h y I R D n O a 8 5 a W F x a X s m v F t b W N z a 3 i t s 7 z S R K J e M N F g W R b P t e w g M R 8 o Y S K u D t W H J v 7 A e 8 5 Y / O d L x 1 w 2 U i o v B S T W L e H X v D U A w E 8 x R R F 3 e 9 o 1 6 x 5 J Q d s + x 5 4 G a g h G z V o + I L r t B H B I Y U Y 3 C E U I Q D e E j o 6 c C F g 5 i 4 L q b E S U L C x D n u U S B t S l m c M j x i R / Q d 0 q 6 T s S H t t W d i 1 I x O C e i V p L R x Q J q I 8 i R h f Z p t 4 q l x 1 u x v 3 l P j q e 8 2 o b + f e Y 2 J V b g m 9 i / d L P O / O l 2 L w g C n p g Z B N c W G 0 d W x z C U 1 X d E 3 t 7 9 U p c g h J k 7 j P s U l Y W a U s z 7 b R p O Y 2 n V v P R N / M 5 m a 1 X u W 5 a Z 4 1 7 e k A b s / x z k P m p W y 6 5 T d 8 + N S t Z K N O o 8 9 7 O O Q 5 n m C K m q o o 0 H e Q z z i C c 9 W z Q q t 1 L r 9 T L V y m W Y X 3 5 b 1 8 A E N + 5 A T &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C L X q B p c I l H 0 j f t D 1 W T P R S D D H 1 d 0 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I q 6 L L g p s u K 9 g G 1 l G Q 6 r U P T J E w m a i m C P + B W P 0 3 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h y I R D n O a 8 5 a W F x a X s m v F t b W N z a 3 i t s 7 z S R K J e M N F g W R b P t e w g M R 8 o Y S K u D t W H J v 7 A e 8 5 Y / O d L x 1 w 2 U i o v B S T W L e H X v D U A w E 8 x R R F 3 e 9 o 1 6 x 5 J Q d s + x 5 4 G a g h G z V o + I L r t B H B I Y U Y 3 C E U I Q D e E j o 6 c C F g 5 i 4 L q b E S U L C x D n u U S B t S l m c M j x i R / Q d 0 q 6 T s S H t t W d i 1 I x O C e i V p L R x Q J q I 8 i R h f Z p t 4 q l x 1 u x v 3 l P j q e 8 2 o b + f e Y 2 J V b g m 9 i / d L P O / O l 2 L w g C n p g Z B N c W G 0 d W x z C U 1 X d E 3 t 7 9 U p c g h J k 7 j P s U l Y W a U s z 7 b R p O Y 2 n V v P R N / M 5 m a 1 X u W 5 a Z 4 1 7 e k A b s / x z k P m p W y 6 5 T d 8 + N S t Z K N O o 8 9 7 O O Q 5 n m C K m q o o 0 H e Q z z i C c 9 W z Q q t 1 L r 9 T L V y m W Y X 3 5 b 1 8 A E N + 5 A T &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C L X q B p c I l H 0 j f t D 1 W T P R S D D H 1 d 0 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I q 6 L L g p s u K 9 g G 1 l G Q 6 r U P T J E w m a i m C P + B W P 0 3 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h y I R D n O a 8 5 a W F x a X s m v F t b W N z a 3 i t s 7 z S R K J e M N F g W R b P t e w g M R 8 o Y S K u D t W H J v 7 A e 8 5 Y / O d L x 1 w 2 U i o v B S T W L e H X v D U A w E 8 x R R F 3 e 9 o 1 6 x 5 J Q d s + x 5 4 G a g h G z V o + I L r t B H B I Y U Y 3 C E U I Q D e E j o 6 c C F g 5 i 4 L q b E S U L C x D n u U S B t S l m c M j x i R / Q d 0 q 6 T s S H t t W d i 1 I x O C e i V p L R x Q J q I 8 i R h f Z p t 4 q l x 1 u x v 3 l P j q e 8 2 o b + f e Y 2 J V b g m 9 i / d L P O / O l 2 L w g C n p g Z B N c W G 0 d W x z C U 1 X d E 3 t 7 9 U p c g h J k 7 j P s U l Y W a U s z 7 b R p O Y 2 n V v P R N / M 5 m a 1 X u W 5 a Z 4 1 7 e k A b s / x z k P m p W y 6 5 T d 8 + N S t Z K N O o 8 9 7 O O Q 5 n m C K m q o o 0 H e Q z z i C c 9 W z Q q t 1 L r 9 T L V y m W Y X 3 5 b 1 8 A E N + 5 A T &lt; / l a t e x i t &gt; y 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D F 3 Q y n H e 2 D 2 p z i C p V s g 5 A B U J 3 q Y = " &gt; A A A C x 3 i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I i 6 L L g R n c V 7 A O 0 l C S d t k P z Y j I p l u L C H 3 C r f y b + g f 6 F d 8 Y p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h r y T D r O a 8 F a W l 5 Z X S u u l z Y 2 t 7 Z 3 y r t 7 r S z J R c C a Q R I m o u N 7 G Q t 5 z J q S y 5 B 1 U s G 8 y A 9 Z 2 x + f q 3 h 7 w k T G k / h a T l P W j b x h z A c 8 8 K S i p j 2 3 1 C t X n K q j l 7 0 I X A M q M K u R l F 9 w i z 4 S B M g R g S G G J B z C Q 0 b P D V w 4 S I n r Y k a c I M R 1 n O E e J d L m l M U o w y N 2 T N 8 h 7 W 4 M G 9 N e e W Z a H d A p I b 2 C l D a O S J N Q n i C s T r N 1 P N f O i v 3 N e 6 Y 9 1 d 2 m 9 P e N V 0 S s x I j Y v 3 T z z P / q V C 0 S A 5 z p G j j V l G p G V R c Y l 1 x 3 R d 3 c / l K V J I e U O I X 7 F B e E A 6 2 c 9 9 n W m k z X r n r r 6 f i b z l S s 2 g c m N 8 e 7 u i U N 2 P 0 5 z k X Q q l V d p + p e n V T q N T P q I g 5 w i G O a 5 y n q u E A D T f I e 4 R F P e L Y u r c S a W H e f q V b B a P b x b V k P H 0 o q k C Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D F 3 Q y n H e 2 D 2 p z i C p V s g 5 A B U J 3 q Y = " &gt; A A A C x 3 i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I i 6 L L g R n c V 7 A O 0 l C S d t k P z Y j I p l u L C H 3 C r f y b + g f 6 F d 8 Y p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h r y T D r O a 8 F a W l 5 Z X S u u l z Y 2 t 7 Z 3 y r t 7 r S z J R c C a Q R I m o u N 7 G Q t 5 z J q S y 5 B 1 U s G 8 y A 9 Z 2 x + f q 3 h 7 w k T G k / h a T l P W j b x h z A c 8 8 K S i p j 2 3 1 C t X n K q j l 7 0 I X A M q M K u R l F 9 w i z 4 S B M g R g S G G J B z C Q 0 b P D V w 4 S I n r Y k a c I M R 1 n O E e J d L m l M U o w y N 2 T N 8 h 7 W 4 M G 9 N e e W Z a H d A p I b 2 C l D a O S J N Q n i C s T r N 1 P N f O i v 3 N e 6 Y 9 1 d 2 m 9 P e N V 0 S s x I j Y v 3 T z z P / q V C 0 S A 5 z p G j j V l G p G V R c Y l 1 x 3 R d 3 c / l K V J I e U O I X 7 F B e E A 6 2 c 9 9 n W m k z X r n r r 6 f i b z l S s 2 g c m N 8 e 7 u i U N 2 P 0 5 z k X Q q l V d p + p e n V T q N T P q I g 5 w i G O a 5 y n q u E A D T f I e 4 R F P e L Y u r c S a W H e f q V b B a P b x b V k P H 0 o q k C Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D F 3 Q y n H e 2 D 2 p z i C p V s g 5 A B U J 3 q Y = " &gt; A A A C x 3 i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I i 6 L L g R n c V 7 A O 0 l C S d t k P z Y j I p l u L C H 3 C r f y b + g f 6 F d 8 Y p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h r y T D r O a 8 F a W l 5 Z X S u u l z Y 2 t 7 Z 3 y r t 7 r S z J R c C a Q R I m o u N 7 G Q t 5 z J q S y 5 B 1 U s G 8 y A 9 Z 2 x + f q 3 h 7 w k T G k / h a T l P W j b x h z A c 8 8 K S i p j 2 3 1 C t X n K q j l 7 0 I X A M q M K u R l F 9 w i z 4 S B M g R g S G G J B z C Q 0 b P D V w 4 S I n r Y k a c I M R 1 n O E e J d L m l M U o w y N 2 T N 8 h 7 W 4 M G 9 N e e W Z a H d A p I b 2 C l D a O S J N Q n i C s T r N 1 P N f O i v 3 N e 6 Y 9 1 d 2 m 9 P e N V 0 S s x I j Y v 3 T z z P / q V C 0 S A 5 z p G j j V l G p G V R c Y l 1 x 3 R d 3 c / l K V J I e U O I X 7 F B e E A 6 2 c 9 9 n W m k z X r n r r 6 f i b z l S s 2 g c m N 8 e 7 u i U N 2 P 0 5 z k X Q q l V d p + p e n V T q N T P q I g 5 w i G O a 5 y n q u E A D T f I e 4 R F P e L Y u r c S a W H e f q V b B a P b x b V k P H 0 o q k C Y = &lt; / l a t e x i t &gt; y 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T o R A e S A j J X f q b p 5 c p o 7 s M K O B c 9 Y = " &gt; A A A C x 3 i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I i 6 L L g R n c V 7 A O 0 l G Q 6 b U P z Y j I p l u L C H 3 C r f y b + g f 6 F d 8 Y p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h o G m X S c 1 4 K 1 t L y y u l Z c L 2 1 s b m 3 v l H f 3 W l m S C 8 a b L A k T 0 f G 9 j I d B z J s y k C H v p I J 7 k R / y t j 8 + V / H 2 h I s s S O J r O U 1 5 N / K G c T A I m C c V N e 3 V S r 1 y x a k 6 e t m L w D W g A r M a S f k F t + g j A U O O C B w x J O E Q H j J 6 b u D C Q U p c F z P i B K F A x z n u U S J t T l m c M j x i x / Q d 0 u 7 G s D H t l W e m 1 Y x O C e k V p L R x R J q E 8 g R h d Z q t 4 7 l 2 V u x v 3 j P t q e 4 2 p b 9 v v C J i J U b E / q W b Z / 5 X p 2 q R G O B M 1 x B Q T a l m V H X M u O S 6 K + r m 9 p e q J D m k x C n c p 7 g g z L R y 3 m d b a z J d u + q t p + N v O l O x a s 9 M b o 5 3 d U s a s P t z n I u g V a u 6 T t W 9 O q n U a 2 b U R R z g E M c 0 z 1 P U c Y E G m u Q 9 w i O e 8 G x d W o k 1 s e 4 + U</formula><formula xml:id="formula_5">7 s M K O B c 9 Y = " &gt; A A A C x 3 i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I i 6 L L g R n c V 7 A O 0 l G Q 6 b U P z Y j I p l u L C H 3 C r f y b + g f 6 F d 8 Y p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h o G m X S c 1 4 K 1 t L y y u l Z c L 2 1 s b m 3 v l H f 3 W l m S C 8 a b L A k T 0 f G 9 j I d B z J s y k C H v p I J 7 k R / y t j 8 + V / H 2 h I s s S O J r O U 1 5 N / K G c T A I m C c V N e 3 V S r 1 y x a k 6 e t m L w D W g A r M a S f k F t + g j A U O O C B w x J O E Q H j J 6 b u D C Q U p c F z P i B K F A x z n u U S J t T l m c M j x i x / Q d 0 u 7 G s D H t l W e m 1 Y x O C e k V p L R x R J q E 8 g R h d Z q t 4 7 l 2 V u x v 3 j P t q e 4 2 p b 9 v v C J i J U b E / q W b Z / 5 X p 2 q R G O B M 1 x B Q T a l m V H X M u O S 6 K + r m 9 p e q J D m k x C n c p 7 g g z L R y 3 m d b a z J d u + q t p + N v O l O x a s 9 M b o 5 3 d U s a s P t z n I u g V a u 6 T t W 9 O q n U a 2 b U R R z g E M c 0 z 1 P U c Y E G m u Q 9 w i O e 8 G x d W o k 1 s e 4 + U</formula><formula xml:id="formula_6">7 s M K O B c 9 Y = " &gt; A A A C x 3 i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I i 6 L L g R n c V 7 A O 0 l G Q 6 b U P z Y j I p l u L C H 3 C r f y b + g f 6 F d 8 Y p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h o G m X S c 1 4 K 1 t L y y u l Z c L 2 1 s b m 3 v l H f 3 W l m S C 8 a b L A k T 0 f G 9 j I d B z J s y k C H v p I J 7 k R / y t j 8 + V / H 2 h I s s S O J r O U 1 5 N / K G c T A I m C c V N e 3 V S r 1 y x a k 6 e t m L w D W g A r M a S f k F t + g j A U O O C B w x J O E Q H j J 6 b u D C Q U p c F z P i B K F A x z n u U S J t T l m c M j x i x / Q d 0 u 7 G s D H t l W e m 1 Y x O C e k V p L R x R J q E 8 g R h d Z q t 4 7 l 2 V u x v 3 j P t q e 4 2 p b 9 v v C J i J U b E / q W b Z / 5 X p 2 q R G O B M 1 x B Q T a l m V H X M u O S 6 K + r m 9 p e q J D m k x C n c p 7 g g z L R y 3 m d b a z J d u + q t p + N v O l O x a s 9 M b o 5 3 d U s a s P t z n I u g V a u 6 T t W 9 O q n U a 2 b U R R z g E M c 0 z 1 P U c Y E G m u Q 9 w i O e 8 G x d W o k 1 s e 4 + U 6 2 C 0 e z j 2 7 I e P g B M i 5 A n &lt; / l a t e x i t &gt; y 3 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 6 B Z b A K K J l v f Q B 0 I O a R 6 s p f 6 X Z w M = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I q 6 L L g p s u K t h V q K c l 0 W o f m R T J R S h H 8 A b f 6 a e I f 6 F 9 4 Z 5 y C W k Q n J D l z 7 j 1 n 5 t 7 r J 4 H I p O O 8 F q y F x a X l l e J q a W 1 9 Y 3 O r v L 3 T z u I 8 Z b z F 4 i B O r 3 w v 4 4 G I e E s K G f C r J O V e 6 A e 8 4 4 / P V L x z y 9 N M x N G l n C S 8 F 3 q j S A w F 8 y R R F 5 P + U b 9 c c a q O X v Y 8 c A 2 o w K x m X H 7 B N Q a I w Z A j B E c E S T i A h 4 y e L l w 4 S I j r Y U p c S k j o O M c 9 S q T N K Y t T h k f s m L 4 j 2 n U N G 9 F e e W Z a z e i U g N 6 U l D Y O S B N T X k p Y n W b r e K 6 d F f u b 9 1 R 7 q r t N 6 O 8 b r 5 B Y i R t i / 9 L N M v + r U 7 V I D H G q a x B U U 6 I Z V R 0 z L r n u i r q 5 / a U q S Q 4 J c Q o P K J 4 S Z l o 5 6 7 O t N Z m u X f X W 0 / E 3 n a l Y t W c m N 8 e 7 u i U N 2 P 0 5 z n n Q r l V d p + q e H 1 f q N T P q I v a w j 0 O a 5 w n q a K C J F n m P 8 I g n P F s N K 7 J y 6 + 4 z 1 S o Y z S 6 + L e v h A x B d k B Q = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 6 B Z b A K K J l v f Q B 0 I O a R 6 s p f 6 X Z w M = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I q 6 L L g p s u K t h V q K c l 0 W o f m R T J R S h H 8 A b f 6 a e I f 6 F 9 4 Z 5 y C W k Q n J D l z 7 j 1 n 5 t 7 r J 4 H I p O O 8 F q y F x a X l l e J q a W 1 9 Y 3 O r v L 3 T z u I 8 Z b z F 4 i B O r 3 w v 4 4 G I e E s K G f C r J O V e 6 A e 8 4 4 / P V L x z y 9 N M x N G l n C S 8 F 3 q j S A w F 8 y R R F 5 P + U b 9 c c a q O X v Y 8 c A 2 o w K x m X H 7 B N Q a I w Z A j B E c E S T i A h 4 y e L l w 4 S I j r Y U p c S k j o O M c 9 S q T N K Y t T h k f s m L 4 j 2 n U N G 9 F e e W Z a z e i U g N 6 U l D Y O S B N T X k p Y n W b r e K 6 d F f u b 9 1 R 7 q r t N 6 O 8 b r 5 B Y i R t i / 9 L N M v + r U 7 V I D H G q a x B U U 6 I Z V R 0 z L r n u i r q 5 / a U q S Q 4 J c Q o P K J 4 S Z l o 5 6 7 O t N Z m u X f X W 0 / E 3 n a l Y t W c m N 8 e 7 u i U N 2 P 0 5 z n n Q r l V d p + q e H 1 f q N T P q I v a w j 0 O a 5 w n q a K C J F n m P 8 I g n P F s N K 7 J y 6 + 4 z 1 S o Y z S 6 + L e v h A x B d k B Q = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 6 B Z b A K K J l v f Q B 0 I O a R 6 s p f 6 X Z w M = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I q 6 L L g p s u K t h V q K c l 0 W o f m R T J R S h H 8 A b f 6 a e I f 6 F 9 4 Z 5 y C W k Q n J D l z 7 j 1 n 5 t 7 r J 4 H I p O O 8 F q y F x a X l l e J q a W 1 9 Y 3 O r v L 3 T z u I 8 Z b z F 4 i B O r 3 w v 4 4 G I e E s K G f C r J O V e 6 A e 8 4 4 / P V L x z y 9 N M x N G l n C S 8 F 3 q j S A w F 8 y R R F 5 P + U b 9 c c a q O X v Y 8 c A 2 o w K x m X H 7 B N Q a I w Z A j B E c E S T i A h 4 y e L l w 4 S I j r Y U p c S k j o O M c 9 S q T N K Y t T h k f s m L 4 j 2 n U N G 9 F e e W Z a z e i U g N 6 U l D Y O S B N T X k p Y n W b r e K 6 d F f u b 9 1 R 7 q r t N 6 O 8 b r 5 B Y i R t i / 9 L N M v + r U 7 V I D H G q a x B U U 6 I Z V R 0 z L r n u i r q 5 / a U q S Q 4 J c Q o P K J 4 S Z l o 5 6 7 O t N Z m u X f X W 0 / E 3 n a l Y t W c</formula><p>m N 8 e 7 u i U N 2 P 0 5 z n n Q r l V d p + q e H 1 f q N T P q I v a w j 0 O a 5 w n q a K C J F n m P 8 I g n P F s N K 7 J y 6 + 4 z 1 S o Y z S 6 + L e v h A x B d k B Q = &lt; / l a t e x i t &gt; y 4 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 6 x L j T T z s k g </p><formula xml:id="formula_7">5 P K H q n W s I k u k V Y 9 2 0 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z J S 0 G X B T Z c V 7 Q O 0 l C S d 1 q F 5 M Z k o p Q j + g F v 9 N P E P 9 C + 8 M 0 5 B L a I T k p w 5 9 5 4 z c + / 1 0 5 B n 0 n F e C 9 b S 8 s r q W n G 9 t L G 5 t b 1 T 3 t 3 r Z E k u A t Y O k j A R P d / L W M h j 1 p Z c h q y X C u Z F f s i 6 / u R M x b u 3 T G Q 8 i S / l N G X 9 y B v H f M Q D T x J 1 M R 3 U B + W K U 3 X 0 s h e B a 0 A F Z r W S 8 g u u M U S C A D k i M M S Q h E N 4 y O i 5 g g s H K X F 9 z I g T h L i O M 9 y j R N q c s h h l e M R O 6 D u m 3 Z V h Y 9 o r z 0 y r A z o l p F e Q 0 s Y R a R L K E 4 T V a b a O 5 9 p Z s b 9 5 z 7 S n u t u U / r 7 x i o i V u C H 2 L 9 0 8 8 7 8 6 V Y v E C K e 6 B k 4 1 p Z p R 1 Q X G J d d d U T e 3 v 1 Q l y S E l T u E h x Q X h Q C v n f b a 1 J t O 1 q 9 5 6 O v 6 m M x W r 9 o H J z f G u b k k D d n + O c x F 0 a l X X q b r n 9 U q j Z k Z d x A E O c U z z P E E D T b T Q J u 8 x H v G E Z 6 t p x V Z u 3 X 2 m W g W j 2 c e 3 Z T 1 8 A B K 9 k B U = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 6 x L j T T z s k g 5 P K H q n W s I k u k V Y 9 2 0 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z J S 0 G X B T Z c V 7 Q O 0 l C S d 1 q F 5 M Z k o p Q j + g F v 9 N P E P 9 C + 8 M 0 5 B L a I T k p w 5 9 5 4 z c + / 1 0 5 B n 0 n F e C 9 b S 8 s r q W n G 9 t L G 5 t b 1 T 3 t 3 r Z E k u A t Y O k j A R P d / L W M h j 1 p Z c h q y X C u Z F f s i 6 / u R M x b u 3 T G Q 8 i S / l N G X 9 y B v H f M Q D T x J 1 M R 3 U B + W K U 3 X 0 s h e B a 0 A F Z r W S 8 g u u M U S C A D k i M M S Q h E N 4 y O i 5 g g s H K X F 9 z I g T h L i O M 9 y j R N q c s h h l e M R O 6 D u m 3 Z V h Y 9 o r z 0 y r A z o l p F e Q 0 s Y R a R L K E 4 T V a b a O 5 9 p Z s b 9 5 z 7 S n u t u U / r 7 x i o i V u C H 2 L 9 0 8 8 7 8 6 V Y v E C K e 6 B k 4 1 p Z p R 1 Q X G J d d d U T e 3 v 1 Q l y S E l T u E h x Q X h Q C v n f b a 1 J t O 1 q 9 5 6 O v 6 m M x W r 9 o H J z f G u b k k D d n + O c x F 0 a l X X q b r n 9 U q j Z k Z d x A E O c U z z P E E D T b T Q J u 8 x H v G E Z 6 t p x V Z u 3 X 2 m W g W j 2 c e 3 Z T 1 8 A B K 9 k B U = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 6 x L j T T z s k g 5 P K H q n W s I k u k V Y 9 2 0 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z J S 0 G X B T Z c V 7 Q O 0 l C S d 1 q F 5 M Z k o p Q j + g F v 9 N P E P 9 C + 8 M 0 5 B L a I T k p w 5 9 5 4 z c + / 1 0 5 B n 0 n F e C 9 b S 8 s r q W n G 9 t L G 5 t b 1 T 3 t 3 r Z E k u A t Y O k j A R P d / L W M h j 1 p Z c h q y X C u Z F f s i 6 / u R M x b u 3 T G Q 8 i S / l N G X 9 y B v H f M Q D T x J 1 M R 3 U B + W K U 3 X 0 s h e B a 0 A F Z r W S 8 g u u M U S C A D k i M M S Q h E N 4 y O i 5 g g s H K X F 9 z I g T h L i O M 9 y j R N q c s h h l e M R O 6 D u m 3 Z V h Y 9 o r z 0 y r A z o l p F e Q 0 s Y R a R L K E 4 T V a b a O 5 9 p Z s b 9 5 z 7 S n u t u U / r 7 x i o i V u C H 2 L 9 0 8 8 7 8 6 V Y v E C K e 6 B k 4 1 p Z p R 1 Q X G J d d d U T e 3 v 1 Q l y S E l T u E h x Q X h Q C v n f b a 1 J t O 1 q 9 5 6 O v 6 m M x W r 9 o H J z f G u b k k D d n + O c x F 0 a l X X q b r n 9 U q j Z k Z d x A E O c U z z P E E D T b T Q J u 8 x H v G E Z 6 t p x V Z u 3 X 2 m W g W j 2 c e 3 Z T 1 8 A B K 9 k B U = &lt; / l a t e x i t &gt; y 5 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " x d k k X H F E e f y f B w X 0 a b Q s / y j N / a U = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z K i 6 L L g p s u K t h V q K c l 0 W o f m R T J R S h</formula><formula xml:id="formula_8">Y i R t i / 9 L N M v + r U 7 V I D H G q a x B U U 6 I Z V R 0 z L r n u i r q 5 / a U q S Q 4 J c Q o P K J 4 S Z l o 5 6 7 O t N Z m u X f X W 0 / E 3 n a l Y t W</formula><p>c m N 8 e 7 u i U N 2 P 0 5 z n n Q r l V d p + q e H 1 X q N T P q I v a w j 0 O a 5 w n q a K C J F n m P 8 I g n P F s N K 7 J y 6 + 4 z 1 S o Y z S 6 + L e v h A x U d k B Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " </p><formula xml:id="formula_9">x d k k X H F E e f y f B w X 0 a b Q s / y j N / a U = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z K i 6 L L g p s u K t h V q K c l 0 W o f m R T J R S h</formula><formula xml:id="formula_10">Y i R t i / 9 L N M v + r U 7 V I D H G q a x B U U 6 I Z V R 0 z L r n u i r q 5 / a U q S Q 4 J c Q o P K J 4 S Z l o 5 6 7 O t N Z m u X f X W 0 / E 3 n a l Y t W</formula><p>c m N 8 e 7 u i U N 2 P 0 5 z n n Q r l V d p + q e H 1 X q N T P q I v a w j 0 O a 5 w n q a K C J F n m P 8 I g n P F s N K 7 J y 6 + 4 z 1 S o Y z S 6 + L e v h A x U d k B Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " </p><formula xml:id="formula_11">x d k k X H F E e f y f B w X 0 a b Q s / y j N / a U = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z K i 6 L L g p s u K t h V q K c l 0 W o f m R T J R S h</formula><formula xml:id="formula_12">Y i R t i / 9 L N M v + r U 7 V I D H G q a x B U U 6 I Z V R 0 z L</formula><p>r n u i r q 5 / a U q S Q 4 J c Q o P K J 4 S Z l o 5 6 7 O t N Z m u X f X W 0 / E 3 n a l Y t W c m N 8 e 7 u i U N 2 P 0 5 z n n Q r l V d p + q e H 1 X q N T P q I v a w j 0 O a 5 w n q a K C J F n m P 8 I g n P F s N K 7 J y 6 + 4 z 1 S o Y z S 6 + L e v h A x U d k B Y = &lt; / l a t e x i t &gt; x 4 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " r F</p><formula xml:id="formula_13">F U q Q Z l V A D K 5 + C 9 H 2 U / T l v p H k c = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z J S 0 G X B T Z c V 7 Q N q K c l 0 W o e m S Z h M 1 F I E f 8 C t f p r 4 B / o X 3 h l T U I v o h C R n z r 3 n z N x 7 / T g Q i X K c 1 5 y 1 t L y y u p Z f L 2 x s b m 3 v F H f 3 W k m U S s a b L A o i 2 f G 9 h A c i 5 E 0 l V M A 7 s e T e x A 9 4 2 x + f 6 X j 7 h s t E R O G l m s a 8 N / F G o R g K 5 i m i L u 7 6 1 X 6 x 5 J Q d s + x F 4 G a g h G w 1 o u I L r j B A B I Y U E 3 C E U I Q D e E j o 6 c K F g 5 i 4 H m b E S U L C x D n u U S B t S l m c M j x i x / Q d 0 a 6 b s S H t t W d i 1 I x O C e i V p L R x R J q I 8 i R h f Z p t 4 q l x 1 u x v 3 j P j q e 8 2 p b + f e U 2 I V b g m 9 i / d P P O / O l 2 L w h C n p g Z B N c W G 0 d W x z C U 1 X d E 3 t 7 9 U p c g h J k 7 j A c U l Y W a U 8 z 7 b R p O Y 2 n V v P R N / M 5 m a 1 X u W 5 a Z 4 1 7 e k A b s / x 7 k I W p W y 6 5 T d 8 2 q p V s l G n c c B D n F M 8 z x B D X U 0 0 C T v E R 7 x h G e r b o V W a t 1 + p l q 5 T L O P b 8 t 6 + A A Q W 5 A U &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " r F F U q Q Z l V A D K 5 + C 9 H 2 U / T l v p H k c = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z J S 0 G X B T Z c V 7 Q N q K c l 0 W o e m S Z h M 1 F I E f 8 C t f p r 4 B / o X 3 h l T U I v o h C R n z r 3 n z N x 7 / T g Q i X K c 1 5 y 1 t L y y u p Z f L 2 x s b m 3 v F H f 3 W k m U S s a b L A o i 2 f G 9 h A c i 5 E 0 l V M A 7 s e T e x A 9 4 2 x + f 6 X j 7 h s t E R O G l m s a 8 N / F G o R g K 5 i m i L u 7 6 1 X 6 x 5 J Q d s + x F 4 G a g h G w 1 o u I L r j B A B I Y U E 3 C E U I Q D e E j o 6 c K F g 5 i 4 H m b E S U L C x D n u U S B t S l m c M j x i x / Q d 0 a 6 b s S H t t W d i 1 I x O C e i V p L R x R J q I 8 i R h f Z p t 4 q l x 1 u x v 3 j P j q e 8 2 p b + f e U 2 I V b g m 9 i / d P P O / O l 2 L w h C n p g Z B N c W G 0 d W x z C U 1 X d E 3 t 7 9 U p c g h J k 7 j A c U l Y W a U 8 z 7 b R p O Y 2 n V v P R N / M 5 m a 1 X u W 5 a Z 4 1 7 e k A b s / x 7 k I W p W y 6 5 T d 8 2 q p V s l G n c c B D n F M 8 z x B D X U 0 0 C T v E R 7 x h G e r b o V W a t 1 + p l q 5 T L O P b 8 t 6 + A A Q W 5 A U &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " r F F U q Q Z l V A D K 5 + C 9 H 2 U / T l v p H k c = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z J S 0 G X B T Z c V 7 Q N q K c l 0 W o e m S Z h M 1 F I E f 8 C t f p r 4 B / o X 3 h l T U I v o h C R n z r 3 n z N x 7 / T g Q i X K c 1 5 y 1 t L y y u p Z f L 2 x s b m 3 v F H f 3 W k m U S s a b L A o i 2 f G 9 h A c i 5 E 0 l V M A 7 s e T e x A 9 4 2 x + f 6 X j 7 h s t E R O G l m s a 8 N / F G o R g K 5 i m i L u 7 6 1 X 6 x 5 J Q d s + x F 4 G a g h G w 1 o u I L r j B A B I Y U E 3 C E U I Q D e E j o 6 c K F g 5 i 4 H m b E S U L C x D n u U S B t S l m c M j x i x / Q d 0 a 6 b s S H t t W d i 1 I x O C e i V p L R x R J q I 8 i R h f Z p t 4 q l x 1 u x v 3 j P j q e 8 2 p b + f e U 2 I V b g m 9 i / d P P O / O l 2 L w h C n p g Z B N c W G 0 d W x z C U 1 X d E 3 t 7 9 U p c g h J k 7 j A c U l Y W a U 8 z 7 b R p O Y 2 n V v P R N / M 5 m a 1 X u W 5 a Z 4 1 7 e k A b s / x 7 k I W p W y 6 5 T d 8 2 q p V s l G n c c B D n F M 8 z x B D X U 0 0 C T v E R 7 x h G e r b o V W a t 1 + p l q 5 T L O P b 8 t 6 + A A Q W 5 A U &lt; / l a t e x i t &gt; x 5 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 R R w X c H / u v F A U 8 k 4 3 0 Z o I H v g O s k = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z K i 6 L L g p s u K 9 g G 1 l G Q 6 r U P T J E w m a i m C P + B W P 0 3 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h y I R D n O a 8 5 a W F x a X s m v F t b W N z a 3 i t s 7 z S R K J e M N F g W R b P t e w g M R 8 o Y S K u D t W H J v 7 A e 8 5 Y / O d L x 1 w 2 U i o v B S T W L e H X v D U A w E 8 x R R F 3 e 9 4 1 6 x 5 J Q d s + x 5 4 G a g h G z V o + I L r t B H B I Y U Y 3 C E U I Q D e E j o 6 c C F g 5 i 4 L q b E S U L C x D n u U S B t S l m c M j x i R / Q d 0 q 6 T s S H t t W d i 1 I x O C e i V p L R x Q J q I 8 i R h f Z p t 4 q l x 1 u x v 3 l P j q e 8 2 o b + f e Y 2 J V b g m 9 i / d L P O / O l 2 L w g C n p g Z B N c W G 0 d W x z C U 1 X d E 3 t 7 9 U p c g h J k 7 j P s U l Y W a U s z 7 b R p O Y 2 n V v P R N / M 5 m a 1 X u W 5 a Z 4 1 7 e k A b s / x z k P m p W y 6 5 T d 8 6 N S t Z K N O o 8 9 7 O O Q 5 n m C K m q o o 0 H e Q z z i C c 9 W z Q q t 1 L r 9 T L V y m W Y X 3 5 b 1 8 A E S u 5 A V &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 R R w X c H / u v F A U 8 k 4 3 0 Z o I H v g O s k = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z K i 6 L L g p s u K 9 g G 1 l G Q 6 r U P T J E w m a i m C P + B W P 0 3 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h y I R D n O a 8 5 a W F x a X s m v F t b W N z a 3 i t s 7 z S R K J e M N F g W R b P t e w g M R 8 o Y S K u D t W H J v 7 A e 8 5 Y / O d L x 1 w 2 U i o v B S T W L e H X v D U A w E 8 x R R F 3 e 9 4 1 6 x 5 J Q d s + x 5 4 G a g h G z V o + I L r t B H B I Y U Y 3 C E U I Q D e E j o 6 c C F g 5 i 4 L q b E S U L C x D n u U S B t S l m c M j x i R / Q d 0 q 6 T s S H t t W d i 1 I x O C e i V p L R x Q J q I 8 i R h f Z p t 4 q l x 1 u x v 3 l P j q e 8 2 o b + f e Y 2 J V b g m 9 i / d L P O / O l 2 L w g C n p g Z B N c W G 0 d W x z C U 1 X d E 3 t 7 9 U p c g h J k 7 j P s U l Y W a U s z 7 b R p O Y 2 n V v P R N / M 5 m a 1 X u W 5 a Z 4 1 7 e k A b s / x z k P m p W y 6 5 T d 8 6 N S t Z K N O o 8 9 7 O O Q 5 n m C K m q o o 0 H e Q z z i C c 9 W z Q q t 1 L r 9 T L V y m W Y X 3 5 b 1 8 A E S u 5 A V &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 R R w X c H / u v F A U 8 k 4 3 0 Z o I H v g O s k = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z K i 6 L L g p s u K 9 g G 1 l G Q 6 r U P T J E w m a i m C P + B W P 0 3 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h y I R D n O a 8 5 a W F x a X s m v F t b W N z a 3 i t s 7 z S R K J e M N F g W R b P t e w g M R 8 o Y S K u D t W H J v 7 A e 8 5 Y / O d L x 1 w 2 U i o v B S T W L e H X v D U A w E 8 x R R F 3 e 9 4 1 6 x 5 J Q d s + x 5 4 G a g h G z V o + I L r t B H B I Y U Y 3 C E U I Q D e E j o 6 c C F g 5 i 4 L q b E S U L C x D n u U S B t S l m c M j x i R / Q d 0 q 6 T s S H t t W d i 1 I x O C e i V p L R x Q J q I 8 i R h f Z p t 4 q l x 1 u x v 3 l P j q e 8 2 o b + f e Y 2 J V b g m 9 i / d L P O / O l 2 L w g C n p g Z B N c W G 0 d W x z C U 1 X d E 3 t 7 9 U p c g h J k 7 j P s U l Y W a U s z 7 b R p O Y 2 n V v P R N / M 5 m a 1 X u W 5 a Z 4 1 7 e k A b s / x z k P m p W y 6 5 T d 8 6 N S t Z K N O o 8 9 7 O O Q 5 n m C K m q o o 0 H e Q z z i C c 9 W z Q q t 1 L r 9 T L V y m W Y X 3 5 b 1 8 A E S u 5 A V &lt; / l a t e x i t &gt; (a) Full. x 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I E E G y U V A W x T a u u X 3 x + X f M m 7 U r O c = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J N N p D c 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h k E m H e e 1 Y C 0 s L i 2 v F F f X 1 j c 2 t 7 Z L O 7 u t L M k F 4 0 2 W h I n o + F 7 G w y D m T R n I k H d S w b 3 I D 3 n b H 5 + p e P u G i y x I 4 k s 5 S X k v 8 k Z x M A y Y J 4 m 6 u O u 7 / V L Z q T h 6 2 f P A N a A M s x p J 6 Q V X G C A B Q 4 4 I H D E k 4 R A e M n q 6 c O E g J a 6 H K X G C U K D j H P d Y I 2 1 O W Z w y P G L H 9 B 3 R r m v Y m P b K M 9 N q R q e E 9 A p S 2 j g k T U J 5 g r A 6 z d b x X D s r 9 j f v q f Z U d 5 v Q 3 z d e E b E S 1 8 T + p Z t l / l e n a p E Y 4 l T X E F B N q W Z U d c y 4 5 L o r 6 u b 2 l 6 o k O a T E K T y g u C D M t H L W Z 1 t r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q z 0 x u j n d 1 S x q w + 3 O c 8 6 B V r b h O x T 0 / L t e q Z t R F 7 O M A R z T P E 9 R Q R w N N 8 h 7 h E U 9 4 t u p W b O X W 7 W e q V T C a P X x b 1 s M H C T u Q E Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I E E G y U V A W x T a u u X 3 x + X f M m 7 U r O c = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J N N p D c 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h k E m H e e 1 Y C 0 s L i 2 v F F f X 1 j c 2 t 7 Z L O 7 u t L M k F 4 0 2 W h I n o + F 7 G w y D m T R n I k H d S w b 3 I D 3 n b H 5 + p e P u G i y x I 4 k s 5 S X k v 8 k Z x M A y Y J 4 m 6 u O u 7 / V L Z q T h 6 2 f P A N a A M s x p J 6 Q V X G C A B Q 4 4 I H D E k 4 R A e M n q 6 c O E g J a 6 H K X G C U K D j H P d Y I 2 1 O W Z w y P G L H 9 B 3 R r m v Y m P b K M 9 N q R q e E 9 A p S 2 j g k T U J 5 g r A 6 z d b x X D s r 9 j f v q f Z U d 5 v Q 3 z d e E b E S 1 8 T + p Z t l / l e n a p E Y 4 l T X E F B N q W Z U d c y 4 5 L o r 6 u b 2 l 6 o k O a T E K T y g u C D M t H L W Z 1 t r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q z 0 x u j n d 1 S x q w + 3 O c 8 6 B V r b h O x T 0 / L t e q Z t R F 7 O M A R z T P E 9 R Q R w N N 8 h 7 h E U 9 4 t u p W b O X W 7 W e q V T C a P X x b 1 s M H C T u Q E Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I E E G y U V A W x T a u u X 3 x + X f M m 7 U r O c = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J N N p D c 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h k E m H e e 1 Y C 0 s L i 2 v F F f X 1 j c 2 t 7 Z L O 7 u t L M k F 4 0 2 W h I n o + F 7 G w y D m T R n I k H d S w b 3 I D 3 n b H 5 + p e P u G i y x I 4 k s 5 S X k v 8 k Z x M A y Y J 4 m 6 u O u 7 / V L Z q T h 6 2 f P A N a A M s x p J 6 Q V X G C A B Q 4 4 I H D E k 4 R A e M n q 6 c O E g J a 6 H K X G C U K D j H P d Y I 2 1 O W Z w y P G L H 9 B 3 R r m v Y m P b K M 9 N q R q e E 9 A p S 2 j g k T U J 5 g r A 6 z d b x X D s r 9 j f v q f Z U d 5 v Q 3 z d e E b E S 1 8 T + p Z t l / l e n a p E Y 4 l T X E F B N q W Z U d c y 4 5 L o r 6 u b 2 l 6 o k O a T E K T y g u C D M t H L W Z 1 t r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q z 0 x u j n d 1 S x q w + 3 O c 8 6 B V r b h O x T 0 / L t e q Z t R F 7 O M A R z T P E 9 R Q R w N N 8 h 7 h E U 9 4 t u p W b O X W 7 W e q V T C a P X x b 1 s M H C T u Q E Q = = &lt; / l a t e x i t &gt; x 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w + O 2 A V A N 5 t G Y s 3 Q A W 4 f 1 x G 8 R q I 4 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J O m 0 D s 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h j y T j v N a s B Y W l 5 Z X i q t r 6 x u b W 9 u l n d 1 W l u Q i Y M 0 g C R P R 8 b 2 M h T x m T c l l y D q p Y F 7 k h 6 z t j 8 9 U v H 3 D R M a T + F J O U t a L v F H M h z z w J F E X d / 1 q v 1 R 2 K o 5 e 9 j x w D S j D r E Z S e s E V B k g Q I E c E h h i S c A g P G T 1 d u H C Q E t f D l D h B i O s 4 w z 3 W S J t T F q M M j 9 g x f U e 0 6 x o 2 p r 3 y z L Q 6 o F N C e g U p b R y S J q E 8 Q V i d Z u t 4 r p 0 V + 5 v 3 V H u q u 0 3 o 7 x u v i F i J a 2 L / 0 s 0 y / 6 t T t U g M c a p r 4 F R T q h l V X W B c c t 0 V d X P 7 S 1 W S H F L i F B 5 Q X B A O t H L W Z 1 t</formula><p>r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q H 5 j c H O / q l j R g 9 + c 4 5 0 G r W n G d i n t + X K 5 V z a i L 2 M c B j m i e J 6 i h j g a a 5 D 3 C I 5 7 w b N W t 2 M q t 2 8 9 U q 2 A 0 e / i 2 r I c P C 5 u Q E g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w</p><formula xml:id="formula_14">+ O 2 A V A N 5 t G Y s 3 Q A W 4 f 1 x G 8 R q I 4 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J O m 0 D s 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h j y T j v N a s B Y W l 5 Z X i q t r 6 x u b W 9 u l n d 1 W l u Q i Y M 0 g C R P R 8 b 2 M h T x m T c l l y D q p Y F 7 k h 6 z t j 8 9 U v H 3 D R M a T + F J O U t a L v F H M h z z w J F E X d / 1 q v 1 R 2 K o 5 e 9 j x w D S j D r E Z S e s E V B k g Q I E c E h h i S c A g P G T 1 d u H C Q E t f D l D h B i O s 4 w z 3 W S J t T F q M M j 9 g x f U e 0 6 x o 2 p r 3 y z L Q 6 o F N C e g U p b R y S J q E 8 Q V i d Z u t 4 r p 0 V + 5 v 3 V H u q u 0 3 o 7 x u v i F i J a 2 L / 0 s 0 y / 6 t T t U g M c a p r 4 F R T q h l V X W B c c t 0 V d X P 7 S 1 W S H F L i F B 5 Q X B A O t H L W Z 1 t</formula><p>r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q H 5 j c H O / q l j R g 9 + c 4 5 0 G r W n G d i n t + X K 5 V z a i L 2 M c B j m i e J 6 i h j g a a 5 D 3 C I 5 7 w b N W t 2 M q t 2 8 9 U q 2 A 0 e / i 2 r I c P C 5 u Q E g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w</p><formula xml:id="formula_15">+ O 2 A V A N 5 t G Y s 3 Q A W 4 f 1 x G 8 R q I 4 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J O m 0 D s 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h j y T j v N a s B Y W l 5 Z X i q t r 6 x u b W 9 u l n d 1 W l u Q i Y M 0 g C R P R 8 b 2 M h T x m T c l l y D q p Y F 7 k h 6 z t j 8 9 U v H 3 D R M a T + F J O U t a L v F H M h z z w J F E X d / 1 q v 1 R 2 K o 5 e 9 j x w D S j D r E Z S e s E V B k g Q I E c E h h i S c A g P G T 1 d u H C Q E t f D l D h B i O s 4 w z 3 W S J t T F q M M j 9 g x f U e 0 6 x o 2 p r 3 y z L Q 6 o F N C e g U p b R y S J q E 8 Q V i d Z u t 4 r p 0 V + 5 v 3 V H u q u 0 3 o 7 x u v i F i J a 2 L / 0 s 0 y / 6 t T t U g M c a p r 4 F R T q h l V X W B c c t 0 V d X P 7 S 1 W S H F L i F B 5 Q X B A O t H L W Z 1 t</formula><p>r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q H 5 j c H O / q l j R g 9 + c 4 5 0 G r W n G d i n t + X K 5 V z a i L 2 M c B j m i e J 6 i h j g a a 5 D 3 C I 5 7 w b N W t 2 M q t 2 8 9 U q 2 A 0 e / i 2 r I c P C 5 u Q E g = = &lt; / l a t e x i t &gt; x 3 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C L X q B p c I l H 0 j f t D 1 W T P R S D D H 1 d 0 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I q 6 L L g p s u K 9 g G 1 l G Q 6 r U P T J E w m a i m C P + B W P 0 3 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h y</p><formula xml:id="formula_16">I R D n O a 8 5 a W F x a X s m v F t b W N z a 3 i t s 7 z S R K J e M N F g W R b P t e w g M R 8 o Y S K u D t W H J v 7 A e 8 5 Y / O d L x 1 w 2 U i o v B S T W L e H X v D U A w E 8 x R R F 3 e 9 o 1 6 x 5 J Q d s + x 5 4 G a g h G z V o + I L r t B H B I Y U Y 3 C E U I Q D e E j o 6 c C F g 5 i 4 L q b E S U L C x D n u U S B t S l m c M j x i R / Q d 0 q 6 T s S H t t W d i 1 I x O C e i V p L R x Q J q I 8 i R h f Z p t 4 q l x 1 u x v 3 l P j q e 8 2 o b + f e Y 2 J V b g m 9 i / d L P O / O l 2 L w g C n p g Z B N c W G 0 d W x z C U 1 X d E 3 t 7 9 U p c g h J k 7 j P s U l Y W a U s z 7 b R p O Y 2 n V v P R N / M 5 m a 1 X u W 5 a Z 4 1 7 e k A b s / x z k P m p W y 6 5 T d 8 + N S t Z K N O o 8 9 7 O O Q 5 n m C K m q o o 0 H e Q z z i C c 9 W z Q q t 1 L r 9 T L V y m W Y X 3 5 b 1 8 A E N + 5 A T &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C L X q B p c I l H 0 j f t D 1 W T P R S D D H 1 d 0 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I q 6 L L g p s u K 9 g G 1 l G Q 6 r U P T J E w m a i m C P + B W P 0 3 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h y I R D n O a 8 5 a W F x a X s m v F t b W N z a 3 i t s 7 z S R K J e M N F g W R b P t e w g M R 8 o Y S K u D t W H J v 7 A e 8 5 Y / O d L x 1 w 2 U i o v B S T W L e H X v D U A w E 8 x R R F 3 e 9 o 1 6 x 5 J Q d s + x 5 4 G a g h G z V o + I L r t B H B I Y U Y 3 C E U I Q D e E j o 6 c C F g 5 i 4 L q b E S U L C x D n u U S B t S l m c M j x i R / Q d 0 q 6 T s S H t t W d i 1 I x O C e i V p L R x Q J q I 8 i R h f Z p t 4 q l x 1 u x v 3 l P j q e 8 2 o b + f e Y 2 J V b g m 9 i / d L P O / O l 2 L w g C n p g Z B N c W G 0 d W x z C U 1 X d E 3 t 7 9 U p c g h J k 7 j P s U l Y W a U s z 7 b R p O Y 2 n V v P R N / M 5 m a 1 X u W 5 a Z 4 1 7 e k A b s / x z k P m p W y 6 5 T d 8 + N S t Z K N O o 8 9 7 O O Q 5 n m C K m q o o 0 H e Q z z i C c 9 W z Q q t 1 L r 9 T L V y m W Y X 3 5 b 1 8 A E N + 5 A T &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C L X q B p c I l H 0 j f t D 1 W T P R S D D H 1 d 0 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I q 6 L L g p s u K 9 g G 1 l G Q 6 r U P T J E w m a i m C P + B W P 0 3 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h y I R D n O a 8 5 a W F x a X s m v F t b W N z a 3 i t s 7 z S R K J e M N F g W R b P t e w g M R 8 o Y S K u D t W H J v 7 A e 8 5 Y / O d L x 1 w 2 U i o v B S T W L e H X v D U A w E 8 x R R F 3 e 9 o 1 6 x 5 J Q d s + x 5 4 G a g h G z V o + I L r t B H B I Y U Y 3 C E U I Q D e E j o 6 c C F g 5 i 4 L q b E S U L C x D n u U S B t S l m c M j x i R / Q d 0 q 6 T s S H t t W d i 1 I x O C e i V p L R x Q J q I 8 i R h f Z p t 4 q l x 1 u x v 3 l P j q e 8 2 o b + f e Y 2 J V b g m 9 i / d L P O / O l 2 L w g C n p g Z B N c W G 0 d W x z C U 1 X d E 3 t 7 9 U p c g h J k 7 j P s U l Y W a U s z 7 b R p O Y 2 n V v P R N / M 5 m a 1 X u W 5 a Z 4 1 7 e k A b s / x z k P m p W y 6 5 T d 8 + N S t Z K N O o 8 9 7 O O Q 5 n m C K m q o o 0 H e Q z z i C c 9 W z Q q t 1 L r 9 T L V y m W Y X 3 5 b 1 8 A E N + 5 A T &lt; / l a t e x i t &gt; x 4 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " r F F U q Q Z l V A D K 5 + C 9 H 2 U / T l v p H k c = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z J S 0 G X B T Z c V 7 Q N q K c l 0 W o e m S Z h M 1 F I E f 8 C t f p r 4 B / o X 3 h l T U I v o h C R n z r 3 n z N x 7 / T g Q i X K c 1 5 y 1 t L y y u p Z f L 2 x s b m 3 v F H f 3 W k m U S s a b L A o i 2 f G 9 h A c i 5 E 0 l V M A 7 s e T e x A 9 4 2 x + f 6 X j 7 h s t E R O G l m s a 8 N / F G o R g K 5 i m i L u 7 6 1 X 6 x 5 J Q d s + x F 4 G a g h G w 1 o u I L r j B A B I Y U E 3 C E U I Q D e E j o 6 c K F g 5 i 4 H m b E S U L C x D n u U S B t S l m c M j x i x / Q d 0 a 6 b s S H t t W d i 1 I x O C e i V p L R x R J q I 8 i R h f Z p t 4 q l x 1 u x v 3 j P j q e 8 2 p b + f e U 2 I V b g m 9 i / d P P O / O l 2 L w h C n p g Z B N c W G 0 d W x z C U 1 X d E 3 t 7 9 U p c g h J k 7 j A c U l Y W a U 8 z 7 b R p O Y 2 n V v P R N / M 5 m a 1 X u W 5 a Z 4 1 7 e k A b s / x 7 k I W p W y 6 5 T d 8 2 q p V s l G n c c B D n F M 8 z x B D X U 0 0 C T v E R 7 x h G e r b o V W a t 1 + p l q 5 T L O P b 8 t 6 + A A Q W 5 A U &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " r F F U q Q Z l V A D K 5 + C 9 H 2 U / T l v p H k c = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z J S 0 G X B T Z c V 7 Q N q K c l 0 W o e m S Z h M 1 F I E f 8 C t f p r 4 B / o X 3 h l T U I v o h C R n z r 3 n z N x 7 / T g Q i X K c 1 5 y 1 t L y y u p Z f L 2 x s b m 3 v F H f 3 W k m U S s a b L A o i 2 f G 9 h A c i 5 E 0 l V M A 7 s e T e x A 9 4 2 x + f 6 X j 7 h s t E R O G l m s a 8 N / F G o R g K 5 i m i L u 7 6 1 X 6 x 5 J Q d s + x F 4 G a g h G w 1 o u I L r j B A B I Y U E 3 C E U I Q D e E j o 6 c K F g 5 i 4 H m b E S U L C x D n u U S B t S l m c M j x i x / Q d 0 a 6 b s S H t t W d i 1 I x O C e i V p L R x R J q I 8 i R h f Z p t 4 q l x 1 u x v 3 j P j q e 8 2 p b + f e U 2 I V b g m 9 i / d P P O / O l 2 L w h C n p g Z B N c W G 0 d W x z C U 1 X d E 3 t 7 9 U p c g h J k 7 j A c U l Y W a U 8 z 7 b R p O Y 2 n V v P R N / M 5 m a 1 X u W 5 a Z 4 1 7 e k A b s / x 7 k I W p W y 6 5 T d 8 2 q p V s l G n c c B D n F M 8 z x B D X U 0 0 C T v E R 7 x h G e r b o V W a t 1 + p l q 5 T L O P b 8 t 6 + A A Q W 5 A U &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " r F F U q Q Z l V A D K 5 + C 9 H 2 U / T l v p H k c = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z J S 0 G X B T Z c V 7 Q N q K c l 0 W o e m S Z h M 1 F I E f 8 C t f p r 4 B / o X 3 h l T U I v o h C R n z r 3 n z N x 7 / T g Q i X K c 1 5 y 1 t L y y u p Z f L 2 x s b m 3 v F H f 3 W k m U S s a b L A o i 2 f G 9 h A c i 5 E 0 l V M A 7 s e T e x A 9 4 2 x + f 6 X j 7 h s t E R O G l m s a 8 N / F G o R g K 5 i m i L u 7 6 1 X 6 x 5 J Q d s + x F 4 G a g h G w 1 o u I L r j B A B I Y U E 3 C E U I Q D e E j o 6 c K F g 5 i 4 H m b E S U L C x D n u U S B t S l m c M j x i x / Q d 0 a 6 b s S H t t W d i 1 I x O C e i V p L R x R J q I 8 i R h f Z p t 4 q l x 1 u x v 3 j P j q e 8 2 p b + f e U 2 I V b g m 9 i / d P P O / O l 2 L w h C n p g Z B N c W G 0 d W x z C U 1 X d E 3 t 7 9 U p c g h J k 7 j A c U l Y W a U 8 z 7 b R p O Y 2 n V v P R N / M 5 m a 1 X u W 5 a Z 4 1 7 e k A b s / x 7 k I W p W y 6 5 T d 8 2 q p V s l G n c c B D n F M 8 z x B D X U 0 0 C T v E R 7 x h G e r b o V W a t 1 + p l q 5 T L O P b 8 t 6 + A A Q W 5 A U &lt; / l a t e x i t &gt; x 5 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 R R w X c H / u v F A U 8 k 4 3 0 Z o I H v g O s k = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z K i 6 L L g p s u K 9 g G 1 l G Q 6 r U P T J E w m a i m C P + B W P 0 3 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h y I R D n O a 8 5 a W F x a X s m v F t b W N z a 3 i t s 7 z S R K J e M N F g W R b P t e w g M R 8 o Y S K u D t W H J v 7 A e 8 5 Y / O d L x 1 w 2 U i o v B S T W L e H X v D U A w E 8 x R R F 3 e 9 4 1 6 x 5 J Q d s + x 5 4 G a g h G z V o + I L r t B H B I Y U Y 3 C E U I Q D e E j o 6 c C F g 5 i 4 L q b E S U L C x D n u U S B t S l m c M j x i R / Q d 0 q 6 T s S H t t W d i 1 I x O C e i V p L R x Q J q I 8 i R h f Z p t 4 q l x 1 u x v 3 l P j q e 8 2 o b + f e Y 2 J V b g m 9 i / d L P O / O l 2 L w g C n p g Z B N c W G 0 d W x z C U 1 X d E 3 t 7 9 U p c g h J k 7 j P s U l Y W a U s z 7 b R p O Y 2 n V v P R N / M 5 m a 1 X u W 5 a Z 4 1 7 e k A b s / x z k P m p W y 6 5 T d 8 6 N S t Z K N O o 8 9 7 O O Q 5 n m C K m q o o 0 H e Q z z i C c 9 W z Q q t 1 L r 9 T L V y m W Y X 3 5 b 1 8 A E S u 5 A V &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 R R w X c H / u v F A U 8 k 4 3 0 Z o I H v g O s k = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z K i 6 L L g p s u K 9 g G 1 l G Q 6 r U P T J E w m a i m C P + B W P 0 3 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h y I R D n O a 8 5 a W F x a X s m v F t b W N z a 3 i t s 7 z S R K J e M N F g W R b P t e w g M R 8 o Y S K u D t W H J v 7 A e 8 5 Y / O d L x 1 w 2 U i o v B S T W L e H X v D U A w E 8 x R R F 3 e 9 4 1 6 x 5 J Q d s + x 5 4 G a g h G z V o + I L r t B H B I Y U Y 3 C E U I Q D e E j o 6 c C F g 5 i 4 L q b E S U L C x D n u U S B t S l m c M j x i R / Q d 0 q 6 T s S H t t W d i 1 I x O C e i V p L R x Q J q I 8 i R h f Z p t 4 q l x 1 u x v 3 l P j q e 8 2 o b + f e Y 2 J V b g m 9 i / d L P O / O l 2 L w g C n p g Z B N c W G 0 d W x z C U 1 X d E 3 t 7 9 U p c g h J k 7 j P s U l Y W a U s z 7 b R p O Y 2 n V v P R N / M 5 m a 1 X u W 5 a Z 4 1 7 e k A b s / x z k P m p W y 6 5 T d 8 6 N S t Z K N O o 8 9 7 O O Q 5 n m C K m q o o 0 H e Q z z i C c 9 W z Q q t 1 L r 9 T L V y m W Y X 3 5 b 1 8 A E S u 5 A V &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 R R w X c H / u v F A U 8 k 4 3 0 Z o I H v g O s k = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z K i 6 L L g p s u K 9 g G 1 l G Q 6 r U P T J E w m a i m C P + B W P 0 3 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h y I R D n O a 8 5 a W F x a X s m v F t b W N z a 3 i t s 7 z S R K J e M N F g W R b P t e w g M R 8 o Y S K u D t W H J v 7 A e 8 5 Y / O d L x 1 w 2 U i o v B S T W L e H X v D U A w E 8 x R R F 3 e 9 4 1 6 x 5 J Q d s + x 5 4 G a g h G z V o + I L r t B H B I Y U Y 3 C E U I Q D e E j o 6 c C F g 5 i 4 L q b E S U L C x D n u U S B t S l m c M j x i R / Q d 0 q 6 T s S H t t W d i 1 I x O C e i V p L R x Q J q I 8 i R h f Z p t 4 q l x 1 u x v 3 l P j q e 8 2 o b + f e Y 2 J V b g m 9 i / d L P O / O l 2 L w g C n p g Z B N c W G 0 d W x z C U 1 X d E 3 t 7 9 U p c g h J k 7 j P s U l Y W a U s z 7 b R p O Y 2 n V v P R N / M 5 m a 1 X u W 5 a Z 4 1 7 e k A b s / x z k P m p W y 6 5 T d 8 6 N S t Z K N O o 8 9 7 O O Q 5 n m C K m q o o 0 H e Q z z i C c 9 W z Q q t 1 L r 9 T L V y m W Y X 3 5 b 1 8 A E S u 5 A V &lt; / l a t e x i t &gt; (b) Diagonal. x 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I E E G y U V A W x T a u u X 3 x + X f M m 7 U r O c = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J N N p D c 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h k E m H e e 1 Y C 0 s L i 2 v F F f X 1 j c 2 t 7 Z L O 7 u t L M k F 4 0 2 W h I n o + F 7 G w y D m T R n I k H d S w b 3 I D 3 n b H 5 + p e P u G i y x I 4 k s 5 S X k v 8 k Z x M A y Y J 4 m 6 u O u 7 / V L Z q T h 6 2 f P A N a A M s x p J 6 Q V X G C A B Q 4 4 I H D E k 4 R A e M n q 6 c O E g J a 6 H K X G C U K D j H P d Y I 2 1 O W Z w y P G L H 9 B 3 R r m v Y m P b K M 9 N q R q e E 9 A p S 2 j g k T U J 5 g r A 6 z d b x X D s r 9 j f v q f Z U d 5 v Q 3 z d e E b E S 1 8 T + p Z t l / l e n a p E Y 4 l T X E F B N q W Z U d c y 4 5 L o r 6 u b 2 l 6 o k O a T E K T y g u C D M t H L W Z 1 t r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q z 0 x u j n d 1 S x q w + 3 O c 8 6 B V r b h O x T 0 / L t e q Z t R F 7 O M A R z T P E 9 R Q R w N N 8 h 7 h E U 9 4 t u p W b O X W 7 W e q V T C a P X x b 1 s M H C T u Q E Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I E E G y U V A W x T a u u X 3 x + X f M m 7 U r O c = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J N N p D c 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h k E m H e e 1 Y C 0 s L i 2 v F F f X 1 j c 2 t 7 Z L O 7 u t L M k F 4 0 2 W h I n o + F 7 G w y D m T R n I k H d S w b 3 I D 3 n b H 5 + p e P u G i y x I 4 k s 5 S X k v 8 k Z x M A y Y J 4 m 6 u O u 7 / V L Z q T h 6 2 f P A N a A M s x p J 6 Q V X G C A B Q 4 4 I H D E k 4 R A e M n q 6 c O E g J a 6 H K X G C U K D j H P d Y I 2 1 O W Z w y P G L H 9 B 3 R r m v Y m P b K M 9 N q R q e E 9 A p S 2 j g k T U J 5 g r A 6 z d b x X D s r 9 j f v q f Z U d 5 v Q 3 z d e E b E S 1 8 T + p Z t l / l e n a p E Y 4 l T X E F B N q W Z U d c y 4 5 L o r 6 u b 2 l 6 o k O a T E K T y g u C D M t H L W Z 1 t r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q z 0 x u j n d 1 S x q w + 3 O c 8 6 B V r b h O x T 0 / L t e q Z t R F 7 O M A R z T P E 9 R Q R w N N 8 h 7 h E U 9 4 t u p W b O X W 7 W e q V T C a P X x b 1 s M H C T u Q E Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I E E G y U V A W x T a u u X 3 x + X f M m 7 U r O c = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J N N p D c 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h k E m H e e 1 Y C 0 s L i 2 v F F f X 1 j c 2 t 7 Z L O 7 u t L M k F 4 0 2 W h I n o + F 7 G w y D m T R n I k H d S w b 3 I D 3 n b H 5 + p e P u G i y x I 4 k s 5 S X k v 8 k Z x M A y Y J 4 m 6 u O u 7 / V L Z q T h 6 2 f P A N a A M s x p J 6 Q V X G C A B Q 4 4 I H D E k 4 R A e M n q 6 c O E g J a 6 H K X G C U K D j H P d Y I 2 1 O W Z w y P G L H 9 B 3 R r m v Y m P b K M 9 N q R q e E 9 A p S 2 j g k T U J 5 g r A 6 z d b x X D s r 9 j f v q f Z U d 5 v Q 3 z d e E b E S 1 8 T + p Z t l / l e n a p E Y 4 l T X E F B N q W Z U d c y 4 5 L o r 6 u b 2 l 6 o k O a T E K T y g u C D M t H L W Z 1 t r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q z 0 x u j n d 1 S x q w + 3 O c 8 6 B V r b h O x T 0 / L t e q Z t R F 7 O M A R z T P E 9 R Q R w N N 8 h 7 h E U 9 4 t u p W b O X W 7 W e q V T C a P X x b 1 s M H C T u Q E Q = = &lt; / l a t e x i t &gt; x 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w + O 2 A V A N 5 t G Y s 3 Q A W 4 f 1 x G 8 R q I 4 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J O m 0 D s 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h j y T j v N a s B Y W l 5 Z X i q t r 6 x u b W 9 u l n d 1 W l u Q i Y M 0 g C R P R 8 b 2 M h T x m T c l l y D q p Y F 7 k h 6 z t j 8 9 U v H 3 D R M a T + F J O U t a L v F H M h z z w J F E X d / 1 q v 1 R 2 K o 5 e 9 j x w D S j D r E Z S e s E V B k g Q I E c E h h i S c A g P G T 1 d u H C Q E t f D l D h B i O s 4 w z 3 W S J t T F q M M j 9 g x f U e 0 6 x o 2 p r 3 y z L Q 6 o F N C e g U p b R y S J q E 8 Q V i d Z u t 4 r p 0 V + 5 v 3 V H u q u 0 3 o 7 x u v i F i J a 2 L / 0 s 0 y / 6 t T t U g M c a p r 4 F R T q h l V X W B c c t 0 V d X P 7 S 1 W S H F L i F B 5 Q X B A O t H L W Z 1 t r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q H 5 j c H O / q l j R g 9 + c 4 5 0 G r W n G d i n t + X K 5 V z a i L 2 M c B j m i e J 6 i h j g a a 5 D 3 C I 5 7 w b N W t 2 M q t 2 8 9 U q 2 A 0 e / i 2 r I c P C 5 u Q E g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w + O 2 A V A N 5 t G Y s 3 Q A W 4 f 1 x G 8 R q I 4 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J O m 0 D s 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h j y T j v N a s B Y W l 5 Z X i q t r 6 x u b W 9 u l n d 1 W l u Q i Y M 0 g C R P R 8 b 2 M h T x m T c l l y D q p Y F 7 k h 6 z t j 8 9 U v H 3 D R M a T + F J O U t a L v F H M h z z w J F E X d / 1 q v 1 R 2 K o 5 e 9 j x w D S j D r E Z S e s E V B k g Q I E c E h h i S c A g P G T 1 d u H C Q E t f D l D h B i O s 4 w z 3 W S J t T F q M M j 9 g x f U e 0 6 x o 2 p r 3 y z L Q 6 o F N C e g U p b R y S J q E 8 Q V i d Z u t 4 r p 0 V + 5 v 3 V H u q u 0 3 o 7 x u v i F i J a 2 L / 0 s 0 y / 6 t T t U g M c a p r 4 F R T q h l V X W B c c t 0 V d X P 7 S 1 W S H F L i F B 5 Q X B A O t H L W Z 1 t r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q H 5 j c H O / q l j R g 9 + c 4 5 0 G r W n G d i n t + X K 5 V z a i L 2 M c B j m i e J 6 i h j g a a 5 D 3 C I 5 7 w b N W t 2 M q t 2 8 9 U q 2 A 0 e / i 2 r I c P C 5 u Q E g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w + O 2 A V A N 5 t G Y s 3 Q A W 4 f 1 x G 8 R q I 4 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J O m 0 D s 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h j y T j v N a s B Y W l 5 Z X i q t r 6 x u b W 9 u l n d 1 W l u Q i Y M 0 g C R P R 8 b 2 M h T x m T c l l y D q p Y F 7 k h 6 z t j 8 9 U v H 3 D R M a T + F J O U t a L v F H M h z z w J F E X d / 1 q v 1 R 2 K o 5 e 9 j x w D S j D r E Z S e s E V B k g Q I E c E h h i S c A g P G T 1 d u H C Q E t f D l D h B i O s 4 w z 3 W S J t T F q M M j 9 g x f U e 0 6 x o 2 p r 3 y z L Q 6 o F N C e g U p b R y S J q E 8 Q V i d Z u t 4 r p 0 V + 5 v 3 V H u q u 0 3 o 7 x u v i F i J a 2 L / 0 s 0 y / 6 t T t U g M c a p r 4 F R T q h l V X W B c c t 0 V d X P 7 S 1 W S H F L i F B 5 Q X B A O t H L W Z 1 t r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q H 5 j c H O / q l j R g 9 + c 4 5 0 G r W n G d i n t + X K 5 V z a i L 2 M c B j m i e J 6 i h j g a a 5 D 3 C I 5 7 w b N W t 2 M q t 2 8 9 U q 2 A 0 e / i 2 r I c P C 5 u Q E g = = &lt; / l a t e x i t &gt; x 3 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C L X q B p c I l H 0 j f t D 1 W T P R S D D H 1 d 0 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I q 6 L L g p s u K 9 g G 1 l G Q 6 r U P T J E w m a i m C P + B W P 0 3 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h y I R D n O a 8 5 a W F x a X s m v F t b W N z a 3 i t s 7 z S R K J e M N F g W R b P t e w g M R 8 o Y S K u D t W H J v 7 A e 8 5 Y / O d L x 1 w 2 U i o v B S T W L e H X v D U A w E 8 x R R F 3 e 9 o 1 6 x 5 J Q d s + x 5 4 G a g h G z V o + I L r t B H B I Y U Y 3 C E U I Q D e E j o 6 c C F g 5 i 4 L q b E S U L C x D n u U S B t S l m c M j x i R / Q d 0 q 6 T s S H t t W d i 1 I x O C e i V p L R x Q J q I 8 i R h f Z p t 4 q l x 1 u x v 3 l P j q e 8 2 o b + f e Y 2 J V b g m 9 i / d L P O / O l 2 L w g C n p g Z B N c W G 0 d W x z C U 1 X d E 3 t 7 9 U p c g h J k 7 j P s U l Y W a U s z 7 b R p O Y 2 n V v P R N / M 5 m a 1 X u W 5 a Z 4 1 7 e k A b s / x z k P m p W y 6 5 T d 8 + N S t Z K N O o 8 9 7 O O Q 5 n m C K m q o o 0 H e Q z z i C c 9 W z Q q t 1 L r 9 T L V y m W Y X 3 5 b 1 8 A E N + 5 A T &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C L X q B p c I l H 0 j f t D 1 W T P R S D D H 1 d 0 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I q 6 L L g p s u K 9 g G 1 l G Q 6 r U P T J E w m a i m C P + B W P 0 3 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h y I R D n O a 8 5 a W F x a X s m v F t b W N z a 3 i t s 7 z S R K J e M N F g W R b P t e w g M R 8 o Y S K u D t W H J v 7 A e 8 5 Y / O d L x 1 w 2 U i o v B S T W L e H X v D U A w E 8 x R R F 3 e 9 o 1 6 x 5 J Q d s + x 5 4 G a g h G z V o + I L r t B H B I Y U Y 3 C E U I Q D e E j o 6 c C F g 5 i 4 L q b E S U L C x D n u U S B t S l m c M j x i R / Q d 0 q 6 T s S H t t W d i 1 I x O C e i V p L R x Q J q I 8 i R h f Z p t 4 q l x 1 u x v 3 l P j q e 8 2 o b + f e Y 2 J V b g m 9 i / d L P O / O l 2 L w g C n p g Z B N c W G 0 d W x z C U 1 X d E 3 t 7 9 U p c g h J k 7 j P s U l Y W a U s z 7 b R p O Y 2 n V v P R N / M 5 m a 1 X u W 5 a Z 4 1 7 e k A b s / x z k P m p W y 6 5 T d 8 + N S t Z K N O o 8 9 7 O O Q 5 n m C K m q o o 0 H e Q z z i C c 9 W z Q q t 1 L r 9 T L V y m W Y X 3 5 b 1 8 A E N + 5 A T &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C L X q B p c I l H 0 j f t D 1 W T P R S D D H 1 d 0 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I q 6 L L g p s u K 9 g G 1 l G Q 6 r U P T J E w m a i m C P + B W P 0 3 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h y I R D n O a 8 5 a W F x a X s m v F t b W N z a 3 i t s 7 z S R K J e M N F g W R b P t e w g M R 8 o Y S K u D t W H J v 7 A e 8 5 Y / O d L x 1 w 2 U i o v B S T W L e H X v D U A w E 8 x R R F 3 e 9 o 1 6 x 5 J Q d s + x 5 4 G a g h G z V o + I L r t B H B I Y U Y 3 C E U I Q D e E j o 6 c C F g 5 i 4 L q b E S U L C x D n u U S B t S l m c M j x i R / Q d 0 q 6 T s S H t t W d i 1 I x O C e i V p L R x Q J q I 8 i R h f Z p t 4 q l x 1 u x v 3 l P j q e 8 2 o b + f e Y 2 J V b g m 9 i / d L P O / O l 2 L w g C n p g Z B N c W G 0 d W x z C U 1 X d E 3 t 7 9 U p c g h J k 7 j P s U l Y W a U s z 7 b R p O Y 2 n V v P R N / M 5 m a 1 X u W 5 a Z 4 1 7 e k A b s / x z k P m p W y 6 5 T d 8 + N S t Z K N O o 8 9 7 O O Q 5 n m C K m q o o 0 H e Q z z i C c 9 W z Q q t 1 L r 9 T L V y m W Y X 3 5 b 1 8 A E N + 5 A T &lt; / l a t e x i t &gt; x 4 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " r F F U q Q Z l V A D K 5 + C 9 H 2 U / T l v p H k c = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z J S 0 G X B T Z c V 7 Q N q K c l 0 W o e m S Z h M 1 F I E f 8 C t f p r 4 B / o X 3 h l T U I v o h C R n z r 3 n z N x 7 / T g Q i X K c 1 5 y 1 t L y y u p Z f L 2 x s b m 3 v F H f 3 W k m U S s a b L A o i 2 f G 9 h A c i 5 E 0 l V M A 7 s e T e x A 9 4 2 x + f 6 X j 7 h s t E R O G l m s a 8 N / F G o R g K 5 i m i L u 7 6 1 X 6 x 5 J Q d s + x F 4 G a g h G w 1 o u I L r j B A B I Y U E 3 C E U I Q D e E j o 6 c K F g 5 i 4 H m b E S U L C x D n u U S B t S l m c M j x i x / Q d 0 a 6 b s S H t t W d i 1 I x O C e i V p L R x R J q I 8 i R h f Z p t 4 q l x 1 u x v 3 j P j q e 8 2 p b + f e U 2 I V b g m 9 i / d P P O / O l 2 L w h C n p g Z B N c W G 0 d W x z C U 1 X d E 3 t 7 9 U p c g h J k 7 j A c U l Y W a U 8 z 7 b R p O Y 2 n V v P R N / M 5 m a 1 X u W 5 a Z 4 1 7 e k A b s / x 7 k I W p W y 6 5 T d 8 2 q p V s l G n c c B D n F M 8 z x B D X U 0 0 C T v E R 7 x h G e r b o V W a t 1 + p l q 5 T L O P b 8 t 6 + A A Q W 5 A U &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " r F F U q Q Z l V A D K 5 + C 9 H 2 U / T l v p H k c = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z J S 0 G X B T Z c V 7 Q N q K c l 0 W o e m S Z h M 1 F I E f 8 C t f p r 4 B / o X 3 h l T U I v o h C R n z r 3 n z N x 7 / T g Q i X K c 1 5 y 1 t L y y u p Z f L 2 x s b m 3 v F H f 3 W k m U S s a b L A o i 2 f G 9 h A c i 5 E 0 l V M A 7 s e T e x A 9 4 2 x + f 6 X j 7 h s t E R O G l m s a 8 N / F G o R g K 5 i m i L u 7 6 1 X 6 x 5 J Q d s + x F 4 G a g h G w 1 o u I L r j B A B I Y U E 3 C E U I Q D e E j o 6 c K F g 5 i 4 H m b E S U L C x D n u U S B t S l m c M j x i x / Q d 0 a 6 b s S H t t W d i 1 I x O C e i V p L R x R J q I 8 i R h f Z p t 4 q l x 1 u x v 3 j P j q e 8 2 p b + f e U 2 I V b g m 9 i / d P P O / O l 2 L w h C n p g Z B N c W G 0 d W x z C U 1 X d E 3 t 7 9 U p c g h J k 7 j A c U l Y W a U 8 z 7 b R p O Y 2 n V v P R N / M 5 m a 1 X u W 5 a Z 4 1 7 e k A b s / x 7 k I W p W y 6 5 T d 8 2 q p V s l G n c c B D n F M 8 z x B D X U 0 0 C T v E R 7 x h G e r b o V W a t 1 + p l q 5 T L O P b 8 t 6 + A A Q W 5 A U &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " r F F U q Q Z l V A D K 5 + C 9 H 2 U / T l v p H k c = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z J S 0 G X B T Z c V 7 Q N q K c l 0 W o e m S Z h M 1 F I E f 8 C t f p r 4 B / o X 3 h l T U I v o h C R n z r 3 n z N x 7 / T g Q i X K c 1 5 y 1 t L y y u p Z f L 2 x s b m 3 v F H f 3 W k m U S s a b L A o i 2 f G 9 h A c i 5 E 0 l V M A 7 s e T e x A 9 4 2 x + f 6 X j 7 h s t E R O G l m s a 8 N / F G o R g K 5 i m i L u 7 6 1 X 6 x 5 J Q d s + x F 4 G a g h G w 1 o u I L r j B A B I Y U E 3 C E U I Q D e E j o 6 c K F g 5 i 4 H m b E S U L C x D n u U S B t S l m c M j x i x / Q d 0 a 6 b s S H t t W d i 1 I x O C e i V p L R x R J q I 8 i R h f Z p t 4 q l x 1 u x v 3 j P j q e 8 2 p b + f e U 2 I V b g m 9 i / d P P O / O l 2 L w h C n p g Z B N c W G 0 d W x z C U 1 X d E 3 t 7 9 U p c g h J k 7 j A c U l Y W a U 8 z 7 b R p O Y 2 n V v P R N / M 5 m a 1 X u W 5 a Z 4 1 7 e k A b s / x 7 k I W p W y 6 5 T d 8 2 q p V s l G n c c B D n F M 8 z x B D X U 0 0 C T v E R 7 x h G e r b o V W a t 1 + p l q 5 T L O P b 8 t 6 + A A Q W 5 A U &lt; / l a t e x i t &gt; x 5 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 R R w X c H / u v F A U 8 k 4 3 0 Z o I H v g O s k = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z K i 6 L L g p s u K 9 g G 1 l G Q 6 r U P T J E w m a i m C P + B W P 0 3 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h y I R D n O a 8 5 a W F x a X s m v F t b W N z a 3 i t s 7 z S R K J e M N F g W R b P t e w g M R 8 o Y S K u D t W H J v 7 A e 8 5 Y / O d L x 1 w 2 U i o v B S T W L e H X v D U A w E 8 x R R F 3 e 9 4 1 6 x 5 J Q d s + x 5 4 G a g h G z V o + I L r t B H B I Y U Y 3 C E U I Q D e E j o 6 c C F g 5 i 4 L q b E S U L C x D n u U S B t S l m c M j x i R / Q d 0 q 6 T s S H t t W d i 1 I x O C e i V p L R x Q J q I 8 i R h f Z p t 4 q l x 1 u x v 3 l P j q e 8 2 o b + f e Y 2 J V b g m 9 i / d L P O / O l 2 L w g C n p g Z B N c W G 0 d W x z C U 1 X d E 3 t 7 9 U p c g h J k 7 j P s U l Y W a U s z 7 b R p O Y 2 n V v P R N / M 5 m a 1 X u W 5 a Z 4 1 7 e k A b s / x z k P m p W y 6 5 T d 8 6 N S t Z K N O o 8 9 7 O O Q 5 n m C K m q o o 0 H e Q z z i C c 9 W z Q q t 1 L r 9 T L V y m W Y X 3 5 b 1 8 A E S u 5 A V &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 R R w X c H / u v F A U 8 k 4 3 0 Z o I H v g O s k = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z K i 6 L L g p s u K 9 g G 1 l G Q 6 r U P T J E w m a i m C P + B W P 0 3 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h y I R D n O a 8 5 a W F x a X s m v F t b W N z a 3 i t s 7 z S R K J e M N F g W R b P t e w g M R 8 o Y S K u D t W H J v 7 A e 8 5 Y / O d L x 1 w 2 U i o v B S T W L e H X v D U A w E 8 x R R F 3 e 9 4 1 6 x 5 J Q d s + x 5 4 G a g h G z V o + I L r t B H B I Y U Y 3 C E U I Q D e E j o 6 c C F g 5 i 4 L q b E S U L C x D n u U S B t S l m c M j x i R / Q d 0 q 6 T s S H t t W d i 1 I x O C e i V p L R x Q J q I 8 i R h f Z p t 4 q l x 1 u x v 3 l P j q e 8 2 o b + f e Y 2 J V b g m 9 i / d L P O / O l 2 L w g C n p g Z B N c W G 0 d W x z C U 1 X d E 3 t 7 9 U p c g h J k 7 j P s U l Y W a U s z 7 b R p O Y 2 n V v P R N / M 5 m a 1 X u W 5 a Z 4 1 7 e k A b s / x z k P m p W y 6 5 T d 8 6 N S t Z K N O o 8 9 7 O O Q 5 n m C K m q o o 0 H e Q z z i C c 9 W z Q q t 1 L r 9 T L V y m W Y X 3 5 b 1 8 A E S u 5 A V &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 R R w X c H / u v F A U 8 k 4 3 0 Z o I H v g O s k = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z K i 6 L L g p s u K 9 g G 1 l G Q 6 r U P T J E w m a i m C P + B W P 0 3 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h y I R D n O a 8 5 a W F x a X s m v F t b W N z a 3 i t s 7 z S R K J e M N F g W R b P t e w g M R 8 o Y S K u D t W H J v 7 A e 8 5 Y / O d L x 1 w 2 U i o v B S T W L e H X v D U A w E 8 x R R F 3 e 9 4 1 6 x 5 J Q d s + x 5 4 G a g h G z V o + I L r t B H B I Y U Y 3 C E U I Q D e E j o 6 c C F g 5 i 4 L q b E S U L C x D n u U S B t S l m c M j x i R / Q d 0 q 6 T s S H t t W d i 1 I x O C e i V p L R x Q J q I 8 i R h f Z p t 4 q l x 1 u x v 3 l P j q e 8 2 o b + f e Y 2 J V b g m 9 i / d L P O / O l 2 L w g C n p g Z B N c W G 0 d W x z C U 1 X d E 3 t 7 9 U p c g h J k 7 j P s U l Y W a U s z 7 b R p O Y 2 n V v P R N / M 5 m a 1 X u W 5 a Z 4 1 7 e k A b s / x z k P m p W y 6 5 T d 8 6 N S t Z K N O o 8 9 7 O O Q 5 n m C K m q o o 0 H e Q z z i C c 9 W z Q q t 1 L r 9 T L V y m W Y X 3 5 b 1 8 A E S u 5 A V &lt; / l a t e x i t &gt; (c) Mixture.</formula><p>Figure <ref type="figure" target="#fig_14">2</ref>: Three popular attention mask patterns, where the subscript t indicates the t-th timestep. A shaded box at (i, j) indicates that the attention mechanism is allowed to attend to the input element i at output time step j. A white box indicates that the attention mechanism is not allowed to attend to the corresponding i and j combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Typical Pre-training Methods</head><p>With the above concepts in mind, we introduce four popular pre-training methods, resulting from diverse combinations of objective, noising function, and directionality. These are described below, and summarized in Fig. <ref type="figure" target="#fig_18">3</ref> and Tab. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Left-to-Right Language Model</head><p>Left-to-right LMs (L2R LMs), a variety of auto-regressive LM, predict the upcoming words or assign a probability P (x) to a sequence of words x = x 1 , • • • , x n (Jurafsky and Martin, 2021). The probability is commonly broken down using the chain rule in a left-to-right fashion:</p><formula xml:id="formula_17">P (x) = P (x 1 ) × • • • P (x n |x 1 • • • x n−1 ). 3 3.4 Typical Pre-training Methods x 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I E E G y U V A W x T a u u X 3 x + X f M m 7 U r O c = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J N N p D c 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h k E m H e e 1 Y C 0 s L i 2 v F F f X 1 j c 2 t 7 Z L O 7 u t L M k F 4 0 2 W h I n o + F 7 G w y D m T R n I k H d S w b 3 I D 3 n b H 5 + p e P u G i y x I 4 k s 5 S X k v 8 k Z x M A y Y J 4 m 6 u O u 7 / V L Z q T h 6 2 f P A N a A M s x p J 6 Q V X G C A B Q 4 4 I H D E k 4 R A e M n q 6 c O E g J a 6 H K X G C U K D j H P d Y I 2 1 O W Z w y P G L H 9 B 3 R r m v Y m P b K M 9 N q R q e E 9 A p S 2 j g k T U J 5 g r A 6 z d b x X D s r 9 j f v q f Z U d 5 v Q 3 z d e E b E S 1 8 T + p Z t l / l e n a p E Y 4 l T X E F B N q W Z U d c y 4 5 L o r 6 u b 2 l 6 o k O a T E K T y g u C D M t H L W Z 1 t r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q z 0 x u j n d 1 S x q w + 3 O c 8 6 B V r b h O x T 0 / L t e q Z t R F 7 O M A R z T P E 9 R Q R w N N 8 h 7 h E U 9 4 t u p W b O X W 7 W e q V T C a P X x b 1 s M H C T u Q E Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I E E G y U V A W x T a u u X 3 x + X f M m 7 U r O c = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J N N p D c 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h k E m H e e 1 Y C 0 s L i 2 v F F f X 1 j c 2 t 7 Z L O 7 u t L M k F 4 0 2 W h I n o + F 7 G w y D m T R n I k H d S w b 3 I D 3 n b H 5 + p e P u G i y x I 4 k s 5 S X k v 8 k Z x M A y Y J 4 m 6 u O u 7 / V L Z q T h 6 2 f P A N a A M s x p J 6 Q V X G C A B Q 4 4 I H D E k 4 R A e M n q 6 c O E g J a 6 H K X G C U K D j H P d Y I 2 1 O W Z w y P G L H 9 B 3 R r m v Y m P b K M 9 N q R q e E 9 A p S 2 j g k T U J 5 g r A 6 z d b x X D s r 9 j f v q f Z U d 5 v Q 3 z d e E b E S 1 8 T + p Z t l / l e n a p E Y 4 l T X E F B N q W Z U d c y 4 5 L o r 6 u b 2 l 6 o k O a T E K T y g u C D M t H L W Z 1 t r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q z 0 x u j n d 1 S x q w + 3 O c 8 6 B V r b h O x T 0 / L t e q Z t R F 7 O M A R z T P E 9 R Q R w N N 8 h 7 h E U 9 4 t u p W b O X W 7 W e q V T C a P X x b 1 s M H C T u Q E Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I E E G y U V A W x T a u u X 3 x + X f M m 7 U r O c = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J N N p D c 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h k E m H e e 1 Y C 0 s L i 2 v F F f X 1 j c 2 t 7 Z L O 7 u t L M k F 4 0 2 W h I n o + F 7 G w y D m T R n I k H d S w b 3 I D 3 n b H 5 + p e P u G i y x I 4 k s 5 S X k v 8 k Z x M A y Y J 4 m 6 u O u 7 / V L Z q T h 6 2 f P A N a A M s x p J 6 Q V X G C A B Q 4 4 I H D E k 4 R A e M n q 6 c O E g J a 6 H K X G C U K D j H P d Y I 2 1 O W Z w y P G L H 9 B 3 R r m v Y m P b K M 9 N q R q e E 9 A p S 2 j g k T U J 5 g r A 6 z d b x X D s r 9 j f v q f Z U d 5 v Q 3 z d e E b E S 1 8 T + p Z t l / l e n a p E Y 4 l T X E F B N q W Z U d c y 4 5 L o r 6 u b 2 l 6 o k O a T E K T y g u C D M t H L W Z 1 t r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q z 0 x u j n d 1 S x q w + 3 O c 8 6 B V r b h O x T 0 / L t e q Z t R F 7 O M A R z T P E 9 R Q R w N N 8 h 7 h E U 9 4 t u p W b O X W 7 W e q V T C a P X x b 1 s M H C T u Q E Q = = &lt; / l a t e x i t &gt; x 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w + O 2 A V A N 5 t G Y s 3 Q A W 4 f 1 x G 8 R q I 4 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J O m 0 D s 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h j y T j v N a s B Y W l 5 Z X i q t r 6 x u b W 9 u l n d 1 W l u Q i Y M 0 g C R P R 8 b 2 M h T x m T c l l y D q p Y F 7 k h 6 z t j 8 9 U v H 3 D R M a T + F J O U t a L v F H M h z z w J F E X d / 1 q v 1 R 2 K o 5 e 9 j x w D S j D r E Z S e s E V B k g Q I E c E h h i S c A g P G T 1 d u H C Q E t f D l D h B i O s 4 w z 3 W S J t T F q M M j 9 g x f U e 0 6 x o 2 p r 3 y z L Q 6 o F N C e g U p b R y S J q E 8 Q V i d Z u t 4 r p 0 V + 5 v 3 V H u q u 0 3 o 7 x u v i F i J a 2 L / 0 s 0 y / 6 t T t U g M c a p r 4 F R T q h l V X W B c c t 0 V d X P 7 S 1 W S H F L i F B 5 Q X B A O t H L W Z 1 t</formula><p>r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q H 5 j c H O / q l j R g 9 + c 4 5 0 G r W n G d i n t + X K 5 V z a i L 2 M c B j m i e J 6 i h j g a a 5 D 3 C I 5 7 w b N W t 2 M q t 2 8 9 U q 2 A 0 e / i 2 r I c P C 5 u Q E g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w</p><formula xml:id="formula_18">+ O 2 A V A N 5 t G Y s 3 Q A W 4 f 1 x G 8 R q I 4 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J O m 0 D s 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h j y T j v N a s B Y W l 5 Z X i q t r 6 x u b W 9 u l n d 1 W l u Q i Y M 0 g C R P R 8 b 2 M h T x m T c l l y D q p Y F 7 k h 6 z t j 8 9 U v H 3 D R M a T + F J O U t a L v F H M h z z w J F E X d / 1 q v 1 R 2 K o 5 e 9 j x w D S j D r E Z S e s E V B k g Q I E c E h h i S c A g P G T 1 d u H C Q E t f D l D h B i O s 4 w z 3 W S J t T F q M M j 9 g x f U e 0 6 x o 2 p r 3 y z L Q 6 o F N C e g U p b R y S J q E 8 Q V i d Z u t 4 r p 0 V + 5 v 3 V H u q u 0 3 o 7 x u v i F i J a 2 L / 0 s 0 y / 6 t T t U g M c a p r 4 F R T q h l V X W B c c t 0 V d X P 7 S 1 W S H F L i F B 5 Q X B A O t H L W Z 1 t</formula><p>r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q H 5 j c H O / q l j R g 9 + c 4 5 0 G r W n G d i n t + X K 5 V z a i L 2 M c B j m i e J 6 i h j g a a 5 D 3 C I 5 7 w b N W t 2 M q t 2 8 9 U q 2 A 0 e / i 2 r I c P C 5 u Q E g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w</p><formula xml:id="formula_19">+ O 2 A V A N 5 t G Y s 3 Q A W 4 f 1 x G 8 R q I 4 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J O m 0 D s 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h j y T j v N a s B Y W l 5 Z X i q t r 6 x u b W 9 u l n d 1 W l u Q i Y M 0 g C R P R 8 b 2 M h T x m T c l l y D q p Y F 7 k h 6 z t j 8 9 U v H 3 D R M a T + F J O U t a L v F H M h z z w J F E X d / 1 q v 1 R 2 K o 5 e 9 j x w D S j D r E Z S e s E V B k g Q I E c E h h i S c A g P G T 1 d u H C Q E t f D l D h B i O s 4 w z 3 W S J t T F q M M j 9 g x f U e 0 6 x o 2 p r 3 y z L Q 6 o F N C e g U p b R y S J q E 8 Q V i d Z u t 4 r p 0 V + 5 v 3 V H u q u 0 3 o 7 x u v i F i J a 2 L / 0 s 0 y / 6 t T t U g M c a p r 4 F R T q h l V X W B c c t 0 V d X P 7 S 1 W S H F L i F B 5 Q X B A O t H L W Z 1 t</formula><p>r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q H 5 j c H O / q l j R g 9 + c 4 5 0 G r W n G d i n t + X K 5 V z a i L 2 M c B j m i e J 6 i h j g a a 5 D 3 C I 5 7 w b N W t 2 M q t 2 8 9 U q 2 A 0 e / i 2 r I c P C 5 u Q E g = = &lt; / l a t e x i t &gt;</p><formula xml:id="formula_20">x 3 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C L X q B p c I l H 0 j f t D 1 W T P R S D D H 1 d 0 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I q 6 L L g p s u K 9 g G 1 l G Q 6 r U P T J E w m a i m C P + B W P 0 3 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h y I R D n O a 8 5 a W F x a X s m v F t b W N z a 3 i t s 7 z S R K J e M N F g W R b P t e w g M R 8 o Y S K u D t W H J v 7 A e 8 5 Y / O d L x 1 w 2 U i o v B S T W L e H X v D U A w E 8 x R R F 3 e 9 o 1 6 x 5 J Q d s + x 5 4 G a g h G z V o + I L r t B H B I Y U Y 3 C E U I Q D e E j o 6 c C F g 5 i 4 L q b E S U L C x D n u U S B t S l m c M j x i R / Q d 0 q 6 T s S H t t W d i 1 I x O C e i V p L R x Q J q I 8 i R h f Z p t 4 q l x 1 u x v 3 l P j q e 8 2 o b + f e Y 2 J V b g m 9 i / d L P O / O l 2 L w g C n p g Z B N c W G 0 d W x z C U 1 X d E 3 t 7 9 U p c g h J k 7 j P s U l Y W a U s z 7 b R p O Y 2 n V v P R N / M 5 m a 1 X u W 5 a Z 4 1 7 e k A b s / x z k P m p W y 6 5 T d 8 + N S t Z K N O o 8 9 7 O O Q 5 n m C K m q o o 0 H e Q z z i C c 9 W z Q q t 1 L r 9 T L V y m W Y X 3 5 b 1 8 A E N + 5 A T &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C L X q B p c I l H 0 j f t D 1 W T P R S D D H 1 d 0 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I q 6 L L g p s u K 9 g G 1 l G Q 6 r U P T J E w m a i m C P + B W P 0 3 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h y I R D n O a 8 5 a W F x a X s m v F t b W N z a 3 i t s 7 z S R K J e M N F g W R b P t e w g M R 8 o Y S K u D t W H J v 7 A e 8 5 Y / O d L x 1 w 2 U i o v B S T W L e H X v D U A w E 8 x R R F 3 e 9 o 1 6 x 5 J Q d s + x 5 4 G a g h G z V o + I L r t B H B I Y U Y 3 C E U I Q D e E j o 6 c C F g 5 i 4 L q b E S U L C x D n u U S B t S l m c M j x i R / Q d 0 q 6 T s S H t t W d i 1 I x O C e i V p L R x Q J q I 8 i R h f Z p t 4 q l x 1 u x v 3 l P j q e 8 2 o b + f e Y 2 J V b g m 9 i / d L P O / O l 2 L w g C n p g Z B N c W G 0 d W x z C U 1 X d E 3 t 7 9 U p c g h J k 7 j P s U l Y W a U s z 7 b R p O Y 2 n V v P R N / M 5 m a 1 X u W 5 a Z 4 1 7 e k A b s / x z k P m p W y 6 5 T d 8 + N S t Z K N O o 8 9 7 O O Q 5 n m C K m q o o 0 H e Q z z i C c 9 W z Q q t 1 L r 9 T L V y m W Y X 3 5 b 1 8 A E N + 5 A T &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C L X q B p c I l H 0 j f t D 1 W T P R S D D H 1 d 0 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I q 6 L L g p s u K 9 g G 1 l G Q 6 r U P T J E w m a i m C P + B W P 0 3 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h y I R D n O a 8 5 a W F x a X s m v F t b W N z a 3 i t s 7 z S R K J e M N F g W R b P t e w g M R 8 o Y S K u D t W H J v 7 A e 8 5 Y / O d L x 1 w 2 U i o v B S T W L e H X v D U A w E 8 x R R F 3 e 9 o 1 6 x 5 J Q d s + x 5 4 G a g h G z V o + I L r t B H B I Y U Y 3 C E U I Q D e E j o 6 c C F g 5 i 4 L q b E S U L C x D n u U S B t S l m c M j x i R / Q d 0 q 6 T s S H t t W d i 1 I x O C e i V p L R x Q J q I 8 i R h f Z p t 4 q l x 1 u x v 3 l P j q e 8 2 o b + f e Y 2 J V b g m 9 i / d L P O / O l 2 L w g C n p g Z B N c W G 0 d W x z C U 1 X d E 3 t 7 9 U p c g h J k 7 j P s U l Y W a U s z 7 b R p O Y 2 n V v P R N / M 5 m a 1 X u W 5 a Z 4 1 7 e k A b s / x z k P m p W y 6 5 T d 8 + N S t Z K N O o 8 9 7 O O Q 5 n m C K m q o o 0 H e Q z z i C c 9 W z Q q t 1 L r 9 T L V y m W Y X 3 5 b 1 8 A E N + 5 A T &lt; / l a t e x i t &gt; y 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D F 3 Q y n H e 2 D 2 p z i C p V s g 5 A B U J 3 q Y = " &gt; A A A C x 3 i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I i 6 L L g R n c V 7 A O 0 l C S d t k P z Y j I p l u L C H 3 C r f y b + g f 6 F d 8 Y p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h r y T D r O a 8 F a W l 5 Z X S u u l z Y 2 t 7 Z 3 y r t 7 r S z J R c C a Q R I m o u N 7 G Q t 5 z J q S y 5 B 1 U s G 8 y A 9 Z 2 x + f q 3 h 7 w k T G k / h a T l P W j b x h z A c 8 8 K S i p j 2 3 1 C t X n K q j l 7 0 I X A M q M K u R l F 9 w i z 4 S B M g R g S G G J B z C Q 0 b P D V w 4 S I n r Y k a c I M R 1 n O E e J d L m l M U o w y N 2 T N 8 h 7 W 4 M G 9 N e e W Z a H d A p I b 2 C l D a O S J N Q n i C s T r N 1 P N f O i v 3 N e 6 Y 9 1 d 2 m 9 P e N V 0 S s x I j Y v 3 T z z P / q V C 0 S A 5 z p G j j V l G p G V R c Y l 1 x 3 R d 3 c / l K V J I e U O I X 7 F B e E A</formula><p>6 2 c 9 9 n W m k z X r n r r 6 f i b z l S s 2 g c m N 8 e 7 u i U N 2 P 0 5 z k X Q q l V d p + p e n V T q N T P q I g 5 w i G O a 5 y n q u E A D T f I e 4 R F P e L Y u r c S a W H e f q V b B a P b x b V k P H 0 o q k C Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = "</p><formula xml:id="formula_21">D F 3 Q y n H e 2 D 2 p z i C p V s g 5 A B U J 3 q Y = " &gt; A A A C x 3 i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I i 6 L L g R n c V 7 A O 0 l C S d t k P z Y j I p l u L C H 3 C r f y b + g f 6 F d 8 Y p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h r y T D r O a 8 F a W l 5 Z X S u u l z Y 2 t 7 Z 3 y r t 7 r S z J R c C a Q R I m o u N 7 G Q t 5 z J q S y 5 B 1 U s G 8 y A 9 Z 2 x + f q 3 h 7 w k T G k / h a T l P W j b x h z A c 8 8 K S i p j 2 3 1 C t X n K q j l 7 0 I X A M q M K u R l F 9 w i z 4 S B M g R g S G G J B z C Q 0 b P D V w 4 S I n r Y k a c I M R 1 n O E e J d L m l M U o w y N 2 T N 8 h 7 W 4 M G 9 N e e W Z a H d A p I b 2 C l D a O S J N Q n i C s T r N 1 P N f O i v 3 N e 6 Y 9 1 d 2 m 9 P e N V 0 S s x I j Y v 3 T z z P / q V C 0 S A 5 z p G j j V l G p G V R c Y l 1 x 3 R d 3 c / l K V J I e U O I X 7 F B e E A</formula><p>6 2 c 9 9 n W m k z X r n r r 6 f i b z l S s 2 g c m N 8 e 7 u i U N 2 P 0 5 z k X Q q l V d p + p e n V T q N T P q I g 5 w i G O a 5 y n q u E A D T f I e 4 R F P e L Y u r c S a W H e f q V b B a P b x b V k P H 0 o q k C Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " <ref type="bibr" target="#b16">6</ref> 2 C 0 e z j 2 7 I e P g B M i 5 A n &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T o R A e S A j J X f q b p 5 c p o <ref type="bibr" target="#b16">6</ref> 2 C 0 e z j 2 7 I e P g B M i 5 A n &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T o R A e S A j J X f q b p 5 c p o</p><formula xml:id="formula_22">D F 3 Q y n H e 2 D 2 p z i C p V s g 5 A B U J 3 q Y = " &gt; A A A C x 3 i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I i 6 L L g R n c V 7 A O 0 l C S d t k P z Y j I p l u L C H 3 C r f y b + g f 6 F d 8 Y p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h r y T D r O a 8 F a W l 5 Z X S u u l z Y 2 t 7 Z 3 y r t 7 r S z J R c C a Q R I m o u N 7 G Q t 5 z J q S y 5 B 1 U s G 8 y A 9 Z 2 x + f q 3 h 7 w k T G k / h a T l P W j b x h z A c 8 8 K S i p j 2 3 1 C t X n K q j l 7 0 I X A M q M K u R l F 9 w i z 4 S B M g R g S G G J B z C Q 0 b P D V w 4 S I n r Y k a c I M R 1 n O E e J d L m l M U o w y N 2 T N 8 h 7 W 4 M G 9 N e e W Z a H d A p I b 2 C l D a O S J N Q n i C s T r N 1 P N f O i v 3 N e 6 Y 9 1 d 2 m 9 P e N V 0 S s x I j Y v 3 T z z P / q V C 0 S A 5 z p G j j V l G p G V R c Y l 1 x 3 R d 3 c / l K V J I e U O I X 7 F B e E A 6 2 c 9 9 n W m k z X r n r r 6 f i b z l S s 2 g c m N 8 e 7 u i U N 2 P 0 5 z k X Q q l V d p + p e n V T q N T P q I g 5 w i G O a 5 y n q u E A D T f I e 4 R F P e L Y u r c S a W H e f q V b B a P b x b V k P H 0 o q k C Y = &lt; / l a t e x i t &gt; y 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T o R A e S A j J X f q b p 5 c p o 7 s M K O B c 9 Y = " &gt; A A A C x 3 i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I i 6 L L g R n c V 7 A O 0 l G Q 6 b U P z Y j I p l u L C H 3 C r f y b + g f 6 F d 8 Y p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h o G m X S c 1 4 K 1 t L y y u l Z c L 2 1 s b m 3 v l H f 3 W l m S C 8 a b L A k T 0 f G 9 j I d B z J s y k C H v p I J 7 k R / y t j 8 + V / H 2 h I s s S O J r O U 1 5 N / K G c T A I m C c V N e 3 V S r 1 y x a k 6 e t m L w D W g A r M a S f k F t + g j A U O O C B w x J O E Q H j J 6 b u D C Q U p c F z P i B K F A x z n u U S J t T l m c M j x i x / Q d 0 u 7 G s D H t l W e m 1 Y x O C e k V p L R x R J q E 8 g R h d Z q t 4 7 l 2 V u x v 3 j P t q e 4 2 p b 9 v v C J i J U b E / q W b Z / 5 X p 2 q R G O B M 1 x B Q T a l m V H X M u O S 6 K + r m 9 p e q J D m k x C n c p 7 g g z L R y 3 m d b a z J d u + q t p + N v O l O x a s 9 M b o 5 3 d U s a s P t z n I u g V a u 6 T t W 9 O q n U a 2 b U R R z g E M c 0 z 1 P U c Y E G m u Q 9 w i O e 8 G x d W o k 1 s e 4 + U</formula><formula xml:id="formula_23">7 s M K O B c 9 Y = " &gt; A A A C x 3 i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I i 6 L L g R n c V 7 A O 0 l G Q 6 b U P z Y j I p l u L C H 3 C r f y b + g f 6 F d 8 Y p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h o G m X S c 1 4 K 1 t L y y u l Z c L 2 1 s b m 3 v l H f 3 W l m S C 8 a b L A k T 0 f G 9 j I d B z J s y k C H v p I J 7 k R / y t j 8 + V / H 2 h I s s S O J r O U 1 5 N / K G c T A I m C c V N e 3 V S r 1 y x a k 6 e t m L w D W g A r M a S f k F t + g j A U O O C B w x J O E Q H j J 6 b u D C Q U p c F z P i B K F A x z n u U S J t T l m c M j x i x / Q d 0 u 7 G s D H t l W e m 1 Y x O C e k V p L R x R J q E 8 g R h d Z q t 4 7 l 2 V u x v 3 j P t q e 4 2 p b 9 v v C J i J U b E / q W b Z / 5 X p 2 q R G O B M 1 x B Q T a l m V H X M u O S 6 K + r m 9 p e q J D m k x C n c p 7 g g z L R y 3 m d b a z J d u + q t p + N v O l O x a s 9 M b o 5 3 d U s a s P t z n I u g V a u 6 T t W 9 O q n U a 2 b U R R z g E M c 0 z 1 P U c Y E G m u Q 9 w i O e 8 G x d W o k 1 s e 4 + U</formula><formula xml:id="formula_24">7 s M K O B c 9 Y = " &gt; A A A C x 3 i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I i 6 L L g R n c V 7 A O 0 l G Q 6 b U P z Y j I p l u L C H 3 C r f y b + g f 6 F d 8 Y p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h o G m X S c 1 4 K 1 t L y y u l Z c L 2 1 s b m 3 v l H f 3 W l m S C 8 a b L A k T 0 f G 9 j I d B z J s y k C H v p I J 7 k R / y t j 8 + V / H 2 h I s s S O J r O U 1 5 N / K G c T A I m C c V N e 3 V S r 1 y x a k 6 e t m L w D W g A r M a S f k F t + g j A U O O C B w x J O E Q H j J 6 b u D C Q U p c F z P i B K F A x z n u U S J t T l m c M j x i x / Q d 0 u 7 G s D H t l W e m 1 Y x O C e k V p L R x R J q E 8 g R h d Z q t 4 7 l 2 V u x v 3 j P t q e 4 2 p b 9 v v C J i J U b E / q W b Z / 5 X p 2 q R G O B M 1 x B Q T a l m V H X M u O S 6 K + r m 9 p e q J D m k x C n c p 7 g g z L R y 3 m d b a z J d u + q t p + N v O l O x a s 9 M b o 5 3 d U s a s P t z n I u g V a u 6 T t W 9 O q n U a 2 b U R R z g E M c 0 z 1 P U c Y E G m u Q 9 w i O e 8 G x d W o k 1 s e 4 + U 6 2 C 0 e z j 2 7 I e P g B M i 5 A n &lt; / l a t e x i t &gt; x 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w + O 2 A V A N 5 t G Y s 3 Q A W 4 f 1 x G 8 R q I 4 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J O m 0 D s 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h j y T j v N a s B Y W l 5 Z X i q t r 6 x u b W 9 u l n d 1 W l u Q i Y M 0 g C R P R 8 b 2 M h T x m T c l l y D q p Y F 7 k h 6 z t j 8 9 U v H 3 D R M a T + F J O U t a L v F H M h z z w J F E X d / 1 q v 1 R 2 K o 5 e 9 j x w D S j D r E Z S e s E V B k g Q I E c E h h i S c A g P G T 1 d u H C Q E t f D l D h B i O s 4 w z 3 W S J t T F q M M j 9 g x f U e 0 6 x o 2 p r 3 y z L Q 6 o F N C e g U p b R y S J q E 8 Q V i d Z u t 4 r p 0 V + 5 v 3 V H u q u 0 3 o 7 x u v i F i J a 2 L / 0 s 0 y / 6 t T t U g M c a p r 4 F R T q h l V X W B c c t 0 V d X P 7 S 1 W S H F L i F B 5 Q X B A O t H L W Z 1 t</formula><p>r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q H 5 j c H O / q l j R g 9 + c 4 5 0 G r W n G d i n t + X K 5 V z a i L 2 M c B j m i e J 6 i h j g a a 5 D 3 C I 5 7 w b N W t 2 M q t 2 8 9 U q 2 A 0 e / i 2 r I c P C 5 u Q E g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w +</p><formula xml:id="formula_25">O 2 A V A N 5 t G Y s 3 Q A W 4 f 1 x G 8 R q I 4 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J O m 0 D s 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h j y T j v N a s B Y W l 5 Z X i q t r 6 x u b W 9 u l n d 1 W l u Q i Y M 0 g C R P R 8 b 2 M h T x m T c l l y D q p Y F 7 k h 6 z t j 8 9 U v H 3 D R M a T + F J O U t a L v F H M h z z w J F E X d / 1 q v 1 R 2 K o 5 e 9 j x w D S j D r E Z S e s E V B k g Q I E c E h h i S c A g P G T 1 d u H C Q E t f D l D h B i O s 4 w z 3 W S J t T F q M M j 9 g x f U e 0 6 x o 2 p r 3 y z L Q 6 o F N C e g U p b R y S J q E 8 Q V i d Z u t 4 r p 0 V + 5 v 3 V H u q u 0 3 o 7 x u v i F i J a 2 L / 0 s 0 y / 6 t T t U g M c a p r 4 F R T q h l V X W B c c t 0 V d X P 7 S 1 W S H F L i F B 5 Q X B A O t H L W Z 1 t</formula><p>r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q H 5 j c H O / q l j R g 9 + c 4 5 0 G r W n G d i n t + X K 5 V z a i L 2 M c B j m i e J 6 i h j g a a 5 D 3 C I 5 7 w b N W t 2 M q t 2 8 9 U q 2 A 0 e / i 2 r I c P C 5 u Q E g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w +</p><formula xml:id="formula_26">O 2 A V A N 5 t G Y s 3 Q A W 4 f 1 x G 8 R q I 4 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J O m 0 D s 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h j y T j v N a s B Y W l 5 Z X i q t r 6 x u b W 9 u l n d 1 W l u Q i Y M 0 g C R P R 8 b 2 M h T x m T c l l y D q p Y F 7 k h 6 z t j 8 9 U v H 3 D R M a T + F J O U t a L v F H M h z z w J F E X d / 1 q v 1 R 2 K o 5 e 9 j x w D S j D r E Z S e s E V B k g Q I E c E h h i S c A g P G T 1 d u H C Q E t f D l D h B i O s 4 w z 3 W S J t T F q M M j 9 g x f U e 0 6 x o 2 p r 3 y z L Q 6 o F N C e g U p b R y S J q E 8 Q V i d Z u t 4 r p 0 V + 5 v 3 V H u q u 0 3 o 7 x u v i F i J a 2 L / 0 s 0 y / 6 t T t U g M c a p r 4 F R T q h l V X W B c c t 0 V d X P 7 S 1 W S H F L i F B 5 Q X B A O t H L W Z 1 t</formula><p>r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q H 5 j c H O / q l j R g 9 + c 4 5 0 G r W n G d i n t + X K 5 V z a i L 2 M c B j m i e J 6 i h j g a a 5 D 3 C I 5 7 w b N W t 2 M q t 2 8 9 U q 2 A 0 e / i 2 r I c P C 5 u Q E g = = &lt; / l a t e x i t &gt; x 3 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C L X q B p c I l H 0 j f t</p><formula xml:id="formula_27">D 1 W T P R S D D H 1 d 0 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I q 6 L L g p s u K 9 g G 1 l G Q 6 r U P T J E w m a i m C P + B W P 0 3 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h y I R D n O a 8 5 a W F x a X s m v F t b W N z a 3 i t s 7 z S R K J e M N F g W R b P t e w g M R 8 o Y S K u D t W H J v 7 A e 8 5 Y / O d L x 1 w 2 U i o v B S T W L e H X v D U A w E 8 x R R F 3 e 9 o 1 6 x 5 J Q d s + x 5 4 G a g h G z V o + I L r t B H B I Y U Y 3 C E U I Q D e E j o 6 c C F g 5 i 4 L q b E S U L C x D n u U S B t S l m c M j x i R / Q d 0 q 6 T s S H t t W d i 1 I x O C e i V p L R x Q J q I 8 i R h f Z p t 4 q l x 1 u x v 3 l P j q e 8 2 o b + f e Y 2 J V b g m 9 i / d L P O / O l 2 L w g C n p g Z B N c W G 0 d W x z C U 1 X d E 3 t 7 9 U p c g h J k 7 j P s U l Y W a U s z 7 b R p O Y 2 n V v P R N / M 5 m a 1 X u W 5 a Z 4 1 7 e k A b s / x z k P m p W y 6 5 T d 8 + N S t Z K N O o 8 9 7 O O Q 5 n m C K m q o o 0 H e Q z z i C c 9 W z Q q t 1 L r 9 T L V y m W Y X 3 5 b 1 8 A E N + 5 A T &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C L X q B p c I l H 0 j f t D 1 W T P R S D D H 1 d 0 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I q 6 L L g p s u K 9 g G 1 l G Q 6 r U P T J E w m a i m C P + B W P 0 3 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h y I R D n O a 8 5 a W F x a X s m v F t b W N z a 3 i t s 7 z S R K J e M N F g W R b P t e w g M R 8 o Y S K u D t W H J v 7 A e 8 5 Y / O d L x 1 w 2 U i o v B S T W L e H X v D U A w E 8 x R R F 3 e 9 o 1 6 x 5 J Q d s + x 5 4 G a g h G z V o + I L r t B H B I Y U Y 3 C E U I Q D e E j o 6 c C F g 5 i 4 L q b E S U L C x D n u U S B t S l m c M j x i R / Q d 0 q 6 T s S H t t W d i 1 I x O C e i V p L R x Q J q I 8 i R h f Z p t 4 q l x 1 u x v 3 l P j q e 8 2 o b + f e Y 2 J V b g m 9 i / d L P O / O l 2 L w g C n p g Z B N c W G 0 d W x z C U 1 X d E 3 t 7 9 U p c g h J k 7 j P s U l Y W a U s z 7 b R p O Y 2 n V v P R N / M 5 m a 1 X u W 5 a Z 4 1 7 e k A b s / x z k P m p W y 6 5 T d 8 + N S t Z K N O o 8 9 7 O O Q 5 n m C K m q o o 0 H e Q z z i C c 9 W z Q q t 1 L r 9 T L V y m W Y X 3 5 b 1 8 A E N + 5 A T &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C L X q B p c I l H 0 j f t D 1 W T P R S D D H 1 d 0 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I q 6 L L g p s u K 9 g G 1 l G Q 6 r U P T J E w m a i m C P + B W P 0 3 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h y I R D n O a 8 5 a W F x a X s m v F t b W N z a 3 i t s 7 z S R K J e M N F g W R b P t e w g M R 8 o Y S K u D t W H J v 7 A e 8 5 Y / O d L x 1 w 2 U i o v B S T W L e H X v D U A w E 8 x R R F 3 e 9 o 1 6 x 5 J Q d s + x 5 4 G a g h G z V o + I L r t B H B I Y U Y 3 C E U I Q D e E j o 6 c C F g 5 i 4 L q b E S U L C x D n u U S B t S l m c M j x i R / Q d 0 q 6 T s S H t t W d i 1 I x O C e i V p L R x Q J q I 8 i R h f Z p t 4 q l x 1 u x v 3 l P j q e 8 2 o b + f e Y 2 J V b g m 9 i / d L P O / O l 2 L w g C n p g Z B N c W G 0 d W x z C U 1 X d E 3 t 7 9 U p c g h J k 7 j P s U l Y W a U s z 7 b R p O Y 2 n V v P R N / M 5 m a 1 X u W 5 a Z 4 1 7 e k A b s / x z k P m p W y 6 5 T d 8 + N S t Z K N O o 8 9 7 O O Q 5 n m C K m q o o 0 H e Q z z i C c 9 W z Q q t 1 L r 9 T L V y m W Y X 3 5 b 1 8 A E N + 5 A T &lt; / l a t e x i t &gt; y 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D F 3 Q y n H e 2 D 2 p z i C p V s g 5 A B U J 3 q Y = " &gt; A A A C x 3 i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I i 6 L L g R n c V 7 A O 0 l C S d t k P z Y j I p l u L C H 3 C r f y b + g f 6 F d 8 Y p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h r y T D r O a 8 F a W l 5 Z X S u u l z Y 2 t 7 Z 3 y r t 7 r S z J R c C a Q R I m o u N 7 G Q t 5 z J q S y 5 B 1 U s G 8 y A 9 Z 2 x + f q 3 h 7 w k T G k / h a T l P W j b x h z A c 8 8 K S i p j 2 3 1 C t X n K q j l 7 0 I X A M q M K u R l F 9 w i z 4 S B M g R g S G G J B z C Q 0 b P D V w 4 S I n r Y k a c I M R 1 n O E e J d L m l M U o w y N 2 T N 8 h 7 W 4 M G 9 N e e W Z a H d A p I b 2 C l D a O S J N Q n i C s T r N 1 P N f O i v 3 N e 6 Y 9 1 d 2 m 9 P e N V 0 S s x I j Y v 3 T z z P / q V C 0 S A 5 z p G j j V l G p G V R c Y l 1 x 3 R d 3 c / l K V J I e U O I X 7 F B e E A</formula><p>6 2 c 9 9 n W m k z X r n r r 6 f i b z l S s 2 g c m N 8 e 7 u i U N 2 P 0 5 z k X Q q l V d p + p e n V T q N T P q I g 5 w i G O a 5 y n q u E A D T f I e 4 R F P e L Y u r c S a W H e f q V b B a P b x b V k P H 0 o q k C Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = "</p><formula xml:id="formula_28">D F 3 Q y n H e 2 D 2 p z i C p V s g 5 A B U J 3 q Y = " &gt; A A A C x 3 i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I i 6 L L g R n c V 7 A O 0 l C S d t k P z Y j I p l u L C H 3 C r f y b + g f 6 F d 8 Y p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h r y T D r O a 8 F a W l 5 Z X S u u l z Y 2 t 7 Z 3 y r t 7 r S z J R c C a Q R I m o u N 7 G Q t 5 z J q S y 5 B 1 U s G 8 y A 9 Z 2 x + f q 3 h 7 w k T G k / h a T l P W j b x h z A c 8 8 K S i p j 2 3 1 C t X n K q j l 7 0 I X A M q M K u R l F 9 w i z 4 S B M g R g S G G J B z C Q 0 b P D V w 4 S I n r Y k a c I M R 1 n O E e J d L m l M U o w y N 2 T N 8 h 7 W 4 M G 9 N e e W Z a H d A p I b 2 C l D a O S J N Q n i C s T r N 1 P N f O i v 3 N e 6 Y 9 1 d 2 m 9 P e N V 0 S s x I j Y v 3 T z z P / q V C 0 S A 5 z p G j j V l G p G V R c Y l 1 x 3 R d 3 c / l K V J I e U O I X 7 F B e E A</formula><p>6 2 c 9 9 n W m k z X r n r r 6 f i b z l S s 2 g c m N 8 e 7 u i U N 2 P 0 5 z k X Q q l V d p + p e n V T q N T P q I g 5 w i G O a 5 y n q u E A D T f I e 4 R F P e L Y u r c S a W H e f q V b B a P b x b V k P H 0 o q k C Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " <ref type="bibr" target="#b16">6</ref> 2 C 0 e z j 2 7 I e P g B M i 5 A n &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T o R A e S A j J X f q b p 5 c p o <ref type="bibr" target="#b16">6</ref> 2 C 0 e z j 2 7 I e P g B M i 5 A n &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T o R A e S A j J X f q b p 5 c p o <ref type="bibr" target="#b16">6</ref> 2 C 0 e z j 2 7 I e P g B M i 5 A n &lt; / l a t e x i t &gt; .</p><formula xml:id="formula_29">D F 3 Q y n H e 2 D 2 p z i C p V s g 5 A B U J 3 q Y = " &gt; A A A C x 3 i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I i 6 L L g R n c V 7 A O 0 l C S d t k P z Y j I p l u L C H 3 C r f y b + g f 6 F d 8 Y p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h r y T D r O a 8 F a W l 5 Z X S u u l z Y 2 t 7 Z 3 y r t 7 r S z J R c C a Q R I m o u N 7 G Q t 5 z J q S y 5 B 1 U s G 8 y A 9 Z 2 x + f q 3 h 7 w k T G k / h a T l P W j b x h z A c 8 8 K S i p j 2 3 1 C t X n K q j l 7 0 I X A M q M K u R l F 9 w i z 4 S B M g R g S G G J B z C Q 0 b P D V w 4 S I n r Y k a c I M R 1 n O E e J d L m l M U o w y N 2 T N 8 h 7 W 4 M G 9 N e e W Z a H d A p I b 2 C l D a O S J N Q n i C s T r N 1 P N f O i v 3 N e 6 Y 9 1 d 2 m 9 P e N V 0 S s x I j Y v 3 T z z P / q V C 0 S A 5 z p G j j V l G p G V R c Y l 1 x 3 R d 3 c / l K V J I e U O I X 7 F B e E A 6 2 c 9 9 n W m k z X r n r r 6 f i b z l S s 2 g c m N 8 e 7 u i U N 2 P 0 5 z k X Q q l V d p + p e n V T q N T P q I g 5 w i G O a 5 y n q u E A D T f I e 4 R F P e L Y u r c S a W H e f q V b B a P b x b V k P H 0 o q k C Y = &lt; / l a t e x i t &gt; y 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T o R A e S A j J X f q b p 5 c p o 7 s M K O B c 9 Y = " &gt; A A A C x 3 i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I i 6 L L g R n c V 7 A O 0 l G Q 6 b U P z Y j I p l u L C H 3 C r f y b + g f 6 F d 8 Y p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h o G m X S c 1 4 K 1 t L y y u l Z c L 2 1 s b m 3 v l H f 3 W l m S C 8 a b L A k T 0 f G 9 j I d B z J s y k C H v p I J 7 k R / y t j 8 + V / H 2 h I s s S O J r O U 1 5 N / K G c T A I m C c V N e 3 V S r 1 y x a k 6 e t m L w D W g A r M a S f k F t + g j A U O O C B w x J O E Q H j J 6 b u D C Q U p c F z P i B K F A x z n u U S J t T l m c M j x i x / Q d 0 u 7 G s D H t l W e m 1 Y x O C e k V p L R x R J q E 8 g R h d Z q t 4 7 l 2 V u x v 3 j P t q e 4 2 p b 9 v v C J i J U b E / q W b Z / 5 X p 2 q R G O B M 1 x B Q T a l m V H X M u O S 6 K + r m 9 p e q J D m k x C n c p 7 g g z L R y 3 m d b a z J d u + q t p + N v O l O x a s 9 M b o 5 3 d U s a s P t z n I u g V a u 6 T t W 9 O q n U a 2 b U R R z g E M c 0 z 1 P U c Y E G m u Q 9 w i O e 8 G x d W o k 1 s e 4 + U</formula><formula xml:id="formula_30">7 s M K O B c 9 Y = " &gt; A A A C x 3 i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I i 6 L L g R n c V 7 A O 0 l G Q 6 b U P z Y j I p l u L C H 3 C r f y b + g f 6 F d 8 Y p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h o G m X S c 1 4 K 1 t L y y u l Z c L 2 1 s b m 3 v l H f 3 W l m S C 8 a b L A k T 0 f G 9 j I d B z J s y k C H v p I J 7 k R / y t j 8 + V / H 2 h I s s S O J r O U 1 5 N / K G c T A I m C c V N e 3 V S r 1 y x a k 6 e t m L w D W g A r M a S f k F t + g j A U O O C B w x J O E Q H j J 6 b u D C Q U p c F z P i B K F A x z n u U S J t T l m c M j x i x / Q d 0 u 7 G s D H t l W e m 1 Y x O C e k V p L R x R J q E 8 g R h d Z q t 4 7 l 2 V u x v 3 j P t q e 4 2 p b 9 v v C J i J U b E / q W b Z / 5 X p 2 q R G O B M 1 x B Q T a l m V H X M u O S 6 K + r m 9 p e q J D m k x C n c p 7 g g z L R y 3 m d b a z J d u + q t p + N v O l O x a s 9 M b o 5 3 d U s a s P t z n I u g V a u 6 T t W 9 O q n U a 2 b U R R z g E M c 0 z 1 P U c Y E G m u Q 9 w i O e 8 G x d W o k 1 s e 4 + U</formula><formula xml:id="formula_31">7 s M K O B c 9 Y = " &gt; A A A C x 3 i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I i 6 L L g R n c V 7 A O 0 l G Q 6 b U P z Y j I p l u L C H 3 C r f y b + g f 6 F d 8 Y p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h o G m X S c 1 4 K 1 t L y y u l Z c L 2 1 s b m 3 v l H f 3 W l m S C 8 a b L A k T 0 f G 9 j I d B z J s y k C H v p I J 7 k R / y t j 8 + V / H 2 h I s s S O J r O U 1 5 N / K G c T A I m C c V N e 3 V S r 1 y x a k 6 e t m L w D W g A r M a S f k F t + g j A U O O C B w x J O E Q H j J 6 b u D C Q U p c F z P i B K F A x z n u U S J t T l m c M j x i x / Q d 0 u 7 G s D H t l W e m 1 Y x O C e k V p L R x R J q E 8 g R h d Z q t 4 7 l 2 V u x v 3 j P t q e 4 2 p b 9 v v C J i J U b E / q W b Z / 5 X p 2 q R G O B M 1 x B Q T a l m V H X M u O S 6 K + r m 9 p e q J D m k x C n c p 7 g g z L R y 3 m d b a z J d u + q t p + N v O l O x a s 9 M b o 5 3 d U s a s P t z n I u g V a u 6 T t W 9 O q n U a 2 b U R R z g E M c 0 z 1 P U c Y E G m u Q 9 w i O e 8 G x d W o k 1 s e 4 + U</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e h Y g i</head><formula xml:id="formula_32">A t + B F X i s r 1 K Y T 8 P w b J O Z V o = " &gt; A A A C x H i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w F Z I i 6 L I g i M s W 7 A N q k W Q 6 r U P z I j M R S t E f c K v f J v 6 B / o V 3 x h T U I j o h y Z l z 7 z k z 9 9 4 g D Y V U r v t a s p a W V 1 b X y u u V j c 2 t 7 Z 3 q 7 l 5 H J n n G e J s l Y Z L 1 A l / y U M S 8 r Y Q K e S / N u B 8 F I e 8 G k 3 M d 7 9 7 x T I o k v l L T l A 8 i f x y L k W C + I q r l 3 F R r r u O a Z S 8 C r w A 1 F K u Z V F 9 w j S E S M O S I w B F D E Q 7 h Q 9 L T h w c X K X E D z I j L C A k T 5 7 h H h b Q 5 Z X H K 8 I m d 0 H d M u 3 7 B x r T X n t K o G Z 0 S 0 p u R 0 s Y R a R L K y w j r 0 2 w T z 4 2 z Z n / z n h l P f b c p / Y P C K y J W 4 Z b Y v 3 T z z P / q d C 0 K I 5 y Z G g T V l B p G V 8 c K l 9 x 0 R d / c / l K V I o e U O I 2 H F M 8 I M 6 O c 9 9 k 2 G m l q 1 7 3 1 T f z N Z G p W 7 1 m R m + N d 3 5 I G 7 P 0 c 5 y L o 1 B 3 P d b z W S a 1 R L 0 Z d x g E O c U z z P E U D l 2 i i b b w f 8 Y R n 6 8 I K L W n l n 6 l W q d D</formula><p>s 4 9 u y H j 4 A t n 2 P I w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e h Y g i</p><formula xml:id="formula_33">A t + B F X i s r 1 K Y T 8 P w b J O Z V o = " &gt; A A A C x H i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w F Z I i 6 L I g i M s W 7 A N q k W Q 6 r U P z I j M R S t E f c K v f J v 6 B / o V 3 x h T U I j o h y Z l z 7 z k z 9 9 4 g D Y V U r v t a s p a W V 1 b X y u u V j c 2 t 7 Z 3 q 7 l 5 H J n n G e J s l Y Z L 1 A l / y U M S 8 r Y Q K e S / N u B 8 F I e 8 G k 3 M d 7 9 7 x T I o k v l L T l A 8 i f x y L k W C + I q r l 3 F R r r u O a Z S 8 C r w A 1 F K u Z V F 9 w j S E S M O S I w B F D E Q 7 h Q 9 L T h w c X K X E D z I j L C A k T 5 7 h H h b Q 5 Z X H K 8 I m d 0 H d M u 3 7 B x r T X n t K o G Z 0 S 0 p u R 0 s Y R a R L K y w j r 0 2 w T z 4 2 z Z n / z n h l P f b c p / Y P C K y J W 4 Z b Y v 3 T z z P / q d C 0 K I 5 y Z G g T V l B p G V 8 c K l 9 x 0 R d / c / l K V I o e U O I 2 H F M 8 I M 6 O c 9 9 k 2 G m l q 1 7 3 1 T f z N Z G p W 7 1 m R m + N d 3 5 I G 7 P 0 c 5 y L o 1 B 3 P d b z W S a 1 R L 0 Z d x g E O c U z z P E U D l 2 i i b b w f 8 Y R n 6 8 I K L W n l n 6 l W q d D</formula><p>s 4 9 u y H j 4 A t n 2 P I w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e h Y g i  </p><formula xml:id="formula_34">A t + B F X i s r 1 K Y T 8 P w b J O Z V o = " &gt; A A A C x H i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w F Z I i 6 L I g i M s W 7 A N q k W Q 6 r U P z I j M R S t E f c K v f J v 6 B / o V 3 x h T U I j o h y Z l z 7 z k z 9 9 4 g D Y V U r v t a s p a W V 1 b X y u u V j c 2 t 7 Z 3 q 7 l 5 H J n n G e J s l Y Z L 1 A l / y U M S 8 r Y Q K e S / N u B 8 F I e 8 G k 3 M d 7 9 7 x T I o k v l L T l A 8 i f x y L k W C + I q r l 3 F R r r u O a Z S 8 C r w A 1 F K u Z V F 9 w j S E S M O S I w B F D E Q 7 h Q 9 L T h w c X K X E D z I j L C A k T 5 7 h H h b Q 5 Z X H K 8 I m d 0 H d M u 3 7 B x r T X n t K o G Z 0 S 0 p u R 0 s Y R a R L K y w j r 0 2 w T z 4 2 z Z n / z n h l P f b c p / Y P C K y J W 4 Z b Y v 3 T z z P / q d C 0 K I 5 y Z G g T V l B p G V 8 c K l 9 x 0 R d / c / l K V I o e U O I 2 H F M 8 I M 6 O c 9 9 k 2 G m l q 1 7 3 1 T f z N Z G p W</formula><formula xml:id="formula_35">I E E G y U V A W x T a u u X 3 x + X f M m 7 U r O c = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J N N p D c 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h k E m H e e 1 Y C 0 s L i 2 v F F f X 1 j c 2 t 7 Z L O 7 u t L M k F 4 0 2 W h I n o + F 7 G w y D m T R n I k H d S w b 3 I D 3 n b H 5 + p e P u G i y x I 4 k s 5 S X k v 8 k Z x M A y Y J 4 m 6 u O u 7 / V L Z q T h 6 2 f P A N a A M s x p J 6 Q V X G C A B Q 4 4 I H D E k 4 R A e M n q 6 c O E g J a 6 H K X G C U K D j H P d Y I 2 1 O W Z w y P G L H 9 B 3 R r m v Y m P b K M 9 N q R q e E 9 A p S 2 j g k T U J 5 g r A 6 z d b x X D s r 9 j f v q f Z U d 5 v Q 3 z d e E b E S 1 8 T + p Z t l / l e n a p E Y 4 l T X E F B N q W Z U d c y 4 5 L o r 6 u b 2 l 6 o k O a T E K T y g u C D M t H L W Z 1 t r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q z 0 x u j n d 1 S x q w + 3 O c 8 6 B V r b h O x T 0 / L t e q Z t R F 7 O M A R z T P E 9 R Q R w N N 8 h 7 h E U 9 4 t u p W b O X W 7 W e q V T C a P X x b 1 s M H C T u Q E Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I E E G y U V A W x T a u u X 3 x + X f M m 7 U r O c = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J N N p D c 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h k E m H e e 1 Y C 0 s L i 2 v F F f X 1 j c 2 t 7 Z L O 7 u t L M k F 4 0 2 W h I n o + F 7 G w y D m T R n I k H d S w b 3 I D 3 n b H 5 + p e P u G i y x I 4 k s 5 S X k v 8 k Z x M A y Y J 4 m 6 u O u 7 / V L Z q T h 6 2 f P A N a A M s x p J 6 Q V X G C A B Q 4 4 I H D E k 4 R A e M n q 6 c O E g J a 6 H K X G C U K D j H P d Y I 2 1 O W Z w y P G L H 9 B 3 R r m v Y m P b K M 9 N q R q e E 9 A p S 2 j g k T U J 5 g r A 6 z d b x X D s r 9 j f v q f Z U d 5 v Q 3 z d e E b E S 1 8 T + p Z t l / l e n a p E Y 4 l T X E F B N q W Z U d c y 4 5 L o r 6 u b 2 l 6 o k O a T E K T y g u C D M t H L W Z 1 t r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q z 0 x u j n d 1 S x q w + 3 O c 8 6 B V r b h O x T 0 / L t e q Z t R F 7 O M A R z T P E 9 R Q R w N N 8 h 7 h E U 9 4 t u p W b O X W 7 W e q V T C a P X x b 1 s M H C T u Q E Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I E E G y U V A W x T a u u X 3 x + X f M m 7 U r O c = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J N N p D c 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h k E m H e e 1 Y C 0 s L i 2 v F F f X 1 j c 2 t 7 Z L O 7 u t L M k F 4 0 2 W h I n o + F 7 G w y D m T R n I k H d S w b 3 I D 3 n b H 5 + p e P u G i y x I 4 k s 5 S X k v 8 k Z x M A y Y J 4 m 6 u O u 7 / V L Z q T h 6 2 f P A N a A M s x p J 6 Q V X G C A B Q 4 4 I H D E k 4 R A e M n q 6 c O E g J a 6 H K X G C U K D j H P d Y I 2 1 O W Z w y P G L H 9 B 3 R r m v Y m P b K M 9 N q R q e E 9 A p S 2 j g k T U J 5 g r A 6 z d b x X D s r 9 j f v q f Z U d 5 v Q 3 z d e E b E S 1 8 T + p Z t l / l e n a p E Y 4 l T X E F B N q W Z U d c y 4 5 L o r 6 u b 2 l 6 o k O a T E K T y g u C D M t H L W Z 1 t r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q z 0 x u j n d 1 S x q w + 3 O c 8 6 B V r b h O x T 0 / L t e q Z t R F 7 O M A R z T P E 9 R Q R w N N 8 h 7 h E U 9 4 t u p W b O X W 7 W e q V T C a P X x b 1 s M H C T u Q E Q = = &lt; / l a t e x i t &gt; x 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w + O 2 A V A N 5 t G Y s 3 Q A W 4 f 1 x G 8 R q I 4 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J O m 0 D s 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h j y T j v N a s B Y W l 5 Z X i q t r 6 x u b W 9 u l n d 1 W l u Q i Y M 0 g C R P R 8 b 2 M h T x m T c l l y D q p Y F 7 k h 6 z t j 8 9 U v H 3 D R M a T + F J O U t a L v F H M h z z w J F E X d / 1 q v 1 R 2 K o 5 e 9 j x w D S j D r E Z S e s E V B k g Q I E c E h h i S c A g P G T 1 d u H C Q E t f D l D h B i O s 4 w z 3 W S J t T F q M M j 9 g x f U e 0 6 x o 2 p r 3 y z L Q 6 o F N C e g U p b R y S J q E 8 Q V i d Z u t 4 r p 0 V + 5 v 3 V H u q u 0 3 o 7 x u v i F i J a 2 L / 0 s 0 y / 6 t T t U g M c a p r 4 F R T q h l V X W B c c t 0 V d X P 7 S 1 W S H F L i F B 5 Q X B A O t H L W Z 1 t</formula><p>r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q H 5 j c H O / q l j R g 9 + c 4 5 0 G r W n G d i n t + X K 5 V z a i L 2 M c B j m i e J 6 i h j g a a 5 D 3 C I 5 7 w b N W t 2 M q t 2 8 9 U q 2 A 0 e / i 2 r I c P C 5 u Q E g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w +</p><formula xml:id="formula_36">O 2 A V A N 5 t G Y s 3 Q A W 4 f 1 x G 8 R q I 4 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J O m 0 D s 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h j y T j v N a s B Y W l 5 Z X i q t r 6 x u b W 9 u l n d 1 W l u Q i Y M 0 g C R P R 8 b 2 M h T x m T c l l y D q p Y F 7 k h 6 z t j 8 9 U v H 3 D R M a T + F J O U t a L v F H M h z z w J F E X d / 1 q v 1 R 2 K o 5 e 9 j x w D S j D r E Z S e s E V B k g Q I E c E h h i S c A g P G T 1 d u H C Q E t f D l D h B i O s 4 w z 3 W S J t T F q M M j 9 g x f U e 0 6 x o 2 p r 3 y z L Q 6 o F N C e g U p b R y S J q E 8 Q V i d Z u t 4 r p 0 V + 5 v 3 V H u q u 0 3 o 7 x u v i F i J a 2 L / 0 s 0 y / 6 t T t U g M c a p r 4 F R T q h l V X W B c c t 0 V d X P 7 S 1 W S H F L i F B 5 Q X B A O t H L W Z 1 t</formula><p>r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q H 5 j c H O / q l j R g 9 + c 4 5 0 G r W n G d i n t + X K 5 V z a i L 2 M c B j m i e J 6 i h j g a a 5 D 3 C I 5 7 w b N W t 2 M q t 2 8 9 U q 2 A 0 e / i 2 r I c P C 5 u Q E g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w +</p><formula xml:id="formula_37">O 2 A V A N 5 t G Y s 3 Q A W 4 f 1 x G 8 R q I 4 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J O m 0 D s 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h j y T j v N a s B Y W l 5 Z X i q t r 6 x u b W 9 u l n d 1 W l u Q i Y M 0 g C R P R 8 b 2 M h T x m T c l l y D q p Y F 7 k h 6 z t j 8 9 U v H 3 D R M a T + F J O U t a L v F H M h z z w J F E X d / 1 q v 1 R 2 K o 5 e 9 j x w D S j D r E Z S e s E V B k g Q I E c E h h i S c A g P G T 1 d u H C Q E t f D l D h B i O s 4 w z 3 W S J t T F q M M j 9 g x f U e 0 6 x o 2 p r 3 y z L Q 6 o F N C e g U p b R y S J q E 8 Q V i d Z u t 4 r p 0 V + 5 v 3 V H u q u 0 3 o 7 x u v i F i J a 2 L / 0 s 0 y / 6 t T t U g M c a p r 4 F R T q h l V X W B c c t 0 V d X P 7 S 1 W S H F L i F B 5 Q X B A O t H L W Z 1 t</formula><p>r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q H 5 j c H O / q l j R g 9 + c 4 5 0 G r W n G d i n t + X K 5 V z a i L 2 M c B j m i e J 6 i h j g a a 5 D 3 C I 5 7 w b N W t 2 M q t 2 8 9 U q 2 A 0 e / i 2 r I c P C 5 u Q E g = = &lt; / l a t e x i t &gt; x 3 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C L X q B p c I l H 0 j f t    </p><formula xml:id="formula_38">D 1 W T P R S D D H 1 d 0 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I q 6 L L g p s u K 9 g G 1 l G Q 6 r U P T J E w m a i m C P + B W P 0 3 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h y I R D n O a 8 5 a W F x a X s m v F t b W N z a 3 i t s 7 z S R K J e M N F g W R b P t e w g M R 8 o Y S K u D t W H J v 7 A e 8 5 Y / O d L x 1 w 2 U i o v B S T W L e H X v D U A w E 8 x R R F 3 e 9 o 1 6 x 5 J Q d s + x 5 4 G a g h G z V o + I L r t B H B I Y U Y 3 C E U I Q D e E j o 6 c C F g 5 i 4 L q b E S U L C x D n u U S B t S l m c M j x i R / Q d 0 q 6 T s S H t t W d i 1 I x O C e i V p L R x Q J q I 8 i R h f Z p t 4 q l x 1 u x v 3 l P j q e 8 2 o b + f e Y 2 J V b g m 9 i / d L P O / O l 2 L w g C n p g Z B N c W G 0 d W x z C U 1 X d E 3 t 7 9 U p c g h J k 7 j P s U l Y W a U s z 7 b R p O Y 2 n V v P R N / M 5 m a 1 X u W 5 a Z 4 1 7 e k A b s / x z k P m p W y 6 5 T d 8 + N S t Z K N O o 8 9 7 O O Q 5 n m C K m q o o 0 H e Q z z i C c 9 W z Q q t 1 L r 9 T L V y m W Y X 3 5 b 1 8 A E N + 5 A T &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C L X q B p c I l H 0 j f t D 1 W T P R S D D H 1 d 0 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I q 6 L L g p s u K 9 g G 1 l G Q 6 r U P T J E w m a i m C P + B W P 0 3 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h y I R D n O a 8 5 a W F x a X s m v F t b W N z a 3 i t s 7 z S R K J e M N F g W R b P t e w g M R 8 o Y S K u D t W H J v 7 A e 8 5 Y / O d L x 1 w 2 U i o v B S T W L e H X v D U A w E 8 x R R F 3 e 9 o 1 6 x 5 J Q d s + x 5 4 G a g h G z V o + I L r t B H B I Y U Y 3 C E U I Q D e E j o 6 c C F g 5 i 4 L q b E S U L C x D n u U S B t S l m c M j x i R / Q d 0 q 6 T s S H t t W d i 1 I x O C e i V p L R x Q J q I 8 i R h f Z p t 4 q l x 1 u x v 3 l P j q e 8 2 o b + f e Y 2 J V b g m 9 i / d L P O / O l 2 L w g C n p g Z B N c W G 0 d W x z C U 1 X d E 3 t 7 9 U p c g h J k 7 j P s U l Y W a U s z 7 b R p O Y 2 n V v P R N / M 5 m a 1 X u W 5 a Z 4 1 7 e k A b s / x z k P m p W y 6 5 T d 8 + N S t Z K N O o 8 9 7 O O Q 5 n m C K m q o o 0 H e Q z z i C c 9 W z Q q t 1 L r 9 T L V y m W Y X 3 5 b 1 8 A E N + 5 A T &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C L X q B p c I l H 0 j f t D 1 W T P R S D D H 1 d 0 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I q 6 L L g p s u K 9 g G 1 l G Q 6 r U P T J E w m a i m C P + B W P 0 3 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h y I R D n O a 8 5 a W F x a X s m v F t b W N z a 3 i t s 7 z S R K J e M N F g W R b P t e w g M R 8 o Y S K u D t W H J v 7 A e 8 5 Y / O d L x 1 w 2 U i o v B S T W L e H X v D U A w E 8 x R R F 3 e 9 o 1 6 x 5 J Q d s + x 5 4 G a g h G z V o + I L r t B H B I Y U Y 3 C E U I Q D e E j o 6 c C F g 5 i 4 L q b E S U L C x D n u U S B t S l m c M j x i R / Q d 0 q 6 T s S H t t W d i 1 I x O C e i V p L R x Q J q I 8 i R h f Z p t 4 q l x 1 u x v 3 l P j q e 8 2 o b + f e Y 2 J V b g m 9 i / d L P O / O l 2 L w g C n p g Z B N c W G 0 d W x z C U 1 X d E 3 t 7 9 U p c g h J k 7 j P s U l Y W a U s z 7 b R p O Y 2 n V v P R N / M 5 m a 1 X u W 5 a Z 4 1 7 e k A b s / x z k P m p W y 6 5 T d 8 + N S t Z K N O o 8 9 7 O O Q 5 n m C K m q o o 0 H e Q z z i C c 9 W z Q q t 1 L r 9 T L V y m W Y X 3 5 b 1 8 A E N + 5 A T &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S J a g h o q w + W E R z M / u V S J O x y g / 7 W M = " &gt; A A A C x H i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w Y 0 m K o M u C I C 5 b s L V Q i y T T a Q 2 d J m F m I p S i P + B W v 0 3 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 Y S o i p T 3 v t e A s L C 4 t r x R X S 2 v r G 5 t b 5 e 2 d t k o y y X i L J S K R n T B Q X E Q x b + l I C 9 5 J J Q / G o e B X 4 e j M x K / u u F R R E l / q S c p 7 4 2 A Y R 4 O I B Z q o 5 t F N u e J V P b v c e e D n o I J 8 N Z L y C 6 7 R R w K G D G N w x N C E B Q I o e r r w 4 S E l r o c p c Z J Q Z O M c 9 y i R N q M s T h k B s S P 6 D m n X z d m Y 9 s Z T W T W j U w S 9 k p Q u D k i T U J 4 k b E 5 z b T y z z o b 9 z X t q P c 3 d J v Q P c 6 8 x s R q 3 x P 6 l m 2 X + V 2 d q 0 R j g 1 N Y Q U U 2 p Z U x 1 L H f J b F f M z d 0 v V W l y S I k z u E 9 x S Z h Z</formula><formula xml:id="formula_39">+ W E R z M / u V S J O x y g / 7 W M = " &gt; A A A C x H i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w Y 0 m K o M u C I C 5 b s L V Q i y T T a Q 2 d J m F m I p S i P + B W v 0 3 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 Y S o i p T 3 v t e A s L C 4 t r x R X S 2 v r G 5 t b 5 e 2 d t k o y y X i L J S K R n T B Q X E Q x b + l I C 9 5 J J Q / G o e B X 4 e j M x K / u u F R R E l / q S c p 7 4 2 A Y R 4 O I B Z q o 5 t F N u e J V P b v c e e D n o I J 8 N Z L y C 6 7 R R w K G D G N w x N C E B Q I o e r r w 4 S E l r o c p c Z J Q Z O M c 9 y i R N q M s T h k B s S P 6 D m n X z d m Y 9 s Z T W T W j U w S 9 k p Q u D k i T U J 4 k b E 5 z b T y z z o b 9 z X t q P c 3 d J v Q P c 6 8 x s R q 3 x P 6 l m 2 X + V 2 d q 0 R j g 1 N Y Q U U 2 p Z U x 1 L H f J b F f M z d 0 v V W l y S I k z u E 9 x S Z h Z</formula><formula xml:id="formula_40">+ W E R z M / u V S J O x y g / 7 W M = " &gt; A A A C x H i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w Y 0 m K o M u C I C 5 b s L V Q i y T T a Q 2 d J m F m I p S i P + B W v 0 3 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 Y S o i p T 3 v t e A s L C 4 t r x R X S 2 v r G 5 t b 5 e 2 d t k o y y X i L J S K R n T B Q X E Q x b + l I C 9 5 J J Q / G o e B X 4 e j M x K / u u F R R E l / q S c p 7 4 2 A Y R 4 O I B Z q o 5 t F N u e J V P b v c e e D n o I J 8 N Z L y C 6 7 R R w K G D G N w x N C E B Q I o e r r w 4 S E l r o c p c Z J Q Z O M c 9 y i R N q M s T h k B s S P 6 D m n X z d m Y 9 s Z T W T W j U w S 9 k p Q u D k i T U J 4 k b E 5 z b T y z z o b 9 z X t q P c 3 d J v Q P c 6 8 x s R q 3 x P 6 l m 2 X + V 2 d q 0 R j g 1 N Y Q U U 2 p Z U x 1 L H f J b F f M z d 0 v V W l y S I k z u E 9 x S Z h Z</formula><formula xml:id="formula_41">I E E G y U V A W x T a u u X 3 x + X f M m 7 U r O c = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J N N p D c 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h k E m H e e 1 Y C 0 s L i 2 v F F f X 1 j c 2 t 7 Z L O 7 u t L M k F 4 0 2 W h I n o + F 7 G w y D m T R n I k H d S w b 3 I D 3 n b H 5 + p e P u G i y x I 4 k s 5 S X k v 8 k Z x M A y Y J 4 m 6 u O u 7 / V L Z q T h 6 2 f P A N a A M s x p J 6 Q V X G C A B Q 4 4 I H D E k 4 R A e M n q 6 c O E g J a 6 H K X G C U K D j H P d Y I 2 1 O W Z w y P G L H 9 B 3 R r m v Y m P b K M 9 N q R q e E 9 A p S 2 j g k T U J 5 g r A 6 z d b x X D s r 9 j f v q f Z U d 5 v Q 3 z d e E b E S 1 8 T + p Z t l / l e n a p E Y 4 l T X E F B N q W Z U d c y 4 5 L o r 6 u b 2 l 6 o k O a T E K T y g u C D M t H L W Z 1 t r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q z 0 x u j n d 1 S x q w + 3 O c 8 6 B V r b h O x T 0 / L t e q Z t R F 7 O M A R z T P E 9 R Q R w N N 8 h 7 h E U 9 4 t u p W b O X W 7 W e q V T C a P X x b 1 s M H C T u Q E Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I E E G y U V A W x T a u u X 3 x + X f M m 7 U r O c = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J N N p D c 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h k E m H e e 1 Y C 0 s L i 2 v F F f X 1 j c 2 t 7 Z L O 7 u t L M k F 4 0 2 W h I n o + F 7 G w y D m T R n I k H d S w b 3 I D 3 n b H 5 + p e P u G i y x I 4 k s 5 S X k v 8 k Z x M A y Y J 4 m 6 u O u 7 / V L Z q T h 6 2 f P A N a A M s x p J 6 Q V X G C A B Q 4 4 I H D E k 4 R A e M n q 6 c O E g J a 6 H K X G C U K D j H P d Y I 2 1 O W Z w y P G L H 9 B 3 R r m v Y m P b K M 9 N q R q e E 9 A p S 2 j g k T U J 5 g r A 6 z d b x X D s r 9 j f v q f Z U d 5 v Q 3 z d e E b E S 1 8 T + p Z t l / l e n a p E Y 4 l T X E F B N q W Z U d c y 4 5 L o r 6 u b 2 l 6 o k O a T E K T y g u C D M t H L W Z 1 t r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q z 0 x u j n d 1 S x q w + 3 O c 8 6 B V r b h O x T 0 / L t e q Z t R F 7 O M A R z T P E 9 R Q R w N N 8 h 7 h E U 9 4 t u p W b O X W 7 W e q V T C a P X x b 1 s M H C T u Q E Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I E E G y U V A W x T a u u X 3 x + X f M m 7 U r O c = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J N N p D c 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h k E m H e e 1 Y C 0 s L i 2 v F F f X 1 j c 2 t 7 Z L O 7 u t L M k F 4 0 2 W h I n o + F 7 G w y D m T R n I k H d S w b 3 I D 3 n b H 5 + p e P u G i y x I 4 k s 5 S X k v 8 k Z x M A y Y J 4 m 6 u O u 7 / V L Z q T h 6 2 f P A N a A M s x p J 6 Q V X G C A B Q 4 4 I H D E k 4 R A e M n q 6 c O E g J a 6 H K X G C U K D j H P d Y I 2 1 O W Z w y P G L H 9 B 3 R r m v Y m P b K M 9 N q R q e E 9 A p S 2 j g k T U J 5 g r A 6 z d b x X D s r 9 j f v q f Z U d 5 v Q 3 z d e E b E S 1 8 T + p Z t l / l e n a p E Y 4 l T X E F B N q W Z U d c y 4 5 L o r 6 u b 2 l 6 o k O a T E K T y g u C D M t H L W Z 1 t r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q z 0 x u j n d 1 S x q w + 3 O c 8 6 B V r b h O x T 0 / L t e q Z t R F 7 O M A R z T P E 9 R Q R w N N 8 h 7 h E U 9 4 t u p W b O X W 7 W e q V T C a P X x b 1 s M H C T u Q E Q = = &lt; / l a t e x i t &gt; x 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w + O 2 A V A N 5 t G Y s 3 Q A W 4 f 1 x G 8 R q I 4 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J O m 0 D s 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h j y T j v N a s B Y W l 5 Z X i q t r 6 x u b W 9 u l n d 1 W l u Q i Y M 0 g C R P R 8 b 2 M h T x m T c l l y D q p Y F 7 k h 6 z t j 8 9 U v H 3 D R M a T + F J O U t a L v F H M h z z w J F E X d / 1 q v 1 R 2 K o 5 e 9 j x w D S j D r E Z S e s E V B k g Q I E c E h h i S c A g P G T 1 d u H C Q E t f D l D h B i</formula><p>O s 4 w z 3 W S J t T F q M M j 9 g x f U e 0 6 x o 2 p r 3 y z L Q 6 o F N C e g U p b R y S J q E 8 Q V i d Z u t 4 r p 0 V + 5 v 3 V H u q u 0 3 o 7 x u v i F i J a 2 L / 0 s 0 y / 6 t T t U g M c a p r 4 F R T q h l V X W B c c t 0 V d X P 7 S 1 W S H F L i F B 5 Q X B A O t H L W Z 1 t r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q H 5 j c H O / q l j R g 9 + c 4 5 0 G r W n G d i n t + X K 5 V z a i L 2 M c B j m i e J 6 i h j g a a 5 D 3 C I 5 7 w b N W t 2 M q t 2 8 9 U q 2 A 0 e / i 2 r I c P C 5 u Q E g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w</p><formula xml:id="formula_42">+ O 2 A V A N 5 t G Y s 3 Q A W 4 f 1 x G 8 R q I 4 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J O m 0 D s 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h j y T j v N a s B Y W l 5 Z X i q t r 6 x u b W 9 u l n d 1 W l u Q i Y M 0 g C R P R 8 b 2 M h T x m T c l l y D q p Y F 7 k h 6 z t j 8 9 U v H 3 D R M a T + F J O U t a L v F H M h z z w J F E X d / 1 q v 1 R 2 K o 5 e 9 j x w D S j D r E Z S e s E V B k g Q I E c E h h i S c A g P G T 1 d u H C Q E t f D l D h B i</formula><p>O s 4 w z 3 W S J t T F q M M j 9 g x f U e 0 6 x o 2 p r 3 y z L Q 6 o F N C e g U p b R y S J q E 8 Q V i d Z u t 4 r p 0 V + 5 v 3 V H u q u 0 3 o 7 x u v i F i J a 2 L / 0 s 0 y / 6 t T t U g M c a p r 4 F R T q h l V X W B c c t 0 V d X P 7 S 1 W S H F L i F B 5 Q X B A O t H L W Z 1 t r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q H 5 j c H O / q l j R g 9 + c 4 5 0 G r W n G d i n t + X K 5 V z a i L 2 M c B j m i e J 6 i h j g a a 5 D 3 C I 5 7 w b N W t 2 M q t 2 8 9 U q 2 A 0 e / i 2 r I c P C 5 u Q E g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w</p><formula xml:id="formula_43">+ O 2 A V A N 5 t G Y s 3 Q A W 4 f 1 x G 8 R q I 4 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J O m 0 D s 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h j y T j v N a s B Y W l 5 Z X i q t r 6 x u b W 9 u l n d 1 W l u Q i Y M 0 g C R P R 8 b 2 M h T x m T c l l y D q p Y F 7 k h 6 z t j 8 9 U v H 3 D R M a T + F J O U t a L v F H M h z z w J F E X d / 1 q v 1 R 2 K o 5 e 9 j x w D S j D r E Z S e s E V B k g Q I E c E h h i S c A g P G T 1 d u H C Q E t f D l D h B i</formula><p>O s 4 w z 3 W S J t T F q M M j 9 g x f U e 0 6 x o 2 p r 3 y z L Q 6 o F N C e g U p b R y S J q E 8 Q V i d Z u t 4 r p 0 V + 5 v 3 V H u q u 0 3 o 7 x u v i F i J a 2 L / 0 s 0 y / 6 t T t U g M c a p r 4 F R T q h l V X W B c c t 0 V d X P 7 S 1 W S H F L i F B 5 Q X B A O t H L W Z 1 t r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q H 5 j c H O / q l j R g 9 + c 4 5 0 G r W n G d i n t + X K 5 V z a i L 2 M c B j m i e J 6 i h j g a a 5 D 3 C I 5 7 w b N W t 2 M q t 2 8 9 U q 2 A 0 e / i 2 r I c P C 5 u Q E g = = &lt; / l a t e x i t &gt; x 3 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C L X q B p c I l H 0 j f t D 1 W T P R S D D H 1 d 0 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I q 6 L L g p s u K 9 g G 1 l G Q 6 r U P T J E w m a i m C P + B W P 0 3 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h y</p><formula xml:id="formula_44">I R D n O a 8 5 a W F x a X s m v F t b W N z a 3 i t s 7 z S R K J e M N F g W R b P t e w g M R 8 o Y S K u D t W H J v 7 A e 8 5 Y / O d L x 1 w 2 U i o v B S T W L e H X v D U A w E 8 x R R F 3 e 9 o 1 6 x 5 J Q d s + x 5 4 G a g h G z V o + I L r t B H B I Y U Y 3 C E U I Q D e E j o 6 c C F g 5 i 4 L q b E S U L C x D n u U S B t S l m c M j x i R / Q d 0 q 6 T s S H t t W d i 1 I x O C e i V p L R x Q J q I 8 i R h f Z p t 4 q l x 1 u x v 3 l P j q e 8 2 o b + f e Y 2 J V b g m 9 i / d L P O / O l 2 L w g C n p g Z B N c W G 0 d W x z C U 1 X d E 3 t 7 9 U p c g h J k 7 j P s U l Y W a U s z 7 b R p O Y 2 n V v P R N / M 5 m a 1 X u W 5 a Z 4 1 7 e k A b s / x z k P m p W y 6 5 T d 8 + N S t Z K N O o 8 9 7 O O Q 5 n m C K m q o o 0 H e Q z z i C c 9 W z Q q t 1 L r 9 T L V y m W Y X 3 5 b 1 8 A E N + 5 A T &lt; /</formula><p>l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C L X q B p c I l H 0 j f t D 1 W T P R S D D H 1 d 0 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I q 6 L L g p s u K 9 g G 1 l G Q 6 r U P T J E w m a i m C P + B W P 0 3 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h y</p><formula xml:id="formula_45">I R D n O a 8 5 a W F x a X s m v F t b W N z a 3 i t s 7 z S R K J e M N F g W R b P t e w g M R 8 o Y S K u D t W H J v 7 A e 8 5 Y / O d L x 1 w 2 U i o v B S T W L e H X v D U A w E 8 x R R F 3 e 9 o 1 6 x 5 J Q d s + x 5 4 G a g h G z V o + I L r t B H B I Y U Y 3 C E U I Q D e E j o 6 c C F g 5 i 4 L q b E S U L C x D n u U S B t S l m c M j x i R / Q d 0 q 6 T s S H t t W d i 1 I x O C e i V p L R x Q J q I 8 i R h f Z p t 4 q l x 1 u x v 3 l P j q e 8 2 o b + f e Y 2 J V b g m 9 i / d L P O / O l 2 L w g C n p g Z B N c W G 0 d W x z C U 1 X d E 3 t 7 9 U p c g h J k 7 j P s U l Y W a U s z 7 b R p O Y 2 n V v P R N / M 5 m a 1 X u W 5 a Z 4 1 7 e k A b s / x z k P m p W y 6 5 T d 8 + N S t Z K N O o 8 9 7 O O Q 5 n m C K m q o o 0 H e Q z z i C c 9 W z Q q t 1 L r 9 T L V y m W Y X 3 5 b 1 8 A E N + 5 A T &lt; /</formula><p>l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C L X q B p c I l H 0 j f t D 1 W T P R S D D H 1 d 0 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I q 6 L L g p s u K 9 g G 1 l G Q 6 r U P T J E w m a i m C P + B W P 0 3 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h y</p><formula xml:id="formula_46">I R D n O a 8 5 a W F x a X s m v F t b W N z a 3 i t s 7 z S R K J e M N F g W R b P t e w g M R 8 o Y S K u D t W H J v 7 A e 8 5 Y / O d L x 1 w 2 U i o v B S T W L e H X v D U A w E 8 x R R F 3 e 9 o 1 6 x 5 J Q d s + x 5 4 G a g h G z V o + I L r t B H B I Y U Y 3 C E U I Q D e E j o 6 c C F g 5 i 4 L q b E S U L C x D n u U S B t S l m c M j x i R / Q d 0 q 6 T s S H t t W d i 1 I x O C e i V p L R x Q J q I 8 i R h f Z p t 4 q l x 1 u x v 3 l P j q e 8 2 o b + f e Y 2 J V b g m 9 i / d L P O / O l 2 L w g C n p g Z B N c W G 0 d W x z C U 1 X d E 3 t 7 9 U p c g h J k 7 j P s U l Y W a U s z 7 b R p O Y 2 n V v P R N / M 5 m a 1 X u W 5 a Z 4 1 7 e k A b s / x z k P m p W y 6 5 T d 8 + N S t Z K N O o 8 9 7 O O Q 5 n m C K m q o o 0 H e Q z z i C c 9 W z Q q t 1 L r 9 T L V y m W Y X 3 5 b 1 8 A E N + 5 A T &lt; / l a t e x i t &gt; y 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D F 3 Q y n H e 2 D 2 p z i C p V s g 5 A B U J 3 q Y = " &gt; A A A C x 3 i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I i 6 L L g R n c V 7 A O 0 l C S d t k P z Y j I p l u L C H 3 C r f y b + g f 6 F d 8 Y p q E V</formula><p>0 Q p I z 5 9 5 z Z u 6 9 f h r y T D r O a 8 F a W l 5 Z X S u u l z Y 2 t 7 Z 3 y r t 7 r S z J R c C a Q R I m o u N 7 G Q t 5 z J q S y 5 B 1 U s G 8 y A 9 Z 2 x + f q 3 h 7 w k T G k / h a T l P W j b x h z A c 8 8 K S i p j 2 3 1 C t X n K q j l 7 0</p><formula xml:id="formula_47">I X A M q M K u R l F 9 w i z 4 S B M g R g S G G J B z C Q 0 b P D V w 4 S I n r Y k a c I M R 1 n O E e J d L m l M U o w y N 2 T N 8 h 7 W 4 M G 9 N e e W Z a H d A p I b 2 C l D a O S J N Q n i C s T r N 1 P N f O i v 3 N e 6 Y</formula><p>9 1 d 2 m 9 P e N V 0 S s x I j Y v 3 T z z P / q V C 0 S A 5 z p G j j V l G p G V R c Y l 1 x 3 R d 3 c / l K V J I e U O I X 7 F B e E A 6 2 c 9 9 n W m k z X r n r r 6 f i b z l S s 2 g c m N 8 e 7 u i U N 2 P 0 5 z k X Q q l V d p + p e n V T q N T P q I g 5 w i G O a 5 y n q u E A D T f I e 4 R F P e L Y u r c S a W H e f q V b B a P b x b V k P H 0 o q k C Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D F 3 Q y n H e 2 D 2 p z i C p V s g 5 A B U J 3 q Y = " &gt; A</p><formula xml:id="formula_48">A A C x 3 i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I i 6 L L g R n c V 7 A O 0 l C S d t k P z Y j I p l u L C H 3 C r f y b + g f 6 F d 8 Y p q E V</formula><p>0 Q p I z 5 9 5 z Z u 6 9 f h r y T D r O a 8 F a W l 5 Z X S u u l z Y 2 t 7 Z 3 y r t 7 r S z J R c C a Q R I m o u N 7 G Q t 5 z J q S y 5 B 1 U s G 8 y A 9 Z 2 x + f q 3 h 7 w k T G k / h a T l P W j b x h z A c 8 8 K S i p j 2 3 1 C t X n K q j l 7 0</p><formula xml:id="formula_49">I X A M q M K u R l F 9 w i z 4 S B M g R g S G G J B z C Q 0 b P D V w 4 S I n r Y k a c I M R 1 n O E e J d L m l M U o w y N 2 T N 8 h 7 W 4 M G 9 N e e W Z a H d A p I b 2 C l D a O S J N Q n i C s T r N 1 P N f O i v 3 N e 6 Y</formula><p>9 1 d 2 m 9 P e N V 0 S s x I j Y v 3 T z z P / q V C 0 S A 5 z p G j j V l G p G V R c Y l 1 x 3 R d 3 c / l K V J I e U O I X 7 F B e E A 6 2 c 9 9 n W m k z X r n r r 6 f i b z l S s 2 g c m N 8 e 7 u i U N 2 P 0 5 z k X Q q l V d p + p e n V T q N T P q I g 5 w i G O a 5 y n q u E A D T f I e 4 R F P e L Y u r c S a W H e f q V b B a P b x b V k P H 0 o q k C Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D F 3 Q y n H e 2 D 2 p z i C p V s g 5 A B U J 3 q Y = " &gt; A</p><formula xml:id="formula_50">A A C x 3 i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I i 6 L L g R n c V 7 A O 0 l C S d t k P z Y j I p l u L C H 3 C r f y b + g f 6 F d 8 Y p q E V</formula><p>0 Q p I z 5 9 5 z Z u 6 9 f h r y T D r O a 8 F a W l 5 Z X S u u l z Y 2 t 7 Z 3 y r t 7 r S z J R c C a Q R I m o u N 7 G Q t 5 z J q S y 5 B 1 U s G 8 y A 9 Z 2 x + f q 3 h 7 w k T G k / h a T l P W j b x h z A c 8 8 K S i p j 2 3 1 C t X n K q j l 7 0</p><formula xml:id="formula_51">I X A M q M K u R l F 9 w i z 4 S B M g R g S G G J B z C Q 0 b P D V w 4 S I n r Y k a c I M R 1 n O E e J d L m l M U o w y N 2 T N 8 h 7 W 4 M G 9 N e e W Z a H d A p I b 2 C l D a O S J N Q n i C s T r N 1 P N f O i v 3 N e 6 Y</formula><p>9 1 d 2 m 9 P e N V 0 S s x I j Y v 3 T z z P / q V C 0 S A 5 z p G j j V l G p G V R c Y l 1 x 3 R d 3 c / l K V J I e U O I X 7 F B e E A 6 2 c 9 9 n W m k z X r n r r 6 f i b z l S s 2 g c m N 8 e 7 u i U N 2 P 0 5 z k X Q q l V d p + p e n V T q N T P q I g 5 w i G O a 5 y n q u E A D T f I e 4 R F P e L Y u r c S a W H e f q V b B a P b x b V k P H 0 o q k C Y = &lt; / l a t e x i t &gt; y 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T o R A e S A j J X f q b p 5 c p o 7 s M   </p><formula xml:id="formula_52">K O B c 9 Y = " &gt; A A A C x 3 i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I i 6 L L g R n c V 7 A O 0 l G Q 6 b U P z Y j I p l u L C H 3 C r f y b + g f 6 F d 8 Y p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h o G m X S c 1 4 K 1 t L y y u l Z c L 2 1 s b m 3 v l H f 3 W l m S C 8 a b L A k T 0 f G 9 j I d B z J s y k C H v p I J 7 k R / y t j 8 + V / H 2 h I s s S O J r O U 1 5 N / K G c T A I m C c V N e 3 V S r 1 y x a k 6 e t m L w D W g A r M a S f k F t + g j A U O O C B w x J O E Q H j J 6 b u D C Q U p c F z P i B K F A x z n u U S J t T l m c M j x i x / Q d 0 u 7 G s D H t l W e m 1 Y x O C e k V p L R x R J q E 8 g R h d Z q t 4 7 l 2 V u x v 3 j P t q e 4 2 p b 9 v v C J i J U b E / q W b Z / 5 X p 2 q R G O B M 1 x B Q T a l m V H X M u O S 6 K +</formula><formula xml:id="formula_53">Y = " &gt; A A A C x 3 i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I i 6 L L g R n c V 7 A O 0 l G Q 6 b U P z Y j I p l u L C H 3 C r f y b + g f 6 F d 8 Y p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h o G m X S c 1 4 K 1 t L y y u l Z c L 2 1 s b m 3 v l H f 3 W l m S C 8 a b L A k T 0 f G 9 j I d B z J s y k C H v p I J 7 k R / y t j 8 + V / H 2 h I s s S O J r O U 1 5 N / K G c T A I m C c V N e 3 V S r 1 y x a k 6 e t m L w D W g A r M a S f k F t + g j A U O O C B w x J O E Q H j J 6 b u D C Q U p c F z P i B K F A x z n u U S J t T l m c M j x i x / Q d 0 u 7 G s D H t l W e m 1 Y x O C e k V p L R x R J q E 8 g R h d Z q t 4 7 l 2 V u x v 3 j P t q e 4 2 p b 9 v v C J i J U b E / q W b Z / 5 X p 2 q R G O B M 1 x B Q T a l m V H X M u O S 6 K +</formula><formula xml:id="formula_54">Y = " &gt; A A A C x 3 i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I i 6 L L g R n c V 7 A O 0 l G Q 6 b U P z Y j I p l u L C H 3 C r f y b + g f 6 F d 8 Y p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h o G m X S c 1 4 K 1 t L y y u l Z c L 2 1 s b m 3 v l H f 3 W l m S C 8 a b L A k T 0 f G 9 j I d B z J s y k C H v p I J 7 k R / y t j 8 + V / H 2 h I s s S O J r O U 1 5 N / K G c T A I m C c V N e 3 V S r 1 y x a k 6 e t m L w D W g A r M a S f k F t + g j A U O O C B w x J O E Q H j J 6 b u D C Q U p c F z P i B K F A x z n u U S J t T l m c M j x i x / Q d 0 u 7 G s D H t l W e m 1 Y x O C e k V p L R x R J q E 8 g R h d Z q t 4 7 l 2 V u x v 3 j P t q e 4 2 p b 9 v v C J i J U b E / q W b Z / 5 X p 2 q R G O B M 1 x B Q T a l m V H X M u O S 6 K +</formula><formula xml:id="formula_55">O 2 A V A N 5 t G Y s 3 Q A W 4 f 1 x G 8 R q I 4 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J O m 0 D s 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h j y T j v N a s B Y W l 5 Z X i q t r 6 x u b W 9 u l n d 1 W l u Q i Y M 0 g C R P R 8 b 2 M h T x m T c l l y D q p Y F 7 k h 6 z t j 8 9 U v H 3 D R M a T + F J O U t a L v F H M h z z w J F E X d / 1 q v 1 R 2 K o 5 e 9 j x w D S j D r E Z S e s E V B k g Q I E c E h h i S c A g P G T 1 d u H C Q E t f D l D h B i</formula><p>O s 4 w z 3 W S J t T F q M M j 9 g x f U e 0 6 x o 2 p r 3 y z L Q 6 o F N C e g U p b R y S J q E 8 Q V i d Z u t 4 r p 0 V + 5 v 3 V H u q u 0 3 o 7 x u v i F i J a 2 L / 0 s 0 y / 6 t T t U g M c a p r 4 F R T q h l V X W B c c t 0 V d X P 7 S 1 W S H F L i F B 5 Q X B A O t H L W Z 1 t r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q H 5 j c H O / q l j R g 9 + c 4 5 0 G r W n G d i n t + X K 5 V z a i L 2 M c B j m i e J 6 i h j g a a 5 D 3 C I 5 7 w b N W t 2 M q t 2 8 9 U q 2 A 0 e / i 2 r I c P C 5 u Q E g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w +</p><formula xml:id="formula_56">O 2 A V A N 5 t G Y s 3 Q A W 4 f 1 x G 8 R q I 4 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J O m 0 D s 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h j y T j v N a s B Y W l 5 Z X i q t r 6 x u b W 9 u l n d 1 W l u Q i Y M 0 g C R P R 8 b 2 M h T x m T c l l y D q p Y F 7 k h 6 z t j 8 9 U v H 3 D R M a T + F J O U t a L v F H M h z z w J F E X d / 1 q v 1 R 2 K o 5 e 9 j x w D S j D r E Z S e s E V B k g Q I E c E h h i S c A g P G T 1 d u H C Q E t f D l D h B i</formula><p>O s 4 w z 3 W S J t T F q M M j 9 g x f U e 0 6 x o 2 p r 3 y z L Q 6 o F N C e g U p b R y S J q E 8 Q V i d Z u t 4 r p 0 V + 5 v 3 V H u q u 0 3 o 7 x u v i F i J a 2 L / 0 s 0 y / 6 t T t U g M c a p r 4 F R T q h l V X W B c c t 0 V d X P 7 S 1 W S H F L i F B 5 Q X B A O t H L W Z 1 t r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q H 5 j c H O / q l j R g 9 + c 4 5 0 G r W n G d i n t + X K 5 V z a i L 2 M c B j m i e J 6 i h j g a a 5 D 3 C I 5 7 w b N W t 2 M q t 2 8 9 U q 2 A 0 e / i 2 r I c P C 5 u Q E g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w +</p><formula xml:id="formula_57">O 2 A V A N 5 t G Y s 3 Q A W 4 f 1 x G 8 R q I 4 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J O m 0 D s 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h j y T j v N a s B Y W l 5 Z X i q t r 6 x u b W 9 u l n d 1 W l u Q i Y M 0 g C R P R 8 b 2 M h T x m T c l l y D q p Y F 7 k h 6 z t j 8 9 U v H 3 D R M a T + F J O U t a L v F H M h z z w J F E X d / 1 q v 1 R 2 K o 5 e 9 j x w D S j D r E Z S e s E V B k g Q I E c E h h i S c A g P G T 1 d u H C Q E t f D l D h B i</formula><p>O s 4 w z 3 W S J t T F q M M j 9 g x f U e 0 6 x o 2 p r 3 y z L Q 6 o F N C e g U p b R y S J q E 8 Q V i d Z u t 4 r p 0 V + 5 v 3 V H u q u 0 3 o 7 x u v i F i J a 2 L / 0 s 0 y / 6 t T t U g M c a p r 4 F R T q h l V X W B c c t 0 V d X P 7 S 1 W S H F L i F B 5 Q X B A O t H L W Z 1 t r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q H 5 j c H O / q l j R g 9 + c 4 5 0 G r W n G d i n t + X K 5 V z a i L 2 M c B j m i e J 6 i h j g a a 5 D 3 C I 5 7 w b N W t 2 M q t 2 8 9 U q 2 A 0 e / i 2 r I c P C 5 u Q E g = = &lt; / l a t e x i t &gt; x 3 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C L X q B p c I l H 0 j f t D 1 W T P R S D D H 1 d 0 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I q 6 L L g p s u K 9 g G 1 l G Q 6 r U P T J E w m a i m C P + B W P 0 3 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h y</p><formula xml:id="formula_58">I R D n O a 8 5 a W F x a X s m v F t b W N z a 3 i t s 7 z S R K J e M N F g W R b P t e w g M R 8 o Y S K u D t W H J v 7 A e 8 5 Y / O d L x 1 w 2 U i o v B S T W L e H X v D U A w E 8 x R R F 3 e 9 o 1 6 x 5 J Q d s + x 5 4 G a g h G z V o + I L r t B H B I Y U Y 3 C E U I Q D e E j o 6 c C F g 5 i 4 L q b E S U L C x D n u U S B t S l m c M j x i R / Q d 0 q 6 T s S H t t W d i 1 I x O C e i V p L R x Q J q I 8 i R h f Z p t 4 q l x 1 u x v 3 l P j q e 8 2 o b + f e Y 2 J V b g m 9 i / d L P O / O l 2 L w g C n p g Z B N c W G 0 d W x z C U 1 X d E 3 t 7 9 U p c g h J k 7 j P s U l Y W a U s z 7 b R p O Y 2 n V v P R N / M 5 m a 1 X u W 5 a Z 4 1 7 e k A b s / x z k P m p W y 6 5 T d 8 + N S t Z K N O o 8 9 7 O O Q 5 n m C K m q o o 0 H e Q z z i C c 9 W z Q q t 1 L r 9 T L V y m W Y X 3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">b 1 8 A E N + A T &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C L X q B p c I l H 0 j f t D 1 W T P R S D D H 1 d 0 = " &gt;</head><formula xml:id="formula_59">A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I q 6 L L g p s u K 9 g G 1 l G Q 6 r U P T J E w m a i m C P + B W P 0 3 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h y I R D n O a 8 5 a W F x a X s m v F t b W N z a 3 i t s 7 z S R K J e M N F g W R b P t e w g M R 8 o Y S K u D t W H J v 7 A e 8 5 Y / O d L x 1 w 2 U i o v B S T W L e H X v D U A w E 8 x R R F 3 e 9 o 1 6 x 5 J Q d s + x 5 4 G a g h G z V o + I L r t B H B I Y U Y 3 C E U I Q D e E j o 6 c C F g 5 i 4 L q b E S U L C x D n u U S B t S l m c M j x i R / Q d 0 q 6 T s S H t t W d i 1 I x O C e i V p L R x Q J q I 8 i R h f Z p t 4 q l x 1 u x v 3 l P j q e 8 2 o b + f e Y 2 J V b g m 9 i / d L P O / O l 2 L w g C n p g Z B N c W G 0 d W x z C U 1 X d E 3 t 7 9 U p c g h J k 7 j P s U l Y W a U s z 7 b R p O Y 2 n V v P R N / M 5 m a 1 X u W 5 a Z 4 1 7 e k A b s / x z k P m p W y 6 5 T d 8 + N S t Z K N O o 8 9 7 O O Q 5 n m C K m q o o 0 H e Q z z i C c 9 W z Q q t 1 L r 9 T L V y m W Y X 3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">b 1 8 A E N + A T &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C L X q B p c I l H 0 j f t D 1 W T P R S D D H 1 d 0 = " &gt;</head><formula xml:id="formula_60">A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I q 6 L L g p s u K 9 g G 1 l G Q 6 r U P T J E w m a i m C P + B W P 0 3 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h y I R D n O a 8 5 a W F x a X s m v F t b W N z a 3 i t s 7 z S R K J e M N F g W R b P t e w g M R 8 o Y S K u D t W H J v 7 A e 8 5 Y / O d L x 1 w 2 U i o v B S T W L e H X v D U A w E 8 x R R F 3 e 9 o 1 6 x 5 J Q d s + x 5 4 G a g h G z V o + I L r t B H B I Y U Y 3 C E U I Q D e E j o 6 c C F g 5 i 4 L q b E S U L C x D n u U S B t S l m c M j x i R / Q d 0 q 6 T s S H t t W d i 1 I x O C e i V p L R x Q J q I 8 i R h f Z p t 4 q l x 1 u x v 3 l P j q e 8 2 o b + f e Y 2 J V b g m 9 i / d L P O / O l 2 L w g C n p g Z B N c W G 0 d W x z C U 1 X d E 3 t 7 9 U p c g h J k 7 j P s U l Y W a U s z 7 b R p O Y 2 n V v P R N / M 5 m a 1 X u W 5 a Z 4 1 7 e k A b s / x z k P m p W y 6 5 T d 8 + N S t Z K N O o 8 9 7 O O Q 5 n m C K m q o o 0 H e Q z z i C c 9 W z Q q t 1 L r 9 T L V y m W Y X 3 5 b 1 8 A E N + 5 A T &lt; / l a t e x i t &gt; y 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D F 3 Q y n H e 2 D 2 p z i C p V s g 5 A B U J 3 q Y = " &gt; A A A C x 3 i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I i 6 L L g R n c V 7 A O 0 l C S d t k P z Y j I p l u L C H 3 C r f y b + g f 6 F d 8 Y p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h r y T D r O a 8 F a W l 5 Z X S u u l z Y 2 t 7 Z 3 y r t 7 r S z J R c C a Q R I m o u N 7 G Q t 5 z J q S y 5 B 1 U s G 8 y A 9 Z 2 x + f q 3 h 7 w k T G k / h a T l P W j b x h z A c 8 8 K S i p j 2 3 1 C t X n K q j l 7 0 I X A M q M K u R l F 9 w i z 4 S B M g R g S G G J B z C Q 0 b P D V w 4 S I n r Y k a c I M R 1 n O E e J d L m l M U o w y N 2 T N 8 h 7 W 4 M G 9 N e e W Z a H d A p I b 2 C l D a O S J N Q n i C s T r N 1 P N f O i v 3 N e 6 Y</formula><p>9 1 d 2 m 9 P e N V 0 S s x I j Y v 3 T z z P / q V C 0 S A 5 z p G j j V l G p G V R c Y l 1 x 3 R d 3 c / l K V J I e U O I X 7 F B e E A 6 2 c 9 9 n W m k z X r n r r 6 f i b z l S s 2 g c m N 8 e 7 u i U N 2 P 0 5 z k X Q q l V d p + p e n V T q N T P q I g 5 w i G O a 5 y n q u E A D T f I e 4 R F P e L Y u r c S a W H e f q V b B a P b x b V k P H 0 o q k C Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D F 3 Q y n H e 2 D 2 p z i C p V s g 5 A B U J 3 q Y = " &gt; A</p><formula xml:id="formula_61">A A C x 3 i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I i 6 L L g R n c V 7 A O 0 l C S d t k P z Y j I p l u L C H 3 C r f y b + g f 6 F d 8 Y p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h r y T D r O a 8 F a W l 5 Z X S u u l z Y 2 t 7 Z 3 y r t 7 r S z J R c C a Q R I m o u N 7 G Q t 5 z J q S y 5 B 1 U s G 8 y A 9 Z 2 x + f q 3 h 7 w k T G k / h a T l P W j b x h z A c 8 8 K S i p j 2 3 1 C t X n K q j l 7 0 I X A M q M K u R l F 9 w i z 4 S B M g R g S G G J B z C Q 0 b P D V w 4 S I n r Y k a c I M R 1 n O E e J d L m l M U o w y N 2 T N 8 h 7 W 4 M G 9 N e e W Z a H d A p I b 2 C l D a O S J N Q n i C s T r N 1 P N f O i v 3 N e 6 Y</formula><p>9 1 d 2 m 9 P e N V 0 S s x I j Y v 3 T z z P / q V C 0 S A 5 z p G j j V l G p G V R c Y l 1 x 3 R d 3 c / l K V J I e U O I X 7 F B e E A 6 2 c 9 9 n W m k z X r n r r 6 f i b z l S s 2 g c m N 8 e 7 u i U N 2 P 0 5 z k X Q q l V d p + p e n V T q N T P q I g 5 w i G O a 5 y n q u E A D T f I e 4 R F P e L Y u r c S a W H e f q V b B a P b x b V k P H 0 o q k C Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D F</p><formula xml:id="formula_62">3 Q y n H e 2 D 2 p z i C p V s g 5 A B U J 3 q Y = " &gt; A A A C x 3 i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I i 6 L L g R n c V 7 A O 0 l C S d t k P z Y j I p l u L C H 3 C r f y b + g f 6 F d 8 Y p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h r y T D r O a 8 F a W l 5 Z X S u u l z Y 2 t 7 Z 3 y r t 7 r S z J R c C a Q R I m o u N 7 G Q t 5 z J q S y 5 B 1 U s G 8 y A 9 Z 2 x + f q 3 h 7 w k T G k / h a T l P W j b x h z A c 8 8 K S i p j 2 3 1 C t X n K q j l 7 0 I X A M q M K u R l F 9 w i z 4 S B M g R g S G G J B z C Q 0 b P D V w 4 S I n r Y k a c I M R 1 n O E e J d L m l M U o w y N 2 T N 8 h 7 W 4 M G 9 N e e W Z a H d A p I b 2 C l D a O S J N Q n i C s T r N 1 P N f O i v 3 N e 6 Y 9 1 d 2 m 9 P e N V 0 S s x I j Y v 3 T z z P / q V C 0 S A 5 z p G j j V l G p G V R c Y l 1 x 3 R d 3 c / l K V J I e U O I X 7 F B e E A</formula><p>6 2 c 9 9 n W m k z X r n r r 6 f i b z l S s 2 g c m N 8 e 7 u i U N 2 P 0 5 z k X Q q l V d p + p e n V T q N T P q I g 5 w i G O a 5 y n q u E A D T f I e 4 R F P e L Y u r c S a W H e f q V b B a P b x b V k P H 0 o q k C Y = &lt; / l a t e x i t &gt; y 2 <ref type="bibr" target="#b16">6</ref> 2 C 0 e z j 2 7 I e P g B M i 5 A n &lt; / l a t e x i t &gt; .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T o R A e S A j J X f q b p 5 c p o 7 s M K O B c 9 Y = " &gt;</head><formula xml:id="formula_63">A A A C x 3 i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I i 6 L L g R n c V 7 A O 0 l G Q 6 b U P z Y j I p l u L C H 3 C r f y b + g f 6 F d 8 Y p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h o G m X S c 1 4 K 1 t L y y u l Z c L 2 1 s b m 3 v l H f 3 W l m S C 8 a b L A k T 0 f G 9 j I d B z J s y k C H v p I J 7 k R / y t j 8 + V / H 2 h I s s S O J r O U 1 5 N / K G c T A I m C c V N e 3 V S r 1 y x a k 6 e t m L w D W g A r M a S f k F t + g j A U O O C B w x J O E Q H j J 6 b u D C Q U p c F z P i B K F A x z n u U S J t T l m c M j x i x / Q d 0 u 7 G s D H t l W e m 1 Y x O C e k V p L R x R J q E 8 g R h d Z q t 4 7 l 2 V u x v 3 j P t q e 4 2 p b 9 v v C J i J U b E / q W b Z / 5 X p 2 q R G O B M 1 x B Q T a l m V H X M u O S 6 K + r m 9 p e q J D m k x C n c p 7 g g z L R y 3 m d b a z J d u + q t p + N v O l O x a s 9 M b o 5 3 d U s a s P t z n I u g V a u 6 T t W 9 O q n U a 2 b U R R z g E M c 0 z 1 P U c Y E G m u Q 9 w i O e 8 G x d W o k 1 s e 4 + U 6 2 C 0 e z j 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">I e P g B M i 5 A n &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T o R A e S A j J X f q b p 5 c p o s M K O B c 9 Y = " &gt;</head><formula xml:id="formula_64">A A A C x 3 i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I i 6 L L g R n c V 7 A O 0 l G Q 6 b U P z Y j I p l u L C H 3 C r f y b + g f 6 F d 8 Y p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h o G m X S c 1 4 K 1 t L y y u l Z c L 2 1 s b m 3 v l H f 3 W l m S C 8 a b L A k T 0 f G 9 j I d B z J s y k C H v p I J 7 k R / y t j 8 + V / H 2 h I s s S O J r O U 1 5 N / K G c T A I m C c V N e 3 V S r 1 y x a k 6 e t m L w D W g A r M a S f k F t + g j A U O O C B w x J O E Q H j J 6 b u D C Q U p c F z P i B K F A x z n u U S J t T l m c M j x i x / Q d 0 u 7 G s D H t l W e m 1 Y x O C e k V p L R x R J q E 8 g R h d Z q t 4 7 l 2 V u x v 3 j P t q e 4 2 p b 9 v v C J i J U b E / q W b Z / 5 X p 2 q R G O B M 1 x B Q T a l m V H X M u O S 6 K + r m 9 p e q J D m k x C n c p 7 g g z L R y 3 m d b a z J d u + q t p + N v O l O x a s 9 M b o 5 3 d U s a s P t z n I u g V a u 6 T t W 9 O q n U a 2 b U R R z g E M c 0 z 1 P U c Y E G m u Q 9 w i O e 8 G x d W o k 1 s e 4 + U 6 2 C 0 e z j 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">I e P g B M i 5 A n &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T o R A e S A j J X f q b p 5 c p o s M K O B c 9 Y = " &gt;</head><formula xml:id="formula_65">A A A C x 3 i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I i 6 L L g R n c V 7 A O 0 l G Q 6 b U P z Y j I p l u L C H 3 C r f y b + g f 6 F d 8 Y p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h o G m X S c 1 4 K 1 t L y y u l Z c L 2 1 s b m 3 v l H f 3 W l m S C 8 a b L A k T 0 f G 9 j I d B z J s y k C H v p I J 7 k R / y t j 8 + V / H 2 h I s s S O J r O U 1 5 N / K G c T A I m C c V N e 3 V S r 1 y x a k 6 e t m L w D W g A r M a S f k F t + g j A U O O C B w x J O E Q H j J 6 b u D C Q U p c F z P i B K F A x z n u U S J t T l m c M j x i x / Q d 0 u 7 G s D H t l W e m 1 Y x O C e k V p L R x R J q E 8 g R h d Z q t 4 7 l 2 V u x v 3 j P t q e 4 2 p b 9 v v C J i J U b E / q W b Z / 5 X p 2 q R G O B M 1 x B Q T a l m V H X M u O S 6 K + r m 9 p e q J D m k x C n c p 7 g g z L R y 3 m d b a z J d u + q t p + N v O l O x a s 9 M b o 5 3 d U s a s P t z n I u g V a u 6 T t W 9 O q n U a 2 b U R R z g E M c 0 z 1 P U c Y E G m u Q 9 w i O e 8 G x d W o k 1 s e 4 + U</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e h Y g i</head><formula xml:id="formula_66">A t + B F X i s r 1 K Y T 8 P w b J O Z V o = " &gt; A A A C x H i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w F Z I i 6 L I g i M s W 7 A N q k W Q 6 r U P z I j M R S t E f c K v f J v 6 B / o V 3 x h T U I j o h y Z l z 7 z k z 9 9 4 g D Y V U r v t a s p a W V 1 b X y u u V j c 2 t 7 Z 3 q 7 l 5 H J n n G e J s l Y Z L 1 A l / y U M S 8 r Y Q K e S / N u B 8 F I e 8 G k 3 M d 7 9 7 x T I o k v l L T l A 8 i f x y L k W C + I q r l 3 F R r r u O a Z S 8 C r w A 1 F K u Z V F 9 w j S E S M O S I w B F D E Q 7 h Q 9 L T h w c X K X E D z I j L C A k T 5 7 h H h b Q 5 Z X H K 8 I m d 0 H d M u 3 7 B x r T X n t K o G Z 0 S 0 p u R 0 s Y R a R L K y w j r 0 2 w T z 4 2 z Z n / z n h l P f b c p / Y P C K y J W 4 Z b Y v 3 T z z P / q d C 0 K I 5 y Z G g T V l B p G V 8 c K l 9 x 0 R d / c / l K V I o e U O I 2 H F M 8 I M 6 O c 9 9 k 2 G m l q 1 7 3 1 T f z N Z G p W 7 1 m R m + N d 3 5 I G 7 P 0 c 5 y L o 1 B 3 P d b z W S a 1 R L 0 Z d x g E O c U z z P E U D l 2 i i b b w f 8 Y R n 6 8 I K L W n l n 6 l W q d D s 4</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">u y H j 4 A t n 2 P I w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e h Y g i</head><formula xml:id="formula_67">A t + B F X i s r 1 K Y T 8 P w b J O Z V o = " &gt; A A A C x H i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w F Z I i 6 L I g i M s W 7 A N q k W Q 6 r U P z I j M R S t E f c K v f J v 6 B / o V 3 x h T U I j o h y Z l z 7 z k z 9 9 4 g D Y V U r v t a s p a W V 1 b X y u u V j c 2 t 7 Z 3 q 7 l 5 H J n n G e J s l Y Z L 1 A l / y U M S 8 r Y Q K e S / N u B 8 F I e 8 G k 3 M d 7 9 7 x T I o k v l L T l A 8 i f x y L k W C + I q r l 3 F R r r u O a Z S 8 C r w A 1 F K u Z V F 9 w j S E S M O S I w B F D E Q 7 h Q 9 L T h w c X K X E D z I j L C A k T 5 7 h H h b Q 5 Z X H K 8 I m d 0 H d M u 3 7 B x r T X n t K o G Z 0 S 0 p u R 0 s Y R a R L K y w j r 0 2 w T z 4 2 z Z n / z n h l P f b c p / Y P C K y J W 4 Z b Y v 3 T z z P / q d C 0 K I 5 y Z G g T V l B p G V 8 c K l 9 x 0 R d / c / l K V I o e U O I 2 H F M 8 I M 6 O c 9 9 k 2 G m l q 1 7 3 1 T f z N Z G p W 7 1 m R m + N d 3 5 I G 7 P 0 c 5 y L o 1 B 3 P d b z W S a 1 R L 0 Z d x g E O c U z z P E U D l 2 i i b b w f 8 Y R n 6 8 I K L W n l n 6 l W q d D s 4</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">u y H j 4 A t n 2 P I w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e h Y g i</head><formula xml:id="formula_68">A t + B F X i s r 1 K Y T 8 P w b J O Z V o = " &gt; A A A C x H i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w F Z I i 6 L I g i M s W 7 A N q k W Q 6 r U P z I j M R S t E f c K v f J v 6 B / o V 3 x h T U I j o h y Z l z 7 z k z 9 9 4 g D Y V U r v t a s p a W V 1 b X y u u V j c 2 t 7 Z 3 q 7 l 5 H J n n G e J s l Y Z L 1 A l / y U M S 8 r Y Q K e S / N u B 8 F I e 8 G k 3 M d 7 9 7 x T I o k v l L T l A 8 i f x y L k W C + I q r l 3 F R r r u O a Z S 8 C r w A 1 F K u Z V F 9 w j S E S M O S I w B F D E Q 7 h Q 9 L T h w c X K X E D z I j L C A k T 5 7 h H h b Q 5 Z X H K 8 I m d 0 H d M u 3 7 B x r T X n t K o G Z 0 S 0 p u R 0 s Y R a R L K y w j r 0 2 w T z 4 2 z Z n / z n h l P f b c p / Y P C K y J W 4 Z b Y v 3 T z z P / q d C 0 K I 5 y Z G g T V l B p G V 8 c K l 9 x 0 R d / c / l K V I o e U O I 2 H F M 8 I M 6 O c 9 9 k 2 G m l q 1 7 3 1 T f z N Z G p W</formula><formula xml:id="formula_69">I E E G y U V A W x T a u u X 3 x + X f M m 7 U r O c = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J N N p D c 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h k E m H e e 1 Y C 0 s L i 2 v F F f X 1 j c 2 t 7 Z L O 7 u t L M k F 4 0 2 W h I n o + F 7 G w y D m T R n I k H d S w b 3 I D 3 n b H 5 + p e P u G i y x I 4 k s 5 S X k v 8 k Z x M A y Y J 4 m 6 u O u 7 / V L Z q T h 6 2 f P A N a A M s x p J 6 Q V X G C A B Q 4 4 I H D E k 4 R A e M n q 6 c O E g J a 6 H K X G C U K D j H P d Y I 2 1 O W Z w y P G L H 9 B 3 R r m v Y m P b K M 9 N q R q e E 9 A p S 2 j g k T U J 5 g r A 6 z d b x X D s r 9 j f v q f Z U d 5 v Q 3 z d e E b E S 1 8 T + p Z t l / l e n a p E Y 4 l T X E F B N q W Z U d c y 4 5 L o r 6 u b 2 l 6 o k O a T E K T y g u C D M t H L W Z 1 t r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q z 0 x u j n d 1 S x q w + 3 O c 8 6 B V r b h O x T 0 / L t e q Z t R F 7 O M A R z T P E 9 R Q R w N N 8 h 7 h E U 9 4 t u p W b O X W 7 W e q V T C a P X x b 1 s M H C T u Q E Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I E E G y U V A W x T a u u X 3 x + X f M m 7 U r O c = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J N N p D c 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h k E m H e e 1 Y C 0 s L i 2 v F F f X 1 j c 2 t 7 Z L O 7 u t L M k F 4 0 2 W h I n o + F 7 G w y D m T R n I k H d S w b 3 I D 3 n b H 5 + p e P u G i y x I 4 k s 5 S X k v 8 k Z x M A y Y J 4 m 6 u O u 7 / V L Z q T h 6 2 f P A N a A M s x p J 6 Q V X G C A B Q 4 4 I H D E k 4 R A e M n q 6 c O E g J a 6 H K X G C U K D j H P d Y I 2 1 O W Z w y P G L H 9 B 3 R r m v Y m P b K M 9 N q R q e E 9 A p S 2 j g k T U J 5 g r A 6 z d b x X D s r 9 j f v q f Z U d 5 v Q 3 z d e E b E S 1 8 T + p Z t l / l e n a p E Y 4 l T X E F B N q W Z U d c y 4 5 L o r 6 u b 2 l 6 o k O a T E K T y g u C D M t H L W Z 1 t r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q z 0 x u j n d 1 S x q w + 3 O c 8 6 B V r b h O x T 0 / L t e q Z t R F 7 O M A R z T P E 9 R Q R w N N 8 h 7 h E U 9 4 t u p W b O X W 7 W e q V T C a P X x b 1 s M H C T u Q E Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I E E G y U V A W x T a u u X 3 x + X f M m 7 U r O c = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J N N p D c 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h k E m H e e 1 Y C 0 s L i 2 v F F f X 1 j c 2 t 7 Z L O 7 u t L M k F 4 0 2 W h I n o + F 7 G w y D m T R n I k H d S w b 3 I D 3 n b H 5 + p e P u G i y x I 4 k s 5 S X k v 8 k Z x M A y Y J 4 m 6 u O u 7 / V L Z q T h 6 2 f P A N a A M s x p J 6 Q V X G C A B Q 4 4 I H D E k 4 R A e M n q 6 c O E g J a 6 H K X G C U K D j H P d Y I 2 1 O W Z w y P G L H 9 B 3 R r m v Y m P b K M 9 N q R q e E 9 A p S 2 j g k T U J 5 g r A 6 z d b x X D s r 9 j f v q f Z U d 5 v Q 3 z d e E b E S 1 8 T + p Z t l / l e n a p E Y 4 l T X E F B N q W Z U d c y 4 5 L o r 6 u b 2 l 6 o k O a T E K T y g u C D M t H L W Z 1 t r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q z 0 x u j n d 1 S x q w + 3 O c 8 6 B V r b h O x T 0 / L t e q Z t R F 7 O M A R z T P E 9 R Q R w N N 8 h 7 h E U 9 4 t u p W b O X W 7 W e q V T C a P X x b 1 s M H C T u Q E Q = = &lt; / l a t e x i t &gt; x 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w + O 2 A V A N 5 t G Y s 3 Q A W 4 f 1 x G 8 R q I 4 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J O m 0 D s 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h j y T j v N a s B Y W l 5 Z X i q t r 6 x u b W 9 u l n d 1 W l u Q i Y M 0 g C R P R 8 b 2 M h T x m T c l l y D q p Y F 7 k h 6 z t j 8 9 U v H 3 D R M a T + F J O U t a L v F H M h z z w J F E X d / 1 q v 1 R 2 K o 5 e 9 j x w D S j D r E Z S e s E V B k g Q I E c E h h i S c A g P G T 1 d u H C Q E t f D l D h B i O s 4 w z 3 W S J t T F q M M j 9 g x f U e 0 6 x o 2 p r 3 y z L Q 6 o F N C e g U p b R y S J q E 8 Q V i d Z u t 4 r p 0 V + 5 v 3 V H u q u 0 3 o 7 x u v i F i J a 2 L / 0 s 0 y / 6 t T t U g M c a p r 4 F R T q h l V X W B c c t 0 V d X P 7 S 1 W S H F L i F B 5 Q X B A O t H L W Z 1 t</formula><p>r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q H 5 j c H O / q l j R g 9 + c 4 5 0 G r W n G d i n t + X K 5 V z a i L 2 M c B j m i e J 6 i h j g a a 5 D 3 C I 5 7 w b N W t 2 M q t 2 8 9 U q 2 A 0 e / i 2 r I c P C 5 u Q E g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w</p><formula xml:id="formula_70">+ O 2 A V A N 5 t G Y s 3 Q A W 4 f 1 x G 8 R q I 4 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J O m 0 D s 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h j y T j v N a s B Y W l 5 Z X i q t r 6 x u b W 9 u l n d 1 W l u Q i Y M 0 g C R P R 8 b 2 M h T x m T c l l y D q p Y F 7 k h 6 z t j 8 9 U v H 3 D R M a T + F J O U t a L v F H M h z z w J F E X d / 1 q v 1 R 2 K o 5 e 9 j x w D S j D r E Z S e s E V B k g Q I E c E h h i S c A g P G T 1 d u H C Q E t f D l D h B i O s 4 w z 3 W S J t T F q M M j 9 g x f U e 0 6 x o 2 p r 3 y z L Q 6 o F N C e g U p b R y S J q E 8 Q V i d Z u t 4 r p 0 V + 5 v 3 V H u q u 0 3 o 7 x u v i F i J a 2 L / 0 s 0 y / 6 t T t U g M c a p r 4 F R T q h l V X W B c c t 0 V d X P 7 S 1 W S H F L i F B 5 Q X B A O t H L W Z 1 t</formula><p>r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q H 5 j c H O / q l j R g 9 + c 4 5 0 G r W n G d i n t + X K 5 V z a i L 2 M c B j m i e J 6 i h j g a a 5 D 3 C I 5 7 w b N W t 2 M q t 2 8 9 U q 2 A 0 e / i 2 r I c P C 5 u Q E g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w</p><formula xml:id="formula_71">+ O 2 A V A N 5 t G Y s 3 Q A W 4 f 1 x G 8 R q I 4 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r / q u u n Q T L I K r k h R B l w U 3 X V a 0 D 6 i l J O m 0 D s 2 L y U Q t R f A H 3 O q n i X + g f + G d c Q p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + f h j y T j v N a s B Y W l 5 Z X i q t r 6 x u b W 9 u l n d 1 W l u Q i Y M 0 g C R P R 8 b 2 M h T x m T c l l y D q p Y F 7 k h 6 z t j 8 9 U v H 3 D R M a T + F J O U t a L v F H M h z z w J F E X d / 1 q v 1 R 2 K o 5 e 9 j x w D S j D r E Z S e s E V B k g Q I E c E h h i S c A g P G T 1 d u H C Q E t f D l D h B i O s 4 w z 3 W S J t T F q M M j 9 g x f U e 0 6 x o 2 p r 3 y z L Q 6 o F N C e g U p b R y S J q E 8 Q V i d Z u t 4 r p 0 V + 5 v 3 V H u q u 0 3 o 7 x u v i F i J a 2 L / 0 s 0 y / 6 t T t U g M c a p r 4 F R T q h l V X W B c c t 0 V d X P 7 S 1 W S H F L i F B 5 Q X B A O t H L W Z 1 t</formula><p>r M l 2 7 6 q 2 n 4 2 8 6 U 7 F q H 5 j c H O / q l j R g 9 + c 4 5 0 G r W n G d i n t + X K 5 V z a i L 2 M c B j m i e J 6 i h j g a a 5 D 3 C I 5 7 w b N W t 2 M q t 2 8 9 U q 2 A 0 e / i 2 r I c P C 5 u Q E g = = &lt; / l a t e x i t &gt; x 3 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C L X q B p c I l H 0 j f t D 1 W T P R S D D H 1 d 0 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I q 6 L L g p s u K 9 g G 1 l G Q 6 r U P T J E w m a i m C P + B W P 0 3 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h y</p><formula xml:id="formula_72">I R D n O a 8 5 a W F x a X s m v F t b W N z a 3 i t s 7 z S R K J e M N F g W R b P t e w g M R 8 o Y S K u D t W H J v 7 A e 8 5 Y / O d L x 1 w 2 U i o v B S T W L e H X v D U A w E 8 x R R F 3 e 9 o 1 6 x 5 J Q d s + x 5 4 G a g h G z V o + I L r t B H B I Y U Y 3 C E U I Q D e E j o 6 c C F g 5 i 4 L q b E S U L C x D n u U S B t S l m c M j x i R / Q d 0 q 6 T s S H t t W d i 1 I x O C e i V p L R x Q J q I 8 i R h f Z p t 4 q l x 1 u x v 3 l P j q e 8 2 o b + f e Y 2 J V b g m 9 i / d L P O / O l 2 L w g C n p g Z B N c W G 0 d W x z C U 1 X d E 3 t 7 9 U p c g h J k 7 j P s U l Y W a U s z 7 b R p O Y 2 n V v P R N / M 5 m a 1 X u W 5 a Z 4 1 7 e k A b s / x z k P m p W y 6 5 T d 8 + N S t Z K N O o 8 9 7 O O Q 5 n m C K m q o o 0 H e Q z z i C c 9 W z Q q t 1 L r 9 T L V y m W Y X 3 5 b 1 8 A E N + 5 A T &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C L X q B p c I l H 0 j f t D 1 W T P R S D D H 1 d 0 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I q 6 L L g p s u K 9 g G 1 l G Q 6 r U P T J E w m a i m C P + B W P 0 3 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h y I R D n O a 8 5 a W F x a X s m v F t b W N z a 3 i t s 7 z S R K J e M N F g W R b P t e w g M R 8 o Y S K u D t W H J v 7 A e 8 5 Y / O d L x 1 w 2 U i o v B S T W L e H X v D U A w E 8 x R R F 3 e 9 o 1 6 x 5 J Q d s + x 5 4 G a g h G z V o + I L r t B H B I Y U Y 3 C E U I Q D e E j o 6 c C F g 5 i 4 L q b E S U L C x D n u U S B t S l m c M j x i R / Q d 0 q 6 T s S H t t W d i 1 I x O C e i V p L R x Q J q I 8 i R h f Z p t 4 q l x 1 u x v 3 l P j q e 8 2 o b + f e Y 2 J V b g m 9 i / d L P O / O l 2 L w g C n p g Z B N c W G 0 d W x z C U 1 X d E 3 t 7 9 U p c g h J k 7 j P s U l Y W a U s z 7 b R p O Y 2 n V v P R N / M 5 m a 1 X u W 5 a Z 4 1 7 e k A b s / x z k P m p W y 6 5 T d 8 + N S t Z K N O o 8 9 7 O O Q 5 n m C K m q o o 0 H e Q z z i C c 9 W z Q q t 1 L r 9 T L V y m W Y X 3 5 b 1 8 A E N + 5 A T &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C L X q B p c I l H 0 j f t D 1 W T P R S D D H 1 d 0 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I q 6 L L g p s u K 9 g G 1 l G Q 6 r U P T J E w m a i m C P + B W P 0 3 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h y I R D n O a 8 5 a W F x a X s m v F t b W N z a 3 i t s 7 z S R K J e M N F g W R b P t e w g M R 8 o Y S K u D t W H J v 7 A e 8 5 Y / O d L x 1 w 2 U i o v B S T W L e H X v D U A w E 8 x R R F 3 e 9 o 1 6 x 5 J Q d s + x 5 4 G a g h G z V o + I L r t B H B I Y U Y 3 C E U I Q D e E j o 6 c C F g 5 i 4 L q b E S U L C x D n u U S B t S l m c M j x i R / Q d 0 q 6 T s S H t t W d i 1 I x O C e i V p L R x Q J q I 8 i R h f Z p t 4 q l x 1 u x v 3 l P j q e 8 2 o b + f e Y 2 J V b g m 9 i / d L P O / O l 2 L w g C n p g Z B N c W G 0 d W x z C U 1 X d E 3 t 7 9 U p c g h J k 7 j P s U l Y W a U s z 7 b R p O Y 2 n V v P R N / M 5 m a 1 X u W 5 a Z 4 1 7 e k A b s / x z k P m p W y 6 5 T d 8 + N S t Z K N O o 8 9 7 O O Q 5 n m C K m q o o 0 H e Q z z i C c 9 W z Q q t 1 L r 9 T L V y m W Y X 3 5 b 1 8 A E N + 5 A T &lt; / l a t e x i t &gt; y 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D F 3 Q y n H e 2 D 2 p z i C p V s g 5 A B U J 3 q Y = " &gt; A A A C x 3 i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I i 6 L L g R n c V 7 A O 0 l C S d t k P z Y j I p l u L C H 3 C r f y b + g f 6 F d 8 Y p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h r y T D r O a 8 F a W l 5 Z X S u u l z Y 2 t 7 Z 3 y r t 7 r S z J R c C a Q R I m o u N 7 G Q t 5 z J q S y 5 B 1 U s G 8 y A 9 Z 2 x + f q 3 h 7 w k T G k / h a T l P W j b x h z A c 8 8 K S i p j 2 3 1 C t X n K q j l 7 0 I X A M q M K u R l F 9 w i z 4 S B M g R g S G G J B z C Q 0 b P D V w 4 S I n r Y k a c I M R 1 n O E e J d L m l M U o w y N 2 T N 8 h 7 W 4 M G 9 N e e W Z a H d A p I b 2 C l D a O S J N Q n i C s T r N 1 P N f O i v 3 N e 6 Y 9 1 d 2 m 9 P e N V 0 S s x I j Y v 3 T z z P / q V C 0 S A 5 z p G j j V l G p G V R c Y l 1 x 3 R d 3 c / l K V J I e U O I X 7 F B e E A</formula><p>6 2 c 9 9 n W m k z X r n r r 6 f i b z l S s 2 g c m N 8 e 7 u i U N 2 P 0 5 z k X Q q l V d p + p e n V T q N T P q I g 5 w i G O a 5 y n q u E A D T f I e 4 R F P e L Y u r c S a W H e f q V b B a P b x b V k P H 0 o q k C Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D F</p><formula xml:id="formula_73">3 Q y n H e 2 D 2 p z i C p V s g 5 A B U J 3 q Y = " &gt; A A A C x 3 i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I i 6 L L g R n c V 7 A O 0 l C S d t k P z Y j I p l u L C H 3 C r f y b + g f 6 F d 8 Y p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h r y T D r O a 8 F a W l 5 Z X S u u l z Y 2 t 7 Z 3 y r t 7 r S z J R c C a Q R I m o u N 7 G Q t 5 z J q S y 5 B 1 U s G 8 y A 9 Z 2 x + f q 3 h 7 w k T G k / h a T l P W j b x h z A c 8 8 K S i p j 2 3 1 C t X n K q j l 7 0 I X A M q M K u R l F 9 w i z 4 S B M g R g S G G J B z C Q 0 b P D V w 4 S I n r Y k a c I M R 1 n O E e J d L m l M U o w y N 2 T N 8 h 7 W 4 M G 9 N e e W Z a H d A p I b 2 C l D a O S J N Q n i C s T r N 1 P N f O i v 3 N e 6 Y 9 1 d 2 m 9 P e N V 0 S s x I j Y v 3 T z z P / q V C 0 S A 5 z p G j j V l G p G V R c Y l 1 x 3 R d 3 c / l K V J I e U O I X 7 F B e E A</formula><p>6 2 c 9 9 n W m k z X r n r r 6 f i b z l S s 2 g c m N 8 e 7 u i U N 2 P 0 5 z k X Q q l V d p + p e n V T q N T P q I g 5 w i G O a 5 y n q u E A D T f I e 4 R F P e L Y u r c S a W H e f q V b B a P b x b V k P H 0 o q k C Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D F</p><formula xml:id="formula_74">3 Q y n H e 2 D 2 p z i C p V s g 5 A B U J 3 q Y = " &gt; A A A C x 3 i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I i 6 L L g R n c V 7 A O 0 l C S d t k P z Y j I p l u L C H 3 C r f y b + g f 6 F d 8 Y p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h r y T D r O a 8 F a W l 5 Z X S u u l z Y 2 t 7 Z 3 y r t 7 r S z J R c C a Q R I m o u N 7 G Q t 5 z J q S y 5 B 1 U s G 8 y A 9 Z 2 x + f q 3 h 7 w k T G k / h a T l P W j b x h z A c 8 8 K S i p j 2 3 1 C t X n K q j l 7 0 I X A M q M K u R l F 9 w i z 4 S B M g R g S G G J B z C Q 0 b P D V w 4 S I n r Y k a c I M R 1 n O E e J d L m l M U o w y N 2 T N 8 h 7 W 4 M G 9 N e e W Z a H d A p I b 2 C l D a O S J N Q n i C s T r N 1 P N f O i v 3 N e 6 Y 9 1 d 2 m 9 P e N V 0 S s x I j Y v 3 T z z P / q V C 0 S A 5 z p G j j V l G p G V R c Y l 1 x 3 R d 3 c / l K V J I e U O I X 7 F B e E A</formula><p>6 2 c 9 9 n W m k z X r n r r 6 f i b z l S s 2 g c m N 8 e 7 u i U N 2 P 0 5 z k X Q q l V d p + p e n V T q N T P q I g 5 w i G O a 5 y n q u E A D T f I e 4 R F P e L Y u r c S a W H e f q V b B a P b x b V k P H 0 o q k C Y = &lt; / l a t e x i t &gt; y 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T o R A e S A j J X f q b p 5 c p o <ref type="bibr" target="#b16">6</ref> 2 C 0 e z j 2 7 I e P g B M i 5 A n &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T o R A e S A j J X f q b p 5 c p o <ref type="bibr" target="#b16">6</ref> 2 C 0 e z j 2 7 I e P g B M i 5 A n &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T o R A e S A j J X f q b p 5 c p o <ref type="bibr" target="#b16">6</ref> 2 C 0 e z j 2 7 I e P g B M i 5 A n &lt; / l a t e x i t &gt; y 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T o R A e S A j J X f q b p 5 c p o <ref type="bibr" target="#b16">6</ref> 2 C 0 e z j 2 7 I e P g B M i 5 A n &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T o R A e S A j J X f q b p 5 c p o <ref type="bibr" target="#b16">6</ref> 2 C 0 e z j 2 7 I e P g B M i 5 A n &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T o R A e S A j J X f q b p 5 c p o <ref type="bibr" target="#b16">6</ref> 2 C 0 e z j 2 7 I e P g B M i 5 A n &lt; / l a t e x i t &gt; .</p><formula xml:id="formula_75">7 s M K O B c 9 Y = " &gt; A A A C x 3 i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I i 6 L L g R n c V 7 A O 0 l G Q 6 b U P z Y j I p l u L C H 3 C r f y b + g f 6 F d 8 Y p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h o G m X S c 1 4 K 1 t L y y u l Z c L 2 1 s b m 3 v l H f 3 W l m S C 8 a b L A k T 0 f G 9 j I d B z J s y k C H v p I J 7 k R / y t j 8 + V / H 2 h I s s S O J r O U 1 5 N / K G c T A I m C c V N e 3 V S r 1 y x a k 6 e t m L w D W g A r M a S f k F t + g j A U O O C B w x J O E Q H j J 6 b u D C Q U p c F z P i B K F A x z n u U S J t T l m c M j x i x / Q d 0 u 7 G s D H t l W e m 1 Y x O C e k V p L R x R J q E 8 g R h d Z q t 4 7 l 2 V u x v 3 j P t q e 4 2 p b 9 v v C J i J U b E / q W b Z / 5 X p 2 q R G O B M 1 x B Q T a l m V H X M u O S 6 K + r m 9 p e q J D m k x C n c p 7 g g z L R y 3 m d b a z J d u + q t p + N v O l O x a s 9 M b o 5 3 d U s a s P t z n I u g V a u 6 T t W 9 O q n U a 2 b U R R z g E M c 0 z 1 P U c Y E G m u Q 9 w i O e 8 G x d W o k 1 s e 4 + U</formula><formula xml:id="formula_76">7 s M K O B c 9 Y = " &gt; A A A C x 3 i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I i 6 L L g R n c V 7 A O 0 l G Q 6 b U P z Y j I p l u L C H 3 C r f y b + g f 6 F d 8 Y p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h o G m X S c 1 4 K 1 t L y y u l Z c L 2 1 s b m 3 v l H f 3 W l m S C 8 a b L A k T 0 f G 9 j I d B z J s y k C H v p I J 7 k R / y t j 8 + V / H 2 h I s s S O J r O U 1 5 N / K G c T A I m C c V N e 3 V S r 1 y x a k 6 e t m L w D W g A r M a S f k F t + g j A U O O C B w x J O E Q H j J 6 b u D C Q U p c F z P i B K F A x z n u U S J t T l m c M j x i x / Q d 0 u 7 G s D H t l W e m 1 Y x O C e k V p L R x R J q E 8 g R h d Z q t 4 7 l 2 V u x v 3 j P t q e 4 2 p b 9 v v C J i J U b E / q W b Z / 5 X p 2 q R G O B M 1 x B Q T a l m V H X M u O S 6 K + r m 9 p e q J D m k x C n c p 7 g g z L R y 3 m d b a z J d u + q t p + N v O l O x a s 9 M b o 5 3 d U s a s P t z n I u g V a u 6 T t W 9 O q n U a 2 b U R R z g E M c 0 z 1 P U c Y E G m u Q 9 w i O e 8 G x d W o k 1 s e 4 + U</formula><formula xml:id="formula_77">7 s M K O B c 9 Y = " &gt; A A A C x 3 i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I i 6 L L g R n c V 7 A O 0 l G Q 6 b U P z Y j I p l u L C H 3 C r f y b + g f 6 F d 8 Y p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h o G m X S c 1 4 K 1 t L y y u l Z c L 2 1 s b m 3 v l H f 3 W l m S C 8 a b L A k T 0 f G 9 j I d B z J s y k C H v p I J 7 k R / y t j 8 + V / H 2 h I s s S O J r O U 1 5 N / K G c T A I m C c V N e 3 V S r 1 y x a k 6 e t m L w D W g A r M a S f k F t + g j A U O O C B w x J O E Q H j J 6 b u D C Q U p c F z P i B K F A x z n u U S J t T l m c M j x i x / Q d 0 u 7 G s D H t l W e m 1 Y x O C e k V p L R x R J q E 8 g R h d Z q t 4 7 l 2 V u x v 3 j P t q e 4 2 p b 9 v v C J i J U b E / q W b Z / 5 X p 2 q R G O B M 1 x B Q T a l m V H X M u O S 6 K + r m 9 p e q J D m k x C n c p 7 g g z L R y 3 m d b a z J d u + q t p + N v O l O x a s 9 M b o 5 3 d U s a s P t z n I u g V a u 6 T t W 9 O q n U a 2 b U R R z g E M c 0 z 1 P U c Y E G m u Q 9 w i O e 8 G x d W o k 1 s e 4 + U</formula><formula xml:id="formula_78">7 s M K O B c 9 Y = " &gt; A A A C x 3 i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I i 6 L L g R n c V 7 A O 0 l G Q 6 b U P z Y j I p l u L C H 3 C r f y b + g f 6 F d 8 Y p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h o G m X S c 1 4 K 1 t L y y u l Z c L 2 1 s b m 3 v l H f 3 W l m S C 8 a b L A k T 0 f G 9 j I d B z J s y k C H v p I J 7 k R / y t j 8 + V / H 2 h I s s S O J r O U 1 5 N / K G c T A I m C c V N e 3 V S r 1 y x a k 6 e t m L w D W g A r M a S f k F t + g j A U O O C B w x J O E Q H j J 6 b u D C Q U p c F z P i B K F A x z n u U S J t T l m c M j x i x / Q d 0 u 7 G s D H t l W e m 1 Y x O C e k V p L R x R J q E 8 g R h d Z q t 4 7 l 2 V u x v 3 j P t q e 4 2 p b 9 v v C J i J U b E / q W b Z / 5 X p 2 q R G O B M 1 x B Q T a l m V H X M u O S 6 K + r m 9 p e q J D m k x C n c p 7 g g z L R y 3 m d b a z J d u + q t p + N v O l O x a s 9 M b o 5 3 d U s a s P t z n I u g V a u 6 T t W 9 O q n U a 2 b U R R z g E M c 0 z 1 P U c Y E G m u Q 9 w i O e 8 G x d W o k 1 s e 4 + U</formula><formula xml:id="formula_79">7 s M K O B c 9 Y = " &gt; A A A C x 3 i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I i 6 L L g R n c V 7 A O 0 l G Q 6 b U P z Y j I p l u L C H 3 C r f y b + g f 6 F d 8 Y p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h o G m X S c 1 4 K 1 t L y y u l Z c L 2 1 s b m 3 v l H f 3 W l m S C 8 a b L A k T 0 f G 9 j I d B z J s y k C H v p I J 7 k R / y t j 8 + V / H 2 h I s s S O J r O U 1 5 N / K G c T A I m C c V N e 3 V S r 1 y x a k 6 e t m L w D W g A r M a S f k F t + g j A U O O C B w x J O E Q H j J 6 b u D C Q U p c F z P i B K F A x z n u U S J t T l m c M j x i x / Q d 0 u 7 G s D H t l W e m 1 Y x O C e k V p L R x R J q E 8 g R h d Z q t 4 7 l 2 V u x v 3 j P t q e 4 2 p b 9 v v C J i J U b E / q W b Z / 5 X p 2 q R G O B M 1 x B Q T a l m V H X M u O S 6 K + r m 9 p e q J D m k x C n c p 7 g g z L R y 3 m d b a z J d u + q t p + N v O l O x a s 9 M b o 5 3 d U s a s P t z n I u g V a u 6 T t W 9 O q n U a 2 b U R R z g E M c 0 z 1 P U c Y E G m u Q 9 w i O e 8 G x d W o k 1 s e 4 + U</formula><formula xml:id="formula_80">7 s M K O B c 9 Y = " &gt; A A A C x 3 i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V Z I i 6 L L g R n c V 7 A O 0 l G Q 6 b U P z Y j I p l u L C H 3 C r f y b + g f 6 F d 8 Y p q E V 0 Q p I z 5 9 5 z Z u 6 9 f h o G m X S c 1 4 K 1 t L y y u l Z c L 2 1 s b m 3 v l H f 3 W l m S C 8 a b L A k T 0 f G 9 j I d B z J s y k C H v p I J 7 k R / y t j 8 + V / H 2 h I s s S O J r O U 1 5 N / K G c T A I m C c V N e 3 V S r 1 y x a k 6 e t m L w D W g A r M a S f k F t + g j A U O O C B w x J O E Q H j J 6 b u D C Q U p c F z P i B K F A x z n u U S J t T l m c M j x i x / Q d 0 u 7 G s D H t l W e m 1 Y x O C e k V p L R x R J q E 8 g R h d Z q t 4 7 l 2 V u x v 3 j P t q e 4 2 p b 9 v v C J i J U b E / q W b Z / 5 X p 2 q R G O B M 1 x B Q T a l m V H X M u O S 6 K + r m 9 p e q J D m k x C n c p 7 g g z L R y 3 m d b a z J d u + q t p + N v O l O x a s 9 M b o 5 3 d U s a s P t z n I u g V a u 6 T t W 9 O q n U a 2 b U R R z g E M c 0 z 1 P U c Y E G m u Q 9 w i O e 8 G x d W o k 1 s e 4 + U</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e h Y g i</head><formula xml:id="formula_81">A t + B F X i s r 1 K Y T 8 P w b J O Z V o = " &gt; A A A C x H i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w F Z I i 6 L I g i M s W 7 A N q k W Q 6 r U P z I j M R S t E f c K v f J v 6 B / o V 3 x h T U I j o h y Z l z 7 z k z 9 9 4 g D Y V U r v t a s p a W V 1 b X y u u V j c 2 t 7 Z 3 q 7 l 5 H J n n G e J s l Y Z L 1 A l / y U M S 8 r Y Q K e S / N u B 8 F I e 8 G k 3 M d 7 9 7 x T I o k v l L T l A 8 i f x y L k W C + I q r l 3 F R r r u O a Z S 8 C r w A 1 F K u Z V F 9 w j S E S M O S I w B F D E Q 7 h Q 9 L T h w c X K X E D z I j L C A k T 5 7 h H h b Q 5 Z X H K 8 I m d 0 H d M u 3 7 B x r T X n t K o G Z 0 S 0 p u R 0 s Y R a R L K y w j r 0 2 w T z 4 2 z Z n / z n h l P f b c p / Y P C K y J W 4 Z b Y v 3 T z z P / q d C 0 K I 5 y Z G g T V l B p G V 8 c K l 9 x 0 R d / c / l K V I o e U O I 2 H F M 8 I M 6 O c 9 9 k 2 G m l q 1 7 3 1 T f z N Z G p W 7 1 m R m + N d 3 5 I G 7 P 0 c 5 y L o 1 B 3 P d b z W S a 1 R L 0 Z d x g E O c U z z P E U D l 2 i i b b w f 8 Y R n 6 8 I K L W n l n 6 l W q d D</formula><p>s 4 9 u y H j 4 A t n 2 P I w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e h Y g i</p><formula xml:id="formula_82">A t + B F X i s r 1 K Y T 8 P w b J O Z V o = " &gt; A A A C x H i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w F Z I i 6 L I g i M s W 7 A N q k W Q 6 r U P z I j M R S t E f c K v f J v 6 B / o V 3 x h T U I j o h y Z l z 7 z k z 9 9 4 g D Y V U r v t a s p a W V 1 b X y u u V j c 2 t 7 Z 3 q 7 l 5 H J n n G e J s l Y Z L 1 A l / y U M S 8 r Y Q K e S / N u B 8 F I e 8 G k 3 M d 7 9 7 x T I o k v l L T l A 8 i f x y L k W C + I q r l 3 F R r r u O a Z S 8 C r w A 1 F K u Z V F 9 w j S E S M O S I w B F D E Q 7 h Q 9 L T h w c X K X E D z I j L C A k T 5 7 h H h b Q 5 Z X H K 8 I m d 0 H d M u 3 7 B x r T X n t K o G Z 0 S 0 p u R 0 s Y R a R L K y w j r 0 2 w T z 4 2 z Z n / z n h l P f b c p / Y P C K y J W 4 Z b Y v 3 T z z P / q d C 0 K I 5 y Z G g T V l B p G V 8 c K l 9 x 0 R d / c / l K V I o e U O I 2 H F M 8 I M 6 O c 9 9 k 2 G m l q 1 7 3 1 T f z N Z G p W 7 1 m R m + N d 3 5 I G 7 P 0 c 5 y L o 1 B 3 P d b z W S a 1 R L 0 Z d x g E O c U z z P E U D l 2 i i b b w f 8 Y R n 6 8 I K L W n l n 6 l W q d D</formula><p>s 4 9 u y H j 4 A t n 2 P I w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e h Y g i  ) and ponderous to train, or not even available publicly. Thus using these models in the pre-train and fine-tune regimen is often not possible.  </p><formula xml:id="formula_83">A t + B F X i s r 1 K Y T 8 P w b J O Z V o = " &gt; A A A C x H i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w F Z I i 6 L I g i M s W 7 A N q k W Q 6 r U P z I j M R S t E f c K v f J v 6 B / o V 3 x h T U I j o h y Z l z 7 z k z 9 9 4 g D Y V U r v t a s p a W V 1 b X y u u V j c 2 t 7 Z 3 q 7 l 5 H J n n G e J s l Y Z L 1 A l / y U M S 8 r Y Q K e S / N u B 8 F I e 8 G k 3 M d 7 9 7 x T I o k v l L T l A 8 i f x y L k W C + I q r l 3 F R r r u O a Z S 8 C r w A 1 F K u Z V F 9 w j S E S M O S I w B F D E Q 7 h Q 9 L T h w c X K X E D z I j L C A k T 5 7 h H h b Q 5 Z X H K 8 I m d 0 H d M u 3 7 B x r T X n t K o G Z 0 S 0 p u R 0 s Y R a R L K y w j r 0 2 w T z 4 2 z Z n / z n h l P f b c p / Y P C K y J W 4 Z b Y v 3 T z z P / q d C 0 K I 5 y Z G g T V l B p G V 8 c K l 9 x 0 R d / c / l K V I o e U O I 2 H F M 8 I M 6 O c 9 9 k 2 G m l q 1 7 3 1 T f z N Z G p W 7 1 m R m + N d 3 5 I G 7 P 0 c 5 y L o 1 B 3 P d b z W S a 1 R L 0 Z d x g E O c U z z P E U D l 2 i i b b w f 8 Y R n 6 8 I K L W n l n 6 l W q d D</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Masked Language Models</head><p>While autoregressive language models provide a powerful tool for modeling the probability of text, they also have disadvantages such as requiring representations be calculated from left-to-right. When the focus is shifted to generating the optimal representations for down-stream tasks such as classification, many other options become possible, and often preferable. One popular bidirectional objective function used widely in representation learning is the masked language model (MLM; Devlin et al. ( <ref type="formula">2019</ref>)), which aims to predict masked text pieces based on surrounded context. For example, P (x i |x 1 , . . . , x i−1 , x i+1 , . . . , x n ) represents the probability of the word x i given the surrounding context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example &amp; Applicable Scenario</head><p>Representative pre-trained models using MLMs include: BERT (Devlin et al., 2019), ERNIE <ref type="bibr" target="#b209">(Zhang et al., 2019;</ref><ref type="bibr" target="#b179">Sun et al., 2019b</ref>) and many variants. In prompting methods, MLMs are generally most suitable for natural language understanding or analysis tasks (e.g., text classification, natural language inference , and extractive question answering). These tasks are often relatively easy to be reformulated into cloze problems, which are consistent with the training objectives of the MLM. Additionally, MLMs have been a pre-trained model of choice when exploring methods that combine prompting with fine-tuning, elaborated further in §7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Prefix and Encoder-Decoder</head><p>For conditional text generation tasks such as translation and summarization where an input text</p><formula xml:id="formula_84">x = x 1 , • • • , x n is</formula><p>given and the goal is to generate target text y, we need a pre-trained model that is both capable of encoding the input text and generating the output text. There are two popular architectures for this purpose that share a common thread of (1) using an encoder with fully-connected mask to encode the source x first and then (2) decode the target y auto-regressively (from the left to right).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prefix Language Model</head><p>The prefix LM is a left-to-right LM that decodes y conditioned on a prefixed sequence x, which is encoded by the same model parameters but with a fully-connected mask. Notably, to encourage the prefix LM to learn better representations of the input, a corrupted text reconstruction objective is usually applied over x, in addition to a standard conditional language modeling objective over y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder-decoder</head><p>The encoder-decoder model is a model that uses a left-to-right LM to decode y conditioned on a separate encoder for text x with a fully-connected mask; the parameters of the encoder and decoder are not shared. Similarly to the prefix LM, diverse types of noising can be applied to the input x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example &amp; Applicable Scenario</head><p>Prefix LMs have been explored in UniLM </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Prompt Engineering</head><p>Prompt engineering is the process of creating a prompting function f prompt (x) that results in the most effective performance on the downstream task. In many previous works, this has involved prompt template engineering, where a human engineer or algorithm searches for the best template for each task the model is expected to perform. As shown in the "Prompt Engineering" section of Fig. <ref type="figure" target="#fig_7">1</ref>, one must first consider the prompt shape, and then decide whether to take a manual or automated approach to create prompts of the desired shape, as detailed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Prompt Shape</head><p>As noted above, there are two main varieties of prompts: cloze prompts (Petroni et al., 2019; Cui et al., 2021), which fill in the blanks of a textual string, and prefix prompts (Li and Liang, 2021; Lester et al., 2021), which continue a string prefix. Which one is chosen will depend both on the task and the model that is being used to solve the task. In general, for tasks regarding generation, or tasks being solved using a standard auto-regressive LM, prefix prompts tend to be more conducive, as they mesh well with the left-to-right nature of the model. For tasks that are solved using masked LMs, cloze prompts are a good fit, as they very closely match the form of the pre-training task. Full text reconstruction models are more versatile, and can be used with either cloze or prefix prompts. Finally, for some tasks regarding multiple inputs such as text pair classification, prompt templates must contain space for two inputs, [X1] and [X2], or more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Manual Template Engineering</head><p>Perhaps the most natural way to create prompts is to manually create intuitive templates based on human introspection. For example, the seminal LAMA dataset <ref type="bibr" target="#b143">(Petroni et al., 2019)</ref> provides manually created cloze templates to probe knowledge in LMs. Brown et al. (2020) create manually crafted prefix prompts to handle a wide variety of tasks, including question answering, translation, and probing tasks for common sense reasoning. Schick and Schütze (2020, 2021a,b) use pre-defined templates in a few-shot learning setting on text classification and conditional text generation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Automated Template Learning</head><p>While the strategy of manually crafting templates is intuitive and does allow solving various tasks with some degree of accuracy, there are also several issues with this approach: (1) creating and experimenting with these prompts is an art that takes time and experience, particularly for some complicated tasks such as semantic parsing <ref type="bibr" target="#b168">(Shin et al., 2021)</ref>; (2) even experienced prompt designers may fail to manually discover optimal prompts (Jiang et al., 2020c).</p><p>To address these problems, a number of methods have been proposed to automate the template design process. In particular, the automatically induced prompts can be further separated into discrete prompts, where the prompt is an 4.3 Automated Template Learning actual text string, and continuous prompts, where the prompt is instead described directly in the embedding space of the underlying LM.</p><p>One other orthogonal design consideration is whether the prompting function f prompt (x) is static, using essentially the same prompt template for each input, or dynamic, generating a custom template for each input. Both static and dynamic strategies have been used for different varieties of discrete and continuous prompts, as we will mention below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Discrete Prompts</head><p>Works on discovering discrete prompts (a.k.a hard prompts) automatically search for templates described in a discrete space, usually corresponding to natural language phrases. We detail several methods that have been proposed for this below: D1: Prompt Mining Jiang et al. (2020c)'s MINE approach is a mining-based method to automatically find templates given a set of training inputs x and outputs y. This method scrapes a large text corpus (e.g. Wikipedia) for strings containing x and y, and finds either the middle words or dependency paths between the inputs and outputs. Frequent middle words or dependency paths can serve as a template as in "[X] middle words [Z]". D2: Prompt Paraphrasing Paraphrasing-based approaches take in an existing seed prompt (e.g. manually constructed or mined), and paraphrases it into a set of other candidate prompts, then selects the one that achieves the highest training accuracy on the target task. This paraphrasing can be done in a number of ways, including using round-trip translation of the prompt into another language then back (Jiang et al., 2020c), using replacement of phrases from a thesaurus <ref type="bibr" target="#b203">(Yuan et al., 2021b)</ref>, or using a neural prompt rewriter specifically optimized to improve accuracy of systems using the prompt <ref type="bibr" target="#b68">(Haviv et al., 2021)</ref>. Notably, Haviv et al. (2021) perform paraphrasing after the input x is input into the prompt template, allowing a different paraphrase to be generated for each individual input.</p><p>D3: Gradient-based Search Wallace et al. (2019a) applied a gradient-based search over actual tokens to find short sequences that can trigger the underlying pre-trained LM to generate the desired target prediction. This search is done in an iterative fashion, stepping through tokens in the prompt . Built upon this method, Shin et al. (2020) automatically search for template tokens using downstream application training samples and demonstrates strong performance in prompting scenarios. D4: Prompt Generation Other works treat the generation of prompts as a text generation task and use standard natural language generation models to perform this task. For example, <ref type="bibr" target="#b56">Gao et al. (2021)</ref> introduce the seq2seq pre-trained model T5 into the template search process. Since T5 has been pre-trained on a task of filling in missing spans, they use T5 to generate template tokens by <ref type="bibr" target="#b11">(1)</ref> specifying the position to insert template tokens within a template <ref type="foot" target="#foot_4">4</ref> (2) provide training samples for T5 to decode template tokens. Ben-David et al. (2021) propose a domain adaptation algorithm that trains T5 to generate unique domain relevant features (DRFs; a set of keywords that characterize domain information) for each input. Then those DRFs can be concatenated with the input to form a template and be further used by downstream tasks. D5: Prompt Scoring Davison et al. (2019) investigate the task of knowledge base completion and design a template for an input (head-relation-tail triple) using LMs. They first hand-craft a set of templates as potential candidates, and fill the input and answer slots to form a filled prompt. They then use a unidirectional LM to score those filled prompts, selecting the one with the highest LM probability. This will result in custom template for each individual input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Continuous Prompts</head><p>Because the purpose of prompt construction is to find a method that allows an LM to effectively perform a task, rather than being for human consumption, it is not necessary to limit the prompt to human-interpretable natural language. Because of this, there are also methods that examine continuous prompts (a.k.a. soft prompts) that perform prompting directly in the embedding space of the model. Specifically, continuous prompts remove two constraints: (1) relax the constraint that the embeddings of template words be the embeddings of natural language (e.g., English) words. (2) Remove the restriction that the template is parameterized by the pre-trained LM's parameters. Instead, templates have their own parameters that can be tuned based on training data from the downstream task. We highlight several representative methods below.</p><p>C1: Prefix Tuning Prefix Tuning (Li and Liang, 2021) is a method that prepends a sequence of continuous task-specific vectors to the input, while keeping the LM parameters frozen. Mathematically, this consists of optimizing over the following log-likelihood objective given a trainable prefix matrix M φ and a fixed pre-trained LM parameterized by θ. max</p><formula xml:id="formula_85">φ log P (y|x; θ; φ) = max φ yi log P (y i |h &lt;i ; θ; φ)<label>(2)</label></formula><p>In Eq. 2,</p><formula xml:id="formula_86">h &lt;i = [h<label>(1)</label></formula><p>&lt;i ;</p><formula xml:id="formula_87">• • • ; h (n)</formula><p>&lt;i ] is the concatenation of all neural network layers at time step i. It is copied from M φ directly if the corresponding time step is within the prefix (h i is M φ [i]), otherwise it is computed using the pre-trained LM.</p><p>Experimentally, Li and Liang (2021) observe that such continuous prefix-based learning is more sensitive to different initialization in low-data settings than the use of discrete prompts with real words. Similarly, <ref type="bibr" target="#b101">Lester et al. (2021)</ref> prepend the input sequence with special tokens to form a template and tune the embeddings of these tokens directly. Compared to Li and Liang (2021)'s method, this adds fewer parameters as it doesn't introduce additional tunable parameters within each network layer. Tsimpoukelli et al. (2021) train a vision encoder that encodes an image into a sequence of embeddings that can be used to prompt a frozen auto-regressive LM to generate the appropriate caption. They show that the resulting model can perform few-shot learning for vision-language tasks such as visual question answering etc. Different from the above two works, the prefix used in <ref type="bibr" target="#b184">(Tsimpoukelli et al., 2021</ref>) is sample-dependent, namely a representation of input images, instead of a task embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C2: Tuning Initialized with Discrete Prompts</head><p>There are also methods that initialize the search for a continuous prompt using a prompt that has already been created or discovered using discrete prompt search methods. For example, <ref type="bibr" target="#b213">Zhong et al. (2021b)</ref> first define a template using a discrete search method such as AUTOPROMPT (Shin et al., 2020)'s, initialize virtual tokens based on this discovered prompt, then fine-tune the embeddings to increase task accuracy. This work found that initializing with manual templates can provide a better starting point for the search process. <ref type="bibr" target="#b147">Qin and Eisner (2021)</ref> propose to learn a mixture of soft templates for each input where the weights and parameters for each template are jointly learned using training samples. The initial set of templates they use are either manually crafted ones or those obtained using the "prompt mining" method. Similarly, <ref type="bibr" target="#b65">Hambardzumyan et al. (2021)</ref> introduce the use of a continuous template whose shape follows a manual prompt template.</p><p>C3: Hard-Soft Prompt Hybrid Tuning Instead of using a purely learnable prompt template, these methods insert some tunable embeddings into a hard prompt template. <ref type="bibr" target="#b10">Liu et al. (2021b)</ref> propose "P-tuning", where continuous prompts are learned by inserting trainable variables into the embedded input. To account for interaction between prompt tokens, they represent prompt embeddings as the output of a BiLSTM <ref type="bibr" target="#b61">(Graves et al., 2013)</ref>. P-tuning also introduces the use of task-related anchor tokens (such as "capital" in relation extraction) within the template for further improvement. These anchor tokens are not tuned during training. <ref type="bibr" target="#b194">Han et al. (2021)</ref> propose prompt tuning with rules (PTR), which uses manually crafted sub-templates to compose a complete template using logic rules. To enhance the representation ability of the resulting template, they also insert several virtual tokens whose embeddings can be tuned together with the pre-trained LMs parameters using training samples. The template tokens in PTR contain both actual tokens and virtual tokens. Experiment results demonstrate the effectiveness of this prompt design method in relation classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Answer Engineering</head><p>In contrast to prompt engineering, which designs appropriate inputs for prompting methods, answer engineering aims to search for an answer space Z and a map to the original output Y that results in an effective predictive model. Fig. <ref type="figure" target="#fig_26">1'</ref>s "Answer Engineering" section illustrates two dimensions that must be considered when performing answer engineering: deciding the answer shape and choosing an answer design method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Answer Shape</head><p>The shape of an answer characterizes its granularity. Some common choices include:</p><p>• Tokens: One of the tokens in the pre-trained LM's vocabulary, or a subset of the vocabulary.</p><p>• Span: A short multi-token span. These are usually used together with cloze prompts.</p><p>• Sentence: A sentence or document. These are commonly used with prefix prompts.</p><p>In practice, how to choose the shape of acceptable answers depends on the task we want to perform. Token or text-span answer spaces are widely used in classification tasks (e.g. sentiment classification; Yin et al. ( <ref type="formula">2019</ref>)), but also other tasks such as relation extraction <ref type="bibr" target="#b143">(Petroni et al., 2019)</ref> or named entity recognition <ref type="bibr" target="#b39">(Cui et al., 2021)</ref>. Longer phrasal or sentential answers are often used in language generation tasks <ref type="bibr" target="#b150">(Radford et al., 2019)</ref>, but also </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Answer Space Design Methods</head><p>The next question to answer is how to design the appropriate answer space Z, as well as the mapping to the output space Y if the answers are not used as the final outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Manual Design</head><p>In manual design, the space of potential answers Z and its mapping to Y are crafted manually by an interested system or benchmark designer. There are a number of strategies that can be taken to perform this design.</p><p>Unconstrained Spaces In many cases, the answer space Z is the space of all tokens <ref type="bibr" target="#b143">(Petroni et al., 2019)</ref>, fixed-length spans <ref type="bibr" target="#b76">(Jiang et al., 2020a)</ref>, or token sequences <ref type="bibr" target="#b150">(Radford et al., 2019)</ref>. In these cases, it is most common to directly map answer z to the final output y using the identity mapping.</p><p>Constrained Spaces However, there are also cases where the space of possible outputs is constrained. This is often performed for tasks with a limited label space such as text classification or entity recognition, or multiplechoice question answering. To give some examples, <ref type="bibr" target="#b200">Yin et al. (2019)</ref> manually design lists of words relating to relevant topics ("health", "finance", "politics", "sports", etc.), emotions ("anger", "joy", "sadness", "fear", etc.), or other aspects of the input text to be classified. <ref type="bibr" target="#b39">Cui et al. (2021)</ref> manually design lists such as "person", "location", etc. for NER tasks. In these cases, it is necessary to have a mapping between the answer Z and the underlying class Y.</p><p>With regards to multiple-choice question answering, it is common to use an LM to calculate the probability of an output among multiple choices, with Zweig et al. (2012) being an early example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Discrete Answer Search</head><p>As with manually created prompts, it is possible that manually created answers are sub-optimal for getting the LM to achieve ideal prediction performance. Because of this, there is some work on automatic answer search, albeit less than that on searching for ideal prompts. These work on both discrete answer spaces (this section) and continuous answer spaces (the following).</p><p>Answer Paraphrasing These methods start with an initial answer space Z , and then use paraphrasing to expand this answer space to broaden its coverage <ref type="bibr">(Jiang et al., 2020b)</ref>. Given a pair of answer and output z , y , we define a function that generates a paraphrased set of answers para(z ). The probability of the final output is then defined as the marginal probability all of the answers in this paraphrase set P (y|x) = z∈para(z ) P (z|x). This paraphrasing can be performed using any method, but Jiang et al. (2020b) specifically use a back-translation method, first translating into another language then back to generate a list of multiple paraphrased answers.</p><p>Prune-then-Search In these methods, first, an initial pruned answer space of several plausible answers Z is generated, and then an algorithm further searches over this pruned space to select a final set of answers. Note that in some of the papers introduced below, they define a function from label y to a single answer token z, which is often called a verbalizer (Schick and Schütze, 2021a). Schick and Schütze (2021a); <ref type="bibr">Schick et al. (2020)</ref> find tokens containing at least two alphabetic characters that are frequent in a large unlabeled dataset. In the search step, they iteratively compute a word's suitability as a representative answer z for a label y by maximizing the likelihood of the label over training data. Shin et al. (2020) learn a logistic classifier using the contextualized representation of the [Z] token as input. In the search step, they select the top-k tokens that achieve the highest probability score using the learned logistic classifier in the first step. Those selected tokens will form the answer. <ref type="bibr">Gao</ref>   <ref type="formula">2021b</ref>) automatically decompose each relation label into its constituent words and use them as an answer. For example, for the relation per:city of death, the decomposed label words would be {person, city, death}. The probability of the answer span will be calculated as the sum of each token's probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Continuous Answer Search</head><p>Very few works explore the possibility of using soft answer tokens which can be optimized through gradient descent. <ref type="bibr" target="#b65">Hambardzumyan et al. (2021)</ref> assign a virtual token for each class label and optimize the token embedding for each China's capital is <ref type="bibr">[MASK]</ref>. PR1</p><p>[MASK] is the capital of China. PR2</p><p>The capital of China is <ref type="bibr">[MASK]</ref>. PR3 Subject: China; Relation: isCapital Input Google became a subsidiary of Alphabet. Input (X)</p><formula xml:id="formula_88">[X] The [MASK] Google. Sub-PR1 [X] The [MASK] Alphabet. Sub-PR2 Sub-PR3 [X] Google [MASK] Alphabet. [X] The [MASK] Google [MASK] the [MASK] Alphabet. PR [X] Mike is [MASK] entity type. Sub-PR1</formula><p>Mike went to New York yesterday. Input (X) (a) Prompt Ensembling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>China's capital is [MASK]. PR1</head><p>[MASK] is the capital of China. PR2</p><p>The capital of China is <ref type="bibr">[MASK]</ref>. PR3 Subject: China; Relation: isCapital Input Google became a subsidiary of Alphabet. Input (X)</p><formula xml:id="formula_89">[X] The [MASK] Google. Sub-PR1 [X] The [MASK] Alphabet. Sub-PR2 Sub-PR3 [X] Google [MASK] Alphabet. [X] The [MASK] Google [MASK] the [MASK] Alphabet. PR [X] Mike is [MASK] entity type. Sub-PR1</formula><p>Mike went to New York yesterday. Input (X) China's capital is <ref type="bibr">[MASK]</ref>. PR1</p><p>[MASK] is the capital of China. PR2</p><p>The capital of China is <ref type="bibr">[MASK]</ref>. PR3 Subject: China; Relation: isCapital Input Google became a subsidiary of Alphabet. Input (X)</p><formula xml:id="formula_90">[X] The [MASK] Google. Sub-PR1 [X] The [MASK] Alphabet. Sub-PR2 Sub-PR3 [X] Google [MASK] Alphabet. [X] The [MASK] Google [MASK] the [MASK] Alphabet. PR [X] Mike is [MASK] entity type. Sub-PR1</formula><p>Mike went to New York yesterday. Input (X) (c) Prompt Composition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>China's capital is [MASK]. PR1</head><p>[MASK] is the capital of China. PR2</p><p>The capital of China is <ref type="bibr">[MASK]</ref>. PR3 Subject: China; Relation: isCapital Input Google became a subsidiary of Alphabet. Input (X)</p><formula xml:id="formula_91">[X] The [MASK] Google. Sub-PR1 [X] The [MASK] Alphabet. Sub-PR2 Sub-PR3 [X] Google [MASK] Alphabet. [X] The [MASK] Google [MASK] the [MASK] Alphabet. PR [X] Mike is [MASK] entity type. Sub-PR1</formula><p>Mike went to New York yesterday. Input (X)  Mike went to New York yesterday.</p><p>[X] Mike is " for sub-prompt. We use the following abbreviations. "PR" for prompt, "Ans-PR" for answered prompt, "Sub-PR" for sub-prompt. class together with prompt token embeddings. Since the answer tokens are optimized directly in the embedding space, they do not make use of the embeddings learned by the LM and instead learn an embedding from scratch for each label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Multi-Prompt Learning</head><p>The prompt engineering methods we discussed so far focused mainly on constructing a single prompt for an input. However, a significant body of research has demonstrated that the use of multiple prompts can further improve the efficacy of prompting methods, and we will call these methods multi-prompt learning methods. In practice, there are several ways to extend the single prompt learning to the use multiple prompts, which have a variety of motivations. We summarize representative methods in the "Multi-prompt Learning" section of Fig. <ref type="figure" target="#fig_7">1</ref> as well as Fig. <ref type="figure" target="#fig_25">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Prompt Ensembling</head><p>Prompt ensembling is the process of using multiple unanswered prompts for an input at inference time to make predictions. An example is shown in Fig. <ref type="figure" target="#fig_27">4-(a</ref>). The multiple prompts can either be discrete prompts or continuous prompts. <ref type="foot" target="#foot_5">5</ref> This sort of prompt ensembling can (1) leverage the complementary advantages of different prompts, (2) alleviate the cost of prompt engineering, since choosing one best-performing prompt is challenging, (3) stabilize performance on downstream tasks.</p><p>Prompt ensembling is connected to ensembling methods that are used to combine together multiple systems, which have a long history in machine learning (Ting and Witten, 1997; <ref type="bibr" target="#b214">Zhou et al., 2002;</ref><ref type="bibr" target="#b48">Duh et al., 2011)</ref>. Current research also borrows ideas from these works to derive effective ways for prompt ensembling, as described below.</p><p>Uniform averaging The most intuitive way to combine the predictions when using multiple prompts is to take the average of probabilities from different prompts. Concretely, this indicates that P (z|x) := 1 K K i P (z|f prompt,i (x)) where f prompt,i (•) is the ith prompt in the prompt ensemble. <ref type="bibr" target="#b78">Jiang et al. (2020c)</ref> first filter their prompts by selecting K prompts that achieve the highest accuracy on the training set, and then use the average log probabilities obtained from the top K prompts to calculate the probability for a single token at [Z] position when performing factual probing tasks. Schick and Schütze (2021a) also try a simple average when using an ensemble model to annotate an unlabeled dataset. When performing text generation evaluation, <ref type="bibr" target="#b203">Yuan et al. (2021b)</ref> formulates this task as a text generation problem and take the average of the final generation scores obtained using different prompts.</p><p>Weighted averaging Simple uniform averaging of results from multiple prompts is easy to implement, but can also be suboptimal given that some prompts are more performant than others. To account for this, some works also 6.2 Prompt Augmentation explore to use of weighted averages for prompt ensembling where each prompt is associated with a weight. The weights are typically pre-specified based on prompt performance or optimized using a training set. For example, Jiang et al. (2020c) learn the weight for each prompt by maximizing the probability of the target output over training data. Qin and Eisner (2021) use the same approach except that the weight for each prompt is optimized together with soft prompt parameters. Besides, Qin and Eisner (2021) also introduce a data-dependent weighting strategy where the probability of the input appearing in that prompt is considered in weighting different prompts as well. Schick and Schütze (2021a,b) set the weight for each prompt proportional to the accuracy on the training set before training.</p><p>Majority voting For classification tasks, majority voting can also be used to combine the results from different prompts <ref type="bibr" target="#b101">(Lester et al., 2021;</ref><ref type="bibr" target="#b65">Hambardzumyan et al., 2021)</ref>.</p><p>Knowledge distillation An ensemble of deep learning models can typically improve the performance, and this superior performance can be distilled into a single model using knowledge distillation (Allen-Zhu and Li, 2020). To incorporate this idea, Schick and Schütze (2021a,b, 2020) train a separate model for each manually-created template-answer pair, and use the ensemble of them to annotate an unlabeled dataset. Then the final model is trained to distill the knowledge from the annotated dataset. <ref type="bibr" target="#b56">Gao et al. (2021)</ref> use a similar ensemble method on their automatically generated templates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prompt ensembling for text generation</head><p>There is relatively little work on prompt ensembling for generation tasks (i.e. tasks where the answers is a string of tokens instead of a single one). A simple way to perform ensembling in this case is to use standard methods that generate the output based on the ensembled probability of the next word in the answer sequence P (z t |x, z &lt;t ) := 1 K K i P (z t |f prompt,i (x), z &lt;t ). In contrast, Schick and Schütze (2020) train a separate model for each prompt f prompt,i (x), and thus storing each of these fine-tuned LMs in memory is infeasible. Instead, they first decode generations using each model and then score each generation by averaging their generation probability across all models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Prompt Augmentation</head><p>Prompt augmentation, also sometimes called demonstration learning <ref type="bibr" target="#b56">(Gao et al., 2021)</ref>, provides a few additional answered prompts that can be used to demonstrate how the LM should provide the answer to the actual prompt instantiated with the input x. For example, instead of just providing a prompt of "China's capital is <ref type="bibr">[Z]</ref> .", the prompt can be prefaced by a few examples such as "Great Britain's capital is London . Japan's capital is Tokyo . China's capital is [Z] ." Another example of performing addition of two numbers can be found in Fig. <ref type="figure" target="#fig_27">4-(b</ref>). These few-shot demonstrations take advantage of the ability of strong language models to learn repetitive patterns <ref type="bibr">(Brown et al., 2020)</ref>.</p><p>Although the idea of prompt augmentation is simple, there are several aspects that make it challenging: Prompt augmentation is closely related to retrieval-based methods that provide more textual context to the model to improve performance <ref type="bibr" target="#b62">(Guu et al., 2018)</ref>, a method which has also been shown to be effective in prompt-based learning <ref type="bibr" target="#b142">(Petroni et al., 2020)</ref>. However, the key difference lies in the fact that prompt augmentation also leverages the template and answer, while larger context learning does not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Prompt Composition</head><p>For those composable tasks, which can be composed based on more fundamental subtasks, we can also perform prompt composition, using multiple sub-prompts, each for one subtask, and then defining a composite prompt based on those sub-prompts. This process is illustrated in Fig. <ref type="figure" target="#fig_27">4-(c</ref>). For example, in the relation extraction task, which aims to extract the relation of two entities, we can break down the task into several subtasks including identifying the characteristics of entities and classifying the relationships between entities. Based on this intuition, Han et al. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Prompt Decomposition</head><p>For tasks where multiple predictions should be performed for one sample (e.g., sequence labeling), directly defining a holistic prompt with regards to the entire input text x is challenging. One intuitive method to address this problem is to break down the holistic prompt into different sub-prompts, and then answer each sub-prompt separately. Fig. <ref type="figure" target="#fig_27">4-(d</ref>) illustrates this idea with an example from the named entity recognition task, which aims to identify all named entities in an input sentence. In this case, the input will first be converted into a set of text spans, and the model can then be prompted to predict the entity type (including "Not an Entity") for each span. It is not easy to predict all the span types at the same time due to the large number of spans, so different prompts for each span can be created and predicted separately. This sort of prompt decomposition for named entity recognition has been explored by <ref type="bibr" target="#b39">Cui et al. (2021)</ref> where they apply the approach we discussed here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Training Strategies for Prompting Methods</head><p>With the methods in the above sections, it is now clear how to obtain an appropriate prompt (or prompts) and corresponding answers. Now we discuss about methods that explicitly train models in concert with prompting methods, as outlined in the "Training Strategies" section of Fig. <ref type="figure" target="#fig_7">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Training Settings</head><p>In many cases, prompting methods can be used without any explicit training of the LM for the down-stream task, simply taking an LM that has been trained to predict the probability of text P (x) and applying it as-is to fill the cloze or prefix prompts defined to specify the task. This is traditionally called the zero-shot setting, as there is zero training data for the task of interest.</p><p>However, there are also methods that use training data to train the model in concert with prompting methods. These consist of either full-data learning, where a reasonably large number of training examples are used to train the model, or few-shot learning where a very small number of examples are used to train the model. Prompting methods are particularly useful in the latter case, as there are generally not enough training examples to fully specify the desired behavior, and thus using a prompt to push the model in the right direction is particularly effective.</p><p>One thing to note is that for many of the prompt engineering methods described in §4, although annotated training samples are not explicitly used in the training of the downstream task model, they are often used in the construction or validation of the prompts that the downstream task will use. As noted by <ref type="bibr" target="#b139">Perez et al. (2021)</ref>, this is arguably not true zero-shot learning with respect to the downstream task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Parameter Update Methods</head><p>In prompt-based downstream task learning, there are usually two types of parameters, namely those from (1) pre-trained models and (2) prompts. Which part of parameters should be updated is one important design decision, which can lead to different levels of applicability in different scenarios. We summarize five tuning strategies (as shown in Tab. 6) based on (i) whether the parameters of the underlying LM are tuned, (ii) whether there are additional prompt-related parameters, (iii) if there are additional prompt-related parameters, whether those parameters are tuned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Strategy LM Params Prompt Params Example Additional Tuned</head><p>Promptless Fine-tuning Tuned -ELMo <ref type="bibr" target="#b140">[130]</ref>, BERT <ref type="bibr" target="#b42">[32]</ref>, BART <ref type="bibr" target="#b104">[94]</ref> Tuning-free Prompting Frozen GPT-3 <ref type="bibr" target="#b26">[16]</ref>, AutoPrompt <ref type="bibr" target="#b169">[159]</ref>, LAMA <ref type="bibr" target="#b143">[133]</ref> Fixed-LM Prompt Tuning Frozen Tuned Prefix-Tuning <ref type="bibr" target="#b106">[96]</ref>, Prompt-Tuning <ref type="bibr" target="#b101">[91]</ref> Fixed-prompt LM Tuning Tuned PET-TC <ref type="bibr" target="#b163">[153]</ref>, PET-Gen <ref type="bibr" target="#b162">[152]</ref>, LM-BFF <ref type="bibr" target="#b56">[46]</ref> Prompt+LM Fine-tuning Tuned Tuned PADA <ref type="bibr" target="#b18">[8]</ref>, P-Tuning <ref type="bibr" target="#b113">[103]</ref>, PTR <ref type="bibr" target="#b66">[56]</ref> Table <ref type="table">6</ref>: Characteristics of different tuning strategies. "Additional" represents if there are additional parameters beyond LM parameters while "Tuned" denotes if parameters are updated.</p><p>7.2 Parameter Update Methods</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1">Promptless Fine-tuning</head><p>As mentioned in the introduction, the pre-train and fine-tune strategy has been widely used in NLP since before the popularization of prompting methods. Here we refer to pre-training and fine-tuning without prompts as promptless fine-tuning, to contrast with the prompt-based learning methods introduced in the following sections. In this strategy, given a dataset of a task, all (or some (Howard and Ruder, 2018; Peters et al., 2019)) of the parameters of the pre-trained LM will be updated via gradients induced from downstream training samples. Typical examples of pre-trained models tuned in this way include BERT <ref type="bibr" target="#b42">[32]</ref> and RoBERTa <ref type="bibr" target="#b115">[105]</ref>. This is a simple, powerful, and widely-used method, but it may overfit or not learn stably on small datasets <ref type="bibr" target="#b44">(Dodge et al., 2020)</ref>. Models are also prone to catastrophic forgetting, where the LM loses its ability to do things that it was able to do before fine-tuning (McCloskey and Cohen, 1989).</p><p>• Advantages: Simplicity, no need for prompt design. Tuning all the LM parameters allows the model to fit to larger training datasets. • Disadvantages: LMs may overfit or not learn stably on smaller datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.2">Tuning-free Prompting</head><p>Tuning-free prompting directly generates the answers without changing the parameters of the pre-trained LMs based only on a prompt, as described in the simplest incarnation of prompting in §2. These can be optionally augmenting input with answered prompts as described in §6.2, and this combination of tuning-free prompting and prompt augmentation is also referred to as in-context learning <ref type="bibr">(Brown et al., 2020)</ref>. Typical examples of tuning-free prompting include LAMA <ref type="bibr" target="#b143">[133]</ref> and GPT-3 <ref type="bibr" target="#b26">[16]</ref>.</p><p>• Advantages: Efficiency, there is no parameter update process. No catastrophic forgetting, as LM parameters remain fixed. Applicable in zero-shot settings. • Disadvantages: Because prompts are the only method that provide the task specification, heavy engineering is necessary to achieve high accuracy. In particular in the in-context learning setting, providing many answered prompts can be slow at test time, and thus cannot easily use large training datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.3">Fixed-LM Prompt Tuning</head><p>In the scenario where additional prompt-relevant parameters are introduced besides parameters of the pre-trained model, fixed-LM prompt tuning updates only the prompts' parameters using the supervision signal obtained from the downstream training samples, while keeping the entire pre-trained LM unchanged. Typical examples are Prefix-Tuning <ref type="bibr" target="#b106">[96]</ref> and WARP <ref type="bibr" target="#b65">[55]</ref>.</p><p>• Advantages: Similarly to tuning-free prompting, it can retain knowledge in LMs and is suitable in few-shot scenarios. Often superior accuracy to tuning-free prompting. • Disadvantages: Not applicable in zero-shot scenarios. While effective in few-shot scenarios, representation power is limited in large-data settings. Prompt engineering through choice of hyperparameters or seed prompts is necessary. Prompts are usually not human-interpretable or manipulable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.4">Fixed-prompt LM Tuning</head><p>Fixed-prompt LM tuning tunes the parameters of the LM, as in the standard pre-train and fine-tune paradigm, but additionally uses prompts with fixed parameters to specify the model behavior. This potentially leads to improvements, particularly in few-shot scenarios.</p><p>The most natural way to do so is to provide a discrete textual template that is applied to every training and test example. Typical examples include PET-TC <ref type="bibr" target="#b163">[153]</ref>, PET-Gen <ref type="bibr" target="#b162">[152]</ref>, LM-BFF <ref type="bibr" target="#b56">[46]</ref>. Logan IV et al. (2021) more recently observe that the prompt engineering can be reduced by allowing for a combination of answer engineering and partial LM fine-tuning. For example, they define a very simple template, null prompt, where the input and mask are directly concatenated "[X][Z]" without any template words, and find this achieves competitive accuracy.</p><p>• Advantages: Prompt or answer engineering more completely specify the task, allowing for more efficient learning, particularly in few-shot scenarios. • Disadvantages: Prompt or answer engineering are still required, although perhaps not as much as without prompting. LMs fine-tuned on one downstream task may not be effective on another one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.5">Prompt+LM Tuning</head><p>In this setting, there are prompt-relevant parameters, which can be fine-tuned together with the all or some of the parameters of the pre-trained models. Representative examples include PADA <ref type="bibr" target="#b18">[8]</ref>, P-Tuning <ref type="bibr" target="#b113">[103]</ref>. Notably, this setting is very similar to the standard pre-train and fine-tune paradigm, but the addition of the prompt can provide additional bootstrapping at the start of model training.</p><p>• Advantages: This is the most expressive method, likely suitable for high-data settings.</p><p>• Disadvantages: Requires training and storing all parameters of the models. May overfit to small datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Applications</head><p>In previous sections, we examined prompting methods from the point of view of the mechanism of the method itself.</p><p>In this section, we rather organize prompting methods from the point of view of which applications they have been applied to. We list these applications in Tab. 7-8 and summarize them in the following sections.  <ref type="formula">2020b</ref>) take a closer look at such prompt-based QA systems using sequence to sequence pre-trained models (T5, BART, GPT2) and observe that probabilities from these pre-trained models on QA tasks are not very predictive of whether the model is correct or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.6">Text Generation</head><p>Text generation is a family of tasks that involve generating text, usually conditioned on some other piece of information. Prompting methods can be easily applied to these tasks by using prefix prompts together with autoregressive pre-trained LMs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.7">Automatic Evaluation of Text Generation</head><p>Yuan et al. (2021b) have demonstrated that prompt learning can be used for automated evaluation of generated texts. Specifically, they conceptualize the evaluation of generated text as a text generation problem, modeled using a pre-trained sequence-to-sequence, and then use prefix prompts that bring the evaluation task closer to the pre-training task. They experimentally find that simply adding the phrase "such as" to the translated text when using pre-trained models can lead to a significant improvement in correlation on German-English machine translation (MT) evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.8">Multi-modal Learning</head><p>Tsimpoukelli et al. (2021) shift the application of prompt learning from text-based NLP to the multi-modal setting (vision and language). Generally, they adopt the fixed-LM prompt tuning strategy together with prompt augmentation techniques. They specifically represent each image as a sequence of continuous embeddings, and a pre-trained LM whose parameters are frozen is prompted with this prefix to generate texts such as image captions. Empirical results show few-shot learning ability: with the help of a few demonstrations (answered prompts), system can rapidly learn words for new objects and novel visual categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.9">Meta-Applications</head><p>There are also a number of applications of prompting techniques that are not NLP tasks in and of themselves, but are useful elements of training strong models for any application.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain Adaptation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Controlled</head><p>Text Generation <ref type="bibr">[191; 77; 156]</ref> Input is augmented with additional inputs to control the generation process Controlled generation targets generation of a particular type of text while prompt learning uses prompts to specify the task itself.</p><p>Prompt-based downstream task learning <ref type="bibr">[153; 193]</ref> Supervised Attention <ref type="bibr">[101; 165]</ref> Require external hint to remind the model of which part information should be focused on Research works on supervised attention usually target at salient information from an image or text, while prompt learning aims to utilize relevant knowledge from the pre-trained model. Data augmentation <ref type="bibr">[40; 144]</ref> Improving downstream tasks' performance by introducing additional samples Data augmentation introduce additional training samples in an explicit way while prompts can be regarded as highlycondensed training samples <ref type="bibr" target="#b98">[88]</ref>. Larger-context Learning Larger-context learning aims to improve the system's performance by augmenting the input with additional contextual information, e.g. retrieved from the training set <ref type="bibr" target="#b28">(Cao et al., 2018)</ref> or external data sources <ref type="bibr" target="#b63">(Guu et al., 2020)</ref>. Prompt augmentation can be regarded as adding relevant labeled samples into the input, but a minor difference is in larger-context learning, the introduced context is not necessarily labeled data.</p><p>Query Reformulation Query reformulation <ref type="bibr" target="#b124">(Mathieu and Sabatier, 1986;</ref><ref type="bibr" target="#b40">Daumé III and Brill, 2004</ref>) is commonly used in information retrieval (Nogueira and Cho, 2017) and question answering tasks <ref type="bibr" target="#b27">(Buck et al., 2017;</ref><ref type="bibr" target="#b185">Vakulenko et al., 2020)</ref>, which aim to elicit more relevant texts (documents or answers) by expanding the input query with related query terms <ref type="bibr" target="#b67">(Hassan, 2013)</ref> or generating paraphrases. There are several commonalities between promptbased learning and query reformulation, for example (1) both aim to make better use of some existing knowledge bases by asking a right questions (2) the knowledge bases are usually a black-box, not available to the users, so researchers must learn how to probe it optimally based on solely questions.</p><p>There are also differences: the knowledge base in traditional query reformulation problems is usually a search engine (Nogueira and Cho, 2017), or QA system <ref type="bibr" target="#b27">(Buck et al., 2017)</ref>. By contrast, for prompt-based learning, we usually define this knowledge base as an LM, and need to find the appropriate query to elicit an appropriate answer from it. The input reformulation in prompt learning has changed the form of tasks. For example, an original text classification task has been converted into a cloze question problem, therefore bringing additional complexity regarding how to (1) make an appropriate task formulation, and (2) change the modeling framework accordingly. These steps are not required in traditional query formulation. Despite these discrepancies, some methodologies from query reformulation research still can be borrowed for prompt learning, such as decomposing input query into multiple sub-queries <ref type="bibr" target="#b134">(Nogueira et al., 2019)</ref>, similar to prompt decomposition.</p><p>QA-based Task Formulation QA-based task formulation aims to conceptualize different NLP tasks as a questionanswering problem. <ref type="bibr" target="#b92">(Kumar et al., 2016;</ref><ref type="bibr" target="#b125">McCann et al., 2018)</ref> are earlier works that attempt to unify multiple NLP tasks into a QA framework. Later, this idea has been further explored in information extraction <ref type="bibr" target="#b107">(Li et al., 2020;</ref><ref type="bibr" target="#b192">Wu et al., 2020</ref>) and text classification <ref type="bibr" target="#b29">(Chai et al., 2020)</ref>. These methods are very similar to the prompting methods introduced here in that they use textual questions to specify which task is to be performed. However, one of the key points of prompting methods is how to better use the knowledge in pre-trained LMs, and these were not covered extensively on previous works advocating for QA formulations.</p><p>Controlled Generation Controlled generation aims to incorporate various types of guidance beyond the input text into the generation model <ref type="bibr" target="#b178">(Yu et al., 2020)</ref>. Specifically, the guidance signals could be style tokens <ref type="bibr" target="#b167">(Sennrich et al., 2016b;</ref><ref type="bibr" target="#b51">Fan et al., 2018)</ref>, length specifications <ref type="bibr" target="#b87">(Kikuchi et al., 2016)</ref>, domain tags <ref type="bibr" target="#b34">(Chu et al., 2017)</ref>, or any variety of other pieces of information used to control of the generated text. It could also be keywords <ref type="bibr" target="#b156">(Saito et al., 2020)</ref>, relation triples <ref type="bibr" target="#b215">(Zhu et al., 2020)</ref> or even highlighted phrases or sentences <ref type="bibr" target="#b60">(Grangier and Auli, 2018;</ref><ref type="bibr" target="#b116">Liu et al., 2021c)</ref> to plan the content of generated texts. In a way, many of the prompting methods described here are a type of controllable generation, where the prompt is usually used to specify the task itself. Thus, it is relatively easy to find commonalities between the two genres: <ref type="bibr" target="#b11">(1)</ref> both add extra information to the input text for better generation, and these additional signals are (often) learnable parameters. (2) If "controlled generation" is equipped with seq2seq-based pre-trained models (e.g., BART), then it is can be regarded as prompt learning with input-dependent prompts and the prompt+LM fine-tuning strategy ( §7.2.5), e.g. GSum <ref type="bibr" target="#b46">(Dou et al., 2021)</ref>, where both the prompt's and pre-trained LM's parameters can be tuned.</p><p>Also, some clear discrepancies between controlled generation and prompt-based text generation are: (1) In controlled generation work, the control is generally performed over the style or content of the generations <ref type="bibr" target="#b51">(Fan et al., 2018;</ref><ref type="bibr" target="#b46">Dou et al., 2021)</ref> while the underlying task remains the same. They don't necessarily require a pre-trained model. In contrast, the main motivation for using prompts for text generation is to specify the task itself and better utilize the pre-trained model. (2) Moreover, most of the current work on prompt learning in text generation shares a dataset-or task-level prompt <ref type="bibr">(Li and Liang, 2021)</ref>. Only very few works have explored input-dependent ones <ref type="bibr" target="#b184">(Tsimpoukelli et al., 2021)</ref>. However, this is a common setting and effective in the controlled text generation, which may provide valuable direction for the future work on prompt learning. Supervised Attention Knowing to pay attention to the important information is a key step when extracting useful information from objects such as long text sequences <ref type="bibr" target="#b111">(Liu et al., 2016;</ref><ref type="bibr" target="#b173">Sood et al., 2020)</ref>, images <ref type="bibr" target="#b175">(Sugano and Bulling, 2016;</ref><ref type="bibr" target="#b206">Zhang et al., 2020b)</ref>, or knowledge bases <ref type="bibr" target="#b178">(Yu et al., 2020;</ref><ref type="bibr" target="#b46">Dou et al., 2021)</ref>). Supervised attention <ref type="bibr" target="#b112">(Liu et al., 2017b)</ref> aims to provide explicit supervision over the attention of models based on the fact that completely data-driven attention can overfit to some artifacts <ref type="bibr" target="#b109">(Liu et al., 2017a)</ref>. In this respect, prompt learning and supervised attention share ideas that both aim to extract salient information with some clues, which need to be provided separately. To solve this problem, supervised attention methods tried to use additional loss functions to learn to predict gold attention on a manually labeled corpus <ref type="bibr" target="#b75">(Jiang et al., 2015;</ref><ref type="bibr" target="#b146">Qiao et al., 2018;</ref><ref type="bibr" target="#b55">Gan et al., 2017)</ref>. Research on prompt learning may also borrow ideas from this literature.</p><p>Data Augmentation Data augmentation is a technique that targets increasing the amount of data that can be used for training by making modifications to existing data <ref type="bibr" target="#b50">(Fadaee et al., 2017;</ref><ref type="bibr" target="#b154">Ratner et al., 2017)</ref>. As recently observed by (Scao and Rush, 2021), adding prompts can achieve a similar accuracy improvement to the addition of 100s of data points on average across classification tasks, which suggests that using prompts for a downstream task is similar to conducting data augmentation implicitly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Challenges</head><p>Although prompt-based learning has shown significant potential among different tasks and scenarios, several challenges remain, some of which we detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.1">Prompt Design</head><p>Tasks beyond Classification and Generation Most existing works about prompt-based learning revolve around either text classification or generation-based tasks. Applications to information extraction and text analysis tasks have been discussed less, largely because the design of prompts is less straightforward. We expect that applying prompting methods to these tasks in the future it will require either reformulating these tasks so that they can be solved using classification or text generation-based methods, or performing effective answer engineering that expresses structured outputs in an appropriate textual format.</p><p>Prompting with Structured Information In many NLP tasks, the inputs are imbued with some variety of structure, such as tree, graph, table, or relational structures. How to best express these structures in prompt or answer engineering is a major challenge. Existing works <ref type="bibr" target="#b31">(Chen et al., 2021b</ref>) make a step by making prompts with additional marks to encode lexical information, such as entity markings. <ref type="bibr" target="#b11">Aghajanyan et al. (2021)</ref> present structured prompts based on hyper text markup language for more fine-grained web text generation. However, moving beyond this to more complicated varieties of structure is largely unexplored, and a potentially interesting research area.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.2">Answer Engineering</head><p>Many-class and Long-answer Classification Tasks For classification-based tasks, there are two main challenges for answer engineering: (a) When there are too many classes, how to select an appropriate answer space becomes a difficult combinatorial optimization problem. (b) When using multi-token answers, how to best decode multiple tokens using LMs remains unknown, although some multi-token decoding methods have been proposed <ref type="bibr" target="#b76">(Jiang et al., 2020a)</ref>.</p><p>Multiple Answers for Generation Tasks For text generation tasks, qualified answers can be semantically equivalent but syntactically diverse. So far, almost all works use prompt learning for text generation relying solely on a single answer, with only a few exceptions <ref type="bibr" target="#b78">(Jiang et al., 2020c)</ref>. How to better guide the learning process with multiple references remains a largely open research problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.3">Selection of Tuning Strategy</head><p>As discussed in §7, there are a fairly wide variety of methods for tuning parameters of prompts, LMs, or both. However, given the nascent stage of this research field, we still lack a systematic understanding of the tradeoffs between these methods. The field could benefit from systematic explorations such as those performed in the pre-train and fine-tune paradigm regarding the tradeoffs between these different strategies <ref type="bibr" target="#b141">(Peters et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.4">Multiple Prompt Learning</head><p>Prompt Ensembling In prompt ensembling methods, the space and time complexity increase as we consider more prompts. How to distill the knowledge from different prompts remains underexplored. <ref type="bibr" target="#b7">Schick and</ref><ref type="bibr">Schütze (2020, 2021a,b</ref>) use an ensemble model to annotate a large dataset to distill the knowledge from multiple prompts.</p><p>In addition, how to select ensemble-worthy prompts is also under-explored. For text generation tasks, the study of prompt ensemble learning has not been performed so far, probably because ensemble learning in text generation itself is relatively complicated. To remedy this problem, some recently proposed neural ensembling methods such as Refactor <ref type="bibr" target="#b116">(Liu et al., 2021c</ref>) could be considered as a method for prompt ensembling in text generation tasks.</p><p>Prompt Composition and Decomposition Both prompt composition and decomposition aim to break down the difficulty of a complicated task input by introducing multiple sub-prompts. In practice, how to make a good choice between them is a crucial step. Empirically, for those token <ref type="bibr" target="#b122">(Ma and Hovy, 2016)</ref> or span <ref type="bibr" target="#b54">(Fu et al., 2021)</ref> prediction tasks (e.g., NER), prompt decomposition can be considered, while for those span relation prediction <ref type="bibr" target="#b100">(Lee et al., 2017)</ref> tasks (e.g., entity coreference), prompts composition would be a better choice. In the future, the general idea of de-/composing can be explored in more scenarios.</p><p>Prompt Augmentation Existing prompt augmentation methods are limited by the input length, i.e., feeding too many demonstrations to input is infeasible. Therefore, how to select informative demonstrations, and order them in an appropriate is an interesting but challenging problem <ref type="bibr" target="#b93">(Kumar and Talukdar, 2021)</ref>.</p><p>Prompt Sharing All the above considerations refer to the application of prompt in a single task, domain or language. We may also consider prompt sharing, where prompt learning is applied to multiple tasks, domains, or languages. Some key issues that may arise include how to design individual prompts for different tasks, and how to modulate their interaction with each other. So far this field has not been explored. Fig. <ref type="figure" target="#fig_29">5</ref> illustrates a simple multiple prompt learning strategy for multiple tasks, where prompt templates are partially shared. With plenty of pre-trained LMs to select from (see §3), how to choose them to better leverage prompt-based learning is an interesting and difficult problem. Although we have conceptually introduced ( §3.4) how different paradigms of pre-trained models are selected for diverse NLP tasks, there are few to no systematic comparisons of the benefits brought by prompt-based learning for different pre-trained LMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.6">Theoretical and Empirical Analysis of Prompting</head><p>Despite their success in many scenarios, theoretical analysis and guarantees for prompt-based learning are scarce. <ref type="bibr" target="#b191">Wei et al. (2021)</ref> showed that soft-prompt tuning can relax the non-degeneracy assumptions (the generation probability of each token is linearly independent) needed for downstream recovery (i.e. recover the ground-truth labels of the downstream task.), making it easier to extract task-specific information. <ref type="bibr" target="#b157">Saunshi et al. (2021)</ref> verified that text classification tasks can be reformulated as sentence completion tasks, thus making language modeling a meaningful pre-training task. Scao and Rush (2021) empirically show that prompting is often worth 100s of data points on average across classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.7">Transferability of Prompts</head><p>Understanding the extent to which prompts are specific to the model and improving the transferability of prompts are also important topics. <ref type="bibr" target="#b139">(Perez et al., 2021)</ref> show that prompts selected under tuned few-shot learning scenario (where one has a larger validation set to choose prompts) generalize well across models of similar sizes while prompts selected under true few-shot learning scenario (where one only has a few training samples) do not generalize as effectively as the former setting among models with similar sizes. The transferability is poor when the model sizes are quite different in both scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.8">Combination of Different Paradigms</head><p>Notably, much of the success of the prompting paradigm is built on top of pre-trained models that were developed for the pre-train and fine-tune paradigm, such as BERT. However, are the pre-training methods that are effective for the latter applicable as-is to the former, or can we entirely re-think our pre-training methods to further improve accuracy or ease of applicability to prompting-based learning? This is an important research question that has not been covered extensively by the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.9">Calibration of Prompting Methods</head><p>Calibration (Gleser, 1996) refers to the ability of a model to make good probabilistic predictions. When using the generation probability of the pre-trained LMs (e.g., BART) to predict the answer, we need to be careful since the probability distribution is typically not well calibrated. <ref type="bibr">Jiang et al. (2020b)</ref> observed the probabilities of pre-trained models (e.g., BART, T5, GPT-2) on QA tasks are well calibrated. Zhao et al. (2021) identify three pitfalls (majority label bias, recency bias and common token bias) that lead the pre-trained LMs to be biased toward certain answers when provided answered prompts. For example, if the final answered prompt has a positive label, then this will bias the model towards predicting positive words. To overcome those pitfalls, <ref type="bibr" target="#b211">Zhao et al. (2021)</ref> first use context-free input (e.g. the prompt would be "Input: Subpar acting. Sentiment: Negative\n Input: Beautiful film. Sentiment: Positive\n Input: N/A. Sentiment:") to get the initial probability distribution P 0 , then they use the real input (e.g. the prompt would be "Input: Subpar acting. Sentiment: Negative\n Input: Beautiful film. Sentiment: Positive\n Input: Amazing. Sentiment:") to get the probability distribution P 1 . Finally, these two distributions can be used to get a calibrated generation probability distribution. However, this method has two drawbacks: (1) it comes with the overhead of finding proper context-free input (e.g. whether to use "N/A" or "None") and (2) the probability distribution of the underlying pre-trained LM is still not calibrated.</p><p>Even though we have a calibrated probability distribution, we also need to be careful when we assume a single gold answer for an input. This is because that all surface forms of a same object will compete for finite probability mass <ref type="bibr" target="#b72">(Holtzman et al., 2021)</ref>. For example, if we consider the gold answer to be "Whirlpool bath", the generation probability of it will typically be low since the word "Bathtub" shares the same meaning and it will take over a large probability mass. To address this issue, we could either (i) perform answer engineering to construct a comprehensive gold answer set using paraphrasing methods ( §5.2.2) or (ii) calibrate the probability of a word based on its prior likelihood within the context <ref type="bibr" target="#b72">(Holtzman et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">Meta Analysis</head><p>In this section, we aim to give a quantitative birds-eye view of existing research on prompting methods by performing a meta analysis over existing research works along different dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.1">Timeline</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.1">Timeline</head><p>We first summarize a number of existing research papers in a chronological order with in the form of a timeline, which hopefully, help researchers who are new to this topic understand the evolution of the field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.2">Trend Analysis</head><p>We also calculate the number of prompt-based papers with respect to different dimensions.</p><p>Year With the emergence of different kinds of pre-trained LMs, prompt-based learning has become a more and more active research field, as can be seen in Fig. <ref type="figure" target="#fig_33">6-(a)</ref>. We can see a huge surge in 2021, which is perhaps due to the prevalence of <ref type="bibr">GPT-3 (Brown et al., 2020)</ref>, which greatly increased the popularity of prompting in the few-shot multi-task setting.  Tasks We plot the number of works that investigate various tasks in Fig. <ref type="figure" target="#fig_33">6-(b</ref>). For a task that has fewer than 5 relevant works, we group it into "Others". As the bar chart indicates, most tasks regarding prompt-based learning revolve around text classification and factual probing. We conjecture that this is because that for these tasks, both template engineering and answer engineering are relatively easy to conduct, and experiments are relatively computationally inexpensive.</p><p>Prompt vs. Answer Search As noted in previous sections, both prompt and answer search are important tools to take advantage of pre-trained language models for many tasks. Current research mainly focuses on template search instead of answer search, as shown in Fig. <ref type="figure" target="#fig_33">6-(c</ref>).</p><p>Likely reasons are: (1) For conditional generation tasks (e.g. summarization or translation), the gold references can be directly used as answer. Although there are many sequences that may share the same semantics, how to effectively conduct multi-reference learning in conditional text generation problems is non-trivial. (2) For classification tasks, most of the time, label words are relative easy to select using domain knowledge. Discrete Search vs. Continuous Search Since there are only a few works focus on automatic answer search, we analyze the automatic template search. As time goes by, there has been a shift from discrete search to continuous search for prompt engineering, as shown in Fig. <ref type="figure" target="#fig_33">6-(d)</ref>. Likely reasons are: (1) discrete search is harder to optimize compared to continuous search, (2) soft prompts have greater representation ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12">Conclusion</head><p>In this paper, we have summarized and analyzed several paradigms in the development of statistical natural language processing techniques, and have argued that prompt-based learning is a promising new paradigm that may represent another major change in the way we look at NLP. First and foremost, we hope this survey will help researchers more effectively and comprehensively understand the paradigm of prompt-based learning, and grasp its core challenges so that more scientifically meaningful advances can be made in this field. In addition, looking all the way back to the summary of the four paradigms of NLP research presented in §1, we hope to highlight the commonalities and differences between them, making research on any of these paradigms more full-fledged, and potentially providing a catalyst to inspire work towards the next paradigm shift as well. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Pre-trained Language Model Families</head><p>The increasing number of models makes it difficult for people to clearly grasp the differences between them. Based on this, we cluster the current mainstream pre-training models and characterize them from diverse dimensions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>et al. (2001); Guyon et al. (2002); Och et al. (2004); Zhang and Nivre (2011)), where NLP researchers or engineers used their domain knowledge to define and extract salient features from raw data and provide models with the appropriate inductive bias to learn from this limited data. With the advent of neural network models for NLP, salient features were learned jointly with the training of the model itself (Collobert et al., 2011; Bengio et al., 2013), and hence focus shifted to architecture engineering, where inductive bias was rather provided through the design of a suitable network architecture conducive to learning such features (Tab. 1 b.; e.g. Hochreiter and Schmidhuber (1997); Kalchbrenner et al. (2014); Chung et al. (2014); Kim (2014); Bahdanau et al. (2014); Vaswani et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>and Narasimhan (2018); Peters et al. (2018); Dong et al. (2019); Yang et al. (2019); Lewis et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>1 d.; e.g. Radford et al. (2019); Petroni et al. (2019); Brown et al. (2020); Raffel et al. (2020); Schick and Schütze (2021b); Gao et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>H 8 A b f 6 a e I f 6 F 9 4 Z 5 y C W k Q n J D l z 7 j 1 n 5 t 7 r J 4 H I p O O 8 F q y F x a X l l e J q a W 1 9 Y 3 O r v L 3 T z u I 8 Z b z F 4 i B O r 3 w v 4 4 G I e E s K G f C r J O V e 6 A e 8 4 4 / P V L x z y 9 N M x N G l n C S 8 F 3 q j S A w F 8 y R R F 5 P + c b 9 c c a q O X v Y 8 c A 2 o w K x m X H 7 B N Q a I w Z A j B E c E S T i A h 4 y e L l w 4 S I j r Y U p c S k j o O M c 9 S q T N K Y t T h k f s m L 4 j 2 n U N G 9 F e e W Z a z e i U g N 6 U l D Y O S B N T X k p Y n W b r e K 6 d F f u b 9 1 R 7 q r t N 6 O 8 b r 5 B</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>H 8 A b f 6 a e I f 6 F 9 4 Z 5 y C W k Q n J D l z 7 j 1 n 5 t 7 r J 4 H I p O O 8 F q y F x a X l l e J q a W 1 9 Y 3 O r v L 3 T z u I 8 Z b z F 4 i B O r 3 w v 4 4 G I e E s K G f C r J O V e 6 A e 8 4 4 / P V L x z y 9 N M x N G l n C S 8 F 3 q j S A w F 8 y R R F 5 P + c b 9 c c a q O X v Y 8 c A 2 o w K x m X H 7 B N Q a I w Z A j B E c E S T i A h 4 y e L l w 4 S I j r Y U p c S k j o O M c 9 S q T N K Y t T h k f s m L 4 j 2 n U N G 9 F e e W Z a z e i U g N 6 U l D Y O S B N T X k p Y n W b r e K 6 d F f u b 9 1 R 7 q r t N 6 O 8 b r 5 B</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>H 8 A b f 6 a e I f 6 F 9 4 Z 5 y C W k Q n J D l z 7 j 1 n 5 t 7 r J 4 H I p O O 8 F q y F x a X l l e J q a W 1 9 Y 3 O r v L 3 T z u I 8 Z b z F 4 i B O r 3 w v 4 4 G I e E s K G f C r J O V e 6 A e 8 4 4 / P V L x z y 9 N M x N G l n C S 8 F 3 q j S A w F 8 y R R F 5 P + c b 9 c c a q O X v Y 8 c A 2 o w K x m X H 7 B N Q a I w Z A j B E c E S T i A h 4 y e L l w 4 S I j r Y U p c S k j o O M c 9 S q T N K Y t T h k f s m L 4 j 2 n U N G 9 F e e W Z a z e i U g N 6 U l D Y O S B N T X k p Y n W b r e K 6 d F f u b 9 1 R 7 q r t N 6 O 8 b r 5 B</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>7 1 m R m + N d 3 5 I G 7 P 0 c 5 y L o 1 B 3 P d b z W S a 1 R L 0 Z d x g E O c U z z P E U D l 2 i i b b w f 8 Y R n 6 8 I K L W n l n 6 l W q d D s 4 9 u y H j 4 A t n 2 P I w = = &lt; / l a t e x i t &gt; (a) Left-to-right LM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>x 1 &lt;</head><label>1</label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = "</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>5 a z P r t U o W 7 v p b W D j b z b T s G b P 8 t w M 7 + a W N G D / 5 z j n Q b t W 9 b 2 q 3 z y u 1 G v 5 q I v Y w z 4 O a Z 4 n q O M C D b S s 9 y O e 8 O y c O 8 J R T v a Z 6 h R y z S 6 + L e f h A 7 Q d j y I = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S J a g h o q w</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>5 a z P r t U o W 7 v p b W D j b z b T s G b P 8 t w M 7 + a W N G D / 5 z j n Q b t W 9 b 2 q 3 z y u 1 G v 5 q I v Y w z 4 O a Z 4 n q O M C D b S s 9 y O e 8 O y c O 8 J R T v a Z 6 h R y z S 6 + L e f h A 7 Q d j y I = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S J a g h o q w</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>5 a z P r t U o W 7 v p b W D j b z b T s G b P 8 t w M 7 + a W N G D / 5 z j n Q b t W 9 b 2 q 3 z y u 1 G v 5 q I v Y w z 4 O a Z 4 n q O M C D b S s 9 y O e 8 O y c O 8 J R T v a Z 6 h R y z S 6 + L e f h A 7 Q d j y I = &lt; / l a t e x i t &gt; (b) Masked LM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>x 1 &lt;</head><label>1</label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = "</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>r m 9 p e q J D m k x C n c p 7 g g z L R y 3 m d b a z J d u + q t p + N v O l O x a s 9 M b o 5 3 d U s a s P t z n I u g V a u 6 T t W 9 O q n U a 2 b U R R z g E M c 0 z 1 P U c Y E G m u Q 9 w i O e 8 G x d W o k 1 s e 4 + U 6 2 C 0 e z j 2 7 I e P g B M i 5 A n &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T o R A e S A j J X f q b p 5 c p o 7 s M K O B c 9</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>r m 9 p e q J D m k x C n c p 7 g g z L R y 3 m d b a z J d u + q t p + N v O l O x a s 9 M b o 5 3 d U s a s P t z n I u g V a u 6 T t W 9 O q n U a 2 b U R R z g E M c 0 z 1 P U c Y E G m u Q 9 w i O e 8 G x d W o k 1 s e 4 + U 6 2 C 0 e z j 2 7 I e P g B M i 5 A n &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T o R A e S A j J X f q b p 5 c p o 7 s M K O B c 9</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>2 &lt;</head><label>2</label><figDesc>r m 9 p e q J D m k x C n c p 7 g g z L R y 3 m d b a z J d u + q t p + N v O l O x a s 9 M b o 5 3 d U s a s P t z n I u g V a u 6 T t W 9 O q n U a 2 b U R R z g E M c 0 z 1 P U c Y E G m u Q 9 w i O e 8 G x d W o k 1 s e 4 + U 6 2 C 0 e z j 2 7 I e P g B M i 5 A n &lt; / l a t e x i t &gt;x l a t e x i t s h a 1 _ b a s e 6 4 = " w +</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>7 1 m R m + N d 3 5 I G 7 P 0 c 5 y L o 1 B 3 P d b z W S a 1 R L 0 Z d x g E O c U z z P E U D l 2 i i b b w f 8 Y R n 6 8 I K L W n l n 6 l W q d D s 4 9 u y H j 4 A t n 2 P I w = = &lt; / l a t e x i t &gt; (c) Prefix LM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>x 1 &lt;</head><label>1</label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = "</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>s 4 9 u y H j 4 A t n 2 P I w = = &lt; / l a t e x i t &gt; (d) Encoder-Decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Typical paradigms of pre-trained LMs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>5. 2</head><label>2</label><figDesc>Answer Space Design Methods used in other tasks such as multiple-choice question answering (where the scores of multiple phrases are compared against each-other; Khashabi et al. (2020)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>et al. (2021) first construct a pruned search space Z by selecting top-k vocabulary words based on their generation probability at the [Z] position determined by training samples. Then the search space is further pruned down by only selecting a subset of words within Z based on their zero-shot accuracy on the training samples. (2) In the search step, they fine-tune the LM with fixed templates together with every answer mapping using training data and select the best label word as the answer based on the accuracy on the development set. Label Decomposition When performing relation extraction, Chen et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>[ X ]</head><label>X</label><figDesc>Mike is[MASK] entity type, New York is [MASK] entity type. PR Sub-PR2 [X] New York is [MASK] entity type.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>[ X ]</head><label>X</label><figDesc>Mike is[MASK] entity type, New York is [MASK] entity type. PR Sub-PR2 [X] New York is [MASK] entity type.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>[ X ]</head><label>X</label><figDesc>Mike is[MASK] entity type, New York is [MASK] entity type. PR Sub-PR2 [X] New York is [MASK] entity type.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>[ X ]</head><label>X</label><figDesc>Mike is[MASK] entity type, New York is [MASK] entity type. PR Sub-PR2 [X] New York is [MASK] entity type.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Different multi-prompt learning strategies. We use different colors to differentiate different components as follows. "</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>( 1 )</head><label>1</label><figDesc>Sample Selection: how to choose the most effective examples? (2) Sample Ordering: How to order the chosen examples with the prompt? Sample Selection Researchers have found that the choice of examples used in this few-shot scenario can result in very different performance, ranging from near state-of-the-art accuracy on some tasks to near random guess (Lu et al., 2021). To address this issue, Gao et al. (2021); Liu et al. (2021a) utilize sentence embeddings to sample examples that are close to the input in this embedding space. To measure the generalization capability of pre-trained LMs to perform new tasks based on instructions, Mishra et al. (2021) provide both positive samples and negative samples that highlight things to avoid. Sample Ordering Lu et al. (2021) found that the order of answered prompts provided to the model plays an important role in model performance, and propose entropy-based methods to score different candidate permutations. Kumar and Talukdar (2021) search for a good permutation of training examples as augmented prompts and learn a separator token between the prompts for further gains in performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>6. 4</head><label>4</label><figDesc>Prompt Decomposition (2021) first use multiple manually created sub-prompts for entity recognition and relation classification and then compose them into a complete prompt based on logic rules for relation extraction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Prompt</head><label></label><figDesc>[Question] [Passage] [Z] Prompt [Passage] According to the passage, [Question] [Z] Prompt Based on the following passage, [Question] [Z]. [Passage] Summarization Prompt Text: [X] Summary: [Z] BARTScore [193] Prompt [X] TL;DR: [Z] Prompt [X] In summary, [Z] Machine Translation Prompt French: [French sentence] English: Prompt A French sentence is provided: [French sentence] The French translator translates the sentence into English: [Z] Prompt [French sentence] = [Z]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Multi-prompt learning for multi-task, multi-domain or multi-lingual learning. We use different colors to differentiate different components as follows. "</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>[</head><label></label><figDesc>X] The [MASK] Google [MASK] the [MASK] Alphabet. 1 + 1 = 2 Ans-PR1 Add up two numbers: 6, 8 Input Mike is [MASK] entity type. Sub-PR1 Mike went to New York yesterday. Input (X) [X] Mike is [MASK] entity type, New York is [MASK] entity type. PR PR Sub-PR2 New York is [MASK] entity type. Really awesome movie! Movie Review (X1) It's very easy to use! Product Review (X2) [Domain_name]: This is [MASK]. Template Movie: [X1] This is [MASK]. PR1 Product: [X2] This is [MASK]. capital is [MASK]. PR1 [MASK] is the capital of China. PR2The capital of China is[MASK]. PR3 Subject: China; Relation: isCapital Input Google became a subsidiary of Alphabet. Input (X)[X] The [MASK] Google. Sub-PR1 [X] The [MASK] Alphabet. Sub-PR2 Sub-PR3 [X] Google [MASK] Alphabet. [X] The [MASK] Google [MASK] the [MASK] Alphabet.1 + 1 = 2 Ans-PR1 Add up two numbers: 6, 8 Input Mike is [MASK] entity type. Sub-PR1 Mike went to New York yesterday. Input (X) [X] Mike is [MASK] entity type, New York is [MASK] entity type. PR PR Sub-PR2 New York is [MASK] entity type. Really awesome movie! Movie Review (X1) It's very easy to use! Product Review (X2) [Domain_name]: This is [MASK]. Template Movie: [X1] This is [MASK]. PR1 Product: [X2] This is [MASK].Entanglement of Template and AnswerThe performance of a model will depend on both the templates being used and the answer being considered. How to simultaneously search or learn for the best combination of template and answer remains a challenging question. Current works typically select answers before select template(Gao et  al., 2021; Shin et al., 2020), but Hambardzumyan et al. (2021) have demonstrated the initial potential of simultaneously learning both.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head>10. 5</head><label>5</label><figDesc>Selection of Pre-trained Models 10.5 Selection of Pre-trained Models</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Meta-analyses over different dimensions. The statistics are based on the works in Tab. 7 and Tab. 8. In (d), we use the following abbreviations. TC: text classification, FP: factual probing, GCG: general conditional generation, QA: question answering, CR: commonsense reasoning, SUM: summarization, O: others.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><head>•</head><label></label><figDesc>Multilingual Replaced Token Detection (MRTD) (Chi et al., 2021b): Distinguish real input tokens from corrupted multilingual sentences by a Generative Adversarial Network, where both the generator and the discriminator are shared across languages. • Translation Replaced Token Detection (TRTD) (Chi et al., 2021b): Distinguish the real tokens and masked tokens in the translation pair by the Generative Adversarial Network. • Knowledge Embedding (KE) (Wang et al., 2021): Encode entities and relations in knowledge graphs (KGs) as distributed representations • Image-to-text transfer (ITT) (Wang et al., 2021): Is similar to the image caption that generates a corresponding description for the input image. • Multimodality-to-text transfer (MTT) (Wang et al., 2021): Generate the target text based on both the visual information and the noised linguistic information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-5.png" coords="1,130.31,500.43,340.51,255.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Terminology and notation of prompting methods. z * represents answers that correspond to true output y</figDesc><table /><note>* .2.2.1 Prompt AdditionIn this step a prompting function f prompt (•) is applied to modify the input text x into a prompt x = f prompt (x). In the majority of previous work(Kumar et  al., 2016; McCann et al., 2018; Radford et al., 2019; Schick and Schütze, 2021a), this function consists of a two step process:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Typical architectures for pre-trained LMs. x and y represent text to be encoded and decoded, respectively.</figDesc><table /><note>SLM: Standard language model. CTR: Corrupted text reconstruction. FTR: Full text reconstruction. †: Encoderdecoder architectures usually apply objective functions to the decoder only.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>1-2 (Dong et al., 2019; Bao et al., 2020) and ERNIE-M (Ouyang et al., 2020) while encoder-decoder models are widely used in pre-trained models such as T5 (Raffel et al., 2020), BART (Lewis et al., 2020a), MASS (Song et al., 2019) and their variants. Pre-trained models with prefix LMs and encoder-decoder paradigms can be naturally used to text generation tasks with (Dou et al., 2021) or without (Yuan et al., 2021a; Liu and Liu, 2021) prompting using input texts. However, recent studies reveal that other non-generation tasks, such as information extraction (Cui et al., 2021), question answering (Khashabi et al., 2020) , and text generation evaluation (Yuan et al.,2021b) can be reformulated a generation problems by providing appropriate prompts. Therefore, prompting methods (i) broaden the applicability of these generation-oriented pre-trained models. For example, pre-trained models like BART are less used in NER while prompting methods make BART applicable, and (ii) breaks the difficulty of unified modelling among different tasks<ref type="bibr" target="#b86">(Khashabi et al., 2020)</ref>.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>[MASK] entity type, New York is [MASK] entity type. Mike is [MASK] entity type, New York is [MASK] entity type. PR Sub-PR2 New York is [MASK] entity type. The [MASK] Google [MASK] the [MASK] Alphabet.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Movie Review (X1)</cell><cell>Product Review (X2)</cell></row><row><cell cols="2">Input</cell><cell>Subject: China; Relation: isCapital</cell><cell>Input</cell><cell cols="3">Add up two numbers: 6, 8</cell><cell>Really awesome movie!</cell><cell>It's very easy to use!</cell></row><row><cell>PR1</cell><cell></cell><cell>China's capital is [MASK].</cell><cell cols="2">Ans-PR1</cell><cell>1 + 1 = 2</cell><cell>Template</cell><cell>[Domain_name]: This is [MASK].</cell></row><row><cell>PR2</cell><cell></cell><cell>[MASK] is the capital of China.</cell><cell cols="3">Movie Review (X1) Ans-PR2 2 + 5 = 9</cell><cell>Product Review (X2)</cell><cell>PR1</cell><cell>Movie: [X1] This is [MASK].</cell></row><row><cell cols="7">Really awesome movie! Movie Review (X1) [Domain_name]: This is [MASK]. It's very easy to use! Product Review (X2) Template Movie: [X1] This is [MASK]. PR1 Product: [X2] This is [MASK]. PR2 " for input text, " Add up two numbers: 6, 8 1 + 1 = 2 ns-PR1 nput Really awesome movie! Movie Review (X1) It's very easy to use! Product Review (X2) [Domain_name]: This is [MASK]. Template Movie: [X1] This is [MASK]. PR1 Product: [X2] This is [MASK]. PR2 PR ns-PR2 2 + 5 = 9 6 + 8 = [MASK] " for prompt, " [MASK]. ital of China. na is [MASK]. elation: isCapital me a subsidiary of Alphabet. The [MASK] Google. 1 + 1 = 2 Ans-PR1 Add up two numbers: 6, 8 Input Mike went to New York yesterday. Input (X) [Domain_name]: This is [MASK]. Template Movie: [X1] This is [MASK]. PR1 Product: [X2] This is [MASK]. PR2 PR Ans-PR2 2 + 5 = 9 6 + 8 = [MASK] " for answered prompt. " Google became a subsidiary of Alphabet. Input (X) [X] The [MASK] Google. Sub-PR1 [X] The [MASK] Alphabet. Sub-PR2 Sub-PR3 [X] Google [MASK] Alphabet. Mike is [MASK] entity type. Sub-PR1 Mike went to New York yesterday. Input (X) [X] Mike is [MASK] entity type, New York is [MASK] entity type. PR PR Sub-PR2 New York is [MASK] entity type. [X] Really awesome movie! It's very easy to use! The capital of China is [MASK]. PR3 Product: [X2] This is [MASK]. PR2 PR 6 + 8 = [MASK]</cell></row><row><cell>The [MASK] Alphabet.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Input (X) Google [MASK] Alphabet.</cell><cell cols="2">Mike went to New York yesterday. Sub-PR1</cell><cell cols="2">Mike is [MASK] entity type.</cell><cell></cell></row><row><cell cols="3">[X] Mike is [MASK] entity type, New York is [MASK] entity type. e [MASK] the [MASK] Alphabet. PR</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sub-PR1</cell><cell></cell><cell>Mike is [MASK] entity type.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">New York is [MASK] entity type. Sub-PR2 New York is [MASK] entity type.</cell><cell></cell><cell></cell><cell></cell></row></table><note>[X]</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>8.1 Knowledge ProbingFactual Probing Factual probing (a.k.a. fact retrieval) is one of the earliest scenarios with respect to which prompting methods were applied. The motivation of exploring this task is to quantify how much factual knowledge the pre-trained LM's internal representations bear. In this task, parameters of pre-trained models are usually fixed, and knowledge is retrieved by transforming the original input into a cloze prompt as defined in §2.2, which can be manually crafted or automatically discovered. Relevant datasets including LAMA (Petroni et al., 2019) and X-FACTR<ref type="bibr" target="#b76">(Jiang et al., 2020a)</ref>. Since the answers are pre-defined, fact retrieval only focuses on finding effective templates and analyzing the results of different models using these templates. Both discrete template search (Petroni et al.Text ClassificationFor text classification tasks, most previous work has used cloze prompts, and both prompt engineering (Gao et al., 2021; Hambardzumyan et al., 2021; Lester et al., 2021) and answer engineering (Schick and Schütze, 2021a; Schick et al., 2020; Gao et al., 2021) have been explored extensively.Most existing works explore the efficacy of prompt learning for text classification in the context of few-shot setting with "fixed-prompt LM Tuning" strategies (defined in §7.2.4). Language Inference (NLI) NLI aims to predict the relationship (e.g., entailment) of two given sentences. Similar to text classification tasks, for natural language inference tasks, cloze prompts are commonly used (Schick and Schütze, 2021a). Regarding prompt engineering, researchers mainly focus on the template search in the few-shot learning setting and the answer space Z is usually manually pre-selected from the vocabulary.Question answering (QA) aims to answer a given input question, often based on a context document. QA can take a variety of formats, such as extractive QA (which identifies content from the context document containing the answer; e.g. SQuAD<ref type="bibr" target="#b153">(Rajpurkar et al., 2016)</ref>), multiple-choice QA (where the model has to pick from several choices; e.g. RACE<ref type="bibr" target="#b95">(Lai et al., 2017)</ref>), and free-form QA (where the model can return an arbitrary textual string as a response; e.g. NarrativeQA<ref type="bibr" target="#b89">(Kočiský et al., 2018)</ref>). Generally, these different formats have been handled using different modeling frameworks. One benefit of solving QA problems with LMs, potentially using prompting methods, is that different formats of QA tasks can be solved within the same framework. For example, Khashabi et al. (2020) reformulate many QA tasks as a text generation problem by fine-tuning seq2seq-based pre-trained models (e.g. T5) and appropriate prompts from the context and questions.Jiang et al. (</figDesc><table><row><cell>Natural 8.5 Question Answering</cell></row></table><note>, 2019, 2020; Jiang et al., 2020c,a; Haviv et al., 2021; Shin et al., 2020; Perez et al., 2021) and continuous template learning (Qin and Eisner, 2021; Liu et al., 2021b; Zhong et al., 2021b) have been explored within this context, as well as prompt ensemble learning (Jiang et al., 2020c; Qin and Eisner, 2021). Linguistic Probing Besides factual knowledge, large-scale pre-training also allows LMs to handle linguistic phenomena such as analogies (Brown et al., 2020), negations (Ettinger, 2020), semantic role sensitivity (Ettinger, 2020), semantic similarity (Sun et al., 2021), cant understanding (Sun et al., 2021), and rare word understanding (Schick and Schütze, 2020).The above knowledge can also be elicited by presenting linguistic probing tasks in the form of natural language sentences that are to be completed by the LM.8.2 Classification-based TasksPrompt-based learning has been widely explored in classification-based tasks where prompt templates can be constructed relatively easily, such as text classification<ref type="bibr" target="#b200">(Yin et al., 2019)</ref> and natural language inference (Schick and Schütze, 2021a). The key to prompting for classification-based tasks is reformulating it as an appropriate prompt. For example, Yin et al. (2019) use a prompt such as "the topic of this document is [Z].", which is then fed into mask pre-trained LMs for slot filling.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc><ref type="bibr" target="#b150">Radford et al. (2019)</ref> demonstrated impressive ability of such models to perform generation tasks such as text summarization and machine translation using prompts such as "translate to french, [X], [Z]". Brown et al. (2020) perform in-context learning ( §7.2.2) for text generation, creating a prompt with manual templates and augmenting the input with multiple answered prompts. Schick and Schütze (2020) explore fixed-prompt LM tuning ( §7.2.4) for few-shot text summarization with manually crafted templates. (Li and Liang, 2021) investigate fixed-LM prompt tuning ( §7.2.3) for text summarization and data-to-text generation in few-shot settings, where learnable prefix tokens are prepended to the input while parameters in pre-trained models are kept frozen.<ref type="bibr" target="#b46">Dou et al. (2021)</ref> explored the prompt+LM tuning strategy ( §7.2.5) on text summarization task, where learnable prefix prompts are used and initialized by different types of guidance signals, which can then be updated together with parameters of pre-trained LMs.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Prompt-Neg [X] [Span] is not a named entity. Answer person, location, organization, miscellaneous Prompt-Pos The entity type of Span is [Z]. Prompt-Neg [X] The entity type of [Span] is none entity.</figDesc><table><row><cell>Task</cell><cell>Example Prompt-Answer</cell><cell>Resource</cell></row><row><cell></cell><cell>Prompt Adolphe Adam died in [Z].</cell><cell>LAMA dataset</cell></row><row><cell></cell><cell>Answer V</cell><cell>LPAQA dataset</cell></row><row><cell>Fact Probing</cell><cell>Prompt iPod Touch is produced by [Z]. Answer V</cell><cell>X-FACTR dataset</cell></row><row><cell></cell><cell>Prompt The official language of Mauritius is [Z].</cell><cell></cell></row><row><cell></cell><cell>Answer V</cell><cell></cell></row><row><cell></cell><cell>Prompt Which of these choices best describes the following</cell><cell>Meta [202]</cell></row><row><cell></cell><cell>document? "[Class A]", "[Class B]", "[Class C]".</cell><cell></cell></row><row><cell></cell><cell>[X][Z]</cell><cell></cell></row><row><cell></cell><cell>Answer [Class A], [Class B], [Class C]</cell><cell></cell></row><row><cell></cell><cell>Prompt How is the text best described? : "[Class A]",</cell><cell></cell></row><row><cell></cell><cell>"[Class B]" , or "[Class C]". [X][Z]</cell><cell></cell></row><row><cell>Text Classificatin</cell><cell>Answer [Class A], [Class B], [Class C]</cell><cell></cell></row><row><cell></cell><cell>Prompt This passage is about [Z]: [X]</cell><cell></cell></row><row><cell></cell><cell>Answer [Class A], [Class B], [Class C]</cell><cell></cell></row><row><cell></cell><cell>Prompt [X]. Is this review positive? [Z]</cell><cell></cell></row><row><cell></cell><cell>Answer Yes, No</cell><cell></cell></row><row><cell></cell><cell>Prompt [X] It was [Z].</cell><cell></cell></row><row><cell></cell><cell>Answer great, terrible</cell><cell></cell></row><row><cell></cell><cell>Prompt [X1]? [Z], [X2]</cell><cell></cell></row><row><cell>Natural Language Inference</cell><cell>Answer Yes, No, Maybe Prompt [X1] [Z], [X2]</cell><cell></cell></row><row><cell></cell><cell>Answer Yes, No, Maybe</cell><cell></cell></row><row><cell></cell><cell>Prompt The trophy doesn't fit into the brown suitcase</cell><cell>PDP dataset</cell></row><row><cell></cell><cell>because [Z] is too large.</cell><cell>WSC dataset</cell></row><row><cell>Commonsense Reasoning</cell><cell>Answer trophy, suitcase Prompt Ann asked Mary what time the library closes,</cell><cell>CPRAG-102 dataset</cell></row><row><cell></cell><cell>because [Z] had forgotten.</cell><cell></cell></row><row><cell></cell><cell>Answer Ann, Mary</cell><cell></cell></row><row><cell></cell><cell>Prompt A robin is a [Z].</cell><cell>WNLaMPro dataset</cell></row><row><cell></cell><cell>Answer bird, tree</cell><cell>ROLE-88 dataset</cell></row><row><cell>Linguistic Knowledge Probing</cell><cell>Prompt A robin is not a [Z]. Answer bird, tree</cell><cell>NEG-136 dataset</cell></row><row><cell></cell><cell>Prompt New is the opposite of [Z].</cell><cell></cell></row><row><cell></cell><cell>Answer old, young, current</cell><cell></cell></row><row><cell></cell><cell>Prompt-Pos [X] [Span] is a [Z] entity.</cell><cell>TemplateNER [29]</cell></row><row><cell>Named Entity Recognition</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Answer person, location, organization, miscellaneous</cell><cell></cell></row><row><cell>Question Answering</cell><cell></cell><cell></cell></row><row><cell cols="3">Domain adaptation is the practice of adapting a model from one domain (e.g. news text)</cell></row><row><cell cols="3">to another (e.g. social media text). Ben-David et al. (2021) use self-generated domain related features (DRFs) to</cell></row><row><cell cols="3">augment the original text input and perform sequence tagging as a sequence-to-sequence problem using a seq2seq</cell></row><row><cell>pre-trained model.</cell><cell></cell><cell></cell></row><row><cell cols="3">Debiasing Schick et al. (2021) found that LMs can perform self-diagnosis and self-debiasing based on biased</cell></row><row><cell cols="3">or debiased instructions. For example, to self-diagnosis whether the generated text contains violent information,</cell></row></table><note>we can use the following template "The following text contains violence. [X][Z]". Then we fill [X] with the input text and look at the generation probability at [Z], if the probability of "Yes" is greater than "No", then we would assume the given text contains violence, and vice versa. To perform debiasing when generating text, we first compute the probability of the next word P (x t |x &lt;t ; θ) given the original input. Then we compute the probability</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Commonly used prompts and answers for different tasks. [X] and [Z] denote slots for input and answer respectively. V denotes the vocabulary of the LM. More prompts for each task can be found using the Resource column.</figDesc><table><row><cell cols="2">Prompt Concept</cell><cell>Relevant Topic</cell><cell>Commonality</cell><cell>Peculiarity</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>In prompt ensembling, multiple predic-</cell></row><row><cell cols="2">Prompt Ensembling [68; 153]</cell><cell>Ensemble Learning [171; 204]</cell><cell>Combine results of multiple sys-tems to get better performance</cell><cell>tions result from different prompt vari-ants. This contrasts with architecture or feature variations, each of which re-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>quires separate training.</cell></row><row><cell></cell><cell></cell><cell>Few-shot Learning</cell><cell>Use few examples to learn gen-</cell><cell>Prompt augmentation is a specific subset</cell></row><row><cell></cell><cell></cell><cell>[160; 42]</cell><cell>eralized rules</cell><cell>of few-shot learning.</cell></row><row><cell cols="2">Prompt Augmentation [16; 46]</cell><cell>Larger-context Learning [18; 53]</cell><cell>Introduce larger context to aid the learning process</cell><cell>Additional information introduced in larger-context learning is not necessarily</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>the labeled data.</cell></row><row><cell cols="2">Discrete Prompt Search</cell><cell>Query reformula-</cell><cell>Reformulate the input into a</cell><cell>Query reformulation commonly focuses</cell></row><row><cell>[68; 159]</cell><cell></cell><cell>tion [123; 123]</cell><cell>query form</cell><cell>on information extraction and question</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>answering tasks, while prompt learning</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>can be applied to a variety of NLP tasks</cell></row><row><cell cols="2">Discrete Prompt Fine-</cell><cell>QA-based multi-</cell><cell>Reformulate many tasks into an</cell><cell>QA-based formulations aim to solve dif-</cell></row><row><cell>tuning [46]</cell><cell></cell><cell>task learning [115;</cell><cell>QA form</cell><cell>ferent tasks through question answering,</cell></row><row><cell></cell><cell></cell><cell>97]</cell><cell></cell><cell>while prompting additionally targets full</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>use of pre-trained models.</cell></row><row><cell>Continuous</cell><cell>Prompt</cell><cell></cell><cell></cell></row><row><cell cols="2">Fine-tuning [103; 36]</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Other research topics relevant to prompting methods.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>10.2 Answer Engineering It's very easy to use! Product Review (X2) [Domain_name]: This is [MASK]. Template Movie: [X1] This is [MASK]. PR1 Product: [X2] This is [MASK]. Domain_name]: This is [MASK]. Template Movie: [X1] This is [MASK]. Prompt 1 Product: [X2] This is [MASK].</figDesc><table><row><cell></cell><cell>PR2</cell></row><row><cell>Movie Review (X1)</cell><cell>Really awesome movie!</cell></row><row><cell></cell><cell>Prompt 2</cell></row><row><cell>Product Review (X2)</cell><cell>It's very easy to use!</cell></row></table><note>[</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Even during this stage, there was some use of pre-trained models exemplified by word2vec(Mikolov et al.,  </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2013" xml:id="foot_1">2013b,a) and GloVe (Pennington et</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2014" xml:id="foot_2">al., 2014), but they were used for only a limited portion of the final model parameters.<ref type="bibr" target="#b12">2</ref> This paradigm is less conducive to architectural exploration because (i) unsupervised pre-training allows models to learn with fewer structural priors, and (ii) as pre-training of models is time-consuming, experimenting with structural variants is costly.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3">Similarly, a right-to-left LM can predict preceding words based on the future context, such as P (xi|xi+1, • • • , xn).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4">The number of template tokens do not need to be pre-specified since T5 can decode multiple tokens at a masked position.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5">Multiple continuous prompts are typically learned by using different initializations or different random seeds.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_6">e.g. https://medium.com/reconstruct-inc/the-golden-age-of-computer-vision-338da3e471d1</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Chunting Zhou for her constructive comments on this work.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Information Extraction</head><p>Unlike classification tasks where cloze questions can often be intuitively constructed, for information extraction tasks constructing prompts often requires more finesse.</p><p>Relation Extraction Relation extraction is a task of predicting the relation between two entities in a sentence. , which require the model to identify the antecedent of an ambiguous pronoun within context, or involve completing a sentence given multiple choices. For the former, an example could be "The trophy doesn't fit into the brown suitcase because it is too large." And the task for the model is to infer whether "it" refers to the trophy or the "suitcase". By replacing "it" with its potential candidates in the original sentences and calculating the probability of the different choices, pre-trained LMs can perform quite well by choosing the choice that achieves the highest probability <ref type="bibr" target="#b0">(Trinh and Le, 2018)</ref>. For the latter, an example could be "Eleanor offered to fix her visitor some coffee. Then she realized she didn't have a clean [Z].". The candidate choices are "cup", "bowl" and "spoon". The task for the pre-trained LM is to choose the one from the three candidates that most conforms to common sense. For these kinds of tasks, we can also score the generation probability of each candidate and choose the one with the highest probability <ref type="bibr" target="#b49">(Ettinger, 2020)</ref>.</p><p>Mathematical Reasoning Mathematical reasoning is the ability to solve mathematical problems, e.g. arithmetic addition, function evaluation. Within the context of pre-trained LMs, researchers have found that pre-trained embeddings and LMs can perform simple operations such as addition and subtraction when the number of digits is small, but fail when the numbers are larger <ref type="bibr">(Naik et</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.10">Resources</head><p>We also collect some useful resources for different prompt-based applications.</p><p>Dataset Some datasets specifically designed for few-shot and zero-shot learning are shown in Tab. 9.  Prompts As shown in Tab. 10, we collect existing commonly-used prompts designed manually, which can be regarded as off-the-shelf resource for future research and applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Prompt-relevant Topics</head><p>What is the essence of prompt-based learning and how does it relate to other learning methods? In this section, we connect prompt learning with other similar learning methods.</p><p>Ensemble Learning Ensemble learning (Ting and Witten, 1997; <ref type="bibr" target="#b214">Zhou et al., 2002)</ref> is a technique that aims to improve the performance of a task by taking advantage of the complementarity of multiple systems. Generally, the different systems used in an ensemble result from different choices of architectures, training strategies, data ordering, and/or random initialization. In prompt ensembling ( §6.1), the choice of prompt templates becomes another way to generate multiple results to be combined. This has the clear advantage that this does not necessarily require training the model multiple times. For example, when using discrete prompts, these prompts can simply be changed during the inference stage <ref type="bibr" target="#b78">(Jiang et al., 2020c)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Few-shot Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix on Pre-trained LMs</head><p>In this appendix we present some auxiliary information on pre-trained LMs that may be useful to the readers to better understand the current lay of the land with respect to this dynamic research area.</p><p>A.1 Evolution of Pre-trained LM Parameters </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Auxiliary Objective</head><p>In this subsection, more auxiliary objectives for pre-training language models have been listed. A.   <ref type="bibr" target="#b204">[194]</ref> L2R SLM -----NLG ELECTRA <ref type="bibr" target="#b36">[26]</ref> Mask CTR RTD Tok Tok --NLU,NLG MASS <ref type="bibr" target="#b172">[162]</ref> En-De CTR -Span ---NLG PEGASUS <ref type="bibr" target="#b205">[195]</ref> En-De CTR -Tok, Sent ---Summarization M6 <ref type="bibr" target="#b189">[179]</ref> En-De CTR ITT,MTT Span ---NLG Table <ref type="table">13</ref>: A detailed illustration of different pre-trained models characterized by the four aspects. "Parallel" represents if parallel data have been used for pre-training. Sci, Bio, Fin, K represent scientific, biomedical, financial, and knowledge, respectively. Tok, Sent, Doc denote token, sentence and document, respectively. Region, Frame denote basic units of images and video respectively.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TABLE 12 Timeline of prompt-based learning. The time for each paper is based on its first arXiv version (if exists) or estimated submission time. A web-version can refer to NLPedia-Pretrain. Works in red consider natural language understanding (NLU) tasks; works in blue consider natural language generation (NLG) tasks; works in green consider both NLU tasks and NLG tasks</title>
		<author>
			<persName><forename type="first">Le</forename><forename type="middle">;</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName><surname>Radford</surname></persName>
		</author>
		<idno>2021) 2019.02.14 • GPT-2</idno>
	</analytic>
	<monogr>
		<title level="j">Soft (Qin and Eisner</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
			<date type="published" when="2018">2018. 2018. 2019</date>
		</imprint>
	</monogr>
	<note>04.15 • DINO (Schick and Schütze, 2021) 2019.04</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">LMDiagnose (Ettinger, 2020) 2021.04</title>
		<author>
			<persName><forename type="first">Schütze</forename><forename type="middle">;</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<idno>2021.04.15</idno>
	</analytic>
	<monogr>
		<title level="m">AdaPrompt</title>
				<imprint>
			<date type="published" when="2019-07">2020. 2021b. 2019.07. 2021. 2019.08</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<author>
			<persName><forename type="first">Wallace</forename><surname>Advtrigger</surname></persName>
		</author>
		<idno>2021.04</idno>
	</analytic>
	<monogr>
		<title level="m">Prompt-Tuning</title>
				<imprint>
			<date type="published" when="2011">2019a) 2021.04. 2019. 2019. 2021. 2019.09. 2019) 2021.04. 2021. 2019. 2020. 2021. 2019.11</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="2021" to="2024" />
		</imprint>
	</monogr>
	<note>Natural-Instr</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Misprim</forename><forename type="middle">(</forename><surname>Neg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Schütze ; Truefewshot (</forename><surname>Kassner</surname></persName>
		</author>
		<author>
			<persName><surname>Perez</surname></persName>
		</author>
		<idno>2021.06.03</idno>
	</analytic>
	<monogr>
		<title level="j">TemplateNER</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2019">2020. 2021. 2019. 2020c) 2021.05. 2021. 2019. 2019. 2021</date>
			<publisher>Puri and Catanzaro</publisher>
		</imprint>
	</monogr>
	<note>LPAQA</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">• Pet-Tc (</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Schütze</forename><forename type="middle">;</forename><surname>Petroni</surname></persName>
		</author>
		<idno>09.08</idno>
	</analytic>
	<monogr>
		<title level="j">NullPrompt</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">2021</biblScope>
			<date type="published" when="2020-05">2020) 2021.06. 2021. 2020. 2020. 2021. 2020.05. 2020b) 2021.06. 2021b) 2020.05. 2020) 2021.06. 2021. 2020. CommS2S. 2020</date>
		</imprint>
	</monogr>
	<note>BARTScore. 06.25 • Frozen (Tsimpoukelli et al., 2021) 2020.09</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">• Pet-Sglue (</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Schütze</surname></persName>
		</author>
		<idno>2021.07.05 • ERNIE-B3</idno>
		<imprint>
			<date type="published" when="2009">2021b. 2021. 2020.09</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<author>
			<persName><surname>Toxicityprompts (gehman</surname></persName>
		</author>
		<idno>2021) 2021.07.14</idno>
	</analytic>
	<monogr>
		<title level="m">Codex</title>
				<imprint>
			<date type="published" when="2020">2020. 2021a. 2021</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">• X-Factr (</forename><surname>Jiang</surname></persName>
			<affiliation>
				<orgName type="collaboration">CTRLsum</orgName>
			</affiliation>
		</author>
		<idno>12.08</idno>
	</analytic>
	<monogr>
		<title level="j">AutoPrompt</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2020">2020a) 2021.07. 2021. 2020. 2020. 2020. 2020. 2020. 2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<author>
			<persName><forename type="first">•</forename><surname>Pet-Gen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">(</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Schütze ; Lm-Bff (</forename><surname>Gao</surname></persName>
		</author>
		<idno>2021) 2021.01.01</idno>
	</analytic>
	<monogr>
		<title level="m">Prefix-Tuning</title>
				<imprint>
			<publisher>PromptProg (Reynolds and McDonell</publisher>
			<date type="published" when="2020">2020. 2021. Li and Liang, 2021) 2021.01. 2021a) 2021.02. 2021</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">(</forename><surname>Contxcalibrate</surname></persName>
		</author>
		<author>
			<persName><surname>Zhao</surname></persName>
		</author>
		<idno>2021) 2021.03.09</idno>
		<imprint>
			<date type="published" when="2021-02">2021) 2021.02. 2021. 2021.03. 2021</date>
			<publisher>Scao and Rush</publisher>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<author>
			<persName><forename type="first">• P-Tuning (</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GLM</title>
				<imprint>
			<date type="published" when="2021-03">2021b) 2021.03. 2021) 2021.03. 2021. 2021a) 2021.04. 2021b</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">References</biblScope>
		</imprint>
	</monogr>
	<note>Meta</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmytro</forename><surname>Okhonko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gargi</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.06955</idno>
		<title level="m">Htlm: Hyper-text pre-training and prompting of language models</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Towards understanding ensemble, knowledge distillation and selfdistillation in deep learning</title>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Zhu</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<idno>CoRR, abs/2012.09816</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maxinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tegan</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asja</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="233" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Beit: Bert pre-training of image transformers</title>
		<author>
			<persName><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unilmv2: Pseudo-masked language models for unified language model pre-training</title>
		<author>
			<persName><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songhao</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML 2020</title>
				<meeting>the 37th International Conference on Machine Learning, ICML 2020</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-07-18">2020. 13-18 July 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="642" to="652" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SciBERT: A pretrained language model for scientific text</title>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1371</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3615" to="3620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Pada: A prompt-based autoregressive approach for adaptation to unseen domains</title>
		<author>
			<persName><forename type="first">Eyal</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Oved</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semantic parsing via paraphrasing</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P14-1133</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1415" to="1425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning feedforward one-shot learners</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>João</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="523" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">GPT-Neo: Large scale autoregressive language modeling with mesh-tensorflow</title>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1075</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">FLEX: unifying evaluation for few-shot NLP</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Bragg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<idno>CoRR, abs/2107.07170</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Ask the right questions: Active question reformulation with reinforcement learning</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jannis</forename><surname>Bulian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Gajewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07830</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Retrieve, rerank and rewrite: Soft template based neural summarization</title>
		<author>
			<persName><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1015</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="152" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Description based text classification with reinforcement learning</title>
		<author>
			<persName><forename type="first">Duo</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1371" to="1382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiming</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrique</forename><surname>Ponde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yura</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Brockman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03374</idno>
		<title level="m">Evaluating large language models trained on code</title>
				<imprint>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Adaprompt: Adaptive prompt-based finetuning for relation extraction</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahuan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno>CoRR, abs/2104.07650</idno>
		<imprint>
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">mt6: Multilingual pretrained text-to-text transformer with translation pairs</title>
		<author>
			<persName><forename type="first">Zewen</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian-Ling</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heyan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno>CoRR, abs/2104.08692</idno>
		<imprint>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">XLM-E: cross-lingual language model pre-training via ELECTRA</title>
		<author>
			<persName><forename type="first">Zewen</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saksham</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Payal</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno>CoRR, abs/2106.16138</idno>
		<imprint>
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An empirical comparison of domain adaptation methods for neural machine translation</title>
		<author>
			<persName><forename type="first">Chenhui</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raj</forename><surname>Dabre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-2061</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="385" to="391" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>¸aglar Gülc ¸ehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>ArXiv, abs/1412.3555</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">ELECTRA: pre-training text encoders as discriminators rather than generators</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
				<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26">2020. April 26-30, 2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ronan Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Michael Karlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.747</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07-05">2020. July 5-10, 2020</date>
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Template-based named entity recognition using bart</title>
		<author>
			<persName><forename type="first">Leyang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Web search intent induction via automatic query reformulation</title>
		<author>
			<persName><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Brill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL 2004: Short Papers</title>
				<meeting>HLT-NAACL 2004: Short Papers<address><addrLine>Boston, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="49" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Commonsense knowledge mining from pretrained models</title>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1109</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11-03">2019. November 3-7, 2019</date>
			<biblScope unit="page" from="1173" to="1178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Sumanth</forename><surname>Doddapaneni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gowtham</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anoop</forename><surname>Kunchukuttan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratyush</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitesh</forename><forename type="middle">M</forename><surname>Khapra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00676</idno>
		<title level="m">A primer on pretrained multilingual language models</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06305</idno>
		<title level="m">Finetuning pretrained language models: Weight initializations, data orders, and early stopping</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. 2019. December 8-14, 2019</date>
			<biblScope unit="page" from="13042" to="13054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">GSum: A general framework for guided neural abstractive summarization</title>
		<author>
			<persName><forename type="first">Zi-Yi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.384</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter</title>
				<meeting>the 2021 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4830" to="4842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">All nlp tasks are generation tasks: A general pretraining framework</title>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Generalized minimum bayes risk system combination</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katsuhito</forename><surname>Sudoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianchao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hajime</forename><surname>Tsukada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 5th International Joint Conference on Natural Language Processing</title>
				<meeting>5th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1356" to="1360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models</title>
		<author>
			<persName><forename type="first">Allyson</forename><surname>Ettinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="34" to="48" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Data augmentation for low-resource neural machine translation</title>
		<author>
			<persName><forename type="first">Marzieh</forename><surname>Fadaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arianna</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-2090</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="567" to="573" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Controllable abstractive summarization</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</title>
				<meeting>the 2nd Workshop on Neural Machine Translation and Generation<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="45" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017-08-11">2017a. 2017. 6-11 August 2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017b</date>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Spanner: Named entity re-/recognition as span prediction</title>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.00641</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Vqs: Linking segmentations to questions and answers for supervised attention in vqa and question-focused semantic segmentation</title>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yandong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoxiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1811" to="1820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Making pre-trained language models better few-shot learners</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Realtoxicityprompts: Evaluating neural toxic degeneration in language models</title>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Samuel Gehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3356" to="3369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Measurement, regression, and calibration</title>
		<author>
			<persName><forename type="first">Leon</forename><surname>Jay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gleser</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A bit of progress in language modeling</title>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">T</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="403" to="434" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">QuickEdit: Editing text &amp; translations by crossing words out</title>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1025</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="272" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdel-Rahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2013.6638947</idno>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Generating sentences by editing prototypes</title>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tatsunori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Oren</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00030</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="437" to="450" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zora</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08909</idno>
		<title level="m">Realm: Retrievalaugmented language model pre-training</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Gene selection for cancer classification using support vector machines</title>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Barnhill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="389" to="422" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Warp: Word-level adversarial reprogramming</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Hambardzumyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrant</forename><surname>Khachatrian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename></persName>
		</author>
		<idno>ArXiv, abs/2101.00121</idno>
		<imprint>
			<date type="published" when="2021-05">May. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Ptr: Prompt tuning with rules for text classification</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Identifying web search query reformulation using concept based matching</title>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Hassan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1000" to="1010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">BERTese: Learning to speak to BERT</title>
		<author>
			<persName><forename type="first">Adi</forename><surname>Haviv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
				<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3618" to="3623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Ctrlsum: Towards generic controllable text summarization</title>
		<author>
			<persName><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Kryscinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<idno>CoRR, abs/2012.04281</idno>
		<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
	<note>Nazneen Fatema Rajani, and Caiming Xiong</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Deberta: Decoding-enhanced bert with disentangled attention</title>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03654</idno>
		<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Surface form competition: Why the highest probability answer isn&apos;t always right</title>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vered</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="328" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Cosmos QA: Machine reading comprehension with contextual commonsense reasoning</title>
		<author>
			<persName><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1243</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2391" to="2401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Salicon: Saliency in context</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengsheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanyong</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1072" to="1080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">X-FACTR: Multilingual factual knowledge retrieval from pretrained language models</title>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibo</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.479</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020a</date>
			<biblScope unit="page" from="5943" to="5959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Haibo Ding, and Graham Neubig. 2020b. How can we know when language models know?</title>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Araki</surname></persName>
		</author>
		<idno>CoRR, abs/2012.00955</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">How can we know what language models know? Transactions of the</title>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">F</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00324</idno>
		<imprint>
			<date type="published" when="2020-06">Jun Araki, and Graham Neubig. 2020c</date>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="423" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Tinybert: Distilling BERT for natural language understanding</title>
		<author>
			<persName><forename type="first">Xiaoqi</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linlin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.372</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11-20">2020. 16-20 November 2020</date>
			<biblScope unit="page" from="4163" to="4174" />
		</imprint>
	</monogr>
	<note>EMNLP 2020 of Findings of ACL</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">SpanBERT: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00300</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Speech and language processing: An introduction to natural language processing, computational linguistics, and speech recognition</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">H</forename><surname>Martin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ofir</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03129</idno>
		<title level="m">Learning to remember rare events</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P14-1062</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="655" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly</title>
		<author>
			<persName><forename type="first">Nora</forename><surname>Kassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.698</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07-05">2020. July 5-10, 2020</date>
			<biblScope unit="page" from="7811" to="7818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">CTRL: A conditional transformer language model for controllable generation</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lav</forename><forename type="middle">R</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>CoRR, abs/1909.05858</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">UNIFIEDQA: Crossing format boundaries with a single QA system</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.171</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
				<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1896" to="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Controlling output length in neural encoder-decoders</title>
		<author>
			<persName><forename type="first">Yuta</forename><surname>Kikuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryohei</forename><surname>Sasano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1140</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1328" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">The NarrativeQA reading comprehension challenge</title>
		<author>
			<persName><forename type="first">Tomáš</forename><surname>Kočiský</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gábor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00023</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="317" to="328" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Statistical machine translation</title>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Supervised machine learning: A review of classification techniques</title>
		<author>
			<persName><surname>Sotiris B Kotsiantis</surname></persName>
		</author>
		<author>
			<persName><surname>Zaharakis</surname></persName>
		</author>
		<author>
			<persName><surname>Pintelas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Emerging artificial intelligence applications in computer engineering</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="24" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Ask me anything: Dynamic memory networks for natural language processing</title>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1378" to="1387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Reordering examples helps during priming-based few-shot learning</title>
		<author>
			<persName><forename type="first">Sawan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">RACE: Large-scale ReAding comprehension dataset from examinations</title>
		<author>
			<persName><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1082</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07291</idno>
		<title level="m">Cross-lingual language model pretraining</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">ALBERT: A lite BERT for self-supervised learning of language representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
				<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26">2020. April 26-30, 2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">How many data points is a prompt worth?</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Teven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.208</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2627" to="2636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">BioBERT: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wonjin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chan</forename><surname>Ho So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btz682</idno>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">End-to-end neural coreference resolution</title>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1018</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="188" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">The winograd schema challenge</title>
		<author>
			<persName><forename type="first">Hector</forename><surname>Levesque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ernest</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leora</forename><surname>Morgenstern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">The winograd schema challenge</title>
		<author>
			<persName><forename type="first">Hector</forename><forename type="middle">J</forename><surname>Levesque</surname></persName>
		</author>
		<idno>SS-11-06</idno>
	</analytic>
	<monogr>
		<title level="m">Logical Formalizations of Commonsense Reasoning, Papers from the 2011 AAAI Spring Symposium</title>
				<meeting><address><addrLine>Stanford, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI</publisher>
			<date type="published" when="2011-03-21">2011. March 21-23, 2011</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020a</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Retrieval-augmented generation for knowledge-intensive NLP tasks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Küttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020b. 2020. 2020. December 6-12, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<title level="m" type="main">Prefix-tuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00190</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">A unified MRC framework for named entity recognition</title>
		<author>
			<persName><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingrong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.519</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5849" to="5859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">CommonGen: A constrained text generation challenge for generative commonsense reasoning</title>
		<author>
			<persName><forename type="first">Wangchunshu</forename><surname>Bill Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.165</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
				<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1823" to="1840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Attention correctness in neural image captioning</title>
		<author>
			<persName><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<title level="m" type="main">What makes good in-context examples for</title>
		<author>
			<persName><forename type="first">Jiachang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Neural machine translation with supervised attention</title>
		<author>
			<persName><forename type="first">Lemao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Finch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
				<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3093" to="3102" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Exploiting argument information to improve event detection via supervised attention mechanisms</title>
		<author>
			<persName><forename type="first">Shulin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017b</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1789" to="1798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>2021b. GPT understands, too. CoRR, abs/2103.10385</note>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Multilingual denoising pre-training for neural machine translation</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="726" to="742" />
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized bert pretraining approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>ArXiv, abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">RefSum: Refactoring neural summarization</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi-Yi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.113</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021c</date>
			<biblScope unit="page" from="1437" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<monogr>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.01890</idno>
		<title level="m">Simcls: A simple framework for contrastive learning of abstractive summarization</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Finbert: A pre-trained financial language representation model for financial text mining</title>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Degen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2020/622</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</title>
				<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">Jun Zhao. 2020b</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="4513" to="4519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<monogr>
		<title level="m" type="main">Cutting down on prompts and parameters: Simple few-shot learning with language models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivana</forename><surname>Balažević</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. 2019. December 8-14, 2019</date>
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<monogr>
		<title level="m" type="main">Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Bartolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<idno>ArXiv, abs/2104.08786</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF</title>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1101</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1064" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">An example of statistical investigation of the text eugene onegin concerning the connection of samples in chains</title>
		<author>
			<persName><forename type="first">Andreȋ</forename><surname>Andreevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markov</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science in Context</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="591" to="600" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">INTERFACILE: Linguistic coverage and query reformulation</title>
		<author>
			<persName><forename type="first">Yvette</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Sabatier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 11th International Conference on Computational Linguistics</title>
				<imprint>
			<date type="published" when="1986">1986. 1986</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Coling</note>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
		<title level="m" type="main">The natural language decathlon: Multitask learning as question answering</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08730</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Catastrophic interference in connectionist networks: The sequential learning problem</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neal</forename><forename type="middle">J</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Psychology of learning and motivation</title>
				<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="109" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2013">2013a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eleventh annual conference of the international speech communication association</title>
				<imprint>
			<date type="published" when="2010-01">Jan Černockỳ, and Sanjeev Khudanpur. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2013">2013b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<monogr>
		<title level="m" type="main">Natural instructions: Benchmarking generalization to new tasks from natural language instructions</title>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chitta</forename><surname>Baral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno>CoRR, abs/2104.08773</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Exploring numeracy in word embeddings</title>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhilasha</forename><surname>Ravichander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carolyn</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1329</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3374" to="3380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Probing neural network comprehension of natural language arguments</title>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Niven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung-Yu</forename><surname>Kao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1459</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4658" to="4664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<monogr>
		<title level="m" type="main">Task-oriented query reformulation with reinforcement learning</title>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04572</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b134">
	<monogr>
		<title level="m" type="main">Multi-agent query reformulation: Challenges and the role of diversity. ICLR Workshop on Deep Reinforcement Learning for Structured Prediction</title>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Frassetto Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jannis</forename><surname>Bulian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Ciaramita</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">A smorgasbord of features for statistical machine translation</title>
		<author>
			<persName><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anoop</forename><surname>Khudanpur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shankar</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Libin</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viren</forename><surname>Eng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004</title>
				<meeting>the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004<address><addrLine>Boston, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="161" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<monogr>
		<title level="m" type="main">ERNIE-M: enhanced multilingual representation by aligning cross-lingual semantics with monolingual corpora</title>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<idno>CoRR, abs/2012.15674</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Thumbs up? sentiment classification using machine learning techniques</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivakumar</forename><surname>Vaithyanathan</surname></persName>
		</author>
		<idno type="DOI">10.3115/1118693.1118704</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2002 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jeffrey Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<monogr>
		<title level="m" type="main">True few-shot learning with language models</title>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">To tune or not to tune? adapting pretrained representations to diverse tasks</title>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-4302</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Workshop on Representation Learning for NLP</title>
				<meeting>the 4th Workshop on Representation Learning for NLP<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019. RepL4NLP-2019</date>
			<biblScope unit="page" from="7" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<monogr>
		<title level="m" type="main">How context affects language models&apos; factual predictions</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno>ArXiv, abs/2005.04611</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Language models as knowledge bases?</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1250</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2463" to="2473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">XCOPA: A multilingual dataset for causal commonsense reasoning</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Edoardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goran</forename><surname>Ponti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Glavaš</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianchu</forename><surname>Majewska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Vulić</surname></persName>
		</author>
		<author>
			<persName><surname>Korhonen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.185</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2362" to="2376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<monogr>
		<title level="m" type="main">Zero-shot text classification with generative language models</title>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno>CoRR, abs/1912.10165</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Exploring human-like attention supervision in visual question answering</title>
		<author>
			<persName><forename type="first">Tingting</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duanqing</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Learning how to ask: Querying LMs with mixtures of soft prompts</title>
		<author>
			<persName><forename type="first">Guanghui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5203" to="5212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Pre-trained models for natural language processing: A survey</title>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianxiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yige</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science China Technological Sciences</title>
		<imprint>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In arXiv</note>
</biblStruct>

<biblStruct xml:id="b150">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Explain yourself! leveraging language models for commonsense reasoning</title>
		<author>
			<persName><forename type="first">Nazneen</forename><surname>Fatema Rajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1487</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4932" to="4942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1264</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Learning to compose domain-specific transformations for data augmentation</title>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><forename type="middle">R</forename><surname>Ehrenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeshan</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Dunnmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017</title>
				<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04">2017. December 4-9, 2017</date>
			<biblScope unit="page" from="3236" to="3246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Prompt programming for large language models: Beyond the few-shot paradigm</title>
		<author>
			<persName><forename type="first">Laria</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Mcdonell</surname></persName>
		</author>
		<idno type="DOI">10.1145/3411763.3451760</idno>
	</analytic>
	<monogr>
		<title level="m">Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems, CHI EA &apos;21</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<monogr>
		<title level="m" type="main">Abstractive summarization with combination of pre-trained sequence-to-sequence and saliency models</title>
		<author>
			<persName><forename type="first">Itsumi</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyosuke</forename><surname>Nishida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kosuke</forename><surname>Nishida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junji</forename><surname>Tomita</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.13028</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">A mathematical exploration of why language models help solve downstream tasks</title>
		<author>
			<persName><forename type="first">Nikunj</forename><surname>Saunshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadhika</forename><surname>Malladi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
				<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-05-03">2021. May 3-7, 2021</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b158">
	<monogr>
		<title level="m" type="main">How many data points is a prompt worth?</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Teven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.08493</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Automatically identifying words that can serve as labels for few-shot text classification</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.488</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020</title>
				<meeting>the 28th International Conference on Computational Linguistics, COLING 2020<address><addrLine>Barcelona, Spain (Online)</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-08">2020. December 8-13, 2020</date>
			<biblScope unit="page" from="5569" to="5578" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Rare words: A major problem for contextualized embeddings and how to fix it by attentive mimicking</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020-02-07">2020. February 7-12, 2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="8766" to="8774" />
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b161">
	<monogr>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.07540</idno>
		<title level="m">Generating datasets with pretrained language models</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b162">
	<monogr>
		<title level="m" type="main">Few-shot text generation with pattern-exploiting training</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<monogr>
		<title level="m" type="main">Exploiting cloze questions for few shot text classification and natural language inference</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<monogr>
		<title level="m" type="main">It&apos;s not just size that matters: Small language models are also few-shot learners</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<monogr>
		<title level="m" type="main">Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in nlp</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sahana</forename><surname>Udupa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Controlling politeness in neural machine translation via side constraints</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1005</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016a</date>
			<biblScope unit="page" from="35" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1009</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016b</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="86" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<monogr>
		<title level="m" type="main">Constrained language models yield few-shot semantic parsers</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhro</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanouil</forename><surname>Antonios Platanios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Pauls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno>ArXiv, abs/2104.08768</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">AutoPrompt: Eliciting knowledge from language models with automatically generated prompts</title>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017</title>
				<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04">2017. December 4-9, 2017</date>
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 conference on empirical methods in natural language processing</title>
				<meeting>the 2013 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">MASS: masked sequence to sequence pre-training for language generation</title>
		<author>
			<persName><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
				<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019-06">2019. 9-15 June 2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="5926" to="5936" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Improving natural language processing tasks with human gaze-guided neural attention</title>
		<author>
			<persName><forename type="first">Ekta</forename><surname>Sood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Tannert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Bulling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6327" to="6341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">VL-BERT: pre-training of generic visual-linguistic representations</title>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
				<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26">2020. April 26-30, 2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b175">
	<monogr>
		<author>
			<persName><forename type="first">Yusuke</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Bulling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.05203</idno>
		<title level="m">Seeing with humans: Gaze-assisted neural image captioning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00756</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10-27">2019a. October 27 -November 2, 2019</date>
			<biblScope unit="page" from="7463" to="7472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<monogr>
		<title level="m" type="main">ERNIE 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyuan</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanbin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weixin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weibao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianzhong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhizhou</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<idno>CoRR, abs/2107.02137</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">ERNIE 2.0: A continual pre-training framework for language understanding</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Kun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020-02-07">2020. February 7-12, 2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="8968" to="8975" />
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b179">
	<monogr>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danxiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09223</idno>
		<title level="m">Ernie: Enhanced representation through knowledge integration</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b180">
	<monogr>
		<author>
			<persName><forename type="first">Derek</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Rakesh R Menon</surname></persName>
		</author>
		<author>
			<persName><surname>Bansal</surname></persName>
		</author>
		<title level="m">Shashank Srivastava, and Colin Raffel. 2021. Improving and simplifying pattern exploiting training</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">Stacked generalizations: When does it work?</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Kai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence, IJCAI 97</title>
				<meeting>the Fifteenth International Joint Conference on Artificial Intelligence, IJCAI 97<address><addrLine>Nagoya, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1997-08-23">1997. August 23-29, 1997</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="866" to="873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 shared task: Languageindependent named entity recognition</title>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fien</forename><surname>De Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</title>
				<meeting>the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<monogr>
		<title level="m" type="main">A simple method for commonsense reasoning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Trieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno>CoRR, abs/1806.02847</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<monogr>
		<title level="m" type="main">Multimodal few-shot learning with frozen language models</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Tsimpoukelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serkan</forename><surname>Cabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Ali Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<idno>CoRR, abs/2106.13884</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">A wrong answer or a wrong question? an intricate relationship between question reformulation and answer selection in conversational question answering</title>
		<author>
			<persName><forename type="first">Svitlana</forename><surname>Vakulenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhucheng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raviteja</forename><surname>Anantha</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.scai-1.2</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Search-Oriented Conversational AI (SCAI)</title>
				<meeting>the 5th International Workshop on Search-Oriented Conversational AI (SCAI)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">Universal adversarial triggers for attacking and analyzing NLP</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Kandpal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1221</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11-03">2019a. November 3-7, 2019</date>
			<biblScope unit="page" from="2153" to="2162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">Do NLP models know numbers? probing numeracy in embeddings</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1534</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019b</date>
			<biblScope unit="page" from="5307" to="5315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<analytic>
		<title level="a" type="main">KEPLER: A unified model for knowledge embedding and pre-trained language representation</title>
		<author>
			<persName><forename type="first">Xiaozhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaocheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="176" to="194" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<analytic>
		<title level="a" type="main">Generalizing from a few examples: A survey on few-shot learning</title>
		<author>
			<persName><forename type="first">Yaqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lionel</forename><forename type="middle">M</forename><surname>Ni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<monogr>
		<title level="m" type="main">Why do pretrained language models help in downstream tasks? an analysis of head and prompt tuning</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang</forename><surname>Michael Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main">CorefQA: Coreference resolution as querybased span prediction</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arianna</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.622</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6953" to="6963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main">Erniegram: Pre-training with explicitly n-gram masked language modeling for natural language understanding</title>
		<author>
			<persName><forename type="first">Dongling</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Kun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021</title>
				<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-06-06">2021. June 6-11, 2021</date>
			<biblScope unit="page" from="1702" to="1715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<monogr>
		<author>
			<persName><forename type="first">Han</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhang</forename><surname>Zhengyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gu</forename><surname>Yuxian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huo</forename><surname>Yuqi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiu</forename><surname>Jiezhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Wentao</surname></persName>
		</author>
		<author>
			<persName><surname>Huang Minlie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07139</idno>
		<title level="m">Pre-trained models: Past, present and future</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b195">
	<monogr>
		<title level="m" type="main">Byt5: Towards a token-free future with pre-trained byte-to-byte models</title>
		<author>
			<persName><forename type="first">Linting</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Barua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihir</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<idno>CoRR, abs/2105.13626</idno>
		<imprint>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">Aditya Barua, and Colin Raffel. 2021b. mt5: A massively multilingual pre-trained text-to-text transformer</title>
		<author>
			<persName><forename type="first">Linting</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihir</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021</title>
				<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">June 6-11, 2021</date>
			<biblScope unit="page" from="483" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<analytic>
		<title level="a" type="main">Designing templates for eliciting commonsense knowledge from pretrained sequence-to-sequence models</title>
		<author>
			<persName><forename type="first">Jheng-Hong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng-Chieh</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Feng</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan-Ju</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.307</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
				<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3449" to="3453" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b198">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. 2019. December 8-14, 2019</date>
			<biblScope unit="page" from="5754" to="5764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<analytic>
		<title level="a" type="main">Tabert: Pretraining for joint understanding of textual and tabular data</title>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.745</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07-05">2020. July 5-10, 2020</date>
			<biblScope unit="page" from="8413" to="8426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b200">
	<analytic>
		<title level="a" type="main">Benchmarking zero-shot text classification: Datasets, evaluation and entailment approach</title>
		<author>
			<persName><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamaal</forename><surname>Hay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1404</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11-03">2019. November 3-7, 2019</date>
			<biblScope unit="page" from="3912" to="3921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b201">
	<monogr>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaitang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04389</idno>
		<title level="m">A survey of knowledge-enhanced text generation</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b202">
	<monogr>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.00176</idno>
		<title level="m">Can we automate scientific reviewing?</title>
				<imprint>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b203">
	<monogr>
		<title level="m" type="main">Bartscore: Evaluating generated text as text generation</title>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b204">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaozhe</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenzhang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaisheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyan</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengtao</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dasen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zexuan</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangqing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfeng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yehong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaojie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyue</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanzhi</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaojun</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaowei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuefeng</forename><surname>Jin</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Qun Liu, and Yonghong Tian. 2021. Pangu-α: Large-scale autoregressive pretrained chinese language models with auto-parallel computation</note>
</biblStruct>

<biblStruct xml:id="b205">
	<analytic>
		<title level="a" type="main">PEGASUS: pre-training with extracted gap-sentences for abstractive summarization</title>
		<author>
			<persName><forename type="first">Jingqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
				<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-07-18">2020a. 13-18 July 2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="11328" to="11339" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b206">
	<analytic>
		<title level="a" type="main">Human gaze assisted artificial intelligence: a review</title>
		<author>
			<persName><forename type="first">Ruohan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akanksha</forename><surname>Saran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sihang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Niekum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dana</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><surname>Hayhoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI: Proceedings of the Conference</title>
				<imprint>
			<publisher>NIH Public Access</publisher>
			<date type="published" when="2020">2020b</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">4951</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b207">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing with rich non-local features</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="188" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b208">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaojun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenbo</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanchao</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanzheng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhixing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Xiaoyan Zhu, and Maosong Sun. 2021. CPM-2: large-scale cost-effective pre-trained language models. CoRR, abs/2106.10715</note>
</biblStruct>

<biblStruct xml:id="b209">
	<analytic>
		<title level="a" type="main">ERNIE: enhanced language representation with informative entities</title>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1139</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<title level="s">Long Papers</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07-28">2019. July 28-August 2, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1441" to="1451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b210">
	<monogr>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haozhe</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanchao</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaozhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanqi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenbo</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<idno>CoRR, abs/2012.00413</idno>
		<title level="m">Xiaoyan Zhu, and Maosong Sun. 2020c. CPM: A large-scale generative chinese pre-trained language model</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b211">
	<monogr>
		<title level="m" type="main">Calibrate before use: Improving few-shot performance of language models</title>
		<author>
			<persName><forename type="first">Tony</forename><forename type="middle">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b212">
	<monogr>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristy</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.04670</idno>
		<title level="m">Meta-tuning language models to answer prompts better</title>
				<imprint>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b213">
	<monogr>
		<title level="m" type="main">Factual probing is [MASK]: learning vs. learning to recall</title>
		<author>
			<persName><forename type="first">Zexuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno>CoRR, abs/2104.05240</idno>
		<imprint>
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b214">
	<analytic>
		<title level="a" type="main">Ensembling neural networks: many could be better than all</title>
		<author>
			<persName><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">137</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="239" to="263" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b215">
	<monogr>
		<title level="m" type="main">Enhancing factual consistency of abstractive summarization</title>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Hinthorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruochen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingkai</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08612</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b216">
	<analytic>
		<title level="a" type="main">Computational approaches to sentence completion</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ainur</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yessenalina</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="601" to="610" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
