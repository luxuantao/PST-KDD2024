<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Learning without Poor Local Minima</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-12-27">27 Dec 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Kenji</forename><surname>Kawaguchi</surname></persName>
							<email>kawaguch@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Learning without Poor Local Minima</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-12-27">27 Dec 2016</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1605.07110v3[stat.ML]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we prove a conjecture published in 1989 and also partially address an open problem announced at the Conference on Learning Theory (COLT) 2015. With no unrealistic assumption, we first prove the following statements for the squared loss function of deep linear neural networks with any depth and any widths: 1) the function is non-convex and non-concave, 2) every local minimum is a global minimum, 3) every critical point that is not a global minimum is a saddle point, and 4) there exist "bad" saddle points (where the Hessian has no negative eigenvalue) for the deeper networks (with more than three layers), whereas there is no bad saddle point for the shallow networks (with three layers). Moreover, for deep nonlinear neural networks, we prove the same four statements via a reduction to a deep linear model under the independence assumption adopted from recent work. As a result, we present an instance, for which we can answer the following question: how difficult is it to directly train a deep model in theory? It is more difficult than the classical machine learning models (because of the non-convexity), but not too difficult (because of the nonexistence of poor local minima). Furthermore, the mathematically proven existence of bad saddle points for deeper models would suggest a possible open problem. We note that even though we have advanced the theoretical foundations of deep learning and non-convex optimization, there is still a gap between theory and practice.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning has been a great practical success in many fields, including the fields of computer vision, machine learning, and artificial intelligence. In addition to its practical success, theoretical results have shown that deep learning is attractive in terms of its generalization properties <ref type="bibr" target="#b9">(Livni et al., 2014;</ref><ref type="bibr" target="#b10">Mhaskar et al., 2016)</ref>. That is, deep learning introduces good function classes that may have a low capacity in the VC sense while being able to represent target functions of interest well. However, deep learning requires us to deal with seemingly intractable optimization problems. Typically, training of a deep model is conducted via non-convex optimization. Because finding a global minimum of a general non-convex function is an NP-complete problem <ref type="bibr" target="#b11">(Murty &amp; Kabadi, 1987)</ref>, a hope is that a function induced by a deep model has some structure that makes the non-convex optimization tractable. Unfortunately, it was shown in 1992 that training a very simple neural network is indeed NP-hard <ref type="bibr" target="#b3">(Blum &amp; Rivest, 1992)</ref>. In the past, such theoretical concerns in optimization played a major role in shrinking the field of deep learning. That is, many researchers instead favored classical machining learning models (with or without a kernel approach) that require only convex optimization. While the recent great practical successes have revived the field, we do not yet know what makes optimization in deep learning tractable in theory.</p><p>In this paper, as a step toward establishing the optimization theory for deep learning, we prove a conjecture noted in <ref type="bibr" target="#b8">(Goodfellow et al., 2016)</ref> for deep linear networks, and also address an open problem announced in <ref type="bibr" target="#b5">(Choromanska et al., 2015b)</ref> for deep nonlinear networks. Moreover, for both the conjecture and the open problem, we prove more general and tighter statements than those previously given (in the ways explained in each section).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Deep linear neural networks</head><p>Given the absence of a theoretical understanding of deep nonlinear neural networks, <ref type="bibr" target="#b8">Goodfellow et al. (2016)</ref> noted that it is beneficial to theoretically analyze the loss functions of simpler models, i.e., deep linear neural networks. The function class of a linear multilayer neural network only contains functions that are linear with respect to inputs. However, their loss functions are non-convex in the weight parameters and thus nontrivial. <ref type="bibr" target="#b13">Saxe et al. (2014)</ref> empirically showed that the optimization of deep linear models exhibits similar properties to those of the optimization of deep nonlinear models. Ultimately, for theoretical development, it is natural to start with linear models before working with nonlinear models (as noted in <ref type="bibr" target="#b2">Baldi &amp; Lu, 2012)</ref>, and yet even for linear models, the understanding is scarce when the models become deep.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model and notation</head><p>We begin by defining the notation. Let H be the number of hidden layers, and let (X, Y ) be the training data set, with Y ∈ R dy×m and X ∈ R dx×m , where m is the number of data points. Here, d y ≥ 1 and d x ≥ 1 are the number of components (or dimensions) of the outputs and inputs, respectively. Let Σ = Y X T (XX T ) −1 XY T . We denote the model (weight) parameters by W , which consists of the entries of the parameter matrices corresponding to each layer: W H+1 ∈ R dy×dH , . . . , W k ∈ R d k ×d k−1 , . . . , W 1 ∈ R d1×dx . Here, d k represents the width of the k-th layer, where the 0-th layer is the input layer and the (H + 1)-th layer is the output layer (i.e., d 0 = d x and d H+1 = d y ). Let I d k be the d k × d k identity matrix. Let p = min(d H , . . . , d 1 ) be the smallest width of a hidden layer. We denote the (j, i)-th entry of a matrix M by M j,i . We also denote the j-th row vector of M by M j,• and the i-th column vector of M by M •,i .</p><p>We can then write the output of a feedforward deep linear model, Y (W, X) ∈ R dy×m , as</p><formula xml:id="formula_0">Y (W, X) = W H+1 W H W H−1 • • • W 2 W 1 X.</formula><p>We consider one of the most widely used loss functions, squared error loss:</p><formula xml:id="formula_1">L(W ) = 1 2 m i=1 Y (W, X) •,i − Y •,i 2 2 = 1 2 Y (W, X) − Y 2 F ,</formula><p>where • F is the Frobenius norm. Note that 2 m L(W ) is the usual mean squared error, for which all of our results hold as well, since multiplying L(W ) by a constant in W results in an equivalent optimization problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Background</head><p>Recently, <ref type="bibr" target="#b8">Goodfellow et al. (2016)</ref> remarked that when <ref type="bibr" target="#b1">Baldi &amp; Hornik (1989)</ref> proved Proposition 2.1 for shallow linear networks, they stated Conjecture 2.2 without proof for deep linear networks.</p><p>Proposition 2.1 <ref type="bibr" target="#b1">(Baldi &amp; Hornik, 1989:</ref> </p><formula xml:id="formula_2">shallow linear network) Assume that H = 1 (i.e., Y (W, X) = W 2 W 1 X)</formula><p>, assume that XX T and XY T are invertible, assume that Σ has d y distinct eigenvalues, and assume that p &lt; d x , p &lt; d y and d y = d x (e.g., an autoencoder). Then, the loss function L(W ) has the following properties:</p><formula xml:id="formula_3">(i) It is convex in each matrix W 1 (or W 2 ) when the other W 2 (or W 1 ) is fixed. (ii) Every local minimum is a global minimum.</formula><p>Conjecture 2.2 <ref type="bibr" target="#b1">(Baldi &amp; Hornik, 1989</ref>: deep linear network) Assume the same set of conditions as in Proposition 2.1 except for H = 1. Then, the loss function L(W ) has the following properties:</p><formula xml:id="formula_4">(i) For any k ∈ {1, . . . , H + 1}, it is convex in each matrix W k when for all k ′ = k, W k ′ is fixed.</formula><p>(ii) Every local minimum is a global minimum. <ref type="bibr" target="#b2">Baldi &amp; Lu (2012)</ref> recently provided a proof for Conjecture 2.2 (i), leaving the proof of Conjecture 2.2 (ii) for future work. They also noted that the case of p ≥ d x = d x is of interest, but requires further analysis, even for a shallow network with H = 1. An informal discussion of Conjecture 2.2 can be found in <ref type="bibr" target="#b0">(Baldi, 1989)</ref>. In Appendix D, we provide a more detailed discussion of this subject.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Results</head><p>We now state our main theoretical results for deep linear networks, which imply Conjecture 2.2 (ii) as well as obtain further information regarding the critical points with more generality. The assumptions of having full rank and distinct eigenvalues in the training data matrices in Theorem 2.3 are realistic and practically easy to satisfy, as discussed in previous work (e.g., <ref type="bibr" target="#b1">Baldi &amp; Hornik, 1989)</ref>. In contrast to related previous work <ref type="bibr" target="#b1">(Baldi &amp; Hornik, 1989;</ref><ref type="bibr" target="#b2">Baldi &amp; Lu, 2012)</ref>, we do not assume the invertibility of XY T , p &lt; d x , p &lt; d y nor d y = d x . In Theorem 2.3, p ≥ d x is allowed, as well as many other relationships among the widths of the layers. Therefore, we successfully proved Conjecture 2.2 (ii) and a more general statement. Moreover, Theorem 2.3 (iv) and Corollary 2.4 provide additional information regarding the important properties of saddle points.</p><p>Theorem 2.3 presents an instance of a deep model that would be tractable to train with direct greedy optimization, such as gradient-based methods. If there are "poor" local minima with large loss values everywhere, we would have to search the entire space,<ref type="foot" target="#foot_2">2</ref> the volume of which increases exponentially with the number of variables. This is a major cause of NP-hardness for non-convex optimization. In contrast, if there are no poor local minima as Theorem 2.3 (ii) states, then saddle points are the main remaining concern in terms of tractability. <ref type="foot" target="#foot_3">3</ref> Because the Hessian of L(W ) is Lipschitz continuous, if the Hessian at a saddle point has a negative eigenvalue, it starts appearing as we approach the saddle point. Thus, Theorem 2.3 and Corollary 2.4 suggest that for 1-hidden layer networks, training can be done in polynomial time with a second order method or even with a modified stochastic gradient decent method, as discussed in <ref type="bibr" target="#b7">(Ge et al., 2015)</ref>. For deeper networks, Corollary 2.4 states that there exist "bad" saddle points in the sense that the Hessian at the point has no negative eigenvalue. However, we know exactly when this can happen from Theorem 2.3 (iv) in our deep models. We leave the development of efficient methods to deal with such a bad saddle point in general deep models as an open problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Deep nonlinear neural networks</head><p>Now that we have obtained a comprehensive understanding of the loss surface of deep linear models, we discuss deep nonlinear models. For a practical deep nonlinear neural network, our theoretical results so far for the deep linear models can be interpreted as the following: depending on the nonlinear activation mechanism and architecture, training would not be arbitrarily difficult. While theoretical formalization of this intuition is left to future work, we address a recently proposed open problem for deep nonlinear networks in the rest of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model</head><p>We use the same notation as for the deep linear models, defined in the beginning of Section 2.1. The output of deep nonlinear neural network, Ŷ (W, X) ∈ R dy×m , is defined as</p><formula xml:id="formula_5">Ŷ(W, X) = qσ H+1 (W H+1 σ H (W H σ H−1 (W H−1 • • • σ 2 (W 2 σ 1 (W 1 X)) • ••))),</formula><p>where q ∈ R is simply a normalization factor, the value of which is specified later. Here, σ k : R d k ×m → R d k ×m is the element-wise rectified linear function:</p><formula xml:id="formula_6">σ k       b 11 . . . b 1m . . . . . . . . . b d k 1 • • • b d k m       =    σ(b 11 ) . . . σ(b 1m) . . . . . . . . . σ(b d k 1 ) • • • σ(b d k m )    , where σ(b ij ) = max(0, b ij ).</formula><p>In practice, we usually set σ H+1 to be an identity map in the last layer, in which case all our theoretical results still hold true.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Background</head><p>Following the work by <ref type="bibr" target="#b6">Dauphin et al. (2014)</ref>, <ref type="bibr" target="#b4">Choromanska et al. (2015a)</ref> investigated the connection between the loss functions of deep nonlinear networks and a function well-studied via random matrix theory (i.e., the Hamiltonian of the spherical spin-glass model). They explained that their theoretical results relied on several unrealistic assumptions. Later, <ref type="bibr" target="#b5">Choromanska et al. (2015b)</ref> suggested at the Conference on Learning Theory (COLT) 2015 that discarding these assumptions is an important open problem. The assumptions were labeled A1p, A2p, A3p, A4p, A5u, A6u, and A7p.</p><p>In this paper, we successfully discard most of these assumptions. In particular, we only use a weaker version of assumptions A1p and A5u. We refer to the part of assumption A1p (resp. A5u) that corresponds only to the model assumption as A1p-m (resp. A5u-m). Note that assumptions A1p-m and A5u-m are explicitly used in the previous work <ref type="bibr" target="#b4">(Choromanska et al., 2015a)</ref> and included in A1p and A5u (i.e., we are not making new assumptions here).</p><p>As the model Ŷ (W, X) ∈ R dy×m represents a directed acyclic graph, we can express an output from one of the units in the output layer as</p><formula xml:id="formula_7">Ŷ (W, X) j,i = q Ψ p=1 [X i ] (j,p) [Z i ] (j,p) H+1 k=1 w (k) (j,p) .<label>(1)</label></formula><p>Here, Ψ is the total number of paths from the inputs to each j-th output in the directed acyclic graph. In addition, [X i ] (j,p) ∈ R represents the entry of the i-th sample input datum that is used in the p-th path of the j-th output. For each layer k, w</p><p>(j,p) ∈ R is the entry of W k that is used in the p-th path of the j-th output. Finally, [Z i ] (j,p) ∈ {0, 1} represents whether the p-th path of the j-th output is active ([Z i ] (j,p) = 1) or not ([Z i ] (j,p) = 0) for each sample i as a result of the rectified linear activation.</p><p>Assumption A1p-m assumes that the Z's are Bernoulli random variables with the same probability of success, Pr([Z i ] (j,p) = 1) = ρ for all i and (j, p). Assumption A5u-m assumes that the Z's are independent from the input X's and parameters w's. With assumptions A1p-m and A5u-m, we can write <ref type="bibr" target="#b5">Choromanska et al. (2015b)</ref> noted that A6u is unrealistic because it implies that the inputs are not shared among the paths. In addition, Assumption A5u is unrealistic because it implies that the activation of any path is independent of the input data. To understand all of the seven assumptions (A1p, A2p, A3p, A4p, A5u, A6u, and A7p), we note that <ref type="bibr">Choromanska et al. (2015b,a)</ref> used these seven assumptions to reduce their loss functions of nonlinear neural networks to:</p><formula xml:id="formula_9">E Z [ Ŷ (W, X) j,i ] = q Ψ p=1 [X i ] (j,p) ρ H+1 k=1 w (k) (j,p) .</formula><formula xml:id="formula_10">L previous (W ) = 1 λ H/2 λ i1,i2,...,iH+1=1 X i1,i2,...,iH+1 H+1 k=1 w i k subject to 1 λ λ i=1 w 2 i = 1,</formula><p>where λ ∈ R is a constant related to the size of the network. For our purpose, the detailed definitions of the symbols are not important (X and w are defined in the same way as in equation 1). Here, we point out that the target function Y has disappeared in the loss L previous (W ) (i.e., the loss value does not depend on the target function). That is, whatever the data points of Y are, their loss values are the same. Moreover, the nonlinear activation function has disappeared in L previous (W ) (and the nonlinearity is not taken into account in X or w). In the next section, by using only a strict subset of the set of these seven assumptions, we reduce our loss function to a more realistic loss function of an actual deep model.</p><p>Proposition 3.1 (High-level description of a main result in <ref type="bibr" target="#b4">Choromanska et al., 2015a)</ref>  <ref type="table">Assume A1p  (including A1p-m), A2p, A3p, A4p, A5u (including A5u-m), A6u, and A7p</ref>  <ref type="bibr" target="#b5">(Choromanska et al., 2015b)</ref>. Furthermore, assume that d y = 1. Then, the expected loss of each sample datum, L previous (W ), has the following property: above a certain loss value, the number of local minima diminishes exponentially as the loss value increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>We now state our theoretical result, which partially address the aforementioned open problem. We consider loss functions for all the data points and all possible output dimensionalities (i.e., vectoredvalued output). More concretely, we consider the squared error loss with expectation, Comparing Corollary 3.2 and Proposition 3.1, we can see that we successfully discarded assumptions A2p, A3p, A4p, A6u, and A7p while obtaining a tighter statement in the following sense: Corollary 3.2 states with fewer unrealistic assumptions that there is no poor local minimum, whereas Proposition 3.1 roughly asserts with more unrealistic assumptions that the number of poor local minimum may be not too large. Furthermore, our model Ŷ is strictly more general than the model analyzed in <ref type="bibr">(Choromanska et al., 2015a,b)</ref> (i.e., this paper's model class contains the previous work's model class but not vice versa).</p><formula xml:id="formula_11">L(W ) = 1 2 E Z [ Ŷ (W, X) − Y ] 2 F .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Proof Idea and Important lemmas</head><p>In this section, we provide overviews of the proofs of the theoretical results. Our proof approach largely differs from those in previous work <ref type="bibr" target="#b1">(Baldi &amp; Hornik, 1989;</ref><ref type="bibr" target="#b2">Baldi &amp; Lu, 2012;</ref><ref type="bibr">Choromanska et al., 2015a,b)</ref>. In contrast to <ref type="bibr" target="#b1">(Baldi &amp; Hornik, 1989;</ref><ref type="bibr" target="#b2">Baldi &amp; Lu, 2012)</ref>, we need a different approach to deal with the "bad" saddle points that start appearing when the model becomes deeper (see Section 2.3), as well as to obtain more comprehensive properties of the critical points with more generality. While the previous proofs heavily rely on the first-order information, the main parts of our proofs take advantage of the second order information. In contrast, <ref type="bibr">Choromanska et al. (2015a,b)</ref> used the seven assumptions to relate the loss functions of deep models to a function previously analyzed with a tool of random matrix theory. With no reshaping assumptions (A3p, A4p, and A6u), we cannot relate our loss function to such a function. Moreover, with no distributional assumptions (A2p and A6u) (except the activation), our Hessian is deterministic, and therefore, even random matrix theory itself is insufficient for our purpose. Furthermore, with no spherical constraint assumption (A7p), the number of local minima in our loss function can be uncountable.</p><p>One natural strategy to proceed toward Theorem 2.3 and Corollary 3.2 would be to use the first-order and second-order necessary conditions of local minima (e.g., the gradient is zero and the Hessian is positive semidefinite). <ref type="foot" target="#foot_4">4</ref> However, are the first-order and second-order conditions sufficient to prove Theorem 2.3 and Corollary 3.2? Corollaries 2.4 show that the answer is negative for deep models with H ≥ 2, while it is affirmative for shallow models with H = 1. Thus, for deep models, a simple use of the first-order and second-order information is insufficient to characterize the properties of each critical point. In addition to the complexity of the Hessian of the deep models, this suggests that we must strategically extract the second order information. Accordingly, in section 4.2, we obtain an organized representation of the Hessian in Lemma 4.3 and strategically extract the information in Lemmas 4.4 and 4.6. With the extracted information, we discuss the proofs of Theorem 2.3 and Corollary 3.2 in section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Notations</head><p>Let M ⊗ M ′ be the Kronecker product of M and</p><formula xml:id="formula_12">M ′ . Let D vec(W T k ) f (•) = ∂f (•) ∂ vec(W T k )</formula><p>be the partial</p><formula xml:id="formula_13">derivative of f with respect to vec(W T k ) in the numerator layout. That is, if f : R din → R dout , we have D vec(W T k ) f (•) ∈ R dout×(d k d k−1 )</formula><p>. Let R(M ) be the range (or the column space) of a matrix M . Let M − be any generalized inverse of M . When we write a generalized inverse in a condition or statement, we mean it for any generalized inverse (i.e., we omit the universal quantifier over generalized inverses, as this is clear</p><formula xml:id="formula_14">). Let r = (Y (W, X) − Y ) T ∈ R m×dy be an error matrix. Let C = W H+1 • • • W 2 ∈ R dy×d1 . When we write W k • • • W k ′ ,</formula><p>we generally intend that k &gt; k ′ and the expression denotes a product over W j for integer k ≥ j ≥ k ′ . For notational compactness, two additional cases can arise: when k = k ′ , the expression denotes simply W k , and when k &lt; k ′ , it denotes I d k . For example, in the statement of Lemma 4.1, if we set k := H + 1, we have that</p><formula xml:id="formula_15">W H+1 W H • • • W H+2 I dy .</formula><p>In Lemma 4.6 and the proofs of Theorems 2.3, we use the following additional notation. We denote an eigendecomposition of Σ as Σ = U ΛU T , where the entries of the eigenvalues are ordered as</p><formula xml:id="formula_16">Λ 1,1 &gt; • • • &gt; Λ dy,dy with corresponding orthogonal eigenvector matrix U = [u 1 , . . . , u dy ]. For each k ∈ {1, . . . d y }, u k ∈ R dy×1 is a column eigenvector. Let p = rank(C) ∈ {1, . . . , min(d y , p)}.</formula><p>We define a matrix containing the subset of the p largest eigenvectors as U p = [u 1 , . . . , u p]. Given any ordered set </p><formula xml:id="formula_17">I p = {i 1 , . . . , i p | 1 ≤ i 1 &lt; • • • &lt; i p ≤</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Lemmas</head><p>As discussed above, we extracted the first-order and second-order conditions of local minima as the following lemmas. The lemmas provided here are also intended to be our additional theoretical results that may lead to further insights. The proofs of the lemmas are in the appendix. Lemma 4.1 (Critical point necessary and sufficient condition) W is a critical point of L(W ) if and only if for all k ∈ {1, ..., H + 1},</p><formula xml:id="formula_18">D vec(W T k ) L(W ) T = W H+1 W H • • • W k+1 ⊗ (W k−1 • • • W 2 W 1 X) T T vec(r) = 0. Lemma 4.2 (Representation at critical point) If W is a critical point of L(W ), then W H+1 W H • • • W 2 W 1 = C(C T C) − C T Y X T (XX T ) −1 .</formula><p>Lemma 4.3 (Block Hessian with Kronecker product) Write the entries of ∇ 2 L(W ) in a block form as</p><formula xml:id="formula_19">∇ 2 L(W ) =      D vec(W T H+1 ) D vec(W T H+1 ) L(W ) T • • • D vec(W T 1 ) D vec(W T H+1 ) L(W ) T . . . . . . . . . D vec(W T H+1 ) D vec(W T 1 ) L(W ) T • • • D vec(W T 1 ) D vec(W T 1 ) L(W ) T      .</formula><p>Then, for any k ∈ {1, ..., H + 1},</p><formula xml:id="formula_20">D vec(W T k ) D vec(W T k ) L(W ) T = (W H+1 • • • W k+1 ) T (W H+1 • • • W k+1 ) ⊗ (W k−1 • • • W 1 X)(W k−1 • • • W 1 X) T ,</formula><p>and, for any k ∈ {2, ..., H + 1},</p><formula xml:id="formula_21">D vec(W T k ) D vec(W T 1 ) L(W ) T = C T (W H+1 • • • W k+1 ) ⊗ X(W k−1 • • • W 1 X) T + [(W k−1 • • • W 2 ) T ⊗ X] [I d k−1 ⊗ (rW H+1 • • • W k+1 ) •,1 . . . I d k−1 ⊗ (rW H+1 • • • W k+1 ) •,d k ] .</formula><p>Lemma 4.4 (Hessian semidefinite necessary condition) If ∇ 2 L(W ) is positive semidefinite or negative semidefinite at a critical point, then for any k ∈ {2, ..., H + 1}, </p><formula xml:id="formula_22">R((W k−1 • • • W 3 W 2 ) T ) ⊆ R(C T C) or XrW H+1 W H • • • W k+1 = 0. Corollary 4.5 If ∇ 2 L(W ) is positive semidefinite or negative semidefinite at a critical point, then for any k ∈ {2, ..., H + 1}, rank(W H+1 W H • • • W k ) ≥ rank(W k−1 • • • W 3 W 2 ) or XrW H+1 W H • • • W k+1 = 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Proof sketches of theorems</head><p>We now provide the proof sketch of Theorem 2.3 and Corollary 3.2. We complete the proofs in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Proof sketch of Theorem 2.3 (ii)</head><p>By case analysis, we show that any point that satisfies the necessary conditions and the definition of a local minimum is a global minimum. </p><formula xml:id="formula_23">W H+1 • • • W 1 = U p U T p Y X T (XX T ) −1</formula><p>, which is the orthogonal projection onto the subspace spanned by the p eigenvectors corresponding to the p largest eigenvalues following the ordinary least square regression matrix. This is indeed the expression of a global minimum. We first prove the statement for the base case with k = 1 by using an expression of W 1 that is obtained by a first-order necessary condition: for an arbitrary L 1 ,</p><formula xml:id="formula_24">W 1 = (C T C) − C T Y X T (XX T ) −1 + (I − (C T C) − C T C)L 1 .</formula><p>By using Lemma 4.6 to obtain an expression of C, we deduce that we can have rank(W 1 ) ≥ min(p, d y ) with arbitrarily small perturbation of each entry of W 1 without changing the loss value.</p><p>For the inductive step with k ∈ {2, . . . , H + 1}, from Lemma 4.4, we use the following necessary condition for the Hessian to be (positive or negative) semidefinite at a critical point: for any k ∈ {2, . . . , H + 1},</p><formula xml:id="formula_25">R((W k−1 • • • W 2 ) T ) ⊆ R(C T C) or XrW H+1 • • • W k+1 = 0.</formula><p>We use the inductive hypothesis to conclude that the first condition is false, and thus the second condition must be satisfied at a candidate point of a local minimum. From the latter condition, with extra steps, we can deduce that we can have rank(</p><formula xml:id="formula_26">W k W k−1 • • • W 1 ) ≥ min(p, d x )</formula><p>with arbitrarily small perturbation of each entry of W k while retaining the same loss value.</p><p>We conclude the induction, proving that we can have rank</p><formula xml:id="formula_27">(C) ≥ rank(W H+1 • • • W 1 ) ≥ min(p, d x )</formula><p>with arbitrarily small perturbation of each parameter without changing the value of L(W ). Upon such a perturbation, we have the case where rank(C) ≥ min(p, d y ), for which we have already proven that every local minimum is a global minimum. Summarizing the above, any point that satisfies the definition (and necessary conditions) of a local minimum is indeed a global minimum. Therefore, we conclude the proof sketch of Theorem 2.3 (ii).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Proof sketch of Theorem 2.3 (i), (iii) and (iv)</head><p>We can prove the non-convexity and non-concavity of this function simply from its Hessian (Theorem 2.3 (i)). That is, we can show that in the domain of the function, there exist points at which the Hessian becomes indefinite. Indeed, the domain contains uncountably many points at which the Hessian is indefinite.</p><p>We now consider Theorem 2.3 (iii): every critical point that is not a global minimum is a saddle point. Combined with Theorem 2.3 (ii), which is proven independently, this is equivalent to the statement that there are no local maxima. We first show that if W H+1 • • • W 2 = 0, the loss function always has some strictly increasing direction with respect to W 1 , and hence there is no local maximum. If W H+1 • • • W 2 = 0, we show that at a critical point, if the Hessian is negative semidefinite (i.e., a necessary condition of local maxima), we can have W H+1 • • • W 2 = 0 with arbitrarily small perturbation without changing the loss value. We can prove this by induction on k = 2, . . . , H + 1, similar to the induction in the proof of Theorem 2.3 (ii). This means that there is no local maximum. Theorem 2.3 (iv) follows Theorem 2.3 (ii)-(iii) and the analyses for Case I and Case II in the proof of Theorem 2.3 (ii); when rank(W H • • • W 2 ) = p, if ∇ 2 L(W ) 0 at a critical point, W is a global minimum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Proof sketch of Corollary 3.2</head><p>Since the activations are assumed to be random and independent, the effect of nonlinear activations disappear by taking expectation. As a result, the loss function L(W ) is reduced to L(W ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we addressed some open problems, pushing forward the theoretical foundations of deep learning and non-convex optimization. For deep linear neural networks, we proved the aforementioned conjecture and more detailed statements with more generality. For deep nonlinear neural networks, when compared with the previous work, we proved a tighter statement (in the way explained in section 3) with more generality (d y can vary) and with strictly weaker model assumptions (only two assumptions out of seven). However, our theory does not yet directly apply to the practical situation. To fill the gap between theory and practice, future work would further discard the remaining two out of the seven assumptions made in previous work. Our new understanding of the deep linear models at least provides the following theoretical fact: the bad local minima would arise in a deep nonlinear model but only as an effect of adding nonlinear activations to the corresponding deep linear model. Thus, depending on the nonlinear activation mechanism and architecture, we would be able to efficiently train deep models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Learning without Poor Local Minima Appendix</head><p>A Proofs of lemmas and corollary in Section 4.2</p><p>We complete the proofs of the lemmas and corollary in Section 4.2.</p><p>A.1 Proof of Lemma 4.1</p><formula xml:id="formula_28">Proof Since L(W ) = 1 2 Y (W, X) − Y 2 F = 1 2 vec(r) T vec(r), D vec(W T k ) L(W ) = D vec(r) L(W ) D vec(W T k ) vec(r) = vec(r) T D vec(W T k ) vec(X T I dx W T 1 • • • W T H+1 I dy ) − D vec(W T k ) vec(Y T ) = vec(r) T D vec(W T k ) (W H+1 • • • W k+1 ⊗ (W k−1 • • • W 1 X) T ) vec(W T k ) = vec(r) T W H+1 • • • W k+1 ⊗ (W k−1 • • • W 1 X) T . By setting D vec(W T k ) L(W ) T</formula><p>= 0 for all k ∈ {1, ..., H + 1}, we obtain the statement of Lemma 4.1. For the boundary cases (i.e., k = H + 1 or k = 1), it can be seen from the second to the third lines that we obtain the desired results with the definition,</p><formula xml:id="formula_29">W k • • • W k+1 I d k (i.e., W H+1 • • • W H+2 I dy and W 0 • • • W 1 I dx ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Proof of Lemma 4.2</head><p>Proof From the critical point condition with respect to W 1 (Lemma 4.1),</p><formula xml:id="formula_30">0 = D vec(W T k ) L(W ) T = W H+1 • • • W 2 ⊗ X T T vec(r) = vec(XrW H+1 • • • W 2 ), which is true if and only if XrW H+1 • • • W 2 = 0. By expanding r, 0 = XX T W T 1 C T C − XY T C. By solving for W 1 , W 1 = (C T C) − C T Y X T (XX T ) −1 + (I − (C T C) − C T C)L,<label>(2)</label></formula><p>for an arbitrary matrix L. Due to the property of any generalized inverse <ref type="bibr">(Zhang, 2006, p. 41)</ref>, we have that</p><formula xml:id="formula_31">C(C T C) − C T C = C. Thus, CW1 = C(C T C) − C T Y X T (XX T ) −1 + (C − C(C T C) − C T C)L = C(C T C) − C T Y X T (XX T ) −1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Proof of Lemma 4.3</head><p>Proof For the diagonal blocks: the entries of diagonal blocks are obtained simply using the result of Lemma 4.1 as</p><formula xml:id="formula_32">D vec(W T k ) D vec(W T k ) L(W ) T = W H+1 • • • W k+1 ⊗ (W k−1 • • • W 1 X) T T D vec(W T k ) vec(r).</formula><p>Using the formula of D vec(W T k ) vec(r) computed in the proof of of Lemma 4.1 yields the desired result.</p><p>For the off-diagonal blocks with k = 2, ..., H :</p><formula xml:id="formula_33">D vec(W T k ) [D vec(W T 1 ) L(W )] T = W H+1 • • • W 2 ⊗ X) T T D vec(W T k ) vec(r) + D vec(W T k ) W H+1 • • • W k+1 ⊗ X T T vec(r)</formula><p>The first term above is reduced to the first term of the statement in the same way as the diagonal blocks. For the second term,</p><formula xml:id="formula_34">D vec(W T k ) W H+1 • • • W 2 ⊗ X T T vec(r) = m i=1 dy j=1 D vec(W T k ) W H+1,j W H • • • W 2 ⊗ X T i T r i,j = m i=1 dy j=1 (A k ) j,• ⊗ B T k ⊗ X T i T r i,j = m i=1 dy j=1 (A k ) j,1 B T k ⊗ X i . . . (A k ) j,d k B T k ⊗ X i r i,j = B T k ⊗ m i=1 dy j=1 r i,j (A k ) j,1 X i . . . B T k ⊗ m i=1 dy j=1 r i,j (A k ) j,d k X i .</formula><p>where</p><formula xml:id="formula_35">A k = W H+1 • • • W k+1 and B k = W k−1 • • • W 2 .</formula><p>The third line follows the fact that</p><formula xml:id="formula_36">(W H+1,j W H • • • W 2 ) T = vec(W T 2 • • • W T H W T H+1,j ) = (W H+1,j • • • W k+1 ⊗ W T 2 • • • W T k−1 ) vec(W T k ).</formula><p>In the last line, we have the desired result by rewriting</p><formula xml:id="formula_37">m i=1 dy j=1 r i,j (A k ) j,t X i = X(rW H+1 • • • W k+1 ) •,t .</formula><p>For the off-diagonal blocks with k = H + 1: The first term in the statement is obtained in the same way as above (for the off-diagonal blocks with k = 2, ..., H). For the second term, notice that</p><formula xml:id="formula_38">vec(W T H+1 ) = (W H+1 ) T 1,• . . . (W H+1 ) T dy,•</formula><p>T where (W H+1 ) j,• is the j-th row vector of W H+1</p><p>or the vector corresponding to the j-th output component. That is, it is conveniently organized as the blocks, each of which corresponds to each output component (or rather we chose vec(W T k ) instead of vec(W k ) for this reason, among others). Also,</p><formula xml:id="formula_39">D vec(W T H+1 ) W H+1 • • • W 2 ⊗ X T T vec(r) = = m i=1 D (WH+1) T 1,• C 1,• ⊗ X T i T r i,1 . . . m i=1 D (WH+1) T dy ,• C dy,• ⊗ X T i T r i,dy ,</formula><p>where we also used the fact that</p><formula xml:id="formula_40">m i=1 dy j=1 D vec((WH+1) T t,• ) C j,• ⊗ X T i T r i,j = m i=1 D vec((WH+1) T t,• ) C t,• ⊗ X T i T r i,t .</formula><p>For each block entry t = 1, . . . , d y in the above, similarly to the case of k = 2, ..., H,</p><formula xml:id="formula_41">m i=1 D vec((WH+1) T t,• ) C j,• ⊗ X T i T r i,t = B T H+1 ⊗ m i=1 r i,t (A H+1 ) j,t X i .</formula><p>Here, we have the desired result by rewriting m i=1 r i,t (A H+1 ) j,1 X i = X(rI dy ) •,t = Xr •,t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Proof of Lemma 4.4</head><p>Proof Note that a similarity transformation preserves the eigenvalues of a matrix. For each k ∈ {2, . . . , H + 1}, we take a similarity transform of ∇ 2 L(W ) (whose entries are organized as in Lemma 4.3) as</p><formula xml:id="formula_42">P −1 k ∇ 2 L(W )P k =      D vec(W T 1 ) D vec(W T 1 ) L(W ) T D vec(W T k ) D vec(W T 1 ) L(W ) T • • • D vec(W T 1 ) D vec(W T k ) L(W ) T D vec(W T k ) D vec(W T k ) L(W ) T • • • . . . . . . . . .     </formula><p>Here, P k = e H+1 e k Pk is the permutation matrix where e i is the i-th element of the standard basis (i.e., a column vector with 1 in the i-th entry and 0 in every other entries), and Pk is any arbitrarily matrix that makes P k to be a permutation matrix. Let M k be the principal submatrix of P −1 k ∇ 2 L(W )P k that consists of the first four blocks appearing in the above equation. Then,</p><formula xml:id="formula_43">∇ 2 L(W ) 0 ⇒ ∀k ∈ {2, . . . , H + 1}, M k 0 ⇒ ∀k ∈ {2, . . . , H + 1}, R(D vec(W T k ) (D vec(W T 1 ) L(W )) T ) ⊆ R(D vec(W T 1 ) (D vec(W T 1 ) L(W )) T ),</formula><p>Here, the first implication follows the necessary condition with any principal submatrix and the second implication follows the necessary condition with the Schur complement <ref type="bibr">(Zhang, 2006, theorem 1.20, p. 44)</ref>. <ref type="bibr">Zhang, 2006, p. 41)</ref>. Thus, by plugging in the formulas of</p><formula xml:id="formula_44">Note that R(M ′ ) ⊆ R(M ) ⇔ (I − M M − )M ′ = 0 (</formula><formula xml:id="formula_45">D vec(W T k ) (D vec(W T 1 ) L(W )) T and D vec(W T 1 ) (D vec(W T 1 ) L(W )) T that are derived in Lemma 4.3, ∇ 2 L(W ) 0 ⇒ ∀k ∈ {2, . . . , H + 1}, 0 = I − (C T C ⊗ (XX T ))(C T C ⊗ (XX T )) − (C T A k ⊗ B k W1X) + I − (C T C ⊗ (XX T ))(C T C ⊗ (XX T )) − [B T k ⊗ X] I d k−1 ⊗ (rA k )•,1 . . . I d k−1 ⊗ (rA k ) •,d k where A k = W H+1 • • • W k+1 and B k = W k−1 • • • W 2 . Here, we can replace (C T C ⊗ (XX T )) − by ((C T C) − ⊗ (XX T ) −1 ) (see Appendix A.7). Thus, I − (C T C ⊗ (XX T ))(C T C ⊗ (XX T )) − can be replaced by (I d1 ⊗ I dy ) − (C T C(C T C) − ⊗ I dy ) = (I d1 − C T C(C T C) − ) ⊗ I dy .</formula><p>Accordingly, the first term is reduced to zero as <ref type="bibr">Zhang, 2006, p. 41</ref>). Thus, with the second term remained, the condition is reduced to</p><formula xml:id="formula_46">(I d 1 − C T C(C T C) − ) ⊗ I dy C T A k ⊗ B k W1X = ((I d 1 − C T C(C T C) − )C T A k ) ⊗ B k W1X = 0, since C T C(C T C) − C T = C T (</formula><formula xml:id="formula_47">∀k ∈ {2, . . . , H + 1}, ∀t ∈ {1, . . . , d y }, (B T k − C T C(C T C) − B T k ) ⊗ X(rA k ) •,t = 0. This implies ∀k ∈ {2, . . . , H + 1}, (R(B T k ) ⊆ R(C T C) or XrA k = 0</formula><p>), which concludes the proof for the positive semidefinite case. For the necessary condition of the negative semidefinite case, we obtain the same condition since</p><formula xml:id="formula_48">∇ 2 L(W ) 0 ⇒ ∀k ∈ {2, . . . , H + 1}, M k 0 ⇒ ∀k ∈ {2, . . . , H + 1}, R(−D vec(W T k ) (D vec(W T 1 ) L(W )) T ) ⊆ R(−D vec(W T 1 ) (D vec(W T 1 ) L(W )) T ) ⇒ ∀k ∈ {2, . . . , H + 1}, R(D vec(W T k ) (D vec(W T 1 ) L(W )) T ) ⊆ R(D vec(W T 1 ) (D vec(W T 1 ) L(W )) T ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Proof of Corollary 4.5</head><p>Proof From the first condition in the statement of Lemma 4.4,</p><formula xml:id="formula_49">R(W T 2 • • • W T k−1 ) ⊆ R(W T 2 • • • W T H+1 W H+1 • • • W 2 ) ⇒ rank(W T k • • • W T H+1 ) ≥ rank(W T 2 • • • W T k−1 ) ⇒ rank(W H+1 • • • W k ) ≥ rank(W k−1 • • • W 2 ).</formula><p>The first implication follows the fact that the rank of a product of matrices is at most the minimum of the ranks of the matrices, and the fact that the column space of</p><formula xml:id="formula_50">W T 2 • • • W T H+1 is subspace of the column space of W T 2 • • • W T k−1 .</formula><p>A.6 Proof of Lemma 4.6</p><p>Proof For the (Xr = 0) condition: Let M H+1 be the principal submatrix as defined in the proof of Lemma 4.4 (the principal submatrix of P −1 H+1 ∇ 2 L(W )P H+1 that consists of the first four blocks of</p><formula xml:id="formula_51">it). Let B k = W k−1 • • • W 2 . Let F = B H+1 W 1 XX T W T 1 B T H+1</formula><p>. Using Lemma 4.3 for the blocks corresponding to W 1 and W H+1 ,</p><formula xml:id="formula_52">M H+1 = C T C ⊗ XX T (C T ⊗ XX T (B H+1 W 1 ) T ) + E (C ⊗ B H+1 W 1 XX T ) + E T I dy ⊗ F where E = B T H+1 ⊗ Xr •,1 . . . B T H+1 ⊗ Xr •,dy .</formula><p>Then, by the necessary condition with the Schur complement <ref type="bibr">(Zhang, 2006, theorem 1.20, p. 44)</ref>, M H+1 0 implies</p><formula xml:id="formula_53">0 = ((I dy ⊗ I dH ) − (I dy ⊗ F )(I dy ⊗ F ) − )((C ⊗ B H+1 W 1 XX T ) + E T ) ⇒ 0 = (I dy ⊗ I dH − F F − )(C ⊗ B H+1 W 1 XX T ) + (I dy ⊗ I dH − F F − )E T = (I dy ⊗ I dH − F F − )E T =    I dH − F F − ⊗ I 1 0 . . . 0 I dH − F F − ⊗ I 1       B H+1 ⊗ (Xr •,1 ) T . . . B H+1 ⊗ (Xr •,dy ) T    =    (I dH − F F − )B H+1 ⊗ (Xr •,1 ) T . . . (I dH − F F − )B H+1 ⊗ (Xr •,dy ) T   </formula><p>where the second line follows the fact that (I dy ⊗ F ) − can be replaced by (I dy ⊗ F − ) (see Appendix A.7). The third line follows the fact that</p><formula xml:id="formula_54">(I − F F − )B H+1 W 1 X = 0 because R(B H+1 W 1 X) = R(B H+1 W 1 XX T W T 1 B T H+1 ) = R(F ).</formula><p>In the fourth line, we expanded E and used the definition of the Kronecker product. It implies</p><formula xml:id="formula_55">F F − B H+1 = B H+1 or Xr = 0.</formula><p>Here, if Xr = 0, we have obtained the statement of the lemma. Thus, from now on, we focus on the case where F F − B H+1 = B H+1 and Xr = 0 to obtain the other condition, C(C T C) − C T = U pU p.</p><p>For the (C(C T C) − C T = U pU p) condition: By using another necessary condition of a matrix being positive semidefinite with the Schur complement <ref type="bibr">(Zhang, 2006, theorem 1.20, p. 44)</ref>, M H+1 0 implies that</p><formula xml:id="formula_56">(I dy ⊗ F ) − C ⊗ BH+1W1XX T + E T (C T C ⊗ XX T ) − C T ⊗ XX T (BH+1W1) T + E 0 (3) Since we can replace (C T C ⊗ XX T ) − by (C T C) − ⊗ (XX T ) −1 (see Appendix A.7</formula><p>), the second term in the left hand side is simplified as</p><formula xml:id="formula_57">C ⊗ BH+1W1XX T + E T (C T C ⊗ XX T ) − C T ⊗ XX T (BH+1W1) T + E = C(C T C) − ⊗ BH+1W1 + E T (C T C) − ⊗ (XX T ) −1 C T ⊗ XX T (BH+1W1) T + E = C(C T C) − C T ⊗ F + E T (C T C) − ⊗ (XX T ) −1 E = C(C T C) − C T ⊗ F + r T X T (XX T ) −1 Xr ⊗ BH+1(C T C) − B T H+1<label>(4)</label></formula><p>In the third line, the crossed terms -C(C T C) − ⊗ B H+1 W 1 E and its transpose -are vanished to 0 because of the following. From Lemma 4.1,</p><formula xml:id="formula_58">I dy ⊗ (W H • • • W 1 X) T T vec(r) = 0 ⇔ W H • • • W 1 Xr = B H+1 W 1 Xr = 0 at any critical point. Thus, C(C T C) − ⊗ BH+1W1 E = C(C T C) − B T H+1 ⊗ BH+1W1Xr•,1 . . . C(C T C) − B T H+1 ⊗ BH+1W1Xr •,dy = 0. The forth line follows E T (C T C) − ⊗ (XX T ) −1 E =      BH+1(C T C) − B T H+1 ⊗ (r•,1) T X T (XX T ) −1 Xr•,1 • • • BH+1(C T C) − B T H+1 ⊗ (r•,1) T X T (XX T ) −1 Xr •,dy . . . . . . . . . BH+1(C T C) − B T H+1 ⊗ (r •,dy ) T X T (XX T ) −1 Xr•,1 • • •BH+1(C T C) − B T H+1 ⊗ (r •,dy ) T X T (XX T ) −1 Xr •,dy      = r T X T (XX T ) −1 Xr ⊗ BH+1(C T C) − B T H+1 ,</formula><p>where the last line is due to the fact that ∀t, (r •,t ) T X T (XX T ) −1 Xr •,t is a scalar and the fact that for any matrix L, r</p><formula xml:id="formula_59">T Lr =     (r•,1) T Lr•,1 • • • (r•,1) T Lr •,dy . . . . . . . . . (r •,dy ) T Lr•,1• • •(r •,dy ) T Lr •,dy     .</formula><p>From equations 3 and 4, M H+1 0 ⇒</p><formula xml:id="formula_60">((I dy − C(C T C) − C T ) ⊗ F ) − r T X T (XX T ) −1 Xr ⊗ B H+1 (C T C) − B T H+1 0. (5)</formula><p>In the following, we simplify equation 5 by first showing that R(C) = R(U I p ) and then simplifying r T X T (XX T ) −1 Xr, F and B H+1 (C T C) − B T H+1 . Showing that R(C) = R(U I p ) (following the proof in <ref type="bibr" target="#b1">Baldi &amp; Hornik, 1989)</ref>: Let P C = C(C T C) − C T be the projection operator on R(C). We first show that P C ΣP C = ΣP C = P C Σ.</p><formula xml:id="formula_61">P C ΣP C = W H+1 • • • W 1 XX T W T 1 • • • W T H+1 = Y X T W T 1 • • • W T H+1 = Y X T (XX T ) −1 XY T P C = ΣP C ,</formula><p>where the first line follows Lemma 4.2, the second line is due to Lemma 4.1 with k = H +1 (i.e., 0 = </p><formula xml:id="formula_62">W H • • • W 1 Xr ⇔ W H+1 • • • W 1 XX T W T 1 • • • W T H = Y X T W T 1 • • • W T H ),</formula><formula xml:id="formula_63">U T C = U T C(C T U U T C) − C T U = U T P C U . Thus, U P U T C U T U ΛU T = P C Σ = ΣP C = U ΛU T U P U T C U T , which implies that P U T C Λ = ΛP U T C .</formula><p>Since the eigenvalues (Λ 1,1 , . . . , Λ dy,dy ) are distinct, this implies that P U T C is a diagonal matrix (otherwise, P U T C Λ = ΛP U T C implies Λ i,i = Λ j,j for i = j, resulting in contradiction). Because P U T C is the orthogonal projector of rank p (as P U T C = U T P C U ), this implies that P U T C is a diagonal matrix with its diagonal entries being ones (p times) and zeros (dy − p times). Thus,</p><formula xml:id="formula_64">C(C T C) − C T = P C = U P U T C U T = U I p U T I p , for some index set I p. This means that R(C) = R(U I p ). Simplifying r T X T (XX T ) −1 Xr: r T X T (XX T ) −1 Xr = (CW 1 X − Y )X T (XX T ) −1 X(X T (CW 1 ) T − Y T ) = CW 1 XX T (CW 1 ) T − CW 1 XY T − Y X T (CW 1 ) T + Σ = P C ΣP C − P C Σ − ΣP C + Σ = Σ − U pΛ I p U T p where P C = C(C T C) − C T = U I p U T</formula><p>I p and the last line follows the facts:</p><formula xml:id="formula_65">P C ΣP C = U I p U T I p U ΛU T U I p U T I p = U I p [I p 0] Λ I p 0 0 Λ −I p I p 0 U T I p = U I p Λ I p U T I p , P C Σ = U I p U T I p U ΛU T = U I p [I p 0] Λ I p 0 0 Λ −I p U T I p U −I p = U T I p Λ I p U I p ,</formula><p>and similarly, ΣP C = U T I p Λ I p U I p . Simplifying F : In the proof of Lemma 4.2, by using Lemma 4.1 with k = 1, we obtained that</p><formula xml:id="formula_66">W 1 = (C T C) − C T Y X T (XX T ) −1 + (I − (C T C) − C T C)L. Also, from Lemma 4.4, we have that Xr = 0 or B H+1 (C T C) − C T C = (C T C(C T C) − B T H+1 ) T = B H+1 .</formula><p>If Xr = 0, we got the statement of the lemma, and so we consider the case of B H+1 (C T C) − C T C = B H+1 . Therefore,</p><formula xml:id="formula_67">B H+1 W 1 = B H+1 (C T C) − C T Y X T (XX T ) −1 . Since F = B H+1 W 1 XX T W T 1 B T H+1 , F = B H+1 (C T C) − C T ΣC(C T C) − B T H+1 .</formula><p>From Lemma 4.4 with</p><formula xml:id="formula_68">k = H + 1, R(B T H+1 ) ⊆ R(C T C) = R(B T H+1 W T H+1 W H+1 B H+1 ) ⊆ R(B T H+1 ), which implies that R(B T H+1 ) = R(C T C). Then, we have R(C(C T C) − B T H+1 ) = R(C) = R(U I p ). Accordingly, we can write it in the form, C(C T C) − B T H+1 = [U I p , 0]G 2 , where 0 ∈ R dy×(d1− p) and G 2 ∈ GL d1 (R) (a d 1 × d 1 invertible matrix). Thus, F = G T 2 U T I p 0 U ΛU T [U I p , 0]G 2 = G T 2 I p 0 0 0 Λ I p 0 0 0 G 2 = G T 2 Λ I p 0 0 0 G 2 . Simplifying B H+1 (C T C) − B T H+1 : From Lemma 4.4, C T C(C T C) − B H+1 = B H+1 (again since we are done if Xr = 0). Thus, B H+1 (C T C) − B T H+1 = B H+1 (C T C) − C T C(C T C) − B T H+1 . As discussed above, we write C(C T C) − B T H+1 = [U I p , 0]G 2 . Thus, B H+1 (C T C) − B T H+1 = G T 2 U T I p 0 [U I p , 0]G 2 = G T 2 I p 0 0 0 G 2 .</formula><p>Putting results together: We use the simplified formulas of C(C T C) − C T , r T X T (XX T ) −1 Xr, F and B H+1 (C T C) − B T H+1 in equation 5, obtaining</p><formula xml:id="formula_69">((I dy − U I p U T I p ) ⊗ G T 2 Λ I p 0 0 0 G 2 ) − (Σ − U pΛ I p U T p ) ⊗ G T 2 I p 0 0 0 G 2 0.</formula><p>Due to Sylvester's law of inertia <ref type="bibr">(Zhang, 2006, theorem 1.5, p. 27)</ref>, with a nonsingular matrix U ⊗ G −1 2 (it is nonsingular because each of U and G −1 2 is nonsingular), the necessary condition is reduced to</p><formula xml:id="formula_70">U ⊗ G −1 2 T (I dy − U I p U T I p ) ⊗ G T 2 Λ I p 0 0 0 G 2 − (Σ − UpΛ I p U T p ) ⊗ G T 2 Ip 0 0 0 G 2 U ⊗ G −1 2 = I dy − Ip 0 0 0 ⊗ Λ I p 0 0 0 − Λ − Λ I' p 0 0 0 ⊗ Ip 0 0 0 = 0 0 0 I (dy− p) ⊗ Λ I p 0 0 0 − 0 0 0 Λ −I p ⊗ Ip 0 0 0 =       0 0 0 Λ I p − (Λ −I p ) 1,1 Ip 0 . . . 0 Λ I p − (Λ −I p ) (dy − p),(dy − p) Ip       0,</formula><p>which implies that for all (i, j) ∈ {(i, j) | i ∈ {1, . . . , p}, j ∈ {1, . . . , (d y − p)}}, (Λ I p ) i,i ≥ (Λ −I p ) j,j . In other words, the index set I p must select the largest p eigenvalues whatever p is. Since C(C T C) − C T = U I p U T I p (which is obtained above), we have that C(C T C) − C T = U pU p in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summarizing the above case analysis</head><formula xml:id="formula_71">, if ∇ 2 L(W ) 0 at a critical point, C(C T C) − C T = U pU p or Xr = 0. rank(W k • • • W 1 ) ≥ p</formula><p>with arbitrarily small perturbation of each entry of W k without changing the value of L(W ). Accordingly, suppose that rank(W k−1 • • • W 1 ) ≥ p. From Lemma 4.4, we have the following necessary condition for the Hessian to be (positive or negative) semidefinite at a critical point: for any k ∈ {2, . . . , H + 1},</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R((W</head><formula xml:id="formula_72">k−1 • • • W 2 ) T ) ⊆ R(C T C) or XrW H+1 • • • W k+1 = 0,</formula><p>where the first condition is shown to imply rank(W H+1</p><formula xml:id="formula_73">• • • W k ) ≥ rank(W k−1 • • • W 2 ) in Corollary 4.5. If the former condition is true, rank(C) ≥ rank(W k−1 • • • W 2 ) ≥ rank(W k−1 • • • W 1 ) ≥ p,</formula><p>which is false in the case being analyzed (i.e., the case where rank(C) &lt; p. If this is not the case, we can immediately conclude the desired statement as it has been already proven for the case where rank(C) ≥ p). Thus, we suppose that the latter condition is true.</p><formula xml:id="formula_74">Let A k = W H+1 • • • W k+1 . Then, for an arbitrary L k , 0 = XrW H+1 • • • W k+1 ⇒W k • • • W 1 = A T k A k − A T k Y X T (XX T ) −1 + (I − (A T k A k ) − A T k A k )L k (8) ⇒W H+1 • • • W 1 = A k A T k A k − A T k Y X T (XX T ) −1 = C(C T C) − C T Y X T (XX T ) −1 = U pU T p Y X T (XX T ) −1</formula><p>, where the last two equalities follow Lemmas 4.2 and 4.6 (since if Xr = 0, we immediately obtain the desired result as discussed above). Taking transpose,</p><formula xml:id="formula_75">(XX T ) −1 XY T A k A T k A k − A T k = (XX T ) −1 XY T U pU T p , which implies that XY T A k A T k A k − A k = XY T U pU p.</formula><p>Since XY T is full rank with d y ≤ d x (i.e., rank(XY T ) = d y ), there exists a left inverse and the solution of the above linear system is unique as</p><formula xml:id="formula_76">((XY T ) T XY T ) −1 (XY T ) T XY T = I, yielding, A k A T k A k − A k = U pU T p (= U p(U T p U p) −1 U T p ). In other words, R(A k ) = R(C) = R(U p).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Suppose that (A</head><formula xml:id="formula_77">T k A k ) ∈ R d k ×d k is nonsingular. Then, since R(A k ) = R(C), rank(C) = rank(A k ) = d k ≥ p min(p, d y )</formula><p>, which is false in the case being analyzed (the case of rank(C) &lt; p). Thus, A T k A k is singular. Notice that for the boundary case with k = H + 1, A T k A k = I dy , which is always nonsingular and thus the proof ends here (i.e., For the case with k = H + 1, since the latter condition, XrW H+1 • • • W k+1 = 0, implies a false statement, the former condition, rank(C) ≥ p, which is the desired statement, must be true).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>If A T</head><p>k A k is singular, it is inferred that we can perturb W k to have rank(W k • • • W 1 ) ≥ min(p, d x ). To see this in a concrete algebraic way, first note that since R(A k ) = R(U p), we can write A k = [U p 0]G k for some G k ∈ GL d k (R) where 0 ∈ R dy×(d k − p) . Then, similarly to the base case with k = 1, we select a general inverse (we can do this because it remains to be a necessary condition as explained above) to be</p><formula xml:id="formula_78">(A T k A k ) − := G −1 k I p 0 0 0 G −T k ,</formula><p>and plugging this into the condition in equation 8: for an arbitrary L k ,</p><formula xml:id="formula_79">W k • • • W 1 = G −1 k U T p Y X T (XX T ) −1 [0 I (d k − p) ]G k L k .<label>(9)</label></formula><p>Here, [0 </p><formula xml:id="formula_80">I (d k − p) ]G k L k ∈ R (d k − p)×dx</formula><formula xml:id="formula_81">I (d k − p) ]G k L k . Since W H+1 • • • W 1 = A k W k • • • W 1 = [U p 0]G k W k • • • W 1 , W H+1 • • • W 1 = [U p 0] U T p Y X T (XX T ) −1 [0 I (d k − p) ]G k L k = U pU T p Y X T (XX T ) −1 ,</formula><p>a simple set containing saddle points with the Hessian having no negative eigenvalue. Suppose that W H = W H−1 = • • • = W 2 = W 1 = 0. Then, from Lemma 4.1, it defines an uncountable set of critical points, in which W H+1 can vary in R dy×dH . Since r = Y T = 0 due to rank(Y ) ≥ 1, it is not a global minimum. To see this, we write</p><formula xml:id="formula_82">L(W ) = 1 2 Y (W, X) − Y 2 F = 1 2 tr(r T r) = 1 2 tr(Y Y T ) − 1 2 tr(W H+1 • • • W 1 XY T ) − 1 2 tr((W H+1 • • • W 1 XY T ) T ) + 1 2 tr(W H+1 • • • W 1 XX T (W H+1 • • • W 1 ) T ).</formula><p>For example, with where we can see that there exists a strictly lower value of L(W ) than the loss value with r = Y T , which is 1 2 tr(Y Y T ) (since X = 0 and rank(Σ) = 0). Thus, these are not global minima, and thereby these are saddle points by Theorem 2.3 (ii) and (iii). On the other hand, from the proof of Lemma 4.3, every diagonal and off-diagonal element of the Hessian is zero if The previous work also assumes the use of "independent random" loss functions. Consider the hinge loss, L hinge (W ) j,i = max(0, 1 − Y j,i Ŷ (W, X) j,i ). By modeling the max operator as a Bernoulli random variable ξ, we can then write L hinge (W ) j,i = ξ−q Ψ p=1 Y j,i [X i ] (j,p) ξ[Z i ] (j,p) H+1 k=1 w</p><formula xml:id="formula_83">W H+1 • • • W 1 = ± U p U T p Y X T (XX) −1 , L<label>(</label></formula><formula xml:id="formula_84">W H = W H−1 = • • • = W 2 = W 1 = 0.</formula><p>(k) (j,p) . A1p then assumes that for all i and (j, p), the ξ[Z i ] (j,p) are Bernoulli random variables with equal probabilities of success. Furthermore, A5u assumes that the independence of ξ[Z i ] (j,p) , Y j,i [X i ] (j,p) , and w (j,p) . Finally, A6u assumes that Y j,i [X i ] (j,p) for all (j, p) and i are independent. In section 3.2, we discuss the effect of all of the seven previous assumptions to see why these are unrealistic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Discussion of the 1989 conjecture</head><p>The 1989 conjecture is based on the result for a 1-hidden layer network with p &lt; d y = d x (e.g., an autoencoder). That is, the previous work considered Y = W 2 W 1 X with the same loss function as ours with the additional assumption p &lt; d y = d x . The previous work denotes A W 2 and B W 1 .</p><p>The conjecture was expressed by <ref type="bibr" target="#b1">Baldi &amp; Hornik (1989)</ref> as Our results, and in particular the main features of the landscape of E, hold true in the case of linear networks with several hidden layers.</p><p>Here, the "main features of the landscape of E" refers to the following features, among other minor technical facts: 1) the function is convex in each matrix A (or B) when fixing other B (or A), and 2) every local minimum is a global minimum. No proof was provided in this work for this conjecture.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Corollary 3. 2 (</head><label>2</label><figDesc>Loss surface of deep nonlinear networks) Assume A1p-m and A5u-m. Let q = ρ −1 . Then, we can reduce the loss function of the deep nonlinear model L(W ) to that of the deep linear model L(W ). Therefore, with the same set of conditions as in Theorem 2.3, the loss function of the deep nonlinear model has the following properties: (i) It is non-convex and non-concave. (ii) Every local minimum is a global minimum. (iii) Every critical point that is not a global minimum is a saddle point. (iv) The saddle points have the properties stated in Theorem 2.3 (iv) and Corollary 2.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>min(d y , p)}, we define a matrix containing the subset of the corresponding eigenvectors as U I p = [u i1 , . . . , u ip ]. Note the difference between U p and U I p .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Lemma 4. 6 (</head><label>6</label><figDesc>Hessian positive semidefinite necessary condition) If ∇ 2 L(W ) is positive semidefinite at a critical point, then C(C T C) − C T = U pU T p or Xr = 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Case I: rank(W H • • • W 2 ) = p and d y ≤ p: If d y &lt; p, Corollary 4.5 with k = H + 1 implies the necessary condition of local minima that Xr = 0. If d y = p, Lemma 4.6 with k = H + 1 and k = 2, combined with the fact that R(C) ⊆ R(Y X T ), implies the necessary condition that Xr = 0. Therefore, we have the necessary condition of local minima, Xr = 0 . Interpreting condition Xr = 0, we conclude that W achieving Xr = 0 is indeed a global minimum. Case II: rank(W H • • • W 2 ) = p and d y &gt; p: From Lemma 4.6, we have the necessary condition that C(C T C) − C T = U pU T p or Xr = 0. If Xr = 0, using the exact same proof as in Case I, it is a global minimum. Suppose then that C(C T C) − C T = U pU T p . From Lemma 4.4 with k = H + 1, we conclude that p rank(C) = p. Then, from Lemma 4.2, we write</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Case III: rank(W H • • • W 2 ) &lt; p: We first show that if rank(C) ≥ min(p, d y ), every local minimum is a global minimum. Thus, we consider the case where rank(W H • • • W 2 ) &lt; p and rank(C) &lt; min(p, d y ). In this case, by induction on k = {1, . . . , H +1}, we prove that we can have rank(W k • • • W 1 ) ≥ min(p, d y ) with arbitrarily small perturbation of each entry of W k , . . . , W 1 without changing the value of L(W ). Once this is proved, along with the results of Case I and Case II, we can immediately conclude that any point satisfying the definition of a local minimum is a global minimum.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>the third line follows Lemma 4.2, and the fourth line uses the definition of Σ. Since P C ΣP C is symmetric, ΣP C (= P C ΣP C ) is also symmetric and hence ΣP C = (ΣP C ) T = P T C Σ T = P C Σ. Thus, P C ΣP C = ΣP C = P C Σ. Note that P C = U P U T C U T as P</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>is the last (d k − p) rows of G k L k . Since rank(Y X T (XX T ) −1 ) = d y ,the first p rows in the above have rank p. Thus, W k • • • W 1 has rank at least p and the possible rank deficiency comes from the last (d k − p) rows, [0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Y T ) − tr(U p U T p Σ) − tr(ΣU p U T p ) + tr(U p U T p ΣU p U Y T ) − tr(U p Λ 1:p U T p</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Thus, the Hessian is simply a zero matrix, which has no negative eigenvalue.C.2 Proof of Corollary 3.2 and discussion of the assumptions used in the previous work ProofSince E Z [ Ŷ (W, X)] = qρ Ψ p=1 [X i ] (j,p) H+1 k=1 w (j,p) = Y , L(W ) = 1 2 E Z [ Ŷ (W, X) − Y ] F = 1 2 E Z [ Ŷ (W, X)] − Y 2 F = L(W ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Theorem 2.3 (Loss surface of deep linear networks) Assume that XX T and XY T are of full rank with d y ≤ d x and Σ has d y distinct eigenvalues. Then, for any depth H ≥ 1 and for any layer widths and any input-output dimensions d y , d H , d H−1 , . . . , d 1 , d x ≥ 1 (the widths can arbitrarily differ from each other and from d y and d x ), the loss function L(W ) has the following properties:</figDesc><table><row><cell>(i) It is non-convex and non-concave.</cell></row><row><cell>(ii) Every local minimum is a global minimum.</cell></row><row><cell>(iii) Every critical point that is not a global minimum is a saddle point.</cell></row></table><note>(iv) If rank(W H • • • W 2 ) = p, then the Hessian at any saddle point has at least one (strictly) negative eigenvalue. 1Corollary 2.4 (Effect of deepness on the loss surface) Assume the same set of conditions as in Theorem 2.3 and consider the loss function L(W ). For three-layer networks (i.e., H = 1), the Hessian at any saddle point has at least one (strictly) negative eigenvalue. In contrast, for networks deeper than three layers (i.e., H ≥ 2), there exist saddle points at which the Hessian does not have any negative eigenvalue.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="30" xml:id="foot_0">30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1">If H = 1, to be succinct, we define WH • • • W2 = W1 • • • W2 I d 1 ,with a slight abuse of notation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2">Typically, we do this by assuming smoothness in the values of the loss function.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3">Other problems such as the ill-conditioning can make it difficult to obtain a fast convergence rate.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4">For a non-convex and non-differentiable function, we can still have a first-order and second-order necessary condition (e.g.,Rockafellar &amp; Wets, 2009, theorem 13.24, p. 606).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5">proof: any point satisfying Xr ′ = 0 is a critical point of f , which directly follows the proof of Lemma 4.1. Also, f is convex since its Hessian is positive semidefinite for all input WH+1, and thus any critical point of f is a global minimum. Combining the pervious two statements results in the desired claim</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The author would like to thank Prof. Leslie Kaelbling, Quynh Nguyen, Li Huan and Anirbit Mukherjee for their thoughtful comments on the paper. We gratefully acknowledge support from NSF grant 1420927, from ONR grant N00014-14-1-0486, and from ARO grant W911NF1410433.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 Generalized inverse of Kronecker product</head><p>Proof For a matrix M , the definition of a generalized inverse, M − , is M M − M = M . Setting M := A ⊗ B, we check if (A − ⊗ B − ) satisfies the definition:</p><p>Here, we are not claiming that (A − ⊗ B − ) is the unique generalized inverse of A ⊗ B. Notice that the necessary condition that we have in our proof (where we need a generalized inverse of A ⊗ B) is for any generalized inverse of A ⊗ B. Thus, replacing it by one of any generalized inverse suffices to obtain a necessary condition. Indeed, choosing Moore−Penrose pseudoinverse suffices here, with which we know (A ⊗ B) † = (A † ⊗ B † ). But, to give a simpler argument later, we keep more generality by choosing (A − ⊗ B − ) as a generalized inverse of A ⊗ B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proof of Theorem 2.3</head><p>We complete the proofs of Theorem 2.3. Since we heavily rely on the necessary conditions of local minima, we remind the reader of the elementary logic: for a point to be a local minimum, it must satisfy all the necessary conditions of local minima, but a point satisfying the necessary conditions can be a point that is not a local minimum (in contrast, a point satisfying the sufficient condition of local minimum is a local minimum).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Proof of Theorem 2.3 (ii)</head><p>Proof By case analysis, we show that any point that satisfies the necessary conditions and the definition of a local minimum is a global minimum. When we write a statement in the proof, we often mean that a necessary condition of local minima implies the statement as it should be clear (i.e., we are not claiming that the statement must hold true unless the point is the candidate of local minima.).</p><p>Case I: rank(W H • • • W 2 ) = p and d y ≤ p: Assume that rank(W H • • • W 2 ) = p. We first obtain a necessary condition of the Hessian being positive semidefinite at a critical point, Xr = 0, and then interpret the condition. If d y &lt; p, Corollary 4.5 with k = H + 1 implies the necessary condition that Xr = 0. This is because the other condition p &gt; rank(W H+1 ) ≥ rank(W</p><p>From Corollary 4.5 with k = 2 implies the necessary condition that rank(C) ≥ rank(I d1 ) or XrW H+1 • • • W 3 = 0. Suppose the latter:</p><p>, where the last equality follows the fact that (Xr)</p><p>and thereby the projection of Y X T onto the range of C is Y X T . Therefore, we have the condition, Xr = 0 when d y ≤ p.</p><p>To interpret the condition Xr = 0, consider a loss function with a linear model without any hidden layer, f</p><p>T be the corresponding error matrix. Then, any point satisfying Xr ′ = 0 is known to be a global minimum of f by its convexity. 5 For any values of</p><p>the opposite is also true when d y ≤ p although we don't need it in our proof). That is, image( L) ⊆ image(f ) and image(r) ⊆ image(r ′ ) (as functions of W and W ′ respectively) (the equality is also true when d y ≤ p although we don't need it in our proof). Summarizing the above, whenever Xr = 0, there exists W ′ = W H+1 • • • W 1 such that Xr = Xr ′ = 0, which achieves the global minimum value of f (f * ) and f * ≤ L * (i.e., the global minimum value of f is at most the global minimum value of L since image( L) ⊆ image(f )). In other words, W H+1 • • • W 1 achieving Xr = 0 attains a global minimum value of f that is at most the global minimum value of L. This means that W H+1 • • • W 1 achieving Xr = 0 is a global minimum. Thus, we have proved that when rank(W H • • • W 2 ) = p and d y ≤ p, if ∇ 2 L(W ) 0 at a critical point, it is a global minimum. </p><p>. Thus, we can rewrite the above equation as</p><p>, which is the orthogonal projection on to subspace spanned by the p eigenvectors corresponding to the p largest eigenvalues following the ordinary least square regression matrix. This is indeed the expression of a global minimum <ref type="bibr" target="#b1">(Baldi &amp; Hornik, 1989;</ref><ref type="bibr" target="#b2">Baldi &amp; Lu, 2012)</ref>. Thus, we have proved that when rank(W For this case, we have already proven the desired statement above. On the other hand, if p &gt; d y , we have p rank(C) ≥ d y . Thus,</p><p>, which is a global minimum. We can see this in various ways. For example, Xr = XY T U U T − XY T = 0, which means that it is a global minimum as discussed above.</p><p>Thus, in the following, we consider the remaining case where rank(W H • • • W 2 ) &lt; p and rank(C) &lt; p. In this case, we show that we can have rank(C) ≥ p with arbitrarily small perturbations of each entry of W H+1 , . . . , W 1 , without changing the loss value. In order to show this, by induction on k = {1, . . . , H + 1}, we prove that we can have rank(W k • • • W 1 ) ≥ p with arbitrarily small perturbation of each entry of W k , . . . , W 1 without changing the value of L(W ).</p><p>We start with the base case with k = 1. For convenience, we reprint a necessary condition of local minima that is represented by equation 2 in the proof of Lemmas 4.2: for an arbitrary L 1 ,</p><p>To see this in a concrete algebraic way, first note that from Lemma 4.6, R(C) = R(U p) or Xr = 0. If Xr = 0, with the exact same proof as in the previous case, it is a global minimum. So, we consider the case of p) . Thus,</p><p>Again, note that the set of all generalized inverse of G T 1 I p 0 0 0 G 1 is as follows <ref type="bibr">(Zhang, 2006, p. 41)</ref>:</p><p>Since equation 6 must necessarily hold for any generalized inverse in order for a point to be a local minimum, we choose a generalized inverse with</p><p>By plugging this into equation 6, we obtain the following necessary condition of local minima: for an arbitrary L 1 ,</p><p>Here, [0</p><p>because the multiplication with the invertible matrix preserves the rank), the submatrix with the first p rows in the above have rank p. Thus, W 1 has rank at least p, and the possible rank deficiency comes from the last (d</p><p>This means that changing the values of the last (d 1 − p) rows of G 1 L 1 (i.e., [0 I (d1− p) ]G 1 L 1 ) does not change the value of L(W ). Thus, we consider the perturbation of each entry of W 1 as follows:</p><p>Here, with an appropriate choice of M ptb , we can make W1 to be full rank (see footnote 6 for the proof of the existence of such M ptb ). 6</p><p>Thus, we have shown that we can have rank(W 1 ) ≥ min(d 1 , d x ) ≥ min(p, d y ) = p with arbitrarily small perturbation of each entry of W 1 with the loss value being unchanged. This concludes the proof for the base case of the induction with k = 1.</p><p>For the inductive step 7 with k ∈ {2, . . . , H + 1}, we have the inductive hypothesis that we can have rank(W k−1 • • • W 1 ) ≥ p with arbitrarily small perturbations of each entry of W k−1 , . . . W 1 without changing the loss value. Here, we want to show that if rank(W k−1 • • • W 1 ) ≥ p, we can have 6 In this footnote, we prove the existence of ǫMptb that makes W1 full rank. Although this is trivial since the set of full rank matrices is dense, we show a proof in the following to be complete. Let p′ ≥ p be the rank of</p><p>, there exist p′ linearly independent row vectors including the first p row vectors, denoted by b1, . . . , b p′ ∈ R 1×dx . Then, we denote the rest of row vectors by v1, v2, . . . , v d 1 − p′ ∈ R 1×dx . Let c = min(d1 − p′ , dx − p′ ). There exist linearly independent vectors v1, v2, . . . , vc such that the set, {b1, . . . , b p′ , v1, v2, . . . , vc}, is linearly independent. Setting vi := vi + ǫvi for all i ∈ {1, . . . , c} makes W1 full rank since ǫvi cannot be expressed as a linear combination of other vectors. Thus, a desired perturbation matrix ǫMptb can be obtained by setting ǫMptb to consist of ǫv1, ǫv2, . . . , ǫvc row vectors for the corresponding rows and 0 row vectors for other rows. 7 The boundary cases with k = 2 and k = H + 1 as well pose no problem during the proof for the inductive step: remember our notational definition,</p><p>which means that changing the values of the last (d k − p) rows does not change the value of L(W ).</p><p>We consider the perturbation of each entry of W k as follows. From equation 9, all the possible solutions of W k can be written as: for an arbitrary L 0 k and L k ,</p><p>where</p><p>where</p><p>, where the second line follows equation 9 and the third line is due to the fact that</p><p>Here, we can construct M ptb such that rank( Wk B k ) ≥ p as follows. Let p′ ≥ p be the rank of Wk B k . That is, in </p><p>. Thus, as a result of our perturbation, the original row vectors v 1 , v 2 , . . . , v ( p− p′ ) are perturbated as v i := v i + ǫv i for all i ∈ {1, . . . , p − p′ }, which guarantees rank( Wk B k ) ≥ p since ǫv i cannot be expressed as a linear combination of other row vectors (b 1 , . . . , b p′ and ∀j = i, vj ) by its construction. Therefore, we have that rank(W k • • • W 1 ) ≥ p upon such a perturbation on W k without changing the loss value. Thus, we conclude the induction, proving that we can have rank(W H+1 • • • W 1 ) ≥ p with arbitrarily small perturbation of each parameter without changing the value of L(W ). Since rank(C) ≥ rank(W H+1 • • • W 1 ) ≥ p, upon such a perturbation, we have the case where rank(C) ≥ p, for which we have already proven that a critical point is not a local minimum unless it is a global minimum. This concludes the proof of the case where rank(W H • • • W 2 ) &lt; p.</p><p>Summarizing the above, any point that satisfies the definition (and necessary conditions) of a local minimum is a global minimum, concluding the proof of Theorem 2.3 (ii).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Proof of Theorem 2.3 (i)</head><p>Proof We can prove the non-convexity and non-concavity from its Hessian (Theorem 2.3 (i)). First, consider L(W ). For example, from Corollary 4.5 with k = H + 1, it is necessary for the Hessian to be positive or negative semidefinite at a critical point that rank(W H+1 ) ≥ rank(W H • • • W 2 ) or Xr = 0. The instances of W unsatisfying this condition at critical points form some uncountable set. As an example, consider a uncountable set that consists of the points with W H+1 = W 1 = 0 and with any W H , . . . , W 2 . Then, every point in the set defines a critical point from Lemma 4.1. Also, Xr = XY T = 0 as rank(XY T ) ≥ 1. So, it does not satisfy the first semidefinite condition. On the other hand, with any instance of</p><p>. So, it does not satisfy the second semidefinite condition as well. Thus, we have proven that in the domain of the loss function, there exist points, at which the Hessian becomes indefinite. This implies Theorem 2.3 (i): the functions are non-convex and non-concave.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Proof of Theorem 2.3 (iii)</head><p>Proof We now prove Theorem 2.3 (iii): every critical point that is not a global minimum is a saddle point. Here, we want to show that if the Hessian is negative semidefinite at a critical point, then there is a increasing direction so that there is no local maximum. From Lemma 4.3 with k = 1,</p><p>The positive semidefiniteness follows the fact that</p><p>T has at least one strictly positive eigenvalue (by the spectrum property of Kronecker product). Thus, with other variables being fixed, if W H+1 • • • W 2 = 0, with respect to W 1 at any critical point, there exists some increasing direction that corresponds to the strictly positive eigenvalue. This means that there is no local maximum if</p><p>we claim that at a critical point, if the Hessian is negative semidefinite (i.e., a necessary condition of local maxima), we can make W H+1 • • • W 2 = 0 with arbitrarily small perturbation of each parameter without changing the loss value. We can prove this by using the similar proof procedure to that used for Theorem 2.3 (ii) in the case of rank(W</p><p>By induction on k = {2, . . . , H + 1}, we prove that we can have W k • • • W 2 = 0 with arbitrarily small perturbation of each entry of W k , . . . , W 2 without changing the loss value.</p><p>We start with the base case with k = 2. From Lemma 4.4, we have a following necessary condition for the Hessian to be (positive or negative) semidefinite at a critical point:</p><p>where the first condition is shown to imply rank(W</p><p>From the latter condition, for an arbitrary L 2 ,</p><p>where the last follows the critical point condition (Lemma 4.2). Then, similarly to the proof of Theorem 2.3 (ii),</p><p>In other words, R(A 2 ) = R(C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Suppose that rank(A</head><p>we have that rank(C) ≥ 1, which is false (or else the desired statement). Thus, rank(A T 2 A 2 ) = 0, which implies that A 2 = 0. Then, since W H+1 • • • W 1 = A 2 W 2 W 1 with A 2 = 0, we can have W 2 = 0 without changing the loss value with arbitrarily small perturbation of W 2 .</p><p>For the inductive step with k = {3, . . . , H + 1}, we have the inductive hypothesis that we can have W k−1 • • • W 2 = 0 with arbitrarily small perturbation of each parameter without changing the loss value. Accordingly, suppose that</p><p>which is false (or the desired statement). If the latter is true, for an arbitrary L 1 ,</p><p>, where the last follows the critical point condition (Lemma 4.2). Then, similarly to the above,</p><p>In other words, R(A k ) = R(C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Suppose that rank(A</head><p>without changing the loss value with arbitrarily small perturbation of each parameter. Thus, we conclude the induction, proving that if W H+1 • • • W 2 = 0, with arbitrarily small perturbation of each parameter without changing the value of L(W ), we can have W H+1 • • • W 2 = 0. Thus, at any candidate point for local maximum, the loss function has some strictly increasing direction in an arbitrarily small neighborhood. This means that there is no local maximum. Thus, we obtained the statement of Theorem 2.3 (iii).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Proof of Theorem 2.3 (iv)</head><p>Proof In the proof of Theorem 2.3 (ii), the case analysis with the case, rank(W</p><p>0 at a critical point, W is a global minimum. Thus, when rank(W H • • • W 2 ) = p, if W is not a global minimum at a critical point, its Hessian is not positive semidefinite, containing some negative eigenvalue. From Theorem 2.3 (ii), if it is not a global minimum, it is not a local minimum. From Theorem 2.3 (iii), it is a saddle point. Thus, if rank(W H • • • W 2 ) = p, the Hessian at any saddle point has some negative eigenvalue, which is the statement of Theorem 2.3 (iv).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Proofs of Corollaries 2.4 and 3.2</head><p>We complete the proofs of Corollaries 2.4 and 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Proof of Corollary 2.4</head><p>Proof</p><p>p", which is always true. This is because p is the smallest width of hidden layers and there is only one hidden layer, the width of which is d 1 . Thus, Theorem 2.3 (iv) immediately implies the statement of Corollary 2.4. For the statement of Corollary 2.4 with H ≥ 2, it is suffice to show the existence of In 2012, the proof for the conjecture corresponding to the first feature (convexity in each matrix A (or B) when fixing other B (or A)) was provided in <ref type="bibr" target="#b2">(Baldi &amp; Lu, 2012)</ref> for both real-valued and complex-valued cases, while the proof for the conjecture for the second feature (every local minimum being a global minimum) was left for future work.</p><p>In <ref type="bibr" target="#b0">(Baldi, 1989)</ref>, there is an informal discussion regarding the conjecture. Let i ∈ {1, • • • , H} be an index of a layer with the smallest width p. That is, d i = p. We write</p><p>Then, what A and B can represent is the same as what the original A := W 2 and B := W 1 , respectively, can represent in the 1-hidden layer case, assuming that p &lt; d y = d x (i.e., any element in R dy×p and any element in R p×dx ). Thus, we would conclude that all the local minima in the deeper models always correspond to the local minima of the collapsed 1-hidden layer version with</p><p>However, the above reasoning turns out to be incomplete. Let us prove the incompleteness of the reasoning by contradiction in a way in which we can clearly see what goes wrong. Suppose that the reasoning is complete (i.e., the following statement is true: if we can collapse the model with the same expressiveness with the same rank restriction, then the local minima of the model correspond to the local minima of the collapsed model). Consider f (w) = W 3 W 2 W 1 = 2w 2 + w 3 , where W 1 = [w w w], W 2 = [1 1 w] T and W 3 = w. Then, let us collapse the model as a := W 3 W 2 W 1 and g(a) = a. As a result, what f (w) can represent is the same as what g(a) can represent (i.e., any element in R) with the same rank restriction (with a rank of at most one). Thus, with the same reasoning, we can conclude that every local minimum of f (w) corresponds to a local minimum of g(a). However, this is clearly false, as f (w) is a non-convex function with a local minimum at w = 0 that is not a global minimum, while g(a) is linear (convex and concave) without any local minima. The convexity for g(a) is preserved after the composition with any norm. Thus, we have a contradiction, proving the incompleteness of the reasoning. What is missed in the reasoning is that even if what a model can represent is the same, the different parameterization creates different local structure in the loss surface, and thus different properties of the critical points (global minima, local minima, saddle points, and local maxima). Now that we have proved the incompleteness of this reasoning, we discuss where the reasoning actually breaks down in a more concrete example. From Lemmas 4.1 and 4.2, if H = 1, we have the following representation at critical points:</p><p>where A := W 2 and B := W 1 . In contrast, from Lemmas 4.1 and 4.2, if H is arbitrary,</p><p>where A := W H+1 • • • W i+1 and B := W i • • • W 1 as discussed above, and C = W H+1 • • • W 2 . Note that by using other critical point conditions from Lemmas 4.1, we cannot obtain an expression such that C = A in the above expression unless i = 1. Therefore, even though what A and B can represent is the same, the critical condition becomes different (and similarly, the conditions from the Hessian). Because the proof in the previous work with H = 1 heavily relies on the fact that AB = A(A T A) − A T Y X T (XX T ) −1 , the same proof does not apply for deeper models (we may continue providing more evidence as to why the same proof does not work for deeper models, but one such example suffices for the purpose here).</p><p>In this respect, we have completed the proof of the conjecture and also provided a complete analytical proof for more general and detailed statements; that is, we did not assume that p &lt; d y = d x , and we also proved saddle point properties with negative eigenvalue information.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Linear learning: Landscapes and algorithms</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Baldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural networks and principal component analysis: Learning from examples without local minima</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Baldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Hornik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="58" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Complex-valued autoencoders</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Baldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqin</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="136" to="147" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Training a 3-node neural network is NP-complete</title>
		<author>
			<persName><forename type="first">Avrim</forename><forename type="middle">L</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><surname>Rivest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ronald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="117" to="127" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The Loss Surfaces of Multilayer Networks</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName><surname>Mikael</surname></persName>
		</author>
		<author>
			<persName><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><surname>Arous</surname></persName>
		</author>
		<author>
			<persName><surname>Gerard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics</title>
				<meeting>the Eighteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2015">2015a</date>
			<biblScope unit="page" from="192" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Open Problem: The landscape of the loss surfaces of multilayer networks</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><surname>Yann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gérard</forename><surname>Arous</surname></persName>
		</author>
		<author>
			<persName><surname>Ben</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 28th Conference on Learning Theory</title>
				<meeting>The 28th Conference on Learning Theory</meeting>
		<imprint>
			<date type="published" when="2015">2015b</date>
			<biblScope unit="page" from="1756" to="1760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Identifying and attacking the saddle point problem in high-dimensional non-convex optimization</title>
		<author>
			<persName><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><surname>Razvan</surname></persName>
		</author>
		<author>
			<persName><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><surname>Caglar</surname></persName>
		</author>
		<author>
			<persName><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><surname>Kyunghyun</surname></persName>
		</author>
		<author>
			<persName><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><surname>Surya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2933" to="2941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Escaping From Saddle Points-Online Stochastic Gradient for Tensor Decomposition</title>
		<author>
			<persName><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Furong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 28th Conference on Learning Theory</title>
				<meeting>The 28th Conference on Learning Theory</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="797" to="842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deep Learning. Book in preparation for</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Yoshua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org" />
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On the computational efficiency of training neural networks</title>
		<author>
			<persName><forename type="first">Roi</forename><surname>Livni</surname></persName>
		</author>
		<author>
			<persName><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName><surname>Shai</surname></persName>
		</author>
		<author>
			<persName><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><surname>Ohad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="855" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning Real and Boolean Functions: When Is Deep Better Than Shallow</title>
		<author>
			<persName><surname>Mhaskar</surname></persName>
		</author>
		<author>
			<persName><surname>Hrushikesh</surname></persName>
		</author>
		<author>
			<persName><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><surname>Qianli</surname></persName>
		</author>
		<author>
			<persName><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><surname>Tomaso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Massachusetts Institute of Technology CBMM Memo</title>
		<imprint>
			<biblScope unit="issue">45</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Some NP-complete problems in quadratic and nonlinear programming</title>
		<author>
			<persName><forename type="first">Katta</forename><forename type="middle">G</forename><surname>Murty</surname></persName>
		</author>
		<author>
			<persName><surname>Kabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Santosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical programming</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="117" to="129" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Variational analysis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rockafellar</surname></persName>
		</author>
		<author>
			<persName><surname>Tyrrell</surname></persName>
		</author>
		<author>
			<persName><surname>Wets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J-B</forename><surname>Roger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">317</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><surname>Surya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Fuzhen</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">The Schur complement and its applications</title>
				<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
