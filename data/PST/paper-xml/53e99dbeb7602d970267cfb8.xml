<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Particle Swarm Optimization for Integer Programming</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Laskari</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Mathematics</orgName>
								<orgName type="department" key="dep2">Artificial Intelligence Research Center (UPAIRC)</orgName>
								<orgName type="institution">University of Patras</orgName>
								<address>
									<postCode>GR-26110</postCode>
									<settlement>Patras</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Parsopoulos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Mathematics</orgName>
								<orgName type="department" key="dep2">Artificial Intelligence Research Center (UPAIRC)</orgName>
								<orgName type="institution">University of Patras</orgName>
								<address>
									<postCode>GR-26110</postCode>
									<settlement>Patras</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Vrahatis</surname></persName>
							<email>vrahatis@math.upatras.gr</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Mathematics</orgName>
								<orgName type="department" key="dep2">Artificial Intelligence Research Center (UPAIRC)</orgName>
								<orgName type="institution">University of Patras</orgName>
								<address>
									<postCode>GR-26110</postCode>
									<settlement>Patras</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Particle Swarm Optimization for Integer Programming</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6A31ED91114C2942436E5DEBEE05856D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T10:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The investigation of the performance of the Particle Swarm Optimization (PSO) method in Integer Programming problems, is the n d ~~ theme Of the present paper. Three variants Of pso are technique, on several Integer Programming test problems. Results indicate that PSO handles efficiently such problems, and in most cases it outperforms the Branch and Bound technique.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>where Z is the set of integers, and s is a not necessarily bounded set, which is considered as the feasible region. Maximization of Integer Programming problems is very common in the literature, but we will consider lem can be easily transformed to a minimization problem and vice versa. The problem defined in Eq. (1) is often called "All-Integer Programming Problem", since all the variables are integers, in contrast to the "Mixed-Integer with the used Branch and Bound only the minimization case, since a maximization prob-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>A remarkably wide variety of problems can be represented as discrete optimization models <ref type="bibr" target="#b14">[17]</ref>. An important area of application concerns the efficient management of a limited number of resources so as t o increase productivity and/or profit. Such applications are encountered in Operational Research problems such as goods distribution, production scheduling, and machine sequencing. There are applications in mathematics to the subjects of combinatorics, graph theory and logic.</p><p>Statistical applications include problems of data analysis and reliability. Recent scientific applications involve problems in molecular biology, high energy physics and x-ray crystallography. A political application concerns the division of a region into election districts <ref type="bibr" target="#b14">[17]</ref>. Capital budgeting, portfolio analysis, network and VLSI circuit design, as well as automated production systems are some more applications in which Integer Programming problems are met [ 171.</p><p>Yet another, recent, and promising application is the training of neural networks with integer weights, where the activation function and weight values are confined in a narrow band of integers. Such neural networks are better suited for hardware implementations compared to real weight ones <ref type="bibr">[26]</ref>.</p><p>The Unconstrained Integer Programming problem can be defined as minf(z), z E S c Z", z 0-7803-7282-4/02/$10.00 02002 IEEE Programming Problem", where some of the variables are real.</p><p>Optimization techniques developed for real search spaces can be applied on Integer Programming problems and determine the optimum solution by rounding off the real optimum values to the nearest integer [17], <ref type="bibr" target="#b25">[28]</ref>. One of the most common deterministic approaches for tackling Integer Programming problems, is the Branch and Bound (BB) technique [lo], <ref type="bibr" target="#b15">[18]</ref>, <ref type="bibr" target="#b25">[28]</ref>. According to this technique, the initial feasible region is split into several sub-regions. For each one of these sub-regions, a constrained sub-problem is solved, treating the integer problem as a continuous one. The procedure is repeated until the real variables are fixed to integer values.</p><p>Evolutionary and Swarm Intelligence algorithms are stochastic optimization methods that involve algorithmic mechanisms similar to natural evolution and social behavior respectively. They can cope with problems that involve discontinuous objective functions and disjoint search spaces [7], [14], [30]. Genetic Algorithms (GA), Evolution Strategies (ES), and the Particle Swarm Optimizer (PSO) are the most common paradigms of such methods. GA and ES draw from principles of natural evolution which are regarded as rules in the optimization process. On the other hand, PSO is based on simulation of social behavior. Early approaches in the direction of Evolutionary Algorithms for Integer Programming are reported in [8], [Ill.</p><p>In GA, the potential solutions are encoded in binary bit strings. Since the integer search space, of the prob-lem defined in Eq. ( l ) , is potentially not bounded, the representation of a solution using a fixed length binary string is not feasible <ref type="bibr" target="#b26">[29]</ref>. Alternatively, ES can be used, by embedding the search space Z" into R" and truncating the real values to integers. However, this approach is not always efficient due to the existence of features of ES, which contribute to the detection of real valued minima with arbitrary accuracy. These features are not always needed in integer spaces, since the smallest distance of two points, in -!I-norm, is equal to 1 <ref type="bibr" target="#b26">[29]</ref>. This paper aims to investigate, the performance of the PSO method on Integer Programming problems. The truncation of real values to integers seems not t o affect significantly the performance of the method, as the experimental results indicate. Moreover, PSO outperforms the BB technique for most test problems.</p><p>The rest of the paper is organized as follows: in Section 11, the PSO method is described. In Section 111, the BB algorithm is briefly exposed. In Section IV, the experimental results are reported, and conclusions are reported in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">THE PARTICLE SWARM OPTIMIZATION METHOD PSO is a Swarm Intelligence method for global optimization. It differs from other well-known Evolutionary Algorithms (EA) [2], [4], [7], [14], [30].</head><p>As in EA, a population of potential solutions is used to probe the search space, but no operators, inspired by evolution procedures, are applied on the population to generate new promising solutions. Instead, in PSO, each individual, named particle, of the population, called s w a m , adjusts its trajectory toward its own previous best position, and toward the previous best position attained by any member of its topological neighborhood [12]. In the global variant of PSO, the whole swarm is considered as the neighborhood. Thus, global sharing of information takes place and the particles profit from the discoveries and previous experience of all other companions during the search for promising regions of the landscape. For example, in the single-objective minimization case, such regions possess lower function values than others, visited previously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Several variants of the PSO technique have been proposed so far, following Eberhart and Kennedy [4], [13],</head><p>[14]. In our experiments, three different global versions of PSO were investigated. They are all defined using the equations, described in the following paragraph [14].</p><p>First, let us define the notation adopted in this paper: assuming that the search space is D-dimensional, the i-th particle of the swarm is represented by the Ddimensional vector Xi = (zil, x i 2 , . . . , z i ~) and the best 0-7803-7282-4/02/$10.00 02002 IEEE -1583 particle of the swarm, i.e. the particle with the smallest function value, is denoted by index g. The best previous position (i.e. the position giving the lowest function value) of the i-th particle is recorded and represented as P i = ( P ~I , P ~Z , .</p><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. , P ~D ) ,</head><p>and the position change (velocity) of the i-th particle is K = ( w i l , wi2, . . . , w ~D ) .</p><p>The particles are manipulated according to the following equations (the superscripts denote the iteration):</p><p>(3) where i = 1 , 2 , . . . , N ; N is the swarm's size; x is a constriction factor used to control and constrict velocities; w is the inertia weight; c1 and c2 are two positive constants, called the cognitive and social parameter respectively; r z and r&amp; are two random numbers uniformly distributed within the range [0,1].</p><p>Eq. (2) is used to calculate at each iteration, the i-th particle's new velocity. Three terms are taken into consideration. The first term, wVin, is the particle's previous velocity weighted by the inertia weight w. The second term, (Pin -X l ) , is the distance between the particle's best previous position, and its current position. Finally, the third term, (P: -X l ) , is the distance between the swarm's best experience, and the i-th particle's current position. The parameters c1 r z , egg, provide randomness that render the technique less predictable but more flexible [12]. Eq. (3) provides the new position of the i-th particle, adding its new velocity, to its current position. In general, the performance of each particle is measured according to a fitness function, which is problemdependent. In optimization problems, the fitness function is usually the objective function under consideration.</p><p>The role of the inertia weight w is considered crucial for PSO's convergence behavior. The inertia weight is employed to control the impact of the history of velocities on the current velocity. In this way, the parameter w regulates the trade-off between the global (wide-ranging) and the local (nearby) exploration abilities of the swarm. A large inertia weight facilitates global exploration (searching new areas), while a small one tends to facilitate local exploration, i.e. fine-tuning the current search area. A suitable value for the inertia weight w provides balance between the global and local exploration ability of the swarm, resulting in better convergence rates. Experimental results suggest that it is better to set the inertia t o a large initial value, in order to promote global exploration of the search space, and gradually decrease it to obtain refined solutions <ref type="bibr" target="#b28">[31]</ref>. Our approach, employs a time-decreasing inertia weight value.</p><p>The initial population, as well as the velocities, can be generated either randomly or by a Sobol sequence generator [27], which ensures that the D-dimensional vectors will be uniformly distributed within the search space. Some variants of PSO impose a maximum allowed velocity V, , , to prevent the swarm from exploding. Thus, if v: T1 &gt; V,,, in Eq. (2), then ZJ: ~' = V,,, <ref type="bibr">[14]</ref>.</p><p>PSO resembles, to some extent, EA. Although it does not rely on a direct recombination operator, the recombination concept is accounted for by the stochastic movement of each particle toward its own best previous position, as well as toward the global best position of the entire swarm or its neighborhood's best position, depending on the variant of the PSO that is used <ref type="bibr">[6]</ref>. Moreover, PSO's mutation-like behavior is directional, due to the velocity of each particle, with a kind of momentum built in. In other words, PSO is considered as performing mutation with a "conscience", as pointed out by <ref type="bibr">Eberhart and Shi [6]</ref>.</p><p>The PSO technique has proved to be very effective in solving real valued global optimization problems, in static, noisy as well as continuously changing environments, and for performing neural networks training <ref type="bibr" target="#b16">[19]</ref>- <ref type="bibr" target="#b19">[22]</ref>, exhibiting competitive results with the EA [l]. Moreover, it can cope efficiently with Multiobjective Optimization problems <ref type="bibr" target="#b22">[25]</ref> and specialized problems, like the C1 norm errors-in-variables problems <ref type="bibr" target="#b20">[23]</ref>. Its convergence rates can be improved by properly initializing the population e.g. using a derivative-free method like the Nonlinear Simplex Method of Nelder and Mead <ref type="bibr" target="#b21">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="111.">THE BRANCH AND BOUND TECHNIQUE</head><p>The BB technique is widely used for solving optimization problems. In BB, the feasible region of the problem is relaxed, and subsequently partitioned into several subregions; this is called branching. Over these sub-regions, lower and upper bounds for the values of the function can be determined; this is the bounding part of the algorithm.</p><p>The BB technique can be algorithmically sketched as follows [3], [15], [16]:</p><p>Step 1. Start with a relaxed feasible region MO 3 S and partition MO into finitely many subsets Mi, i = 1,2,. . . , m, where S is the feasible region of the problem.</p><p>Step 2. For each subset Mi, determine lower (and if possible) upper bounds, P(Mi) and a(Mi), respectively, satisfying are "overall" bounds, i.e.</p><formula xml:id="formula_0">P(Mi) &lt; inff(Mi n S ) &lt; a(Mi</formula><p>, O &lt; minf(S) &lt; a.</p><p>Step 3. If a = P (or a -,</p><p>CJ 6 E , for a predefined constant E &gt; 0), then stop.</p><p>Step 4. Otherwise, choose some of the subsets Mi and partition them, in order to obtain a more refined partition of Mo. Determine new (hopefully better) bounds on the new partition elements, and repeat the procedure.</p><p>An advantage of the BB technique is that, during the iteration process, one can usually delete subsets of S, in which, the minimum of f cannot be attained. Important issues that arise during the BB procedure are that of properly partitioning the feasible region and selecting which sub-problem to evaluate.</p><p>The BB technique has been successfully applied to Integer Programming problems. The algorithm applied in this paper, transforms the initial integer problem to a continuous one. Consecutively, following the prescribed procedure, it restricts the domain of the variables, which are still considered continuous, and solves the generated sub-problems using the Sequential Quadratic Programming method. This process is repeated until the variables are fixed to an integer value. For the branching, Depth-First traversal with backtracking was used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>Seven Integer Programming test problems were selected to investigate the performance of the PSO method. Each particle of the swarm was truncated to the closest integer, after the determination of its new position using The considered test problems, defined immediately below, are frequently encountered in the relevant literature:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Eq. (3).</head><p>TEST PROBLEM 1, <ref type="bibr" target="#b26">[29]</ref>:  with solution z* = (0, l)T and F7(2*) = -3833.12. Three variants of PSO were used in the experiments: one with inertia weight and without constriction factor, denoted as PSO-In; one with constriction factor and without inertia weight, denoted as PSO-Co; and one with both constriction factor and inertia weight, denoted as PSO-Bo. For all experiments, the maximum number of allowed function evaluations was set to 25000; the desired accuracy was the constriction factor x was set equal to 0.729; the inertia weight w was gradually decreased from l towards 0.1; c1 = cg = 2; and V, , , = 4.   integer. The maximum number of allowed function evaluations and the desired accuracy were the same as for PSO.</p><formula xml:id="formula_1">FI(z) = 11x111 = 1x11 + ... -k I z D l ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F~(z)</head><p>For both algorithms, the number of successes in detecting the integer global minimum of the corresponding problem, within the maximum number of function evaluations, the mean, the standard deviation, and the median of the required number of function evaluations, were recorded and they are reported in Tables <ref type="table">I</ref> and<ref type="table">11</ref>. The swarm's size was problem dependent. The swarm's size for each test problem is reported in Table <ref type="table" target="#tab_3">111</ref>. It should be noted at this point, that although the swarm's size was problem dependent, the maximum number of allowed function evaluations was equal to 25000, for all cases.</p><p>In a second round of experiments, a PSO with gradually truncated particles was used. Specifically, the particles for the first 50 iterations were rounded to 6 decimal digits (d.d.), for another 100 iterations they were rounded t o 4 d.d., for another 100 iterations they were rounded t o 2 d.d., and for the rest iterations they were rounded t o the nearest integer. The results obtained using this  <ref type="table">I</ref> and<ref type="table" target="#tab_2">I1</ref> for the plain PSO.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>), 0-7803-7282W02/$10.00 02002 IEEE where f is the objective function under consideration. Then, the bounds defined as min P ( W , ._ P .-i=1,2, ..., m and a := min &amp;(Mi), a=1,2, ...,m</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FS( 2 ) = 22 :</head><label>222</label><figDesc>with z = (zl, .. . , Z D ) E [-loo, 100]0, where D is the corresponding dimension. The solution is x; = 0, i = 1 , . . . , D , with Fl(z*) = 0. This problem was considered in dimensions 5, 10, 15, 20, 25, and 30. = X T 2 = ( 2 1 ... X D ) (7) X D with x = ( x l , . . . , z D ) ~ E [-lOO,lOO]D, where D is the corresponding dimension. The solution is xi* = 0, i = 1 , . . . , D, with F2(z*) = 0. This is a quite trivial problem and it was considered in dimension 5= (95: + 22; -11)2 + (321 + 42; -7)2, with solution 2* = (1, l ) T and F4(2*) = 0. TEST PROBLEM 5, [9]: F5(X) = (21 + 1022)2 + 5(23 -2 4 ) 2 + +(XZ -2 ~3 ) ~ + l O ( 2 1 -~4 ) ~, with solution z* = (O,O,O,O)T and F5(2*) = 0. + 32; + 42122 -6x1 -322, with solution E* = (2, -l)T and F6(2*) = -6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Function</head><label></label><figDesc>of PSO, were almost similar to the results reported in Tables</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>182.252122, 0-7803-7282-4/02/$10.00 02002 IEEE</head><label></label><figDesc></figDesc><table><row><cell>= -3803.84 -138.0821 -232.92~2 + 123.082: + +203.64~; + Function Method F1 PSO-In 5 dim PSO-Co 30/30 Succ. 30/30 PSO-BO 30130 BB 30/30 F1 PSO-In 30/30 10 dim PSO-Co 30/30 BB 30/30 PSO-BO 30/30 F1 PSO-In 30/30 15 dim PSO-Co 30/30 BB 30/30 F1 PSO-In 30/30 20 dim PSO-Co 30/30 PSO-Bo 29/30 BB 30/30 F1 PSO-In 30130 11886.6 Mean 1646.0 744.0 692.6 1167.83 4652.0 1362.6 5495.8 1676.3 St.D. Median 661.5 1420 89.8 730 97.2 680 659.8 1166 483.2 4610 254.7 1360 5154 1208.6 162.7 1230 7916.6 624.1 7950 3538.3 526.6 3500 10177.1 2393.4 10011 8991.6 673.3 9050 4871.6 743.3 4700 4408.3 3919.4 3650 16291.3 3797.9 14550 543.7 11900 25 dim PSO-Co 30/30 9686.6 960.1 9450 PSO-Bo 25/30 9553.3 7098.6 6500 BB 20/30 23689.7 2574.2 25043 F 1 PSO-In 30/30 13186.6 667.8 13050 30 dim PSO-Co 30/30 12586.6 1734.9 12500 PSO-BO 30/30 2860.0 220.2 2850 PSO-BO 19/30 13660.0 8863.9 7500 BB 14/30 25908.6 755.5 26078</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The aforementioned values for all PSO's parameters are considered default values, and they are used widely in the relevant literature [14]. There was no preprocessing stage that might yield more suitable values for the parameters. For each test problem, 30 experiments were performed, starting with a swarm and velocities uniformly distributed within the range [-loo,</figDesc><table><row><cell>where D is the</cell></row><row><cell>dimension of the corresponding problem, and truncated</cell></row><row><cell>to the nearest integer.</cell></row><row><cell>For the BB algorithm, 30 experiments were performed</cell></row><row><cell>for each test problem, starting from a randomly selected point within [-loo, and truncated to the nearest</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I1 SUCCESS RATE, MEAN NUMBER, STANDARD DEVIATION, AND MEDIAN OF FUNCTION EVALUATIONS, FOR THE TEST PROBLEMS Fz-F7</head><label>I1</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 111 DIMENSION AND SWARM'S SIZE FOR ALL TEST PROBLEMS.</head><label>111</label><figDesc></figDesc><table><row><cell cols="2">Function Method</cell><cell>Succ.</cell><cell>Mean</cell><cell cols="2">St.D. Median</cell></row><row><cell>Fz</cell><cell>PSO-In</cell><cell>30/30</cell><cell>1655.6</cell><cell>618.4</cell><cell>1650</cell></row><row><cell>5 dim</cell><cell cols="2">PSO-Co 30/30</cell><cell>428.0</cell><cell>57.9</cell><cell>430</cell></row><row><cell></cell><cell cols="2">PSO-BO 30/30</cell><cell>418.3</cell><cell>83.9</cell><cell>395</cell></row><row><cell></cell><cell>BB</cell><cell>30/30</cell><cell>139.7</cell><cell>102.6</cell><cell>93</cell></row><row><cell>F3</cell><cell>PSO-In</cell><cell>30/30</cell><cell>4111.3</cell><cell>1186.7</cell><cell>3850</cell></row><row><cell></cell><cell cols="2">PSO-CO 30/30</cell><cell>2972.6</cell><cell>536.4</cell><cell>2940</cell></row><row><cell></cell><cell cols="2">PSO-BO 30/30</cell><cell>3171.0</cell><cell>493.6</cell><cell>3080</cell></row><row><cell></cell><cell>BB</cell><cell>30/30</cell><cell>4185.5</cell><cell>32.8</cell><cell>4191</cell></row><row><cell>F4</cell><cell>PSO-In</cell><cell>30/30</cell><cell>304.0</cell><cell>101.6</cell><cell>320</cell></row><row><cell></cell><cell cols="2">PSO-CO 30/30</cell><cell>297.3</cell><cell>50.8</cell><cell>290</cell></row><row><cell></cell><cell cols="2">PSO-Bo 30/30</cell><cell>302.0</cell><cell>80.5</cell><cell>320</cell></row><row><cell></cell><cell>BB</cell><cell>30/30</cell><cell>316.9</cell><cell>125.4</cell><cell>386</cell></row><row><cell>F5</cell><cell>PSO-In</cell><cell>30/30</cell><cell>1728.6</cell><cell>518.9</cell><cell>1760</cell></row><row><cell></cell><cell cols="2">PSO-CO 30/30</cell><cell>1100.6</cell><cell>229.2</cell><cell>1090</cell></row><row><cell></cell><cell cols="2">PSO-BO 30/30</cell><cell>1082.0</cell><cell>295.6</cell><cell>1090</cell></row><row><cell></cell><cell>BB</cell><cell>30/30</cell><cell cols="2">2754.0 1030.1</cell><cell>2714</cell></row><row><cell>F6</cell><cell>PSO-In</cell><cell>30/30</cell><cell>178.0</cell><cell>41.9</cell><cell>180</cell></row><row><cell></cell><cell cols="2">PSO-CO 30/30</cell><cell>198.6</cell><cell>59.2</cell><cell>195</cell></row><row><cell></cell><cell cols="2">PSO-BO 30/30</cell><cell>191.0</cell><cell>65.9</cell><cell>190</cell></row><row><cell></cell><cell>BB</cell><cell>30/30</cell><cell>211.1</cell><cell>15.0</cell><cell>209</cell></row><row><cell>F7</cell><cell>PSO-In</cell><cell>30/30</cell><cell>334.6</cell><cell>95.5</cell><cell>340</cell></row><row><cell></cell><cell cols="2">PSO-CO 30/30</cell><cell>324.0</cell><cell>78.5</cell><cell>320</cell></row><row><cell></cell><cell cols="2">PSO-Bo 30/30</cell><cell>306.6</cell><cell>96.7</cell><cell>300</cell></row><row><cell></cell><cell>BB</cell><cell>30/30</cell><cell>358.6</cell><cell>14.7</cell><cell>355</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>The ability of the PSO method to cope with Integer Programming problems formed the core of the paper. Experimental results for seven widely used test problems indicate that PSO is a very effective method and should be considered as a good alternative to handle such problems.</p><p>The behavior of PSO seems to be stable even for high dimensional cases, exhibiting high success rates even in cases in which the BB technique failed. In most cases, PSO outperformed the BB approach, by means of the mean number of required function evaluations.</p><p>Moreover, the method appears not seem to suffer from search stagnation. The aggregate movement of each particle towards its own best position and the best position ever attained by the swarm, added to its weighted previous position change, ensures that particles maintain a position change during the process of optimization, which is of proper magnitude.</p><p>Regarding the three different variants of PSO, PSO-Bo, which utilizes both inertia weight and constriction factor, was the fastest, but the other two approaches posses better global convergence abilities, especially in high dimensional problems. In most experiments, PSO-Co, which utilizes only a constriction factor, was significantly faster than PSO-In, which utilizes only inertia weight. In general, PSO seems an efficient alternative for solving Integer Programming problems, when deterministic approaches fail, or it could be considered as an algorithm for providing good initial points to deterministic methods, as the BB technique, and thus, help them converge to the global minimizer of the integer problem.</p><p>[16] V.M. Manquinho, J.P. Marques Silva, A.L. Oliveira ans K.A. Sakallah, "Branch and Bound Algorithms for Highly</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. ACKNOWLEDGEMENT</head><p>Constrained Integer Programs", Technical Report, Cadence</p><p>Part of this work was done while the authors (K.E.P. and M.N.V.) were at the Department of Computer Science, University of Dortmund, D-44221 Dortmund, Germany. This material was partially supported by the Deutsche Forschungsgemeinschaft-DFG (German National Research Foundation) as a part of the collaborative research center "Computational Intelligence" (SFB 531). </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Evolutionary Optimization Versus Particle Swarm Optimization: Philosophy and Performance Differences</title>
		<author>
			<persName><forename type="first">J</forename><surname>Angeline</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Evolutionary Programming VII</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="601" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Genetic Programming-An Introduction</title>
		<author>
			<persName><forename type="first">W</forename><surname>Banzhaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nordin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Francone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>San Francisco</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Using an Interior Point Method In a Branch and Bound Algorithm For Integer Programming</title>
		<author>
			<persName><forename type="first">B</forename><surname>Borchers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Mitchell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992-07">July 1992</date>
		</imprint>
		<respStmt>
			<orgName>Rensselaer Polytechnic Institute</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Eberhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Dobbins</surname></persName>
		</author>
		<title level="m">Computational Intelligence P C Tools</title>
		<meeting><address><addrLine>Boston</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press Professional</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Evolving Artificial Neural Networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Eberhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Neural Networks and Brain, Beijing</title>
		<meeting>Int. Conf. on Neural Networks and Brain, Beijing</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Comparison Between Genetic Algorithms and Particle Swarm Optimization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Eberhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Evolutionary Programming VII</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="611" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Evolutionary Computation: Toward a New Philosophy of Machine Intelligence</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Fogel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>IEEE Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Practical Multifactor Optimization Criterion</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent Advances in Optimization Techniques</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Levi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Vogl</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1966">1966</date>
			<biblScope unit="page" from="369" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unconstrained Discrete Nonlinear Programming</title>
		<author>
			<persName><forename type="first">A</forename><surname>Glankwahmdee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Liebman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Hogg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Engineering Optimization</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="95" to="107" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Horst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tuy</surname></persName>
		</author>
		<title level="m">Global Optimization, Deterministic A p proaches</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Application of the Adaptive Random Search to Discrete and Mixed Integer Optimization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Kelahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Gaddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Evolutionary Programming VII</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="581" to="587" />
		</imprint>
	</monogr>
	<note>The Behavior of Particles</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Particle Swarm Optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Eberhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conference on Neural Networks</title>
		<meeting>of the IEEE International Conference on Neural Networks<address><addrLine>Piscataway, NJ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="1942" to="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Eberhart</surname></persName>
		</author>
		<title level="m">Swarm Intelligence</title>
		<imprint>
			<publisher>Morgan Kaufmann Publishers</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Branch and Bound Methods: A Survey</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Lawler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Laboratories</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="289" to="298" />
			<date type="published" when="1966">1966. 1978. 1997</date>
			<pubPlace>Portugal</pubPlace>
		</imprint>
	</monogr>
	<note>Operations Research</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Nemhauser</surname></persName>
		</author>
		<title level="m">Handbooks in OR &amp; MS</title>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Rinnooy Kan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Todd</surname></persName>
		</editor>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Integer and Combinatorial Optimization</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Nemhauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Wolsey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>John Wiley and Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Objective Function &quot;Stretching&quot; to Alleviate Convergence to Local Minima</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Parsopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Plagianakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Magoulas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Vrahatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nonlinear Analysis T M A</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stretching Technique for Obtaining Global Minimizers Through Particle Swarm Optimization</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Parsopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Plagianakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Magoulas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Vrahatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Particle Swarm Optimization Workshop</title>
		<meeting>of the Particle Swarm Optimization Workshop<address><addrLine>Indianapolis</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Modification of the Particle Swarm Optimizer for Locating All the Global Minima</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Parsopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Vrahatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks and Genetic Algorithms</title>
		<title level="s">Computer Science Series</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Kurkova</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Steele</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Neruda</surname></persName>
		</editor>
		<editor>
			<persName><surname>Karny</surname></persName>
		</editor>
		<meeting><address><addrLine>Wien</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="324" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Particle Swarm Optimizer in Noisy and Continuously Changing Environments</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Parsopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Vrahatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Soft Computing</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Hamza</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="289" to="294" />
			<date type="published" when="2001">2001</date>
			<publisher>IASTED/ACTA Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Solving -! I Norm Errors-In-Variables Problems Using Particle Swarm Optimizer</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Parsopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Laskari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Vrahatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Applications</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Hamza</surname></persName>
		</editor>
		<imprint>
			<publisher>IASTED/ACTA Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="185" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Initializing the Particle Swarm Optimizer Using the Nonlinear Simplex Method</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Parsopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Vrahatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WSES Evolutionary Computation 2002 Conference, Interlaken</title>
		<meeting>WSES Evolutionary Computation 2002 Conference, Interlaken<address><addrLine>Switzerland</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Particle Swarm Optimization Method in Multiobjective Problems</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Parsopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Vrahatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SAC 2002 Conference</title>
		<meeting><address><addrLine>Madrid, Spain</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Training Neural Networks with Threshold Activation Functions and Constrained Integer Weights</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Plagianakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Vrahatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Joint Conference on Neural Networks (IJCNN 2000)</title>
		<meeting>the IEEE International Joint Conference on Neural Networks (IJCNN 2000)<address><addrLine>Como, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Vetterling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Teukolsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Flannery</surname></persName>
		</author>
		<title level="m">Numerical Recipes in Fortran 77</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Engineering Optimization-Theory and Practice</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Wiley Eastern: New Delhi</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An Evolutionary Algorithm for Integer Programming</title>
		<author>
			<persName><forename type="first">G</forename><surname>Rudolph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parallel Problem Solving from Nature</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Davidor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H.-P</forename><surname>Schwefel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Manner</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="139" to="148" />
			<date type="published" when="1994">1994</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Evolution and Optimum Seeking</title>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Schwefel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Parameter Selection in Particle Swarm Optimization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Eberhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EvolutionaryProgramming VII</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">2001. 2001. 1998</date>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="591" to="600" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
