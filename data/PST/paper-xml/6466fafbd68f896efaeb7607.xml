<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MedBLIP: Bootstrapping Language-Image Pre-training from 3D Medical Images and Texts</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-05-18">18 May 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qiuhui</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xinyue</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zirui</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yi</forename><surname>Hong</surname></persName>
							<email>yi.hong@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MedBLIP: Bootstrapping Language-Image Pre-training from 3D Medical Images and Texts</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-05-18">18 May 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2305.10799v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Vision-language pre-training (VLP) models have been demonstrated to be effective in many computer vision applications. In this paper, we consider developing a VLP model in the medical domain for making computer-aided diagnoses (CAD) based on image scans and text descriptions in electronic health records, as done in practice. To achieve our goal, we present a lightweight CAD system MedBLIP, a new paradigm for bootstrapping VLP from off-the-shelf frozen pre-trained image encoders and frozen large language models. We design a MedQFormer module to bridge the gap between 3D medical images and 2D pre-trained image encoders and language models as well. To evaluate the effectiveness of our MedBLIP, we collect more than 30,000 image volumes from five public Alzheimer's disease (AD) datasets, i.e., ADNI, NACC, OASIS, AIBL, and MIRIAD. On this largest AD dataset we know, our model achieves the SOTA performance on the zero-shot classification of healthy, mild cognitive impairment (MCI), and AD subjects, and shows its capability of making medical visual question answering (VQA). The code and pre-trained models is available online: https://github.com/Qybc/MedBLIP.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Electronic health records (EHR), e.g., radiology images, lab and test results, and patient demographics, are often used in clinical diagnosis. For instance, to diagnose Alzheimer's Disease (AD), apart from brain imaging, physicians also use physical and neurological exams and diagnostic tests, with these test results presented in the text form. In past decades, researchers have gradually collected a large number of EHRs, e.g., <ref type="bibr">ADNI Petersen et al. [2010]</ref>, NACC <ref type="bibr" target="#b1">Beekly et al. [2007]</ref>, OASIS <ref type="bibr" target="#b2">Marcus et al. [2007]</ref>, for studying AD. However, learning how to make diagnoses based on these EHRs, especially how to fuse these medical data from different resources and in different forms, e.g., images and texts, is still a challenging task in computer-aided diagnosis (CAD).</p><p>Recently, large vision language pre-training (VLP) models, e.g., <ref type="bibr">CLIP Radford et al. [2021]</ref>, <ref type="bibr">BLIP Li et al. [2022]</ref>, BLIP-2 <ref type="bibr" target="#b5">Li et al. [2023]</ref>, have achieved great success in many downstream computer vision applications, such as classification <ref type="bibr" target="#b6">Bao et al. [2022]</ref>, segmentation <ref type="bibr" target="#b7">Xu et al. [2021]</ref>. These VLP models learn multi-modal representations from large image and text datasets, by aligning their features into a common space for learning. In the medical domain, researchers propose Medical Bootstrapping Language-Image Pre-training (MedCLIP) <ref type="bibr" target="#b8">Wang et al. [2022]</ref>, which learns generic representation from large-scale medical image-text pairs. This pre-trained medical model presents its generalization to various medical tasks, especially where limited medical data or labels are available for learning. However, most existing VLP models handle the situation that texts are corresponding textual descriptions of their paired images, such as image captions or medical reports.</p><p>In this paper, we consider another scenario where images and texts provide complementary information, that is, texts include additional information except for medical scans in EHRs, e.g., the age, gender, and lab results of a subject, to make an informed CAD decision. Our goal is to learn a VLM that suits this CAD scenario, which has multi-model intelligence to fuse different types of medical data, e.g., 3D medical scans and texts that contain complementary information from EHRs for CAD. Here, we need to address three problems: (1) How to extend a 2D image encoder to extract features from 3D medical images? (2) How to align image and text features and learn multi-model representations? (3) How to obtain a lightweight language model for our CAD purpose? Inspired by BLIP-2 <ref type="bibr" target="#b5">Li et al. [2023]</ref>, we propose MedBLIP as shown in Fig. <ref type="figure" target="#fig_0">1</ref>, a bootstrapping language-image pre-training model to fuse 3D medical images and texts based on a query mechanism. We first adopt a learnable patch embedding to bridge the gap between 3D medical images and a pre-trained image encoder, which greatly reduces the amount of image data required for learning. Then, we propose a MedQFormer, which contains learnable queries to allow aligning visual features with textural ones desired by a language model. Lastly, we choose BioMedLM <ref type="bibr" target="#b9">Venigalla et al. [2022]</ref> as our basic language model and fine-tune it using the LoRA Hu et al. <ref type="bibr">[2021]</ref> technique. Our CAD model MedBLIP is lightweight and trainable on a single NVIDIA RTX 3090 GPU.</p><p>To train and evaluate the effectiveness of our proposed MedBLIP model, we collect more than 30,000 medical image volumes from five public AD datasets, including ADNI Petersen et al. <ref type="bibr">[2010]</ref>, NACC <ref type="bibr" target="#b1">Beekly et al. [2007]</ref>, OASIS <ref type="bibr" target="#b2">Marcus et al. [2007]</ref>, AIBL <ref type="bibr" target="#b11">Ellis et al. [2009]</ref>, and MIRIAD <ref type="bibr" target="#b12">Malone et al. [2013]</ref>. After pre-training on most of the images from ADNI, NACC, and OASIS datasets, we evaluate our MedBLIP on two tasks: (1) zero-shot classification, which directly applies pre-trained MedBLIP to classify unseen subjects from AIBL and MIRIAD datasets into three classes, i.e., normal controls (NC), mild cognitive impairment (MCI), and AD; and (2) zero-shot medical visual question answering (VQA), which generates an initial diagnosis for an unseen AIBL or MIRIAD subject based on input images and text descriptions and also provides some reasons for making such decision.</p><p>Overall, our contributions of this paper are summarized below:</p><p>? We propose a lightweight CAD system MedBLIP, which is pre-trained on electronic health records in the form of images and texts, performs zero-shot classification, and makes medical VQA. The architecture of our CAD system is general and has the potential to incorporate more modalities and extend to other diseases beyond AD. ? We propose a MedQFormer module, which extracts 3D medical image features and aligns them with textural features to be fused into a language model (LM). This module provides a way to align different types of medical data into the common space of LM, which is generic and could be used in other medical applications. ? To our best knowledge, we have collected the largest public dataset for studying AD. On this dataset, our MedBLIP achieves the SOTA performance on separating AD and MCI subjects from healthy controls. Besides, we directly work on raw images without any preprocessing, which makes our system easy to use in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Vision Language Pre-Training. Data collected from different modalities typically provide different views about the data, which often complement each other and provide more complete information to facilitate a holistic understanding of the data. Vision-language pre-training (VLP) aims to learn multimodal foundation models, showing improved performance on various vision-and-language tasks <ref type="bibr" target="#b3">Radford et al. [2021]</ref>. Roughly, we can divide current VLP models into two categories when fusing multi-modal inputs: light fusion and heavy fusion.</p><p>The approaches in the light fusion category focus on multi-modal alignment, which facilitates text matching, retrieval, and other downstream tasks, with representative methods like <ref type="bibr">CLIP Radford et al. [2021]</ref> and <ref type="bibr">ALIGN Jia et al. [2021]</ref>. These methods directly align image representations with the corresponding text representations using a contrastive loss. DeCLIP <ref type="bibr">Li et al. [2021a]</ref> exploits inter/intra-modality supervision to train a CLIP-like model with fewer data. On the other hand, the heavy fusion category focuses on incorporating multi-modal information with an attention mechanism to perform additional tasks. For instance, ALBEF <ref type="bibr">Li et al. [2021b]</ref> proposes a contrastive alignment, which is followed by deeper fusion with a multi-modal encoder. Methods such as BLIP LLMs for Multi-Modal Understanding. Recently, using large language models (LLMs) as decoders in vision-language tasks has gained significant attention. This approach takes advantage of cross-modal transfer, which allows sharing knowledge between language and multi-modal domains.</p><p>VisualGPT <ref type="bibr" target="#b20">Chen et al. [2022]</ref> and Frozen <ref type="bibr" target="#b21">Tsimpoukelli et al. [2021]</ref> have demonstrated the advantage of employing a pre-trained language model as a vision-language model decoder. Flamingo <ref type="bibr" target="#b22">Alayrac et al. [2022]</ref> freezes a pre-trained vision encoder and language model and then fuses vision and language modalities with gated cross-attention. BLIP-2 <ref type="bibr" target="#b5">Li et al. [2023]</ref> designs a Q-Former to align the visual features from the frozen visual encoder with large language models, like FLAN-T5 Chung et al. <ref type="bibr">[2022]</ref> and <ref type="bibr">OPT Zhang et al. [2022]</ref>. FROMAGe <ref type="bibr" target="#b25">Koh et al. [2023]</ref> freezes large language models and visual encoders, and fine-tunes linear mapping layers to achieve cross-modality interactions. This method shows strong zero-shot performances on contextual image retrieval and multi-modal dialogue tasks. Built upon PaLM <ref type="bibr" target="#b26">Chowdhery et al. [2022]</ref>, PaLM-E <ref type="bibr" target="#b27">Driess et al. [2023]</ref> employs features from sensor modalities and integrates real-world continuous sensor modalities into an LLM, thereby establishing a connection between real-world perceptions and human languages. <ref type="bibr">GPT-4 OpenAI [2023]</ref> presents powerful visual understanding and reasoning abilities after pre-training on a vast collection of image-text data.</p><p>Most recently, several domain-specific multi-modal LLMs have been developed. ChatCAD <ref type="bibr" target="#b29">Wang et al. [2023]</ref> combines visual and linguistic information processed by various networks as inputs of large language models to develop a medical-image CAD model, which provides a condensed report and offers interactive explanations and medical recommendations. Open-ended MedVQA van Sonsbeek et al.</p><p>[2023] employs a multi-layer perceptron (MLP) network that maps the extracted visual features from a frozen vision encoder to a set of learnable tokens, which develops an openended VQA for diagnoses and treatment decisions. Differently, our MedBLIP explores a lightweight framework that works on 3D medical scans and aligns different types of medical data for CAD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MedBLIP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>We design a CAD system in the form of dialogue, with application to automatic AD diagnosis. Given inputs of a brain image scan I collected from a subject and a textual description T generated from this subject's EHRs in natural language, for a question asked in natural language Q, our CAD aims to</p><p>sequentially generate an answer A = {A 0 , A 1 , ..., A N } composed of N tokens, by conditioning on all inputs {I, T, Q}. To achieve this goal, we build a CAD model based on a large language model and find its optimal parameters ? * by maximizing the conditional log-likelihood below:</p><formula xml:id="formula_0">? * = arg max ? N i=1 log p ? (A i | I, T, Q, A i-1</formula><p>) .</p><p>(1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Network Framework</head><p>Our CAD model is designed as an encoder-decoder architecture, with a two-stream encoder and a language model (LM) as a decoder, as illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. Specifically, the two-stream encoder takes inputs from two modalities, namely a vision sub-encoder for the image I and the text sub-encoder for the textual description T and the question Q. The language model is defined as a causal language transformer, which generates the answer A in an auto-regressive manner.</p><p>Vision Encoding Stream. To encode a brain image volume and fully leverage an existing large model for reducing data requirements, we employ a pre-trained 2D vision encoder to extract its 3D visual features. To make this work, we need to address two problems: (1) bridging the domain and dimension gaps between a 2D vision encoder and a 3D medical scan, and (2) aligning image features with textural ones, which allows mapping all inputs into the latent space of the LM decoder for learning multi-modal representations. Inspired by <ref type="bibr" target="#b5">Li et al. [2023]</ref>, we propose a query network based on a transformer encoder, which maps the visual features into a visual prefix</p><formula xml:id="formula_1">H v = {v 1 , v 2 , ? ? ? , v v } ? R v ?e</formula><p>for the language model, where v is the length of the vision embedding sequence and e is the embedding size. Also, we have a lightweight projection, which is learnable and adapts 3D image volumes to inputs of a pre-trained image encoder. This medical query transformer (MedQFormer) tackles the above two problems and will be discussed in detail in Sect. 3.3.</p><p>Language Encoding Stream. Regarding the textural description of subjects' EHRs except for image scans and the asked questions, we first utilize a standard tokenization process as in Jain <ref type="bibr">[2022]</ref> to obtain a sequence of tokens, i.e., the textual description</p><formula xml:id="formula_2">T = {t 1 , t 2 , ? ? ? , t t } ? R t?e , the question Q = {q 1 , q 2 , ? ? ? , q q } ? R q ?e</formula><p>, and the answer A = {a 1 , a 2 , ? ? ? , a a } ? R a?e , where t , q , a indicate the length of the embedding sequence of the text, question, and answer, respectively. These tokens are embedded later using the embedding function provided in a pre-trained language model.</p><p>Prompt Structure. To create a structured prompt, following current VQA methods used in language models <ref type="bibr" target="#b5">Li et al. [2023]</ref>, van Sonsbeek et al. <ref type="bibr">[2023]</ref>, we prepend the question and answer tokens with tokenized descriptive strings, namely in the form of question: and answer:. We choose to place the embeddings of the image and text description before the question tokens. As a result, we have the following prompt template:</p><formula xml:id="formula_3">p =[v 1 , v 2 , ? ? ? , v x , t 1 , t 2 , ? ? ? , t t , Question :W hat will this subject be diagnosed with?Answer:],<label>(2)</label></formula><p>which is fed as input to the language model below.</p><p>Language Model. Following standard language modeling systems Venigalla et al.</p><p>[2022], we treat VQA as a process of a conditional generation of text, and we optimize the standard maximum likelihood objective during training. The language model receives the prompt sequence as input and outputs the answer A, token by token. Specifically, at each time step i, the outputs of the model are the logits, which parameterize a categorical distribution p ? (A) over the vocabulary tokens. This distribution is represented as follows:</p><formula xml:id="formula_4">log p ? (A) = la log p ? a i | v 1 , . . . v v , t 1 , . . . t t , q 1 , . . . q q , a 1 , . . . a i-1 .<label>(3)</label></formula><p>The parameters of the language model are initialized from a pre-trained model, which has been previously pre-trained on huge web-collected datasets <ref type="bibr" target="#b32">Dodge et al. [2021]</ref>, <ref type="bibr" target="#b33">Gao et al. [2020]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">MedQFormer</head><p>To bridge the gap between 3D medical images and 2D vision encoders pre-trained on natural images, inspired by BLIP-2 <ref type="bibr" target="#b5">Li et al. [2023]</ref>, we employ a query encoder to extract and align vision features. Image Feature Extraction. We first divide the input image I into a set of 3D sub-volumes {Iv i } Nv i=1 , followed by a linear projection f ?1 that projects 3D cubes into 1D image embeddings</p><formula xml:id="formula_5">{E i = f ?1 (Iv i )} Nv i=1</formula><p>. With the addition of learnable position embeddings f ?2 , the image embeddings can be received as inputs of a standard pre-trained vision encoder to extract desired image features. Although the pre-trained vision encoder f ? has fixed parameters ?, we have learnable linear projection and position embedding to transfer a 2D vision encoder to a 3D medical domain. Hence, we have a medical vision encoder with learnable parameters ? 1 and ? 2 , which maps a volumetric image</p><formula xml:id="formula_6">I into N v visual features f 1 , ? ? ? , f Nv = {f ? (f ?1 (Iv i ), f ?2 (Iv i ))} Nv i=1 .</formula><p>As a result, we obtain the final image embeddings</p><formula xml:id="formula_7">IE = (f i , ? ? ? , f Nv ) for each input image volume I.</formula><p>Query Encoder. To map the visual features {f i } Nv i=1 into the common language space, we use a set of L learnable queries qry i ? R de , where d e is the dimension of query embeddings. These queries have a transformer structure that interacts with the image encoder for adjusting visual feature extraction and a text transformer as a textural feature extractor. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, these learnable queries interact with each other through self-attention layers, then interact with image features through cross-attention layers. As a result, we obtain a visual prefix H v that is aligned with textural features and can be taken by a language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training MedBLIP</head><p>Learnable Parameters. Standard fine-tuning of a language model could hurt its generalization capability, especially if a dataset used for fine-tuning has a small size and is domain-specific as in our case. Therefore, we consider two parameter-efficient strategies that adapt the attention blocks of language models:</p><p>? Frozen LM. The parameters of the language model are kept entirely frozen during training.</p><p>In this setting, only the 3D vision query network is updated through backpropagation. ? Low-Rank Adaptation (LoRA). We add learnable weight matrices to the query Q w and value V w of the attention blocks in each layer of the frozen language model as W + W following Hu et al. <ref type="bibr">[2021]</ref>. In this setting, the 3D vision query network function is trained together with the learnable weight matrices.</p><p>Objective Functions. We have loss functions for MedQformer and LM modules in our MedBLIP model. As discussed in Sect. (4) Similar to BLIP-2 <ref type="bibr" target="#b5">Li et al. [2023]</ref>, we select the one that has the highest similarity with text from multiple output query embeddings to compute the ITC Loss. To supervise the LM component, we use cross entropy to compute language generation loss L LG . Hence, the final loss function is defined as:</p><formula xml:id="formula_8">L total = L F A + ? LG L LG ,<label>(5)</label></formula><p>where ? LG is a hyperparameter to balance these two terms. 4 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Experimental Settings</head><p>We collect more than 30,000 image volumes from five public datasets for studying AD/Dementia and evaluate our CAD system MedBLIP on separating subjects with AD or mild cognitive impairment (MCI) from normal controls (NC). Table <ref type="table" target="#tab_1">1</ref> reports the demographic statistics of these five datasets.</p><p>ADNI <ref type="bibr" target="#b0">Petersen et al. [2010]</ref>. This dataset has 10,387 volumetric T1 MRI scans that went through a series of pre-processing steps, including denoising, bias field correction, skull stripping, and affine registration to the SRI24 atlas, with an image size of 138 ? 176 ? 138. For testing, we subject-wisely sample a subset of 200 images in each class (i.e., NC, MCI, AD), which is named ADNI-3x200.</p><p>NACC <ref type="bibr" target="#b1">Beekly et al. [2007]</ref>. This dataset has a large amount of raw volumetric T1 MRI scans with a variety of resolutions. We select those MRIs having 100?256 slices in all three dimensions, resulting in 15,354 images. Unlike the ADNI dataset, we directly use the raw data; but similarly, we sample subject-wisely a NACC-3x200 dataset for testing.</p><p>OASIS <ref type="bibr" target="#b2">Marcus et al. [2007]</ref>. We collect 3020 volumetric T1 MRIs from OASIS 1&amp;2. These scans went through pre-processing with denoising and skull stripping and have a size of 256 ? 256 ? 256.</p><p>Since OASIS 1 only releases some clinical reports but with no diagnoses (e.g. NC, MCI or dementia), we use all images from OASIS 1 for pre-training. For testing, we sample subject-wisely an OASIS-2x200 subset from OASIS 2 to separate demented and non-demented subjects.</p><p>AIBL <ref type="bibr" target="#b11">Ellis et al. [2009]</ref>. This dataset has 1002 volumetric T1 MRI scans with sizes of 160 ? 240 ? 256, which are collected from demented, MCI, or healthy subjects. We do not use this data for training; for testing, we sample a balanced subset with 200 images each for NC, MCI, and dementia classes.</p><p>MIRIAD <ref type="bibr" target="#b12">Malone et al. [2013]</ref>. We collect 708 raw volumetric T1 MRI scans, which have an image size of 124 ? 256 ? 256. This is a binary classification dataset with two labels, i.e., demented and not-demented subjects. We sample a balanced subset with a 1:1 positive and negative ratio, resulting in 2 ? 200 images for testing. No images are used for training to perform zero-shot experiments.</p><p>As a result, we have most images from ADNI, NACC, and OASIS datasets for pretraining and save images from AIBL and MIRIAD datasets for zero-shot testing. In total, we held 1000 subjects with 2600 samples out for evaluation. To simplify the preprocessing step, all images are first padded to a cube and then scaled to a unified size of 224 ? 224 ? 224 as inputs.</p><p>Implementation Details. For the frozen image encoder, we choose state-of-the-art pre-trained ViT-G/14 from EVA-CLIP <ref type="bibr" target="#b34">Fang et al. [2022]</ref>, which is demonstrated to be effective in BLIP-2 <ref type="bibr" target="#b5">Li et al. [2023]</ref>. For the input image with a size of 224 ? 224 ? 224, the patch size and the stride are both set as 32, resulting in image features with the size of 344 ? 1408. For the MedQformer, we use 32 learnable queries, where each query has a dimension of 768 and the hidden layers N is set to 12.</p><p>Regarding language models, we have three options, i.e., <ref type="bibr">FLAN-T5 Chung et al. [2022]</ref>, <ref type="bibr">BioGPT Luo et al. [2022]</ref>, and BioMedLM <ref type="bibr" target="#b9">Venigalla et al. [2022]</ref>. FLAN-T5 is an instruction-trained model with 3B parameters trained on C4 WebText <ref type="bibr" target="#b32">Dodge et al. [2021]</ref>. BioGPT and BioMedLM are both GPT models relying on GPT-2 architecture, pre-trained on PubMed and biomedical data from the Pile <ref type="bibr" target="#b33">Gao et al. [2020]</ref>, with a size of 1.5B and 2.7B parameters, respectively. All our models are able to fine-tune on a single NVIDIA RTX 3090 GPU. We use the AdamW optimizer with a learning rate of 5e-3. The hyperparameter ? LG is set to 1. The CDR is 0.5. The logical memory score is 2.</p><p>Q: What will this subject be diagnosed with?</p><p>Ground Truth A: Dementia.</p><p>Our A: Dementia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I:</head><p>T: 85-year-old Female. The MMSE score is 30. The CDR is 0.</p><p>Q: What will this subject be diagnosed with?</p><p>Ground Truth A: Mild cognitive impairment.</p><p>Our A: Mild cognitive impairment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I:</head><p>T: 78-year-old Female. The MMSE score is 27. The CDR is 0.5. The logical memory score is 5.</p><p>Q: What will this subject be diagnosed with?</p><p>Ground Truth A: Healthy. Q: What will this subject be diagnosed with?</p><p>Ground Truth A: Mild cognitive impairment.</p><p>Our A: Mild cognitive impairment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q: Why have you made this decision?</head><p>Our A: score is 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I:</head><p>T: 63-year-old Female. The MMSE score is 18.</p><p>The CDR is 1. The logical memory score is 5.</p><p>Q: What will this subject be diagnosed with?</p><p>Ground Truth A: Dementia.</p><p>Our A: Dementia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q:</head><p>What is abnormal in the brain imaging?</p><p>Our A: The brain is atrophic.</p><p>I: e T: 71.6-year-old Male, with 16 years of education. The MMSE score is 30. The CDR score is 0. The logical memory score is 14.</p><p>Q: What will this subject be diagnosed with?</p><p>Ground Truth A: Healthy.</p><p>Our A: Healthy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q:</head><p>What is the reason for you decision?</p><p>Our A: The MMSE socre is higher than the CDR score and the logical memory score is higher than the Dementia score. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Results</head><p>Zero-shot Medical CAD. Table <ref type="table" target="#tab_2">2</ref> reports the evaluation of our MedBLIP using different language models and settings. The language models, i.e., FLAN-T5, BioGPT, and BioMedLM, show their capability of performing monomodal medical CAD, i.e., using text descriptions only, to some extent. Among these three language models, BioMedLM performs the best, showing that it captures some dependencies between prompts and inherent knowledge when generating answers. By adding the visual modality even without fine-tuning, the performance of our model on all datasets has improved significantly. The accuracy improvement varies within [4.0%, 44.8%], and the BioGPT benefits the most from the visual input. This result indicates the necessity of using image scans in diagnosis.</p><p>Using the fine-tuning technique LoRA, our performance is further improved, with at least 1.3% and  Figure <ref type="figure" target="#fig_4">3 (a-c</ref>) visualizes the zero-shot CAD process on unseen subjects sampled from the AIBL dataset. Take Fig. <ref type="figure" target="#fig_4">3(b</ref>) for example, although the text description of this subject shows no significant difference from those of healthy subjects, in brain scans the hippocampus and ventricle show the presence of abnormal atrophy. Our MedBLIP provides the correct diagnosis of MCI.</p><p>Zero-shot Medical VQA. Figure <ref type="figure" target="#fig_4">3 (d-f</ref>) shows the zero-shot Medical Visual question answering(VQA) ability of our MedBLIP. Since our approach is generative, after a simple classificationbased question, MedBLIP provides a natural way of performing VAQ and presents the chain of thoughts. MedBLIP may also generate unsatisfactory answers to users' questions due to various reasons, including inaccurate medical knowledge from the LLM, activating the incorrect reasoning path, or not having up-to-date information about new image content.</p><p>Ablation Study. We perform ablation studies from three aspects to answer the following three questions: (1) Why use a 2D pre-trained vision encoder instead of a trainable large vision encoder? (2) Will a prompt structure make a difference in the final CAD result? and (3) Why need the ITC loss between the image and diagnosis Q&amp;A?</p><p>(1) Benefit of using a frozen 2D pre-trained vision encoder. To demonstrate the effectiveness of our lightweight image encoder based on the 2D pre-trained model, we take the query output embedding from MedQformer and compare it with features extracted from trainable ViT-G <ref type="bibr" target="#b34">Fang et al. [2022]</ref> on ADNI. We add a linear classification head with the cross-entropy loss. Table <ref type="table" target="#tab_3">3</ref> reports that MedQFormer achieves slightly reduced performances, i.e., 0.6% lower than ViT-G in accuracy, but with much fewer parameters (only 15.1% of ViT-G's). This lightweight module benefits downstream tasks and allows building our model on language models and training it on one GPU. We can also see that benefiting from this lightweight visual encoder, our MedBLIP outputs ViT-G by an improvement of 6.5% in the classification accuracy on ADNI.</p><p>(2) Effect of using different prompt structures. To answer the second question above, we investigate the order of three prompting components, i.e., image and text features, the question, and the answer, and its effect on our model's performance. We treat the one with the question in the middle as the regular prompt structure and compare it to the one starting with the question. Table <ref type="table" target="#tab_4">4</ref> shows that on some datasets our MedBLIP prefers the regular prompt, but this is not always the case. We conclude that the prompt strategy will not make a huge difference in the final performance of our model.</p><p>(3) Necessity of using two ITC loss functions. Besides the regular ITC loss between image and text pairs, we have another one between image and diagnosis Q&amp;A, as presented in Eq. 4. Table <ref type="table" target="#tab_5">5</ref> demonstrates that by adding the second ITC loss function, the classification accuracy improves on all datasets. This result is consistent with our motivation of adding the ITC loss between image and diagnosis Q&amp;A, since it enforces the learnable queries to extract image features related to CAD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Conclusion</head><p>In this paper, we propose a novel CAD system MedBLIP that fuses medical multi-modal data, i.e., image and text, from EHRs and shows its capability of performing zero-shot classification and medical VQA. Our MedBLIP introduces MedQFormer, a lightweight trainable 3D vision encoder that acts as a bridge between 3D medical images and a large frozen 2D vision encoder and as a bridge between 3D medical images and language models. Moreover, MedBLIP operates with low computational costs, as it smartly combines large pre-trained vision and language models with no need of training them from scratch or a large dataset in a specific medical domain. Our experiments demonstrate the effectiveness of our approach by outperforming several baselines, which sheds new light on further exploring medical multi-modal CAD.</p><p>Limitations and Future Work. LLMs can perform in-context learning given domain-specific fewshot examples. However, in our experiments with MedBLIP, we do not observe an improved VQA performance when asking something about the input brain scan context, even though it made the correct diagnosis. We attribute the unsatisfactory VQA results to the lack of corresponding textural descriptions of brain scans in our dataset. Without a description of a 3D brain MRI scan, the LLMs have difficulty describing what they "observe" in this image, such as the shrunken hippocampus or the enlarged ventricles. Currently, no such dataset or model is available to provide an image caption/description for a brain scan. We will explore this direction in our future work.</p><p>Besides, degenerative diseases like AD are often studied in the longitudinal setting since longitudinal atrophy has probably happened at an early stage of AD, making it easier to separate MCI subjects from normal controls. In future work, we will extend our model to take longitudinal inputs and further improve our classification accuracy. In addition, in our experiments, we only consider two modalities, i.e., MRIs and texts, other medical data sources, like positron emission tomography (PET) images, and audio, are also useful in diagnosing AD. Fortunately, the architecture of our MedBLIP is flexible enough to incorporate additional modalities, which is also left as our future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture overview of our proposed MedBLIP, a CAD system designed for medical diagnosis with electronic health records via multimodel representation learning in a language model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of our proposed MedQformer that aligns 3D visual and textural features for learning in the unified latent space of language model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>3.3, MedQformer includes both a transformer image encoder E I and a transformer text encoder E T . During training, we have a set of image-text pairs (I, T ) and a set of image and diagnosis Q&amp;A pairs (I, Q&amp;A). We use the image-text contrastive learning (ITC) loss in Radford et al. [2021] to align multi-modal representation, resulting in our feature alignment loss: L F A = contrastive (E I (I), E T (T )) + contrastive (E I (I), E T (Q&amp;A)) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>.2-year-old Female, with 16 years of education. The MMSE score is 27. The CDR score is 0.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Samples of zero-shot results on the AIBL dataset, which are generated by our MedBLIP built on BioMedLM with LoRA fine-tuning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Demographic statistics of used AD datasets. F: female, M: male, Educ: Education level, SES: Socio-Economic Status, MMSE: Mini-Mental State Examination, CDR: Clinical Dementia Rate, E/L/S/PMCI: early, late, stable, and progressive MCI, IMCI: Impaired not MCI, and DEM: demented. # indicates the number.</figDesc><table><row><cell cols="2">Datasets #Images</cell><cell>#F/#M</cell><cell>Age(#)</cell><cell cols="5">Texts Educ(#) SES(#) MMSE(#) CDR(#) Logical Memory(#)</cell><cell>Diagnosis(#)</cell></row><row><cell>ADNI</cell><cell cols="4">10387 4710/5677 45-95(10386) 9860</cell><cell>-</cell><cell>9385</cell><cell>9401</cell><cell>7189</cell><cell>NC,MCI,E/L/S/PMCI,AD (10387)</cell></row><row><cell>NACC</cell><cell cols="4">15354 9058/6296 19-102(15354) 15329</cell><cell>-</cell><cell>7867</cell><cell>15354</cell><cell>7654</cell><cell>NC, IMCI, MCI, DEM (14277)</cell></row><row><cell>OASIS</cell><cell cols="3">3020 1798/1222 18-98(3020)</cell><cell>2300</cell><cell>2153</cell><cell>2293</cell><cell>2300</cell><cell>-</cell><cell>DEM, Non-DEM (336)</cell></row><row><cell>AIBL</cell><cell>1002</cell><cell cols="2">471/531 42-96(1002)</cell><cell>-</cell><cell>-</cell><cell>1002</cell><cell>1002</cell><cell>1002</cell><cell>NC, MCI, AD (997)</cell></row><row><cell>MIRIAD</cell><cell>708</cell><cell>393/315</cell><cell>55-87(708)</cell><cell>-</cell><cell>-</cell><cell>268</cell><cell>46</cell><cell>-</cell><cell>NC, AD(708)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Experimental results of our MedBLIP on five datasets, including zero-shot CAD on the last two datasets. The classification performance is measured in the mean accuracy (ACC) with five runs. The best scores are in bold.</figDesc><table><row><cell>Methods</cell><cell></cell><cell>LM size</cell><cell>#Learnable params</cell><cell>ADNI -3x200</cell><cell>NACC -3x200</cell><cell>OASIS -2x200</cell><cell>AIBL</cell><cell>MIRIAD</cell></row><row><cell>FLAN-T5 Chung et al. [2022]</cell><cell>Text only</cell><cell></cell><cell>-</cell><cell>37.0%</cell><cell>39.5%</cell><cell>46.7%</cell><cell>33.3%</cell><cell>60.0%</cell></row><row><cell>Ours w/ T5</cell><cell>Frozen LoRA</cell><cell>3.4B</cell><cell>151M 156M</cell><cell>50.5% 64.0%</cell><cell>69.2% 77.3%</cell><cell>61.3% 75.8%</cell><cell>54.7% 59.2%</cell><cell>64.0% 66.8%</cell></row><row><cell>BioGPT Luo et al. [2022]</cell><cell>Text only</cell><cell></cell><cell>-</cell><cell>25.7%</cell><cell>21.7%</cell><cell>28.3%</cell><cell>26.7%</cell><cell>50.0%</cell></row><row><cell>Ours w/ BioGPT</cell><cell>Frozen LoRA</cell><cell>1.5B</cell><cell>151M 156M</cell><cell>56.3% 62.2%</cell><cell>66.5% 72.3%</cell><cell>66.0% 71.7%</cell><cell>60.7% 62.4%</cell><cell>55.2% 59.7%</cell></row><row><cell>BioMedLM Venigalla et al. [2022]</cell><cell>Text only</cell><cell></cell><cell>-</cell><cell>62.5%</cell><cell>63.5%</cell><cell>61.8%</cell><cell>65.7%</cell><cell>46.3%</cell></row><row><cell>Ours w/ BioMedLM</cell><cell>Frozen LoRA</cell><cell>2.7B</cell><cell>151M 154M</cell><cell>71.2% 78.7%</cell><cell>82.0% 83.3%</cell><cell>79.8% 85.3%</cell><cell>77.8% 80.8%</cell><cell>66.1% 71.0%</cell></row></table><note><p>T: 78-year-old Male. The MMSE score is 21.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison between a large vision encoder and our MedQFormer on ADNI.</figDesc><table><row><cell>Visual features</cell><cell cols="2">#Params Accuracy</cell></row><row><cell>ViT-G Fang et al. [2022]</cell><cell>1B</cell><cell>72.2%</cell></row><row><cell>Our MedQFormer</cell><cell>151M</cell><cell>71.6%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison between different prompt structures. 5% improvement in accuracy. Overall, our MedBLIP built upon BioMedLM and LoRA fine-tuning shows the best performance on all datasets.</figDesc><table><row><cell>Setting</cell><cell>ADNI -3x200</cell><cell>NACC -3x200</cell><cell>OASIS -2x200</cell><cell>AIBL</cell><cell>MIRIAD</cell></row><row><cell>Regular (I&amp;T, Q, A)</cell><cell>78.7%</cell><cell>83.3%</cell><cell>85.3%</cell><cell>80.8%</cell><cell>71.0%</cell></row><row><cell cols="6">Alternative (Q, I&amp;T, A) 79.3%(+0.6) 82.8%(-0.5) 82.5%(-1.8) 82.8%(+2.0) 70.8%(+0.2)</cell></row><row><cell>at most 14.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Ablation study on loss functions.</figDesc><table><row><cell>Loss Function</cell><cell>ADNI -3x200</cell><cell>NACC -3x200</cell><cell>OASIS -2x200</cell><cell>AIBL</cell><cell>MIRIAD</cell></row><row><cell>contrastive(I, T )</cell><cell>71.7%</cell><cell>80.5%</cell><cell>82.5%</cell><cell>74.7%</cell><cell>66.8%</cell></row><row><cell>contrastive(I, T ) + contrastive(I, Q&amp;A)</cell><cell>78.7%(+7.0)</cell><cell>83.3%(+2.8)</cell><cell>85.3%(+2.8)</cell><cell>80.8%(+6.1)</cell><cell>71.0%(+4.2)</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Alzheimer&apos;s disease neuroimaging initiative (adni): clinical characterization</title>
		<author>
			<persName><forename type="first">Ronald</forename><surname>Carl Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurel</forename><forename type="middle">A</forename><surname>Paul S Aisen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">C</forename><surname>Beckett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><forename type="middle">Collins</forename><surname>Donohue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><forename type="middle">J</forename><surname>Gamst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clifford</forename><forename type="middle">R</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Jack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leslie</forename><forename type="middle">M</forename><surname>Jagust</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><forename type="middle">W</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><surname>Toga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurology</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="201" to="209" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The national alzheimer&apos;s coordinating center (nacc) database: the uniform data set</title>
		<author>
			<persName><forename type="first">Duane</forename><forename type="middle">L</forename><surname>Beekly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erin</forename><forename type="middle">M</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Woodrow</forename><forename type="middle">D</forename><surname>Deitrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><forename type="middle">E</forename><surname>Jacka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joylee</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janene</forename><forename type="middle">L</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">D</forename><surname>Koepsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><forename type="middle">A</forename><surname>Kukull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Alzheimer Disease &amp; Associated Disorders</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="249" to="258" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Open access series of imaging studies (oasis): cross-sectional mri data in young, middle aged, nondemented, and demented older adults</title>
		<author>
			<persName><forename type="first">Tracy</forename><forename type="middle">H</forename><surname>Daniel S Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">G</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">C</forename><surname>Csernansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Randy</forename><forename type="middle">L</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><surname>Buckner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of cognitive neuroscience</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1498" to="1507" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="12888" to="12900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.12597</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Vlmo: Unified vision-language pre-training with mixtureof-modality-experts</title>
		<author>
			<persName><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Owais</forename><surname>Khan Mohammed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kriti</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhojit</forename><surname>Som</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songhao</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="32897" to="32912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A simple baseline for zero-shot semantic segmentation with pre-trained vision-language model</title>
		<author>
			<persName><forename type="first">Mengde</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.14757</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Zifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenbang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinesh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><surname>Medclip</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.10163</idno>
		<title level="m">Contrastive learning from unpaired medical images and text</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Biomedlm: a domain-specific large language model for biomedical text</title>
		<author>
			<persName><surname>Venigalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><surname>Carbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MosaicML</title>
		<imprint>
			<date type="published" when="2022-12-23">Dec, 23, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shean</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Lora</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09685</idno>
		<title level="m">Low-rank adaptation of large language models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The australian imaging, biomarkers and lifestyle (aibl) study of aging: methodology and baseline characteristics of 1112 individuals recruited for a longitudinal study of alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">Kathryn</forename><forename type="middle">A</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashley</forename><forename type="middle">I</forename><surname>Bush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Darby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniela</forename><forename type="middle">De</forename><surname>Fazio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><forename type="middle">T</forename><surname>Lautenschlager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nat</forename><surname>Lenzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><forename type="middle">N</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Maruff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International psychogeriatrics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="672" to="687" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Miriad-public release of a multiple time point alzheimer&apos;s mr imaging dataset</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">B</forename><surname>Malone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Cash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerard</forename><forename type="middle">R</forename><surname>Ridgway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">G</forename><surname>Macmanus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastien</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><forename type="middle">C</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">M</forename><surname>Schott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="33" to="36" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun-Hsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4904" to="4916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm</title>
		<author>
			<persName><forename type="first">Yangguang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.05208</idno>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Align before fuse: Vision and language representation learning with momentum distillation</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramprasaath</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akhilesh</forename><surname>Gotmare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="9694" to="9705" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Legg</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mojtaba</forename><surname>Seyedhosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.01917</idno>
		<title level="m">Coca: Contrastive captioners are image-text foundation models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Expertlevel detection of pathologies from unannotated chest x-ray images via self-supervised learning</title>
		<author>
			<persName><forename type="first">Ekin</forename><surname>Tiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Talius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pujan</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Curtis</forename><forename type="middle">P</forename><surname>Langlotz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Biomedical Engineering</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning to exploit temporal structure for biomedical vision-language processing</title>
		<author>
			<persName><forename type="first">Shruthi</forename><surname>Bannur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Hyland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianchu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Perez-Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Ilse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benedikt</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harshita</forename><surname>Boecking</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenza</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anja</forename><surname>Bouzid</surname></persName>
		</author>
		<author>
			<persName><surname>Thieme</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.04558</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visualgpt: Data-efficient adaptation of pretrained language models for image captioning</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="18030" to="18040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multimodal few-shot learning with frozen language models</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Tsimpoukelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><forename type="middle">L</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serkan</forename><surname>Cabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="200" to="212" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Flamingo: a visual language model for few-shot learning</title>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pauline</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yana</forename><surname>Hasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karel</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="23716" to="23736" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Scaling instruction-finetuned language models</title>
		<author>
			<persName><forename type="first">Chung</forename><surname>Hyung Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Brahma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.11416</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Opt: Open pre-trained transformer language models</title>
		<author>
			<persName><forename type="first">Susan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.01068</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Grounding language models to images for multimodal generation</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Yu Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Fried</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.13823</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02311</idno>
		<title level="m">Scaling language modeling with pathways</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Danny</forename><surname>Driess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corey</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayzaan</forename><surname>Ichter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Wahid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhe</forename><surname>Vuong</surname></persName>
		</author>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.03378</idno>
		<title level="m">Palm-e: An embodied multimodal language model</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">OpenAI. Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Chatcad: Interactive computer-aided diagnosis on medical image using large language models</title>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinggang</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.07257</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Open-ended medical visual question answering through prefix tuning of language models</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Tom Van Sonsbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivona</forename><surname>Mahdi Derakhshani</surname></persName>
		</author>
		<author>
			<persName><surname>Najdenkoska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcel</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><surname>Worring</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.05977</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hugging face</title>
		<author>
			<persName><forename type="first">Mohan</forename><surname>Shashank</surname></persName>
		</author>
		<author>
			<persName><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Introduction to Transformers for NLP: With the Hugging Face Library and Models to Solve Problems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="51" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Documenting the english colossal clean crawled corpus</title>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><surname>Marasovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Agnew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08758</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Travis</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noa</forename><surname>Nabeshima</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00027</idno>
		<title level="m">The pile: An 800gb dataset of diverse text for language modeling</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Exploring the limits of masked visual representation learning at scale</title>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binhui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><surname>Eva</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.07636</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Biogpt: generative pre-trained transformer for biomedical text generation and mining</title>
		<author>
			<persName><forename type="first">Renqian</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings in Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
