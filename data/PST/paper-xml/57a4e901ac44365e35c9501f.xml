<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Extended Local Binary Patterns for Face Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-04-18">April 18, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Li</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Information System and Management</orgName>
								<orgName type="institution">National University of Defense Technology</orgName>
								<address>
									<addrLine>47 Yanwachi</addrLine>
									<postCode>410073</postCode>
									<settlement>Changsha, Hunan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Paul</forename><surname>Fieguth</surname></persName>
							<email>pfieguth@uwaterloo.ca</email>
							<affiliation key="aff3">
								<orgName type="department">Department of Systems Design Engineering</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<postCode>N2L 3G1</postCode>
									<settlement>Waterloo</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guoying</forename><surname>Zhao</surname></persName>
							<email>gyzhao@ee.oulu.fi</email>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">The Center for Machine Vision Research</orgName>
								<orgName type="department" key="dep2">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Oulu</orgName>
								<address>
									<postCode>90014</postCode>
									<settlement>Oulu</settlement>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matti</forename><surname>Pietik Äinen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dewen</forename><surname>Hu</surname></persName>
							<email>dwhu@nudt.edu.cn</email>
							<affiliation key="aff5">
								<orgName type="department">School of Mechatronics and Automation</orgName>
								<orgName type="institution">National University of Defense Technology</orgName>
								<address>
									<addrLine>47 Yanwachi</addrLine>
									<postCode>410073</postCode>
									<settlement>Changsha, Hunan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matti</forename><surname>Pietikäinen</surname></persName>
							<email>matti.pietikainen@ee.oulu.fi</email>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">The Center for Machine Vision Research</orgName>
								<orgName type="department" key="dep2">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Oulu</orgName>
								<address>
									<postCode>90014</postCode>
									<settlement>Oulu</settlement>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Extended Local Binary Patterns for Face Recognition, Information Sciences</orgName>
								<address>
									<postCode>2016)</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Extended Local Binary Patterns for Face Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-04-18">April 18, 2016</date>
						</imprint>
					</monogr>
					<idno type="MD5">7B9B805637AAAB7A99BC1FEABD089CC1</idno>
					<idno type="DOI">10.1016/j.ins.2016.04.021</idno>
					<note type="submission">Received date: 17 June 2015 Revised date: 28 March 2016 Accepted date: 9 April 2016 Preprint submitted to Information Sciences</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Face recognition</term>
					<term>feature extraction</term>
					<term>local binary pattern</term>
					<term>local descriptors</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a simple and novel, yet highly effective approach for robust face recognition. Using LBPlike descriptors based on local accumulated pixel differences -Angular Differences and Radial Differences, the local differences were decomposed into complementary components of signs and magnitudes. Based on these descriptors we developed labeled dominant patterns where the most frequently occurring patterns and their labels were learned to capture discriminative textural information. Six histogram features were obtained from each given face image by concatenating spatial histograms extracted from non-overlapping subregions.</p><p>A whitened PCA technique was used for dimensionality reduction to produce more compact, robust and discriminative features, which were then fused using the nearest neighbor classifier, with Euclidean distance as the similarity measure.</p><p>We evaluated the effectiveness of the proposed method on the Extended Yale B, the large-scale FERET, and CAS-PEAL-R1 databases, and found that that the proposed method impressively outperforms other well-known systems with a recognition rate of 74.6% on the CAS-PEAL-R1 lighting probe set.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Highlights</head><p>• A set of six LBP-like features derived from local intensities and differences;</p><p>• A labeled dominant pattern scheme is proposed to learn salient information;</p><p>• Utilizing Whitened PCA to produce more compact, robust and discriminative features;</p><p>• Fused WPCA features improve the accuracy and robustness of face recognition;</p><p>• Proposed face recognition system is highly robust to illumination variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Face recognition, as one of the most successful applications of image analysis and understanding, has received considerable attention in the past decades due to its challenging nature and vast range of applications <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b17">18]</ref>. In recent years this field progressed significantly and a number of face recognition and modeling systems have been developed <ref type="bibr" target="#b17">[18]</ref>. Even though current face recognition systems have reached a certain level of maturity, the performance of these systems in uncontrolled environments is still far from the capability of the human vision system <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b17">18]</ref>. In other words, there are still many challenges associated with accurate and robust face recognition in regards to computer vision and pattern recognition, especially under unconstrained environments. Therefore, this remains a stimulating field for further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>As a classical pattern recognition problem, face recognition primarily consists of two critical subproblems: feature extraction and classifier designation, both of which have been the subject of significant study.</p><p>Generally, facial feature description plays a relatively more important role if poor features are used, and even the best classifier will fail to achieve good recognition results. Numerous facial feature sets were presented with excellent surveys in reports by Zhao et al. <ref type="bibr" target="#b41">[42]</ref> and Li and Jain <ref type="bibr" target="#b17">[18]</ref>.Nevertheless, designing useful face descriptors remains a great challenge when faced with three competing goals: Computational efficiency, effective discrimination, and robustness to intra-person variations (including changes in illumination, pose, expression, age, blur, and occlusion).</p><p>Most facial extraction approaches can be categorized as utilizing either holistic features or local features.</p><p>The holistic approach uses the entire face region to construct a subspace representing a face image; Influential examples includes Principle Component Analysis (PCA) <ref type="bibr" target="#b35">[36]</ref>, Linear Discriminant Analysis (LDA) <ref type="bibr" target="#b2">[3]</ref>, Locally Linear Embedding (LLE) <ref type="bibr" target="#b32">[33]</ref> and Locally Preserving Projections (LPP) <ref type="bibr" target="#b10">[11]</ref>. In contrast, the local features approach involves local features first being extracted from a subregion of a face and then classified by combining and comparing with corresponding local statistics. Holistic features are unable to capture local variations in face appearance, and are more sensitive to variations in illumination, expression and occlusions <ref type="bibr" target="#b17">[18]</ref>. Local feature based approaches are advantageous in that distributions of face images in local feature space are less affected by changes in facial appearance. As a result, local feature based face recognition approaches have been widely studied in recent years.</p><p>Among local feature approaches, Local Binary Patterns (LBP) have emerged as one of the most prominent face analysis methods since the pioneering work by Ahonen et al. <ref type="bibr" target="#b0">[1]</ref>, and have attracted increasing attention due to their distinguished advantages: (1) ease of implementation; <ref type="bibr" target="#b1">(2)</ref> invariance to monotonic illumination changes; and (3) low computational complexity. However, the original LBP method still has multiple limitations: (i) production of long histograms; (ii) capture of only very local texture structure and not long range information; (iii) limited discriminative capability based purely on local binarized differences; and (iv) limited noise robustness. On the basis of these issues, many LBP variants <ref type="bibr" target="#b11">[12]</ref> have been proposed to improve face recognition performance.</p><p>Our research is motivated by recent work on texture classification in <ref type="bibr" target="#b23">[24]</ref>, where four LBP-like descriptors -Center Intensity based LBP (CI-LBP), Neighborhood Intensity based LBP (NI-LBP), Radial Difference based LBP (RD-LBP) and Angular Difference based LBP (AD-LBP) -were proposed, along with multiscale joint histogram features, which were found to be highly effective to rotation invariant texture classification.</p><p>We expanded this research <ref type="bibr" target="#b23">[24]</ref> to address the face identification problem by proposing a framework and more generalized formulation of the local intensities and differences features. Specifically, the major contributions of our work are summarized as follows:</p><p>1. We proposed a new family of LBP-like descriptors based on local accumulated pixel differences: Angular Differences (AD) and Radial Differences (RD). The descriptors presented advantages of efficiency, complementarity to LBP, robustness, and the encoding of both microstructures and macrostructures, resulting in a more complete image representation. The extraction of the proposed descriptors did not require any training, and thus this approach showed better generalizability compared with popu- lar learning methods, whose performance is degraded if the distribution of the testing sample varies significantly from that of the training set.</p><formula xml:id="formula_0">A C C E P T E D M A N U S C R I P T r c x 2 p p , ,0 r p x , ,1 r p x , ,2 r p x , , 1 r p p x - , , 2 r p p x -</formula><p>2. Using the proposed descriptors, we found that the properties of the original uniform patterns introduced by Ojala et al. <ref type="bibr" target="#b29">[30]</ref> did not hold true, and therefore we suggest the use of full patterns. We also proposed a Labeled Dominant Pattern (LDP) scheme, which learns a set of dominant patterns (the most frequently occurring patterns from a set of training images) to capture discriminative textural information, but with lower dimensionality than the full pattern approach.</p><p>3. Extensive experiments were conducted on the Extended Yale B, theFERET and the large-scale CAS-PEAL-R1 databases. Our approach proved to be highly robust to illumination changes, as evident by our recognition rate of 74.6% on the CAS-PEAL-R1 lighting probe set. This is, to the best of our knowledge, the highest score yet achieved on this data set.</p><p>A preliminary version of this work appeared in <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The primary motivation of this work is to design novel LBP-like descriptors and apply them to face recognition, thus the related literature focuses on LBP and its variants for face recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Brief Review of LBP</head><p>LBP was originally proposed for texture analysis <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>, and was later introduced to face recognition by Ahonen et al. <ref type="bibr" target="#b0">[1]</ref>. Since then, LBP-based facial image analysis has been one of its most popular and successful applications <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>LBP characterizes the spatial structure of a local image texture by thresholding a 3 × 3 square neighborhood with the value of the center pixel and considering only the sign information to form a local binary pattern. A more general formulation defined on circular symmetric neighborhood systems was proposed by Ojala et al. <ref type="bibr" target="#b29">[30]</ref> that allowed for multi-resolution analysis and rotation invariance. Formally, given a pixel x 0,0 in the image, the LBP pattern was computed by comparing its value with those of its p neighboring pixels that were evenly distributed in angle on a circle of radius r centered on x 0,0 , as shown in Fig. <ref type="figure" target="#fig_0">1</ref>, such that the LBP response was calculated as</p><formula xml:id="formula_1">x r,p = [x r,p,0 , . . . , x r,p,p-1 ] T A C C E P T E D M A N U S C R I P T X c X 1 X 2 X 0 X 3 X 4 X 5 X 6 X 7 w w r (a) (b)</formula><formula xml:id="formula_2">LBP r,p = p-1 n=0 s(x r,p,n -x c )2 n ,<label>(1)</label></formula><p>where s(•) was the sign function, defined as follows:</p><formula xml:id="formula_3">s(x) =    1 x ≥ 0 0 x &lt; 0<label>(2)</label></formula><p>Relative to the origin at (0, 0), the coordinates of the neighbors were given by -r sin(2πn/p), r cos(2πn/p).</p><p>The intensity values of neighbors not lying on a pixel location were estimated by interpolation.</p><p>The original descriptor LBP r,p produces 2 p different output values, corresponding to the 2 p different binary patterns formed by the p pixels in the neighborhood. Due to the rapid increase in the number of patterns for basic LBP r,p as p is increased, it is difficult to apply LBP to large neighborhoods, therefore limiting its applicability.</p><p>In order to reduce the dimensionality of the LBP histogram feature and increase its robustness to noise, Ojala et al. <ref type="bibr" target="#b29">[30]</ref> introduced uniform patterns, LBP u2 r,p , where a binary pattern is uniform if it contains a maximum of two bitwise transitions from 0 to 1, or vice versa when the bit pattern is considered circularly.</p><p>There are p(p -1) + 2 uniform patterns, and all of the remaining non-uniform patterns are accumulated into a single bin, resulting in a p(p -1) + 3 dimensional representation, and a great reduction to the 2 p dimensionality of LBP. The success of the LBP u2 r,p operator came from the experimental observations that the uniform patterns appeared to be fundamental properties of face images <ref type="bibr" target="#b0">[1]</ref>, representing salient local texture structure.</p><p>As aforementioned, the original LBP method has limitations to be addressed, and therefore, a number of extensions and modifications to it have been developed <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Changing Neighborhood Topology</head><p>Some LBP variants were proposed by changing the topology of the neighborhood from which the LBP features were computed, such as the Multi-scale Block LBP (MBLBP) <ref type="bibr" target="#b19">[20]</ref>, Three Patch LBP (TPLBP) <ref type="bibr" target="#b37">[38]</ref> A C C E P T E D M A N U S C R I P T and Four Patch LBP (FPLBP) <ref type="bibr" target="#b37">[38]</ref>. In MBLBP, the neighborhood shown in Fig. <ref type="figure" target="#fig_1">2</ref> (b) was adopted: a w × w patch centered on the pixel of interest and n additional patches distributed uniformly in a ring of radius r around it. The LBP computation was done based on comparison of the average values of the neighboring patches against that of the center patch. MBLBP was claimed to be more robust than LBP since it encoded not only microstructures but also macrostructures of image patterns. A similar neighborhood was used by TPLBP <ref type="bibr" target="#b37">[38]</ref>, which computes patch differences, rather than single pixel or averaged pixel differences, as in LBP and MBLBP, respectively. TPLBP captured information complementary to pixel based descriptors.</p><p>The Patterns of Oriented Edge Magnitudes (POEM) descriptor proposed by Vu et al. <ref type="bibr" target="#b36">[37]</ref> also used the same neighborhood structure as MBLBP and TPLBP, but computed LBP-like features on histograms of gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Increasing Discriminative Power</head><p>Researchers found it helpful to preprocess an image prior to LBP computation, where the most popular preprocessing method is Gabor filtering, because Gabor and LBP provides complementary information: LBP captures local information, while Gabor filters are non-local. Two such representative works are Local Gabor Binary Pattern Histogram Sequence (LGBPHS) <ref type="bibr" target="#b40">[41]</ref> and Histogram of Gabor Phase Patterns (HGPP) <ref type="bibr" target="#b39">[40]</ref>. In both methods, an image is first convolved with 40 Gabor filters at five different scales and eight orientations, which is computationally expensive. In LGBPHS, the LBP method is then applied to all 40 Gabor magnitude images. In HGPP, the Gabor phase information is encoded for 90 images of the same size as the original. Due to richer information from this additional Gabor filtering stage, LGBPHS and HGPP improve face recognition performance when compared to the original LBP method. However, the Gabor filtering step is accompanied by a heavy computational burden, and the extremely high dimensionality of histogram representation limits the use of these methods in real-time applications. Other promising work includes the Sobel-LBP approach <ref type="bibr" target="#b42">[43]</ref> and the Local Derivative Pattern (LDP) <ref type="bibr" target="#b38">[39]</ref>.</p><p>Recently researchers presented LBP-like descriptors based on discriminative learning. Notable examples include Decision Tree-based LBP (DTLBP) <ref type="bibr" target="#b24">[25]</ref>, Discriminative LBP (DLBP) <ref type="bibr" target="#b25">[26]</ref>, Local Quantized Patterns (LQP) <ref type="bibr" target="#b13">[14]</ref> and Discriminative Face Descriptor (DFD) <ref type="bibr" target="#b16">[17]</ref>. DTLBP attempts to learn discriminative patterns using decision tree induction, but one drawback of the DTLBP is the high cost of constructing and storing the decision trees, especially when large neighborhoods are used. The DLBP method represents LBP-like descriptors as a set of pixel comparisons between a center pixel and its 48 neighboring pixels in a 7 × 7 patch, and seeks a subset from among all comparisons that maximizes the Fisher discrimination power. Unfortunately, this is an intractable combinatorial optimization problem. Conversely, LQP makes use of vector quantization and lookup tables to allow local pattern features a larger neighborhood and more quantization levels. LQP adopts the neighborhood of Fig. <ref type="figure" target="#fig_1">2</ref> (a), and attempts to learn a codebook from all the LBP codes that have been extracted. This codebook learning is not feasible with a large neighborhood since the number of local binary patterns grows exponentially with the number of neighboring pixels. A further constraint is the size of the lookup table, which must store the output code of each possible input pattern. Finally, the recent DFD descriptor learns the most discriminating local features, by computing the difference vectors between the center patch and each of its neighboring patches to form a Pixel Difference   Matrix, which is then projected and re-grouped to form the discriminant pattern vector. A standard bag-ofwords model <ref type="bibr" target="#b20">[21]</ref> is applied for face representation. The DFD descriptor performs well in face identification and verification, but involves a number of parameters, has high dimensionality, requires time-consuming training, and has high training data needs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Enhancing Noise Robustness</head><p>Tan and Triggs <ref type="bibr" target="#b34">[35]</ref> introduced Local Ternary Patterns (LTP), where the binary LBP code was replaced by a ternary one. The LTP method was more resistant to noise, but not strictly invariant to gray-scale changes, and the selection of additional threshold values was not easy. Ahonen et al. <ref type="bibr" target="#b1">[2]</ref> introduced Soft LBP (SLBP) histograms, which enhanced robustness by incorporating fuzzy membership in the representation of local texture primitives. Fuzzification allows multiple local binary patterns to be generated, and thus one pixel position can contribute to more than a single bin in the histogram of possible LBP patterns. However, this sacrifices invariance to monotonic variations, and computation of the contribution weights creates a computational burden. Noting this disadvantage of SLBP, Ren et al. <ref type="bibr" target="#b31">[32]</ref> proposed a much more efficient variant: the Noise Resistant LBP (NRLBP), which has fast contribution weight computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Combining Approaches</head><p>A current trend in the development of new effective local descriptors is combining the strengths of multiple complementary descriptors. Huang et al. <ref type="bibr" target="#b12">[13]</ref> proposed the Extended LBP (ELBP) method, which not only performed binary comparison between the central pixel and its neighbors, but also encoded their gray value differences. Motivated by ELBP <ref type="bibr" target="#b8">[9]</ref>, Guo et al. developed a Completed LBP (CLBP) method by decomposing the local differences into two complementary components, the signs and the magnitudes, which were coded using two LBP-like descriptors. Chan et al. <ref type="bibr" target="#b5">[6]</ref> proposed to combine multi-scale LBP with multi-scale Local Phase Quantization (LPQ) using kernel fusion for face recognition, and claimed improved performance. Klare et al. <ref type="bibr" target="#b14">[15]</ref> studied the combination of LBP and SIFT for heterogeneous face recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Extended LBP Descriptors</head><p>Guo et al. <ref type="bibr" target="#b8">[9]</ref> proposed a complete LBP for texture classification, which included both the sign and the magnitude components between a given central pixel and its neighbors in order to improve the discriminative power of the original LBP operator. The operator derived from the sign component, denoted as LBP S, is the same as the original LBP operator defined in (2):</p><formula xml:id="formula_4">LBP S r,p = LBP r,p .<label>(3)</label></formula><p>The operator computed from the magnitude component, denoted as LBP M, performed a binary comparison between the absolute value of the difference between the central pixel and its neighbors and a global threshold to generate an LBP-like code:</p><formula xml:id="formula_5">LBP M r,p = p-1 n=0 s(|x r,p,n -x 0,0 | -µ r,p )2 n<label>(4)</label></formula><p>where</p><formula xml:id="formula_6">µ r,p = 1 p p-1 n=0 |x r,p,n -x 0,0 |.<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Angular-Differences Based Descriptors</head><p>LBP encodes only the relationship between a central point and its neighbors. Moreover, its capability of encoding image configuration and pixel-wise relationships is further weakened by the binary quantization of pixel differences. Although extended to facilitate analysis of image textures at multiple scales <ref type="bibr" target="#b29">[30]</ref> by varying the sampling radius (r) and the number of sampling points (p), important information regarding the relationship between neighboring points on the same radius (intra-radius) and the relationship between neighboring points across different radii (inter-radius) was lost.</p><p>A simple example is shown in Fig. <ref type="figure" target="#fig_3">3</ref>. For standard LBP the two patterns were classified as the same class, although the textural structures represented were actually quite different from one another. More specifically, the top and bottom patterns in Fig. <ref type="figure" target="#fig_3">3</ref> show quite different inter-radius and intra-radius intensity variations, information which cannot be revealed by LBP S since it encodes only the relationship between a center pixel and its neighbors. With the aim to address these issues, we investigated the feasibility and effectiveness of encoding angular and radial difference information for face representation, where the angular and radial directions were on a circular grid. More specifically, we proposed two families of related descriptors, the For each pixel in an image, we considered the accumulated angular differences computed from its uniformly distributed neighbors in a ring of radius r. Similar to the sampling scheme in the original LBP approach, we sampled pixels around a central pixel x 0,0 , but restricted the number of points on any circle of radius r sampled to be a multiple of eight, and thus p = 8q for some positive integer q. Therefore, the neighbors of x 0,0 sampled on radius r are x r,8q = [x r,8q,0 , • • • , x r,8q,8q-1 ] T .</p><p>Empirical tests suggested that averaging over two adjacent radii produced the best results, so the ADLBP S descriptor was computed by accumulating the angular differences of radii r and r -1:</p><formula xml:id="formula_7">ψ r,δ,q = ∆ Ang r,δ,q + ∆ Ang r-1,δ,q<label>(6)</label></formula><p>where</p><formula xml:id="formula_8">∆ Ang r,δ,q = [∆ Ang r,δ,q,0 , • • • , ∆ Ang r,δ,q,8q-1 ] T (7) ∆ Ang r,δ,q,n = x r,p,n -x r,p,mod(n+δ,p)<label>(8)</label></formula><p>The latter term is the angular difference computed over a given angular displacement δ(2π/p), where x r,p,n and x r,p,mod(n+δ,p) correspond to the values of pixels at radius r spaced δ elements apart.</p><p>Considering the rapid increase of the number of binary patterns with the increase of sampling points p, we did not want to derive local binary patterns directly from ψ Ang r,δ,q , and instead followed the idea in our recent work in <ref type="bibr" target="#b22">[23]</ref>. We transformed ψ Ang r,δ,q by local averaging along an arc, φ AngSign r,δ,q,n = 1 q q-1 k=0 ψ Ang r,δ,q,(qn+k) , n = 0, . . . , 7,</p><p>as illustrated in Fig. <ref type="figure" target="#fig_5">4</ref>, such that the number of elements in φ AngSign r,δ,q is always eight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Finally, given the sign component vector φ AngSign r,δ,q</p><p>, we trivially computed a binary pattern as in LBP S in (2):</p><formula xml:id="formula_10">ADLBP S r,δ,q = 7 n=0 s(φ AngSign r,δ,q,n )2 n<label>(10)</label></formula><p>Similar to LBP M, we also proposed the ADLBP M descriptor based on the magnitude component of angular differences |ψ Ang r,δ,q |. Specifically, we transformed the absolute angular differences |ψ Ang r,δ,q |, via local averaging as in ( <ref type="formula" target="#formula_9">9</ref>):</p><formula xml:id="formula_11">φ AngMag r,δ,q,n = 1 q q-1 k=0 |ψ Ang r,δ,q,(qn+k) |, n = 0, . . . , 7.<label>(11)</label></formula><p>from which we derived ADLBP M as in LBP M:</p><formula xml:id="formula_12">ADLBP M r,δ,q = 7 n=0 s(φ AngMag r,δ,q,n -µ AngMag r,δ,q )2 n ,<label>(12)</label></formula><p>where µ AngMag r,δ,q is the average of {φ AngMag r,δ,q,n } 7 n=0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Radial-Differences Based Descriptors</head><p>In order to obtain the radial-differences based descriptor RDLBP S and RDLBP M, we first computed the radial difference vector:</p><formula xml:id="formula_13">∆ Rad r,δ,q = [∆ Rad r,δ,q,0 , • • • , ∆ Rad r,δ,q,8q-1 ] T ,<label>(13)</label></formula><p>where ∆ Rad r,δ,q,n = x r,p,nx r-δ,p,n is the radial difference computed with given integer radial displacement δ, and x r,p,n and x r-δ,p,n correspond to the intensity values of pairs of pixels spaced δ apart in radius but in the same direction, as shown in Fig. <ref type="figure" target="#fig_5">4</ref>.</p><p>We computed the transformed sign and magnitude components φ RadSign r,δ,q and φ RadMag r,δ,q by averaging, as before:</p><formula xml:id="formula_14">φ RadSign r,δ,q,n = 1 q q-1 k=0 ∆ Rad r,δ,q,(qn+k) , n = 0, . . . , 7<label>(14)</label></formula><formula xml:id="formula_15">φ RadMag r,δ,q,n = 1 q q-1 k=0 |∆ Rad r,δ,q,(qn+k) |, n = 0, . . . , 7.<label>(15)</label></formula><p>Similar to <ref type="bibr" target="#b9">(10)</ref> and ( <ref type="formula" target="#formula_12">12</ref>), the RDLBP S and RDLBP M descriptors are defined as follows:</p><formula xml:id="formula_16">RDLBP S r,δ,q = 7 n=0 s(φ RadSign r,δ,q,n )2 n (16) RDLBP M r,δ,q = 7 n=0 s(φ RadMag r,δ,q,n -µ RadMag r,δ,q )2 n ,<label>(17)</label></formula><p>where µ RadMag r,δ,q is the average of {φ RadMag r,δ,q,n } 7 n=0 .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Statistical Distribution Examination of Patterns</head><p>In standard face recognition tasks using LBP, only uniform patterns were used <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b36">37]</ref>. The nonuniform patterns were considered in only a single bin of the histogram that was used to extract features in the classification stage. However the representation of texture information using only uniform patterns may be problematic <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b9">10]</ref>, since the uniform patterns may not be the dominant patterns, especially when the radius of the LBP operator increased. Fig. <ref type="figure" target="#fig_7">5</ref> shows distributions of the patterns extracted from the gallery faces in the FERET database <ref type="bibr" target="#b30">[31]</ref> for Increase the corresponding bin by 1, h l(xk) + + end end Sort h in descending order, resulting h sort . Change the configuration of the pattern index vector v according to the element order of h sort , leading to vector v sort .  any image was represented by the probabilities of the dominant patterns. In summary, the pseudo-codes on determining the LDP patterns of an LBP descriptor is presented in Fig. <ref type="figure" target="#fig_10">6</ref>.</p><formula xml:id="formula_18">for k ← 1 to d do if ( k i=1 h sort i )/( d i=1 h sort i ) &gt; τ break end end Return S LDP = {v sort 1 , • • • , v sort k }.</formula><p>The proposed LDP scheme is different from the DLBP approach proposed by Liao et al. <ref type="bibr" target="#b18">[19]</ref>. In DLBP, for each image I i ∈ S train in the training set, the pattern histogram was extracted and sorted in descending order. Then the smallest number of patterns k i for 80% pattern occurrences was found. Finally, the average number k = 1 |S train | k i was defined as the required number of patterns that occupy dominant ratio. The DLBP approach was not limited to consider only a fixed set of patterns. For two different texture images, the dominant patterns could be of different types. It is important to remark that this is an unlabeled feature reduction scheme in which no information is retained about the patterns' labels, and some important information was lost. In contrast, in our approach, the strong pattern occurrences were defined jointly for the entire image set, such that a consistent set of features was extracted for each image, and thus each feature dimension corresponded to a fixed pattern type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Face Representation</head><p>With the proposed descriptors detailed in Section 3, our face feature representation pipeline is similar to the one proposed by Ahonen et al. <ref type="bibr" target="#b0">[1]</ref>, but incorporates a more sophisticated illumination normalization step as used by Tan and Triggs <ref type="bibr" target="#b34">[35]</ref>. Fig. <ref type="figure" target="#fig_5">4</ref> illustrates the proposed extended set of features, and Fig. <ref type="figure" target="#fig_11">7</ref> summarizes the main operation for the face recognition pipeline. Histogram feature extraction from a face image involves the following steps:</p><p>(1) Crop the face region and align the face by mapping the eyes to a canonical location with a similarity transform.</p><p>(2) Photometrically normalize the cropped faces by utilizing the preprocessing sequence approach <ref type="bibr" target="#b34">[35]</ref>.</p><p>(3) Divide the face image into a non-overlapping grid with equally sized subregions. (4) For each subregion, apply a feature extraction operator (e.g. ADLBP S) to each pixel. Afterward, create a histogram of the feature values and concatenate these histograms into a single feature vector to represent the face image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Histogram Features</head><p>In order to incorporate more spatial information into the final descriptor, the proposed feature images were spatially divided into multiple non-overlapping regions and histograms were extracted from each region.</p><p>More precisely, a face image I i is divided into √ w × √ w non-overlapping sub-regions, from each of which a sub-histogram feature of dimensionality m is extracted and is normalized to sum one. By concatenating these regional sub-histograms into a single vector, a final histogram feature h o i of dimensionality n = mw is obtained for face representation. We propose to compare three choices for the extracted features: The uniform ("u2") patterns, all of the patterns ("full"), and the LDP scheme ("ldp") as just discussed in Section 3.3.</p><formula xml:id="formula_19">Let h o 1i , h o 2i , h o 3i , h o 4i , h o 5i</formula><p>and h o 6i denote the histogram feature produced by descriptor LBP S, LBP M, ADLBP S, ADLBP M, RDLBP S and RDLBP M respectively, as illustrated in Fig. <ref type="figure" target="#fig_11">7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Dimensionality Reduction and Feature Fusion</head><p>The proposed extended set of LBP descriptors provide rich information, but need to be used to construct a powerful classifier. Recent methods frequently combine multiple local features such as LBP, LTP, SIFT, LPQ and Gabor <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b14">15]</ref>, where the actual combination strategy ranges from simple summing at the decision level to multiple kernel learning. For the specific problem of face recognition, linear projection models <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b36">37]</ref> and kernel LDA <ref type="bibr" target="#b34">[35]</ref> have been used. We strived to show that the proposed radial and angular difference descriptors, which are designed to encode additional types of local texture information, do capture information complementary to that of intensity-based descriptors and thus, when combined, improve overall face recognition performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>A straightforward way to fuse the proposed features is to concatenate the feature vectors. Since the dimensionality n of a single histogram feature h o ki (k = 1, ..., 6) is already relatively high, a drawback of feature concatenation is the so-called "curse of dimensionality", creating significant burdens on memory, storage and computation. Like other researchers <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17]</ref>, we believe that strong correlations exist between the proposed features and that most of the dimensions carry redundant information, and hence further dimension reduction is necessary.</p><p>The standard PCA technique selects a dimensionality reducing linear projection that maximizes the scatter of all projected samples, which means the scatter being maximized is due not only to the interclass scatter that is useful for classification, but also to the intraclass scatter unwanted for classification purposes.</p><p>PCA has two disadvantages: (1) the leading eigenvectors primarily encode variations such as illumination and expression, rather than information that is useful for discrimination <ref type="bibr" target="#b2">[3]</ref>; and (2) the Mean-Square-Error principle underlying PCA favors low frequencies, and results in the loss of discriminative information contained in high frequency components.</p><p>The idea behind WPCA <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17]</ref> is that discriminant information is equally distributed along all the principle components. Therefore, all the principal components are divided by the square-roots of the corresponding eigenvalues to normalize the projected features to the same variance. Specifically, suppose we have the PCA projected feature y = W T pca h o , where W pca is the projection matrix computed by the standard PCA technique and is a matrix with orthonormal columns. Then, y is subjected to a whitening transformation and yields the final WPCA projected feature h:</p><formula xml:id="formula_20">h = Λ -1/2 y = Λ -1/2 W T pca h o (<label>18</label></formula><formula xml:id="formula_21">)</formula><p>where</p><formula xml:id="formula_22">Λ -1/2 = diag{λ -1/2 1 , λ -1/2 2</formula><p>, ...}, with λ i being its eigenvalue. The integrated projection matrix Λ -1/2 W T pca treats variance along all principal component axes as equally significant by more heavily weighting components corresponding to smaller eigenvalues and is arguably appropriate for discrimination. Consequently, the negative influences of the leading eigenvectors are reduced, whereas the discriminating details encoded in trailing eigenvectors are enhanced <ref type="bibr" target="#b13">[14]</ref>.</p><p>We applied WPCA to each single feature, letting h ki be the WPCA projected feature of h o ki . The similarity function is denoted as d(h ki , h kj ); For fusing K features, we used the sum of the similarity measures of the individual features:</p><formula xml:id="formula_23">d(I i , I j ) = K k d(h ki , h kj ). (<label>19</label></formula><formula xml:id="formula_24">)</formula><p>As adopted in previous research <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17]</ref>, WPCA was conducted on the gallery set only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Distance Measure and Classification</head><p>To perform face recognition, there are two crucial components: (1) extracting low-level features, and</p><p>(2) building classification models. In this work, focus was on evaluating the discrimination properties of the proposed descriptors, and therefore tried to make as few assumptions as possible and chose the simple Nearest Neighbor Classifier (NNC).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>First, we evaluated the effectiveness of the original histogram feature vector {h o ki } 6 k=1 corresponding to the proposed descriptors. The distance between two normalized frequency histograms was measured using the Chi squared distance. The face samples were then classified according to their normalized histogram feature vectors h o i and h o j :</p><formula xml:id="formula_25">χ 2 (h o i , h o j ) = 1 2 k [h o i (k) -h o j (k)] 2 h o i (k) + h o j (k)<label>(20)</label></formula><p>the same distance metric used in <ref type="bibr" target="#b0">[1]</ref>.</p><p>We further evaluated the performance of the more efficient features {h ki } 6 k=1 , which were obtained by applying the WPCA technique on the original histogram feature vectors {h o ki } 6 k=1 . The WPCA projected features were no longer histogram frequencies, and therefore the χ 2 distance may not be a good choice. Instead, we employed the conventional Euclidean distance metric to compare faces in the normalized projected space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We conducted extensive experiments on three commonly-used databases: Extended Yale B, FERET and CAS-PEAL-R1. We used the standard evaluation protocol for each database in order to facilitate comparison with previous work. These databases incorporate several deviations from the ideal conditions, including illumination, expression, occlusion and pose alterations. We adopt several of the standard evaluation protocols reported in the face recognition literature, and we present a comprehensive comparison of the proposed approach with the recent state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Image Data and Experimental Setup</head><p>The Extended Yale B database <ref type="bibr" target="#b15">[16]</ref> consists of 2414 frontal face images of 38 subjects, with about 64 samples per subject, which were captured under controlled lighting conditions and were cropped and normalized to a standard size of 192 × 168 pixels. Instead of randomly selecting half of the images per subject for training and the remaining for testing, we used a more difficult setup <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b33">34]</ref>. The database was divided into five subsets according to light direction with respect to the camera axis (Fig. <ref type="figure">8 (a)</ref>): Subset 1 consisted of gallery images under nominal lighting conditions, while all others were used as probes. Subsets 2 and 3 were characterized by slight-to-moderate illumination variations, while subsets 4 and 5 depicted severe illumination changes.</p><p>The FERET database <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b3">4]</ref> is one of the largest publicly available databases, consisting of a total of 14051 gray scale images representing 1199 subjects. The images contain variations including lighting, facial expressions and pose angle. To ensure the reproducibility of tests, we used the the publicly available CSU face identification evaluation framework <ref type="bibr" target="#b3">[4]</ref> to evaluate the performance of our proposed methods, with a standard gallery (fa) and four probe sets (fb, fc, dup1, dup2). All images were normalized according to the  7. the radius r for extracting the proposed descriptors.</p><p>For the LBP S and LBP M, we used a constant q = 1 (i.e. a constant number of 8 sampling points for any radius r), as done in previous work <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>. For other descriptors, we proposed an intermediate sampling scheme aimed to prevent over-smoothing at larger radii r, and set q = r for r ≤ 4, and q = 4 for r &gt; 4. In terms of parameter δ, we set δ = 1.</p><p>Table <ref type="table" target="#tab_1">1</ref> shows the recognition rates of the individual proposed descriptors on the Extended Yale B database, comparing the three pattern grouping schemes discussed in Section 3.3, using only the elementary histogram features (without dimensionality reduction by WPCA).</p><p>From Table <ref type="table" target="#tab_1">1</ref>, we can make the following observations. First, for each proposed descriptor, the histogram representation generated by the full pattern method is ideal, significantly outperforming the uniform pattern method and marginally outperforming the dominant scheme, clearly demonstrating the insufficiency of the uniform patterns for representing face images. On this basis we will not report results for the uniform pattern scheme in our latter experiments. Second, the magnitude-based descriptors consistently outperformed the corresponding sign-based descriptors, particularly under very severe lighting changes (S4 and S5). This finding is different from those by Guo et al. <ref type="bibr" target="#b8">[9]</ref>, in that the sign component was more informative than magnitude. Finally, the proposed LBP S, LBP M, ADLBP M and RDLBP M (either with full or dominant patterns) outperformed the results obtained using a Linear Regression Classification (LRC) method <ref type="bibr" target="#b26">[27]</ref>,</p><p>particularly under severe lighting variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T  Table <ref type="table" target="#tab_2">2</ref> illustrates the effect of WPCA dimensionality reduction on the FERET database. The principal component axes were calculated using the 1196 gallery faces, with no additional training set. We observed that the recognition performance of the individual descriptors was consistently improved by WPCA+Euclidean relative to Chi-square, with the exception of the radial descriptors RDLBP S and RDLBP M, possibly because the WPCA dimensionality of 1196 is not adequate for these two descriptors to represent face images. A more significant consideration was the computational advantage of WPCA+Euclidean+NNC over Chi square+NNC due to the much lower feature dimensionality of WPCA. In our further experiments we will focus on WPCA+Euclidean+NNC.</p><p>We did not expect individual descriptors to capture all of the salient image features in recognizing a face. Indeed, it has been our premise, from the outset, to wish to identify and fuse those descriptors having complementary information. To simplify the notation, we refer to the full and LDP features as F i and D i , respectively, as identified in Tables <ref type="table" target="#tab_1">1</ref> and<ref type="table" target="#tab_2">2</ref>.</p><p>It would be possible to directly fuse all the six proposed descriptors, but there are two concerns. First, it is not obvious that fusing all six features necessarily gives the best recognition performance, and second, there is a concern regarding processing and computational costs. Other things being equal, we would prefer to fuse the fewest features giving the strongest performance.</p><formula xml:id="formula_26">A C C E P T E D M A N U S C R I P T</formula><p>We tested all 57 possible descriptor sets, both full and dominant, with experiments carried out on the FERET and CAS-PEAL-R1 databases. For the CAS-PEAL-R1 database, the principal component axes were calculated using the 1040 gallery images, and the WPCA dimension was fixed to 1040. Table <ref type="table" target="#tab_3">3</ref> compares the results of the feature fusion methods with those achieved by the single descriptor method. Table <ref type="table" target="#tab_3">3</ref> lists only those feature combinations that performed the best on both the FERET and CAS-PEAL-R1 databases.</p><p>The following observations can be made from Table <ref type="table" target="#tab_3">3</ref>. First, the fused features consistently achieved much better recognition scores than any single descriptor, especially on the FERET dup1, dup2 and the CAS-PEAL-R1 lighting probe sets. For example, the fused feature F 1346 increased the recognition rate by 23%, 19%, 24% and 31% respectively relative to the LBP S (F 1 ), ADLBP S (F 3 ), ADLBP M (F 4 ) and RDLBP M (F 6 ) descriptors alone on the CAS-PEAL-R1 lighting probe set, suggesting that the proposed descriptors indeed captured complementary information. Second, it was observed that among the six proposed descriptors, the LBP S (F 1 ), ADLBP S (F 3 ), ADLBP M (F 4 ) played the more important roles in feature fusion, whereas LBP M (F 2 ) played the least important. Finally, the fused features F 1346 , F 13456 and F 123456 gave robust results on the test databases, differing only in computational cost.</p><p>Paralleling Table <ref type="table" target="#tab_3">3</ref>, Table <ref type="table" target="#tab_4">4</ref> lists the results of the fused LDP features. The results in Table <ref type="table" target="#tab_4">4</ref> support the findings given by Table <ref type="table" target="#tab_3">3</ref>.</p><p>Fig. <ref type="figure" target="#fig_13">9</ref> and Fig. <ref type="figure" target="#fig_17">10</ref> plot the results of the fused full descriptors as a function of the number of sub-regions √ w × √ w. In general, the recognition accuracy peaked for an intermediate value of w, suggesting that a small number of large regions led to a loss of spatial information, whereas a large number of small regions increased the computation time and degraded system accuracy. Conclusively, 9 × 9 was a defensible choice for both the FERET and CAS-PEAL-R1 databases.</p><p>Finally, we addressed the question of radius parameter r with regards to recognition performance.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparative Evaluation</head><p>With parameter settings determined, examined the effectiveness of the proposed fused descriptors against previously established methods: traditional multiscale LBP by Ahonen et al. <ref type="bibr" target="#b0">[1]</ref>; Local Gabor Binary Pattern (LGBP) by Nguyen et al. <ref type="bibr" target="#b27">[28]</ref>, which utilizes Gabor filter bank as preprocessing before LBP extraction;</p><p>Histogram of Gabor Phase Patterns (HGPP) by Zhang et al. <ref type="bibr" target="#b39">[40]</ref>, which quantizes Gabor filtered images into histograms that encode not only the magnitude, but also the phase information from the image; Multi-scale LBP Histograms (MLBPH) with LDA learning and kernel fusion by Chan et al. <ref type="bibr" target="#b4">[5]</ref>; learning based methods Discriminative LBP (DLBP) <ref type="bibr" target="#b25">[26]</ref> and Decision Tree-based LBP (DT-LBP) <ref type="bibr" target="#b24">[25]</ref>; and very recent approaches: POEM by Vu et al. published in 2012 <ref type="bibr" target="#b36">[37]</ref>, which computes LBP-like features from SIFT histograms; LQP by Hussain et al. published in 2012 <ref type="bibr" target="#b13">[14]</ref>, which first learns significant LQP patterns globally with KMeans clustering and then uses them to encode the face images; DFD by Lei et al. published in 2013 <ref type="bibr" target="#b16">[17]</ref>, which first learns discriminative local features from local pixel differences and then applies the standard Bag-of-Features (BoF) model to extract histogram features for face representation. The results are shown in Table <ref type="table" target="#tab_5">5</ref> and Table <ref type="table" target="#tab_6">6</ref>.</p><p>Table <ref type="table" target="#tab_5">5</ref> illustrates that the proposed method either matched or significantly outperformed all other approaches. To the best of our knowledge, these are the best results reported to date on the FERET database. The DFD method was the only one to perform similarly, however it is quite complex, involving LDA learning and KMeans clustering, both of which are time-consuming and require enough training data to extract discriminative and stable face representation. On the contrary, our proposed method is simple, efficient and free of pre-training. Since the face images in the FERET probe sets contain several sources of  variation, such as expression, lighting, and aging, these comparisons illustrated that our proposed methods is impressively robust to these extrinsic imaging conditions.</p><p>Similarly, Table <ref type="table" target="#tab_6">6</ref> compares the results of the proposed method with previously reported results on the CAS-PEAL-R1 database. Methods F were obtained by combining the fused descriptors from two scales, radius r = 4 and r = 5. Methods F * and D * used only one scale r = 4, and were then obtained by optimizing the number of subregions √ w × √ w over the training set from the CAS-PEAL-R1 database.</p><p>As seen in Table <ref type="table" target="#tab_6">6</ref>, our proposed method achieved comparable results on the expression probe set, outperformed all but DFD on the accessory probe set, and significantly outperformed on the lighting probe set. In both accessory and lighting, we outperformed HGPP (which is known to be very computationally expensive) and along with the two learning based methods DT-LBP and DLBP. The high recognition rate on the lighting probe set indicates that the proposed approach is much more robust to illumination variations than other methods in the literature.</p><p>As a separate test, we conducted experiments with multiscale F 13456 , and with 13 × 13 subregions, and obtained a recognition rate of 74.6% on the lighting probe set, which, to the best of our knowledge, is currently the best score achieved on this data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper we proposed a novel extended set of LBP-like descriptors and developed a simplistic The proposed local descriptors significantly exploit available information and as well as contain complementary information, which is shown by significantly improved performance of fused descriptors over single descriptor. We found that both full pattern features and the proposed LDP features have strong recognition performance, and the LDP features have lower feature dimensionality.</p><p>Results show that our proposed approach outperforms all other descriptor-based techniques, and is comparable with other much more complex learning-based descriptors. High performance coupled with low complexity suggests that our algorithm is a strong candidate for face recognition. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: A central pixel and its p circularly and evenly spaced neighbors on a circle of radius r.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Two commonly used neighborhood structures: (a) Concentric rings of interpolated pixels, (b) A ring of image patches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: The two given patterns, left, would be considered equivalent by LBP, although the patterns are, in some ways, quite different from one other, not just due to a rotation, but due to underlying textural properties, revealed via angular and radial differences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Angular</head><label></label><figDesc>Difference-based Local Binary Pattern (ADLBP) and the Radial Difference-based Local BinaryPattern (RDLBP). Similar to LBP S and LBP M, both sign and magnitude components of angular and radial differences were considered, leading to four LBP-like descriptors: ADLBP S, ADLBP M, RDLBP S, and RDLBP M, all illustrated in Fig.4. The proposed new descriptors were not designed to compete with traditional LBP, but rather complement it and extend the set of feature candidates. The derivation of ADLBP S and ADLBP M, were introduced first, followed by RDLBP S and RDLBP M in Section 3.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The proposed extended set of features, made up of (a) local intensity features, (b) local angular-difference based features, and (c) local radial-difference based features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>to 9 are rotation invariant uniform patterns) nonuniform (59.4%)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Proportions of the rotation invariant LBPs for the proposed LBP-like descriptors: LBP S 4,8 , LBP M 4,8 , ADLBP S 4,1,4 , ADLBP M 4,1,4 , RDLBP S 4,1,4 and RDLBP M 4,1,4 using the FERET gallery set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>the proposed descriptors. The first nine bins of each histogram corresponds to the rotation invariant uniform patterns, and the remaining bins to the nonuniform patterns. It is clear that the uniform patterns may not necessarily represent a significant proportion of of overall patterns. This was most striking in the cases of ADLBP S and ADLBP M. Consequently, textural information could not be effectively represented by solely considering the histogram of the uniform patterns. The original uniform patterns introduced by Ojala et al. [30] did not represent most of the observed patterns, and thus we proposed use of the full descriptors, i.e. 256 binary patterns for each descriptor at each scale. Furthermore, as pointed out by Ojala et al. [30], the occurrence frequencies of different patterns vary greatly and some of the patterns rarely occur, as illustrated in Fig. 5. Therefore, we proposed a Labeled Dominant Pattern (LDP) scheme which learned the most frequently occurring patterns from a training set to reduce the dimensionality of the proposed descriptors without information loss. First, the pattern histogram vectors of all images in the training set were computed. Then they were combined into one histogram and sorted in descending order, and the labels of the minimum set of patterns that occupied 80% of the total pattern occurrences were retained. At last, given the learned set of dominant patterns, A C C E P T E D M A N U S C R I P T Input: The gallery face set, and the threshold parameter 0 &lt; τ ≤ 1 to determine a set of labeled dominant patterns (LDP) for an LBP type descriptor. Output: LDP pattern set S LDP Initialization: Initialize the original pattern histogram h where h i = 0 and the pattern index vector v, where v i = i -1, i = 1, • • • , d where d denotes the number of all possible patterns types of interest (e.g. d = 2 8 = 256). for each face image I in the gallery set do for each pixel x k from image I do Compute the binary pattern label of x k , l(x k )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Algorithm 1 :</head><label>1</label><figDesc>Learn the LDP pattern set from a given</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Learning the LDP pattern set from a given gallery face set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Overview of the proposed face recognition framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>provided eye coordinates and cropped to 150 × 130. Example images are shown in Fig. 8 (b). The CAS-PEAL-R1 database [8] is a large-scale Chinese face database for face recognition algorithm training and evaluation, containing 30,863 images of 1040 individuals. The standard experimental protocol A C C E P T E D M A N U S C R I P T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Recognition rates (%) of the WPCA features as a function of the number of subregions on the FERET database.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Recognition rates (%) of the WPCA features as a function of the number of subregions on the CAS-PEAL-R1 database.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Recognition rates (%) of the WPCA features as a function of the radius parameter r on the CAS-PEAL-R1 database.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>framework to fuse the proposed descriptors for face identification. The proposed set of descriptors consists of two intensity based descriptors LBP S, and LBP M, and four accumulated local differences based descriptors ADLBP S, ADLBP M, RDLBP S, and RDLBP M.All of the proposed descriptors have desirable features of robustness to lighting, pose, and expression variations, computational efficiency when compared to many of the competing descriptors, and an encoding of both microstructures and macrostructures resulting a more complete image representation. Because the descriptors have a form similar to that of LBP, they inherit the merits of LBP codes and are readily fused to represent face images. The extraction of the proposed descriptors requires no training, and therefore, in contrast to popular learning methods, generalizability is avoided.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>This work has been supported by the National Natural Science Foundation of China under contract number 61202336, by the Open Project Program of the National Laboratory of Pattern Recognition (201407354), and the National Basic Research Program of China (973 Program) (2013CB329401).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Recognition rates (%) of the proposed single elementary features on the Extended Yale B database, comparing "uniform", "full" and "labeled dominant". The NNC classifier with χ 2 distance measure is used for classification. The radius r is 4 and the number of subregions √ w × √ w is set as 9 × 9.</figDesc><table><row><cell></cell><cell cols="2">Method</cell><cell>S2</cell><cell>S3</cell><cell>S4</cell><cell>S5</cell><cell>Mean</cell></row><row><cell>U1</cell><cell cols="2">LBP S u2</cell><cell>99.8</cell><cell>99.6</cell><cell>93.2</cell><cell>77.7</cell><cell>92.6</cell></row><row><cell>U2</cell><cell cols="2">LBP M u2</cell><cell>99.8</cell><cell>99.2</cell><cell>95.8</cell><cell>91.7</cell><cell>96.6</cell></row><row><cell>U3</cell><cell cols="2">ADLBP S u2</cell><cell>99.8</cell><cell>89.5</cell><cell>28.5</cell><cell>12.5</cell><cell>57.6</cell></row><row><cell>U4</cell><cell cols="2">ADLBP M u2</cell><cell>99.8</cell><cell>99.6</cell><cell>94.5</cell><cell>88.7</cell><cell>95.7</cell></row><row><cell>U5</cell><cell cols="2">RDLBP S u2</cell><cell>99.8</cell><cell>99.4</cell><cell>91.9</cell><cell>68.5</cell><cell>89.9</cell></row><row><cell>U6</cell><cell cols="2">RDLBP M u2</cell><cell cols="2">99.8 99.6</cell><cell>98.2</cell><cell cols="2">91.5 97.3</cell></row><row><cell>F1</cell><cell cols="2">LBP S full</cell><cell>99.8</cell><cell>99.8</cell><cell>99.6</cell><cell>96.2</cell><cell>98.9</cell></row><row><cell>F2</cell><cell cols="2">LBP M full</cell><cell cols="2">99.8 99.6</cell><cell>99.6</cell><cell cols="2">97.6 99.2</cell></row><row><cell>F3</cell><cell cols="2">ADLBP S full</cell><cell>99.8</cell><cell>99.6</cell><cell>91.4</cell><cell>67.1</cell><cell>89.5</cell></row><row><cell cols="3">F4 ADLBP M full</cell><cell cols="2">99.8 99.6</cell><cell>99.4</cell><cell cols="2">97.8 99.2</cell></row><row><cell>F5</cell><cell cols="2">RDLBP S full</cell><cell>99.8</cell><cell>99.6</cell><cell>98.7</cell><cell>86.6</cell><cell>96.2</cell></row><row><cell cols="3">F6 RDLBP M full</cell><cell>99.8</cell><cell>99.6</cell><cell>98.9</cell><cell>95.7</cell><cell>98.5</cell></row><row><cell>D1</cell><cell cols="2">LBP S ldp</cell><cell>99.8</cell><cell>99.6</cell><cell>98.3</cell><cell>92.7</cell><cell>97.6</cell></row><row><cell>D2</cell><cell cols="2">LBP M ldp</cell><cell>99.8</cell><cell cols="2">99.8 99.1</cell><cell cols="2">97.3 99.0</cell></row><row><cell>D3</cell><cell cols="2">ADLBP S ldp</cell><cell>99.8</cell><cell>99.4</cell><cell>90.1</cell><cell>65.1</cell><cell>88.6</cell></row><row><cell cols="3">D4 ADLBP M ldp</cell><cell>99.8</cell><cell>99.6</cell><cell>98.9</cell><cell>96.5</cell><cell>98.7</cell></row><row><cell>D5</cell><cell cols="2">RDLBP S ldp</cell><cell>99.8</cell><cell>99.6</cell><cell>97.1</cell><cell>82.9</cell><cell>94.9</cell></row><row><cell cols="3">D6 RDLBP M ldp</cell><cell>99.8</cell><cell>99.6</cell><cell>98.7</cell><cell>94.8</cell><cell>98.2</cell></row><row><cell></cell><cell cols="2">PCA [27]</cell><cell>98.5</cell><cell>80.0</cell><cell>15.8</cell><cell>24.4</cell><cell>54.7</cell></row><row><cell></cell><cell cols="2">LRC [27]</cell><cell>100</cell><cell>100</cell><cell cols="3">83.27 33.61 79.2</cell></row><row><cell></cell><cell cols="3">LRC Fused [34] 100</cell><cell>100</cell><cell cols="3">88.97 84.73 93.4</cell></row><row><cell cols="5">4. Chi square+NNC vs. WPCA+Euclidean+NNC;</cell><cell></cell><cell></cell></row><row><cell>5. which descriptors to fuse;</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>6. the number of sub-regions</cell><cell>√ w ×</cell><cell>√ w;</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Recognition rates (%) of the proposed WPCA features on the FERET database. The WPCA dimension is 1196 (the size of the gallery set). The NNC classifier with Euclidean distance measure is used for classification. The values of parameters r and w are the same as those in Table1.</figDesc><table><row><cell></cell><cell>Method</cell><cell>fb</cell><cell>fc</cell><cell>dup1 dup2</cell></row><row><cell>F1</cell><cell>LBP S full +χ 2</cell><cell cols="3">96.1 97.4 75.2 70.1</cell></row><row><cell>F2</cell><cell>LBP M full +χ 2</cell><cell cols="3">92.8 95.9 70.4 66.2</cell></row><row><cell>F3</cell><cell>ADLBP S full +χ 2</cell><cell cols="3">97.2 97.4 75.9 73.1</cell></row><row><cell>F4</cell><cell>ADLBP M full +χ 2</cell><cell cols="3">95.7 97.9 73.3 70.9</cell></row><row><cell>F5</cell><cell>RDLBP S full +χ 2</cell><cell cols="3">95.7 97.9 73.3 70.9</cell></row><row><cell>F6</cell><cell>RDLBP M full +χ 2</cell><cell cols="3">91.5 96.9 66.2 66.7</cell></row><row><cell>F1</cell><cell>LBP S full +WPCA</cell><cell cols="3">99.0 100.0 83.2 80.3</cell></row><row><cell>F2</cell><cell>LBP M full +WPCA</cell><cell cols="3">98.3 99.0 79.4 78.6</cell></row><row><cell cols="5">F3 ADLBP S full +WPCA 97.1 95.9 78.9 73.9</cell></row><row><cell cols="5">F4 ADLBP M full +WPCA 98.1 98.5 75.1 73.9</cell></row><row><cell cols="5">F5 RDLBP S full +WPCA 86.9 79.9 50.6 45.7</cell></row><row><cell cols="5">F6 RDLBP M full +WPCA 93.1 86.6 58.7 52.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Recognition rates (%), comparing results of fusing multiple descriptors. WPCA dimension set as 1196 for FERET and 1040 for CAS-PEAL-R1. Values of parameters r and w are the same as those in Table1.</figDesc><table><row><cell></cell><cell></cell><cell>FERET</cell><cell></cell><cell cols="2">CAS-PEAL-R1</cell></row><row><cell cols="2">Method fb</cell><cell cols="5">fc dup1 dup2 Mean Expression Accessary Lighting Mean</cell></row><row><cell>F1</cell><cell cols="2">99.0 100.0 83.2 80.3 92.4</cell><cell>97.5</cell><cell>92.4</cell><cell>42.9</cell><cell>75.5</cell></row><row><cell>F2</cell><cell cols="2">98.3 99.0 79.4 78.6 90.6</cell><cell>94.1</cell><cell>85.1</cell><cell>36.5</cell><cell>69.5</cell></row><row><cell>F3</cell><cell cols="2">97.1 95.9 78.9 73.9 89.1</cell><cell>98.1</cell><cell>93.6</cell><cell>47.0</cell><cell>77.6</cell></row><row><cell>F4</cell><cell cols="2">98.1 98.5 75.1 73.9 88.6</cell><cell>95.1</cell><cell>87.0</cell><cell>42.1</cell><cell>72.6</cell></row><row><cell>F5</cell><cell cols="2">86.9 79.9 50.6 45.7 71.0</cell><cell>96.1</cell><cell>90.5</cell><cell>33.1</cell><cell>70.8</cell></row><row><cell>F6</cell><cell cols="2">93.1 86.6 58.7 52.1 77.9</cell><cell>91.3</cell><cell>78.1</cell><cell>34.8</cell><cell>65.6</cell></row><row><cell>F34</cell><cell cols="2">99.4 100.0 85.6 84.2 93.7</cell><cell>98.4</cell><cell>93.5</cell><cell>59.2</cell><cell>82.1</cell></row><row><cell>F14</cell><cell cols="2">99.4 100.0 87.3 86.3 94.4</cell><cell>98.3</cell><cell>93.6</cell><cell>57.6</cell><cell>81.6</cell></row><row><cell>F13</cell><cell cols="2">99.2 100.0 87.7 84.6 94.3</cell><cell>98.7</cell><cell>94.7</cell><cell>55.8</cell><cell>81.4</cell></row><row><cell cols="3">F134 99.6 100.0 88.8 87.2 95.1</cell><cell>98.9</cell><cell>94.8</cell><cell>63.4</cell><cell>84.3</cell></row><row><cell cols="3">F1346 99.6 100.0 88.9 87.6 95.1</cell><cell>98.4</cell><cell>93.9</cell><cell>65.9</cell><cell>84.8</cell></row><row><cell cols="3">F13456 99.6 100.0 89.1 86.8 95.1</cell><cell>98.7</cell><cell>94.2</cell><cell>66.4</cell><cell>85.1</cell></row><row><cell cols="3">F123456 99.6 100.0 88.8 87.2 95.1</cell><cell>98.5</cell><cell>93.8</cell><cell>66.2</cell><cell>84.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparisons of the recognition rates (%) of fusing the proposed LDP features on the FERET and CAS-PEAL-R1Databases.</figDesc><table><row><cell></cell><cell></cell><cell>FERET</cell><cell></cell><cell cols="2">CAS-PEAL-R1</cell></row><row><cell cols="2">Method fb</cell><cell cols="5">fc dup1 dup2 Mean Expression Accessary Lighting Mean</cell></row><row><cell>D1</cell><cell cols="2">98.8 100.0 81.4 77.8 91.5</cell><cell>96.6</cell><cell>90.8</cell><cell>34.9</cell><cell>71.7</cell></row><row><cell>D2</cell><cell cols="2">98.4 97.9 78.1 75.2 89.8</cell><cell>93.0</cell><cell>83.1</cell><cell>33.4</cell><cell>67.3</cell></row><row><cell>D3</cell><cell cols="2">97.2 98.5 80.9 77.8 90.4</cell><cell>97.5</cell><cell>92.1</cell><cell>41.2</cell><cell>74.8</cell></row><row><cell>D4</cell><cell cols="2">98.2 98.5 75.6 73.9 88.8</cell><cell>94.3</cell><cell>84.2</cell><cell>37.0</cell><cell>69.4</cell></row><row><cell>D5</cell><cell cols="2">94.4 95.9 63.6 58.5 81.4</cell><cell>94.9</cell><cell>88.4</cell><cell>27.6</cell><cell>67.7</cell></row><row><cell>D6</cell><cell cols="2">96.2 94.3 68.7 65.4 84.5</cell><cell>89.9</cell><cell>75.9</cell><cell>31.2</cell><cell>63.1</cell></row><row><cell>D34</cell><cell cols="2">99.3 99.5 86.8 84.6 94.0</cell><cell>98.0</cell><cell>92.8</cell><cell>55.3</cell><cell>80.4</cell></row><row><cell>D14</cell><cell cols="2">99.3 100.0 86.4 84.6 93.9</cell><cell>98.0</cell><cell>92.3</cell><cell>51.5</cell><cell>78.8</cell></row><row><cell>D13</cell><cell cols="2">98.8 99.5 87.5 83.8 93.9</cell><cell>98.3</cell><cell>93.8</cell><cell>50.8</cell><cell>79.2</cell></row><row><cell cols="3">D134 99.3 100.0 88.6 86.8 94.8</cell><cell>98.7</cell><cell>94.5</cell><cell>60.3</cell><cell>83.0</cell></row><row><cell cols="3">D1346 99.5 100.0 89.1 87.6 95.1</cell><cell>98.2</cell><cell>93.6</cell><cell>63.9</cell><cell>83.8</cell></row><row><cell cols="3">D13456 99.6 100.0 89.6 87.6 95.4</cell><cell>98.5</cell><cell>94.1</cell><cell>64.3</cell><cell>84.3</cell></row><row><cell cols="3">D123456 99.5 100.0 89.5 86.8 95.2</cell><cell>98.4</cell><cell>93.9</cell><cell>65.4</cell><cell>84.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparisons of the recognition rates (%) of the proposed methods with previous results on the FERET Database.WPCA dimension is 1196. The values of parameters r and w are identical to those in Table1.</figDesc><table><row><cell>Method</cell><cell>fb</cell><cell>fc dup1 dup2</cell></row><row><cell>LBP [1]</cell><cell cols="2">97.0 79.0 66.0 64.0</cell></row><row><cell>HGPP [40]</cell><cell cols="2">97.5 99.5 79.5 77.8</cell></row><row><cell>DT-LBP [25]</cell><cell cols="2">99.0 100.0 84.0 80.0</cell></row><row><cell>DLBP [26]</cell><cell cols="2">99.0 99.0 86.0 85.0</cell></row><row><cell cols="3">MLBPH+LDA+SUM [5] 99.2 99.5 88.4 85.5</cell></row><row><cell>Gabor+WPCA [7]</cell><cell cols="2">96.3 99.5 78.8 77.8</cell></row><row><cell>LGBP+WPCA [28]</cell><cell cols="2">98.1 98.9 83.3 81.6</cell></row><row><cell>POEM+WPCA [37]</cell><cell cols="2">99.6 99.5 88.8 85.0</cell></row><row><cell>LPQ+WPCA [14]</cell><cell cols="2">99.8 94.3 85.5 78.6</cell></row><row><cell>DFD+WPCA [17]</cell><cell cols="2">99.3 99.0 88.8 87.6</cell></row><row><cell>F1346 (Proposed)</cell><cell cols="2">99.6 100.0 88.9 87.6</cell></row><row><cell>F13456 (Proposed)</cell><cell cols="2">99.6 100.0 89.1 86.8</cell></row><row><cell>F123456 (Proposed)</cell><cell cols="2">99.6 100.0 88.8 87.2</cell></row><row><cell>D1346 (Proposed)</cell><cell cols="2">99.5 100.0 89.1 87.6</cell></row><row><cell>D13456 (Proposed)</cell><cell cols="2">99.6 100.0 89.6 87.6</cell></row><row><cell>D123456 (Proposed)</cell><cell cols="2">99.5 100.0 89.5 86.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Comparisons of the recognition rates (%) of the proposed methods with previous results on the CAS-PEAL-R1Database. WPCA dimension is 1040.</figDesc><table><row><cell>Method</cell><cell cols="3">Expression Accessary Lighting</cell></row><row><cell>HGPP [40]</cell><cell>96.8</cell><cell>92.5</cell><cell>62.9</cell></row><row><cell>DT-LBP [25]</cell><cell>98.0</cell><cell>92.0</cell><cell>41.0</cell></row><row><cell>DLBP [26]</cell><cell>99.0</cell><cell>92.0</cell><cell>41.0</cell></row><row><cell>DFD+WPCA [17]</cell><cell>99.0</cell><cell>96.9</cell><cell>63.9</cell></row><row><cell>F 1346 (Proposed)</cell><cell>98.3</cell><cell>94.2</cell><cell>68.7</cell></row><row><cell>F 13456 (Proposed)</cell><cell>98.5</cell><cell>94.4</cell><cell>70.0</cell></row><row><cell>F 123456 (Proposed)</cell><cell>98.3</cell><cell>94.0</cell><cell>69.6</cell></row><row><cell>F  *  1346 (Proposed)</cell><cell>98.4</cell><cell>93.9</cell><cell>72.2</cell></row><row><cell>F  *  13456 (Proposed)</cell><cell>98.7</cell><cell>94.4</cell><cell>71.9</cell></row><row><cell>F  *  123456 (Proposed)</cell><cell>98.5</cell><cell>94.0</cell><cell>72.3</cell></row><row><cell>D  *  1346 (Proposed)</cell><cell>98.2</cell><cell>93.0</cell><cell>71.3</cell></row><row><cell>D  *  13456 (Proposed)</cell><cell>98.5</cell><cell>93.8</cell><cell>71.7</cell></row><row><cell>D  *  123456 (Proposed)</cell><cell>98.4</cell><cell>93.7</cell><cell>72.9</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For all databases, all face samples were photometrically normalized by the preprocessing sequence approach proposed by Tan and Triggs <ref type="bibr" target="#b34">[35]</ref>. This photometric normalization method reduced the effects of illumination variation, local shadowing and highlights, while still keeping the essential visual appearance information for use in recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Parameter Evaluation</head><p>The key parameters involved in the proposed method are as follows:</p><p>1. number of sampling points 8q on radius r;</p><p>2. the step parameter δ for computing differences;</p><p>3. uniform vs. full vs. LDP;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Face decription with local binary patterns: application to face recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2037" to="2041" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Soft histograms for local binary patterns</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Finnish Signal Processing Symposium</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="645" to="649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Eigenfaces vs. Fisherfaces: recognition using class specific linear projection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hespanha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="711" to="720" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<author>
			<persName><forename type="first">J</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bolme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Teixeira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The csu face identification evaluation system: its purpose, features, and structure</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="128" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Multi-scale local binary pattern histograms for face recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
		<respStmt>
			<orgName>University of Surrey</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multiscale local phase quantization for robust component-based face recognition using kernel fusion of multiple descriptors</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tahir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1164" to="1177" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Gabor-eigen-whiten-cosine: a robust scheme for face recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Analysis and Modelling of Faces and Gestures</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3723</biblScope>
			<biblScope unit="page" from="336" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The CAS-PEAL large-scale chinese face database and baseline evaluations</title>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cy. A Syst. Humans</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="149" to="161" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A completed modeling of local binary pattern operator for texture classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1657" to="1663" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Discriminative features for texture description</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3834" to="3843" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Face recognition using laplacianfaces</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="328" to="340" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Local binary patterns and its application to facial image analysis: a survey</title>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ardabilian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cy. C Appl. Rev</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="765" to="781" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A robust method for near infrared face recognition based on extended local binary pattern</title>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Visual Computing</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4842</biblScope>
			<biblScope unit="page" from="437" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Face recognition using local quantized patterns</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Napoléon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Heterogeneous face recognition using kernel prototype similarities</title>
		<author>
			<persName><forename type="first">B</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1410" to="1422" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Acquiring linear subspaces for face recognition under variable lighting</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="684" to="698" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning discriminant face descriptor</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="289" to="302" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Handbook of Face Recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>Second edition</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dominant local binary patterns for texture classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1107" to="1118" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning multi-scale block local binary patterns for face recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Biometrics</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4642</biblScope>
			<biblScope unit="page" from="828" to="837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Texture classification from random features</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fieguth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="574" to="586" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Extended local binary pattern fusion for face recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fieguth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">BRINT: binary rotation invariant and noise tolerant texture classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fieguth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3071" to="3084" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Extended local binary patterns for texture classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fieguth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="86" to="99" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Face recognition with decision tree-based local binary patterns</title>
		<author>
			<persName><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Soto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="618" to="629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning discriminative local binary patterns for face recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Soto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face Gesture Recognition and Workshops (FG)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="470" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Linear regression for face recognition</title>
		<author>
			<persName><forename type="first">I</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Togneri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2106" to="2112" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Local gabor binary pattern whitened pca: A novel approach for face recognition from single image per person</title>
		<author>
			<persName><forename type="first">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Biometrics</title>
		<imprint>
			<biblScope unit="page" from="269" to="278" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A comparative study of texture measures with classification based on feature distributions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="51" to="59" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Maenpää</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The FERET evaluation methodology for face recognition algorithms</title>
		<author>
			<persName><forename type="first">P</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rizvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1090" to="1104" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Noise-resistant local binary pattern with an embedded error-correction mechanism</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4049" to="4060" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Face recognition using multiscale local phase quantisation and linear regression classifier</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tahir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bouridane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="765" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Enhanced local texture feature sets for face recognition under difficult lighting conditions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1635" to="1650" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Eigenfaces for recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cognitive Neuroscience</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="86" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Enhanced patterns of oriented edge magnitudes for face recognition and image matching</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Caplier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1352" to="1365" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Effective unconstrained face recognition by combining multiple descriptors and learned background statistics</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1978" to="1990" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Local derivative pattern versus local binary pattern: Face recognition with high-order local pattern descriptor</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="533" to="544" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Histogram of gabor phase patterns (HGPP): A novel object representation approach for face recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="68" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Local gabor binary pattern histogram sequence (lgbphs): a novel non-statistical model for face representation and recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="786" to="791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Face recognition: A literature survey</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surveys</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="399" to="458" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Sobel-Lbp</surname></persName>
		</author>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="2144" to="2147" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
