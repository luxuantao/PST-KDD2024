<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improved Customer Lifetime Value Prediction With Sequence-To-Sequence Learning and Feature-Based Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Josef</forename><surname>Bauer</surname></persName>
							<email>josef.b.bauer@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Dietmar</forename><surname>Jannach</surname></persName>
							<email>dietmar.jannach@aau.at.</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Klagenfurt</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Klagenfurt</orgName>
								<address>
									<addrLine>Universit?tsstra?e 65-67</addrLine>
									<postCode>9020</postCode>
									<settlement>Klagenfurt am W?rthersee</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improved Customer Lifetime Value Prediction With Sequence-To-Sequence Learning and Feature-Based Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3441444</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS Concepts:</term>
					<term>Information systems ? Decision support systems</term>
					<term>Business intelligence</term>
					<term>? Computing methodologies ? Neural networks</term>
					<term>Customer lifetime value, machine learning, neural networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The prediction of the Customer Lifetime Value (CLV) is an important asset for tool-supported marketing by customer relationship managers. Since standard methods based on purchase recency, frequency, and past profit and revenue statistics often have limited predictive power, advanced machine learning (ML) techniques were applied to this task in recent years. However, existing approaches are often not fully capable of modeling certain temporal patterns that can be commonly found in practice, such as periodic purchasing behavior of customers. To address these shortcomings, we propose a novel method for CLV prediction based on a combination of several ML techniques. At its core, our method consists of a tailored deep learning approach based on encoder-decoder sequence-to-sequence recurrent neural networks with augmented temporal convolutions. This model is then combined with gradient boosting machines (GBMs) and a set of novel features in a hybrid framework. Empirical evaluations based on real-world data from a larger e-commerce company and a public dataset from the domain of online retail show that already the sequence-based model leads to competitive performance results. Stacking it with the GBM model is synergistic and further improves accuracy, indicating that the two models capture different patterns in the data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The Customer Lifetime Value (CLV) is a common business metric used in Marketing and Customer Relationship Management (CRM) <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b27">28]</ref>. With the CLV metric, the goal is to assess the future, long-term monetary value of existing customers, most commonly in terms of the predicted net profit induced by customers during their entire lifetime. <ref type="foot" target="#foot_0">1</ref> The resulting CLV predictions can then be used as a basis for a number of managerial decisions, in particular for ones related to investments for customer acquisition, customer retention, and churn prevention <ref type="bibr" target="#b21">[22]</ref>. As a side effect, considering the CLV can also help businesses focus more on long-term strategic aspects than solely on short-term profitability.</p><p>A variety of approaches to estimate the CLV have been proposed in the literature during the last decades. The typical factors that are considered in CLV calculations are based on past purchase statistics of individual customers. The most widely used models for example rely on recency, frequency and monetary value (RFM) statistics. Such comparably simple models are easy to apply in practice. However, they are often based on specific distributional assumptions <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26]</ref>, sometimes leading to limited prediction accuracy when the assumptions are not satisfied <ref type="bibr" target="#b23">[24]</ref>.</p><p>In recent years, the success of different machine learning (ML) techniques for various realworld applications has triggered an increased interest in applying such techniques for the CLV prediction problem and the related, but simpler churn prediction problem. <ref type="foot" target="#foot_1">2</ref>There are different ways in which CLV prediction can be modeled as a ML problem. First, it can be considered as a regression problem. In such an approach, the prediction is based on a set of engineered features, which, for example, encode aggregate statistics about the profit that was made by an individual customer so far, the point in time of their most recent transactions, demographic characteristics of the customers, as well as domain-specific factors. The advantage of such models is that they are very general in the sense that a variety of different aspects can be encoded within such a general prediction framework, and that various ML techniques can be directly applied for learning.</p><p>An alternative way of looking at CLV prediction is to consider it as a time series forecasting problem. Specifically, we can try to use past transaction data to make predictions for the immediate next time slots in the future (e.g., for a few weeks), and then aggregate these predictions to obtain the overall CLV for a given time horizon in the future. Compared to approaches that directly model the CLV, e.g., as a sum of predicted profits per time step, the advantage of time series modeling is that the sequential nature of the data is preserved. Therefore, certain patterns in the data like periodic repurchases can be more easily captured by using time series models than by trying to model these aspects with manually engineered features. Moreover, multivariate time series models have the advantage that they can potentially exploit fine-grained information about the customer's preferences, which are often collected over longer periods of time on online retail sites.</p><p>In this work, we propose a hybrid CLV prediction framework that leverages the advantages of both modeling approaches and demonstrate its effectiveness in two application scenarios from the e-commerce domain. The CLV predictions in our extensible framework are obtained by combining the predictions of a more traditional feature engineering approach and a time series modeling approach based on a novel deep learning architecture using Recurrent Neural Networks (RNNs).</p><p>The contributions of our approach can be summarized as follows:</p><p>? The presented work is the first that relies on RNNs for the CLV prediction problem.</p><p>? The proposed RNN-based model consists of a novel architecture with multiple encoderdecoder sequence-to-sequence (S2S) gated recurrent unit (GRU) layers, where major ingredients for the effectiveness of the model are the use of both an extensive set of features as well as stacked temporal convolutions.</p><p>Improved CLV Prediction With Seq-To-Seq Learning and Feature-Based Models 80:3</p><p>? Both prediction models consider a number of novel features, which have not been explored in the literature before. ? We demonstrate the effectiveness both of the novel RNN-based prediction model and in particular the combined model with two real-world datasets from the domain of online retail. The results show that both models are important for the overall effectiveness of the method, as they are able to leverage different types of information. ? Even though some of the evaluated models incorporate domain-specific factors, our modeling approach and computation pipeline is more general and can be used to leverage various types of additional knowledge, including, for example, clickstream data. To ensure reproducibility of our work, we share all code used in the experiments online. The links are provided in the experimental evaluation section.</p><p>This article is organized as follows. Next, in Section 2, we review existing formalizations of the CLV prediction problem and summarize common technical approaches. In Section 3, we provide a motivating example and give an overview of our approach, while Section 4 discusses its technical details. The outcomes of the empirical evaluations are provided in Section 5. This article ends with a discussion of the findings and an outlook on future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PREVIOUS WORKS 2.1 Methods Based on RFM Statistics</head><p>Classical RFM models make use of historical recency, frequency and monetary value data of customers <ref type="bibr" target="#b8">[9]</ref>. They are based on the following basic features: <ref type="foot" target="#foot_2">3</ref>(1) Recency (R): The time since the last purchase.</p><p>(2) Frequency (F): The number of purchases made by the customer.</p><p>(3) Monetary Value (M): The total profit or revenue resulting from purchases by the respective customer.</p><p>One basic RFM prediction approach is to categorize customers into groups based on the quantiles of the R, F, and M attributes, and to then predict the future profits (or: CLV) for an individual customer by using the past average profits of the other customers in the same group <ref type="bibr" target="#b3">[4]</ref>.</p><p>The advantage of many RFM approaches is that they are easy to understand and implement. However, there are several disadvantages. First, pure RFM models are restricted to exactly these three predictors (R, F , and M) and additional types of information are usually not considered. Second, the resulting models do not account for changes over time, like a rise in mean profit across all customers. And third, the quantile-based thresholds are fixed and selected with respect to each attribute individually, rather than being optimized based on the joint frequency distribution. As a result, we can obtain imbalanced groups with some R, F , and M value combinations having only few data points, which can easily lead to overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Models Based on Markov Chains</head><p>Some of the limitations of plain RFM segmentation approaches can be addressed by the use of Markov chain (MC) models. Such models do not try to directly predict CLV values, but in a first step rather aim to predict the customers' future behavior, i.e., their probable next states <ref type="bibr" target="#b34">[35]</ref>, leading to a sequential model. Specifically, the main questions often are to predict whether or not a repurchase is going to happen and how much the customer is going to spend. Such MC models are often based on recency, frequency, and monetary value attributes like the described RFM approaches. Compared to traditional RFM models, however, MC approaches allow us to model customer groups at a more granular level and to consider transitions from one state into another in a probabilistic manner by constructing transition matrices. A potential disadvantage of plain MC approaches, however, is that they do not consider additional types of information that could be relevant to the problem, including, for example, which kinds of products a customer has bought. MC models also do not retain temporal information prior to the last state, due to the Markov property. This implies that potentially relevant information about states prior to the last state is not taken into account. Moreover, MC-based methods can be prone to overfitting, since the natural ordering of values is not considered.</p><p>In terms of application scenarios, an MC-based method has for example been applied in <ref type="bibr" target="#b34">[35]</ref> to design more effective marketing strategies. Its performance has however only been evaluated using simulated examples, whereas the evaluations in our present work are based on real-world data. In our experiments, see Section 5, we use the MC-based method from <ref type="bibr" target="#b34">[35]</ref> as one of the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Negative Binomial Distribution (NBD) Models</head><p>In the research literature, a family of more sophisticated probabilistic approaches were proposed, which are-like some of the above-described MC-based approaches-driven by the intuition that the CLV prediction process can be split into two parts. The first problem is to predict whether or not a customer will buy again. The second prediction problem relates to the question how many orders and how much profit will be made.</p><p>In contrast to MC methods, the specific idea for this type of models is that each step in this process is based on a different distributional assumption, i.e., the purchasing process of every customer is treated as a realization of a certain probability distribution. One distribution models the probability that a customer is still active at a given time. Other distributions model the number of future transactions during the prescribed future time frame as well as the profit made by each customer. Based on these distributions, point estimators like the expected value can be calculated and the resulting numbers can then be used as an estimate for the CLV.</p><p>The most common approaches in this area are based on the negative binomial distribution (NBD). Examples of such approaches are the Pareto/NBD and the BG/NBD model <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b35">36]</ref>. These models also attempt to deal with the so called censorship issue <ref type="bibr" target="#b2">[3]</ref>, which is the problem that only incomplete data is available for customers that are still active at the end of the data collection period. Note that this is a general issue in application scenarios, where the date of the (future) customer churn is unknown, especially in cases without long-term historical data.</p><p>NBD-based approaches have the advantage of being principled and intuitive. They are particularly accurate when the specific distributional assumptions are correct or approximately satisfied and when the CLV is not influenced by other hidden variables to a large extent. However, these assumptions are not always met in practice, which lowers the predictive performance of these models <ref type="bibr" target="#b23">[24]</ref>. Moreover, additional predictor variables and the time series nature of the data are not considered in these models.</p><p>Performance comparisons of the methods described in <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr" target="#b35">[36]</ref> were originally conducted for an online shopping application scenario involving the sale of music, using the CDNOW dataset. This dataset is publicly available but too small for our purposes. These methods were later used in practice and were further investigated by other authors, e.g., in <ref type="bibr" target="#b23">[24]</ref>. In our experiments reported in Section 5, we include the BG/NBD model-which can be seen as an advanced RFM model-as one of the baselines in our performance comparison.</p><p>Improved CLV Prediction With Seq-To-Seq Learning and Feature-Based Models 80:5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">CLV Prediction as a Supervised Learning Task</head><p>More recent CLV prediction methods extend the previously described approaches in different ways. First, they often use more advanced ML techniques, in particular ensemble tree methods. Second, to improve the performance of the predictions, additional features (in the sense of predictor variables) are used besides the classic recency, frequency and monetary value features.</p><p>The work presented in <ref type="bibr" target="#b37">[38]</ref>, for example, is based on a RF model that uses a set of additional, "handcrafted" features in the context of an online bargain (deals) website. Their model includes features based on engagement (conversion rates, e.g., via clicks in emails), location-based features (distance to location of the deals), demographic features, user behavior statistics like vouchers used and times between purchase and redemption, as well as traditional RFM features of varying timescales. Moreover, a quarterly adjustment process is applied and customer segmentation based on purchasing frequencies is performed.</p><p>A very recent approach-also based on a RF model-is presented in <ref type="bibr" target="#b9">[10]</ref>, with an application in the fashion industry. Compared to <ref type="bibr" target="#b37">[38]</ref>, the model from <ref type="bibr" target="#b9">[10]</ref>, like ours uses more features and it in particular relies on features that are derived from user interaction logs (session logs) and the customers' return histories. Furthermore, one specific novel characteristic of <ref type="bibr" target="#b9">[10]</ref> in that context is that it generates a user embedding based on the customer's product view history in the form of sequential clickstream data. The general idea is to use these representations of the users to assess the similarities of their interests and behavior. In our method, we will also use such embeddings, although in a slightly different form, based on purchase instead of view logs (see Section 4.2).</p><p>However, our method is also different from <ref type="bibr" target="#b9">[10]</ref> in different ways. We, for example, directly predict the CLV and not churn, we use more features, and furthermore include an RNN in our architecture to capture the time series nature of the problem. Generally, the prediction of churn (rates) can represent a valuable input to the CLV estimation process. In our e-commerce application scenario, customer churn is usually not observed. Customers of e-commerce sites typically do not close their accounts and decide to not buy again on the site. In the datasets that we use for evaluation, information about churn is also not available, which is why churn is modeled implicitly in our approach when the future CLV is predicted to be zero at some point in time.</p><p>Overall, the advantages and disadvantages of ML-based approaches are as follows. First, one typical key advantage of such models is their often high prediction performance. This is in particular the case when various other predictor variables besides R, F , and M can be used. Second, many ML-based methods do not rely on specific distributional assumptions, which have to be met when NBD-based models are applied. A potential disadvantage is that these models have higher computational demands and require a certain amount of additional data collection and feature engineering. Moreover, depending on the used learning approach, the resulting models might be difficult to interpret.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Time Series Forecasting Approaches</head><p>The sequential, RNN-based component of our model makes predictions about the development of multiple time series over time. The prediction of (multiple) time series has been investigated in depth in the literature in the past decades <ref type="bibr" target="#b7">[8]</ref>. The technical approaches range from statistical techniques, e.g., based on an autoregressive integrated moving average (ARIMA) model, over various types of traditional ML techniques, to the most recent deep learning approaches, see <ref type="bibr" target="#b32">[33]</ref> for a recent comparison. Several multivariate time series approaches <ref type="bibr" target="#b28">[29]</ref> have been proposed for different applications in the literature as well, e.g., cash-flow prediction <ref type="bibr" target="#b30">[31]</ref> and macroeconomics <ref type="bibr" target="#b26">[27]</ref>. The recent interesting work of <ref type="bibr" target="#b19">[20]</ref>, addressing market basket prediction, shares some similarities with our method with respect to the used the concepts, although using a different approach and application setting. In particular, it addresses co-occurrence, sequentiality, periodicity, and recurrence for next basket prediction with the purpose to speed up shopping sessions.</p><p>From the various time series forecasting approaches from the literature, we include a traditional ARMA model (as well as ARIMA/SARIMA variants) as an additional baseline in our experiments. One of the reasons for their use is that a recent comparison in <ref type="bibr" target="#b32">[33]</ref> revealed that such traditional models can lead to performance results that are competitive with more recent and more complex models in certain domains. However, as our results will show, these traditional time series models have their limitations in our problem setting, since they are only able to consider each time series individually. We will discuss this aspect in more detail in Section 5.3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Alternative Ensembling Approaches</head><p>A number of alternative ensembling models exist that could be used instead of those used in our approach. For example, in <ref type="bibr" target="#b39">[40]</ref>, deep forests ("gcForest") are introduced. This method aims to be a substitute to deep artificial neural networks (NNs), incorporating several ideas of the latter while being easier to train. Specifically, it is based on a cascade structure to enable representation learning and uses a multi-grained scanning approach. It could in principle be both used for sequential modeling as well as ensembling. In particular, this method appears promising as an alternative to our GBM-based stacking method. The approach of <ref type="bibr" target="#b39">[40]</ref> has however not been applied to similar problems yet and the majority of datasets that were used in existing experiments contain images, audio, or text. Furthermore, the method is specifically tailored to classification problems. Therefore, it has to be adapted and extended so that it can be applied to our time series based regression problem. We leave such extensions as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OVERVIEW OF THE PROPOSED METHOD</head><p>The main novelty of our learning approach is that we make use of time series information of past customer behavior by means of a deep sequence-to-sequence RNN with temporal convolutions and dynamic features, which we combine with GBMs to achieve high prediction performance. In this section, we first provide examples that illustrate how considering multiple features over time can improve the predictions (see Section 3.1). Afterward, we provide a high-level overview of our method (see Section 3.2). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motivating Examples for Sequential Modeling</head><p>Figure <ref type="figure" target="#fig_1">2</ref> sketches parts of the information that one might have recorded for some Customer A and Customer B over a certain period of time. Every row of each table represents a feature and the numbers in each column represent the corresponding values of this feature for a certain period of time, in this case a week. Some of the features in the example are common in many application domains (e.g., the profit in $), whereas others are particularly relevant for our guiding application, an online shop offering goods for children (e.g., the number or the age of the children).</p><p>The columns, i.e., the weeks, are organized in ascending order. Likewise, the features that refer to the time since the first and the last purchase, as well as the child age, are also recorded in weeks. The number of orders and the number of items represent how many orders were made and how many different items were bought in the respective week. The Customer ID is also listed in this example, but it is treated in a different way. It is passed to an embedding method as described in Section 4.2, which returns a vector of values resembling a learned latent customer behavior vector.</p><p>The question marks represent unknown values in the future. In this example, the CLV should be computed for the limited time horizon of the next four weeks and calculated as the sum of the predicted values for these four weeks.</p><p>Looking at the profit numbers for Customer A, we can identify two characteristics that are typical for time series. First, periodicity seems to play a role. One possible explanation for this pattern could be that the customer replenishes the stock of consumables. Second, we can also observe a trend, resulting in an increasing profit over time. This second aspect of increased profit also seems to be related to the number of unique items bought per week. In practice, this might indicate that the customer purchases more and more supplemental products in addition to the consumables that are purchased every four weeks.</p><p>Considering the periodicity and the trend signals, we would expect the profit in the four weeks to be approximately equal to (0, 78, 0, 0). The zero entries are based on the periodicity pattern, and the predicted value for the second week is based on the trend, which implies a slight increase in profit every month (here a slope value of 4 is added to the previous value of 74). Generally, one main goal of our method is to capture such time-dependent patterns-and more complex onesautomatically, without the need of manual adjustments or additional feature engineering.</p><p>Considering the data collected for Customer B, we can observe a sudden increase in profit from $64 to $127 starting from week 9. If we only consider the development of the profit feature over time, it is difficult to impossible to predict such an increase. However, if we consider additional time series information like the number and the minimum age of the children-which can be derived with high confidence from the viewed or purchased products or are voluntarily entered by the customer-we can observe that the birth of a new baby is the probable reason for this increase.</p><p>Such additional sources of information can be powerful predictor variables for the future interests and needs of a customer. Therefore, considering multiple time series in parallel, as done in our method, helps us predict the increase in profit before it actually happens. Note that some of the observed feature values might be caused by exceptional events, e.g., when the customer purchases something valuable as a present for someone else. To account for such situations, our method includes a technique to deal with outliers and other types of noise in the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Main Ingredients and High-Level Architecture</head><p>3.2.1 Main Ingredients. From a technical perspective, our modeling choice can be summarized at a high level as follows. More details will be provided subsequently in Section 4.</p><p>? We use a tailored encoder-decoder sequence-to-sequence RNN, cf. <ref type="bibr" target="#b12">[13]</ref> for basics on encoder-decoder RNNs, because it can learn such temporal patterns (like trends and seasonality) automatically, without the need of manual feature engineering. This is a major advantage over previous approaches, as it can be time-consuming and difficult to express all possible temporal dependencies through handcrafted features.</p><p>? According to the intuition of the aforementioned examples, we do not only consider profit sequences, but a richer set of sequential features that are relevant to the problem. Our training data therefore is not a matrix consisting of profit sequences, but a tensor consisting of sequences of a number of various features, some of which are shown in Figure <ref type="figure" target="#fig_1">2</ref>. ? In practice, the resulting time series are often noisy and therefore it is beneficial to apply smoothing. We therefore use stacked temporal convolutions as one ingredient of our sequence-to-sequence RNN, as these can be seen as automated smoothing methods, like weighted moving averages. ? To further improve the prediction performance, we use a GBM model and combine it with our sequence-to-sequence RNN model by stacking. GBMs are in general well-performing for tabular data and are furthermore able to detect additional patterns in the data, which are not captured by the RNN. Finally, GBMs are computationally less expensive than RNNs, which allows us to include more features given the same computing resources compared to our RNN model. ? Both for the GBM model and the RNN, we learn embeddings of the recorded purchase logs.</p><p>These embeddings in particular help us to reliably perform clustering of similar customers also in data-sparse situations. Furthermore, to consider temporal aspects also in the input to the GBM model, we do not generate one single feature vector per user, but multiple vectors per user, one for each considered point in time (e.g., each point in time consists of the aggregate over a time frame of one day or one week).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Training Data Generation.</head><p>One additional novelty of our approach compared to existing CLV estimation approaches lies in the way we process the available time series data in order to make sequence-aware predictions with our models. Figure <ref type="figure" target="#fig_2">3</ref> gives an overview of the principles of how we generate the training data for learning.</p><p>Defining Input and Output Sequences for the S2S RNN Model. The sequence-to-sequence model takes a series of values with defined length as an input and returns predictions for a subsequent sequence of values as an output. The length of the output corresponds to the estimated customer lifetime and has to be chosen depending on the domain and the available data. In the example, the output sequence length is set to four weeks for illustration purposes. The sum of these four predicted values is used as an approximation of the CLV. Note that such an estimation and limitation of the customer lifetime is usually necessary, since the actual lifetimes are often unobserved in practice.</p><p>The input sequence consists of the values observed immediately before the output sequence. The length of the input sequence, which is 12 in the example, can be considered as a tuning parameter. Depending on the domain, it can for example be meaningful to restrict the input length to focus only on the more recent-and thus more relevant-input data. Note, however, that even though we use fixed-sized input sequences, information prior to the input sequence can still be considered as features in the model, as will be described below.</p><p>Sliding Window Approach. In order to make the model more robust, we do not only consider one input sequence and one output sequence, but use a sliding window approach. If we would only use one time-based split-i.e., if we only use the first block in Figure <ref type="figure" target="#fig_2">3</ref> for training-we run the risk of overfitting to a small part of the data. Therefore, we repeatedly go back by one time step and create input and output sequences of the defined lengths for earlier parts of the time series, leading to a number of additional training cases per customer. In each step back, we therefore consider an additional point in time in the past. The procedure is repeated until the defined beginning of the time series is reached.</p><p>Generally, using a sliding window approach for time series forecasting is a well established technique in the research literature. However, to our knowledge, it has not been used in this given form for the problem of CLV prediction.</p><p>Temporal Aggregation and Data Format for GBMs. Unlike RNNs, GBMs cannot process training data in time series form. Instead, "temporal aggregations" are needed, which are calculated for each point in time. In our approach, we consider temporal information in GBMs by applying various aggregation functions on time-based features, e.g., the sum of past profit, but also the minimum, maximum and mean profit per order, statistics of times between orders, etc.</p><p>Note that we do not only use these temporal aggregation features in our GBM model, but also in our sequence-to-sequence RNN model, which resulted in an additional performance increase. Remember, however, that a main difference of the two models is that in the GBM model we directly  optimize for the sum of the future profits, whereas we predict the elements of a time series with the RNN to better capture sequentiality in the data.</p><p>Figure <ref type="figure" target="#fig_3">4</ref> visualizes how we generate multiple training data points for each customer in the GBM model. In the example, we assume that the sum of the profit for a customer up to T now -6 is 306. Note that this number cannot be derived from Figure <ref type="figure" target="#fig_2">3</ref>, as Figure <ref type="figure" target="#fig_2">3</ref> only shows profits happening 9 points in time later than the first order of the customer (see the row "Time Since First Order"). We then observe a profit of 67 at T now -6, as shown in Figure <ref type="figure" target="#fig_2">3</ref>, leading to a sum value of 373 in the temporal aggregation Figure <ref type="figure" target="#fig_3">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">High-Level</head><p>Architecture. The high-level architecture of our approach and the process and information flow are illustrated in Figure <ref type="figure" target="#fig_4">5</ref>. The input to the process are the different types of data 1 , including the history of purchase transactions, metadata about the items (e.g., category information), customer data, etc. In a preprocessing phase 2 , we compute the feature values that are later fed into the ML models and also train the embedding models from the sequential transaction data. The resulting feature values and embeddings are then used by the GBM model 3 and the RNN-based model 4 in the main training phase. The outputs of these models are afterward combined in a GBM stacking model 5 , which computes the final CLV predictions for every customer 6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">TECHNICAL DETAILS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Feature Engineering Approach</head><p>A main challenge in practical ML applications is the identification of a suitable set of predictor variables (features) for the given problem. Typical features for the CLV prediction problem, as mentioned above, are based on purchase statistics, e.g., past purchase amounts, the recency of the past purchase, etc. Besides such more general features, there can also be domain-specific features, e.g., related to certain characteristics of the customer like the number of children in our main application scenario. Furthermore, whether or not certain features are used in a practical application depends on the relevance, availability, and quality of the underlying data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">General</head><p>Model. We formulate our task as a supervised learning problem, which supports the integration of an arbitrary number of features, which is a common approach in ML. The generic problem formulation is to learn a function y = f (x) that returns the predicted CLV value, where x is a vector of feature values. These features represent all quantifiable information about a customer that is relevant to the problem. Given such a generic problem formulation, various types of ML algorithms can in principle be applied to train a model that is able to generate predictions y = f (x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Used</head><p>Features. We designed our hybrid CLV prediction approach in the context of a specific application scenario as described above. Therefore, the set of our predictor variables comprises not only more general features (for example, profit could be equal to usage time in an online service scenario, outside traditional e-commerce), but also a smaller set of features that are specific to the given problem. Many of the features, which we summarize next, have never been used in the literature for CLV prediction before, even though some of them are not limited to a particular application domain.</p><p>As explained in Section 3.2.2, a further unique aspect of our feature engineering approach is that we consider all features to be time-dependent. Most of the features we use are in fact dynamic, i.e., change over time. As shown in the example above, we do not consider the number of children of a customer as a static feature that is computed only once, but we derive the feature values for every point in time considered in the past, e.g., each day or week. The groups of features we present in the following should therefore be thought of being dependent on time. They are computed in the same way as shown in Figure <ref type="figure" target="#fig_3">4</ref>.</p><p>The main features of our prediction model can be organized in three groups:</p><p>(1) Features based on customer attributes.</p><p>(2) Features based on orders (item purchases).</p><p>(3) Features based on other item interactions.</p><p>Note that most features of our models-in particular those that are related to orders and other types of customer behavior-represent aggregated values, e.g., the number of purchases or the total profit that was made. In principle, our general model also supports the usage of features at a finer product level. In such a model, each item on sale could represent a feature and the feature value indicates whether or not the customer purchased this product. In real-world cases, however, this would lead to very large feature vectors and the problem of data sparsity. For this reason, we will </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Static and Demographic Features</head><p>? The country, residence and sex of the customer.</p><p>? The customer's estimated income.</p><p>? The channel through which a customer was acquired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamic Customer Attributes</head><p>? The minimum and maximum child age as well as the mean and the median child age (in days).</p><p>? Aggregate statistics about the child ages when orders were placed, since these factors determine the range of relevant products. ? The number of children, because a higher number of children is expected to correlate with a higher purchase probability. ? In other applications, the ages of customers are more relevant than the ages of their children.</p><p>generate features only on the level of product categories, which also helps us address sparsity issues.</p><p>Features Based on Customer Attributes: We consider both static features of customers as well as attributes that change over time. In our specific application, the estimated age and the number of children at every point in time are important domain-specific factors that influence the predicted profit in the future. The list of customer-related features is provided Table <ref type="table" target="#tab_0">1</ref>.</p><p>Features Based on Orders: This set of features is similar to the ones used in classical RFM models. In contrast to traditional RFM models, however, our feature set is not only richer but the features are again determined for different points in time. The specific set of features that was used in our experiments is shown in Table <ref type="table" target="#tab_1">2</ref>. <ref type="foot" target="#foot_4">4</ref> These features are not specific to the given e-commerce application domain (e.g., in a telecommunication provider scenario, one might have usage times instead of orders).</p><p>Features Based on Other Item Interactions: This final set of features serves two purposes. First, by considering various types of interactions with items on the shop, our goal is to (i) predict the likelihood that a customer will try new items at certain price levels, and to (ii) acquire general item popularity levels and incorporate repurchasing patterns in our model, because it is plausible that the repurchase probability is an important predictor for future profit.</p><p>Second, another part of this third set of features is used to deal with sparse data situations. Here, we do not consider item interactions at the individual level but aggregate past user behavior at the level of item categories. A detailed list of the used features is shown in Table <ref type="table" target="#tab_2">3</ref>.</p><p>Novelty and Already Used Features in the Literature: From the various features presented in Tables <ref type="table" target="#tab_0">1</ref><ref type="table" target="#tab_1">2</ref><ref type="table" target="#tab_2">3</ref>, to the best of our knowledge only the following were used in previous work for CLV prediction problems:</p><p>? Traditional recency (time since last order), frequency (number of orders) and monetary value (net sales) features, but also their restriction to more current time windows instead of considering the whole history (like within the last quarter). ? The number of items bought by a customer.</p><p>? The age and country of a customer.</p><p>? The time between the first and the last order.</p><p>? The average order date as well as the standard deviation of order dates.</p><p>Improved CLV Prediction With Seq-To-Seq Learning and Feature-Based Models 80:13 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Features Related to Recency and Frequency</head><p>? Total number of orders by the customer.</p><p>? Number of orders within the last time period (for several time periods, e.g., the whole history, the previous year etc.), indicating how frequently the customer buys in recent times. ? Number of days since the last order, which reflects recency.</p><p>? Number of days since the first order, which informs us how long the customer has been in business with a company. ? Lags of the number of orders (i.e., the previous number of orders for varying time steps in the past) and lags of the days since the last order (meaning how long it took in the past for the customer to order again). ? Median, mean and variance of the number of days since order for all past orders (cf. the reasoning below).</p><p>? Mean and standard deviation between order dates (as proposed in <ref type="bibr" target="#b9">[10]</ref>).</p><p>? Time difference statistics (gaps between orders), such as:</p><p>-average (mean, median) and maximum number of days between orders as well as their variance; -number of days between the last order and the second last order; -number of days between the second last order and the third last order (and optionally further time differences).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Features Related to Monetary Value</head><p>? Total profit and revenue made by the customer.</p><p>? Profit made in the last time period (for several time windows of varying length, corresponding to running sums/means). ? Revenue made in the last time period.</p><p>? Lag variables of the profit and nonzero profit, where the lag of nonzero profit represents previous amounts of profit by the customer (e.g., the first lag of nonzero profit is the profit made in the last order of the customer). ? Mean profit and mean revenue per order.</p><p>? Minimum profit and revenue made in an order by the customer.</p><p>? Maximum profit and revenue made in an order.</p><p>? The variance of profit per order (assessing uncertainty).</p><p>? The mean difference of profit per order (capturing a trend).</p><p>All other features presented above can be considered novel for our application domain. Lag variables have not been used for a CLV prediction problem in the literature. Their usage in other applications is, however, common. Likewise, for GBM models it is common to include categorical features, which are often passed in the form of sparse one hot features (i.e., encoding if a given attribute is "active" for a particular training case).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Learning Embeddings</head><p>Instead of directly using raw sequential data, researchers often rely on the use of embedding representations, in particular in sparse data situations. In the context of CLV prediction, Chamberlain et al. <ref type="bibr" target="#b9">[10]</ref> learn embeddings from logged item view events to identify customers with similar interests. We rely on this method in our framework as well. However, instead of clickstream data, we learn embeddings from purchase data. In case clickstream logs are available, these can be used in our framework as well.</p><p>The intuition of this approach is illustrated in Figure <ref type="figure">6</ref> (compare Figure <ref type="figure" target="#fig_6">7</ref> in <ref type="bibr" target="#b9">[10]</ref>). For each product, we determine the sequence of its purchasers, ordered by purchase time. Using this data representation, our goal is now to identify customers who have ordered similar products at a comparable point of their lifecycle. Technically, we use an adapted SkipGram model with negative sampling <ref type="bibr" target="#b33">[34]</ref>, known as Word2Vec in natural language processing. In our case, the customers </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Features Related to Item Interactions</head><p>? Number of unique items a customer has purchased and the sum of all quantities over every item.</p><p>? Number of unique subcategories from which the customer has bought a product.</p><p>? Number of unique brands from which there was a purchase by the customer.</p><p>? Mean, minimum and maximum price of the items that were purchased (capturing the value of the items typically ordered, like the cheapest and most expensive product a customer has bought). ? Mean cost (for the vendor) of the items that were purchased.</p><p>? Statistics of the number of orders an item appears in (across all customers), which aims to capture popularity (e.g., does the customer prefer to buy rare items?). ? Statistics of the number of customers who have bought an item (with the same intuition).</p><p>? Statistics of the absolute as well as the relative number of repurchases (e.g., the maximum mean number of repurchases is an important feature). ? Statistics of both the absolute and the relative number of customers who reordered the respective item.</p><p>? The maximum price difference of items the customer has bought.</p><p>? Variances of the item prices and the relative number of repurchases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Features to Deal With Data Sparsity</head><p>? For every item category, we model a feature that represents the number of orders the customer has placed that contained an item from the respective category. ? Analogously, for every brand, we record the number of orders which contained an item of this brand.</p><p>? Customer and product IDs, which are not used directly, but are converted into embeddings, as will be described in Section 4.2.</p><p>Fig. <ref type="figure">6</ref>. Visualization of the SkipGram based customer embedding method with the usual context window adapted from <ref type="bibr" target="#b9">[10]</ref>.</p><p>correspond to words and the products can be seen as sentences in terms of the original NLP approach.</p><p>We use the learned embeddings in both main models of our approach: (1) In the GBM model, we use the learned customer embeddings as features. As a result, there are as many additional features as embedding dimensions and the feature values correspond to those of the embedding vector for the customer. ( <ref type="formula" target="#formula_1">2</ref>  with frozen weights (i.e., they are static during the main S2S training). The values of these frozen weights are set to the pretrained customer embedding values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">CLV Prediction With GBMs</head><p>The core of our hybrid approach, as sketched in Figure <ref type="figure" target="#fig_4">5</ref>, consists of (i) a more traditional supervised learning problem formulation of the CLV prediction problem and (ii) an additional formulation based on a time series interpretation and a deep learning model. For the more traditional problem encoding, we rely on Gradient Boosting Machines (GBMs). GBMs, like Random Forests (RF), have shown to lead to high prediction accuracy in many domains. In addition, they are known to lead to generalizable models and are less prone to overfitting than other methods.</p><p>RFs were used for CLV prediction in <ref type="bibr" target="#b9">[10]</ref>. They rely on training a larger number of treeseach on a resampled version of the original data obtained by bagging-as well as on selecting a random subset of features at each split. The goal of this procedure is to reduce the variance of the predictions from a bias-variance tradeoff point of view.</p><p>In our work, we use GBMs instead. <ref type="foot" target="#foot_6">5</ref> GBMs are also additive trees, but their main goal is to reduce the bias of the predictions. At each step of the training process, a new tree is added such that it minimizes the loss of the residuals by means of gradient boosting. It therefore focuses on improving the cases with the highest errors. As our experiments will show, the use of GBMs instead of RFs is advantageous in our problem settings with respect to accuracy.</p><p>Regarding the format of the input data, note that only the sequence-to-sequence RNN model is able to learn from the data in form of ordered sequences that are stacked together as tensors. The GBM always requires a single target value and a row of feature values for every training data point. For that reason, we have extended the training data as described above in Section 3.2.2 to enable the GBM to learn from the entire history. As highlighted in Figure <ref type="figure" target="#fig_3">4</ref>, we do not only have one training data row for each customer, but one training data row for each point in history for that customer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">CLV Prediction With RNNs and Encoder-Decoder Sequence-to-Sequence Modeling</head><p>In the following, we describe our novel deep learning architecture based on RNNs to make predictions using the generated time series data. As sketched in Section 3, we follow an approach, where we make profit predictions for individual time steps in the future (e.g., for every week), and compute the CLV as the sum of these predictions.</p><p>Our final NN architecture is shown as a flow diagram in Figure <ref type="figure" target="#fig_7">8</ref>. In the following subsections, we will first discuss its individual components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">RNNs and GRUs as Building Blocks.</head><p>RNNs are a natural choice for creating deep learning models based on sequential data. <ref type="foot" target="#foot_7">6</ref> In our application scenario, models of this type help us to capture long-term trends in purchasing behavior, e.g., a steady increase in order volume over time, as well as periodic patterns. RNNs are NNs whose weights are shared across time steps with the purpose to generalize with respect to time. Inputs to RNNs are the current time series value as well as the Improved CLV Prediction With Seq-To-Seq Learning and Feature-Based Models 80:17 hidden layer values of the previous time step, which are in turn updated. Note that these hidden states are used because they are able to dynamically store information from arbitrary prior times. This would not be possible if we would instead use a fixed number of explicit previous time series values. The hidden states can be seen as a compressed summary of the time series, capturing its nature.</p><p>Technically, we use networks of GRUs <ref type="bibr" target="#b12">[13]</ref> as building blocks. GRU networks can be considered as a simplification of long-short term memory networks (LSTMs) <ref type="bibr" target="#b22">[23]</ref>, which retain most of the modeling capabilities of LSTMs, while being computationally more efficient and being less prone to overfitting for smaller datasets <ref type="bibr" target="#b13">[14]</ref>. Internally, GRU-based models have a recurrent structure with gates that control the flow of information and states that represent memories, allowing them to capture long-term dependencies that are problematic to be stored for usual RNNs, since gradients tend to either vanish or explode. By using GRU blocks, we are thus able to mitigate the vanishing and exploding gradient problem. Other ways of dealing with this problem include our use of additional features based on temporal statistics, which helps keeping information when backpropagating over a longer time period. Likewise, aggregating on a less granular basis when meaningful (for the Children dataset in our applications) can be helpful, since the time sequences will be shorter and are therefore less prone to these problems. The general structure of GRUs is shown in Figure <ref type="figure" target="#fig_6">7</ref>. Formally, the model can be expressed as follows. Suppose that (x t , h t -1 ) is the concatenation of the current time series input and the previous hidden state vector, which are passed to NNs, leading to the following equations:</p><formula xml:id="formula_0">z t = ? (W z ? (x t , h t -1 ) + b z ),<label>(1)</label></formula><formula xml:id="formula_1">r t = ? (W r ? (x t , h t -1 ) + b r ),<label>(2)</label></formula><p>where W denote the weight matrices and b the biases in the NN operations with corresponding subscripts. Note that z t corresponds to the update and r t to the reset gate NN. Their role and interaction is given as follows: First, a proposed new hidden state candidate h t is computed by an NN and for its input, the previous states are partially reset by the NN that outputs r t :</p><formula xml:id="formula_2">h t = tanh (W h ? (x t , r t h t -1 ) + b h ),<label>(3)</label></formula><p>where denotes the Hadamard (element-wise) product. Finally, the new state h t is an average of the previous state and the proposed new hidden state candidate, with weights computed by the "update-NN" (with output z t ):</p><formula xml:id="formula_3">h t = (1 -z t ) h t -1 + z t h t .<label>(4)</label></formula><p>Note that the function operations (sigmoid and tanh) are applied to every component. The individual networks act as gates and control, which information is updated and what persists in memory.</p><p>The idea can therefore be summarized as follows: The "reset-NN" r t determines the amount of the previous hidden state (from the last time step) that should be taken into account when calculating a new hidden state candidate h t for the current time step. The edge cases are either to remember everything from each element of the hidden state (having a value of 1 for the component of r t ) or nothing (for a value of 0) when calculating the new hidden state candidate. However, this is not final. The second "update-NN" z t is used to determine whether the newly calculated hidden state is actually meaningful, or whether it is better to partially adjust the final values of h t to the previous hidden state vector h t -1 by using a weighted average. Again, for every component of z t , a value of 1 corresponds to using entirely the newly calculated value of h t without taking into account the past state, while 0 corresponds to entirely using the previous hidden state instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">A</head><p>Sequence-to-Sequence Modeling Approach. When using standard GRU or LSTM layers as implemented in libraries such as Keras <ref type="foot" target="#foot_8">7</ref> and TensorFlow<ref type="foot" target="#foot_9">8</ref> , we can either make a prediction for a single time step or generate an output sequence that has the same size as the input sequence. This represents a major limitation in our problem setting. Often, the future period of time for which we want to make predictions is much shorter than the history of past sales of a customer, which may span several years. For instance, if we want to make CLV predictions for the upcoming six months, we could only take the last six months of transactions into account when using the default implementation.</p><p>To deal with this issue, we use a sequence-to-sequence model architecture, which has been successfully used in several fields such as machine translation in the past <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b36">37]</ref>. Following such an approach, we first use an encoder model consisting of a GRU layer that learns a summary of the dependencies within the input time series which is saved in the hidden states. These hidden states are then passed as initial states to another layer of GRUs, the decoder model. The purpose of the decoder model is the generation of the output sequence, which represents the future profit for each time step (e.g., every week in the next six months).</p><p>Since we use the learned weights of the encoder GRUs as input, the information of a longer period in the past is implicitly included in the model. As a result, the model is no longer restricted to the most recent history (like the past six months). To make the model more robust and to be able to take into account all available training data, we furthermore employ a sliding window approach when generating the training data, as discussed in Sections 3.2.2 and 4.6.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Adding Multiple Encoder</head><p>Layers. Convolutional NNs have been successfully applied in different domains, in particular for image recognition. One of the underlying ideas is that in such networks, each convolutional layer captures a different level of abstraction <ref type="bibr" target="#b16">[17]</ref>. In our approach, we transfer this idea to the problem of learning patterns from time series data. We therefore extend our sequence-to-sequence approach with multiple layers of encoder GRU networks. The outputs of each GRU encoder layer are passed through a dense layer of corresponding input and output sizes to learn the optimal way of passing information.</p><p>In our experiments, we used two such encoder layers. The idea is that the first layer learns more fine-grained time series characteristics, while the second one is able to learn more general rules based on the summaries generated by the first encoder. The second encoder therefore has a regularizing effect. The number of hidden units in each of the layers are hyperparameters that have to be determined in the training phase using the validation data. Technically, the mapping to the output sequence by the decoder layer is achieved with a tensor slicing operation to match the sequence length as well as a time-distributed dense layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.4">Incorporating Temporal Convolutions for Smoothing.</head><p>Another important ingredient of our model is the use of multiple one-dimensional temporal convolutions, in particular causal convolutions. These can be seen as automated smoothing methods aiming to increase the generalization ability of the model. They are particularly helpful in the presence of noisy observations. An illustration of this concept by means of an example profit sequence of a customer is shown in Figure <ref type="figure">9</ref>. Given a defined window size, a set of kernels (filters) are learned that perform a specific form of averaging given the neighboring points. Differently from existing static or weighted moving average methods, the approach has the advantage that the averaging method is learned from the data and is not based on certain assumptions. For example, an exponential decay weight [like (1, 2, 4, 8) ? 1/15] would be a special case.</p><p>In our experiments, we found that using two chained convolutions to generate monthly and quarterly statistics led to an increase in performance, due to the improved robustness against overfitting when dealing with largely fluctuating data points. Generally, using multiple temporal convolutions corresponds to the idea that multiple levels of aggregations are learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.4.5</head><p>Overall RNN Architecture. Our overall architecture for the proposed sequence-tosequence approach is summarized in Figure <ref type="figure" target="#fig_7">8</ref>. We start by applying the two temporal convolutions to smooth the data with the previously engineered features, and combine the outcomes with the pretrained embeddings in a concatenated tensor. Then, we have the two GRU encoder layers and the decoder layer. The time-distributed dense layer means that a dense layer is applied to every time step of the decoder output sequence. Using a tensor slicing operation, we finally create the output data, i.e., the sequence of the predicted profit values. Note that the usual dropout and recurrent dropout are options that can be beneficial for the generalization ability of the model, similar to bagging in RFs. They are based on randomly switching off units.</p><p>In Appendix A, we provide further mathematical details of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">GBM-Based Stacking</head><p>As a final step, we use a stacking technique to further improve the prediction accuracy of the CLV predictions. Model stacking can be considered as a meta-modeling technique, where the predictions generated by the individual models are used as features in a higher-level model. In our approach, we use GBMs also for the stacking process. The motivation for using GBMs is that they are designed to make predictions based on rules. In our setup, a GBM could, for example, learn that RNN-based predictions are more accurate than GBM-based ones for customers with a higher number of past purchases. Technically, our stacking procedure can be summarized as follows:</p><p>(1) Train both the GBM model (see Section 4.3) and the RNN model (see Section 4.4) on the training set of a given time-based training-validation-test split of the data. (2) Generate predictions for each customer using both methods for the time frame that is covered by the validation set. (3) Train a second GBM that uses the original set of features as an input but also the predictions of the first GBM and the RNN model to generate predictions for the time frame of the test set. The resulting model makes it possible to compare the values predicted by both (GBM and RNN) model types with the true values on the validation set. Thus, we can decide when it is better to use one type or the other.</p><p>Note that the second level GBM corresponds to the first level GBM with an augmented number of features. At the same time, the predictions of the first level GBM and the S2S RNN model are the most essential features. The remaining features are helpful to enable the model learn in what cases which model or combination works best. In fact, the GBM stacking model does not only distinguish between taking either the prediction of the GBM or the S2S model, but it can also learn a combination like a weighted average of the predictions. Further note that the GBM stacking model is trained with the most recent data only, since data before the respective period is needed for training in order to create the predictions of the first GBM and S2S model.</p><p>The intuition behind this approach is the fact that both model types learn different patterns in the data and that model ensembling can improve performance by reducing variance. GBMs are good at finding rules in a huge set of features, leading to high accuracy for tabular data. But they are not tailored to sequential modeling, where the S2S model has advantages, as it can better learn sequence patterns. This combination aims to get the best out of both approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Additional Implementation Aspects</head><p>In this section, we discuss two more implementation-related aspects of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.1">On-the-Fly Generation of Tensors.</head><p>The generated training data for our combined model using the sliding windows procedure can become large. We have therefore implemented an approach, where we do not necessarily generate the entire training data-i.e., the various tensors for each customer-in advance. Instead, we dynamically generate tensors of training sequences on the fly to make sure that the data fits into working memory also for larger datasets. This is done by randomly sampling a time shift value, e.g., the number of weeks to go back in time, instead of computing the data for all possible shifts at once.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.2">Dealing With Large Training Datasets.</head><p>Our approach, which leverages the sequential nature of the data, is computationally more demanding than if we would use only one feature vector per customer and compute one CLV prediction. We do not only make predictions for each time step in the future, but also consider multiple time series and use a sliding window approach to create multiple subsequences from the data.</p><p>The computational complexity in our problem setting in general is multiplicative in the input parameters, which are the number of customers, the number of features and the sequence length. When relying on a naive implementation, in particular the memory demands can be a limiting factor for the sequence-to-sequence-based model. For example, with one million customers, an input sequence size of 52 weeks, an output sequence size of 26 weeks, 10 sequential features, and a total set of 128 training weeks, the memory demand would be over 100 GB RAM. In fact, memory requirements can already be problematic during preprocessing, since for building the feature matrix a cross join with all time points has to be performed in order to make zeros explicit.</p><p>To deal with this challenge, we take advantage of the fact that training data can be generated for every customer independently and does not need to be stored when this customer is not used for training in the current batch. <ref type="foot" target="#foot_10">9</ref> We therefore first process the training data for a subset of the customers, as we will describe below. We then train the NN with the given subset of the data and store its weights. These weights can then be reloaded when we continue with the next customer Improved CLV Prediction With Seq-To-Seq Learning and Feature-Based Models 80:21 subset. This can all be done in an exhaustive manner, such that all disjoint subsets are successively iterated through, to ensure that all customers are considered during training. Overall, this helps us to avoid to start from scratch every time. The approach is summarized in Algorithm 1. Note that the underlying optimization method is a modified version of mini-batch stochastic gradient descent, which allows for a better use of memory but retains its convergence properties due to the randomization steps <ref type="bibr" target="#b6">[7]</ref>. Also note that in Algorithm 1, a number of rounds n tol with a higher error is accepted (which is treated as a hyperparameter), due to the stochastic nature of the optimization method.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL EVALUATION</head><p>This section describes the details of the experimental setup that was used to validate our approach as well as the obtained results. We developed a customizable and highly configurable software framework for the purpose of the evaluation. We share the code online to make our work reproducible. <ref type="foot" target="#foot_11">10</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Datasets.</head><p>We evaluated our approach on two datasets, where one is publicly available and the other one is proprietary. Unfortunately, public datasets that include profit or revenue information, which are essential for CLV prediction, are generally scarce.</p><p>The larger dataset, termed Children, was shared with us by a European e-commerce company selling products for children and families, having over one million customers. The corresponding purchase data was recorded in a period of over three years. In addition, we validated our findings on a public, <ref type="foot" target="#foot_12">11</ref> but smaller dataset, which we term UKRetail. The dataset consists of 541,909 purchase records by more than 4,000 customers of a UK-based company, which were recorded during a period of about one year <ref type="bibr" target="#b10">[11]</ref>. For the smaller UKRetail dataset, we build our models based on daily data. For the large Children dataset, we used weekly aggregates instead to make the computational processes more efficient. For the same reason, only selected features of high importance were used in the RNN model for the Children dataset. We created training, validation and test data splits as described above in a way that the validation and the test splits span equal periods of time. For the Children dataset, we used the last six months of the available data for testing and all other data for training and validation. For the smaller UKRetail dataset, we use time frames of four weeks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.1.2</head><p>Denoising. An analysis of the datasets revealed that both of them contain purchase records from wholesale clients, i.e., customers who are companies by themselves and resell the products they buy. Compared to the large majority of regular customers, the wholesale clients generate very large revenues. In order not to bias our results, we have focused on regular customers and excluded what we considered wholesale customers in our experiments. We applied empirically determined thresholds for both datasets. For the UKRetail dataset, we excluded 20 % of the customers with the highest sales values based on an analysis of the quantiles of the distribution. We also excluded 1 % of the customers with very low (negative) sales values. Negative revenue values are not natural and could indicate either fraud or artificial transactions by virtual debitors (e.g., stock movements in the logistics process). The Children dataset also includes purchases from wholesale customers, but only to a small extent. We therefore excluded only the 0.1 % of the customers with the highest and lowest sales numbers.</p><p>For the public UKRetail dataset, we additionally found unusually high and low values at the beginning and at the end of the data collection period. We can only speculate about the reasons for these anomalies, which were not discussed in previous literature. They could be the result of external factors like initial promotions when the site was launched, or the results of other datarelated issues (e.g., data imports from existing systems) when the data collection began. The high values at the end of the collection period are also most probably the result of an external effect, like the consideration of additional sales channels. To account for these anomalies, we did not include the five first and last 16 weeks from the UKRetail dataset, based again on an analysis of the distribution. Note that we repeated the experiments mentioned below with the original data as well. Keeping the data leads to similar outcomes, with comparable trends but different values. Nonetheless, we believe that not including the questionable data leads to more reliable insights.</p><p>In Table <ref type="table" target="#tab_3">4</ref>, we summarize the statistics for both datasets before and after denoising. It is noticeable that the overall dataset size of the UKRetail dataset is significantly reduced by denoising. This is partially due to the smaller dataset size after filtering out anomalies as well as the high amount of transactions generated by wholesale clients. This shows the huge effect wholesalers have on this dataset. We also tested a weaker dataset filtering, for which the absolute errors of all methods increase substantially, but the relative performances remain comparable. Nevertheless, the denoising is justified and yields more stable results.</p><p>Improved CLV Prediction With Seq-To-Seq Learning and Feature-Based Models 80:23</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experiment Configurations</head><p>5.2.1 Compared Approaches. We compared our approach with four alternative methods.</p><p>? Adjusted Customer Mean Baseline (ACMB) and ARMA: We use the ACMB as a simple baseline. It predicts the CLV for a given future time frame based on the past profit of the customer. The prediction is computed by adjusting the total profit made by a customer in the past to the length of the prediction time frame. We furthermore extended it to ARMA and related time series models, for which the ACMB can be considered a special case.</p><p>? BG/NBD: The model from <ref type="bibr" target="#b14">[15]</ref>, as described in Section 2.3. This can be considered as a traditional RFM model, since it is based on RFM attributes, but uses a more sophisticated distribution fitting approach. It allows for an easy computation and is the most popular RFM method in the literature. ? MC: As another traditional CLV prediction model, we include the sequential MC-based approach from <ref type="bibr" target="#b34">[35]</ref> as another baseline, see also Section 2.2. ? Embeddings-RF: This is a recent method described in <ref type="bibr" target="#b9">[10]</ref> which is based on embeddings and RFs. Since this method was evaluated in <ref type="bibr" target="#b9">[10]</ref> on a dataset which is not publicly available, we will report the results using the information that was available in our datasets.</p><p>For all investigated methods, we determined optimal hyperparameter values for each dataset based on the validation data. For our own approach, which we term GBM-S2S, we will not only report the results for the combined model, but also the performance numbers of its componentsthe GBM-based model (termed GBM) and the sequence-to-sequence RNN model (termed S2S)when used in isolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Evaluation Measures.</head><p>We use the root mean square error (RMSE) and mean absolute error (MAE) of the predictions as our evaluation measures. Both summarize the difference between the CLVs predicted by a model and the actual, hidden values in the test set. The difference is that the RMSE measure penalizes larger deviations more strongly. This might be desirable in application scenarios where larger errors lead to a disproportionately higher risk for business or if it is especially important to detect high value customers. The advantage of the MAE is that can be directly interpreted as the average error in terms of money.</p><p>Note that in the research literature on time series forecasting, other measures than the MAE or RMSE are commonly used, such as the mean absolute value percentage error (MAPE). In our case, where we deal with a specific application domain, such percentage-based measures are less informative than those that work on the absolute monetary values, in $ in our case. Consider a customer whose CLV we estimate to be $5, while it is in reality $10, and another customer, whose CLV is estimated to be $50, while it is in fact $100. The relative error would be the same, but the absolute numbers are much higher for the second case, and a wrong estimate for the second case represent a higher risk or loss for the company.</p><p>According to the definition by <ref type="bibr" target="#b4">[5]</ref>, the CLV is based on profit data. For the Children dataset, we computed the profit of each purchase by subtracting the costs known for each product (e.g., purchasing price for the vendor, shipping costs) from the revenues. For the UKRetail dataset, no information about costs is provided, which is why we used the revenue as an approximation for the profit.</p><p>The CLV definition of <ref type="bibr" target="#b4">[5]</ref> also takes into account the discount rate of capital, i.e., the effect of the interest rate over time. In our evaluations, we used an interest rate of zero for two reasons. First, during the data collection period of the Children dataset, the risk-free interest rate in Europe was actually close to zero. Second, for the UKRetail dataset, the period of data collection only spans one single year, and the prediction time frame is therefore very short. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>In this section, we provide the outcomes of our experiments and discuss their implications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Outcomes.</head><p>The main results are summarized in Table <ref type="table" target="#tab_4">5</ref> for both the UKRetail dataset and the Children dataset. Overall, our method(s) outperform all baselines in this comparison on both measures and for both datasets, and the lowest (best) RMSE and MAE values are in both cases obtained with the stacked combination of the GBM and sequence-to-sequence RNN model (GBM-S2S). All performance differences between our methods and the compared baselines are statistically significant based on a bootstrap test, with all p-values lower than 0.01 (most of them substantially lower).</p><p>For the smaller UKRetail dataset, the differences between our best-performing combined model (GBM-S2S) and its individual components (GBM and S2S) are not statistically significant, but showed a clear tendency (p &lt; 0.2). For the Children dataset, however, the combined model is clearly advantageous and the observed differences are statistically significant with very low pvalues (p &lt; 10 -5 ). This confirms our assumption that GMBs and RNNs are able to consider different patterns in the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Discussion of the Experimental Results</head><p>. Generally, we can observe that the ACMB performs already quite well for both datasets. While a substantial gain in performance can be obtained with more advanced methods, the results for this baseline show that the simple heuristic of using past profit already yields useful predictions. While only a single feature is used in this baseline, it is directly related to the target. It is also intuitively plausible that the past profit is a key predictor for future profit.</p><p>The established BG/NBD model is advantageous when compared to the simplest baseline. This was expected given that this model (i) is more principled in the sense that it considers the underlying distributions and (ii) also includes recency and frequency information as features.</p><p>The MC model also makes use of recency and frequency information, and for the UKRetail dataset, it performs slightly better than the BG/NBD model with respect to the RMSE. For this dataset and metric its performance is almost identical to the more advanced Embeddings-RF method, while for the MAE the difference is bigger. For the Children dataset, it performs better than the BG/NBD method for both error measures. Generally, the advantage of the MC method Improved CLV Prediction With Seq-To-Seq Learning and Feature-Based Models 80:25 over the BG/NBD model can be explained by the facts that (i) the distributional assumption of the BG/NBD model might not be perfectly satisfied, and (ii) that the MC method is sequential in nature.</p><p>The recent Embeddings-RF model of <ref type="bibr" target="#b9">[10]</ref> achieves yet a higher performance. The difference is bigger for the Children dataset compared to the UKRetail dataset. The probable reason for this difference is the rather small dataset size of the UKRetail dataset. Embedding methods are known to yield higher benefits for larger datasets; therefore, the gain obtained by using embeddings is rather limited for this dataset.</p><p>Finally, our proposed method leads to the best results, as mentioned above. Looking at our GBM and our S2S model individually, we observe that they exhibit a different performance depending on the error metric. While the GBM model obtains better values with respect to the MAE, the S2S model achieves the best RMSE. This difference can be explained by the fact that different patterns are detected in the data by both models, therefore, yielding different types of predictions.</p><p>Stacking both models combines the best of both worlds and helps to further increase the accuracy of the predictions. In addition to the use of two types of models, the inclusion of a rich set of features-among them a small number of domain-specific ones in the case of the Children dataset-explain the success of the method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Experiments With Traditional Time Series Models.</head><p>Our experiments revealed that traditional time series models such as ARMA and its ARIMA and SARIMA extensions, mentioned in Section 2.5, did not improve the performance over the ACMB. Note that the ACMB can be considered as an ARMA(0, 0) model, i.e., without autoregressive and moving average terms. This model actually led to the best results of all tested ARMA configurations. An exception only occurs in the case of a moving window evaluation (see the next section) for the UKRetail dataset, where overall a moving average ARMA(0, 1) model has a marginally lower error (about 0.05 lower RMSE than the ACMB).</p><p>Several factors contribute to the limited performance of these models in our application domain.</p><p>? First, ARMA/ARIMA/SARIMA models only consider individual time series, and each time series model is unable to take into account information from the other time series, e.g., from similar customers. ? Second, these models are usually univariate, and as such, they cannot take into account multiple features. We also tried multiple regression with ARMA errors, but this also did not help to improve the performance. This phenomenon can be explained as follows:</p><p>(a) One problem with these external regressors is that their values have to be known or estimated for the future, which limits their use. In our experiments, we tried to either carry the last observation forward, or used lags of the corresponding features, so that we have actual values available for the future period. Both approaches did not lead to improvements. (b) Furthermore, these additional features are again only from the same customer belonging to the respective time series. In contrast, models like RNNs and GBMs consider the data of all customers and can find general rules. (c) Finally, only linear effects are modeled, while nonlinearities can appear frequently as well. At the same time, since ARMA and its extensions only consider individual time series, they fit parameters separately to each of them, thus being more prone to overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.4">Results for a Moving Window</head><p>Evaluation. So far, our measurements were based on one single training-validation-test split of the data. In order to ensure that these results are not the outcome of this particular split, we made additional measurements in which we applied a "moving window" approach. In such a time series cross-validation approach, multiple training-validationtest splits are created, which span different time periods of the available data. This is done by successively going back in time the output sequence length (equal to the size of the validation set and the test set) multiple times. The measurements are then made for each time-based split and the average results are subsequently reported.</p><p>In our experiments, we created three time folds in order to have a minimum amount of training data for each data split. Remember that we need more than one training example per customer and that the input sequence length is twice as long as the output sequence in our experimental configuration. The results of these measurements are summarized in Table <ref type="table" target="#tab_5">6</ref>.</p><p>The results show that our main conclusions still hold, i.e., our proposed approach leads to the lowest RMSE and MAE values. There are, however, differences in the relative ranking of the baseline methods. In particular, the MC method performs slightly better than the more advanced Embeddings-RF method in the experiments for the UKRetail dataset. Also, the BG/NBD works notably better than the other baselines in terms of the RMSE for this dataset, but not with respect to the MAE. The absolute values of the errors consistently increase for all methods, most likely due to less available data.</p><p>These findings confirm that the more advanced Embeddings-RF model is mainly advantageous when larger amounts of training data are available. In particular, for embedding methods based on Word2Vec (which is used in this method), strong effects of the dataset size on the performance were previously reported in the literature (see, e.g., <ref type="bibr" target="#b0">[1]</ref>). Our proposed methods are also affected by this problem, but to a lesser extent, since we use two different model types and a richer set of features, so that the embeddings influence them less. However, the analysis shows that traditional methods can lead to decent results when the amount of available data is limited.</p><p>In case of the Children dataset, the results are closer to those of the last time fold reported in the previous section, but also with somewhat higher errors. The relative ranking stays mostly the same, except that the MC method is slightly better than the Embeddings-RF method with respect to RMSE, but not MAE. So, the same effect regarding embeddings and training data can be observed, but to a lesser extent. This can probably be attributed to the fact that there are both more customers and the sequences span a longer period of time. Thus mitigating the data sparsity problem slightly.</p><p>While in general a moving time window evaluation is a better way for assessing performance, we need to stress that the data is getting very small in our case, due to a lack of historical records, especially for the UKRetail dataset. For this reason, the last time fold probably provides the best estimate for generalization performance with sufficient data (summarized in the previous Table <ref type="table" target="#tab_4">5</ref>).</p><p>Improved CLV Prediction With Seq-To-Seq Learning and Feature-Based Models 80:27</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">The Effect of Temporal Aspects on Model Performance</head><p>In this section, we analyze the dependency of the model performance on temporal aspects. Specifically, we vary both the temporal granularity of the future time window as well as the prediction horizons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.4.1</head><p>Varying the Temporal Granularity of the Future Window. We first investigate the effects of varying the temporal granularity of the future time window, i.e, the prediction period. As outlined in the introduction, the S2S method, by default, generates predictions for individual time steps in the future (days for the UKRetail dataset and weeks for the larger Children dataset, due to memory constraints). The CLV is then estimated by summing over all individual predictions in the future period. In contrast, the other methods-including our GBM method-predict the sum in the future period directly. The motivation for that is the fact that the S2S method is sequential. The only exception is the ARMA time series method. But in this case, the granularity in the output must be the same as in the input (e.g., a sequence of days). However, for the S2S method, it is possible to choose a different granularity in the output by modeling the aggregated output series. E.g., the highest granularity corresponds to the case when there is an input sequence on a day or week level, while the output sequence consists only of one element representing the sum in the whole future time period. This is the same case as for the other methods, which predict the whole future time period directly.</p><p>Note that since we only change the output aggregation, the time series based patterns in the input data are still preserved. E.g., the model can learn from periodic patterns like purchases every other week. Just instead of predicting those days or weeks separately, the model learns the sum over a longer time frame, the higher periodic values being implicitly included.</p><p>For the UKRetail dataset, we tested an output granularity of daily, weekly, bi-weekly and fourweekly data, the latter corresponding to the entire prediction horizon of 4 weeks. For the Children dataset, we used an output granularity of weekly, four-weekly, 13-weekly and 26-weekly data, the latter value again corresponding to the entire prediction horizon. However, for both datasets, we found that there is only little difference in the performance for varying granularities of the future time window. The deviation of RMSE values is below 0.2 for the UKRetail and below 0.15 for the Children dataset. This also shows that the different choice of future granularities for the S2S method did not have much of an effect on the outcome. These findings indicate that both low and high output granularities can be used. Nevertheless, predicting on the lowest granularity can provide benefits in practice. No information is lost and this way one can also predict how much profit will be made on which days or weeks, unlike the whole time period only. For example, this would enable better planning for marketing activities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Changing the Prediction</head><p>Horizon. Next, we investigate the effect of the length of the forecasting period on accuracy. In general, accuracy can be expected to drop with increasing prediction horizons. This is also the case because our target is a sum of profit values in the future period, which tends to increase with longer periods. To confirm this intuition, we ran additional experiments in which we varied the prediction horizons. Since our objective in this article is the estimation of the lifetime value, we chose quite long prediction horizons in our main experiments with the Children dataset. Given in particular the very limited time range covered by the UKRetail dataset, we focused on experiments with shorter prediction horizons (4 weeks for the UKRetail dataset, while the horizon is 6 months for the Children dataset). The time range restriction in both datasets strongly limits any further increase in the prediction range, since a minimum amount of data is needed for training. We therefore also analyzed the performance for shorter prediction horizons. The results are given in Tables <ref type="table" target="#tab_6">7</ref> and<ref type="table" target="#tab_7">8</ref> for the UKRetail and the Children dataset, respectively. We can observe that the errors increase for increasing prediction horizons. At the same time, the relative performance of the methods stays mostly the same, including our methods. It is notable that for the lowest prediction range (e.g., 2 weeks for the UKRetail dataset), the differences between the methods are smaller. This could be explained by the fact that in a shorter time, fewer repurchases happen, which are less predictable. The result for the longest prediction range, in particular for the UKRetail dataset, could be an indication that the S2S method produces more stable predictions for longer time ranges. For the Children dataset, this difference is however less pronounced. Evaluating this for longer prediction horizons is a topic for future work, provided that suitable datasets can be obtained.  The relative performance values are normalized between 0 (lowest performance -the baseline) and 1 (highest performance -the full model). Note that smaller values of the relative performance for an ablation variant suggest that the removed component is more important.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Ablation Analysis</head><p>Our results reported so far showed that stacking the GBM and S2S models is advantageous, and they give insights about the performance of each model individually. In order to better understand how much each component of our sequence-based method contributes to the overall performance, we conducted an ablation analysis. In this analysis, we tested different variants of our model, in each of which one component was removed. The following variants were tested:</p><p>? Without embeddings is a model without the customer embeddings described in Section 4.2.</p><p>? In the simplified architecture, only one GRU encoder layer is used which is directly passed to the decoder without the further layers in between. Also, no temporal convolutions are used. ? The only basic features variant used a strongly reduced set of features. Specifically, it is based on recency, frequency, and monetary value, as well as the customer embeddings (which were removed in the first ablation variant).</p><p>The average results over both datasets are shown in Table <ref type="table" target="#tab_8">9</ref>. To emphasize on the relative gains obtained by the different components, we used the results obtained by the best-performing baseline (Embeddings-RF) as a reference point, indicated by a zero in the column "Relative Performance" in Table <ref type="table" target="#tab_8">9</ref>. The full model correspondingly has a performance value of 1.</p><p>From Table <ref type="table" target="#tab_8">9</ref>, we can see that each of the components contributes in a rather balanced way to the overall performance. The additional features have the highest impact and the model architecture is the second most important factor. <ref type="foot" target="#foot_14">13</ref> The embedding part has a smaller effect, i.e., it only leads to a smaller performance decrease when omitted. Note, however, that in general there are interactions between all components. For example, there is a synergy between the features and a more advanced model, since the latter is capable of better learning interactions between those features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Feature Importance and Interpretability Analysis</head><p>In order to better understand, which features are the most important ones when predicting the CLV, we made some additional analyses. The results of this feature importance analysis for the Children dataset are summarized in Table <ref type="table" target="#tab_9">10</ref>. We show the top 20 out of 719 features. The importance values are based on the information gain measure, which reflects how much each feature contributes to the overall model across all trees. The traditional features used in RFM models, i.e., the total profit in the past (M), the number of orders (F) and the days since the last order (R), turn out to be among the most important features. In addition to the profit, which is the target of our interest, the past revenue is an important predictor. This can be explained by the fact that profit and revenue are sometimes anticorrelated, with some products being sold with a negative profit margin. Product diversity features are also relatively important (having ranks 7, 12, and 13), which is consistent with existing literature <ref type="bibr" target="#b9">[10]</ref>.</p><p>We further observe that several others of the most relevant features are based on sparse item information, in particular, the number of orders for certain subcategories and brands. For example, a high number of past orders of certain consumables such as diapers is a strong indication that the customer will order these again.</p><p>Another important class of features are those based on statistics of the past purchase frequency. In particular, these are the features medianDaysSinceOrder and meanDaysSinceOrder, which are based on the number of days since the last purchase for each purchase event that happened in the past. Furthermore, the feature meanGapBetweenOrders describes the mean number of days between past orders. Similar features are also included in the list of features with a slightly lower information gain. These features, which capture some of the purchase timing patterns of customers, were a main inspiration for using our sequence-to-sequence RNN.</p><p>Our results also show that various other statistics such as the minimum, maximum, and mean profit per order are helpful. This is due to the fact that they explain typical deviations from the traditional summaries of total profit and number of orders. Providing such ranges as input is useful for dealing with uncertainty.</p><p>Finally, the features maxRelativeNumberOfRepurchases and maxRelativeNumberOf CustReordItem are also intuitive. As described in Section 4.1, they are based on the maximum repurchase probability of all the items a customer has bought. Generally, note that sometimes features with a comparably low gain value can actually still be quite helpful for the model. This is, for example, the case for features related to the ages of the children the customers have. They are not necessarily among the top features, because their values are missing for a large fraction of customers. Therefore, the GBM cannot perform splits on them for the majority of training cases, resulting in a lower gain. However, when this information is available, these features are helpful predictors.</p><p>At this point, note that in the context of interpretable ML alternative approaches exist to analyze how individual features affect the model outcomes. Our combined model is fairly complex and not fully interpretable, especially the S2S part. For this reason, it may be helpful in practice to resort to other methods that aim to provide interpretability for black-box models. In <ref type="bibr" target="#b29">[30]</ref>, the authors provide an extensive overview and taxonomy of such approaches. For instance, according to the classification of methods and metrics in <ref type="bibr" target="#b29">[30]</ref>, the data-driven and local SHAP method <ref type="bibr" target="#b31">[32]</ref> should be particularly suitable for our problem. Specifically, this method can yield a more accurate generalization of feature importance values, which can also be applied to particular prediction examples (in contrast to the entire model).</p><p>Given our combined model, we could in principle investigate the first few decision trees generated by the GBM model, because these individual trees are interpretable. However, this would only lead to a rough approximation of the full model. At the same time, RNNs lack a similar possibility, which is why the application of more advanced methods as described in <ref type="bibr" target="#b29">[30]</ref> seems to be more helpful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS AND FUTURE WORK</head><p>We will now summarize our approach and findings, provide ideas for new features, discuss its applicability to different domains and address promising technical extensions of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Summary of Our Method</head><p>We have proposed a novel method for predicting the CLV by means of a customized sequence-tosequence RNN and GBMs. A key aspect of our approach lies in capturing the time series nature of the problem, and we have demonstrated the benefits of our method based on two real-world datasets. In practice, applying our method enables an accurate detection of the most valuable customers, which in turn can be treated in a special manner by customer relationship approaches, therefore, increasing profit and making companies competitive by strengthening their base of loyal customers.</p><p>Our modeling approach is based on a set of features that can be derived from basic information that is commonly available in practice. Furthermore, we have seen that the consideration of domain-specific aspects can be beneficial to achieve even higher prediction performance. While including domain-specific information requires some additional feature engineering, these features can be easily integrated into our pipeline by making use of the provided templates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Additional Promising Features for CLV Prediction</head><p>While we have already introduced a number of new features to the CLV prediction problem, we identified several additional features, which we-based on our experience in related applicationsconsider helpful for the CLV prediction problem. These types of data were, however, not available in our datasets.</p><p>? Out-of-Stock or Unavailability Information of Items a customer preferred in the past or might prefer in the future: When we know that some of a customer's favorite items are not available, we can expect that the amount of money spent might be lower.</p><p>? Evolution of Prices and Competitor Prices, especially for higher priced products a customer prefers: Such developments can strongly affect a customers' willingness to buy, depending on whether price differences to the competition are low or high. ? Bank Holidays and indicators for times before and after holidays, special seasons, and global trends: Holidays and other time-limited phenomena can affect the mean profit over all customers. Furthermore, major business decisions such as the reduction of shipping times can change the overall mean. ? Marketing Promotion Information: Knowledge about ongoing and future marketing activities often affect sales directly. Incorporating such information in the CLV prediction model seems promising. ? Clickstream Data With Behavioral Information from the website and external sources: These types of fine-grained information-including customer feedback, discounts used, returned products-can be additional predictors in future models.</p><p>Moreover, it appears promising to incorporate features from other works, especially from <ref type="bibr" target="#b18">[19]</ref>, which derives complex features based on advanced mining processes. In particular, in <ref type="bibr" target="#b18">[19]</ref>, the authors investigate the relationship between systematic behavior of customers and their profitability and predictability. By using frequent itemset mining and clustering in a new framework, information entropy based measures are defined both on baskets and spatio-temporal dimensions. For example, systematic customers with repeating behavior can be detected this way. These calculated measures for customers could then be incorporated as additional features. Such features can be expected to be valuable, since systematic behavior can be highly predictive for the future profit of customers. We therefore consider this as promising future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Applicability of Our Method to Different Data Sources and Domains</head><p>The features used in our model are mostly based on customer and sales data, which usually can be easily obtained from a company's CRM or Enterprise Resource Planning (ERP) system. Apart from CLV prediction, our framework could be used for similar problems, like forecasting the future number of purchases.</p><p>Our general methodology can also be based on other types of data in a more or less straightforward manner. For instance, if we use page views instead of purchases, similar features can be generated from clickstream data. Instead of measures such as profit and revenue, one can use the duration of sessions on the website or the number of unique page views instead of the number of unique items bought. Furthermore, the number of site revisits can be used instead of repurchases, etc.</p><p>Our approach can also be used outside e-commerce, for example, for online streaming services (e.g., for music or movies). While usage times can be a substitute for profit in this case, it is still meaningful to construct additional domain-specific features. Examples could be aggregate statistics of the browsing histories and rating behavior, in case such a functionality is available on the website. These statistics can be added in the feature engineering component of our method.</p><p>Another example from a different domain than traditional e-commerce would be the scenario of a telecommunication provider. In this case, products can be contracts with flexible options, e.g., purchasing of additional traffic volumes. Some item specific features are still relevant in this case, like the number of customers who repurchased a given product. Other features may include statistics based on tariff switching, the usage of optional services like extra traffic volume or phone minutes, as well as geolocation information. The difference in such a scenario is that in case of contracts, a part of the profit is already known in the future because of fixed contract times, which reduces the variability of the CLV compared to other scenarios. Also unlike in e-commerce, it is also explicit in the data when a customer ended his or her relationship with the company. This would therefore represent an important feature to include, i.e., whether customers terminated their contracts or sent their notice of cancellation for the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Future Methodological Extensions</head><p>One part of our future work comprises the refinement and extension of our RNN architecture. This includes deeper architectures with various ways of deepness and information exchange between layers. One further promising approach lies in the incorporation of attention mechanisms <ref type="bibr" target="#b38">[39]</ref>. The principle of attention is based on summarizing past information in "context vectors", which are read and updated using a smoothed attention distribution (cf. Neural Turing Machines <ref type="bibr" target="#b17">[18]</ref>). Moreover, an augmented RNN can be used for a given RNN that helps to decide which part of the past we should focus on (Attentional Interfaces <ref type="bibr" target="#b1">[2]</ref>). For instance, using attention one can directly use information from previous GRU layers instead of passing their compressed weights as initial state to the next layer.</p><p>Attention models have been successfully applied in different domains, including in particular NLP applications. However, the value of using such attention layers in our application domain still has to be explored. In our case, we use temporal convolutions and a rich set of features and this approach might already include part of the information that attention might capture. In general, there are also only few works that deal with time series and the benefit of using attention layers in our problem setting is therefore not yet fully clear.</p><p>A second direction for future research lies in the use of neural architecture search <ref type="bibr" target="#b40">[41]</ref>, which we consider a promising approach for developing refinements of our proposed RNN model without having to design and tune such a network manually. For this purpose however-in addition to substantial computing resources-more data are needed, which is currently not available for the CLV domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A TECHNICAL APPENDIX</head><p>In this appendix, we provide some mathematical details about our encoder-decoder sequence-tosequence RNN model presented in Section 4.4 and how its individual components interact. For the simplification of the presentation, we assume that all the feature engineering, the causal temporal convolutions and the pretraining of the embeddings has been performed already, and all features are summarized in a tensor X . Moreover, we omit the details of dropout, but present the model in case without dropout and recurrent dropout.</p><p>Let us denote the sequence of profit values by (y t ) t ?N 0 , which is the sequence of the target we want to predict for a future time period. If we want to estimate the CLV for the period between T + 1 and T + L, its value is given by</p><formula xml:id="formula_4">CLV L = T +L t =T +1 y t .<label>(5)</label></formula><p>Formally, the encoder-decoder sequence-to-sequence model aims to learn the distribution of the sequence of future profit values. That is, P (y t ) t ? {T +1, ...,T +L } | X :,1:T ,: ,</p><p>where X :,1:T ,: denotes the abstract input tensor for all customer data captured by all our F features up to time T , describing the training data distribution. I.e., X consists of the dimensions (data realizations, times, features).</p><p>In the following, we will use superscripts to indicate the depth of objects in the layer hierarchy shown on the bottom of Figure <ref type="figure" target="#fig_7">8</ref>. Like before, let (x t , h t -1 ) denote the concatenation of the current time series input and the previous hidden state vector. According to the GRU equations (see Section 4.4 and the references therein), these are passed to NNs with weight matrices W and biases b with corresponding subscripts, leading to the following equations <ref type="foot" target="#foot_15">14</ref> describing the first encoder GRU:</p><p>z (1)  t = ? W (1)  z ? x t , h (1)  t -1 + b (1)   z ,</p><p>r (1)  t = ? W (1)  r ? x t , h (1)  t -1 + b (1)   r ,</p><p>h (1)  t = tanh W (1)  h ? x t , r (1)   t h (1)  t -1 + b (1)   h ,</p><p>h (1)  t = 1z (1)   t h (1)  t -1 + z (1)   t h (1)  t .</p><p>After the computation for all input t (corresponding to the chosen input sequence length, which is a hyperparameter), the final hidden state of the first encoder GRU is then passed to a dense neural network: h (2) = tanh W (2)  d ? h (1)  T + b (2)   d .</p><p>Note that the dimension is given by W (2)  d ? R d (3) ?d (1) , where d (1) is the number of hidden units of the first encoder GRU layer and d (3) is the number of hidden units of the second encoder GRU layer.</p><p>The output of this network is in turn used to compute the initial state of the second encoder GRU: h (3)  0 := h (2) .</p><p>Afterwards, the second encoder GRU is updated in the same way:</p><formula xml:id="formula_12">z (3) t = ? W (3) z ? x t , h (3) t -1 + b (3) z ,<label>(13)</label></formula><formula xml:id="formula_13">r (3) t = ? W (3) r ? x t , h (3) t -1 + b (3) r ,<label>(14)</label></formula><formula xml:id="formula_14">h (3) t = tanh W (3) h ? x t , r (3) t h (3) t -1 + b (3) h ,<label>(15)</label></formula><formula xml:id="formula_15">h (3) t = 1 -z (3) t h (3) t -1 + z (3) t h (3) t .<label>(16)</label></formula><p>A second dense layer is then applied to the final hidden state of the second encoder GRU layer:</p><formula xml:id="formula_16">h (4) = tanh W (4) d ? h (3) T + b (4) d ,<label>(17)</label></formula><p>where W (4)  d ? R d (5) ?d (3) , with d (5) being the hidden dimension of the second encoder GRU layer. The output of this network is in turn used to compute the initial state of the decoder GRU: h (5)  0 := h (4) .</p><p>While the second encoder GRU can be seen as a refinement for the first encoder GRU, aiming to correct its mistakes, the purpose of the decoder GRU is to match the output size, which is accomplished by slicing a subset of times. So, while the fundamental GRU update equations remain the same, the time subsets as well as the output processing differ. In particular, a time distributed dense layer operation is used. z (5)  t = ? W (5)  z ? x t , h (5)  t -1 + b (5)   z ,</p><p>r (5)  t = ? W (5)  r ? x t , h (5)  t -1 + b (5)   r ,</p><p>h (5)  t = tanh W (5)  h ? x t , r (5)   t h (5)  t -1 + b (5)   h ,</p><p>h (5)  t = 1z (5)   t h (5)  t -1 + z (5)   t h (5)  t ,</p><p>And for all t = T + 1, . . . ,T + L compute: y (6)  t = W (6)  d,t ? h (5)  t + b (6)  d,t ,</p><p>where W (6)  d,t is a (usually small) weight matrix for a dense layer with linear activation, dependent on t. This is the case because the dense transformation is applied to every time step t separately, which is useful to learn specific patterns for individual time steps.</p><p>Every output y (6)  t is then compared to the actual value y t by means of a loss function L, i.e., the average of all L( y (6)  t , y t ) is considered. As described in the main text in Section 3.2, a sliding window approach is used for training to shift the value of T backward, where the maximum time T is chosen such that T + L is still available in the training data (to know actual outcomes to learn from).</p><p>In this way, the abstract time series input vectors x t are fed with actual data. That is, imagine that we construct the entire tensor of possible inputs X ? R N ?T input ?F , where N is the number of data points, T input the temporal input sequence length and F the number of features, we iteratively set (with an example of some of our features)</p><p>x t := X n,t,:</p><p>(</p><p>= PastProfit n,t , NumberOfOrders n,t , TimeSinceLastOrder n,t ,</p><p>TimeSinceFirstOrder n,t , NumberOfItems n,t , ChildAge n,t , MaxReorderRate n,t , . . .</p><p>for some training data row n ? {1, . . . , N } and given time step t, where : denotes the tensor slice consisting of all features. Each n corresponds to a particular customer and time in the past, as constructed by the sliding window approach. And t reflects the time step relative to such an input sequence. Note, however, that the full tensor X does not need to be constructed explicitly, it is sufficient to only generate batches of data for training.</p><p>The training is then performed with the common backpropagation through time algorithm. I.e., automatic differentiation is applied to the temporally unfolded neural network to obtain all gradients. These can then be used for learning by an optimization variant of stochastic gradient descent (e.g., the Adam optimizer).</p><p>Finally, after the training has been performed in the described way, the predicted CLV for the period from T + 1 to T + L is then calculated by CLV L = T +L t =T +1 y (6)  t ,</p><p>which is in turn compared with the actual value CLV L defined above, by taking the average over some loss, like MSE or MAE.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example for sequential features of Customer A.</figDesc><graphic url="image-1.png" coords="6,50.77,82.87,384.00,153.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Example for sequential features of Customer B.</figDesc><graphic url="image-2.png" coords="7,54.44,83.54,376.56,85.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Sliding window approach for training data generation for the sequence-to-sequence RNNs, shown for Customer A. T now represents the current time. The training data are generated by successively shifting the window backward in time. For every shift, the data points taken into consideration are shown in dark shaded color. The last four time steps are always the prediction target in this case.</figDesc><graphic url="image-3.png" coords="9,62.95,83.55,360.00,241.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.4. Temporal aggregation with time-dependent features shown for Customer A, which is done for all customers and stacked together row-wise. This is suitable to learn a GBM model and is also used in time series form in our S2S model.</figDesc><graphic url="image-4.png" coords="10,140.27,83.11,204.72,126.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. High-Level Overview.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>) In the sequence-to-sequence RNN model, we use an additional embedding layer Improved CLV Prediction With Seq-To-Seq Learning and Feature-Based Models 80:15</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Network flow diagram for GRUs with annotations. Dark grey boxes represent separate NN layers. The remaining blocks stand for operations (as stated below). The arrows show the directions of the information flow and processing steps. The layout of this diagram has been adapted from the graphics in http://colah. github.io/posts/2015-08-Understanding-LSTMs/.</figDesc><graphic url="image-7.png" coords="15,139.45,83.75,207.36,153.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Overview flow diagram of our sequence-to-sequence RNN architecture.</figDesc><graphic url="image-8.png" coords="16,120.77,82.94,243.72,335.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>19 Fig. 9 .</head><label>199</label><figDesc>Fig.9. Illustration of temporal convolutions on an example profit sequence. Note that temporal convolutions are not only applied to profit data, but a much richer set of features described in Section 4.1 to improve performance.</figDesc><graphic url="image-9.png" coords="19,76.45,83.26,333.36,97.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Algorithm 1 : 3 Create 4 for 6 ( 7 (</head><label>13467</label><figDesc>Stepwise S2S-Algorithm 1 Load the customer embeddings trained according to Section 4.2 and randomly initialize the remaining weights of the S2S RNN network described in Section 4.4. 2 do K disjoint subsets of customers C k and load the current weights of the S2S RNN network. Every k ? {1, . . . , K } do 5 (i) Generate the sequential training data for all customers in C k by the approach outlined in Section 3.2.2, ii) train the model with this subset of data according to the steps given in Section 4.4 and iii) store the corresponding updated weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>8 end 9 while</head><label>89</label><figDesc>Minimum validation error improved over the past n tol steps;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Improved CLV Prediction With Seq-To-Seq Learning and Feature-Based Models 80:29</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Features Based on Customer Attributes</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Order-related Features</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Interaction-Related Features</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>UKRetail and Children Dataset Statistics Before and After Denoising</figDesc><table><row><cell></cell><cell cols="2">Full Dataset After Denoising</cell></row><row><cell>UKRetail dataset</cell><cell></cell><cell></cell></row><row><cell cols="2">Number of customers 4,373</cell><cell>3,284</cell></row><row><cell>Number of records</cell><cell>541,909</cell><cell>144,863</cell></row><row><cell>Children dataset</cell><cell></cell><cell></cell></row><row><cell cols="2">Number of customers 1,081,443</cell><cell>1,073,872</cell></row><row><cell>Number of records</cell><cell>10,596,431</cell><cell>10,196,814</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Experimental Results for the UKRetail and the Children Dataset</figDesc><table><row><cell>Method</cell><cell>UKRetail RMSE</cell><cell>UKRetail MAE</cell><cell>Children RMSE</cell><cell>Children MAE</cell></row><row><cell>Adjusted Customer Mean Baseline</cell><cell>72.6580</cell><cell>49.7365</cell><cell>21.0767</cell><cell>11.9316</cell></row><row><cell>(ACMB) and ARMA 12</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BG/NBD [15]</cell><cell>69.1706</cell><cell>41.5405</cell><cell>20.1382</cell><cell>8.0887</cell></row><row><cell>Markov Chain (MC) [35]</cell><cell>68.8441</cell><cell>45.1282</cell><cell>18.0898</cell><cell>7.6918</cell></row><row><cell>Embeddings-RF [10]</cell><cell>68.8211</cell><cell>39.4804</cell><cell>17.4837</cell><cell>6.8601</cell></row><row><cell>Our GBM method (GBM)</cell><cell>66.2567</cell><cell>33.7816</cell><cell>12.5034</cell><cell>3.4230</cell></row><row><cell>Our Sequence-To-Sequence RNN</cell><cell>65.3565</cell><cell>34.5502</cell><cell>12.1621</cell><cell>3.9071</cell></row><row><cell>(S2S)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Stacking of our GBM and</cell><cell>64.9491</cell><cell>30.7953</cell><cell>11.6473</cell><cell>3.1366</cell></row><row><cell>Sequence-To-Sequence RNN</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(GBM-S2S)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Moving Time Window Validation Results for the UKRetail and the Children Dataset</figDesc><table><row><cell>Method</cell><cell>UKRetail RMSE</cell><cell>UKRetail MAE</cell><cell>Children RMSE</cell><cell>Children MAE</cell></row><row><cell>Adjusted Customer Mean Baseline</cell><cell>79.0620</cell><cell>54.4851</cell><cell>23.5596</cell><cell>12.2247</cell></row><row><cell>(ACMB) and ARMA</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BG/NBD [15]</cell><cell>72.8156</cell><cell>53.9270</cell><cell>22.3143</cell><cell>11.3459</cell></row><row><cell>Markov Chain (MC) [35]</cell><cell>73.4645</cell><cell>45.9742</cell><cell>19.8856</cell><cell>8.2331</cell></row><row><cell>Embeddings-RF [10]</cell><cell>73.8194</cell><cell>49.6098</cell><cell>20.2490</cell><cell>7.6505</cell></row><row><cell>Our GBM method (GBM)</cell><cell>71.0029</cell><cell>35.6562</cell><cell>15.3135</cell><cell>4.4218</cell></row><row><cell>Our Sequence-To-Sequence RNN</cell><cell>69.8316</cell><cell>36.4251</cell><cell>14.7842</cell><cell>4.8876</cell></row><row><cell>(S2S)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Stacking of our GBM and</cell><cell>69.3053</cell><cell>31.8793</cell><cell>13.9548</cell><cell>3.9471</cell></row><row><cell>Sequence-To-Sequence RNN</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(GBM-S2S)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Results (RMSE First/MAE Second Row) for Different Prediction Horizons for the UKRetail Dataset</figDesc><table><row><cell>Prediction Horizon</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>Results (RMSE First/MAE Second Row) for Different Prediction Horizons for the Children Dataset</figDesc><table><row><cell>Prediction Horizon</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 .</head><label>9</label><figDesc>Average Performance Relative to the Embeddings-RF Baseline and the Full Model When Some Components are Omitted</figDesc><table><row><cell>Ablation Variant</cell><cell>Relative Performance</cell></row><row><cell>Full model</cell><cell>1.0000</cell></row><row><cell>Model without embeddings</cell><cell>0.8368</cell></row><row><cell>Model with simplified architecture</cell><cell>0.6056</cell></row><row><cell>Model with only basic features</cell><cell>0.4218</cell></row><row><cell>Embeddings-RF baseline</cell><cell>0.0000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 .</head><label>10</label><figDesc>Feature Importance Values of the Top 20 Features From Our Experiments</figDesc><table><row><cell>Rank</cell><cell>Feature</cell><cell>Gain</cell></row><row><cell>1</cell><cell>Totalprofit</cell><cell>0.1847</cell></row><row><cell>2</cell><cell>Numberoforders</cell><cell>0.1498</cell></row><row><cell>3</cell><cell>Totalrevenue</cell><cell>0.1078</cell></row><row><cell>4</cell><cell>Brandnumberoforderindicator.1242</cell><cell>0.0673</cell></row><row><cell>5</cell><cell>Dayssincelastorder</cell><cell>0.0491</cell></row><row><cell>6</cell><cell>Mediandayssinceorder</cell><cell>0.0412</cell></row><row><cell>7</cell><cell>Numberofuniquesubcategories</cell><cell>0.0335</cell></row><row><cell>8</cell><cell>Brandnumberoforderindicator.1104</cell><cell>0.0228</cell></row><row><cell>9</cell><cell>Subcategorynumberoforderindicator.155</cell><cell>0.0184</cell></row><row><cell>10</cell><cell>Meandayssinceorder</cell><cell>0.0154</cell></row><row><cell>11</cell><cell>Minorderprofit</cell><cell>0.0130</cell></row><row><cell>12</cell><cell>Numberofuniquebrands</cell><cell>0.0128</cell></row><row><cell>13</cell><cell>Numberofuniqueitems</cell><cell>0.0114</cell></row><row><cell>14</cell><cell>Subcategorynumberoforderindicator.16</cell><cell>0.0104</cell></row><row><cell>15</cell><cell>Maxorderprofit</cell><cell>0.0103</cell></row><row><cell>16</cell><cell>Maxrelativenumberofrepurchases</cell><cell>0.0095</cell></row><row><cell>17</cell><cell>Meangapbetweenorders</cell><cell>0.0085</cell></row><row><cell>18</cell><cell>Subcategorynumberoforderindicator.3</cell><cell>0.0083</cell></row><row><cell>19</cell><cell>Maxrelativenumberofcustreorditem</cell><cell>0.0082</cell></row><row><cell>20</cell><cell>Meanorderprofit</cell><cell>0.0078</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>According to<ref type="bibr" target="#b4">[5]</ref>, the CLV represents the "net present value of the profits linked to a specific customer once the customer has been acquired, after subtracting incremental costs associated with marketing, selling, production, and servicing over the customer's lifetime."</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We consider churn prediction as a simpler problem, because we are only interested in a binary outcome instead of a numerical estimate. ACM Transactions on Knowledge Discovery from Data, Vol. 15, No. 5, Article 80. Publication date: April 2021.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>A comprehensive survey of traditional methods can be found in<ref type="bibr" target="#b20">[21]</ref>.ACM Transactions on Knowledge Discovery from Data, Vol. 15, No. 5, Article 80. Publication date: April 2021.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p>ACM Transactions on Knowledge Discovery from Data, Vol. 15, No. 5, Article 80. Publication date: April 2021.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>Note that some of the features were not available for the online retail dataset, as we will describe in Section</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_5"><p>5.1. ACM Transactions on Knowledge Discovery from Data, Vol. 15, No. 5, Article 80. Publication date: April 2021.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_6"><p>We used the xgboost implementation<ref type="bibr" target="#b11">[12]</ref> of GBMs in our experiments. ACM Transactions on Knowledge Discovery from Data, Vol. 15, No. 5, Article 80. Publication date: April 2021.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_7"><p>See<ref type="bibr" target="#b16">[17]</ref> for an introduction of the basic concepts of RNNs.ACM Transactions on Knowledge Discovery from Data, Vol. 15, No. 5, Article 80. Publication date: April 2021.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_8"><p>https://keras.io.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_9"><p>https://www.tensorflow.org. ACM Transactions on Knowledge Discovery from Data, Vol. 15, No. 5, Article 80. Publication date: April 2021.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_10"><p>By a batch, we mean the data that is used for a mini-batch gradient descent update used in the learning of the sequenceto-sequence model; for stochastic gradient descent this would be just one sample, but in order to make use of GPU parallelization it is beneficial to increase this size. ACM Transactions on Knowledge Discovery from Data, Vol. 15, No. 5, Article 80. Publication date: April 2021.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_11"><p>The source code used in the experiments can be found at https://bitbucket.org/Josef-Bauer/clv_pred/src/master/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_12"><p>http://archive.ics.uci.edu/ml/datasets/online+retail. ACM Transactions on Knowledge Discovery from Data, Vol. 15, No. 5, Article 80. Publication date: April 2021.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_13"><p>Note that the ACMB can be considered a special case of an ARMA time series model, see the discussion in Section 5.3.3. ACM Transactions on Knowledge Discovery from Data, Vol. 15, No. 5, Article 80. Publication date: April 2021.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_14"><p>Note that the relative performance values are inversely correlated with the impact of a component, since the performance is reported with this component removed. ACM Transactions on Knowledge Discovery from Data, Vol. 15, No. 5, Article 80. Publication date: April 2021.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_15"><p>As before, denotes the Hadamard (element-wise) product and all functions (sigmoid and tanh) are also applied elementwise.ACM Transactions on Knowledge Discovery from Data, Vol</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_16"><p>. 15, No. 5, Article 80. Publication date: April 2021.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sentiment analysis of Bengali comments with Word2Vec and sentiment information of words</title>
		<author>
			<persName><forename type="first">Md</forename><surname>Al-Amin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Saiful Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shapan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uzzal</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on Electrical, Computer and Communication Engineering (ECCE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="186" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Competing Risks And Multistate Models With R</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Beyersmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Allignol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Schumacher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Data mining using RFM analysis</title>
		<author>
			<persName><forename type="first">Derya</forename><surname>Birant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Knowledge-Oriented Applications in Data Mining</title>
		<imprint>
			<publisher>InTech</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Blattberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byung-Do</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">A</forename><surname>Neslin</surname></persName>
		</author>
		<title level="m">Database Marketing -Analyzing and Managing Customers</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Customer lifetime value measurement</title>
		<author>
			<persName><forename type="first">Sharad</forename><surname>Borle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Siddharth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipak</forename><forename type="middle">C</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="100" to="112" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stochastic gradient descent tricks</title>
		<author>
			<persName><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="421" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Brockwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Davis</surname></persName>
		</author>
		<title level="m">Introduction to Time Series and Forecasting</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>3 ed.</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Optimal selection for direct mail</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Roelf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bult</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Wansbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="378" to="394" />
			<date type="published" when="1995">1995. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Customer lifetime value prediction using embeddings</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Paul Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelo</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Pagliari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deisenroth</forename><surname>Peter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1753" to="1762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Data mining for the online retail industry: A case study of RFM model-based customer segmentation using data mining</title>
		<author>
			<persName><forename type="first">Daqing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Laing Sain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Database Marketing &amp; Customer Strategy Management</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="197" to="208" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">XGBoost: A scalable tree boosting system</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD&apos;16)</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD&apos;16)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Counting your customers&quot; the easy way: An alternative to the Pareto/NBD model</title>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">S</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ka</forename><forename type="middle">Lok</forename><surname>Hardie</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Science</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="275" to="284" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">RFM and CLV: Using iso-value curves for customer base analysis</title>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">S</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ka</forename><forename type="middle">Lok</forename><surname>Hardie</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Marketing Research</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="415" to="430" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Deep Learning</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2016">2016</date>
			<publisher>MIT Press Cambridge</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Behavioral entropy and profitability in retail</title>
		<author>
			<persName><forename type="first">Riccardo</forename><surname>Guidotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Coscia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dino</forename><surname>Pedreschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Pennacchioli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Data Science and Advanced Analytics (DSAA)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Personalized market basket prediction with temporal annotated recurring sequences</title>
		<author>
			<persName><forename type="first">R</forename><surname>Guidotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rossetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pappalardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Giannotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pedreschi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="2151" to="2163" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Modeling customer lifetime value</title>
		<author>
			<persName><forename type="first">Sunil</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominique</forename><surname>Hanssens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Hardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wiliam</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathaniel</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nalini</forename><surname>Ravishanker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sriram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Service Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="139" to="155" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Customer metrics and their impact on financial performance</title>
		<author>
			<persName><forename type="first">Sunil</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valarie</forename><surname>Zeithaml</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Science</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="718" to="739" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">To model, or not to model: Forecasting for customer prioritization</title>
		<author>
			<persName><forename type="first">Chun-Yao</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="497" to="506" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Customer lifetime value research in marketing: A review and future directions</title>
		<author>
			<persName><forename type="first">Dipak</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><forename type="middle">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Interactive Marketing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="34" to="46" />
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Estimating customer lifetime value based on RFM analysis of customer purchase behavior: Case study</title>
		<author>
			<persName><forename type="first">Mahboubeh</forename><surname>Khajvand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiyana</forename><surname>Zolfaghar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Ashoori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somayeh</forename><surname>Alizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procedia Computer Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="57" to="63" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bayesian multivariate time series methods for empirical macroeconomics</title>
		<author>
			<persName><forename type="first">Gary</forename><surname>Koop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Korobilis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Econometrics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="267" to="358" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Werner</forename><surname>Reinartz</surname></persName>
		</author>
		<title level="m">Customer Relationship Management: Concept, Strategy, and Tools</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Prediction of multivariate time series by autoregressive model fitting</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">C</forename><surname>Reinsel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Multivariate Analysis</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="393" to="411" />
			<date type="published" when="1985">1985. 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A survey of data-driven and knowledge-aware eXplainable AI</title>
		<author>
			<persName><forename type="first">Xiao-Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caleb</forename><surname>Chen Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luyu</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanyuan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenjia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xun</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A multivariate time-series prediction model for cash-flow data</title>
		<author>
			<persName><forename type="first">Kenneth</forename><forename type="middle">S</forename><surname>Lorek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Lee</forename><surname>Willinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Accounting Review</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="81" to="102" />
			<date type="published" when="1996">1996. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A unified approach to interpreting model predictions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Su-In</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4765" to="4774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Statistical and machine learning forecasting methods: Concerns and ways forward</title>
		<author>
			<persName><forename type="first">Spyros</forename><surname>Makridakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Evangelos Spiliotis, and Vassilios Assimakopoulos</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Modeling customer relationships as Markov chains</title>
		<author>
			<persName><forename type="first">Phillip</forename><forename type="middle">E</forename><surname>Pfeifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Carraway</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Interactive Marketing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">43</biblScope>
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Counting your customers: Who-are they and what will they do next?</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">C</forename><surname>Schmittlein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><forename type="middle">G</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Colombo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="1987">1987. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An engagement-based customer lifetime value system for e-commerce</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Vanderveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Addhyan</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajesh</forename><surname>Parekh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="293" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08835</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Deep forest.</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Received February</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
	</analytic>
	<monogr>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016">2016. 2016. 2019. 2020</date>
		</imprint>
	</monogr>
	<note>revised November. accepted December 2020</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
