<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Directed Acyclic Transformer for Non-Autoregressive Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
							<email>&lt;f-huang18@mails.tsinghua.edu</email>
							<affiliation key="aff0">
								<orgName type="department">The CoAI group</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Institute for Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">State Key Lab of Intelligent Technology and Systems</orgName>
								<orgName type="department" key="dep3">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="department" key="dep4">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">ByteDance AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Institute for Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">State Key Lab of Intelligent Technology and Systems</orgName>
								<orgName type="department" key="dep3">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="department" key="dep4">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">ByteDance AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
							<email>&lt;aihuang@tsinghua.edu.cn&gt;</email>
							<affiliation key="aff0">
								<orgName type="department">The CoAI group</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Institute for Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">State Key Lab of Intelligent Technology and Systems</orgName>
								<orgName type="department" key="dep3">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="department" key="dep4">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Directed Acyclic Transformer for Non-Autoregressive Machine Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Non-autoregressive Transformers (NATs) significantly reduce the decoding latency by generating all tokens in parallel. However, such independent predictions prevent NATs from capturing the dependencies between the tokens for generating multiple possible translations. In this paper, we propose Directed Acyclic Transfomer (DA-Transformer), which represents the hidden states in a Directed Acyclic Graph (DAG), where each path of the DAG corresponds to a specific translation. The whole DAG simultaneously captures multiple translations and facilitates fast predictions in a non-autoregressive fashion. Experiments on the raw training data of WMT benchmark show that DA-Transformer substantially outperforms previous NATs by about 3 BLEU on average, which is the first NAT model that achieves competitive results with autoregressive Transformers without relying on knowledge distillation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Transformer has been the most popular architecture for sequence-to-sequence learning, especially for machine translation <ref type="bibr" target="#b50">(Vaswani et al., 2017)</ref>. Vanilla Transformer adopts the autoregressive approach for generation, which obtains strong results but is inefficient in inference due to its sequential decoding. To tackle the problem, Non-autoregressive Transformers (NATs, <ref type="bibr" target="#b14">Gu et al., 2018;</ref><ref type="bibr" target="#b15">Gu et al., 2019;</ref><ref type="bibr" target="#b31">Ma et al., 2019;</ref><ref type="bibr">Ding et al., 2021a;</ref><ref type="bibr" target="#b13">Gu &amp; Kong, 2021)</ref>   proposed, which significantly reduce the inference latency by predicting all tokens in parallel and achieve reasonably high performances in translation. Notably, an NAT-based system obtain the highest BLEU score in German to English translation of WMT21 <ref type="bibr" target="#b38">(Qian et al., 2021b;</ref><ref type="bibr" target="#b0">Akhbardeh et al., 2021)</ref>, even better than a line of Autoregressive Transformer (AT) systems.</p><p>However, current NATs severely suffer from the multimodality problem <ref type="bibr" target="#b14">(Gu et al., 2018)</ref> in both training and inference. <ref type="foot" target="#foot_0">1</ref> Intuitively, in training, as shown in Fig. <ref type="figure" target="#fig_1">1(a)</ref>, NAT models are trained to predict each token independently, where one position may have several possible tokens as labels from several different translation references. In such a case, an NAT model may learn to generate an implausible output mixing multiple translations. Additionally, in inference, the NAT still cannot sample fluent translations even if it captures multi-modal information in training. Since the NAT model generates all tokens simultaneously, no effective sampling approach can be used on top of it. In contrast, ATs do not have the same problem because of their left-to-right generation, where the multi-modality problem for a later position is not so severe since its prefix has been given.</p><p>Currently, the main solution to address the multi-modality problem is to reduce the data modalities by knowledge distillation (KD, <ref type="bibr" target="#b26">Kim &amp; Rush, 2016;</ref><ref type="bibr" target="#b14">Gu et al., 2018)</ref>, namely, replacing the original training targets with predicted sentences from an AT teacher. KD is simple yet effective, which always leads to significant BLEU improvements, e.g., about 8 BLEU points on WMT14 En-De for vanilla NATs.</p><p>However, we argue that current state-of-art NAT models heavily rely on KD, which has two crucial disadvantages. a) Training NAT models by distilling from AT makes the training process redundant. We need to train an AT model first and then regenerate the whole training data. Such complex pre-processing prevents NATs from being practically used. b) Generally, the student model in KD cannot outperform its teacher model with a large margin. In such a case, KD restricts NAT's performance by imposing an upper bound (not strict), which seriously hurts the potential of further developing NAT models.</p><p>In this paper, we propose Directed Acyclic Transformer (DA-Transformer) for Non-Autoregressive Machine Translation, which directly captures many translation modalities via a proposed Directed Acyclic Decoder, instead of indirectly reducing modalities by KD. Specifically, different from decoders of ATs or vanilla NATs, our proposed decoder organizes the hidden states as a Directed Acyclic Graph (DAG) rather than a sequence. As shown in Fig. <ref type="figure" target="#fig_1">1</ref>(b), the DAG has multiple paths, each of which corresponds to a specific sentence. <ref type="foot" target="#foot_1">2</ref> In training, the DAG structure enables DA-Transformer to capture multiple translation modalities simultaneously, which avoids the inconsistent labels in vanilla NAT training. In inference, it can generate sentences along predicted paths, which not only avoids incorrect outputs mixing multiple translations but also enables the generation of diverse translations by sampling different paths.</p><p>Notice that DA-Transformer predicts all translation words in parallel, and the whole model is trained in an end-to-end fashion, which enjoys all merits of NAT models. We propose an objective that does not require multiple references in training, making it applicable to most translation benchmarks. In inference, we propose several sampling methods to decode a translation from DA-Transformer, which provides flexible quality-latency tradeoff in generation.</p><p>Experimental results show that DA-Transformer significantly reduces the gap between NATs and ATs while preserving the inference latency (7x ∼ 14x speedup over ATs).</p><p>Especially on WMT17 Zh-En, our best model outperforms autoregressive Transformer by 0.6 BLEU without the help of knowledge distillation. To our best knowledge, it is the first time that a non-iterative NAT model achieves competitive results with AT models without KD. DA-Transformer outperforms existing NATs (including iterative approaches) with a large margin on the raw data of standard En↔DE and En↔Zh benchmarks, which sufficiently shows the effectiveness of our proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Non-autoregressive Machine Translation <ref type="bibr" target="#b14">Gu et al. (2018)</ref> propose NAT models to reduce the latency in generation or decoding, but there exists a gap in translation quality between NAT and AT models. To bridge the gap, iterative NATs manage to repeatedly refine the generated outputs <ref type="bibr" target="#b28">(Lee et al., 2018;</ref><ref type="bibr" target="#b9">Ghazvininejad et al., 2019;</ref><ref type="bibr" target="#b17">Guo et al., 2020)</ref>. However, as shown in <ref type="bibr" target="#b25">Kasai et al. (2021)</ref>, most iterative NATs are not advantageous against ATs in the qualitylatency tradeoff. Non-iterative NATs are much faster, whose improvements mainly come from alignment-based objectives <ref type="bibr" target="#b30">(Libovický &amp; Helcl, 2018;</ref><ref type="bibr" target="#b10">Ghazvininejad et al., 2020a;</ref><ref type="bibr" target="#b6">Du et al., 2021)</ref>, or incorporating extra decoder inputs <ref type="bibr" target="#b45">(Shu et al., 2020;</ref><ref type="bibr" target="#b37">Qian et al., 2021a;</ref><ref type="bibr" target="#b2">Bao et al., 2021)</ref>. Nevertheless, these NATs heavily rely on knowledge distillation (KD, <ref type="bibr" target="#b14">Gu et al., 2018)</ref>, which is found very effective in reducing the data modalities <ref type="bibr" target="#b53">(Zhou et al., 2020)</ref>. A recent study <ref type="bibr" target="#b21">(Huang et al., 2022b)</ref> provides a unified perspective showing that most existing methods actually modify targets or inputs to reduce the token dependencies in the data distribution, which eases the NAT training but introduces data distortion.</p><p>Unlike existing NATs, our method retains multiple translations instead of dropping the multi-modal information in NAT training. It turns out that our method can effectively tackle the multi-modality problem without modifying the training data and not rely on KD to achieve a good translation performance.</p><p>Lattice-based Model in Machine Translation Word lattices have a long history in Statistic Machine Translation. A word lattice is a directed acyclic graph (DAG) with edges labeled with a token and weight, which can represent an exponential number of sentences in the a compact structure. A phrase-based translation system can generate a word lattice during decoding <ref type="bibr" target="#b49">(Ueffing et al., 2002;</ref><ref type="bibr" target="#b32">Och &amp; Ney, 2004)</ref>. Some models take word lattices as inputs to alleviate input errors brought by word segmentation or speech recognition <ref type="bibr" target="#b7">(Dyer et al., 2008;</ref><ref type="bibr" target="#b27">Koehn et al., 2007;</ref><ref type="bibr" target="#b5">Dong et al., 2014)</ref>.</p><p>There are also studies that combine multiple system outputs into a single lattice <ref type="bibr" target="#b40">(Rosti et al., 2007;</ref><ref type="bibr" target="#b8">Feng et al., 2009)</ref> and decode a good translation from it <ref type="bibr" target="#b48">(Tromble et al., 2008)</ref>.</p><p>Unlike previous studies that construct the word lattices with a search algorithm, our model predicts the whole DAG with multiple translations simultaneously. Moreover, DA-Transformer's training does not require ground-truth word  lattices for supervision, making it applicable to most translation benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Proposed Method</head><p>In this section, we describe our proposed DA-Transformer in detail. Intuitively, to facilitate the explicitly modeling of multiple modalities, we propose to replace the original Nonautoregressive Transformer decoder with a directed acyclic decoder, whose topological structure is a DAG. Each path of the DAG forms a sequence of hidden states that stores a possible translation, and the whole DAG store multiple translations in different paths. DA-Transformer still generates in a non-autoregressive fashion.</p><p>We will first introduce the network structures in Section 3.1, which presents how to construct the DA-Transformer to parameterize the conditional probability. Then in Section 3.2, we will elaborate on the training of DA-Transformer, including how to train it with one reference and the efficient implementation of traversing possible paths. Finally, in Section 3.3, we provide several decoding approaches, aiming to sample fluent sentences efficiently given the well-trained DA-Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Architecture of DA-Transformer</head><p>DA-Transformer consists of a Transformer encoder and a directed acyclic decoder. The encoder is the same as vanilla Transformer while the decoder organizes its hidden states as a DAG. As shown in Fig. <ref type="figure" target="#fig_2">2</ref>, hidden states correspond to vertices of the DAG, which model word distributions in specific positions; and edges of the DAG are transitions between hidden states, which organize generated words into a final sentence.</p><p>Intuitively, given a source sentence X, the directed acyclic decoder generates a sentence in three steps: (1) receiving the position embeddings as inputs and producing hidden states as vertices; (2) calculating the transition probabilities between the vertices based on the vertex states; (3) sampling a path from the DAG following the transitions, and then predicting target tokens using the vertex states on the path.</p><p>Formally, the probability of a target sentence</p><formula xml:id="formula_0">Y = {y 1 , y 2 , • • • , y M } is formulated as P θ (Y |X) = A∈Γ P θ (Y, A|X) = A∈Γ P θ (A|X)P θ (Y |A, X),<label>(1)</label></formula><p>where A = {a 1 , a 2 , • • • , a M } is a path represented by a sequence of vertex indexes, and Γ contains all paths with the same length of the target sentence Y .</p><p>Vertex The directed acyclic decoder utilizes the Transformer layers <ref type="bibr" target="#b50">(Vaswani et al., 2017)</ref> to predict the vertex states. Unlike the autoregressive decoder that generates tokens from left to right, it generates the vertex states in parallel.</p><p>Specifically, we use graph positional embeddings G = {g 1 , • • • , g L } as the decoder inputs, which is identical to the learnable positional embeddings in vanilla Transformer but represents the vertex indexes instead of the token positions. Note that L is the graph size, where we set L to λ times the source length N and tune λ as a hyper-parameter. The decoder then produces the vertex states</p><formula xml:id="formula_1">V = [v 1 , • • • , v L ] T , which is defined as [v 1 , • • • , v L ] = Transformer-Blocks(g 1 , • • • , g L ).</formula><p>Transition Each edge of the DAG is assigned the transition probability between the connecting vertices. The transition probabilities are locally normalized, i.e., the probabilities of outgoing edges sum to one. Formally, the probability of path A is defined as</p><formula xml:id="formula_2">P θ (A|X) = M −1 i=1 P θ (a i+1 |a i , X) = M −1 i=1 E ai,ai+1</formula><p>,</p><p>where E ∈ R L×L is the transition matrix normalized by rows. Specifically, the transition matrix is obtained by</p><formula xml:id="formula_3">E = softmax( QK T √ d ),<label>(2)</label></formula><formula xml:id="formula_4">Q = VW Q , K = VW K ,</formula><p>where d is the hidden size, W Q and W K are learnable parameters. To ensure that there is no cycle in the DAG, we apply lower triangular masking on E, which only allows transitions from vertices with small indexes to large indexes. Note that the matrix E can be calculated in parallel, thereby facilitating fast sampling of paths.</p><p>Token Prediction Conditioned on the vertex states in V and the selected path A, the decoder predicts the target tokens in parallel. Formally, we have</p><formula xml:id="formula_5">P θ (Y |A, X) = M i=1 P θ (y i |a i , X) = M i=1 softmax(W P v ai ),</formula><p>where W P are learnable weights, and v ai is the representation of the i-th vertex on the path A.</p><p>In the implementation, we actually calculate the distributions on all vertices and then skip the vertices not appearing on the chosen path. Specifically, we obtain</p><formula xml:id="formula_6">P = softmax(VW T P ),<label>(3)</label></formula><p>where P ∈ R L×|V| is the matrix containing the token distributions on the L vertices, and P θ (y i |a i , X) = P ai,yi .</p><p>The matrix P facilitates fast calculation for multiple paths since the shared vertices are not calculated twice, which is significant in training and inference introduced later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training</head><p>To capture multiple translation modalities in training, the proposed directed acyclic decoder arranges words from different modalities in different vertex states of the decoder, which can effectively reduces the inconsistent problem in training. In this section, we will elaborate on training details of DA-Transformer, including training with one reference, efficient implementation of marginalizing all paths in the DAG, and modified glancing training techniques according to the graph structures.</p><p>Training DA-Transformer with One Reference Although DA-Transformer retains multiple translations in the DAG, its training objective only requires one reference per sample, which facilitates efficient training on most translation benchmarks. Specifically, it directly maximizes the log-likelihood log P (Y |X) by marginalizing all possible paths A, which can be formulated as follows, where Γ contains all paths with</p><formula xml:id="formula_7">L = − log P θ (Y |X) = − log A∈Γ P θ (Y, A|X),<label>(4)</label></formula><formula xml:id="formula_8">1 = a 1 &lt; • • • &lt; a M = L.</formula><p>To understand why a single reference is adequate for the DAG learning, we analyze the training process by inspecting the gradients. Intuitively, we find that the objective assigns a single reference to several paths, where the vertices on the chosen paths are updated to generate the reference tokens, and the other vertices remain unchanged. The sparse assignment is the key to the successful training, which avoids inconsistent labels in token predictions and preserves the unseen translations stored on the unchanged paths. In such a way, the DAG can be learned across different training instances, each of which only provides a single reference, not requiring an instance with multiple references.</p><p>Specifically, we inspect the gradient of L and find that</p><formula xml:id="formula_9">∂ ∂θ L = A∈Γ w A ∂ ∂θ L A ,<label>(5)</label></formula><p>where</p><formula xml:id="formula_10">L A = − log P θ (Y, A|X),<label>(6)</label></formula><formula xml:id="formula_11">w A = P θ (Y, A|X) A ∈Γ P θ (Y, A |X) .<label>(7)</label></formula><p>L A maximizes the likelihood of sampling Y with the path A, and w A is the weight of L A . Eq(5) indicates that the weights of paths are assigned according to the probability that Y appears on A. If a path A is more probable for the target Y , then a larger weight will be used in optimizing L A , which further strengthens its dominance. In contrast, an unlikely path A will get a negligible weight, indicating that the vertices on A are not affected in the update.</p><p>A real example is shown in Fig. <ref type="figure" target="#fig_3">3</ref>. In the early stage, training with one sample will affect all vertices in the DAG. In the late stage, only some vertices are updated, reserving the other vertices for storing unseen translations. Directed Acyclic Decoder Marginalizing A with Dynamic Programming The objective L requires marginalizing all paths A, which is expensive due to the numerous paths. Similar to <ref type="bibr" target="#b12">Graves et al. (2006)</ref>, we employ dynamic programming to tackle the issue.</p><formula xml:id="formula_12">[M] 𝑦 2 [M] [M] [M] 𝑦 3 [M] 𝑋 𝑌 = {𝑦 1 , 𝑦 2 , 𝑦 3 }</formula><formula xml:id="formula_13">𝑍 = [M] 𝑦 1 𝑦 2 𝑋 [M] 𝑦 2 [M] [M] [M] 𝑌 = {𝑦 1 , 𝑦 2 , 𝑦 3 } Reconstruction loss Attn Attn 𝒈 1 𝒈 2 𝒈 3 𝒈 4 𝒈 5 + + + + + መ 𝐴 = {1, 3, 5} [M] [M] [M] [M] + + + + + [M]</formula><p>Generally, we recurrently calculate the probability sum of path prefixes that end at the vertex u and generate the target prefix Y ≤i , denoted as f i,u . Since the path prefixes that end at the vertex u should pass through a vertex v satisfying v &lt; u, so f i,u can be obtained from </p><formula xml:id="formula_14">f i−1,v .</formula><formula xml:id="formula_15">L glancing = − log P θ (Y |X, Z),<label>(8)</label></formula><p>where </p><formula xml:id="formula_16">Z = [z 1 , • • • , z L ]</formula><formula xml:id="formula_17">:= E.ARGMAX(dim=1) # shape: (L) i := 1, output := [ tokens[1] ] repeat i := edges[i] # jumping along the transition output.APPEND(tokens[i]) until i = L</formula><p>ing strategy proposed by GLAT <ref type="bibr" target="#b37">(Qian et al., 2021a)</ref>, which decides the number of unmasked tokens according to the prediction accuracy. <ref type="foot" target="#foot_3">4</ref> (3) We add Z to the decoder input and train the model by minimizing Eq(8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Inference</head><p>In inference, DA-Transformer constructs a DAG that stores multiple translations, where we aim to find the most probable one. Compared with existing NATs, DA-Transformer utilizes transitions to distinguish different candidates, which improves fluency and avoids errors like repeated tokens. We propose three decoding strategies to find high-quality translations while keeping low latency.</p><p>Greedy The simplest strategy is to take the most likely choices for the transitions and tokens. Specifically, we perform parallel argmax operations to obtain the most likely transition and token for each vertex. Then, we generate the translation by collecting the predicted tokens along the chosen path. The greedy decoding is highly efficient that only uses two parallel operations, as shown in Algo.1.</p><p>Lookahead Lookahead decoding improves the greedy strategy by jointly considering the transitions and the tokens. Specifically, we rearrange P θ (Y, A|X) into</p><formula xml:id="formula_18">P θ (y 1 |a 1 , X) M i=2 P θ (a i |a i−1 , X)P θ (y i |a i , X), (9)</formula><p>which becomes a sequential decision problem of choosing a i and y i in order. We simultaneously obtain y * i , a * i = arg max P θ (y i |a i , X)P θ (a i |a i−1 , X), (10) which can be still implemented in parallel with almost zero overhead, as presented in Algo.1.</p><p>BeamSearch BeamSearch is a more accurate method for solving the above decoding problem. Following <ref type="bibr" target="#b13">Gu &amp; Kong (2021)</ref>, we combine an n-gram language model to improve the performance. Specifically, we search in beam to approximately find the optimal Y * that maximizes</p><formula xml:id="formula_19">1 |Y | α [log P θ (Y |X) + γ log P n-gram (Y )] ,<label>(11)</label></formula><p>where α, γ are hyper-parameters for length penalty and language model scores. Metrics For fair comparisons with previous work, we use tokenized BLEU <ref type="bibr" target="#b35">(Papineni et al., 2002)</ref> for all benchmarks except WMT17 En-Zh, where we use sacreBLEU (Post,   5 We will release an efficient C++ implementation at https: //github.com/thu-coai/DA-Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2018).</head><p>The latency speedup is evaluated on WMT17 En-De test set with a batch size of 1.</p><p>Hyper-parameters Our models generally use the hyperparameters of transformer-base <ref type="bibr" target="#b50">(Vaswani et al., 2017)</ref>. For regularization, we set dropout to 0.1, weight decay to 0.01, and label smoothing to 0.1. All models, including ATs, are trained for 300k updates with a batch of 64k tokens. The learning rate warms up to 5 • 10 −4 within 10k steps and then decays with the inverse square-root schedule. We evaluate the BLEU scores on the validation set every epoch and average the best 5 checkpoints for the final model. For DA-Transformer, we use λ = 8 and Lookahead Decoding unless otherwise specified. We linearly anneal τ from 0.5 to 0.1 for glancing training. For BeamSearch, we set beam size to 200, γ to 0.1, and tune α from [1, 1.4] on the validation set. The training lasts approximately 32 hours on 16 Nvidia V100-32G GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Main Results</head><p>As shown in Table <ref type="table" target="#tab_4">1</ref>, DA-Transformer substantially improves the translation quality and outperforms strong baselines by a large margin. Our model alleviates the multimodality problem by capturing multiple translations within a DAG, which avoids inconsistent labels in training and reduces the errors of mixing translations in inference. We highlight the empirical advantages of our method: 1) Better translation quality compared with non-iterative NATs. As a non-iterative NAT, our model achieves new SoTA results in translation quality while preserving compet-   3) Flexible quality-latency tradeoff. Comparing the decoding strategies of our method, we find that Lookahead Decoding consistently outperforms Greedy Decoding, and the n-gram LM usually benefits BeamSearch, with almost zero overheads. To better show the quality-latency tradeoff, we tune the graph size and beam size with our decoding strategies. As shown in Fig. <ref type="figure" target="#fig_5">5</ref>, our method significantly outperforms existing NATs and provides flexible quality-latency tradeoff for non-autoregressive translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>In this section, we investigate the effects of the graph size and training methods on the raw data of WMT14 En-De.</p><p>Graph Size DA-Transformer utilizes a DAG with L vertices, which is empirically set to λ times of the source length.</p><p>A large DAG can model more translations. However, it also makes the transition predictions difficult. We manually tune λ from 2 to 16, as shown in Fig. <ref type="figure">6</ref>.</p><p>The results show that larger graphs improve the translation quality until λ exceeds 12, where λ is not sensitive around its best value. We compare our methods against CTC, which also utilizes a similar hyper-parameter to determine the output length <ref type="bibr" target="#b30">(Libovický &amp; Helcl, 2018</ref>  Max/Sum represent using the max operation or the sum operation in marginalizing all paths in Eq(4). We compare three masking strategies in obtaining the decoder input: All Masked, Uniform <ref type="bibr" target="#b9">(Ghazvininejad et al., 2019)</ref>, and Adaptive <ref type="bibr" target="#b37">(Qian et al., 2021a)</ref>.</p><p>Transformer when λ = 2, the BLEU score does not increase for a larger λ. We attribute the problem to the inconsistent label problem: a longer output sequence does not help CTC to reduce the inconsistent labels in training, where DA-Transformer benefits from larger graphs by assigning different tokens to distinct vertices. Considering the performance and computation cost, we choose λ = 8 and apply it to all other datasets.</p><p>Training Objectives DA-Transformer is trained with a glancing objective that only requires one reference for each sample, where we investigate two important designs: First, we marginalize all possible paths to obtain L, which is equivalent to optimizing the paths with different weights as discussed in Sec.3.2. We compare it with the objective only optimizing the most probable path, i.e., replacing the sum operation by the max operation in Eq(4). Second, we use glancing training with a masked target as inputs, where the masked tokens are adaptively chosen according to the prediction accuracy <ref type="bibr" target="#b37">(Qian et al., 2021a)</ref>. We compare it with two other strategies: masking all inputs (i.e., do not use glancing training), uniform random masking <ref type="bibr" target="#b9">(Ghazvininejad et al., 2019)</ref>.</p><p>The results are shown in Fig. <ref type="figure" target="#fig_7">7</ref>. First, marginalizing all paths (Sum) outperforms choosing the most probable path (Max). One possible reason is that the max operation makes sharp weight assignments in the early training, leading to a premature convergence in which only several paths are </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Analysis</head><p>This section verifies that DA-Transformer benefits from assigning tokens to vertices in training and explicitly considers the transitions in inference. It also shows some cases of learned DAGs. We present more analyses in the appendix, including the translation performance on different lengths (Appendix C.1), performance with controlled training time (Appendix C.2), and some statistics of DAGs (Appendix E).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DA-Transformer improves token accuracy.</head><p>In training, we assign tokens of different translations to different vertices, which avoids the inconsistent labels in training and thus improves the token accuracy in inference. We compare our model against two baselines, Vanilla NAT and CTC+GLAT. Note that CTC utilizes an alignment-based objective, which also assigns the reference tokens to different positions of Transformer. We calculate the accuracy under the best assignment following two steps: We first obtain the most probable assignment that matches each reference token to a prediction. Then, we calculate the accuracy by comparing the predicted tokens on the best assignment (i.e., the best path in DA-Transformer) against the reference.<ref type="foot" target="#foot_4">6</ref> </p><p>As shown in Table <ref type="table" target="#tab_8">2</ref>, vanilla NAT suffers from label inconsistency problem, leading to low token accuracies. Comparing DA-Transformer and CTC, we find that our method is far more effective, especially on the syntax distant language pair such as WMT17 Zh-En. We conjecture that the advantage mainly comes from our flexible assignment method. CTC only avoids position mismatches by inserting empty or repeated tokens. It requires that the possible translations share similar lexical choices, which cannot handle highly diverse translations.</p><p>DA-Transformer facilitates diverse generation. In infer- ence, DA-Transformer utilizes the transition matrix to avoid incorrect outputs caused by mixing multiple translations. We evaluate the ability to distinguish different translations by sampling diverse translations from the DAG. Specifically, we begin at the start vertex and repeatedly use Nucleus Sampling (top-p sampling, <ref type="bibr" target="#b19">Holtzman et al., 2020)</ref> to choose the next vertex and token according to Eq.( <ref type="formula">9</ref>). We use p = 0.8 and vary the temperature from 0.4 to 1.0.</p><p>We follow <ref type="bibr" target="#b44">Shen et al. (2019)</ref> to evaluate the quality and diversity by multi-reference BLEU and pairwise BLEU. We compare our model against AT and GLAT+CTC, the best non-iterative NAT baseline. We obtain the hypotheses from GLAT+CTC by replacing the argmax operations in decoding with Nucleus Sampling with the same p and temperature.</p><p>The results are shown in Fig. <ref type="figure" target="#fig_8">8</ref>. Compared with GLAT+CTC, DA-Transformer achieves a better tradeoff between quality and diversity. With the same temperature, the generated samples (without KD) by our model are far more diverse than GLAT+CTC. It shows that our model can learn multiple diverse translations and further decode them in inference. Compared with Transformer, DA-Transformer (without KD) is slightly less diverse but achieves a close tradeoff on WMT17 Zh-En, which shows the great potential of our model. Moreover, we find that applying KD to DA-Transformer improves the quality but sacrifices the diversity because KD reduces the data modalities.</p><p>Case Study We choose a test sample of WMT17 Zh-En and illustrate the DAG predicted by our model. For a clear presentation, we use λ = 4 for a small graph and further remove some useless vertices and edges. Specifically, we remove all vertices with passing probabilities smaller than 0.1, where the passing probabilities represent how likely the vertex will appear on a randomly sampled path. We only show the transitions in the top 90% of probabilities.</p><p>As shown in Fig. <ref type="figure" target="#fig_9">9</ref>, the predicted DAG is highly reasonable. Following the transitions, we can clearly distinguish translation expressions, which avoids the errors like repeated tokens shown in the vanilla NAT's output. We present the We present the top candidates on each vertex and the transition probabilities between vertices. We also present the passing probabilities representing how likely a vertex will appear on a sampled path. We remove vertices/edges with small passing/transition probabilities for a clear presentation. The vanilla NAT mixes the tokens from different translations, which can be avoided in DA-Transformer inference. We use the BPE tokenizer <ref type="bibr" target="#b42">(Sennrich et al., 2016)</ref>, where a subword prefix is marked by -. See more examples in Appendix D.</p><p>top-5 hypotheses produced by BeamSearch, which are fluent and diverse.</p><p>However, we can still find errors in the predicted DAG, e.g., a possible incorrect translation "Does that sounds ...". Although the error does not easily occur in Lookahead or BeamSearch decoding, it shows that there is still space for improving the consistency between the tokens in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose DA-  Each valid path in the DAG corresponds to a path in the grid that starts from (1, 1) to (M, L). M is the target length, L is the graph size. P (Y, A|X) can be calculated by multiplying the token probabilities and the transition probabilities. In Dynamic Programming, we recurrently calculate fi,u, which is the probability sum of all paths that start from (1, 1) and end at (i, u). E.g., f3,4 is the sum of two paths' probabilities, A1 and A2. Our training objective L is equal to − log fM,L.</p><p>The training objective of DA-Transformer is formulated in Eq(4), which requires marginalizing all possible paths A. To avoid the expensive cost of enumerating the paths, we employ dynamic programming that reduces the time complexity to O(M L 2 ), where M is the target length, L is the graph size.</p><p>To utilize dynamic programming, we first represent the valid paths in a M × L grid, where each valid path of the DAG corresponds to a path in the grid that starts from the left-upper corner (1, 1) and ends in the right-bottom corner (M, L), as shown in Fig. <ref type="figure" target="#fig_10">10</ref>. Formally, the path</p><formula xml:id="formula_20">A = {a 1 , • • • , a M } satisfying 1 = a 1 &lt; • • • &lt; a M = L corresponds to a path in the grid that passes through (1, a 1 ), (2, a 2 ), • • • , (M, a M ).</formula><p>Then we find that the probability on the path A, i.e., P θ (Y, A|X), can be decomposed and calculated by multiplying the probabilities of the token predictions and transitions. Specifically, we have </p><formula xml:id="formula_21">P θ (Y, A|X) = M j=1 P θ (y j |a j , X) M j=2 P θ (a j |a j−1 , X)<label>(12)</label></formula><p>where P aj ,yj can be regarded as the token probability on the point (j, a j ), and the E aj−1,aj can be regarded as the transition probability on the edge connecting (j − 1, a j−1 ) with (j, a j ).</p><p>Recall that our objective requires the sum of the probabilities of all valid paths. We can recurrently calculate f i,u , which is defined as the probability sum of the paths that start from (1, 1) but end at (i, u). Since each valid path that ends at (i, u) must pass through a point (i − 1, v) where v ∈ [1, u), we reach a recurrence formula that obtains f i,u from f i−1,v :</p><formula xml:id="formula_23">f i,u = P u,yi u−1 v=1 f i−1,v E v,u (2 ≤ i ≤ M, 1 ≤ u ≤ L),<label>(14)</label></formula><p>where the boundary conditions are:</p><formula xml:id="formula_24">f 1,1 = P 1,y1 ; f 1,u = 0 (2 ≤ u ≤ L). (<label>15</label></formula><formula xml:id="formula_25">)</formula><p>Algorithm 2 Dynamic Programming Algorithm in Pytorch-like Parallel Pseudocode Finally, the loss can be obtained by L = − log f M,L .</p><formula xml:id="formula_26">Input: Target Length M , Graph Size L, Target Sentence Y , Transition Matrix E ∈ R L×L , Token Distributions P ∈ R L×|V| Initialize a zero matrix f ∈ R M ×L f [1, 1] := 1 for i = 2, 3, • • • , M do f [i, :] := P[:, yi] ⊗ (f [i − 1, :] × E) # ⊗</formula><p>Since the product-sum operations can be calculated by matrix multiplications, the above recurrent process can be implemented with O(M ) parallel operations, as shown in Algorithm 2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation of Beam Search</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dropped</head><p>The final result is 𝐵 3 = [&lt;s&gt;, Yes, &lt;/s&gt;] Figure <ref type="figure" target="#fig_1">11</ref>. An step-by-step example of the beam search algorithm presented in Algorithm 3. One beam represents a translation prefix, which may appear on multiple paths. For demonstration, we only preserve the top-1 beam in each step, limit the candidate number when expanding the beams, and set α = 1 for the length penalty and γ = 0 to disable the n-gram language model.</p><p>Our concept of beam is similar to the prefix beam search <ref type="bibr" target="#b18">(Hannun et al., 2014)</ref>, where a beam represents a translation prefix but may appear on multiple paths. E.g., in Fig. <ref type="figure" target="#fig_1">11</ref>, [ s , Yes] is a beam that appears on two paths, {v 1 , v 2 } and {v 1 , v 3 }.</p><p>Our beam search aims to calculate the probability sum of all paths that produce the same translation, which approximates P (Y |X) and works better than finding a single path that maximizes P (Y, A|X).</p><p>To achieve an effective calculation of the scores, we maintain the probability sum for a beam B during the beam search. Specifically, we define s i (B) as the probability sum of the paths ends at vertex i. When sorting the beams, we use the beam score defined in Eq(11), where P (Y |X) is equal to the probability sum of all paths, i.e., L i=1 s i (B). Our algorithm is presented in Algorithm 3 with an example shown in Fig. <ref type="figure" target="#fig_1">11</ref>. We further apply some tricks to reduce the computation costs:</p><p>• Unlike vanilla beam search that all beams have the same length in each step, our algorithm may compare beams with different lengths. To avoid a length bias in the selected beams, we only preserve the top-10 for each length. If the total number of beams is still too large, we choose the top-200 beams. • When expanding beams, we only use the top-5 candidates. A candidate is a v, t pair, indicating the next vertex and token, where we jointly consider their probabilities as Eq(10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. More Analyses</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Translation Performance on Different Lengths</head><p>To investigate the translation performance on different lengths, we split the test set into 6 buckets according to reference lengths and evaluate the BLEU score in each bucket as shown in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Performance with Controlled Training Time</head><p>One update step of DA-Transformer's training is slower than many previous NATs because our Directed Acyclic Decoder has to process a longer sequence whose length is about 8 times of the original target. In Fig. <ref type="figure" target="#fig_14">13</ref>, we show that DA-Transformer still outperforms strong NAT baselines when the training time is controlled. Moreover, we observe that our performance is more stable than the baselines during the training process. Two more test cases from WMT17 Zh-En are presented in Fig. <ref type="figure" target="#fig_16">14</ref>.</p><p>Source 这太 令人 难以置信 了 。 Reference It was just incredible .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DA-Transformer</head><p>This is incredible .</p><p>This is un-believ-able .</p><p>This is too incredible . This is too un-believ-able .</p><p>This is too un-able . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DA-Transformer</head><p>The woman died on the spot .</p><p>The woman died instantly .</p><p>The woman was killed on the spot .</p><p>The woman was killed instantly .</p><p>The woman was instantly killed on the spot . We show the top candidates on each vertex and the transition probabilities between vertices. We remove vertices/edges with small passing/transition probabilities for a clear presentation. We use the BPE tokenizer <ref type="bibr" target="#b42">(Sennrich et al., 2016)</ref>, where a subword prefix is marked by -.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Statistics of DAGs</head><p>For a better understanding of DA-Transformer, we collect some statistics of predicted DAGs on WMT17 Zh-En. We use a DA-Transformer with λ = 4. The distribution of numbers of vertices' outgoing edges. We only consider the most likely edges accounting for 80% of the transition probabilities and ignore the vertices with passing probabilities smaller than 0.2. We further merge the edges that are linked to vertices predicting the same token.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>have been Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author(s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. (a) The multi-modality problem in vanilla NAT. Multiple possible references provide inconsistent labels at some positions, leading to an implausible prediction that mixes several translations. (b) The proposed DA-Transformer. Tokens from different translations are assigned to distinct vertices to avoid the inconsistent labels. In inference, we follow the transitions to recover the output translation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Overview of Directed Acyclic Decoder. The decoder organizes the hidden states V = [v1, • • • , vL] T in a directed acyclic graph (DAG) structure, which stores representations of multiple translations. The transition probabilities in E are predicted based on the vertex states. To generate one of the translations, a path A is first sampled from the DAG structure, and the selected vertices predict the whole sentence in parallel. The other vertices and their predicted tokens are skipped in the output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure3. The cumulative weight wA of paths that pass through each vertex, where lines represent token labels for the vertex.3 The weights are sparse in the late stage of training, indicating that only several vertices are updated to fit the sample. Source: "我们 需要 更多 的 交流 。" Target: "We need more communication ."</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Glancing training for DA-Transformer, which is similar to the masked language model for promoting representation learning. Glancing training requires two forward passes of Directed Acyclic Decoder: The first pass assigns target tokens to appropriate positions. The second pass calculates the reconstruction loss. [M] indicates a masked token whose embeddings are all zeros.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Quality-latency tradeoff on WMT14 En-De and WMT17 Zh-En with varying graph sizes and beam sizes. The graph size is λ times of the source length. We use Beamsearch + 5-gram LM with the beam size (bs) of 200, 100, 50. bs = 1 indicates Lookahead Decoding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Ablation study of training objectives on WMT14 En-De.Max/Sum represent using the max operation or the sum operation in marginalizing all paths in Eq(4). We compare three masking strategies in obtaining the decoder input: All Masked, Uniform<ref type="bibr" target="#b9">(Ghazvininejad et al., 2019)</ref>, and Adaptive<ref type="bibr" target="#b37">(Qian et al., 2021a)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Figure8. Quality-diversity tradeoff of sampled sentences from different models. Each curve has 4 points with sampling temperatures t = [0.4, 0.6, 0.8, 1] (from left to right). We sample hypotheses with the same number of human-written references.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 .</head><label>9</label><figDesc>Figure9. A test sample of WMT17 Zh-En with the DAG and BeamSearch results. We present the top candidates on each vertex and the transition probabilities between vertices. We also present the passing probabilities representing how likely a vertex will appear on a sampled path. We remove vertices/edges with small passing/transition probabilities for a clear presentation. The vanilla NAT mixes the tokens from different translations, which can be avoided in DA-Transformer inference. We use the BPE tokenizer<ref type="bibr" target="#b42">(Sennrich et al., 2016)</ref>, where a subword prefix is marked by -. See more examples in Appendix D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 .</head><label>10</label><figDesc>Figure10. An overview of dynamic programming for DA-Transformer training. Each valid path in the DAG corresponds to a path in the grid that starts from (1, 1) to (M, L). M is the target length, L is the graph size. P (Y, A|X) can be calculated by multiplying the token probabilities and the transition probabilities. In Dynamic Programming, we recurrently calculate fi,u, which is the probability sum of all paths that start from (1, 1) and end at (i, u). E.g., f3,4 is the sum of two paths' probabilities, A1 and A2. Our training objective L is equal to − log fM,L.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>is the element-wise multiplication, × is the vector-matrix multiplication end for Update the model by minimizing L = − log f [M, L].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. The BLEU score on WMT14 En-De and WMT17 Zh-En bucketed by the reference length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 13 .</head><label>13</label><figDesc>Figure13. The valid BLEU on WMT14 En-De. We do not apply the checkpoint average trick. We evaluate the model approximately every 8 GPU-hours. The training costs 500 GPU-hours, which has about 300k, 490k, 970k updates for DA-Transformer, GLAT+CTC, GLAT, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>当场 死亡 。Reference She died at the scene .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 14 .</head><label>14</label><figDesc>Figure14. Two test samples of WMT17 Zh-En with the DAG and BeamSearch results. We show the top candidates on each vertex and the transition probabilities between vertices. We remove vertices/edges with small passing/transition probabilities for a clear presentation. We use the BPE tokenizer<ref type="bibr" target="#b42">(Sennrich et al., 2016)</ref>, where a subword prefix is marked by -.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 15 .</head><label>15</label><figDesc>Figure15. Statistics of DAGs. (a) The distribution of vertices' passing probabilities and max token probabilities. The passing probability represents how likely a vertex would appear on a randomly sampled path. The max token probability represents the probability of the most probable token on the vertex. (b) The distribution of numbers of vertices' outgoing edges. We only consider the most likely edges accounting for 80% of the transition probabilities and ignore the vertices with passing probabilities smaller than 0.2. We further merge the edges that are linked to vertices predicting the same token.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Greedy / Lookahead Decoding in Pytorch-like Parallel Pseudocode Input: Graph Size L, Transition Matrix E ∈ R L×L , Token Distributions P ∈ R L×|V| if Using Lookahead then E := E ⊗ [P.MAX(dim=1).UNSQUEEZE(dim=0)]</figDesc><table><row><cell># E now jointly considers P and E</cell></row><row><cell># ⊗ is element-wise multiplication</cell></row><row><cell>end if</cell></row><row><cell>tokens := P.ARGMAX(dim=1) # shape: (L)</cell></row><row><cell>edges</cell></row><row><cell>is a randomly masked target pro-</cell></row><row><cell>vided as an extra decoder input, and P θ (Y |X, Z) is similarly</cell></row><row><cell>defined as Eq(4).</cell></row><row><cell>The glancing training follows three steps, as shown in</cell></row><row><cell>Fig.4. (1) We assign the target tokens to appropriate ver-</cell></row></table><note>tices since the decoder input is longer than the target sentence. The assignment follows the most probable path Â = arg max A∈Γ P θ (Y, A|X), which requires a forward pass of the decoder and dynamic programming. (2) We obtain Z by masking some tokens. We utilize the mask-Algorithm 1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 .</head><label>1</label><figDesc>Results on WMT14 En↔De and WMT17 Zh↔En. We present DA-Transformer' results with mean and standard deviation of three runs with different random seeds. Best performance of non-iterative NATs (iter=1) are bolded. Average Gap is the gap of BLEU against the best AT model, excluding the missing values. * indicates results of our re-implementation. Our autoregressive transformer is better than previously reported results because we use the same training setting as NATs (300k steps, 64k tokens/batch; previous results use 100k steps, 32k tokens/batch). † uses reranking methods in NAT decoding (LPD,<ref type="bibr" target="#b51">Wei et al., 2019)</ref>.</figDesc><table><row><cell>Model</cell><cell cols="3">Iter WMT14 En-De # Raw KD</cell><cell cols="2">WMT14 De-En Raw KD</cell><cell cols="2">WMT17 En-Zh Raw KD</cell><cell cols="5">WMT17 Zh-En Average Gap ↓ Speedup Raw KD Raw KD</cell></row><row><cell>Transformer (Vaswani et al., 2017)</cell><cell cols="2">M 27.6</cell><cell>27.8</cell><cell>31.4</cell><cell>31.3</cell><cell>34.3</cell><cell>34.4</cell><cell>23.7</cell><cell>24.0</cell><cell cols="2">0.45 0.49</cell><cell>1.0x</cell></row><row><cell>Transformer (Ours)</cell><cell cols="3">M 28.07* 28.54*</cell><cell cols="2">31.94* 31.54*</cell><cell cols="2">34.89* 34.69*</cell><cell cols="2">23.89* 24.68*</cell><cell>0</cell><cell>0</cell><cell>1.0x</cell></row><row><cell>CMLM (Ghazvininejad et al., 2019)</cell><cell cols="2">10 24.61</cell><cell>27.03</cell><cell>29.40</cell><cell>30.53</cell><cell>-</cell><cell>33.19</cell><cell>-</cell><cell>23.21</cell><cell cols="2">3.00 1.37</cell><cell>2.2x</cell></row><row><cell>SMART (Ghazvininejad et al., 2020b)</cell><cell cols="2">10 25.10</cell><cell>27.65</cell><cell>29.58</cell><cell>31.27</cell><cell>-</cell><cell>34.06</cell><cell>-</cell><cell>23.78</cell><cell cols="2">2.67 0.67</cell><cell>2.2x</cell></row><row><cell>DisCo (Kasai et al., 2020)</cell><cell cols="2">≈4 25.64</cell><cell>27.34</cell><cell>-</cell><cell>31.31</cell><cell>-</cell><cell>34.63</cell><cell>-</cell><cell>23.83</cell><cell cols="2">2.43 0.59</cell><cell>3.5x</cell></row><row><cell>Imputer (Saharia et al., 2020)</cell><cell>8</cell><cell>25.0</cell><cell>28.2</cell><cell>-</cell><cell>31.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">3.07 0.04</cell><cell>2.7x</cell></row><row><cell>CMLMC (Huang et al., 2022c)</cell><cell cols="2">10 26.40</cell><cell>28.37</cell><cell>30.92</cell><cell>31.41</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">1.35 0.15</cell><cell>1.7x</cell></row><row><cell>Vanilla NAT (Gu et al., 2018)</cell><cell>1</cell><cell cols="2">11.79* 19.99*</cell><cell cols="2">16.27* 25.77*</cell><cell cols="2">18.92* 25.84*</cell><cell cols="2">8.69* 14.81*</cell><cell cols="2">15.78 8.26</cell><cell>15.3x</cell></row><row><cell>CTC (Libovický &amp; Helcl, 2018)</cell><cell>1</cell><cell cols="2">18.42* 25.52</cell><cell cols="2">23.65* 28.73</cell><cell cols="2">26.84* 31.39*</cell><cell cols="2">12.23* 19.93*</cell><cell cols="2">9.41 3.47</cell><cell>14.6x</cell></row><row><cell>AXE  † (Ghazvininejad et al., 2020a)</cell><cell>1</cell><cell>20.40</cell><cell>23.53</cell><cell>24.90</cell><cell>27.90</cell><cell>-</cell><cell>30.88</cell><cell>-</cell><cell>19.79</cell><cell cols="2">7.36 4.34</cell><cell>14.2x</cell></row><row><cell>GLAT (Qian et al., 2021a)</cell><cell>1</cell><cell cols="2">19.42* 25.21</cell><cell cols="2">26.51* 29.84</cell><cell cols="2">29.79* 32.22*</cell><cell cols="2">18.88* 21.84*</cell><cell cols="2">6.05 2.59</cell><cell>15.3x</cell></row><row><cell>OaXE  † (Du et al., 2021)</cell><cell>1</cell><cell>22.4</cell><cell>26.1</cell><cell>26.8</cell><cell>30.2</cell><cell>-</cell><cell>32.9</cell><cell>-</cell><cell>22.1</cell><cell>5.4</cell><cell>2.0</cell><cell>14.2x</cell></row><row><cell>CTC + GLAT (Qian et al., 2021a)</cell><cell>1</cell><cell cols="2">25.02* 26.39</cell><cell cols="2">29.14* 29.54</cell><cell cols="2">30.65* 32.51*</cell><cell cols="2">19.92* 23.11*</cell><cell cols="2">3.52 1.98</cell><cell>14.6x</cell></row><row><cell>CTC + DSLP (Huang et al., 2022a)</cell><cell>1</cell><cell>24.81</cell><cell>27.02</cell><cell>28.33</cell><cell>31.61</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">3.44 0.73</cell><cell>14.0x</cell></row><row><cell cols="2">DA-Transformer + Greedy (Ours) 1</cell><cell cols="10">26.08±.25 27.31±.08 30.48±.18 31.30±.06 33.27±.12 33.80±.11 22.66±.12 24.04±.09 1.58 0.75</cell><cell>14.0x</cell></row><row><cell>+ Lookahead</cell><cell>1</cell><cell cols="10">26.57±.21 27.49±.05 30.68±.24 31.37±.06 33.83±.13 34.08±.13 22.82±.20 24.23±.14 1.22 0.57</cell><cell>13.9x</cell></row><row><cell>+ BeamSearch</cell><cell>1</cell><cell cols="10">27.02±.15 27.78±.07 31.24±.18 31.80±.03 34.21±.21 34.35±.12 24.22±.10 24.90±.16 0.53 0.16</cell><cell>7.1x</cell></row><row><cell>+ BeamSearch + 5-gram LM</cell><cell>1</cell><cell cols="10">27.25±.12 27.91±.07 31.54±.20 31.95±.06 34.23±.17 34.27±.05 24.49±.06 25.01±.18 0.32 0.08</cell><cell>7.0x</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 2 .</head><label>2</label><figDesc>Token accuracy under the best assignment. An assignment matches each reference token with a predicted token, which is called an alignment in CTC or a path in DA-Transformer.</figDesc><table><row><cell>Model</cell><cell cols="4">WMT14 En-De WMT17 Zh-En Train Valid Train Valid</cell></row><row><cell>Vanilla NAT</cell><cell>29.7</cell><cell>29.6</cell><cell>39.8</cell><cell>22.4</cell></row><row><cell>CTC + GLAT</cell><cell>50.2</cell><cell>51.7</cell><cell>47.1</cell><cell>32.3</cell></row><row><cell>DA-Transformer</cell><cell>69.3</cell><cell>69.9</cell><cell>80.1</cell><cell>67.0</cell></row><row><cell cols="5">used. Second, the glancing training (Uniform or Adaptive)</cell></row><row><cell cols="5">is better than the vanilla training (All Masked), which im-</cell></row><row><cell cols="5">proves the translation quality by promoting representation</cell></row><row><cell cols="5">learning. Moreover, the adaptive strategy can further boost</cell></row><row><cell cols="5">performance by choosing the masking ratio dynamically.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Insert 𝐵 𝑖𝑛𝑖𝑡 =[&lt;s&gt;] into 𝒢 1 ; Set 𝑠 1 𝐵 𝑖𝑛𝑖𝑡 : = 1 Yes, &lt;/s&gt;], add 𝐵 3 to 𝒢 4 𝒔 𝟒 𝑩 𝟑 : = 𝒔 𝟐 𝑩 𝟏 × 𝐄 𝟐,𝟒 × 𝐏 𝟒,&lt;/𝒔&gt; = 𝟎. 𝟑𝟔 𝑩 𝟏 = [&lt;s&gt;, Yes], add 𝑩 𝟏 to 𝓖 𝟑 𝑠 3 𝐵 1 : = 𝑠 1 𝐵 𝑖𝑛𝑖𝑡 × 𝐄 1,3 × 𝐏 3,Yes = 0.4 Yes, &lt;/s&gt;], add 𝐵 3 to 𝒢 4 𝒔 𝟒 𝑩 𝟑 : = 𝒔 𝟒 𝑩 𝟑 + 𝒔 𝟑 𝑩 𝟏 × 𝐄 𝟑,𝟒 × 𝐏 𝟒,&lt;/𝒔&gt; = 𝟎. 𝟕𝟐</figDesc><table><row><cell cols="2">𝒗 2 Yes 𝐏 𝟐,Yes = 0.8 𝑬 𝟐,𝟑 = 0.1 𝒗 3 Yes 𝐏 𝟑,Yes = 0.8 𝑬 𝟐,𝟒 = 0.9 𝑬 𝟑,𝟒 = 1 𝑬 𝟏,𝟑 = 0.5 𝑬 𝟏,𝟐 = 0.5 𝐏 𝟏,&lt;𝒔&gt; = 1 𝒗 1 &lt;s&gt;</cell><cell>𝒗 4 &lt;/s&gt; 𝐏 𝟒,Yes = 1</cell><cell cols="2">Step 2: 1 𝐵 3 = [&lt;s&gt;, Rank Beams in 𝓖 𝟐 𝐵 1 = [&lt;s&gt;, Yes] Candidate 1: 𝑣 = 3, 𝑡 = Yes = log 𝑠 2 𝐵 1 + 𝑠 3 𝐵 1 Beam Score 𝑙𝑒𝑛 𝐵 1 𝐵 2 = [&lt;s&gt;, Yes, Yes], add 𝐵 2 to 𝒢 3 𝑠 3 𝐵 2 : = 𝑠 2 𝐵 1 × 𝐄 2,3 × 𝐏 3,Yes = 0.032 Candidate 2: 𝑣 = 4, 𝑡 = &lt;/s&gt;</cell><cell>= −0.11</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Step 3:</cell></row><row><cell>Rank Step 0: Step 1: 1 One beam may</cell><cell cols="2">Beams in 𝓖 𝟏 𝐵 𝑖𝑛𝑖𝑡 = [&lt;s&gt;] Candidate 1: 𝑣 = 2, 𝑡 = Yes Beam Score = log 𝑠 1 𝐵 𝑖𝑛𝑖𝑡 /𝑙𝑒𝑛 𝐵 𝑖𝑛𝑖𝑡 = 0 𝑩 𝟏 : = &lt;s&gt;, Yes , add 𝑩 𝟏 to 𝓖 𝟐 𝑠 2 𝐵 1 : = 𝑠 1 𝐵 𝑖𝑛𝑖𝑡 × 𝐄 1,2 × 𝐏 2,Yes = 0.4 Candidate 2: 𝑣 = 3, 𝑡 = Yes</cell><cell cols="2">Rank 1 2 Candidate 1: 𝑣 = 4, 𝑡 = &lt;/s&gt; Beams in 𝓖 𝟑 𝐵 1 = [&lt;s&gt;, Yes] = log 𝑠 2 𝐵 1 + 𝑠 3 𝐵 1 Beam Score 𝑙𝑒𝑛 𝐵 1 𝐵 2 = [&lt;s&gt;, Yes, Yes] = log 𝑠 3 𝐵 2 /𝑙𝑒𝑛(𝐵 3 ) = −1.15 = −0.11 Step 4: 𝐵 3 = [&lt;s&gt;, Rank Beams in 𝓖 𝟒 Beam Score</cell><cell>Updated by merging probabilities of multiple paths.</cell></row><row><cell>appear at</cell><cell></cell><cell></cell><cell>1</cell><cell>𝐵 3 = [&lt;s&gt;, Yes, &lt;/s&gt;]</cell><cell>= log 𝑠 4 𝐵 3 /𝑙𝑒𝑛 𝐵 3 = −0.09</cell></row><row><cell>different vertices.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">The multi-modality here refers to the fact that there are multiple possible translations for a single source sentence.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">The DAG is similar to the concept of word lattice<ref type="bibr" target="#b39">(Richardson et al., 1995)</ref>. The words are represented by edges instead of vertices in the word lattice, and in contrast, each vertex of the DAG in our model represents a word distribution rather than concrete words.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">For example, the orange line (the second token, need) on the vertex v20 is the sum of wA for the paths A = {a1, a2, • • • , a5} satisfying a2 = 20. wA is defined in Eq(7).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">The number of unmasked token t = τ M i=1 [yi = ŷi], where ŷi = arg max P θ (•|ai, X), and τ ∈ [0, 1] is a hyper-parameter.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4">In CTC, a reference token may be matched with several predictions, so we average the accuracies for the reference token. The special empty tokens are not counted in the accuracy.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the National Science Foundation for Distinguished Young Scholars (with No. 62125604) and the NSFC projects (Key project with No. 61936010 and regular project with No. 61876096). This work was also supported by the Guoqiang Institute of Tsinghua University, with Grant No. 2019GQG1 and 2020GQG0005, and sponsored by Tsinghua-Toyota Joint Research Fund.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source:</head><p>S-ounds tricky ? In Fig. <ref type="figure">15</ref> (a), we present the distribution of vertices with passing probability and max token probability. We generally divide the vertices into three categories:</p><p>• Vertices with Passing Prob &gt; 0.5 (accounting for 19.6%): They are very likely to appear in the generated translation. Since the average target length is about 1 λ = 25% of the graph size, these vertices generate most of the tokens in the outputs.</p><p>• Vertices with Passing Prob &lt; 0.5 and Max Token Prob &gt; 0.5 (accounting for 40.2%): They have high confidence in predicting tokens but do not usually appear in the translation. They may contain some rare expressions. • Vertices with Passing Prob &lt; 0.2 and Max Token Prob &lt; 0.2 (accounting for 15.4%): These vertices do not have specific meanings. We think the vertices are not well learned. It may be helpful if we encourage them to be more confident in generating some specific tokens.</p><p>In Fig. <ref type="figure">15</ref> (b), we present the number of outgoing edges of vertices. We find that half of the vertices have only one outgoing edge, and the other half have multiple edges. The result shows that the predicted DAGs have complicated structures, which do not degenerate into chains.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Findings of the 2021 conference on machine translation (WMT21)</title>
		<author>
			<persName><forename type="first">F</forename><surname>Akhbardeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Arkhangorodsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Biesialska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Costa-Jussa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>España-Bonet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Harter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Homan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Amponsah-Kaakyire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kocmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Morishita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nagata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nagesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nakazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Tapo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Turchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vydrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zampieri</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2021.wmt-1.1" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Conference on Machine Translation</title>
				<meeting>the Sixth Conference on Machine Translation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-11">November 2021</date>
			<biblScope unit="page" from="1" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Non-autoregressive transformer by position learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<idno>CoRR, abs/1911.10677</idno>
		<ptr target="http://arxiv.org/abs/1911.10677" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Non-autoregressive translation by learning target categorical codes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rumshisky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hakkani-Tür</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.458</idno>
		<ptr target="https://doi.org/10.18653/v1/2021.naacl-main.458" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021</title>
				<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">June 6-11, 2021</date>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Understanding and improving lexical choice in non-autoregressive translation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ZTFeSBIX9C" />
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
				<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">May 3-7, 2021</date>
		</imprint>
	</monogr>
	<note>OpenReview.net, 2021a</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rejuvenating low-frequency words: Making the most of parallel data in non-autoregressive translation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.266</idno>
		<ptr target="https://doi.org/10.18653/v1/2021.acl-long.266" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</title>
		<title level="s">Long Papers</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Zong</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Navigli</surname></persName>
		</editor>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</meeting>
		<imprint>
			<date type="published" when="2021">August 1-6, 2021. 2021b</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3431" to="3441" />
		</imprint>
	</monogr>
	<note>Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Query lattice for translation retrieval</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Izuha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hao</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/C14-1192/" />
	</analytic>
	<monogr>
		<title level="m">COLING 2014, 25th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers</title>
				<editor>
			<persName><forename type="first">J</forename><surname>Hajic</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Tsujii</surname></persName>
		</editor>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">August 23-29, 2014. 2014</date>
			<biblScope unit="page" from="2031" to="2041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Order-agnostic cross entropy for non-autoregressive machine translation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v139/du21c.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
				<editor>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021-07-24">18-24 July 2021. 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="2849" to="2859" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generalizing word lattice translation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Resnik</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/P08-1115/" />
	</analytic>
	<monogr>
		<title level="m">ACL 2008, Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics</title>
				<editor>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Mckeown</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Moore</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Teufel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Allan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Furui</surname></persName>
		</editor>
		<meeting><address><addrLine>Columbus, Ohio, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">June 15-20, 2008. 2008</date>
			<biblScope unit="page" from="1012" to="1020" />
		</imprint>
	</monogr>
	<note>The Association for Computer Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Latticebased system combination for statistical machine translation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lü</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/D09-1115/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-08-07">2009, 6-7 August 2009. 2009</date>
			<biblScope unit="page" from="1105" to="1113" />
		</imprint>
	</monogr>
	<note>A meeting of SIG-DAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mask-predict: Parallel decoding of conditional masked language models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1633</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1633" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019</title>
				<editor>
			<persName><forename type="first">K</forename><surname>Inui</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Ng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Wan</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename></persName>
		</editor>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">November 3-7, 2019. 2019</date>
			<biblScope unit="page" from="6111" to="6120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Aligned cross entropy for non-autoregressive machine translation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v119/ghazvininejad20a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML 2020</title>
				<meeting>the 37th International Conference on Machine Learning, ICML 2020</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-07-18">13-18 July 2020. 2020a</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="3515" to="3523" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Semiautoregressive training improves mask-predict decoding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>CoRR, abs/2001.08785</idno>
		<ptr target="https://arxiv.org/abs/2001.08785" />
		<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1145/1143844.1143891</idno>
		<ptr target="https://doi.org/10.1145/1143844.1143891" />
	</analytic>
	<monogr>
		<title level="m">Machine Learning, Proceedings of the Twenty-Third International Conference (ICML 2006)</title>
				<editor>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</editor>
		<meeting><address><addrLine>Pittsburgh, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">June 25-29, 2006. 2006</date>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fully non-autoregressive neural machine translation: Tricks of the trade</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.11</idno>
		<ptr target="https://doi.org/10.18653/v1/2021.findings-acl.11" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021</title>
				<editor>
			<persName><forename type="first">C</forename><surname>Zong</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Navigli</surname></persName>
		</editor>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">August 1-6, 2021</date>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>ACL/IJCNLP 2021 of Findings of ACL</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Non-autoregressive neural machine translation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">O K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno>ICLR 2018</idno>
		<ptr target="https://openreview.net/forum?id=B1l8BtlCb" />
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
				<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-05-03">April 30 -May 3, 2018. 2018</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Levenshtein transformer</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Alché-Buc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/hash/675f9820626f5bc0afb47b57890b466e-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">B</forename><surname>Garnett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. December 8-14, 2019. 2019</date>
			<biblScope unit="page" from="11179" to="11189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Non-autoregressive neural machine translation with enhanced decoder input</title>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33013723</idno>
		<ptr target="https://doi.org/10.1609/aaai.v33i01.33013723" />
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
				<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019-02-01">January 27 -February 1, 2019. 2019</date>
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="3723" to="3730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Jointly masked sequence-tosequence model for non-autoregressive neural machine translation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.36</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.acl-main.36" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</title>
				<editor>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Chai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Schluter</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Tetreault</surname></persName>
		</editor>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">July 5-10, 2020. 2020</date>
			<biblScope unit="page" from="376" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">First-pass large vocabulary continuous speech recognition using bi-directional recurrent dnns</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<idno>CoRR, abs/1408.2873</idno>
		<ptr target="http://arxiv.org/abs/1408.2873" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The curious case of neural text degeneration</title>
		<author>
			<persName><forename type="first">A</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rygGQyrFvH" />
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
				<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">April 26-30, 2020. 2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Nonautoregressive translation with layer-wise prediction and deep supervision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">R</forename><surname>Zaïane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2110.07515" />
	</analytic>
	<monogr>
		<title level="m">The Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022</title>
				<imprint>
			<date type="published" when="2022">2022a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On the learning of non-autoregressive transformers</title>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International Conference on Machine Learning, ICML 2022</title>
				<meeting>the 39th International Conference on Machine Learning, ICML 2022</meeting>
		<imprint>
			<date type="published" when="2022">2022b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improving nonautoregressive translation models without distillation</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Volkovs</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=I2Hw58KHp8O" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2022">2022c</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fast decoding in sequence models using discrete latent variables</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v80/kaiser18a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan</title>
				<editor>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</editor>
		<meeting>the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">July 10-15, 2018. 2018</date>
			<biblScope unit="page" from="2395" to="2404" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Nonautoregressive machine translation with disentangled context transformer</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v119/kasai20a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML 2020</title>
				<meeting>the 37th International Conference on Machine Learning, ICML 2020</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-07-18">13-18 July 2020. 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="5144" to="5155" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep encoder, shallow decoder: Reevaluating non-autoregressive machine translation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=KpfasTaLUpq" />
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
				<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">May 3-7, 2021. 2021</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sequence-level knowledge distillation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d16-1139</idno>
		<ptr target="https://doi.org/10.18653/v1/d16-1139" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
				<editor>
			<persName><forename type="first">J</forename><surname>Su</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Carreras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Duh</surname></persName>
		</editor>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2016-11-01">2016. November 1-4, 2016. 2016</date>
			<biblScope unit="page" from="1317" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Open source toolkit for statistical machine translation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Herbst</surname></persName>
		</author>
		<author>
			<persName><surname>Moses</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/P07-2045/" />
	</analytic>
	<monogr>
		<title level="m">ACL 2007, Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics</title>
				<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Carroll</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Van Den Bosch</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Zaenen</surname></persName>
		</editor>
		<meeting><address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2007">June 23-30, 2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deterministic nonautoregressive neural sequence modeling by iterative refinement</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-1149</idno>
		<ptr target="https://doi.org/10.18653/v1/d18-1149" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<editor>
			<persName><forename type="first">E</forename><surname>Riloff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Chiang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Tsujii</surname></persName>
		</editor>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11-04">October 31 -November 4, 2018. 2018</date>
			<biblScope unit="page" from="1173" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Iterative refinement in the continuous space for non-autoregressive neural machine translation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.73</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.emnlp-main.73" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
				<editor>
			<persName><forename type="first">B</forename><surname>Webber</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Cohn</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</editor>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11-16">2020. November 16-20, 2020. 2020</date>
			<biblScope unit="page" from="1006" to="1015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">End-to-end non-autoregressive neural machine translation with connectionist temporal classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Libovický</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Helcl</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-1336</idno>
		<ptr target="https://doi.org/10.18653/v1/d18-1336" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<editor>
			<persName><forename type="first">E</forename><surname>Riloff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Chiang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Tsujii</surname></persName>
		</editor>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11-04">October 31 -November 4, 2018. 2018</date>
			<biblScope unit="page" from="3016" to="3021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Non-autoregressive conditional sequence generation with generative flow</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><surname>Flowseq</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1437</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1437" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019</title>
				<editor>
			<persName><forename type="first">K</forename><surname>Inui</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Ng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Wan</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename></persName>
		</editor>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">November 3-7, 2019. 2019</date>
			<biblScope unit="page" from="4281" to="4291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The alignment template approach to statistical machine translation</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<idno>doi: 10.1162</idno>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="417" to="449" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<idno type="DOI">10.1162/0891201042544884</idno>
		<ptr target="https://doi.org/10.1162/0891201042544884" />
		<title level="m">URL</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><surname>Fairseq</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-4009</idno>
		<ptr target="https://doi.org/10.18653/v1/n19-4009" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
				<editor>
			<persName><forename type="first">W</forename><surname>Ammar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Louis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Mostafazadeh</surname></persName>
		</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">June 2-7, 2019. 2019</date>
			<biblScope unit="page" from="48" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
		<ptr target="https://aclanthology.org/P02-1040/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">July 6-12, 2002. 2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A call for clarity in reporting BLEU scores</title>
		<author>
			<persName><forename type="first">M</forename><surname>Post</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/W18-6319" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Research Papers</title>
				<meeting>the Third Conference on Machine Translation: Research Papers<address><addrLine>Belgium, Brussels</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10">October 2018</date>
			<biblScope unit="page" from="186" to="191" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Glancing transformer for nonautoregressive neural machine translation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2021.acl-long.155" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</title>
		<title level="s">Long Papers</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Zong</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Navigli</surname></persName>
		</editor>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">August 1-6, 2021. 2021a</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1993" to="2003" />
		</imprint>
	</monogr>
	<note>Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The volctrans GLAT system: Non-autoregressive translation meets WMT21</title>
		<author>
			<persName><forename type="first">L</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2021.wmt-1.17" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Conference on Machine Translation</title>
				<meeting>the Sixth Conference on Machine Translation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-11">November 2021b</date>
			<biblScope unit="page" from="187" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Latticebased search strategies for large vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Rohlicek</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.1995.479663</idno>
		<ptr target="https://doi.org/10.1109/ICASSP.1995.479663" />
	</analytic>
	<monogr>
		<title level="m">1995 International Conference on Acoustics, Speech, and Signal Processing, ICASSP &apos;95</title>
				<meeting><address><addrLine>Detroit, Michigan, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1995">May 08-12, 1995. 1995</date>
			<biblScope unit="page" from="576" to="579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Combining outputs from multiple machine translation systems</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Rosti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">F</forename><surname>Ayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Matsoukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Dorr</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/N07-1029/" />
	</analytic>
	<monogr>
		<title level="m">Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings</title>
				<editor>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Sidner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Stone</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</editor>
		<meeting><address><addrLine>Rochester, New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">April 22-27, 2007. 2007</date>
			<biblScope unit="page" from="228" to="235" />
		</imprint>
	</monogr>
	<note>The Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Nonautoregressive machine translation with latent alignments</title>
		<author>
			<persName><forename type="first">C</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.83</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.emnlp-main.83" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
				<editor>
			<persName><forename type="first">B</forename><surname>Webber</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Cohn</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</editor>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020-11-16">2020. November 16-20, 2020. 2020</date>
			<biblScope unit="page" from="1098" to="1108" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p16-1162</idno>
		<ptr target="https://doi.org/10.18653/v1/p16-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<title level="s">Long Papers. The Association for Computer Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">August 7-12, 2016. 2016</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Minimizing the bag-of-ngrams difference for nonautoregressive neural machine translation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://aaai.org/ojs/index.php/AAAI/article/view/5351" />
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020">February 7-12, 2020. 2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="198" to="205" />
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Mixture models for diverse machine translation: Tricks of the trade</title>
		<author>
			<persName><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/shen19c.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
				<editor>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019-06-15">9-15 June 2019. 2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="5719" to="5728" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Latentvariable non-autoregressive neural machine translation with deterministic inference using a delta posterior</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nakayama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<ptr target="https://aaai.org/ojs/index.php/AAAI/article/view/6413" />
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020">February 7-12, 2020. 2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="8846" to="8853" />
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">An EM approach to non-autoregressive conditional sequence generation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v119/sun20c.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
				<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-07-18">13-18 July 2020. 2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="9249" to="9258" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Fast structured decoding for sequence models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Alché-Buc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/hash/74563ba21a90da13dacf2a73e3ddefa7-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">B</forename><surname>Garnett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. December 8-14, 2019. 2019</date>
			<biblScope unit="page" from="3011" to="3020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Lattice minimum bayes-risk decoding for statistical machine translation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tromble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/D08-1065/" />
	</analytic>
	<monogr>
		<title level="m">2008 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-10-27">2008. 25-27 October 2008. 2008</date>
			<biblScope unit="page" from="620" to="629" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Generation of word graphs in statistical machine translation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ueffing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<idno type="DOI">10.3115/1118693.1118714</idno>
		<ptr target="https://aclanthology.org/W02-1021/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2002 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Philadelphia, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-07-06">2002. July 6-7, 2002. 2002</date>
			<biblScope unit="page" from="156" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017</title>
				<editor>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Garnett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename></persName>
		</editor>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">December 4-9, 2017. 2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Imitation learning for non-autoregressive neural machine translation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1125</idno>
		<ptr target="https://doi.org/10.18653/v1/p19-1125" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<title level="s">Long Papers</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Korhonen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Traum</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Màrquez</surname></persName>
		</editor>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-08-02">July 28-August 2, 2019. 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1304" to="1312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Posconstrained parallel decoding for non-autoregressive generation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lv</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.467</idno>
		<ptr target="https://doi.org/10.18653/v1/2021.acl-long.467" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</title>
				<editor>
			<persName><forename type="first">C</forename><surname>Zong</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Navigli</surname></persName>
		</editor>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">August 1-6, 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Long Papers</note>
	<note>Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Understanding knowledge distillation in non-autoregressive machine translation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BygFVAEKDH" />
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
				<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">April 26-30, 2020. 2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
