<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Laplacian Regularized Low-Rank Representation and Its Applications</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ming</forename><surname>Yin</surname></persName>
							<email>yiming@gdut.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Junbin</forename><surname>Gao</surname></persName>
							<email>jbgao@csu.edu.au</email>
						</author>
						<author>
							<persName><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
							<email>zlin@pku.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Guangdong University of Tech-nology</orgName>
								<address>
									<postCode>510006</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Ministry of Education</orgName>
								<orgName type="laboratory">Key Laboratory of Au-tonomous Systems and Networked Control</orgName>
								<address>
									<postCode>510640</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Computing and Mathematics</orgName>
								<orgName type="institution">Charles Sturt University</orgName>
								<address>
									<postCode>2795</postCode>
									<settlement>Bathurst</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (MOE)</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country>P. R. China. He</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Cooperative Medianet Innovation Center</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country>China. Zhouchen Lin</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Laplacian Regularized Low-Rank Representation and Its Applications</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6062306E885753CB96410C60850067EE</idno>
					<idno type="DOI">10.1109/TPAMI.2015.2462360</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2015.2462360, IEEE Transactions on Pattern Analysis and Machine Intelligence IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2015.2462360, IEEE Transactions on Pattern Analysis and Machine Intelligence This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2015.2462360, IEEE Transactions on Pattern Analysis and Machine Intelligence IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 3</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Low-Rank Representation</term>
					<term>Graph</term>
					<term>Hyper-Laplacian</term>
					<term>Manifold Structure</term>
					<term>Laplacian Matrix</term>
					<term>Regularization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Low-Rank Representation (LRR) has recently attracted a great deal of attention due to its pleasing efficacy in exploring low-dimensional subspace structures embedded in data. For a given set of observed data corrupted with sparse errors, LRR aims at learning a lowest-rank representation of all data jointly. LRR has broad applications in pattern recognition, computer vision and signal processing. In the real world, data often reside on low-dimensional manifolds embedded in a highdimensional ambient space. However, the LRR method does not take into account the non-linear geometric structures within data, thus the locality and similarity information among data may be missing in the learning process. To improve LRR in this regard, we propose a general Laplacian regularized lowrank representation framework for data representation where a hypergraph Laplacian regularizer can be readily introduced into, i.e., a Non-negative Sparse Hyper-Laplacian regularized LRR model (NSHLRR). By taking advantage of the graph regularizer, our proposed method not only can represent the global lowdimensional structures, but also capture the intrinsic non-linear geometric information in data. The extensive experimental results on image clustering, semi-supervised image classification and dimensionality reduction tasks demonstrate the effectiveness of the proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>L OW-Rank Representation (LRR) [30], [33], as a promis- ing method to capture the underlying low-dimensional structures of data, has attracted great interest in the pattern analysis and signal processing communities. Specifically, the problems involving the estimation of low-rank matrices have drawn considerable attention in recent years. LRR has been widely used in subspace segmentation <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b48">[49]</ref>, image destriping <ref type="bibr" target="#b34">[35]</ref>, image clustering <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b55">[56]</ref> and video background/foreground separation <ref type="bibr" target="#b0">[1]</ref>. The low-rank regularizer in LRR has a deep link with the recent theoretical advances on robust principal component analysis (RPCA) <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, which leads to new and powerful modeling options for many applications.</p><p>Under the practical assumption that observed data all lie near some low-dimensional subspaces, the data matrix stacked from all the vectorized observations should be approximately of low rank <ref type="bibr" target="#b43">[44]</ref>. The conventional Principal Component Analysis (PCA) <ref type="bibr" target="#b23">[24]</ref> is one of ways to recover the best lowrank representation in terms of 2 errors. It is well known that PCA, widely applied in computer vision and other research areas, is fragile to outliers or sparse/extreme errors introduced by occlusion and disguise <ref type="bibr" target="#b25">[26]</ref>. Researchers have long been working on different kinds of approaches aiming to robustify PCA and to recover a low-dimensional subspace from corrupted data, such as 1 PCA <ref type="bibr" target="#b16">[17]</ref> and robust PCA <ref type="bibr" target="#b9">[10]</ref>. These robust PCA schemes show that 1 -type noise models can offer better robustness than the conventional 2 -type ones. Although most of robust PCA approaches can achieve resilience with respect to grossly corrupted observation data, unfortunately none of them yields a polynomial time algorithm under broad conditions <ref type="bibr" target="#b9">[10]</ref>. Since these methods are modified by adding extra regularization terms, the problems usually become nonconvex and corresponding optimization algorithms can easily get stuck at local minima, resulting in performance degradation.</p><p>The LRR method <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b32">[33]</ref> focuses on lowrank data representation, based on the hypothesis that data approximately jointly span several low-dimensional subspaces. The authors of <ref type="bibr" target="#b31">[32]</ref> generalize LRR model to take care of largely contaminated outliers by incorporating a 1 / 2 noise model and prove that under mild technical conditions, the LRR model exactly recovers the subspace of samples and detect the outliers as well. Thus LRR can accurately recover the row space of the original data and detect outliers under mild conditions <ref type="bibr" target="#b29">[30]</ref>. In general, the resulting problem, which minimizes a combination of the nuclear norm and the 1 -norm, is convex and can be solved in polynomial time <ref type="bibr" target="#b9">[10]</ref>. In order to handle the cases where the number of observed data is insufficient or data themselves are too badly corrupted, Liu and Yan <ref type="bibr" target="#b32">[33]</ref> further proposed a latent low-rank representation approach. In the latent LRR, hidden data can be regarded as the input data matrix after being transposed. This idea has been recently used in designing a classifier for image classification <ref type="bibr" target="#b3">[4]</ref>. As for LRR, only the row space information is recovered and the column space information of the input data matrix is not sufficiently exploited for learning subspace structures. To address this issue, Yin et al. <ref type="bibr" target="#b48">[49]</ref> proposed a novel approach, termed double LRR, which simultaneously learns the row space and column space information embedded in a given dataset.</p><p>If all the data in a high-dimensional space actually lie on the union of several linear subspaces, LRR can easily pick up the low-dimensional structures embedded in the data. However, in real world applications the assumption cannot be ensured. For instance, it is well known that facial images are sampled from a non-linear low-dimensional manifold which is embedded in a high-dimensional ambient space <ref type="bibr" target="#b21">[22]</ref>. Therefore, in this case, the LRR method may fail to discover the intrinsic geometric and discriminating structures of data <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b51">[52]</ref>, which is essential to actual applications.</p><p>To preserve local geometric structures embedded in a highdimensional space, numerous researchers have considered the manifold learning methods, such as the locally linear embedding (LLE) <ref type="bibr" target="#b36">[37]</ref>, ISOMAP <ref type="bibr" target="#b39">[40]</ref>, Locality Preserving Projection (LPP) <ref type="bibr" target="#b21">[22]</ref>, Neighborhood Preserving Embedding (NPE) <ref type="bibr" target="#b20">[21]</ref> and Laplacian Eigenmap (LE) <ref type="bibr" target="#b1">[2]</ref>. All these algorithms are motivated by the idea of the so-called local invariance <ref type="bibr" target="#b18">[19]</ref>, which aims to estimate geometric and topological properties of an unknown manifold from random points (scattered data) lying around it.</p><p>In practice, it is reasonable to assume that if two data points are close in the intrinsic manifold of the data distribution, then the representations of these two points in a new space are close to each other too <ref type="bibr" target="#b19">[20]</ref>. In recent years, this observation inspires Zheng et al. <ref type="bibr" target="#b51">[52]</ref> to propose a graph regularized sparse coding to learn a sparse representation that explicitly takes into account the local manifold structures of data. Similarly, Gao et al. <ref type="bibr" target="#b17">[18]</ref> also proposed two Laplacian regularized sparse codings, termed as Laplacian sparse coding (LSc) and Hypergraph Laplacian sparse coding (HLSc) by incorporating a similarity preserving term into the objective of sparse coding. In <ref type="bibr" target="#b5">[6]</ref>, Cai et al. developed a graph based approach for non-negative matrix factorization <ref type="bibr" target="#b24">[25]</ref> of data representation in order to address the failure in representing geometric structures in data. To exploit the intrinsic geometry of the data distribution, He et al. <ref type="bibr" target="#b19">[20]</ref> proposed a Laplacian regularized Gaussian Mixture Model (LapGMM) based on manifold structures for data clustering. What is more, from the matrix factorization perspective, a novel low-rank matrix factorization model that incorporates manifold regularization into matrix factorization is proposed in <ref type="bibr" target="#b50">[51]</ref>. More recently, to comprehensively consider the high spectral correlation between the observed subimages in different bands and the local manifold structures of the hyperspectral data space, Lu et al. <ref type="bibr" target="#b34">[35]</ref> proposed a novel graph-regularized LRR destriping approach by incorporating the LRR technique. Their formulation shares the similar idea as that used in our proposed model, but with the purpose of removing striping noise in hyperspectral images. In contrast, we consider data clustering and classification, and further incorporate both the sparsity and non-negative constraints in our model. Motivated by the above works, in this paper, we propose a non-negative sparse hyper-Laplacian regularized low-rank representation model, or NSHLRR for short, for image representation. Note that here we use a hypergraph, instead of a normal graph, to describe similarity structures among data in order to introduce a general manifold regularization to the LRR model. Although the manifolds that data points reside near are non-linear, every data point can still be linearly represented by points in its neighborhood because any manifold can be locally approximated by linear subspaces. So we still can characterize the local geometry of data by the linear coefficients that reconstruct each data point from its neighbors, which are mostly on the same manifold unless the to-be-represented data point is far away along the manifold. Hence the optimal representation by our proposed model is close to block-diagonal, which facilitates non-linear manifold clustering.</p><p>In summary, our main contributions in this paper lie in the following three aspects: Recently, many literatures <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b55">[56]</ref> have shown that the non-negative constraint leads to a parts-based representation for data, which can make the model more physically interpretable. Since we use locally linear subspaces to approximate the non-linear manifold, it is better if the sample to be represented is at the center of the locally linear subspace so that the approximation is valid. This is distinct from the model proposed in <ref type="bibr" target="#b34">[35]</ref>. The remainder of this paper is organized as follows. In Section II, we give a brief review on related works. Section III is dedicated to introducing the proposed NSHLRR. In Section IV, we apply the linearized alternating direction method with adaptive penalty (LADMAP) <ref type="bibr" target="#b28">[29]</ref> to solve the optimization problem of NSHLRR. Section V presents experimental results on image clustering and semi-supervised image classification tasks. The results on applying the proposed model to hypergraph-based applications are provided in Sections VI and VII. Finally, Section VIII concludes our paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>In recent years, the success of low-rank matrix and graph representations has inspired researchers to extend these representations by jointly considering the two approaches. Before we introduce our model, in this section, we review the recent development of LRR <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b32">[33]</ref> and graph based analysis <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b51">[52]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Low-Rank Matrix Recovery Formulation</head><p>The LRR model <ref type="bibr" target="#b29">[30]</ref> is based on the assumption that data are approximately sampled from a union of multiple lowdimensional subspaces. Given a set of data samples, LRR aims at finding the lowest-rank representation of all data jointly. It has been demonstrated that LRR is very effective in exploring low-dimensional subspace structures embedded in data <ref type="bibr" target="#b29">[30]</ref>.</p><p>Consider the case that data Y are drawn from a union of multiple subspaces given by k i=1 S i , where S 1 , S 2 , ..., S k are low-dimensional subspaces. The LRR model for the given data Y is defined as the following rank minimization problem</p><formula xml:id="formula_0">min Z,E rank(Z) + λ E 0 s.t. Y = AZ + E,<label>(1)</label></formula><p>where the columns of A are a set of known bases or dictionary items, E denotes the error components, • 0 is the 0 pseudonorm, i.e., the number of nonzeros in a matrix or vector, and λ is a penalty parameter for balancing the low-rank term and the reconstruction fidelity. As it is difficult to solve the above optimization problem (1) due to the discrete nature of the rank function and the 0 pseudo-norm, a convex relaxation of the above optimization problem is proposed as</p><formula xml:id="formula_1">min Z,E Z * + λ E 1 s.t. Y = AZ + E.<label>(2)</label></formula><p>where Z * is the nuclear norm, defined as the sum of all singular values of Z, which is the convex envelope of the rank function <ref type="bibr" target="#b8">[9]</ref>, and E 1 is the 1 norm, defined as the sum of absolutes of all entries, which is the convex envelope of the 0 pseudo-norm. In fact, it has been proved in <ref type="bibr" target="#b29">[30]</ref> that in the noise free case the solution to (2) is also the solution to <ref type="bibr" target="#b0">(1)</ref>. Given a low rank solution Z to (2) or (1), the clean representation AZ for data Y is also of low rank. Denote X = AZ. Instead of solving the optimization problem with respect to the representation coefficient matrix Z, we may solve for the clean data X directly. This links LRR with Robust PCA (RPCA) <ref type="bibr" target="#b9">[10]</ref>, whose optimization problem is min</p><formula xml:id="formula_2">X,E X * + λ E 1 s.t. Y = X + E.<label>(3)</label></formula><p>RPCA is broadly used in the computer vision community <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b49">[50]</ref>. RPCA only aims to recover the low-rank clean data from given noisy data. Compared to model (3), the LRR model is able to reveal the implicit data membership. To see this, let X = [x 1 , x 2 , ..., x n ] be the matrix whose columns are clean data samples drawn from independent subspaces {S i }. Given a dictionary A = [a 1 , a 2 , ..., a m ], or more generally a dictionary of atoms, linearly spanning the whole data space, we can represent the data as X = AZ, where Z = [z 1 , z 2 , ..., z n ] is the coefficient matrix with each z i being the representation of x i . Under an appropriately designed dictionary A, the optimal solution Z * of LRR can actually reveal some underlying affinity between data points, which can be exploited to uncover the intrinsic structures of data. In <ref type="bibr" target="#b29">[30]</ref>, the dictionary A is chosen as the given data Y . In this case, the coefficient matrix element z ij can be explained as the "similarity" between data points y i and y j .</p><p>A number of methods have been proposed for solving lowrank matrix problems, such as singular value thresholding <ref type="bibr" target="#b7">[8]</ref>, accelerated proximal gradient (APG) <ref type="bibr" target="#b27">[28]</ref>, and augmented Lagrange Multiplier Method (ALM) <ref type="bibr" target="#b26">[27]</ref>. Lin et al. <ref type="bibr" target="#b28">[29]</ref> proposed a fast method, termed linearized alternating direction method with adaptive penalty (LADMAP), which uses less auxiliary variables and no matrix inversions, and hence converges faster than the usual alternating direction method (ADM) <ref type="bibr" target="#b42">[43]</ref>. Most recently, in order to handle the multi-block cases, Lin et al. <ref type="bibr" target="#b33">[34]</ref> further proposed a new algorithm, called LADMPSAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Graph Based Manifold Analysis</head><p>Without loss of generality, suppose Y = [y 1 , y 2 , ..., y n ] are sampled from an underlying manifold M. Since a non-linear manifold can be locally approximated by linear subspaces, it is reasonable to assume that the relationship between a data point and its neighbors is linear <ref type="bibr" target="#b2">[3]</ref>. Thus, the local geometry of these data points can be effectively characterized by the linear coefficients that reconstruct each data point from its neighbors. Given data Y ∈ R d×n , we construct a k nearest neighbor graph G with n vertices, where each vertex denoting a data point. We also define a symmetric weight matrix W ∈ R n×n , in which W ij is the weight of the edge joining vertices i and j. That is, a weighted undirected graph G = (V, E; W ) is constructed with the weight matrix W , where V = {v i } n i=1 is the vertex set with each node v i corresponding to a data point y i , and E = {e ij } is the edge set with each edge e ij associating nodes v i and v j with a weight W ij . The value of W ij is set as follows.</p><formula xml:id="formula_3">W ij = 1 if y i ∈ N k (y j ) or y j ∈ N k (y i ) 0 otherwise<label>(4)</label></formula><p>where N k (y i ) denotes the set of k nearest neighbors of y i . Graph embedding is based on the natural assumption that if two data points y i and y j are close in the intrinsic geometry of the data distribution, their embedding/mappings in a new space are also close to each other. This assumption can be implemented in terms of similarity. So graph embedding aims at describing each vertex of the graph by a low-dimensional vector that preserves affinity between the vertices, where the similarity is measured by the edge weight. In mathematics, this relationship defined by the manifold assumption is formulated as</p><formula xml:id="formula_4">min {z k } ij z i -z j 2 W ij ,<label>(5)</label></formula><p>where z i and z j are the mappings of y i and y j under some transformation, respectively. This rule plays a critical role in developing various kinds of algorithms, including dimensionality reduction <ref type="bibr" target="#b46">[47]</ref>, clustering <ref type="bibr" target="#b4">[5]</ref> and semi-supervised learning <ref type="bibr" target="#b55">[56]</ref>. The degree matrix is defined as D, which is a diagonal matrix, whose ith diagonal entry D ii corresponds to the summation of all the similarities related to y i , i.e., D ii = j W ij . Then the graph Laplacian matrix is defined by</p><formula xml:id="formula_5">[14] L = D -W.</formula><p>It is easy to prove that the graph embedding ( <ref type="formula" target="#formula_4">5</ref>) can be rewritten as min tr(ZLZ T ).</p><p>In many applications, however, the relationship among the data of interest is more complicated than pairwise, which cannot be simply represented by a normal graph. For a long time, the hypergraph relations have often been transformed into a normal graph, which is easier to handle <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b53">[54]</ref>. If this complex relationship is simplified into pairwise one, it is inevitable to lose some useful information for targeted learning tasks. To effectively represent the complex relationship among given data, the concept of hypergraph is employed in manifold learning tasks <ref type="bibr" target="#b53">[54]</ref>, such as clustering, classification and dimensionality reduction via embedding. In fact, a hypergraph is an extension of a normal graph, in which an edge can "connect" more than two vertices.</p><p>For the convenience of subsequent presentation, we here introduce some basic notations on hypergraphs. Given a hypergraph G = (V, E), V denotes the vertex set and E represents the hyperedge set, in which each e ∈ E is a subset of vertices. The weight corresponding to a hyperedge e is denoted by W (e), which is usually a non-negative number. The degree of a vertex v is defined as and the diagonal degree matrix D E consists of diagonal entries d(e). Similarly, denote D V the diagonal matrix whose diagonal entries correspond to the degree of each vertex. Then the unnormalized hyper-Laplacian matrix <ref type="bibr" target="#b53">[54]</ref> can be defined as</p><formula xml:id="formula_6">d(v) = e∈E W (e)h(v, e),</formula><formula xml:id="formula_7">L h = D V -HW E D -1 E H T ,<label>(6)</label></formula><p>where W E is the diagonal matrix whose diagonal entries are the edge weights W (e).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. LAPLACIAN REGULARIZED LOW-RANK REPRESENTATION</head><p>In this section, a novel LRR model with Laplacian regularization is proposed, in which we consider the local manifold structures of given data. Note that, under the new model, the coefficients Z not only capture the global structures of the whole data, but preserve the local geometric structures among data points. Different from the graph regularized sparse representation <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b51">[52]</ref>, our proposed model can effectively represent data by themselves without relying on a large dictionary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Non-negative Sparse Hyper-Laplacian Regularized LRR</head><p>If we use data Y itself as the dictionary A in (2), there are two explanations to the coefficient matrix Z. Firstly, the element z ij of Z reflects the "similarity" between data pair y i and y j , hence Z is sometimes called an affinity matrix. Secondly, we can explain each column z i of Z as a new representation of the data point y i in terms of other samples in Y . In other words, z i can be used as a representation for y i .</p><p>To introduce richer information over Z, people consider imposing some helpful regularizations on the coefficient matrix, such as sparsity <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b44">[45]</ref> and a non-negative constraint <ref type="bibr" target="#b24">[25]</ref>. For example, given that the sparsity criterion can better capture the local structure around each data vector, a non-negative sparse LRR model can be formulated as follows</p><formula xml:id="formula_8">min Z,E Z * + λ Z 1 + γ E 1 , s.t. Y = Y Z + E, Z ≥ 0.<label>(7)</label></formula><p>In fact, in order to deal with grossly corrupted data, Zhuang et al. <ref type="bibr" target="#b55">[56]</ref> proposed a non-negative low-rank and sparse (NNLRS) graph for semi-supervised learning with 2,1 -norm for the error term E.</p><p>Motivated by the graph based manifold learning <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b13">[14]</ref>, we here can incorporate the Laplacian regularization into the objective function ( <ref type="formula" target="#formula_8">7</ref>) so that similar data points have similar representation coefficients. Specifically denote by W the similarity measure for the dataset, for example as defined in (4). As the columns of Z are new representations of the data under certain dictionary, the distance between z i and z j is actually one of dissimilarity measures for the original data points y i and y j . Thus we can add a similarity matching regularization term i,j z i -z j 2 W ij to the objective function <ref type="bibr" target="#b6">(7)</ref>. However, to assume the pairwise relationship between complicated data points is far from complete, especially on high order relationship in gene data, web images and co-authorship articles, etc. This relationship cannot be simply represented by a normal graph since the information that helps our grouping task would be lost. Therefore, we consider a more general case, instead of pairs, by introducing a hyper-Laplacian regularizer.</p><p>According to the hypergraph definition, we enforce the LRR coefficients corresponding to data points within the same hyperedge be similar to each other. Instead of using the normal weight W ij , we weight the summation of pairwise distances among the given data points within each hyperedge e by W (e)/d(e).</p><p>Then the NSHLRR is formulated as</p><formula xml:id="formula_9">min Z,E Z * + λ Z 1 + β (i,j)⊂e∈E z i -z j 2 W (e) d(e) + γ E 1 , s.t. Y = Y Z + E, Z ≥ 0.<label>(8)</label></formula><p>The non-negative constraint on Z aims to guarantee that each data point is in the middle of its neighbors, which is more meaningful and can better embody the dependency among the data points than otherwise.</p><p>As can be seen, it is easy to transform the formulation of hyper-Laplacian regularizer to the case of graph Laplacian regularizer, when each hyperedge only contains two vertices, i.e, d(e) = 2. In fact, a hypergrah is a generalized model of the normal graph, where the edges are arbitrary non-empty subsets of the vertex set. If the subset of the vertex set is degenerated to only two vertices, we have a special case of (8), i.e., a Non-negative Sparse Laplacian regularized LRR model, termed NSLLRR, defined as follow</p><formula xml:id="formula_10">min Z,E Z * + λ Z 1 + β ij z i -z j 2 W ij + γ E 1 , s.t. Y = Y Z + E, Z ≥ 0.<label>(9)</label></formula><p>Therefore, we are actually considering a generalized version of the graph regularized LRR model. The advantage of this generalized model is as follows. By introducing a hypergraph regularization term, we can effectively preserve high order affinity among samples belonging to the same hyperedge so as to keep the manifold structures of data. In particular, it is more powerful to explore all-round relations among more data points.</p><p>Finally, by some algebraic manipulations we have the matrix form of hyper-Laplacian regularized LRR model min</p><formula xml:id="formula_11">Z,E Z * + λ Z 1 + βtr(ZL h Z T ) + γ E 1 , s.t. Y = Y Z + E, Z ≥ 0,<label>(10)</label></formula><p>where L h is the hyper-Laplacian matrix defined in ( <ref type="formula" target="#formula_7">6</ref>) and λ, β and γ are penalty parameters for balancing the regularization terms.</p><p>From <ref type="bibr" target="#b9">(10)</ref>, it is easy to rewrite the above formulation ( <ref type="formula" target="#formula_10">9</ref>) as</p><formula xml:id="formula_12">min Z,E Z * + λ Z 1 + βtr(ZLZ T ) + γ E 1 , s.t. Y = Y Z + E, Z ≥ 0,<label>(11)</label></formula><p>where L is the Laplacian matrix for the graph built on W and other parameters are similar to those in the NSHLRR model <ref type="bibr" target="#b9">(10)</ref>. In this model, we introduce the sum of distance of pairwise LRR coefficients to the traditional LRR model, where the distance is weighted by the similarity between the given data points.</p><p>In our model <ref type="bibr" target="#b9">(10)</ref>, the term Y = Y Z ensures each sample can be linearly represented by its neighbors, i.e., the representation can still be written as Y = Y Z + E by taking noise effect into account. In fact, this term is not contradictory to the assumption of a non-linear manifold. The data itself could reside on a union of non-linear manifolds. However, except near the intersections, the non-linear manifolds can all be locally approximated by linear subspaces, centered at every sample point. In addition, the Laplacian and the sparsity constraints encourage choosing nearby samples (which most likely belong to the same cluster), rather than the faraway samples (which may belong to other clusters), to represent the sample at the center of the locally linear manifold. So the optimum Z will be close to block-diagonal. Then the subsequent normalized cut can correct some of the errors in Z, resulting in pleasing manifold clustering results.</p><p>As for the non-negative constraint, i.e., Z ≥ 0, our idea originates from non-negative matrix factorization which shows better results in clustering tasks. It has been noted in many literatures <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b55">[56]</ref> that the non-negative constraint actually leads to a parts-based representation for data which can render the model more physically interpretable. In LRR, the columns of the coefficient matrix Z can be regarded as the representation of y i 's. Since we use locally linear manifold to approximate the non-linear manifold, it is better that the sample to be represented is at the center of the locally linear manifold so that the approximation is valid. To this end, we require Z to be non-negative. The advantage of this constraint is also demonstrated by our subsequent experimental results.</p><p>In light of the definition of hypergraph, the hyper-Laplacian regularized LRR model is a more general one compared to the Laplacian regularized LRR model. It is obvious that the NSHLRR model <ref type="bibr" target="#b9">(10)</ref> becomes the NSLLRR model <ref type="bibr" target="#b10">(11)</ref> when there exists a hyperedge for every vertex pair and each hyperedge just includes two vertices. On the other hand, the NSLLRR model <ref type="bibr" target="#b10">(11)</ref> can also be changed into the NSHLRR model <ref type="bibr" target="#b9">(10)</ref> under some special definition of the weight W <ref type="bibr" target="#b17">[18]</ref>. To this end, we can unify the way to apply some manifold constraint into the LRR framework for better exploiting the geometric structures of data space.</p><p>In the above viewpoint, the graph-regularized LRR (GLRR) proposed in <ref type="bibr" target="#b34">[35]</ref> is actually a special case of our NSHLRR model merely considering the pairwise relation between data points. The similar idea can be seen in another recent work <ref type="bibr" target="#b52">[53]</ref>. However, the authors used the second explanation of Z, i.e., z ij , is the similarity between the data points y i and y j . Hence the corresponding regularized term is defined as</p><formula xml:id="formula_13">|z ij | y i -y j 2 .</formula><p>It can also be easily extended to incorporate all-round relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Laplacian Regularized LRR for Unsupervised Learning</head><p>According to the principle of LRR, Z is actually a new representation of each data point using the dataset itself as the dictionary. Specifically, Z reflects the affinity between the data pair y i and y j , which can be used for data clustering <ref type="bibr" target="#b32">[33]</ref>. In recent years, many similarity-based clustering algorithms have emerged, such as K-means and spectral clustering method <ref type="bibr" target="#b37">[38]</ref>, which do not require explicit assumptions on the distribution of data. In this regard, our proposed method should greatly benefit image clustering as it can effectively learn similarity among samples by incorporating data neighborhood information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Laplacian Regularized LRR for Semi-supervised Learning</head><p>When we consider the case that a hyperedge just includes two vertices, NSLLRR not only can perform well in unsupervised learning, but also can be adopted for semi-supervised learning effectively. In the pattern recognition community, semi-supervised learning has been attracting considerable attention over the past decades <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b55">[56]</ref> because labeled samples are hard or expensive to acquire whereas unlabeled ones are easy and inexpensive. Recently, graph-based learning methods have been widely used to develop high performance algorithms for semi-supervised learning tasks <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b55">[56]</ref>.</p><p>In general, most of graph-based semi-supervised learning algorithms use a common assumption on the well-posedness of the constructed graph. However, this assumption is not always true because some parameters need manual settings when constructing graphs. Furthermore, the graph construction process is critical for the performance of algorithms. Given that our proposed model well preserves the locality and the similarity of data, we expect that our proposed method also has the potential to convey more discriminative information than the traditional graph-based methods do.</p><p>In this section, we present a novel semi-supervised learning framework incorporating the above proposed Laplacian regularized LRR model. The task is to assign the unlabeled samples with proper labels. A popular way is to compute the similarities between samples first, and then propagate the labels from the known samples to unknown ones, using the principle that similar samples are likely to be in the same class. Therefore, one of the critical steps is to compute the affinity matrix of data at first. Then, the objective of graph based semi-supervised learning can be formulated as follows <ref type="bibr" target="#b46">[47]</ref> min</p><formula xml:id="formula_14">{f k } ij W l ij f i -f j 2 ,</formula><p>where W l ij is graph weight possibly related to label information, f i and f j denote the probability distributions of y i and y j being different classes, respectively.</p><p>Under semi-supervised learning framework, we denote a label matrix B = [B l ; B u ] ∈ R |V |×c where B l represents the label information for the labeled data, B u is the information for the unlabeled data and c denotes the number of the classes. If a sample y i is associated with label k ∈ {1, ..., c}, then B ik = 1. Otherwise, B ik = 0. Furthermore, we can define a classification function F = [F l ; F u ] ∈ R |V |×c . F l and F u include the class probability vectors for the labeled and unlabeled samples, respectively. Thus, mathematically the objective of graph based semi-supervised learning is readily transferred into</p><formula xml:id="formula_15">min {f k } ij W l ij f i -f j 2 = tr F T L w F ,<label>(12)</label></formula><p>where L w is the graph Laplacian matrix corresponding to a weighted graph G l = (V, E; W l ). The graph G l is constructed via using relationship among the labeled and unlabeled data points, where edge weights encode the similarity between samples. To achieve this target, our work focuses on how to effectively learn the graph. Specifically, in this paper, instead of explicitly designing weights W l , we directly choose L w to be the Laplacian of the symmetrized affinity matrix</p><formula xml:id="formula_16">1 2 |Z| + |Z T |</formula><p>, where Z is learned by the Laplacian regularized LRR method. In other words, the graph we are using is the one learned from the data.</p><p>In order to efficiently predict the labels of unlabeled samples, we adopt a Gaussian field, rather than a random field, over the label set as the basis for semi-supervised classification. That is, we form a Gaussian field to assign a probability distribution of f i . Then the minimum energy function ( <ref type="formula" target="#formula_15">12</ref>) is harmonic. Namely, it should simultaneously satisfy L w F = 0 on unlabeled data points, denoted by (L w F ) u = 0, and F l = B l on labeled ones. Please refer to <ref type="bibr" target="#b54">[55]</ref> for more details.</p><p>Based on the above analysis, we finally learn a Gaussian Harmonic Function (GHF) <ref type="bibr" target="#b54">[55]</ref>, which combines Gaussian field and harmonic function, to realize the label propagation, formulated as follows min</p><formula xml:id="formula_17">F ∈R |V |×c tr F T L w F , s.t. (L w F ) u = 0, F l = B l .</formula><p>In our case, the matrix L w is positively definite, hence the above question is convex. The solution is given by F u = --1 uu ul F l through separating L w into blocks for labeled and unlabeled nodes <ref type="bibr" target="#b54">[55]</ref>, such as</p><formula xml:id="formula_18">L w = ll lu ul uu</formula><p>.</p><p>Hence an error rate of graph based semi-supervised learning method can be computed using the propagated label information. The above algorithm is actually a generalization of the case of binary classes in <ref type="bibr" target="#b54">[55]</ref> to the multiple classes. The theoretical foundation is based on the maximum principle of harmonic function <ref type="bibr" target="#b14">[15]</ref>, and the solution is unique and satisfies 0 &lt; F u (j) &lt; 1 for each unlabeled data point y j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. LADMAP FOR SOLVING LAPLACIAN REGULARIZED LRR</head><p>In recent years, a lot of algorithms have been proposed for solving low-rank optimization problems <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b43">[44]</ref>. In particular, the alternating direction method (ADM) has drawn considerable attention <ref type="bibr" target="#b42">[43]</ref>. ADM updates the variables alternately by minimizing the augmented Lagrangian function in a Gauss-Seidel manner. We also apply ADM to solve our optimization problems. Unfortunately, directly applying ADM to solve <ref type="bibr" target="#b9">(10)</ref> results in sub-problems as follows</p><formula xml:id="formula_19">min Z Z * + λ 2 C (Z) -D 2 F (<label>13</label></formula><formula xml:id="formula_20">)</formula><p>where C is a linear mapping and • F is the matrix Frobenius norm defined as</p><formula xml:id="formula_21">Y 2 F = m i=1 n j=1 |Y ij | 2 .</formula><p>When C is the identity mapping, (13) has a closed form solution <ref type="bibr" target="#b7">[8]</ref>. However, when C is not the identity mapping, (13) can only be solved iteratively. For example, Lu et al. <ref type="bibr" target="#b34">[35]</ref> solved (13) by the accelerated gradient method <ref type="bibr" target="#b22">[23]</ref>, making the overall algorithm inefficient.</p><p>To remedy this issue in solving <ref type="bibr" target="#b9">(10)</ref>, we adopt the Linearized ADM with Adaptive Penalty (LADMAP) <ref type="bibr" target="#b28">[29]</ref> instead. We first introduce an auxiliary variable J in order to make the objective function of (10) separable. Thus the optimization problem can be rewritten as follows</p><formula xml:id="formula_22">min Z,J,E Z * + λ J 1 + βtr(ZL h Z T ) + γ E 1 , s.t. Y = Y Z + E, Z = J, J ≥ 0. (<label>14</label></formula><formula xml:id="formula_23">)</formula><p>The augmented Lagrangian function of problem <ref type="bibr" target="#b13">(14)</ref> is</p><formula xml:id="formula_24">L(J, E, Z, Y 1 , Y 2 ) = Z * + λ J 1 + βtr(ZL h Z T ) + γ E 1 + M 1 , Y -Y Z -E + M 2 , Z -J<label>(15)</label></formula><formula xml:id="formula_25">+ µ 2 ( Y -Y Z -E 2 F + Z -J 2 F ),</formula><p>where M 1 and M 2 are Lagrange multipliers and µ &gt; 0 is a penalty parameter. The variables are updated alternately by minimizing the augmented Lagrangian function, with other variables fixed. If the augmented Lagrangian function is difficult to minimize with respect to a variable, the smooth component of it can be linearized. Hence this algorithm bears the name Linearized Alternating Direction Method <ref type="bibr" target="#b28">[29]</ref>. Then the Lagrange multipliers are updated using the feasibility errors. The iteration stops when the convergence conditions are met. We provide details of solving <ref type="bibr" target="#b9">(10)</ref> with LADMAP in the following.</p><p>A. Updating Z</p><formula xml:id="formula_26">Updating Z by minimizing L(J k , E k , Z, M k 1 , M k 2 )</formula><p>is equivalent to minimizing the following objective function</p><formula xml:id="formula_27">L 1 = Z * + βtr(ZL h Z T ) + µ 2 Z -J k + 1 µ M k 2 2 F + µ 2 Y -Y Z -E k + 1 µ M k 1 2 F ,<label>(16)</label></formula><p>which does not have a closed-form solution. By the spirit of LADMAP <ref type="bibr" target="#b28">[29]</ref>, we denote the smooth component of L 1 by</p><formula xml:id="formula_28">q(Z, E k , J k , M k 1 , M k 2 ) =βtr(ZL h Z T ) + µ 2 Z -J k + 1 µ M k 2 2 F + µ 2 Y -Y Z -E k + 1 µ Y k 1 2 F .</formula><p>Then according to LADMAP, minimizing L 1 can be replaced by solving the following problem</p><formula xml:id="formula_29">min Z Z * + ∇ Z q(Z k ), Z -Z k + η 1 2 Z -Z k 2 F<label>(17)</label></formula><p>where</p><formula xml:id="formula_30">q(Z, E k , J k , M k 1 , M k 2 ) is approximated by its lin- earization ∇ Z q(Z k ), Z -Z k at Z k plus a proximal term η1 2 Z -Z k 2 F and ∇ Z q(Z k ) is the gradient of q w.r.t. Z. As long as η 1 &gt; 2β L 2 + µ(1 + Y 2 2 )</formula><p>, where • 2 is the spectral norm of a matrix, i.e., the largest singular value, the above replacement is valid. Then (17) has a closed-form solution given by</p><formula xml:id="formula_31">Z * k+1 = Θ (η1) -1 (Z k -∇ Z q (Z k ) /η 1 )<label>(18)</label></formula><p>where Θ ε (A) = U S ε (Σ)V T is the singular value thresholding operator (SVT) <ref type="bibr" target="#b7">[8]</ref>, in which U ΣV T is the singular value decomposition (SVD) of A and S ε (x) = sgn(x) max(|x|ε, 0) is the soft thresholding operator <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Updating J and E</head><p>We view (J, E) as a larger block of variables. We can update (J, E) by minimizing L(J, E, Z k+1 , M k 1 , M k 2 ), which naturally splits into to subproblems for J and E, respectively, as J and E are independent of each other in this minimization problem.</p><p>After some simple manipulation, the problem for updating E is</p><formula xml:id="formula_32">min E γ E 1 + µ 2 E -Y -Y Z + 1 µ M k 1 2 F ,<label>(19)</label></formula><p>which has the following closed-form solution [8]</p><formula xml:id="formula_33">E k+1 = S γ µ Y -Y Z k+1 + 1 µ M k 1 (20)</formula><p>Similarly, the problem for updating J is</p><formula xml:id="formula_34">min J≥0 λ J 1 + µ 2 J -Z + 1 µ M 2 2 F<label>(21)</label></formula><p>which has the following closed-form solution [8]</p><formula xml:id="formula_35">J k+1 = max S λ µ Z k+1 + 1 µ M k 2 , 0 .<label>(22)</label></formula><p>For faster convergence, µ can be adjusted using the adaptive updating strategy as suggested in <ref type="bibr" target="#b28">[29]</ref> (see step 5) in Algorithm 1). The detailed procedure of solving the proposed hyper-Laplacian regularized problem is described in Algorithm 1. Note that the Laplacian regularized LRR problem 11 can also be efficiently solved by Algorithm 1 simply replacing L h with L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Convergence and Complexity Analysis</head><p>The algorithm described above converges to a globally optimal solution of <ref type="bibr" target="#b9">(10)</ref> as it is a direct application of LADMAP.</p><p>When updating Z by singular value thresholding, see ( <ref type="formula" target="#formula_31">18</ref>), we may predict the rank r of Z k+1 using the rank prediction strategy described in <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b26">[27]</ref>, which grows from a small value and stablizes at the true rank when the iteration goes on. Moreover, as in <ref type="bibr" target="#b28">[29]</ref> we may use the Lanczos method to compute the leading singular values and singular vectors, which only requires multiplication of Zk = Z k -∇ Z q(Z k )/η 1 , and its transpose, with vectors, which can be efficiently computed by successive matrix-vector multiplication, rather than forming Zk explicitly and then multiplying it with vectors. With such treatments, the complexity of each iteration of Algorithm 1 is O(rn 2 ), where n is the number of samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. GRAPH REGULARIZED LRR FOR IMAGE CLUSTERING</head><p>AND IMAGE CLASSIFICATION Firstly, we consider the special case of our proposed model, i.e., normal graph based LRR, in this section. That is, each hyperedge for vertex pairs only includes two vertices and our model particularly refers to NSLLRR in this section. In order to investigate the performance of our approach, we conducted comprehensive experiments on both unsupervised learning (image clustering) and semi-supervised learning (image classification) tasks. All of the experiments were done on a PC with an Intel i3 3.30 GHz CPU and 8GB memory, running Windows 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. A Synthetic Example</head><p>Firstly, we test with a synthetic dataset. We consider a set of data points constructed in two moons pattern, as shown in Fig. <ref type="figure" target="#fig_2">1(a)</ref>. There are two natural clusters, i.e., the two half moons intersecting each other. By using the conventional LRR method, we can achieve the result shown in Fig. <ref type="figure" target="#fig_2">1(b</ref>) where the algorithm failed to distinguish the two half moons and the clustering accuracy is only 57%. By using our proposed Laplacian regularized LRR, i.e., NSLLRR, we can better separate the mixed clusters. As illustrated in Fig. <ref type="figure" target="#fig_2">1(c</ref>), the clustering accuracy is as high as 96.5%. This clearly While not converged (k = 0, 1, ...) do 1) update Z:</p><formula xml:id="formula_36">Z k+1 = Θ (η1) -1 (Z k -∇ Z q(Z k )/η 1 )</formula><p>where</p><formula xml:id="formula_37">∇ Z q(Z k ) =β(Z k L h T + Z k L h ) + µ k Z k -J k + M k 2 µ k + µ k Y T Y Z k -Y + E k - M k 1 µ k η 1 =2β L h 2 + µ k (1 + Y 2 2 ) 2) update E: E k+1 = S γ µ k Y -Y Z k+1 + 1 µ k M k<label>1</label></formula><p>3) update J:</p><formula xml:id="formula_38">J k+1 = max S λ µ k Z k+1 + 1 µ k M k 2 ,<label>0</label></formula><p>4) update Lagrange multipliers M 1 and M 2 :</p><formula xml:id="formula_39">M k+1 1 = M k 1 + µ k (Y -Y Z k+1 -E k+1 ) M k+1 2 = M k 2 + µ k (Z k+1 -J k+1 ) 5) update µ: µ k+1 = min (µ max , ρ k µ k )</formula><p>where</p><formula xml:id="formula_40">ρ k =      ρ 0 , if max {η 1 Z k+1 -Z k , µ k J k+1 -J k , µ k E k+1 -E k } ≤ ε 2 1, otherwise 6) check convergence: if Y -Y Z k+1 -E k+1 / Y &lt; ε 1 and max {η 1 Z k+1 -Z k , µ k J k+1 -J k , µ k E k+1 -E k } &lt; ε 2 , then stop.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>End while</head><p>demonstrates that the local structural information coded in the Laplacian regularizer is helpful for non-linear manifold clustering. In particular, this pleasing result is beneficial from the block-diagonal structure of the affinity matrix. As shown in Fig. <ref type="figure" target="#fig_3">2</ref>, we can see the affinity matrix obtained by NSLLRR has a relative distinct block-diagonal structure which is helpful for data clustering. Note that the affinity matrices in Fig. <ref type="figure" target="#fig_3">2</ref> are both permuted according to the order of two true clusters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Unsupervised Learning: Image Clustering</head><p>Data clustering is to group samples into different groups. To quantitatively evaluate the clustering performance, we adopt two metrics, accuracy (AC) and normalized mutual information (NMI) <ref type="bibr" target="#b6">[7]</ref>, in our experiments. Given a data point x i , let F and F be the ground truth label and the label produced by a clustering approach, respectively, then the AC measure is defined by</p><formula xml:id="formula_41">AC = n i=1 δ F(i), Match ( F ,F ) (i)</formula><formula xml:id="formula_42">n</formula><p>where n is the number of samples in total, function δ(a, b) is set to 1 if and only if a = b, and Match(•) is the best matching function that permutes F to match F, which is usually fulfilled by the Kuhn-Munkres algorithm <ref type="bibr" target="#b11">[12]</ref>. The NMI measure between two index sets K and K is defined as</p><formula xml:id="formula_43">NMI(K, K ) = MI(K, K ) max (H (K) , H (K ))</formula><p>where H(K) and H(K ) denote the entropy 1 of K and K , respectively, and</p><formula xml:id="formula_44">MI (K, K ) = y∈K x∈K p (x, y) log 2 p (x, y) p (x) p (y)</formula><p>where p (y) and p (x) denote the marginal probability distribution functions of K and K , respectively, and p (x, y) is the joint probability distribution function of K and K . Usually, NMI(K, K ) ranges from 0 to 1, for which the value 1 means that the two clusters are identical and the value 0 means that two are independent. Different from AC, NMI is invariant with the permutation of labels. Namely, it does not require the matching between two clusters in advance.</p><p>To extensively assess the clustering performance of our proposed approach, the following five methods for image clustering are compared with:</p><p>1) K-means clustering algorithm (K-means); 2) Normalized cut (Ncut) <ref type="bibr" target="#b37">[38]</ref>;</p><p>3) Principle component analysis (PCA); 4) Traditional LRR <ref type="bibr" target="#b30">[31]</ref>; 5) Locally linear embedding (LLE) <ref type="bibr" target="#b41">[42]</ref>. K-means usually serves as a baseline of image clustering. PCA is a well known unsupervised dimensionality reduction method, which is applied to discard the noisy information corresponding to the small eigenvalues of the data covariance matrix. The Ncut method is a spectral clustering based algorithm which can be used to seek the cluster indicator information for data. Both the traditional LRR and NSLLRR algorithms can learn an affinity matrix measuring the relationship among data points. However, the clustering information, obtained by above methods except K-means, cannot be directly used as the final clustering results because they lack the clear cluster structures. Therefore, here K-means is adopted to assign final labels on the learned low dimensional representation space. The NSLLRR can be regarded as a non-linear manifold clustering 1 The entropy H(K) is defined as H(K) = y∈K -p(y) • log 2 (p(y)) where p(y) is the probability that a sample belongs to a cluster. algorithm. So it would be informative to compare it with some existing graph-based clustering methods. Here, we compare it with a classical method, locally linear embedding (LLE) <ref type="bibr" target="#b41">[42]</ref>, to demonstrate the clustering effectiveness of our proposed method.</p><p>In this experiment, the real world CMU-PIE<ref type="foot" target="#foot_0">2</ref> face image database is utilized to compare the performance of related approaches. CMU-PIE is a popular face database, which is widely used in many kinds of learning tasks. It includes 68 subjects with 41,368 face images in total. In this dataset, the size of each sample is 32×32 pixels and images of each subject are captured under 13 different poses, 43 different illumination conditions and 4 different expressions. We only select part of images by fixing the pose and expression so that, for each subject, we have 21 images under different lighting conditions. For computational convenience, we first apply PCA to reduce the data dimensionality to 64. The clustering experiments are conducted with different cluster numbers. That is, we use the first k classes in the dataset for the corresponding data clustering experiments. The detailed clustering results are reported in Table <ref type="table" target="#tab_1">I</ref>. The bold numbers denote the best result at corresponding cluster numbers. We can see that our proposed method outperforms other algorithms, especially when the number of clusters is large, which is more difficult. It also shows that K-means is generally worse than other methods. As can be seen, the results by our proposed NSLLRR are consistently much better in terms of the NMI values when compared with the LLE method. However, it can be observed that there exists a fluctuation in clustering accuracy and NMI from the proposed method. The phenomenon could be due to applying K-means, whose result may be different under different initializations of cluster centers, to the learned low dimensional representation during the clustering step. The similar phenomenon occurs for the LLE method and LRR as well.</p><p>There are several regularization parameters affecting the performance of NSLLRR. In the following, we study the influence of parameters λ, γ and β on NMI by setting them at different values. We vary a parameter while keeping others fixed. The NMI results with different parameter settings on the CMU-PIE dataset are given in Fig. <ref type="figure" target="#fig_4">3</ref>. From the results, we can conclude that our proposed Laplacian regularized LRR outperforms other algorithms over a large range of parameter values. We can also note that the performance of NSLLRR is insensitive to the choice of β. Moreover, the results on λ and γ show that both sparsity and low-rankness are critical for data clustering. Thus, we set λ = 0.2, γ = 5 and β = 5 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Semi-supervised Classification</head><p>In order to evaluate our proposed model on semi-supervised learning task, we select two publicly available image datasets for our experiments, i.e., CMU-PIE and USPS <ref type="foot" target="#foot_1">3</ref> . CMU-PIE is a face database and USPS is a handwritten digit database. CMU-PIE includes many face images under different illumination   proposed to construct a robust and datum-adaptive graph by solving the sparsest representation of every sample, using other samples as a dictionary. The coefficient matrix is then symmetrized. 4) LRR-graph based classification method: Following <ref type="bibr" target="#b29">[30]</ref>,</p><p>we construct the LRR-graph by solving the lowest-rank coefficient matrix and then symmetrizing it as 1 -graph does. The parameters of LRR are the same as those in <ref type="bibr" target="#b29">[30]</ref>. 5) Non-negative low-rank and sparse (NNLRS)-graph based classification method: Following <ref type="bibr" target="#b55">[56]</ref>, we construct a graph for which the weights are obtained by seeking a non-negative low-rank and sparse matrix that represents each data points as a linear combination of  others. The coefficient matrix is also symmetrized.</p><p>Similar to the LRR-graph based method, our proposed approach also seeks a low-rank coefficient matrix. We further require that the coefficient matrix is sparse and nonnegative. We call our approach as the NSLLRR-graph based classification method in our experiments.</p><p>The purpose of semi-supervised learning task is to assign labels to unlabeled data, according to the labeled data and the overall data distribution. We make the percentage of labeled samples range from 10% to 60%. Empirically, we found that λ = 0.08, γ = 2 and β = 5 are good choices for these parameters. Therefore, we did not tune these parameters in the experiments. The classification results on CMU-PIE and USPS are reported in Table <ref type="table" target="#tab_3">II</ref> and<ref type="table">Table</ref>  <ref type="table" target="#tab_4">III</ref>, respectively. The bold numbers in each table denote the best performance under corresponding labeling percentages. From these results, we can see that our proposed NSLLRR almost consistently outperforms other methods, especially on the CMU-PIE database. This suggests that our proposed method is effective for semisupervised learning. This also shows that the manifold regularizer can enhance the robustness of the LRR graph-based method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. HYPERGRAPH REGULARIZED LRR FOR SEMI-SUPERVISED LEARNING</head><p>In the real world, there usually exists a co-occurrence relationship that involves more than two samples <ref type="bibr" target="#b35">[36]</ref>. This can be more effectively represented by a hypergraph. In this section, we consider some categorical data whose samples have one or more attributes, i.e., the Zoo dataset, which comes from the UCI Machine Learning Depository 4 . The Zoo dataset covers 101 animals with 17 Boolean-valued attributes, where the attributes contain hair, feathers, eggs, milk, legs, tail, etc. Moreover, these animal data have been manually grouped into seven classes beforehand. Note that we have found that there are two repeated samples of "frog" in the dataset. So we manually remove the repeated items in our experiment. is, the test dataset contains 100 samples in our experiment. In order to evaluate our proposed hypergraph regularized LRR, we conduct a semi-supervised classification task on the Zoo dataset, where we define a hypergraph with 36 hyper-edges.</p><p>As in <ref type="bibr" target="#b35">[36]</ref>, we construct a hypergraph by taking samples as vertices and creating a hyperedge for each value of the attributes of Zoo. For instance, attribute 2 (hair) is a boolean variable. So two hyperedges are created for this attribute: F020 and F021 (F denotes feature, 02 represents the 2nd attribute and 0 or 1 corresponds to the value of the attribute). Then, each hyperedge includes the instances that share the same attribute value. After this pre-processing, we find that this data set contains 36 hyperedges.</p><p>In our experiment, the labels of some vertices in the hypergraph are supposed to be known. For fair comparison, the labeled vertices are randomly chosen to ensure that all labels appear. That is, the labeled data should cover all classes in the entire dataset. The labeling percentage ranges from 10% to 60%. For each labeling percentage, we run 10 times for each algorithm with the labeled data sets randomly chosen and then average the error rates. We empirically set the parameters as λ = 0.01, γ = 0.1 and β = 0.001. To also assess the 4 http://archive.ics.uci.edu/ml/datasets/Zoo effectiveness of learning the lowest rank representation, we compare our proposed method, NSHLRR, with LRR-graph methods and normal graph based NSLLRR. Moreover, we compare with kNN-graph based method which is usually used as a baseline. As before, we also select five nearest neighbors for each data point. The classification results are reported in Table <ref type="table" target="#tab_1">IV</ref> and Fig. <ref type="figure" target="#fig_8">5</ref> shows the plot of classification error rate versus the percentage of labeled points on the Zoo dataset. As can be seen, the hypergraph regularized NSHLRR method consistently outperforms LRR and the kNN graph based method. Among these methods, kNN graph based method produces the worst classification performance. NSHLRR is significantly better than all other methods except NSLLRR in the cases of 30% and 40% labeling percentages. This result shows that a hypergraph is more sensible to modeling data with more complex relationship than a normal graph approach does, since the complicated interactions among data can be well represented by a hypergraph. The experiment demonstrates that our proposed Laplacian regularized LRR model can not only preserve the globally non-linear structures, but also represent the locally linear structures, by incorporating a hypergraph regularizer, and has the capability of learning high order relationship (i.e., three-way or higher) in the semi-supervised settings.   VII. DIMENSIONALITY REDUCTION THROUGH HYPERGRAPH EMBEDDING In this section, we consider dimensionality reduction task via hypergraph embedding that projects the vertices of G onto a low dimensional Euclidean space, where the vertices are best characterized by the similarity relationship between the vertex pairs in hypergraphs <ref type="bibr" target="#b35">[36]</ref>. The general graph-spectral approach for embedding is implemented by Laplician Eigenmaps <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b46">[47]</ref>, in which the tailing eigenvectors of the Laplacian matrix of G, except for the eigenvector corresponding to eigenvalue 0, is often exploited for embedding.</p><p>To evaluate the effectiveness of our proposed method on hypergraph embedding, we follow the Laplacian Eigenmaps framework <ref type="bibr" target="#b1">[2]</ref>. First we use both hypergraph-based NSHLRR and normal-graph-based NSLLRR to learn the similarity Z of high dimensional data, respectively. Then we do graph embedding by Laplacian Eigenmaps by using the Laplacian matrix derived from the learned Z. Thus, the embedding of vertex v in a k-dimensional space is just a row vector of the matrix that contains columns of the eigenvectors associated with the k smallest eigenvalues, except for the eigenvector corresponding to eigenvalue 0, of the Laplacian matrix of G <ref type="bibr" target="#b53">[54]</ref>.</p><p>In this experiment, we test both hypergraph-based NSHLRR and normal-graph-based NSLLRR methods on the Zoo dataset. First we visualize the dimensionality reduction results in Fig. <ref type="figure" target="#fig_10">6</ref> by looking at the embeddings along the second and third smallest eigenvectors direction, i.e., in a 2-dimensional space. As can be seen from the figure, visualization for both methods demonstrates obvious separation among different classes, however classes 1, 5 and 7 under the normal-graph-based NSLLRR are scattered much more largely than under the hypergraphbased NSHLRR.</p><p>In order to quantitatively validate the above visual observation, we compute the cluster accuracy of two methods on the Zoo dataset after graph embedding based on a post K-mean process. The cluster accuracy of the hypergraph-based NSHLRR is 91% while the normal-graph-based NSLLRR only achieves 74% cluster accuracy.</p><p>Since some sample (e.g., tortoise) has a special attribution, the embedding with the hypergraph-based NSHLRR fails to give better grouping. Nevertheless, both the visualization in Fig. <ref type="figure" target="#fig_10">6</ref> and the quantity result demonstrate overall efficacy of the hypergraph-based NSHLRR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>In this paper, we propose a generalized Laplacian regularized low-rank representation framework, in which we explicitly consider the manifold structures of data space by introducing a Laplacian regularization term. In particular, a hypergraph Laplacian regularizer is introduced into the conventional LRR objective function to form the hypergraph Laplacian regularized LRR model. Our proposed method not only can represent the global low-dimensional structures, but also capture the intrinsic non-linear geometric information in data. The extensive experimental results on image clustering, classification and embedding show the efficacy of our proposed method. However, a theoretical analysis the benefits of using nonlinearities and hyper-edges will be further investigated as the future work and the application of the proposed model to a broader range of problems is to be explored.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>where h(v, e) is the entry of the incidence matrix H of G. The value of the entry h(v, e) is set as follows h(v, e) = 1, if v ∈ e; 0, otherwise. Denote the degree of a hyperedge e by d(e) = v∈V h(v, e)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 :</head><label>1</label><figDesc>LADMAP for solving non-negative sparse Laplacian regularized LRR Input: Y , λ, β , γ and the number of nearest neighbors. Output: Z k+1 , E k+1 . Initialization: Compute L h , Z 0 = E 0 = J 0 = M 0 1 = M 0 2 = 0, λ = 0.02, β = 1.0, γ = 5.0, ρ 0 = 2.5, µ 0 = 10 -6 , µ max = 10 6 ; ε 1 = 10 -6 , ε 2 = 10 -2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Clustering on a toy dataset. (a) A synthetic dataset with two intersecting half moons. (b) Clustering result given by LRR, whose accuracy is 57 %. (c) Clustering result given by NSLLRR, whose accuracy is 96.5%.</figDesc><graphic coords="8,318.25,360.49,238.49,127.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of the affinity matrix produced by different methods. a) LRR. b) NSLLRR.</figDesc><graphic coords="9,69.93,61.71,102.81,102.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Performance of NSLLRR on the CMU-PIE face database, with different parameter settings. a) λ varies. b) γ varies. c) β varies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>conditions and with different expressions, as described in the last section. USPS is composed of 9298 handwritten digit images of size 16×16 pixels. Each image is represented by a 256-dimensional vector. Some samples of these datasets are shown in Fig.4. In order to show the efficacy of NSLLRR, the following five methods for image classification are chosen for performance comparison. 1) k-nearest-neighbor (kNN) based classification method [40]: We adopt the Euclidean distance as the similarity measure and set the numbers of nearest neighbors to be 5. 2) LLE-graph based classification method [42]: We construct an LLE-graph with the numbers of nearest neighbors being 8. 3) 1 -graph based classification method [13]: Cheng et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a) CMU-PIE samples. (b) USPS samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Samples of test databases.</figDesc><graphic coords="10,360.39,534.47,154.22,121.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Classification error rate versus the labeling percentage on the Zoo dataset, based on LRR, normal-graph NSLLRR and NSHLRR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>With hypergraph-based NSHLRR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The embeddings of the Zoo dataset with eigenvectors corresponding to the 2nd and 3rd smallest eigenvalues. (a) The cluster accuracy is 74%. (b) The cluster accuracy is 91%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I CLUSTERING</head><label>I</label><figDesc>RESULTS ON THE CMU-PIE DATA SET ( # IS THE NUMBER OF CLUSTERS).</figDesc><table><row><cell>Cluster</cell><cell></cell><cell></cell><cell></cell><cell>Accuracy</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>NMI</cell><cell></cell></row><row><cell cols="2">Num. # K-means</cell><cell>PCA</cell><cell>Ncut</cell><cell>LLE-graph</cell><cell>LRR</cell><cell cols="2">NSLLRR K-means</cell><cell>PCA</cell><cell>Ncut</cell><cell>LLE-graph</cell><cell>LRR</cell><cell>NSLLRR</cell></row><row><cell>4</cell><cell>48.60</cell><cell>52.30</cell><cell>99.10</cell><cell>88.10</cell><cell>100</cell><cell>100</cell><cell>46.81</cell><cell cols="2">46.81 98.60</cell><cell>77.33</cell><cell>100</cell><cell>100</cell></row><row><cell>12</cell><cell>41.60</cell><cell>57.14</cell><cell>86.40</cell><cell>63.89</cell><cell>89.68</cell><cell>76.98</cell><cell>52.66</cell><cell cols="2">52.66 91.20</cell><cell>82.10</cell><cell>95.35</cell><cell>87.81</cell></row><row><cell>20</cell><cell>38.40</cell><cell>36.40</cell><cell>78.30</cell><cell>73.57</cell><cell>81.19</cell><cell>86.19</cell><cell>59.12</cell><cell cols="2">58.88 88.60</cell><cell>86.95</cell><cell>90.74</cell><cell>94.46</cell></row><row><cell>28</cell><cell>35.40</cell><cell>34.80</cell><cell>78.20</cell><cell>78.74</cell><cell>77.21</cell><cell>86.91</cell><cell>60.48</cell><cell cols="2">61.06 89.60</cell><cell>90.30</cell><cell>89.89</cell><cell>93.99</cell></row><row><cell>36</cell><cell>34.90</cell><cell>34.6</cell><cell>75.60</cell><cell>75.00</cell><cell>68.92</cell><cell>76.59</cell><cell>61.69</cell><cell cols="2">60.56 88.90</cell><cell>91.13</cell><cell>82.15</cell><cell>93.55</cell></row><row><cell>44</cell><cell>33.20</cell><cell>33.70</cell><cell>74.20</cell><cell>68.18</cell><cell>71.86</cell><cell>80.20</cell><cell>62.30</cell><cell cols="2">62.57 89.50</cell><cell>88.27</cell><cell>84.16</cell><cell>93.70</cell></row><row><cell>52</cell><cell>33.20</cell><cell>33.70</cell><cell>72.60</cell><cell>74.45</cell><cell>71.25</cell><cell>74.18</cell><cell>63.44</cell><cell cols="2">61.58 89.10</cell><cell>89.39</cell><cell>84.81</cell><cell>93.01</cell></row><row><cell>60</cell><cell>33.10</cell><cell>33.20</cell><cell>71.10</cell><cell>69.92</cell><cell>65.87</cell><cell>78.73</cell><cell>64.06</cell><cell cols="2">63.58 88.50</cell><cell>88.59</cell><cell>80.39</cell><cell>93.43</cell></row><row><cell>68</cell><cell>31.70</cell><cell>32.90</cell><cell>70.80</cell><cell>69.91</cell><cell>65.27</cell><cell>85.08</cell><cell>63.30</cell><cell cols="2">66.60 88.30</cell><cell>89.92</cell><cell>79.12</cell><cell>96.62</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II CLASSIFICATION</head><label>II</label><figDesc>ERROR RATES (%) ON CMU-PIE, BASED ON VARIOUS GRAPHS UNDER DIFFERENT PERCENTAGES OF LABELED SAMPLES.</figDesc><table><row><cell>Labeling percentage#</cell><cell cols="2">kNN [40] LLE-graph</cell><cell cols="4">1 graph LRR-graph [30] NNLRS-graph [56] NSLLRR-graph</cell></row><row><cell>10</cell><cell>34.84</cell><cell>33.06</cell><cell>22.88</cell><cell>47.30</cell><cell>11.11</cell><cell>35.66</cell></row><row><cell>20</cell><cell>37.46</cell><cell>35.05</cell><cell>22.94</cell><cell>21.60</cell><cell>22.81</cell><cell>9.28</cell></row><row><cell>30</cell><cell>35.3</cell><cell>32.52</cell><cell>22.33</cell><cell>11.80</cell><cell>17.86</cell><cell>5.83</cell></row><row><cell>40</cell><cell>35.81</cell><cell>32.51</cell><cell>23.14</cell><cell>7.80</cell><cell>16.25</cell><cell>3.36</cell></row><row><cell>50</cell><cell></cell><cell>31.41</cell><cell>23.01</cell><cell>4.30</cell><cell>19.25</cell><cell>2.10</cell></row><row><cell>60</cell><cell>35.63</cell><cell>32.76</cell><cell>25.76</cell><cell>2.50</cell><cell>21.56</cell><cell>1.62</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III CLASSIFICATION</head><label>III</label><figDesc>ERROR RATES (%) ON USPS, BASED ON VARIOUS GRAPHS UNDER DIFFERENT PERCENTAGES OF LABELED SAMPLES.</figDesc><table><row><cell>Labeling percentage#</cell><cell cols="2">kNN [40] LLE-graph</cell><cell cols="4">1 graph LRR-graph [30] NNLRS-graph [56] NSLLRR-graph</cell></row><row><cell>10</cell><cell>11.97</cell><cell>17.10</cell><cell>43.27</cell><cell>67.00</cell><cell>11.57</cell><cell>10.55</cell></row><row><cell>20</cell><cell>12.31</cell><cell>22.92</cell><cell>41.27</cell><cell>48.30</cell><cell>9.30</cell><cell>5.90</cell></row><row><cell>30</cell><cell>5.88</cell><cell>21.26</cell><cell>38.31</cell><cell>33.60</cell><cell>4.95</cell><cell>4.80</cell></row><row><cell>40</cell><cell>7.87</cell><cell>19.21</cell><cell>34.86</cell><cell>25.80</cell><cell>7.44</cell><cell>4.01</cell></row><row><cell>50</cell><cell>17.19</cell><cell>18.41</cell><cell>29.42</cell><cell>15.50</cell><cell>11.27</cell><cell>3.20</cell></row><row><cell>60</cell><cell>11.04</cell><cell>14.80</cell><cell>23.36</cell><cell>10.00</cell><cell>6.09</cell><cell>2.05</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>http://www.cad.zju.edu.cn/home/dengcai/Data/FaceData.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>http://www-i6.informatik.rwth-aachen.de/˜keysers/usps.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The research project is supported by the Australian Research Council (ARC) through the grant DP130100364. Ming Yin is supported by the Foundation of Key Laboratory of Autonomous Systems and Networked Control, Ministry of Education, P.R. China (No. 2013A06), and Guangdong Natural Science Foundation (No. 2014A030313511). Zhouchen Lin is supported by 973 Program of China (grant no. 2015CB352502), NSF China (grant nos. 61231002 and</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sparse Bayesian methods for low-rank matrix estimation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Babacan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Luessi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3964" to="3977" />
			<date type="published" when="2012-08">August 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps for dimensionality reduction and data representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1373" to="1396" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps and spectral techniques for embedding and clustering</title>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="585" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Transposed low rank representation for image classification</title>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Bull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbin</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of DICTA</title>
		<meeting>DICTA</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Spectral regression: A unified approach for sparse subspace learning</title>
		<author>
			<persName><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICDM</title>
		<meeting>ICDM</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="73" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Graph regularized non-negative matrix factorization for data representation</title>
		<author>
			<persName><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1548" to="1560" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Locality preserving nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A singular value thresholding algorithm for matrix completion</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1956" to="1982" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exact matrix completion via convex optimization</title>
		<author>
			<persName><forename type="first">E</forename><surname>Candés</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations of Computational Mathematics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="717" to="772" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust principal component analysis?</title>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Low-rank matrix recovery with structural incoherence for robust face recognition</title>
		<author>
			<persName><forename type="first">Chih-Fan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chia-Po</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Chiang</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2618" to="2625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Atomic decomposition by basis pursuit</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">L</forename><surname>Scott Shaobing Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><surname>Saunders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="129" to="159" />
			<date type="published" when="2001-01">January 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning with 1 -graph for image analysis</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="858" to="866" />
			<date type="published" when="2010-04">April 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Spectral Graph Theory</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R K</forename><surname>Chung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>American Mathematical Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Random Walks and Electric Networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Snell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Mathematical Association of America</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Compressed sensing</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transcations on Information Theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1289" to="1306" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Robust 1 principal component analysis and its Bayesian variational inference</title>
		<author>
			<persName><forename type="first">Junbin</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="555" to="572" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Laplacian sparse coding, hypergraph Laplacian sparse coding, and applications</title>
		<author>
			<persName><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivor</forename><surname>Wai-Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang-Tien</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><surname>Chia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="92" to="104" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Laplacian regularized Gaussian mixture model for data clustering</title>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanlong</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1406" to="1418" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neighborhood preserving embedding</title>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong-Jiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Face recognition using Laplacianfaces</title>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machince Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="328" to="340" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An accelerated gradient method for trace norm minimization</title>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieping</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="457" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Principal component analysis</title>
		<author>
			<persName><forename type="first">I</forename><surname>Jolliffe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>2 edition</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Nonnegative matrix factorization based on alternating nonnegativity constrained least squares and active set method</title>
		<author>
			<persName><forename type="first">Hyunsoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haesun</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="713" to="730" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Robust principal component analysis for computer vision</title>
		<author>
			<persName><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="362" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The augmented Lagrange multiplier method for exact recovery of corrupted low-rank matrices</title>
		<author>
			<persName><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
		<idno>UILU-ENG-09-2215</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="report_type">UIUC Technical Report</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Fast convex optimization algorithms for exact recovery of a corrupted low-rank matrix</title>
		<author>
			<persName><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leqin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
		<idno>UILU-ENG-09-2214</idno>
		<imprint>
			<date type="published" when="2009-07">July 2009</date>
		</imprint>
	</monogr>
	<note type="report_type">UIUC Technical Report</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Linearized alternating direction method with adaptive penalty for low-rank representation</title>
		<author>
			<persName><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Risheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhixun</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NPIS</title>
		<meeting>NPIS</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="612" to="620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Robust recovery of subspace structures by low-rank representation</title>
		<author>
			<persName><forename type="first">Guangcan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ju</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machince Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="171" to="184" />
			<date type="published" when="2013-01">Jan. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Robust subspace segmentation by low-rank representation</title>
		<author>
			<persName><forename type="first">Guangcan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="663" to="670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Exact subspace segmentation and outlier detection by low-rank representation</title>
		<author>
			<persName><forename type="first">Guangcan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AISTATS</title>
		<meeting>AISTATS</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="703" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Latent low-rank representation for subspace segmentation and feature extraction</title>
		<author>
			<persName><forename type="first">Guangcan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1615" to="1622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Linearized alternating direction method with parallel splitting and adaptive penalty for separable convex programs in machine learning</title>
		<author>
			<persName><forename type="first">Risheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhixun</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Graph-regularized low-rank representation for destriping of hyperspectral images</title>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">7-1</biblScope>
			<biblScope unit="page" from="4009" to="4018" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hypergraph Learning with Hyperedge Expansion</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boi</forename><surname>Faltings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECML-PKDD</title>
		<meeting>ECML-PKDD</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Recovering low-rank and sparse components of matrices from incomplete and noisy observations</title>
		<author>
			<persName><forename type="first">Min</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoming</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="57" to="81" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">D</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">An accelerated proximal gradient algorithms for nuclear norm regularized least squares problems</title>
		<author>
			<persName><forename type="first">K</forename><surname>Toh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pacific Journal of Optimization</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="615" to="640" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Linear neighborhood propagation and its applications</title>
		<author>
			<persName><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1600" to="1615" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Alternating direction augmented Lagrangian methods for semidefinite programming</title>
		<author>
			<persName><forename type="first">Zaiwen</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Goldfarb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wotao</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming Computation</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="203" to="230" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Robust principal component analysis: Exact recovery of corrupted low-rank matrices by convex optimization</title>
		<author>
			<persName><forename type="first">John</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shankar</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Sparse representation for computer vision and pattern recognition</title>
		<author>
			<persName><forename type="first">John</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Convex optimization based low-rank matrix completion and recovery for photometric stereo and factor classification. submitted to IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-08">August 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Graph embedding and extensions: A general framework for dimensionality reduction</title>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong-Jiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machince Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="40" to="51" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Semi-supervised low-rank representation graph for pattern recognition</title>
		<author>
			<persName><forename type="first">Shuyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuxiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Licheng</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Image Processing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="131" to="136" />
			<date type="published" when="2013-03">March 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Robust face recognition via double low-rank matrix recovery for feature extraction</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuting</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbin</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICIP</title>
		<meeting>ICIP</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3770" to="3774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Tilt: Transform invariant low-rank textures</title>
		<author>
			<persName><forename type="first">Zhengdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACCV</title>
		<meeting>ACCV</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Low-rank matrix approximation with manifold regularization</title>
		<author>
			<persName><forename type="first">Zhenyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keke</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1717" to="1729" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Graph regularized sparse coding for image representation</title>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Miao Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1327" to="1336" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Low-rank representation with local constraint for graph construction</title>
		<author>
			<persName><forename type="first">Yaoguo</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangrong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Licheng</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="page" from="398" to="405" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning with hypergraphs: Clustering, classification, and embedding</title>
		<author>
			<persName><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Scholkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using Gaussian fields and harmonic functions</title>
		<author>
			<persName><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Ming Yin received the Ph.D. degree in information and communication engineering from</title>
		<author>
			<persName><forename type="first">Liansheng</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyuan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR<address><addrLine>Wuhan, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2012. 2006</date>
			<biblScope unit="page" from="2328" to="2335" />
		</imprint>
		<respStmt>
			<orgName>Huazhong University of Science and Technology (HUST)</orgName>
		</respStmt>
	</monogr>
	<note>Non-negative low rank and sparse graph for semisupervised learning</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
