<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SIGN: Scalable Inception Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-04-23">23 Apr 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Emanuele</forename><surname>Rossi</surname></persName>
							<email>erossi@twitter.com</email>
						</author>
						<author>
							<persName><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
							<email>ffrasca@twitter.com</email>
						</author>
						<author>
							<persName><forename type="first">Davide</forename><surname>Eynard</surname></persName>
							<email>deynard@twitter.com</email>
						</author>
						<author>
							<persName><forename type="first">Ben</forename><surname>Chamberlain</surname></persName>
							<email>bchamberlain@twitter.com</email>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
							<email>mbronstein@twitter.com</email>
						</author>
						<author>
							<persName><forename type="first">Federico</forename><surname>Monti</surname></persName>
							<email>fmonti@twitter.com</email>
						</author>
						<title level="a" type="main">SIGN: Scalable Inception Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-04-23">23 Apr 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2004.11198v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Geometric deep learning, a novel class of machine learning algorithms extending classical deep learning architectures to non-Euclidean structured data such as manifolds and graphs, has recently been applied to a broad spectrum of problems ranging from computer graphics and chemistry to high energy physics and social media. Despite the plenitude of different graph neural network architectures that have been proposed, until recently relatively little effort has been dedicated to developing methods that scale to very large graphs -which precludes their industrial applications e.g. in social networks. In this paper, we propose SIGN, a scalable graph neural network analogous to the popular inception module used in classical convolutional architectures. We show that our architecture is able to effectively deal with large-scale graphs via pre-computed multi-scale neighborhood features. Extensive experimental evaluation on various open benchmarks shows the competitive performance of our approach compared to a variety of popular architectures, while requiring a fraction of training and inference time. * Equal contribution Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning on graphs, also known as geometric deep learning (GDL) or graph representation learning (GRL), has emerged in a matter of just a few years from a niche topic to one of the most prominent fields in machine learning. Graph convolutional neural networks (GCNs), which can be traced back to the seminal work of Scarselli et al. <ref type="bibr" target="#b38">Scarselli et al. (2008)</ref>, seek to generalize classical convolutional architectures (CNNs) to graph-structured data. A wide variety of convolution-like operations have been developed on graphs, including ChebNet <ref type="bibr" target="#b11">Defferrard et al. (2016)</ref>, <ref type="bibr">MoNet Monti et al. (2016)</ref>, <ref type="bibr">GCN Kipf and Welling (2017)</ref>, S- <ref type="bibr">GCN Wu et al. (2019)</ref>, <ref type="bibr">GAT Velickovic et al. (2018), and</ref><ref type="bibr">GraphSAGE Hamilton et al. (2017a)</ref>. We refer the reader to recent review papers <ref type="bibr">Bronstein et al. (2017)</ref>; <ref type="bibr" target="#b17">Hamilton et al. (2017b)</ref>; <ref type="bibr" target="#b1">Battaglia et al. (2018)</ref>; <ref type="bibr">Zhang et al. (2018)</ref> for a comprehensive overview of deep learning on graphs and its mathematical underpinnings.</p><p>Graph deep learning models have been extremely successful in modeling relational data in a variety of different domains, including social network link prediction <ref type="bibr" target="#b52">Zhang and Chen (2018)</ref>, human-object interaction <ref type="bibr" target="#b36">Qi et al. (2018)</ref>, computer graphics <ref type="bibr" target="#b30">Monti et al. (2016)</ref>, particle physics <ref type="bibr" target="#b10">Choma et al. (2018</ref><ref type="bibr">), chemistry Duvenaud et al. (2015)</ref>; <ref type="bibr" target="#b15">Gilmer et al. (2017)</ref>, medicine <ref type="bibr" target="#b33">Parisot et al. (2018)</ref>, drug repositioning <ref type="bibr" target="#b54">Zitnik et al. (2018)</ref>, discovery of anti-cancer foods <ref type="bibr" target="#b45">Veselkov et al. (2019)</ref>, modeling of proteins <ref type="bibr" target="#b14">Gainza et al. (2019)</ref> and nucleic acids <ref type="bibr" target="#b37">Rossi et al. (2019)</ref>, and fake news detection on social media <ref type="bibr" target="#b30">Monti et al. (2019)</ref> to mention a few. Somewhat surprisingly, often very simple architectures perform well in many applications <ref type="bibr" target="#b40">Shchur et al. (2018a)</ref>. In particular, graph convolutional networks (GCN) <ref type="bibr" target="#b25">Kipf and Welling (2017)</ref> and their more recent variant S- <ref type="bibr">GCN Wu et al. (2019)</ref> apply a shared node-wise linear transformation of the node features, followed by one or more iterations of diffusion on the graph.</p><p>Until recently, most of the research in the field has focused on small-scale datasets (CORA <ref type="bibr" target="#b39">Sen et al. (2008)</ref> with only ∼ 5K nodes still being among the most widely used), and relatively little effort has been devoted to scaling these methods to web-scale graphs such as the Facebook or Twitter social networks. Scaling is indeed a major challenge precluding the wide application of graph deep learning methods in large-scale industrial settings: compared to Euclidean neural networks where the training loss can be decomposed into individual samples and computed independently, graph convolutional networks diffuse information between nodes along the edges of the graph, making the loss computation interdependent for different nodes. Furthermore, in typical graphs the number of nodes grows exponentially with the increase of the filter receptive field, incurring significant computational and memory complexity. In this paper, we propose a simple scalable graph neural network architecture generalizing GCN, S-GCN, ChebNet and related methods. Our architecture is analogous to the inception module <ref type="bibr" target="#b43">Szegedy et al. (2015)</ref>; <ref type="bibr" target="#b22">Kazi et al. (2019)</ref> and combines graph convolutional filters of different size that are amenable to efficient precomputation, allowing extremely fast training and inference. Furthermore, our architecture is compatible with various sampling approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph sampling approaches</head><p>We provide extensive experimental validation showing that, despite its simplicity, our approach produces comparable results to state-of-the-art architectures on a variety of large-scale graph datasets while being significantly faster (orders of magnitude) in training and inference.</p><p>2 Background and Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Deep learning on graphs</head><p>Broadly speaking, the goal of graph representation learning is to construct a set of features ('embeddings') representing the structure of the graph and the data thereon. We can distinguish among Node-wise embeddings, representing each node of the graph, Edge-wise embeddings, representing each edge in the graph, and Graph-wise embeddings representing the graph as a whole. In the context of node-wise prediction problems (e.g. node-wise classification tasks), we can make the distinction between the following different settings or problems. Transductive learning assumes that the entire graph is known, and thus the same graph is used during training and testing (albeit different nodes are used for training and testing). In the Inductive setting, training and testing are performed on different graphs. Supervised learning uses a training set of labeled nodes (or graphs, respectively) and tries to predict these labels on a test set. The goal of Unsupervised learning is to compute a representation of the nodes (or the graph, respectively) capturing the underlying structure. Typical representatives of this class of architectures are graph autoencoders <ref type="bibr" target="#b24">Kipf and Welling (2016)</ref> and random walk-based embeddings <ref type="bibr" target="#b16">Grover and Leskovec (2016)</ref>; <ref type="bibr" target="#b35">Perozzi et al. (2014)</ref>.</p><p>A typical graph neural network architecture consists of graph Convolution-like operators (discussed in details in Section 2.3) performing local aggregation of features by means of message passing with the neighbor nodes, and possibly Pooling amounting to fixed <ref type="bibr" target="#b12">Dhillon et al. (2007)</ref> or learnable <ref type="bibr" target="#b50">Ying et al. (2018b);</ref><ref type="bibr" target="#b4">Bianchi et al. (2019a)</ref> graph coarsening. Additionally, graph Sampling schemes (detailed in Section 2.4) can be employed on large-scale graphs to reduce the computational complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Basic notions</head><p>Let G = (V = {1, . . . , n}, E, W) be an undirected weighted graph, represented by the symmetric n × n adjacency matrix W, where w ij &gt; 0 if (i, j) ∈ E and zero otherwise. The diagonal degree matrix D = diag( n j=1 w 1j , . . . , n j=1 w nj ) represents the number of neighbors of each node. We further assume that each node is endowed with a d-dimensional feature vector and arrange all the node features as rows of the n × d-dimensional matrix X.</p><p>The normalized graph Laplacian is an n × n positive semi-definite matrix ∆ = I − D −1/2 WD −1/2 . Given the n × d-dimensional node feature matrix X, the Laplacian amounts to computing the difference of the feature at each node with the local weighted average:</p><formula xml:id="formula_0">(∆X) i = x i − j∈N (i) w ij d i d j x j ;</formula><p>where N (i) = {j : (i, j) ∈ E} is the neighborhood of node i.</p><p>The Laplacian admits an eigendecomposition of the form ∆ = ΦΛΦ with orthogonal eigenvectors Φ = (φ 1 , . . . , φ n ) and non-negative eigenvalues Λ = diag(λ 1 , . . . , λ n ) arranged in increasing order 0 = λ 1 ≤ λ 2 ≤ . . . ≤ λ n . The eigenvectors play the role of a Fourier basis on the graph and the corresponding eigenvalues can be interpreted as frequencies. The graph Fourier transform is given by X = Φ X, and one can define the spectral analogy of a convolution on the graph as</p><formula xml:id="formula_1">X G = Φ( X • Ĝ) = Φ((Φ X) • (Φ G)),</formula><p>where • denotes the element-wise matrix product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Convolution-like operators on graphs</head><p>Spectral graph CNNs. <ref type="bibr" target="#b7">Bruna et al. Bruna et al. (2014)</ref> used the graph Fourier transform to generalize convolutional neural networks (CNN) <ref type="bibr" target="#b27">LeCun et al. (1989)</ref> to graphs. This approach has multiple drawbacks. First, the computation of the Fourier transform has O(n 2 ) complexity, in addition to O(n 3 ) precomputation of the eigenvectors Φ. Second, the number of filter parameters is O(n). Third, the filters are not guaranteed to be localized in the node domain. Fourth, the construction explicitly assumes the underlying graph to be undirected, in order for the Laplacian to be a symmetric matrix with orthogonal eigenvectors. Finally and most importantly, filters learned on one graph do not generalize to another.</p><p>ChebNet. A way to address these issues is to model the filter as a transfer function ĝ(λ), applied to the Laplacian as ĝ(∆) = Φĝ(Λ)Φ . Unlike the construction of <ref type="bibr" target="#b7">Bruna et al. (2014)</ref> that does not generalize across graphs, the filter computed in the above manner is stable under graph perturbations <ref type="bibr" target="#b28">Levie et al. (2019)</ref>. If ĝ is a smooth function, the resulting filters are localized in the node domain <ref type="bibr" target="#b19">Henaff et al. (2015)</ref>. In the case when ĝ is expressed as simple matrix-vector operations (e.g. a polynomial <ref type="bibr" target="#b11">Defferrard et al. (2016)</ref> or rational function <ref type="bibr" target="#b29">Levie et al. (2018)</ref>), the eigendecomposition of the Laplacian can be avoided altogether.</p><p>A particularly simple choice is a polynomial spectral filter ĝ(λ) = K k=0 θ k λ k of degree K, allowing the convolution to be computed entirely in the spatial domain as</p><formula xml:id="formula_2">Y = ĝ(∆)X = K k=0 θ k ∆ k X.</formula><p>(1)</p><p>Note that such a filter has O(K) parameters θ 0 , . . . , θ r , does not require explicit multiplication by Φ, and has a compact support of K hops in the node domain (due to the fact that ∆ k affects only neighbors within k-hops). Though originating from a spectral construction, the resulting filter is an operation in the node domain amounting to a successive aggregation of features in the neighbor nodes. Second, using recursively-defined Chebyshev polynomials T j+1 (λ) = 2λT j (λ) − T j−1 (λ) with T 1 (λ) = λ and T 0 (λ) = 1, the computation can be performed with complexity O(|E|K) = O(nK) for sparsely-connected graphs. Finally, the polynomial filters can be combined with non-linearities, concatenated in multiple layers, and interleaved with pooling layers based on graph coarsening <ref type="bibr" target="#b11">Defferrard et al. (2016)</ref>.</p><p>GCN. In the case K = 1, equation ( <ref type="formula">1</ref>) reduces to computing (I + D −1/2 WD −1/2 )X, which can be interpreted as a combination of the node features and their diffused version. Kipf and Welling Kipf and Welling (2017) proposed a model of graph convolutional networks (GCN) combining node-wise and graph diffusion operations:</p><formula xml:id="formula_3">Y = D−1/2 W D−1/2 XΘ = AXΘ.<label>(2)</label></formula><p>Here W = I + W is the adjacency matrix with self-loops, D = diag( n j=1 w1j , . . . , n j=1 wnj ) is the respective degree matrix, and Θ is a matrix of learnable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S-GCN.</head><p>Stacking L GCN layers with element-wise non-linearity σ and a final softmax layer for node classification, it is possible to obtain filters with larger receptive fields on the graph nodes,</p><formula xml:id="formula_4">Y = softmax(A • • • σ(AXΘ (1) ) • • • Θ (L) ).</formula><p>Wu et al. <ref type="bibr" target="#b46">Wu et al. (2019)</ref> argued that graph convolutions with large filters is practically equivalent to multiple convolutional layers with small filters. They showed that all but the last non-linearities can be removed without harming the performance, resulting in the simplified GCN (S-GCN) model,</p><formula xml:id="formula_5">Y = softmax(A L XΘ (1) • • • Θ (L) ) = σ(A L XΘ).</formula><p>(3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Graph sampling</head><p>A characteristic of many graphs, in particular 'small-world' social networks, is the exponential growth of the neighborhood size with number of hops L. In this case, the matrix A L becomes dense very quickly even for small values of L. For Web-scale graphs such as Facebook or Twitter that typically have n = 10 8 ∼ 10 9 nodes and |E| = 10 10 ∼ 10 11 edges, the diffusion matrix cannot be stored in memory for training. In such a scenario, classic Graph Convolutional Neural Networks models such as GCN, GAT or MoNet are not applicable. Graph sampling has been shown to be a successful technique to scale GNNs to large graphs, by approximating A L with a matrix that has a significantly simpler structure amenable for computation. Generally, graph sampling produces a graph G = (V , E ) such that V ⊂ V and E ⊂ E. We can distinguish between three types of sampling schemes (Figure <ref type="figure" target="#fig_2">2</ref>):</p><p>Node-wise sampling strategies perform graph convolutions on partial node neighborhoods to reduce computational and memory complexity. For each node of the graph, a selection criterion is employed to sample a fixed number of neighbors involved in the convolution operation and an aggregation tree is constructed out of the extracted nodes.</p><p>To overcome memory limitations, node-wise sampling strategies are coupled with minibatch training, where each training step is performed only on a batch of nodes rather than on the whole graph. A training batch is assembled by first choosing b 'optimization' nodes (marked in orange in Figure <ref type="figure" target="#fig_2">2</ref>, left), and partially expanding their corresponding neighborhoods. In a single training step, the loss is computed and optimized only for optimization nodes.  subgraphs such as a random walk-based sampler, which is able to co-sample nodes having high influence on each other and guarantees each edge has a non-negligible probability of being sampled.</p><p>At the time of writing, GraphSAINT is the state-of-the-art method for large graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Scalable Inception Graph Neural Networks</head><p>In this work we propose SIGN, an alternative method to scale graph neural networks to extremely large graphs. SIGN is not based on sampling nodes or subgraphs as these operations introduce bias into the optimization procedure.</p><p>We take inspiration from two recent findings: (i) despite its simplicity, the S-GCN (3) model appears to be extremely efficient and to attain similar results to models with multiple stacked convolutional layers <ref type="bibr" target="#b46">Wu et al. (2019)</ref>; (ii) GCN aggregation schemes (2) have been essentially shown to learn low-pass filters NT and Maehara (2019) while still performing on par with models with more complex aggregation functions in the task of semi-supervised node classification <ref type="bibr" target="#b40">Shchur et al. (2018a)</ref>.</p><p>Accordingly, we propose the following architecture for node-wise classification:</p><formula xml:id="formula_6">Z = σ [XΘ (0) | BXΘ (1) | . . . | B r XΘ (r) ] Y = ξ (ZΩ) ,<label>(4)</label></formula><p>where r) , Ω are learnable matrices respectively of dimensions d × d and d (r + 1) × c for c classes, | is the concatenation operataion and σ, ξ are non-linearities, the second one computing class probabilities, e.g. via softmax or sigmoid function, depending on the task at hand. Note that the model in equation ( <ref type="formula" target="#formula_6">4</ref>) is analogous to the popular Inception module <ref type="bibr" target="#b43">Szegedy et al. (2015)</ref> for classic CNN architectures (Figure <ref type="figure">1</ref>): it consists of convolutional filters of different sizes determined by the parameter r, where r = 0 corresponds to 1 × 1 convolutions in the inception module (amounting to linear transformations of the features in each node without diffusion across nodes). Owing to this analogy, we refer to our model as the Scalable Inception Graph Network (SIGN). We notice that one work extending the idea of an Inception module to GNNs is the one in <ref type="bibr" target="#b22">Kazi et al. (2019)</ref>; in this work, however, authors do not discuss the inclusion of a linear, non-diffusive term (r = 0) which effectively accounts for a skip connection. Additionally, the focus is not on scaling the model to large graphs, but rather on capturing intra-and inter-graph structural heterogeneity.</p><formula xml:id="formula_7">B = row_normalize(D −1/2 WD −1/2 ) is a fixed n × n graph diffusion matrix, Θ (0) , . . . , Θ<label>(</label></formula><p>Generalization of other models. It is also easy to observe that various graph convolutional layers can be obtained as particular settings of (4). In particular, by setting the σ non-linearity to PReLU, that is</p><formula xml:id="formula_8">σ(x) = P ReLU (x) = x if x &gt; 0 αx else</formula><p>where α is a learnable parameter, ChebNet, GCN, and S-GCN can be automatically learnt if suitable diffusion operator B and activation ξ are used (see Table <ref type="table">1</ref>).</p><formula xml:id="formula_9">B α r Θ (0) , . . . , Θ (r) Ω ChebNet Defferrard et al. (2016) ∆ 1 K Θ 0 , . . . , Θ K [I | . . . | I] GCN Kipf and Welling (2017) A 1 1 0, Θ [0 | I] S-GCN Wu et al. (2019) A 1 L 0, . . . , 0, Θ [0 | . . . | 0 | I]</formula><p>Table <ref type="table">1</ref>: SIGN as a generalization of ChebNet, GCN and S-GCN. By appropriate configuration, our inception layer is able to replicate popular GNN layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Efficient computation.</head><p>Finally and most importantly, we observe that the matrix products X, BX, . . . , B r X, in equation ( <ref type="formula" target="#formula_6">4</ref>) do not depend on the learnable model parameters and can be easily precomputed. For large graphs distributed computing infrastructures such as Apache Spark can speed up computation. This effectively reduces the computational complexity of the overall model to that of a multi-layer perceptron<ref type="foot" target="#foot_0">2</ref> .  <ref type="table" target="#tab_0">2</ref>: Time complexity where L is the number of layers, r is the filter size, N the number of nodes (in training or inference, respectively), |E| the number of edges, and d the feature dimensionality (assumed fixed for all layers). For GraphSAGE, k is the number of sampled neighbors per node. The forward pass complexity corresponds to an entire epoch where all nodes are seen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Our method is applicable to both transductive and inductive learning. In the inductive setting, test and validation nodes are held out at training time, i.e. training nodes are only connected to other training nodes. During model evaluation, on the contrary, all the original edges are considered. In the transductive (semi-supervised) setting, all the nodes are seen at training time, even though only a small fraction of the nodes have training labels.</p><p>Inductive experiments are performed using four large public datasets: Reddit, Flickr, Yelp and PPI. Introduced in <ref type="bibr">Hamilton et al. (2017a)</ref>, Reddit is the gold standard benchmark for GNNs on large scale graphs. Flickr and Yelp were introduced in <ref type="bibr" target="#b51">Zeng et al. (2019)</ref> and PPI in <ref type="bibr" target="#b55">Zitnik and Leskovec (2017)</ref>. In agreement with <ref type="bibr" target="#b51">Zeng et al. (2019)</ref>, we confirm that the performance of a variety of models on the last three datasets is unstable, meaning that large variations in the results are observed for very small changes in architecture and optimization parameters. We hypothesize that this is due to errors in the data, or to unfortunate a priori choices of the training, validation, and test splits. We still report results on these datasets for the purpose of comparing with the work in <ref type="bibr" target="#b51">Zeng et al. (2019)</ref>. Amongst the considered inductive datasets, Reddit and Flickr are multiclass node-wise classification problems: in the former, the task is to predict communities of online posts based on user comments; in the latter, the task is image categorization based on the description and common properties of online images.  <ref type="table" target="#tab_3">and 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Setup</head><p>For all datasets we use an inception convolutional layer with r = 2, σ = PReLU activation <ref type="bibr" target="#b18">He et al. (2015)</ref> and diffusion operator B = D −1 A A with A = D −1/2 WD −1/2 . To allow for larger model capacity in the inception module and in computing final model predictions, we replace the single-layer projections performed by parameters Θ (0) , . . . , Θ (r) and Ω with multiple feedforward layers. Model outputs for multiclass classification problems were normalized via softmax; for the multilabel classification tasks we use element-wise sigmoid functions. Model parameters are found by minimizing the cross-entropy loss via minibatch gradient descent with the Adam optimizer <ref type="bibr" target="#b23">Kingma and Ba (2014)</ref> and an early stopping patience of 10, i.e. we stop training if the validation loss does not decrease for 5 consecutive evaluation phases. In order to limit overfitting, we apply the standard regularization techniques of weight decay and dropout <ref type="bibr" target="#b42">Srivastava et al. (2014)</ref>. Additionally, batch-normalization <ref type="bibr" target="#b21">Ioffe and Szegedy (2015)</ref> has been used in every layer to stabilize training and increase convergence speed. Model hyperparameters (weight decay coefficient, dropout rate, hidden layer sizes, batch size, learning rate, number of feedforward layers in the inception module, number of feedforward layers for the final classification) are optimized on the validation sets using bayesian optimization with a tree parzen estimator surrogate function <ref type="bibr" target="#b3">Bergstra et al. (2011)</ref>. Table <ref type="table" target="#tab_1">3</ref> shows the hyperparameter ranges defining the search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines.</head><p>On the large scale inductive datasets, we compare our method to GCN Kipf and Welling ( <ref type="formula">2017</ref>  <ref type="formula">2019</ref>), which constitute the current state-of-the-art. To make the comparison fair, all models have 2 graph convolutional layers. The results for the baselines are reported from <ref type="bibr" target="#b51">Zeng et al. (2019)</ref>. On the smaller transductive datasets, we compare to the well established methods GCN Kipf and Welling (2017), <ref type="bibr">GAT Velickovic et al. (2018)</ref>, <ref type="bibr">JK Xu et al. (2018)</ref>, <ref type="bibr">GIN Xu et al. (2019</ref><ref type="bibr">), ARMA Bianchi et al. (2019b)</ref>, and the current state-of-the-art DIGL <ref type="bibr" target="#b26">Klicpera et al. (2019)</ref>.</p><p>Implementation and machine specifications. All experiments, including timings, are run on an AWS p2.8xlarge instance, with 8 NVIDIA K80 GPUs, 32 vCPUs, a processor Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz and 488GiB of RAM. SIGN is implemented using Pytorch <ref type="bibr" target="#b34">Paszke et al. (2019)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>Inductive. Table <ref type="table" target="#tab_6">8</ref> presents the results on the inductive datasets. In line with <ref type="bibr" target="#b51">Zeng et al. (2019)</ref>, we report the micro-averaged F1 score means and standard deviations computed over 10 runs. Here we match the state-of-the-art accuracy on Reddit, while consistently performing competitively on other datasets.</p><p>Runtime. While being comparable in terms of accuracy, our method has the advantage of being significantly faster than other methods for large graphs, both in training and inference. In Figure <ref type="figure" target="#fig_4">3</ref>, we plot the validation F1 score on Reddit from the start of the training as a function of runtime. SIGN converges faster than both other methods, while also converging to a much better F1 score than ClusterGCN. We don't report runtime results for GraphSAGE as it is substantially slower than other methods <ref type="bibr" target="#b9">Chiang et al. (2019)</ref>.</p><p>In Table <ref type="table" target="#tab_4">6</ref>, we report the preprocessing time needed to extract the data and prepare it for training, as well as the inference time on the entire test set for Reddit. For this experiment, we used the authors published code. While being slightly slower than GraphSAINT in the preprocessing phase, SIGN takes a fraction of the time for inference, outperforming GraphSAINT and GraphSAGE by over two orders of magnitude. It is important to note that while our implementation is in Pytorch, the implementations of GraphSAINT<ref type="foot" target="#foot_1">3</ref> and ClusterGCN<ref type="foot" target="#foot_2">4</ref> are in Tensorflow, which according to <ref type="bibr" target="#b9">Chiang et al. (2019)</ref>, is up to six times faster than PyTorch. Moreover, while GraphSAINT's preprocessing is parallelized, ours is not. Aiming at a further performance speedup, a TensorFlow implementation of our model, together with parallelization of the preprocessing routines, is left as future work.</p><p>Transductive. To further validate our method, we compare it to classic, as well as state-of-the-art, (non-scalable) GNN methods on well established small scale benchmarks.</p><p>Table <ref type="table" target="#tab_5">7</ref> presents the results on these transductive datasets, averaged over 100 different train/val/test splits. While our focus is on node-wise classification on large scale graphs, we show that SIGN is competitive also on smaller well-established transductive benchmarks, outperforming classical methods and getting close to the current state-of-the-art method (DIGL). This suggests that while being scalable and fast -therefore well-suited to large scale applications -it can also be effective to tackle problems involving modest sized graphs.</p><p>Effect of convolution size r. We perform a sensitivity analysis on the power parameter r, defining the size of the largest convolutional filter in the inception layer. On Reddit, r = 2 works best and we keep this configuration on all datasets. Figure <ref type="figure">4</ref> depicts the convergence test F1 scores as a function of r. It is interesting to see that while the model with r = 3 is a generalization of the model with r = 2, increasing r is actually detrimental in this case. We hypothesize this is due to the features aggregated from the 3-hop neighborhood of a node not being informative, but actually misleading for the model.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>Our results are consistent with previous reports <ref type="bibr" target="#b40">Shchur et al. (2018a)</ref> advocating in favor of simple architectures (with just a single graph convolutional layer) in graph learning tasks. Our architecture achieves a good trade off between simplicity, allowing efficient and scalable applications to very large graphs, and expressiveness achieving competitive performance in a variety of applications. For this reason, SIGN is well suited to industrial large-scale systems. Our architecture achieves competitive results on common graph learning benchmarks, while being significantly faster in training and up to two orders of magnitude faster in inference than other scalable approaches.</p><p>Extensions. Though in this paper we applied our model to the supervised node-wise classification setting, it is generic and can also be used for graph-wise classification tasks and unsupervised representation learning (e.g. graph autoencoders). The latter is a particularly important setting in recommender systems <ref type="bibr" target="#b2">Berg et al. (2018)</ref>.</p><p>While we focused our discussion on undirected graphs for the sake of simplicity, our model is straighforwardly applicable to directed graphs, in which case B is a non-symmetric diffusion operator. Furthermore, more complex aggregation operations e.g. higher-order Laplacians <ref type="bibr" target="#b0">Barbarossa and Sardellitti (2019)</ref> or directed diffusion based on graph motifs <ref type="bibr" target="#b31">Monti et al. (2018)</ref> can be straightforwardly incorporated as additional channels in the inception module.</p><p>Finally, while our method relies on linear graph aggregation operations of the form BX for efficient precomputation, it is possible to make the diffusion operator dependent on the node features (and edge features, if available) as a matrix of the form B(X).</p><p>Limitations. Graph attention <ref type="bibr" target="#b45">Veselkov et al. (2019)</ref> and similar mechanisms <ref type="bibr" target="#b30">Monti et al. (2017)</ref> require a more elaborate parametric aggregation operator of the form B θ (X), where θ are learnable parameters. This precludes efficient precomputation, which is key to the efficiency of our approach. Attention can be implemented in our scheme by training on a small subset of the graph to first </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Hamilton et al. (2017a); Ying et al. (2018a); Chen et al. (2018); Huang et al. (2018); Chen and Zhu (2018) attempt to alleviate the cost of training graph neural networks by selecting a small number of neighbors. GraphSAGE Hamilton et al. (2017a) uniformly randomly samples the neighborhood of a given node. PinSAGE Ying et al. (2018a) uses random walks to improve the quality of such approximation. ClusterGCN Chiang et al. (2019) clusters the graph and enforces diffusion of information only within the computed clusters. GraphSAINT Chiang et al. (2019) uses unbiased estimators of neighborhood graphs. They propose multiple methods to sample minibatch subgraphs during training while using normalization technique to eliminate bias.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 1: The SIGN architecture with the inception layer highlighted. Θ (i) represents the i-th dense layer required for producing filters of radius i, | is the concatenation operation and Ω the dense layer used to compute the final predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparison of different sampling strategies. In node-wise and layer-wise sampling, only a fraction of the nodes in the batch are optimization nodes (in orange) where the loss is computed, graph-wise operates on entire subgraphs of optimization nodes, making more efficient use of each batch.</figDesc><graphic url="image-3.png" coords="5,373.44,73.06,57.02,144.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>), FastGCN Chen et al. (2018), Stochastic-GCN Chen and Zhu (2018), AS-GCN Huang et al. (2018), GraphSAGE Hamilton et al. (2017a), ClusterGCN Chiang et al. (2019), and Graph-SAINT Zeng et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Convergence in time for various methods on Reddit dataset. Our method attains much faster training convergence. We don't report timing results for GraphSAGE as it is substantially slower than other methods Chiang et al. (2019).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2</head><label>2</label><figDesc>compares the complexity of our SIGN model to other scalable architectures GraphSAGE, ClusterGCN, and GraphSAINT.</figDesc><table><row><cell></cell><cell>Preprocessing</cell><cell>Forward Pass</cell></row><row><cell>GraphSAGE Hamilton et al. (2017a)</cell><cell>O(k L N )</cell><cell>O(k L N d 2 )</cell></row><row><cell>ClusterGCN Chiang et al. (2019)</cell><cell>O(|E|)</cell><cell>O(L|E|d + LN d 2 )</cell></row><row><cell>GraphSAINT Zeng et al. (2019)</cell><cell>O(kN )</cell><cell>O(L|E|d + LN d 2 )</cell></row><row><cell>SIGN (Ours)</cell><cell>O(r|E|d)</cell><cell>O(LN d 2 )</cell></row><row><cell>Table</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Ranges explored during the hyperparameter optimization in the form of[low, high]. Inception FF and Classification FF are the number of feedforward layers in the representation part of the model (replacing Θ) and the classification part of the model (replacing Ω) respectively.</figDesc><table><row><cell cols="8">Learning Rate Batch Size Dropout Weight Decay Hidden Dimensions Inception FF Classification FF</cell></row><row><cell cols="2">Range [0.00001, 0.01] [32, 2048]</cell><cell>[0, 1]</cell><cell>[0, 0.01]</cell><cell cols="2">[16, 1000]</cell><cell>[1, 2]</cell><cell>[1, 2]</cell></row><row><cell></cell><cell>n</cell><cell>|E|</cell><cell>Avg. Degree</cell><cell>d</cell><cell cols="2">Classes Train / Val / Test</cell></row><row><cell cols="3">Reddit 232,965 11,606,919</cell><cell>50</cell><cell>602</cell><cell>41(s)</cell><cell>0.66 / 0.10 / 0.24</cell></row><row><cell>Yelp</cell><cell cols="2">716,847 6,977,410</cell><cell>10</cell><cell cols="3">300 100(m) 0.75 / 0.10 / 0.15</cell></row><row><cell>Flickr</cell><cell>89,250</cell><cell>899,756</cell><cell>10</cell><cell>500</cell><cell>7(s)</cell><cell>0.50 / 0.25 / 0.25</cell></row><row><cell>PPI</cell><cell>14,755</cell><cell>225,270</cell><cell>15</cell><cell cols="3">50 121(m) 0.66 / 0.12 / 0.22</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Summary of inductive datasets statistics with (s)ingle and (m)ulticlass.Yelp and PPI are multilabel classification problems: the objective of the former is to predict business attributes based on customer reviews, while the later task consists of predicting protein functions from the interactions of human tissue proteins.While our focus is on large graphs, we also experiment with smaller, but well established transductive datasets to compare SIGN to traditional GNN methods: Amazon-ComputersShchur et al. (2018b),Amazon-PhotosShchur et al. (2018b), and Coauthor-CSShchur et al. (2018b). These datasets are used in the most recent state-of-the-art methods evaluation presented in<ref type="bibr" target="#b26">Klicpera et al. (2019)</ref>. Following<ref type="bibr" target="#b26">Klicpera et al. (2019)</ref>, we use 20 training nodes per class; 1500 validation nodes are in Amazon-Photos and Amazon-Computers and 5000 in CoauthorCS. Dataset statistics are reported in Tables4</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Summary of transductive datasets statistics.</figDesc><table><row><cell></cell><cell>n</cell><cell>|E|</cell><cell>Avg. Degree</cell><cell>d</cell><cell cols="2">Classes Label rate</cell></row><row><cell>Computers</cell><cell cols="2">13,381 245,778</cell><cell>35.76</cell><cell>767</cell><cell>10(s)</cell><cell>0.015</cell></row><row><cell>Photos</cell><cell>7487</cell><cell>119,043</cell><cell>31,13</cell><cell>745</cell><cell>8(s)</cell><cell>0.021</cell></row><row><cell cols="3">Coauthor-CS 18,333 81,894</cell><cell>8.93</cell><cell>6805</cell><cell>15(s)</cell><cell>0.016</cell></row><row><cell></cell><cell></cell><cell cols="2">Preprocessing</cell><cell cols="2">Inference</cell><cell></cell></row><row><cell></cell><cell cols="5">ClusterGCN 415.29 ± 5.83 9.23 ± 0.10</cell><cell></cell></row><row><cell></cell><cell cols="2">GraphSAINT</cell><cell cols="3">34.29 ± 0.06 3.47 ± 0.03</cell><cell></cell></row><row><cell></cell><cell cols="5">SIGN (Ours) 234.27 ± 3.79 0.17 ± 0.00</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Mean and standard deviation of preprocessing and inference time in seconds on Reddit computed over 10 runs.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Our method attains much faster training convergence. We don't report timing results for GraphSAGE as it is substantially slower than other methods<ref type="bibr" target="#b9">Chiang et al. (2019)</ref>. Micro-averaged F1 score average and standard deviation over 100 train/val/test splits for different models, and a different model initialization for each split. The top three performance scores are highlighted as: First, Second, Third.</figDesc><table><row><cell></cell><cell></cell><cell>0.98</cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.97</cell><cell></cell></row><row><cell></cell><cell>Test f1 score</cell><cell>0.95 0.96</cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.94</cell><cell></cell></row><row><cell></cell><cell></cell><cell>1</cell><cell>2 r</cell><cell>3</cell></row><row><cell></cell><cell cols="4">Figure 4: Convergence test F1 scores on Red-</cell></row><row><cell></cell><cell cols="4">dit as a function of the convolution depth</cell></row><row><cell></cell><cell cols="3">(power parameter r).</cell></row><row><cell></cell><cell cols="4">Amazon-Computer Amazon-Photo CoauthorCS</cell></row><row><cell>GCN Kipf and Welling (2017)</cell><cell cols="2">84.75 ± 0.23</cell><cell>92.08 ± 0.20</cell><cell>91.83 ± 0.08</cell></row><row><cell>GAT Velickovic et al. (2018)</cell><cell cols="2">45.37 ± 4.20</cell><cell>53.40 ± 5.49</cell><cell>90.89 ± 0.13</cell></row><row><cell>JK Xu et al. (2018)</cell><cell cols="2">83.33 ± 0.27</cell><cell>91.07 ± 0.26</cell><cell>91.11 ± 0.09</cell></row><row><cell>GIN Xu et al. (2019)</cell><cell cols="2">55.44 ± 0.83</cell><cell>68.34 ± 1.16</cell><cell>-</cell></row><row><cell>ARMA Bianchi et al. (2019b)</cell><cell cols="2">84.36 ± 0.26</cell><cell>91.41 ± 0.22</cell><cell>91.32 ± 0.08</cell></row><row><cell>DIGL Klicpera et al. (2019)</cell><cell cols="2">86.67 ± 0.21</cell><cell>92.93 ± 0.21</cell><cell>92.97 ± 0.07</cell></row><row><cell>SIGN (Ours)</cell><cell cols="2">85.93 ± 1.21</cell><cell>91.72 ± 1.20</cell><cell>91.98 ± 0.50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc><ref type="bibr" target="#b25">Kipf and Welling (2017)</ref> 0.933 ± 0.000 0.492 ± 0.003 0.515 ± 0.006 0.378 ± 0.001FastGCN Chen et al. (2018)   0.924 ± 0.001 0.504 ± 0.001 0.513 ± 0.032 0.265 ± 0.053 Stochastic-GCN Chen and Zhu (2018)  0.964 ± 0.001 0.482 ± 0.003 0.963 ± 0.010 0.640 ± 0.002 AS-GCN Huang et al. (2018) 0.958 ± 0.001 0.504 ± 0.002 0.687 ± 0.012 -GraphSage Hamilton et al. (2017a) 0.953 ± 0.001 0.501 ± 0.013 0.637 ± 0.006 0.634 ± 0.006 ClusterGCN Chiang et al. (2019) 0.954 ± 0.001 0.481 ± 0.005 0.875 ± 0.004 0.609 ± 0.005 GraphSAINT Zeng et al. (2019) 0.966 ± 0.001 0.511 ± 0.001 0.981 ± 0.004 0.653 ± 0.003 SIGN (Ours)0.966 ± 0.003 0.503 ± 0.003 0.965 ± 0.002 0.623 ± 0.005 Micro-averaged F1 score average and standard deviation over 10 runs with the same train/val/test split but different random model initialization. The top three performance scores are highlighted as: First, Second, Third.determine the attention parameters, then fixing them to precompute the diffusion operator that is used during training and inference. For the same reason, it is easy to do only one graph convolutional layer, though the architecture supports multiple linear layers. Architectures with many convolutional layers are achievable by layer-wise training.</figDesc><table><row><cell>Reddit</cell><cell>Flickr</cell><cell>PPI</cell><cell>Yelp</cell></row><row><cell>GCN</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">i.e. O(LN d 2 ), where d is the number of features, N the number of nodes in the training/testing graph and L is the overall number of feed-forward layers in the model.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">https://github.com/GraphSAINT/GraphSAINT</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">https://github.com/google-research/google-research/tree/master/cluster_gcn</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Barbarossa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefania</forename><surname>Sardellitti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11577</idno>
		<title level="m">Topological Signal Processing over Simplicial Complexes</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<title level="m">Relational inductive biases, deep learning, and graph networks</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Graph convolutional matrix completion</title>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Algorithms for Hyper-Parameter Optimization</title>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">S</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Bardenet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balázs</forename><surname>Kégl</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 24</title>
				<editor>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2546" to="2554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Maria</forename><surname>Filippo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniele</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cesare</forename><surname>Grattarola</surname></persName>
		</author>
		<author>
			<persName><surname>Alippi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.00481</idno>
		<title level="m">Mincut pooling in graph neural networks</title>
				<imprint>
			<date type="published" when="2019">2019a. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Graph Neural Networks with convolutional ARMA filters. CoRR abs/1901</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Filippo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniele</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Grattarola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cesare</forename><surname>Livi</surname></persName>
		</author>
		<author>
			<persName><surname>Alippi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.01343</idno>
		<ptr target="http://arxiv.org/abs/1901.01343" />
		<imprint>
			<date type="published" when="2019">2019b. 2019</date>
			<biblScope unit="page">1343</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Geometric Deep Learning: Going beyond Euclidean data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Proc. Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017-07">2017. July 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Wei-Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanqing</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2018">2018. 2019</date>
		</imprint>
	</monogr>
	<note>Stochastic Training of Graph Convolutional Networks</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph neural networks for icecube signal classification</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Choma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Gerhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Palczewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zahra</forename><surname>Ronaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prabhat</forename><surname>Prabhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wahid</forename><surname>Bhimji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spencer</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMLA</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Weighted graph cuts without eigenvectors a multilevel approach</title>
		<author>
			<persName><forename type="first">Yuqiang</forename><surname>Inderjit S Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><surname>Kulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1944" to="1957" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convolutional Networks on Graphs for Learning Molecular Fingerprints</title>
		<author>
			<persName><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Bombarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deciphering interaction fingerprints from protein molecular surfaces using geometric deep learning</title>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Gainza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="184" to="192" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural Message Passing for Quantum Chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<editor>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Kdd</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rex</forename><surname>Hamilton</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</editor>
		<editor>
			<persName><surname>Leskovec</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2016">2016. 2017a</date>
		</imprint>
	</monogr>
	<note>node2vec: Scalable feature learning for networks</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Representation learning on graphs: Methods and applications</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Engineering Bulletin</title>
		<imprint>
			<date type="published" when="2017">2017b. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>arXiv:cs.CV/1502.01852</idno>
		<title level="m">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adaptive Sampling Towards Fast Graph Representation Learning</title>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v37/ioffe15.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (Proceedings of Machine Learning Research</title>
				<editor>
			<persName><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">David</forename><surname>Blei</surname></persName>
		</editor>
		<meeting>the 32nd International Conference on Machine Learning ( Machine Learning Research<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">InceptionGCN: Receptive Field Aware Graph Convolutional Network for Disease Prediction</title>
		<author>
			<persName><forename type="first">Anees</forename><surname>Kazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayan</forename><surname>Shekarforoush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Arvind</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hendrik</forename><surname>Burwinkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerome</forename><surname>Vivar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Kortüm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyed-Ahmad</forename><surname>Ahmadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shadi</forename><surname>Albarqouni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Processing in Medical Imaging</title>
				<editor>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Albert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">James</forename><forename type="middle">C</forename><surname>Chung</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paul</forename><forename type="middle">A</forename><surname>Gee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Siqi</forename><surname>Yushkevich</surname></persName>
		</editor>
		<editor>
			<persName><surname>Bao</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="73" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<title level="m">Adam: A Method for Stochastic Optimization. International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Bayesian Deep Learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Diffusion Improves Graph Learning</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Weißenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donnie</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989">1989. 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Ron</forename><surname>Levie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gitta</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName><surname>Kutyniok</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.12972</idno>
		<title level="m">Transferability of Spectral Graph Convolutional Neural Networks</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cayleynets: Graph convolutional neural networks with complex rational spectral filters</title>
		<author>
			<persName><forename type="first">Ron</forename><surname>Levie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><surname>Michael M Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Signal Proc</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="97" to="109" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Geometric Deep Learning on Graphs and Manifolds Using Mixture Model CNNs</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName><surname>Michael M Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.06673</idno>
		<ptr target="http://arxiv.org/abs/1902.06673" />
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<editor>
			<persName><forename type="first">Cvpr</forename><forename type="middle">Federico</forename><surname>Monti</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Davide</forename><surname>Eynard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Damon</forename><surname>Mannion</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1902">2016. 2017. 2019. 1902. 2019</date>
			<biblScope unit="page">6673</biblScope>
		</imprint>
	</monogr>
	<note>Geometric deep learning on graphs and manifolds using mixture model cnns</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Motifnet: a motif-based graph convolutional network for directed graphs</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Otness</surname></persName>
		</author>
		<author>
			<persName><surname>Michael M Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DSW</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">T</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takanori</forename><surname>Maehara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09550</idno>
		<title level="m">Revisiting Graph Neural Networks: All We Have is Low-Pass Filters</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Disease prediction using graph convolutional networks: Application to Autism Spectrum Disorder and Alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Parisot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sofia</forename><forename type="middle">Ira</forename><surname>Ktena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enzo</forename><surname>Ferrante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Guerrero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med Image Anal</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="117" to="130" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Alche-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Learning human-object interactions by graph parsing neural networks</title>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoxiong</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="401" to="417" />
		</imprint>
	</monogr>
	<note>In ECCV</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">ncRNA Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD Workshop on Deep Learning on Graphs</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename><surname>Ah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pitfalls of Graph Neural Network Evaluation</title>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Relational Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2018">2018a. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Pitfalls of Graph Neural Network Evaluation</title>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05868</idno>
		<ptr target="http://arxiv.org/abs/1811.05868" />
		<imprint>
			<date type="published" when="2018">2018b. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014-01">2014. Jan. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">HyperFoods: Machine intelligent mapping of cancer-beating molecules in foods</title>
		<author>
			<persName><forename type="first">Kirill</forename><surname>Veselkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Simplifying Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">How Powerful are Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ryGs6iA5Km" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Representation Learning on Graphs with Jumping Knowledge Networks</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Graph Convolutional Neural Networks for Web-Scale Recommender Systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2018">2018a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">GraphSAINT: Graph Sampling Based Inductive Learning Method</title>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><forename type="middle">K</forename><surname>Prasanna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04931</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In NIPS</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04202</idno>
		<title level="m">Deep learning on graphs: A survey</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Modeling polypharmacy side effects with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monica</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="457" to="466" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Predicting multicellular function through multi-layer tissue networks</title>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btx252</idno>
		<ptr target="https://doi.org/10.1093/bioinformatics/btx252" />
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="190" to="198" />
			<date type="published" when="2017-07">2017. Jul 2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
