<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Bidirectional Language-Knowledge Graph Pretraining</title>
				<funder ref="#_Xa4UgU2">
					<orgName type="full">HAI</orgName>
				</funder>
				<funder ref="#_fQmqNcj">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder>
					<orgName type="full">NEC</orgName>
				</funder>
				<funder ref="#_XQeJwxk">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder>
					<orgName type="full">UnitedHealth Group</orgName>
				</funder>
				<funder>
					<orgName type="full">Hitachi</orgName>
				</funder>
				<funder>
					<orgName type="full">Amazon</orgName>
				</funder>
				<funder ref="#_r66n7yU #_RMfRsMt">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
				<funder>
					<orgName type="full">Wu Tsai Neurosciences Institute</orgName>
				</funder>
				<funder>
					<orgName type="full">Intel</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
							<email>antoineb@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
							<email>hyren@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xikun</forename><surname>Zhang</surname></persName>
							<email>xikunz2@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
							<email>manning@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
							<email>pliang@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Bidirectional Language-Knowledge Graph Pretraining</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pretraining a language model (LM) on text has been shown to help various downstream NLP tasks. Recent works show that a knowledge graph (KG) can complement text data, offering structured background knowledge that provides a useful scaffold for reasoning. However, these works are not pretrained to learn a deep fusion of the two modalities at scale, limiting the potential to acquire fully joint representations of text and KG. Here we propose DRAGON (Deep Bidirectional Language-Knowledge Graph Pretraining), a self-supervised method to pretrain a deeply joint language-knowledge foundation model from text and KG at scale. Specifically, our model takes pairs of text segments and relevant KG subgraphs as input and bidirectionally fuses information from both modalities. We pretrain this model by unifying two self-supervised reasoning tasks, masked language modeling and KG link prediction. DRAGON outperforms existing LM and LM+KG models on diverse downstream tasks including question answering across general and biomedical domains, with +5% absolute gain on average. In particular, DRAGON achieves strong performance on complex reasoning about language and knowledge (+10% on questions involving long contexts or multi-step reasoning) and low-resource QA (+8% on OBQA and RiddleSense), and new state-of-the-art results on various BioNLP tasks. Our code and trained models are available at https://github.com/michiyasunaga/dragon. Glamping GNN 1st Layer Glamping GNN Final Layer camp trip beach school beach RoBERTa: A. camp (?) GreaseLM: C. camp (?) Glamping: B. school (?) school trip Glamping GNN 1st Layer Glamping GNN Final Layer garage trip beach school beach garage movie Glamping GNN 1st Layer Glamping GNN Final Layer entertain ment station movie Glamping GNN 1st Layer Glamping GNN Final Layer live movie movie entertain ment station live Model Prediction RoBERTa: B. school (?) GreaseLM: B. school (?) Glamping: C. beach (?) Model Prediction B. movie theater (?) GreaseLM: B. movie theater (?) Glamping: B. movie theater (?) Model Prediction B. movie theater (?) GreaseLM: B. movie theater (?)</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pretraining learns self-supervised representations from massive raw data to help various downstream tasks <ref type="bibr" target="#b0">[1]</ref>. Language models (LMs) pretrained on large amounts of text data, such as BERT <ref type="bibr" target="#b1">[2]</ref> and GPTs <ref type="bibr" target="#b2">[3]</ref>, have shown strong performance on many natural language processing (NLP) tasks. The success of these models comes from deeply interactive (contextualized) representations of input tokens learned at scale via self-supervision <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref>. Meanwhile, large knowledge graphs (KGs), such as Freebase <ref type="bibr" target="#b4">[5]</ref>, Wikidata <ref type="bibr" target="#b5">[6]</ref> and ConceptNet <ref type="bibr" target="#b6">[7]</ref>, can provide complementary information to text data. KGs offer structured background knowledge by representing entities as nodes and relations between them as edges, and also offer scaffolds for structured, multi-step reasoning about entities <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref> ( ?3.4.1). The dual strengths of text data and KGs motivate research in pretraining deeply interactive representations of the two modalities at scale. How to effectively combine text and KGs for pretraining is an open problem and presents challenges. Given text and KG, we need both (i) a deeply bidirectional model for the two modalities to interact, and (ii) a self-supervised objective to learn joint reasoning over text and KG at scale. Several existing works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref> propose methods for self-supervised pretraining, but they fuse text and KG in a shallow or uni-directional manner. Another line of work <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> proposes bidirectional models for text and KG, but these models focus on finetuning on labeled downstream tasks and do not perform 36th Conference on Neural Information Processing Systems (NeurIPS 2022).</p><p>Figure <ref type="figure">1</ref>: Overview of our approach, DRAGON. Left: Given raw data of a text corpus and a large knowledge graph, we create aligned (text, local KG) pairs by sampling a text segment from the corpus and extracting a relevant subgraph from the KG ( ?2.1). As the structured knowledge in KG can ground the text and the text can provide the KG with rich context for reasoning, we aim to pretrain a language-knowledge model jointly from the text-KG pairs <ref type="bibr">(DRAGON)</ref>. Right: To model the interactions over text and KG, DRAGON uses a cross-modal encoder that bidirectionally exchanges information between them to produce fused text token and KG node representations ( ?2.2). To pretrain DRAGON jointly on text and KG, we unify two self-supervised reasoning tasks: (1) masked language modeling, which masks some tokens in the input text and then predicts them, and (2) link prediction, which holds out some edges from the input KG and then predicts them. This joint objective encourages text and KG to mutually inform each other, facilitating the model to learn joint reasoning over text and KG ( ?2.3).</p><p>self-supervised learning. Consequently, existing methods may have limited their potential to model and learn deep interactions over text and KG.</p><p>To address both of the above challenges and fully unify the strengths of text and KG, we propose DRAGON (Deep Bidirectional Language-Knowledge Graph Pretraining), an approach that performs deeply bidirectional, self-supervised pretraining of a language-knowledge model from text and KG. DRAGON has two core components: a cross-modal model that bidirectionally fuses text and KG, and a bidirectional self-supervised objective that learns joint reasoning over text and KG. Concretely, as in Figure <ref type="figure">1</ref>, we take a text corpus and a KG as raw data, and create inputs for the model by sampling a text segment from the corpus and extracting a relevant subgraph from the KG via entity linking, obtaining a (text, local KG) pair. We use a cross-modal model to encode this input into fused representations, where each layer of the model encodes the text with an LM and the KG with a graph neural network (GNN), and fuses the two with a bidirectional modality interaction module (GreaseLM <ref type="bibr" target="#b8">[9]</ref>). We pretrain this model by unifying two self-supervised reasoning tasks: (1) masked language modeling (MLM), which masks and predicts tokens in the input text, and (2) link prediction, which drops and predicts edges in the input KG. The intuition is that by combining the two tasks, MLM makes the model use the text jointly with structured knowledge in the KG to reason about masked tokens in the text (e.g., in Figure <ref type="figure">1</ref>, using the "round brush"-"art supply" multi-hop path from the KG helps), and link prediction makes the model use the KG structure jointly with the textual context to reason about missing links in the KG (e.g., recognizing that "round brush could be used for hair" from the text helps). This joint objective thus enables text to be grounded by KG structure and KG to be contextualized by text simultaneously, producing a deeply-unified language-knowledge pretrained model where information flows bidirectionally between text and KG for reasoning.</p><p>We pretrain DRAGON in two domains: a general domain, using the Book corpus and ConceptNet KG <ref type="bibr" target="#b6">[7]</ref> ( ?3), and a biomedical domain, using the PubMed corpus and UMLS KG <ref type="bibr" target="#b16">[17]</ref> ( ?4). We show that DRAGON improves on existing LM and LM+KG models on diverse downstream tasks across domains. For the general domain, DRAGON outperforms RoBERTa <ref type="bibr" target="#b17">[18]</ref>, our base LM without KGs, on various commonsense reasoning tasks such as CSQA, OBQA, RiddleSense and HellaSwag, with +8% absolute accuracy gain on average. For the biomedical domain, DRAGON improves on the previous best LM, BioLinkBERT <ref type="bibr" target="#b18">[19]</ref>, and sets a new state of the art on BioNLP tasks such as MedQA and PubMedQA, with +3% accuracy gain. In particular, DRAGON exhibits notable improvements on QA tasks involving complex reasoning (+10% gain on multi-step, negation, hedge, or long context reasoning) and on downstream tasks with limited training data (+8% gain). These results show that our deep bidirectional self-supervision over text and KG produces significantly improved language-knowledge representations compared to existing models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related work</head><p>Knowledge-augmented LM pretraining. Knowledge integration is an active research for improving LMs. One line of works is retrieval-augmented LMs <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>, which retrieve relevant text from a corpus and integrate it into LMs as additional knowledge. Orthogonal to these works, we focus on using knowledge bases as background knowledge, to ground reasoning about entities and facts.</p><p>Closest to our work are works that integrate knowledge bases in LM pretraining. One line of research aims to add entity features to LMs <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>; Some works use the KG entity information or structure to create additional training signals <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>; Several works add KG triplet information directly to the LM input <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>. While these methods have achieved substantial progress, they typically propagate information between text and KG in a shallow or uni-directional (e.g., KG to text) manner, which might limit the potential to perform fully joint reasoning over the two modalities. To improve on the above works, we propose to bidirectionally interact text and KG via a deep cross-modal model and joint self-supervision, so that text and KG are grounded and contextualized by each other. We find that this improves model performance on various reasoning tasks ( ?3). Another distinction is that existing works in this space typically focus on adding entity-or triplet-level knowledge from KGs to LMs, and focus on solving entity/relation classification tasks. Our work significantly expands this scope in that we use larger KG subgraphs (200 nodes) as input to enable richer contextualization between KG and text, and we achieve performance improvements on a broader set of NLP tasks including QA, reasoning and text classification tasks. KG-augmented question answering. Various works designed KG-augmented reasoning models for question answering <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref>. In particular, recent works such as QAGNN <ref type="bibr" target="#b7">[8]</ref> and GreaseLM <ref type="bibr" target="#b8">[9]</ref> suggest that a KG can scaffold reasoning about entities with its graph structure, and help for complex question answering (e.g., negation, multi-hop reasoning). These works typically focus on training or finetuning models on particular QA datasets. In contrast, we generalize this and integrate KG-augmented reasoning into general-purpose pretraining. Our motivation is that self-supervised pretraining allows the model to learn from larger and more diverse data, helping to learn richer interactions between text and KGs and to acquire more diverse reasoning abilities beyond specific QA tasks. We find that our proposed pretraining approach (DRAGON) offers significant boosts over the baseline QA models (e.g. GreaseLM) on diverse downstream tasks ( ?3). This opens a new research avenue in scaling up various carefully-designed QA models to pretraining. KG representation learning. Our link prediction task used in pretraining is motivated by research in KG representation learning. Link prediction is a fundamental task in KGs <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>, and various works study methods to learn KG entity and relation embeddings for link prediction, such as TransE <ref type="bibr" target="#b44">[45]</ref>, DistMult <ref type="bibr" target="#b45">[46]</ref> and RotatE <ref type="bibr" target="#b46">[47]</ref>. Several works additionally use textual data or pretrained LMs to aid learning KG embeddings and link prediction <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref>. While these works focus on the KG-side representations, we extend the scope and use the KG-side objective (link prediction) jointly with a text-side objective (language modeling) to train a mutually-interactive text-KG model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Deep Bidirectional Language-Knowledge Graph Pretraining (DRAGON)</head><p>We propose DRAGON, an approach that performs deeply bidirectional, self-supervised pretraining of a language-knowledge model from text and KG. Specifically, as illustrated in Figure <ref type="figure">1</ref>, we take a text corpus and a large knowledge graph as raw data, and create input instances for the model by sampling coarsely-aligned (text segment, local KG) pairs ( ?2.1). To learn mutual interactions over text and KG, DRAGON consists of a cross-modal encoder (GreaseLM) that fuses the input text-KG pair bidirectionally ( ?2.2), and a pretraining objective that performs bidirectional self-supervision on the text-KG input ( ?2.3). Our pretraining objective unifies masked language modeling (MLM) and KG link prediction (LinkPred) to make text and KG mutually inform each other and learn joint reasoning over them. Finally, we describe how we finetune the pretrained DRAGON model for downstream tasks ( ?2.4). While each individual piece of our approach (GreaseLM, MLM, LinkPred) is not new in itself, we are the first to bring them together effectively and demonstrate that the resulting model has strong empirical results. ( ?3, ?4).</p><p>Definitions. We define a text corpus W as a set of text segments W = {W }, and each text segment W as a sequence of tokens (words), W = (w 1 , ..., w I ). We define a knowledge graph (KG) as a multi-relational graph G = (V, E), where V is the set of entity nodes in the KG and E ? V ? R ? V is the set of edges (triplets) that connect nodes in V, with R being the set of relation types {r}. Each triplet (h, r, t) in a KG can represent a knowledge fact such as (Paris, in, France). As a raw KG is often large, with millions of nodes, a subgraph of the raw KG (local KG) is considered: G = (V, E) where V = {v 1 , ..., v J } ? V and E ? E. We define a language-knowledge model to be a composition of two functions, f head (f enc (X)), where the encoder f enc takes in an input X = (text segment W, local KG G), and produces a contextualized vector representation for each text token, (H 1 , ..., H I ), and for each KG node, (V 1 , ..., V J ). A language model is a special case of a language-knowledge model with no KG (J = 0). The head f head uses these representations to perform self-supervised tasks in the pretraining step and to perform downstream tasks in the finetuning step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Input representation</head><p>Given a text corpus W and a large knowledge graph G, we create input instances for the model by preparing (text segment W , local KG G) pairs. We want each pair's text and KG to be (roughly) semantically aligned so that the text and KG can mutually inform each other and facilitate the model to learn interactive reasoning between the two modalities. Specifically, for each text segment W from W, we extract a relevant local KG G for it from G via the following KG retrieval process.</p><p>KG retrieval. Given a text segment W , we link entity mentions in W to entity nodes in G to get an initial set of nodes V el . We then add their 2-hop bridge nodes from G to get the total retrieved nodes V ? V. Lastly, we add all edges that span these nodes in G to get E ? E, which yields the final local KG, G = (V, E), as well as our final input instance X = (W, G). Appendix B.1 provides more details on KG retrieval. Henceforth, we use "KG" to refer to this local KG G unless noted otherwise.</p><p>Modality interaction token/node. For each resulting (text, KG) pair, we further add a special token (interaction token) w int to the text and a special node (interaction node) v int to the KG, which will serve as an information pooling point for each modality as well as an interface for modality interaction in our cross-modal encoder ( ?2.2). Specifically, we prepend w int to the original text W = (w 1 , ..., w I ), and connect v int to the entity-linked nodes in the original KG, V el ? V = {v 1 , ..., v J }, using a new relation type r el . The interaction token and node can also be used to produce a pooled representation of the whole input, e.g., when finetuning for classification tasks ( ?2.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Cross-modal encoder</head><p>To model mutual interactions over the text and KG, we use a bidirectional sequence-graph encoder for f enc which takes in the text tokens and KG nodes and exchanges information across them for multiple layers to produce a fused representation of each token and node (Figure <ref type="figure">1</ref> right):</p><formula xml:id="formula_0">(H int , H 1 , ..., H I ), (V int , V 1 , ..., V J ) = f enc ((w int , w 1 , ..., w I ), (v int , v 1 , ..., v J ))<label>(1)</label></formula><p>While we may use any deep bidirectional sequence-graph encoder for f enc , for controlled comparison with existing works, we adopt the existing top-performing sequence-graph architecture, GreaseLM <ref type="bibr" target="#b8">[9]</ref>, which combines Transformers <ref type="bibr" target="#b53">[54]</ref> and graph neural networks (GNNs) to fuse text-KG inputs.</p><p>Specifically, GreaseLM first uses N layers of Transformer language model (LM) layers to map the input text into initial token representations, and uses KG node embeddings to map the input KG nodes into initial node representations,</p><formula xml:id="formula_1">(H (0) int , H (0) 1 , ..., H<label>(0)</label></formula><formula xml:id="formula_2">I ) = LM-Layers(w int , w 1 ..., w I ),<label>(2)</label></formula><formula xml:id="formula_3">(V (0) int , V (0) 1 , ..., V<label>(0)</label></formula><formula xml:id="formula_4">J ) = Node-Embedding(v int , v 1 , ..., v J ).<label>(3)</label></formula><p>Then it uses M layers of text-KG fusion layers to encode these token/node representations jointly into the final token/node representations,</p><formula xml:id="formula_5">(H int , ..., H I ), (V int , ..., V J ) = Fusion-Layers((H (0) int , ..., H<label>(0)</label></formula><formula xml:id="formula_6">I ), (V (0) int , ..., V (0) J )),<label>(4)</label></formula><p>where each of the fusion layers ( = 1, ..., M ) performs the following:</p><formula xml:id="formula_7">( H ( ) int , H ( ) 1 , ..., H ( ) I ) = LM-Layer(H ( -1) int , H ( -1) 1 , ..., H ( -1) I ),<label>(5)</label></formula><formula xml:id="formula_8">( V ( ) int , V ( ) 1 , ..., V ( ) J ) = GNN-Layer(V ( -1) int , V ( -1) 1 , ..., V ( -1) J ),<label>(6)</label></formula><p>[H</p><formula xml:id="formula_9">( ) int ; V ( ) int ] = MInt([ H ( ) int ; V ( ) int ]).<label>(7)</label></formula><p>Here GNN induces graph structure-aware representations of KG nodes, [? ; ?] does concatenation, and MInt (modality interaction module) exchanges information between the interaction token (text side) and interaction node (KG side) via an MLP. For more details on GreaseLM, we refer readers to <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Pretraining objective</head><p>We aim to pretrain the DRAGON model so that it learns joint reasoning over text and a KG. To ensure that the text and KG mutually inform each other and the model learns bidirectional information flow, we unify two self-supervised reasoning tasks: masked language modeling and KG link prediction.</p><p>Masked language modeling (MLM). MLM is a common pretraining task used for language models (e.g., BERT <ref type="bibr" target="#b1">[2]</ref>, RoBERTa <ref type="bibr" target="#b17">[18]</ref>), which masks some tokens in the input text and predicts them. This task makes the model use non-masked context to reason about masked tokens, and in particular, as our approach takes a joint text-KG pair as input, we expect that MLM can encourage the model to learn to use the text jointly with structured knowledge in the KG to reason about masks in the text (e.g., in the example of Figure <ref type="figure">1</ref>, besides the textual context, recognizing the "round brush"-"art supply" path from the KG can help together to predict the masked tokens "art supplies").</p><p>Concretely, to perform the MLM task, we mask a subset of tokens in the input text, M ? W , with a special token [MASK], and let the task head f head be a linear layer that takes the contextualized token vectors {H i } from the encoder to predict the original tokens. The objective is a cross-entropy loss:</p><formula xml:id="formula_10">L MLM = - i?M log p(w i | H i ).<label>(8)</label></formula><p>Link prediction (LinkPred). While the MLM task predicts for the text side, link prediction holds out some edges and predicts them for the input KG. Link prediction is a fundamental task in KGs <ref type="bibr" target="#b46">[47]</ref> and makes the model use the structure of KGs to perform reasoning (e.g., using a compositional path "X's mother's husband is Y" to deduce a missing link "X's father is Y"). In particular, as our approach takes a joint text-KG pair as input, we expect that link prediction can encourage the model to learn to use the KG structure jointly with the textual context to reason about missing links in the KG (e.g., in Figure <ref type="figure">1</ref>, besides the KG structure, recognizing that "round brush could be used for hair" from the text can help together to predict the held-out edge (round_brush, at, hair)).</p><p>Concretely, to perform the link prediction task, we hold out a subset of edge triplets from the input KG, S = {(h, r, t)} ? E. For the task head f head , we adopt a KG representation learning framework, which maps each entity node (h or t) and relation (r) in the KG to a vector, h, t, r, and defines a scoring function ? r (h, t) to model positive/negative triplets. Specifically, we let h = V h , t = V t , r = R r , with {V j } being the contextualized node vectors from the encoder, and R = {r 1 , ..., r |R| } being learnable relation embeddings. We consider a KG triplet scoring function ? r (h, t) such as DistMult <ref type="bibr" target="#b45">[46]</ref>: h, r, t , TransE <ref type="bibr" target="#b44">[45]</ref>:</p><formula xml:id="formula_11">-h + r -t , RotatE [47]: -h r -t ,<label>(9)</label></formula><p>where ?, ?, ? denotes the trilinear dot product and the Hadamard product. A higher ? indicates a higher chance of (h, r, t) being a positive triplet (edge) instead of negative (no edge). We analyze the choices of scoring functions in ?3.4.3. For training, we optimize the objective:</p><formula xml:id="formula_12">L LinkPred = (h,r,t)?S ? ? -log ?(? r (h, t) + ?) + 1 n (h ,r,t ) log ?(? r (h , t ) + ?) ? ? ,<label>(10)</label></formula><p>where (h , r, t ) are n negative samples corresponding to the positive triplet (h, r, t), ? is the margin, and ? is the sigmoid function. The intuition of this objective is to make the model predict triplets of the held-out edges S as positive and other random triplets as negative.</p><p>Joint training. To pretrain DRAGON, we optimize the MLM and LinkPred objectives jointly: L = L MLM + L LinkPred . This joint objective unifies the effects of MLM and LinkPred, which encourage the model to simultaneously ground text with KG structure and contextualize KG with text, facilitating bidirectional information flow between text and KGs for reasoning. We show in ?3.4.3 that the joint objective yields a more performant model than using one of the objectives alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Finetuning</head><p>Lastly, we describe how we finetune DRAGON for downstream tasks such as text classification and multiple-choice QA (MCQA). Given an input text W (e.g., concatenation of a question and an answer choice in the case of MCQA), we follow the same steps as ?2.1 and ?2.2 to retrieve a relevant local KG G and encode them jointly into contextualized token/node vectors, (H int , H 1 , ...,</p><formula xml:id="formula_13">H I ), (V int , V 1 , ..., V J ).</formula><p>We then compute a pooled representation of the whole input as X = MLP(H int , V int , G), where G denotes attention-based pooling of {V j | v j ? {v 1 , ..., v J }} using H int as a query. Finally, the pooled representation X is used to perform the downstream task, in the same way as how the [CLS] representation is used in LMs such as BERT and RoBERTa.</p><p>The difference from GreaseLM is that while GreaseLM only performs finetuning as described in this section (hence, it is an LM finetuned with KGs), DRAGON performs self-supervised pretraining as described in ?2.3 (hence, it can be viewed as an LM pretrained + finetuned with KGs).</p><p>3 Experiments: General domain</p><p>We experiment with the proposed approach DRAGON in a general domain first. We pretrain DRAGON using the Book corpus and ConceptNet KG ( ?3.1), and evaluate on diverse downstream tasks ( ?3.2). We show that DRAGON significantly improves on existing models ( ?3.4). We extensively analyze the effect of DRAGON's key design choices such as self-supervision and use of KGs ( ?3.4.1, 3.4.2, 3.4.3). We also experiment in the biomedical domain in ?4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pretraining setup</head><p>Data. For the text data, we use BookCorpus <ref type="bibr" target="#b54">[55]</ref>, a general-domain corpus widely used in LM pretraining (e.g., BERT, RoBERTa). It has 6GB of text from online books. For the KG data, we use ConceptNet <ref type="bibr" target="#b6">[7]</ref>, a general-domain knowledge graph designed to capture background commonsense knowledge. It has 800K nodes and 2M edges in total. To create a training instance, we sample a text segment of length up to 512 tokens from the text corpus, then retrieve a relevant KG subgraph of size up to 200 nodes (details in Appendix B.1), by which we obtain an aligned (text, local KG) pair.</p><p>Implementation. For our encoder ( ?2.2), we use the exact same architecture as GreaseLM <ref type="bibr" target="#b8">[9]</ref> (19 LM layers followed by 5 text-KG fusion layers; 360M parameters in total). As done by <ref type="bibr" target="#b8">[9]</ref>, we initialize parameters in the LM component with the RoBERTa-Large release <ref type="bibr" target="#b17">[18]</ref> and initialize the KG node embeddings with pre-computed ConceptNet entity embeddings (details in Appendix B.2).</p><p>For the link prediction objective ( ?2.3, Equation <ref type="formula" target="#formula_12">10</ref>), we use DistMult <ref type="bibr" target="#b45">[46]</ref> for KG triplet scoring, with a negative exampling of 128 triplets and a margin of ? = 0. To pretrain the model, we perform MLM with a token masking rate of 15% and link prediction with an edge drop rate of 15%. We pretrain for 20,000 steps with a batch size of 8,192 and a learning rate of 2e-5 for parameters in the LM component and 3e-4 for the others. Training took 7 days on eight A100 GPUs using FP16.</p><p>Additional details on the hyperparameters can be found in Appendix B.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Downstream evaluation tasks</head><p>We finetune and evaluate DRAGON on nine diverse commonsense reasoning benchmarks: Common-senseQA (CSQA) <ref type="bibr" target="#b55">[56]</ref>, OpenbookQA (OBQA) <ref type="bibr" target="#b56">[57]</ref>, RiddleSense (Riddle) <ref type="bibr" target="#b57">[58]</ref>, AI2 Reasoning Challenge-Challenge Set (ARC) <ref type="bibr" target="#b58">[59]</ref>, CosmosQA <ref type="bibr" target="#b59">[60]</ref>, HellaSwag <ref type="bibr" target="#b60">[61]</ref>, Physical Interaction QA (PIQA) <ref type="bibr" target="#b61">[62]</ref>, Social Interaction QA (SIQA) <ref type="bibr" target="#b62">[63]</ref>, and Abductive Natural Language Inference (aNLI) <ref type="bibr" target="#b63">[64]</ref>. For CSQA, we follow the in-house data splits used by prior works <ref type="bibr" target="#b31">[32]</ref>. For OBQA, we follow the original setting where the models only use the question as input and do not use the extra science facts. Appendix B.4 provides the full details on these tasks and data splits. Hyperparameters used for finetuning can be found in Appendix B.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Baselines</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LM.</head><p>To study the effect of using KGs, we compare DRAGON with the vanilla language model, RoBERTa <ref type="bibr" target="#b17">[18]</ref>. As we initialize DRAGON's parameters using the RoBERTa-Large release ( ?3.1), for fair comparison, we let the baseline be such that we take the RoBERTa-Large release and continue pretraining it with the vanilla MLM objective on the same text data for the same number of steps as DRAGON. Hence, the only difference is that DRAGON uses KGs during pretraining while RoBERTa does not. We then perform standard LM finetuning of RoBERTa on downstream tasks.</p><p>LM finetuned with KG. We also compare with existing KG-augmented QA models, QAGNN <ref type="bibr" target="#b7">[8]</ref> and GreaseLM <ref type="bibr" target="#b8">[9]</ref>, which finetune a vanilla LM (i.e. RoBERTa-Large) with a KG on downstream tasks, but do not pretrain with a KG. GreaseLM is the existing top-performing model in this paradigm.</p><p>As we use the same encoder architecture as GreaseLM for DRAGON, the only difference from GreaseLM is that DRAGON performs self-supervised pretraining while GreaseLM does not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results</head><p>Table <ref type="table" target="#tab_0">1</ref> shows performance on the 9 downstream commonsense reasoning tasks. Across all tasks, DRAGON consistently outperforms the existing LM (RoBERTa) and KG-augmented QA models (QAGNN, GreaseLM), e.g., +7% absolute accuracy boost over RoBERTa and +5% over GreaseLM on OBQA. These accuracy boosts indicate the advantage of DRAGON over RoBERTa (KG reasoning) and over GreaseLM (pretraining). The gain is especially significant on datasets that have small training data such as ARC, Riddle and OBQA, and datasets that require complex reasoning such as CosmosQA and HellaSwag, which we analyze in more detail in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Analysis: Effect of knowledge graph</head><p>The first key contribution of DRAGON (w.r.t. existing LM pretraining methods) is that we incorporate KGs. We find that this significantly improves the model's performance for robust and complex reasoning, such as resolving multi-step reasoning and negation, as we discuss below.</p><p>Quantitative analysis. In Table <ref type="table" target="#tab_1">2</ref>, we study downstream task performance of DRAGON on questions involving complex reasoning. Building on <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, we consider several proxies to categorize complex questions: (i) presence of negation (e.g. no, never), (ii) presence of conjunction (e.g. and, but), (iii) presence of hedge (e.g. sometimes, maybe), (iv) number of prepositional phrases, and (v) number of entity mentions. Having negation or conjunction indicates logical multi-step reasoning, having more prepositional phrases or entity mentions indicates involving more reasoning steps or constraints, and having hedge terms indicates involving complex textual nuance. DRAGON significantly outperforms the baseline LM (RoBERTa) across all these categories (e.g., +14% accuracy for negation), which confirms that our joint language-knowledge pretraining boosts reasoning performance. DRAGON also consistently outperforms the existing KG-augmented QA models (QAGNN, GreaseLM). We find that QAGNN and GreaseLM only improve moderately on RoBERTa for some categories like conjunction or many prepositional phrases (=2, 3), but DRAGON provides substantial boosts. This suggests that through self-supervised pretraining with larger and diverse data, DRAGON has learned more general-purpose reasoning abilities than the finetuning-only models like GreaseLM.</p><p>Qualitative analysis. Using the CSQA dataset, we further conducted case studies on the behavior of DRAGON's KG reasoning component, where we visualize how graph attention weights change given different question variations (Figure <ref type="figure" target="#fig_0">2</ref>). We find that DRAGON exhibits abilities to extrapolate DRAGON exhibits abilities to extrapolate and perform robust reasoning. DRAGON adjusts the entity attention weights and final predictions accordingly when conjunction or negation is given about entities (A1, A2) or when extra context is added to an original question (B1?B2), but existing models, RoBERTa and GreaseLM, struggle to predict the correct answers. A1: DRAGON's final GNN layer shows strong attention to "school" but weak attention to "trip", likely because the question states "and store one"-hence, the chair is not used for a trip. A2: DRAGON shows strong attention to "trip" and "beach", likely because the question now states "but not store one"-hence, the chair is used for a trip. B1?B2: DRAGON's final GNN layer shows strong attention to "movie" in the original question (B1), but after adding the extra context "don't enjoy pre-record" (B2), DRAGON shows strong attention to "live" and "concert", leading to making the correctly adjusted prediction "concert hall". One interpretation of these findings is that DRAGON leverages the KG's graph structure as a scaffold for performing complex reasoning. This insight is related to recent works that provide LMs with scratch space for intermediate reasoning <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b65">66]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>CosmosQA (10% train) PIQA    and perform robust reasoning. For instance, DRAGON adjusts the entity attention weights and final predictions accordingly when we add conjunction or negation about entities (A1, A2) or when we add extra context to an original question (B1?B2), but existing models, RoBERTa and GreaseLM, struggle to predict the correct answers. As these questions are more complex than ones typically seen in the CSQA training set, our insight is that while vanilla LMs (RoBERTa) and finetuning (GreaseLM) have limitation in learning complex reasoning, KG-augmented pretraining (DRAGON) helps acquire generalizable reasoning abilities that extrapolate to harder test examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Analysis: Effect of pretraining</head><p>Another key contribution of DRAGON (w.r.t. existing QA models like GreaseLM) is pretraining.</p><p>Here we discuss when and why our pretraining is useful. Considering the three core factors in machine learning (data, task complexity, and model capacity), pretraining helps when the available downstream task data is smaller compared to the downstream task complexity or model capacity.</p><p>Concretely, we find that DRAGON is especially helpful for the following three scenarios.</p><p>Downstream tasks with limited data. In Table <ref type="table" target="#tab_0">1</ref>, we find that DRAGON provides significant boosts over GreaseLM on downstream tasks with limited finetuning data available, such as ARC (3K training instances; +4% accuracy gain), Riddle (3K instances; +4% accuracy) and OBQA (5K instances; +5% accuracy). For other tasks, we also experimented with a low-resource setting where 10% of finetuning data is used (Table <ref type="table" target="#tab_3">3</ref>). Here we also see that DRAGON attains significant gains over GreaseLM (+5% accuracy on PIQA), suggesting the improved data-efficiency of DRAGON.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complex downstream tasks.</head><p>In Table <ref type="table" target="#tab_0">1</ref>, we find that DRAGON provides substantial gains over GreaseLM on downstream tasks involving more complex reasoning, such as CosmosQA and Hel-laSwag, where the inputs have longer context and more entities (thus bigger local KGs). For these tasks, improvements of GreaesLM over RoBERTa were small (+0.1% on CosmosQA), but DRAGON provides substantial boosts (+1.8%). Our insight is that through self-supervised pretraining with larger and more diverse data, DRAGON has learned richer text-KG interactions than GreaseLM, enabling solving more complex downstream tasks. Similarly, as seen in ?3.4.1, DRAGON also attains large gains over GreaseLM on complex questions containing negation, conjunction and prepositional phrases (Table <ref type="table" target="#tab_1">2</ref>), and extrapolates to questions more complex than seen in training sets (Figure <ref type="figure" target="#fig_0">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Increased model capacity.</head><p>In Table <ref type="table" target="#tab_4">4</ref>, we study downstream performance when the model capacity is increased-the number of text-KG fusion layers is increased from 5 to 7-for both GreaseLM and DRAGON. We find that increased capacity does not help for the finetuning-only model (GreaseLM) as was also reported in the original GreaseLM paper, but it helps when pretrained (DRAGON). This result reveals that increased model capacity can actually be beneficial when combined with pretraining, and suggests the promise of DRAGON to be further scaled up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Analysis: Design choices of DRAGON</head><p>Pretraining objective (Table <ref type="table" target="#tab_5">5</ref> top). The first important design choice of DRAGON is the joint pretraining objective: MLM + LinkPred ( ?2.3). Using the joint objective outperforms using MLM or LinkPred alone (+5% accuracy on OBQA). This suggests that having the bidirectional self-supervised tasks on text and KG facilitates the model to fuse the two modalities for reasoning. Link prediction head choice (Table <ref type="table" target="#tab_5">5 middle 1</ref>). KG representation learning is an active area of research, and various KG triplet scoring models are proposed (Equation <ref type="formula" target="#formula_11">9</ref>). We hence experimented with using different scoring models for DRAGON's link prediction head ( ?2.3). We find that while DistMult has a slight edge, all variants we tried (DistMult, TransE, RotatE) are outperforming the baseline without LinkPred ("MLM only"). This result suggests the generality of DRAGON and its promise to be combined with various KG representation learning techniques.</p><p>Cross-modal model (Table <ref type="table" target="#tab_5">5 middle 2</ref>). Another core component of DRAGON is the cross-modal encoder with bidirectional text-KG fusion layers ( ?2.2). We find that if we ablate them and simply concatenate text and KG representations at the end, the performance drops substantially. This result suggests that deep bidirectional fusion is crucial to model interactions over text and KG for reasoning. KG structure (Table <ref type="table" target="#tab_5">5</ref> bottom). The final key design of DRAGON is that we leverage the graph structure of KGs via a sequence-graph encoder and link prediction objective. Here we experimented with an alternative pretraining method that drops the graph structure: we convert triplets in the local KG into sentences using a template <ref type="bibr" target="#b32">[33]</ref>, append them to the main text input, and perform vanilla MLM pretraining. We find that DRAGON substantially outperforms this variant (+2% accuracy on OBQA), which suggests that the graph structure of KGs helps the model perform reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments: Biomedical domain</head><p>Biomedicine is a domain with extensive background knowledge <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b0">1]</ref>, and experts curate various knowledge bases for it <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b71">72]</ref>. We hypothesize that these biomedical KGs can enable deeper understanding and reasoning about biomedical text. With this motivation, we pretrain DRAGON on a biomedical corpus and KG, and evaluate on biomedical downstream tasks.</p><p>Pretraining setup. For the text data, we use PubMed <ref type="bibr" target="#b72">[73]</ref>, a widely-used corpus in biomedial LM training (e.g., BioBERT <ref type="bibr" target="#b73">[74]</ref>, PubmedBERT It contains the abstracts of biomedical papers on PubMed and has 21GB of text. For the KG data, we use the Unified Medical Language System (UMLS) <ref type="bibr" target="#b16">[17]</ref>, a widely-used knowledge graph in biomedicine. It has 300K nodes and 1M edges in total. For training, we follow the same procedure as the experiment in the general domain ( ?3.1), except that we initialize DRAGON's LM component with BioLinkBERT-Large <ref type="bibr" target="#b18">[19]</ref>, the state-of-theart biomedical LM, instead of RoBERTa-Large. Note that while "BioLinkBERT" has "Link" in its name, it is not about KG links but about citation links that the model was originally pretrained with.  DRAGON outperforms all previous biomedical LMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Downstream evaluation tasks.</head><p>We finetune and evaluate DRAGON on three popular biomedical NLP and reasoning benchmarks: MedQA-USMLE (MedQA) <ref type="bibr" target="#b75">[76]</ref>, PubMedQA <ref type="bibr" target="#b76">[77]</ref>, and BioASQ <ref type="bibr" target="#b77">[78]</ref>. Appendix B.4 provides details on these tasks and data splits.</p><p>Baselines. We compare DRAGON with the vanilla LM (BioLinkBERT) and LMs finetuned with the KG (QAGNN and GreaseLM seeded with Bi-oLinkBERT).</p><p>Results. Table <ref type="table" target="#tab_7">6</ref> summarizes model performance on the downstream tasks. Across tasks, DRAGON outperforms all the existing biomedical LMs and KG-augmented QA models, e.g., +3% absolute accuracy boost over BioLinkBERT and +2% over GreaseLM on MedQA, achieving new state-of-theart performance on these tasks. This result suggests significant efficacy of KG-augmented pretraining for improving biomedical reasoning tasks. Combined with the results in the general commonsense domain ( ?3.4), our experiments also suggest the domain-generality of DRAGON, serving as an effective pretraining method across domains with different combinations of text, KGs and seed LMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We presented DRAGON, a self-supervised pretraining method to learn a deeply bidirectional languageknowledge model from text and knowledge graphs (KGs) at scale. In both general and biomedical domains, DRAGON outperforms existing language models and KG-augmented models on various NLP tasks, and exhibits strong performance on complex reasoning such as answering questions involving long context or multi-step reasoning.</p><p>One limitation of DRAGON is that it is currently an encoder model (analogous to BERT) and does not perform language generation. An important future research would be to extend DRAGON to generation, and advance KG-enhanced language generation <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b78">79]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reproducibility</head><p>Pretrained models, code and data are available at https://github.com/michiyasunaga/dragon. Experiments are available at https://worksheets.codalab.org/worksheets/0xcf9cddffff864fb382e1a2f1393c8934.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Ethics, limitations and risks</head><p>We outline potential ethical issues with our work below. First, DRAGON is a method to fuse language representations and knowledge graph representations for joint reasoning. Consequently, DRAGON could reflect the same biases and toxic behaviors exhibited by language models and knowledge graphs that are used to initialize it. For example, language models have been shown to encode biases about race, gender, and other demographic attributes <ref type="bibr" target="#b79">[80,</ref><ref type="bibr" target="#b80">81]</ref> and generate toxic outputs <ref type="bibr" target="#b81">[82]</ref>. Because DRAGON is seeded with pretrained language models that often learn these patterns, it is possible to reflect them in open-world settings. Second, the ConceptNet knowledge graph <ref type="bibr" target="#b6">[7]</ref> used in this work has been shown to encode stereotypes <ref type="bibr" target="#b82">[83]</ref>, rather than completely clean commonsense knowledge. If DRAGON were used outside these standard benchmarks in conjunction with ConceptNet as a KG, it might rely on unethical relationships in its knowledge resource to arrive at conclusions. Consequently, while DRAGON could be used for applications outside these standard benchmarks, we would encourage implementers to use the same precautions they would apply to other language models and methods that use noisy knowledge sources.</p><p>Another source of ethical concern is the use of the MedQA-USMLE evaluation. While we find this clinical reasoning task to be an interesting testbed for DRAGON and for joint language and knowledge reasoning in general, we do not encourage users to use these models for real world clinical prediction.</p><p>Reference: <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experimental Setup Details B.1 KG retrieval</head><p>Given each input text segment W , we follow the procedure from Yasunaga et al. <ref type="bibr" target="#b7">[8]</ref> to retrieve a relevant local KG G from the raw KG G = (V, E). First, we use the entity linker from the spaCy library 1 to link entity mentions in W to entity nodes in G, obtaining an initial set of nodes V el . Second, we add any bridge entities in G that are in a 2-hop path between any pair of linked entities in V el to get the total retrieved nodes V ? V. If the number of nodes in V exceeds 200, we prune V by randomly sampling 200 nodes from it to be the final retrieved nodes V . Lastly, we retrieve all the edges in G that connect any two nodes in V to obtain E ? E, forming the final local KG, G = (V, E).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Graph initialization</head><p>For the ConceptNet knowledge graph used in the general commonsense domain ( ?3), we follow the method of MHGRN <ref type="bibr" target="#b32">[33]</ref> to prepare the initial KG node embeddings. Specifically, we convert triplets in the KG into sentences using pre-defined templates for each relation. Then, these sentences are fed into BERT-Large <ref type="bibr" target="#b1">[2]</ref> to compute embeddings for each sentence. Finally, for each entity, we collect all sentences containing the entity, extract all token representations of the entity's mention spans in these sentences, and return the mean pooling of these representations.</p><p>For the UMLS knowledge graph used in the biomedical domain ( ?4), node embeddings are initialized similarly using the pooled token output embeddings of the entity name from BioLinkBERT <ref type="bibr" target="#b18">[19]</ref>.</p><p>While extremely rare (&lt; 1%), in case when the input text does not yield any linked entity, we represent the graph using a dummy node initialized with 0, i.e., DRAGON backs off to only using the text side representations because the graph propagates no information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Hyperparameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Downstream evaluation tasks</head><p>We use the following nine commonsense reasoning benchmarks for the experiments in the general domain ( ?3).</p><p>CommonsenseQA (CSQA) <ref type="bibr" target="#b55">[56]</ref> is a 5-way multiple-choice QA task testing commonsense reasoning. The dataset has 12,102 questions. We use the in-house data splits by <ref type="bibr" target="#b31">[32]</ref>.</p><p>OpenbookQA (OBQA) <ref type="bibr" target="#b56">[57]</ref> is a 4-way multiple-choice QA task containing elementary science questions. It has 5,957 questions. We use the original data splits in <ref type="bibr" target="#b35">[36]</ref>.</p><p>RiddleSense (Riddle) <ref type="bibr" target="#b57">[58]</ref> is a 5-way multiple-choice task testing complex riddle-style commonsense reasoning. It has 5,715 questions. We split the dev set in half to make in-house dev/test sets.</p><p>AI2 Reasoning Challenge, Challenge Set (ARC) <ref type="bibr" target="#b58">[59]</ref> is a 4-way multiple-choice QA task containing science exam questions. It has 2,590 questions. We use the original data splits in <ref type="bibr" target="#b58">[59]</ref>.</p><p>CosmosQA <ref type="bibr" target="#b59">[60]</ref> is a 4-way multiple-choice QA task testing commonsense reasoning with long narratives. It has 35.6K questions. We split the dev set in half to make in-house dev/test sets.</p><p>HellaSwag <ref type="bibr" target="#b60">[61]</ref> is a 4-way multiple-choice task testing grounded commonsense reasoning about events. It has 70K questions. We split the dev set in half to make in-house dev/test sets.</p><p>Physical Interaction QA (PIQA) <ref type="bibr" target="#b61">[62]</ref> is a 3-way multiple-choice QA task testing physics reasoning about objects. It has 20K questions. We split the dev set in half to make in-house dev/test sets.</p><p>Social Interaction QA (SIQA) <ref type="bibr" target="#b62">[63]</ref> is a 3-way multiple-choice QA task testing social commonsense reasoning. It has 37K questions. We use the original data splits in <ref type="bibr" target="#b62">[63]</ref>.</p><p>Abductive Natural Language Inference (aNLI) <ref type="bibr" target="#b63">[64]</ref> is a 2-way multiple-choice task testing abductive commonsense reasoning. It has 170K questions. We use the original data splits in <ref type="bibr" target="#b63">[64]</ref>.</p><p>For the experiments in the biomedical domain ( ?4), we use the following three biomedical NLP and reasoning benchmarks.</p><p>MedQA-USMLE (MedQA) <ref type="bibr" target="#b75">[76]</ref> is a 4-way multiple-choice task containing United States Medical License Exam questions. The dataset has 12,723 questions. We use the original data splits in <ref type="bibr" target="#b75">[76]</ref>.</p><p>PubMedQA <ref type="bibr" target="#b76">[77]</ref> is a 3-way multiple-choice task testing biomedical language understanding and reasoning. The dataset has 1,000 questions. We use the original data splits in <ref type="bibr" target="#b76">[77]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Analysis of DRAGON's graph reasoning, where we visualize how graph attention weights and final predictions change given question variations. Darker and thicker edges indicate higher attention weights. DRAGON exhibits abilities to extrapolate and perform robust reasoning. DRAGON adjusts the entity attention weights and final predictions accordingly when conjunction or negation is given about entities (A1, A2) or when extra context is added to an original question (B1?B2), but existing models, RoBERTa and GreaseLM, struggle to predict the correct answers. A1: DRAGON's final GNN layer shows strong attention to "school" but weak attention to "trip", likely because the question states "and store one"-hence, the chair is not used for a trip. A2: DRAGON shows strong attention to "trip" and "beach", likely because the question now states "but not store one"-hence, the chair is used for a trip. B1?B2: DRAGON's final GNN layer shows strong attention to "movie" in the original question (B1), but after adding the extra context "don't enjoy pre-record" (B2), DRAGON shows strong attention to "live" and "concert", leading to making the correctly adjusted prediction "concert hall". One interpretation of these findings is that DRAGON leverages the KG's graph structure as a scaffold for performing complex reasoning. This insight is related to recent works that provide LMs with scratch space for intermediate reasoning<ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b65">66]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Accuracy on downstream commonsense reasoning tasks. DRAGON consistently outperforms the existing LM (RoBERTa) and KG-augmented QA models (QAGNN, GreaseLM) on all tasks. The gain is especially significant on tasks that have small training data (OBQA, Riddle, ARC) and tasks that require complex reasoning (CosmosQA, HellaSwag).</figDesc><table><row><cell>RoBERTa [18]</cell><cell>68.7</cell><cell>64.9</cell><cell>60.7</cell><cell>43.0</cell><cell>80.5</cell><cell></cell><cell>82.3</cell><cell cols="2">79.4</cell><cell>75.9</cell><cell>82.7</cell></row><row><cell>QAGNN [8]</cell><cell>73.4</cell><cell>67.8</cell><cell>67.0</cell><cell>44.4</cell><cell>80.7</cell><cell></cell><cell>82.6</cell><cell cols="2">79.6</cell><cell>75.7</cell><cell>83.0</cell></row><row><cell>GreaseLM [9]</cell><cell>74.2</cell><cell>66.9</cell><cell>67.2</cell><cell>44.7</cell><cell>80.6</cell><cell></cell><cell>82.8</cell><cell cols="2">79.6</cell><cell>75.5</cell><cell>83.3</cell></row><row><cell>DRAGON (Ours)</cell><cell>76.0</cell><cell>72.0</cell><cell>71.3</cell><cell>48.6</cell><cell>82.3</cell><cell></cell><cell>85.2</cell><cell cols="2">81.1</cell><cell>76.8</cell><cell>84.0</cell></row><row><cell></cell><cell></cell><cell cols="3">Negation Conjunction Hedge</cell><cell cols="4"># Prepositional Phrases</cell><cell cols="2"># Entities</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell></cell><cell>&gt;10</cell></row><row><cell>RoBERTa</cell><cell></cell><cell>61.7</cell><cell>70.9</cell><cell cols="4">68.6 67.6 71.0 71.1</cell><cell>73.1</cell><cell></cell><cell>74.5</cell></row><row><cell>QAGNN</cell><cell></cell><cell>65.1</cell><cell>74.5</cell><cell cols="4">74.2 72.1 71.6 75.6</cell><cell>71.3</cell><cell></cell><cell>78.6</cell></row><row><cell>GreaseLM</cell><cell></cell><cell>65.1</cell><cell>74.9</cell><cell cols="4">76.6 75.6 73.8 74.7</cell><cell>73.6</cell><cell></cell><cell>79.4</cell></row><row><cell cols="2">DRAGON (Ours)</cell><cell>75.2</cell><cell>79.6</cell><cell cols="4">77.5 79.1 78.2 77.8</cell><cell>80.9</cell><cell></cell><cell>83.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Accuracy of DRAGON on CSQA + OBQA dev sets for questions involving complex reasoning such as negation terms, conjunction terms, hedge terms, prepositional phrases, and more entity mentions. DRAGON consistently outperforms the existing LM (RoBERTa) and KG-augmented QA models (QAGNN, GreaseLM) in these complex reasoning settings.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>(10% train)   </figDesc><table><row><cell>RoBERTa</cell><cell>72.2</cell><cell>66.4</cell></row><row><cell>GreaseLM</cell><cell>73.0</cell><cell>67.0</cell></row><row><cell>DRAGON (Ours)</cell><cell>77.9</cell><cell>72.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance in low-resource setting where 10% of finetuning data is used. DRAGON attains large gains, suggesting its benefit for downstream data efficiency.</figDesc><table><row><cell>Method</cell><cell cols="2">CSQA OBQA</cell></row><row><cell>GreaseLM</cell><cell>74.2</cell><cell>66.9</cell></row><row><cell>GreaseLM-Ex</cell><cell>73.9</cell><cell>66.2</cell></row><row><cell>DRAGON (Ours)</cell><cell>76.0</cell><cell>72.0</cell></row><row><cell>DRAGON-Ex (Ours)</cell><cell>76.3</cell><cell>72.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Downstream performance when model</figDesc><table><row><cell cols="2">Ablation Type Ablation</cell><cell cols="2">CSQA OBQA</cell></row><row><cell></cell><cell>MLM + LinkPred (final)</cell><cell>76.0</cell><cell>72.0</cell></row><row><cell>Pretraining objective</cell><cell>MLM only</cell><cell>74.3</cell><cell>67.2</cell></row><row><cell></cell><cell>LinkPred only</cell><cell>73.8</cell><cell>66.4</cell></row><row><cell></cell><cell>DistMult (final)</cell><cell>76.0</cell><cell>72.0</cell></row><row><cell>LinkPred head</cell><cell>TransE</cell><cell>75.7</cell><cell>71.4</cell></row><row><cell></cell><cell>RotatE</cell><cell>75.8</cell><cell>71.7</cell></row><row><cell>Cross-modal model</cell><cell>Bidirectional interaction (final) Concatenate at end</cell><cell>76.0 74.5</cell><cell>72.0 68.0</cell></row><row><cell>KG structure</cell><cell>Use graph (final) Convert to sentence</cell><cell>76.0 74.7</cell><cell>72.0 70.1</cell></row><row><cell>capacity-number of text-KG fusion layers-is increased</cell><cell></cell><cell></cell><cell></cell></row><row><cell>("-Ex"). Increased capacity does not help for the</cell><cell></cell><cell></cell><cell></cell></row><row><cell>finetuning-only model (GreaseLM), but helps when pre-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>trained (DRAGON), suggesting the promise of DRAGON</cell><cell></cell><cell></cell><cell></cell></row><row><cell>to be further scaled up.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Ablation study of DRAGON. Using joint pretraining</figDesc><table><row><cell>objective MLM + LinkPred ( ?2.3) outperforms using one of them</cell></row><row><cell>only. All variants of LinkPred scoring models (DistMult, TransE,</cell></row><row><cell>RotatE) outperform the baseline without LinkPred ("MLM only"),</cell></row><row><cell>suggesting that DRAGON can be combined with various KG repre-</cell></row><row><cell>sentation learning models. Cross-modal model with bidirectional</cell></row><row><cell>modality interaction ( ?2.2) outperforms combining text and KG</cell></row><row><cell>representations only at the end. Finally, using KG as graph out-</cell></row><row><cell>performs converting KG as sentences, suggesting the benefit of</cell></row><row><cell>graph structure for reasoning.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Accuracy on biomedical NLP tasks.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Hyperparameter settings for models and experiments</figDesc><table><row><cell>Category</cell><cell>Hyperparameter</cell><cell cols="2">Commonsense domain</cell><cell cols="2">Biomedical domain</cell></row><row><cell></cell><cell></cell><cell>Pretrain</cell><cell>Finetune</cell><cell>Pretrain</cell><cell>Finetune</cell></row><row><cell></cell><cell>Number of text-KG fusion layers M</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>5</cell></row><row><cell></cell><cell>Number of Unimodal LM layers N</cell><cell>19</cell><cell>19</cell><cell>19</cell><cell>19</cell></row><row><cell>Model architecture</cell><cell>Number of attention heads in GNN</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell></row><row><cell></cell><cell>Dimension of node embeddings and the messages in GNN</cell><cell>200</cell><cell>200</cell><cell>200</cell><cell>200</cell></row><row><cell></cell><cell>Dimension of MLP hidden layers (except MInt operator)</cell><cell>200</cell><cell>200</cell><cell>200</cell><cell>200</cell></row><row><cell></cell><cell>Number of hidden layers of MLPs</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell></cell><cell>Dimension of MInt operator hidden layer</cell><cell>400</cell><cell>400</cell><cell>400</cell><cell>400</cell></row><row><cell>Regularization</cell><cell>Dropout rate of the embedding layer, GNN layers and dense layers</cell><cell>0.2</cell><cell>0.2</cell><cell>0.2</cell><cell>0.2</cell></row><row><cell></cell><cell>Learning rate of parameters in LM</cell><cell>2e-5</cell><cell>{1e-5, 2e-5, 3e-5}</cell><cell>2e-5</cell><cell>{1e-5, 2e-5, 3e-5}</cell></row><row><cell></cell><cell>Learning rate of parameters not in LM</cell><cell>3e-4</cell><cell>{3e-4, 1e-3}</cell><cell>3e-4</cell><cell>{1e-4, 3e-4}</cell></row><row><cell></cell><cell>Number of epochs in which LM's parameters are kept frozen</cell><cell>2</cell><cell>4</cell><cell>2</cell><cell>4</cell></row><row><cell>Optimization</cell><cell>Optimizer</cell><cell>RAdam</cell><cell>RAdam</cell><cell>RAdam</cell><cell>RAdam</cell></row><row><cell></cell><cell>Learning rate schedule</cell><cell>linear warmup and decay</cell><cell>linear warmup and decay</cell><cell>linear warmup and decay</cell><cell>linear warmup and decay</cell></row><row><cell></cell><cell>Warmup ratio</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell></cell><cell>Batch size</cell><cell>8,192</cell><cell>128</cell><cell>8,192</cell><cell>128</cell></row><row><cell></cell><cell>Number of epochs</cell><cell>-</cell><cell>10-70</cell><cell>-</cell><cell>10-70</cell></row><row><cell></cell><cell>Number of steps</cell><cell>20,000</cell><cell>-</cell><cell>20,000</cell><cell>-</cell></row><row><cell></cell><cell>Max gradient norm (gradient clipping)</cell><cell>1.0</cell><cell>1.0</cell><cell>1.0</cell><cell>1.0</cell></row><row><cell>Data</cell><cell>Max number of nodes</cell><cell>200</cell><cell>200</cell><cell>200</cell><cell>200</cell></row><row><cell></cell><cell>Max number of tokens</cell><cell>512</cell><cell>{128, 256}</cell><cell>512</cell><cell>512</cell></row></table><note><p>1 https://spacy.io/</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>We thank <rs type="person">Rok Sosic</rs>, <rs type="person">Hamed Nilforoshan</rs>, <rs type="person">Michael Moor</rs>, <rs type="person">Qian Huang</rs>, members of the Stanford SNAP, <rs type="person">P-Lambda</rs>, and NLP groups, as well as our anonymous reviewers for valuable feedback. We also gratefully acknowledge the support of <rs type="funder">HAI</rs> Google Cloud Credits 1051203844499; DARPA under Nos. <rs type="grantNumber">HR00112190039</rs> (TAMI), N660011924033 (MCS); ARO under Nos. <rs type="grantNumber">W911NF-16-1-0342</rs> (MURI), <rs type="grantNumber">W911NF-16-1-0171</rs> (DURIP); <rs type="funder">NSF</rs> under Nos. <rs type="grantNumber">OAC-1835598</rs> (CINES), OAC-1934578 (HDR), CCF-1918940 (Expeditions), IIS-2030477 (RAPID), <rs type="funder">NIH</rs> under No. <rs type="grantNumber">R56LM013365</rs>; <rs type="institution">Stanford Data Science Initiative</rs>, <rs type="funder">Wu Tsai Neurosciences Institute</rs>, <rs type="person">Chan Zuckerberg Biohub</rs>, <rs type="funder">Amazon</rs>, <rs type="person">JPMorgan Chase</rs>, <rs type="person">Docomo</rs>, <rs type="funder">Hitachi</rs>, <rs type="funder">Intel</rs>, <rs type="institution">JD.com, KDDI, Toshiba</rs>, <rs type="funder">NEC</rs>, and <rs type="funder">UnitedHealth Group</rs>. The content is solely the responsibility of the authors and does not necessarily represent the official views of the funding entities.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Xa4UgU2">
					<idno type="grant-number">HR00112190039</idno>
				</org>
				<org type="funding" xml:id="_XQeJwxk">
					<idno type="grant-number">W911NF-16-1-0342</idno>
				</org>
				<org type="funding" xml:id="_fQmqNcj">
					<idno type="grant-number">W911NF-16-1-0171</idno>
				</org>
				<org type="funding" xml:id="_r66n7yU">
					<idno type="grant-number">OAC-1835598</idno>
				</org>
				<org type="funding" xml:id="_RMfRsMt">
					<idno type="grant-number">R56LM013365</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Glamping: C. concert hall (?)</head><note type="other">Model Prediction</note><p>BioASQ <ref type="bibr" target="#b77">[78]</ref> is a 2-way multiple-choice task testing biomedical language understanding and reasoning. The dataset has 885 questions. We use the original data splits in <ref type="bibr" target="#b77">[78]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MedQA-USMLE</head><p>A 57-year-old man presents to his primary care physician with a 2-month history of right upper and lower extremity weakness. He noticed the weakness when he started falling far more frequently while running errands. Since then, he has had increasing difficulty with walking and lifting objects. His past medical history is significant only for well-controlled hypertension, but he says that some members of his family have had musculoskeletal problems. His right upper extremity shows forearm atrophy and depressed reflexes while his right lower extremity is hypertonic with a positive Babinski sign. Which of the following is most likely associated with the cause of this patients symptoms? (A) HLA-B8 haplotype (B) HLA-DR2 haplotype (C) <ref type="bibr">Mutation</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional Experimental Results</head><p>Method Hit@3 DistMult (i.e., KG only) 61.3 DRAGON (i.e., KG + text)</p><p>78.1</p><p>Table <ref type="table">9</ref>: KG link prediction performance on ConceptNet. In addition to the NLP tasks we mainly used for downstream evaluation, DRAGON can also perform KG link prediction tasks in downstream. We find that DRAGON (which uses retrieved text besides the KG) achieves improved performance on the KG link prediction task compared to the baseline DistMult model (which does not use text).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">On the opportunities and risks of foundation models</title>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07258</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter</title>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Matthew E Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter</title>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Wikidata: A free collaborative knowledgebase</title>
		<author>
			<persName><forename type="first">Denny</forename><surname>Vrande?i?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Kr?tzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An open multilingual graph of general knowledge</title>
		<author>
			<persName><forename type="first">Robyn</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">QA-GNN: Reasoning with language models and knowledge graphs for question answering</title>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter</title>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Greaselm: Graph reasoning enhanced language models for question answering</title>
		<author>
			<persName><forename type="first">Xikun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Query2box: Reasoning over knowledge graphs in vector space using box embeddings</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hongyu Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Latent execution-guided reasoning for multi-hop question answering on knowledge graphs</title>
		<author>
			<persName><forename type="first">Hanjun</forename><surname>Hongyu Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitian</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><surname>Lego</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ernie: Enhanced language representation with informative entities</title>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pretrained encyclopedia: Weakly supervised knowledge-pretrained language model</title>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Kepler: A unified model for knowledge embedding and pre-trained language representation</title>
		<author>
			<persName><forename type="first">Xiaozhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaocheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training</title>
		<author>
			<persName><forename type="first">Oshin</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heming</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siamak</forename><surname>Shakeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>In North American Chapter</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyuan</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanbin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.02137</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The unified medical language system (UMLS): Integrating biomedical terminology</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bodenreider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">LinkBERT: Pretraining language models with document links</title>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Realm: Retrieval-augmented language model pre-training</title>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zora</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Retrieval-augmented generation for knowledge-intensive nlp tasks</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>K?ttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Improving language models by retrieving from trillions of tokens</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Lespiau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Damoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.04426</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Knowledge enhanced contextual word representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Iv Robertllogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Corby</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.00655</idno>
		<title level="m">Knowledge-aware language model pretraining</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Exploiting structured knowledge in text via graph-guided representation learning</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Self-alignment pretraining for biomedical entity representations</title>
		<author>
			<persName><forename type="first">Fangyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Shareghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaiqiao</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Basaldella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter</title>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Jaket: Joint pre-training of knowledge graph and language understanding</title>
		<author>
			<persName><forename type="first">Donghan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Jointgt: Graph-text joint representation learning for text generation from knowledge graphs</title>
		<author>
			<persName><forename type="first">Pei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haozhe</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of ACL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">K-bert: Enabling language representation with knowledge graph</title>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiruo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haotang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Colake: Contextualized language and knowledge embedding</title>
		<author>
			<persName><forename type="first">Tianxiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaru</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan-Jing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Linguistics (COLING)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Integrating graph contextualized knowledge into pre-trained language models</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinghui</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Jing Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of EMNLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Kagnet: Knowledge-aware graph networks for commonsense reasoning</title>
		<author>
			<persName><forename type="first">Xinyue</forename><surname>Bill Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Scalable multi-hop relational reasoning for knowledge-aware question answering</title>
		<author>
			<persName><forename type="first">Yanlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyue</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graph-based reasoning over heterogeneous external knowledge for commonsense question answering</title>
		<author>
			<persName><forename type="first">Shangwen</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjun</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guihong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songlin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Gnn is a counter? revisiting gnn for question answering</title>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Knowledgeable reader: Enhancing cloze-style reading comprehension with external commonsense knowledge</title>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anette</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Enhancing pre-trained language representations with rich knowledge for machine reading comprehension</title>
		<author>
			<persName><forename type="first">An</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yajuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaoqiao</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Open domain question answering using early fusion of knowledge bases and text</title>
		<author>
			<persName><forename type="first">Haitian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><surname>Mazaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pullnet: Open domain question answering with iterative retrieval on knowledge bases and text</title>
		<author>
			<persName><forename type="first">Haitian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tania</forename><surname>Bedrax-Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Nedim Lipka, and Xiang Ren. Learning contextualized knowledge structures for commonsense reasoning</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mrigank</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Handong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungchul</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of ACL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Jointlk: Joint reasoning with language models and knowledge graphs for commonsense question answering</title>
		<author>
			<persName><forename type="first">Yueqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter</title>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Human parity on commonsenseqa: Augmenting self-attention with external attention</title>
		<author>
			<persName><forename type="first">Yichong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName><forename type="first">Th?o</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning (ICML)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Simple embedding for link prediction in knowledge graphs</title>
		<author>
			<persName><forename type="first">Seyed</forename><surname>Mehran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazemi</forename></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Rotate: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Relation extraction with matrix factorization and universal schemas</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><forename type="middle">M</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter</title>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Representing text for joint embedding of text and knowledge bases</title>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pallavi</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Representation learning of knowledge graphs with entity descriptions</title>
		<author>
			<persName><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Kg-bert: Bert for knowledge graph completion</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengsheng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03193</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Multi-task learning for knowledge graph completion with pre-trained language models</title>
		<author>
			<persName><forename type="first">Bosung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesuk</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjoong</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungyun</forename><surname>Seo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Linguistics (COLING)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Multi-task pre-training language model for semantic network completion</title>
		<author>
			<persName><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kele</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukai</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaimin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.04843</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Commonsenseqa: A question answering challenge targeting commonsense knowledge</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter</title>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Can a suit of armor conduct electricity? a new dataset for open book question answering</title>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Riddlesense: Reasoning about riddle questions featuring linguistic creativity and commonsense knowledge</title>
		<author>
			<persName><forename type="first">Ziyi</forename><surname>Bill Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong-Ho</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of ACL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Cowhey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carissa</forename><surname>Schoenick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05457</idno>
		<title level="m">Think you have solved question answering? try arc, the ai2 reasoning challenge</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Cosmos qa: Machine reading comprehension with contextual commonsense reasoning</title>
		<author>
			<persName><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Hellaswag: Can a machine really finish your sentence?</title>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Piqa: Reasoning about physical commonsense in natural language</title>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Socialiqa: Commonsense reasoning about social interactions</title>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Lebras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Abductive commonsense reasoning</title>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Nye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Johan Andreassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Gur-Ari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henryk</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bieber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Lewkowycz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.00114</idno>
		<title level="m">Show your work: Scratchpads for intermediate computation with language models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Chain of thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.11903</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">The medical dictionary for regulatory activities (meddra)</title>
		<author>
			<persName><forename type="first">Louise</forename><surname>Elliot G Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sue</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Drug safety</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="117" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Medical subject headings (mesh)</title>
		<author>
			<persName><forename type="first">Carolyn</forename><forename type="middle">E</forename><surname>Lipscomb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the Medical Library Association</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">265</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Modeling polypharmacy side effects with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monica</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="457" to="466" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Gene ontology: tool for the unification of biology</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Ashburner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><forename type="middle">A</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judith</forename><forename type="middle">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Botstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heather</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Michael Cherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allan</forename><forename type="middle">P</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kara</forename><surname>Dolinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Selina</forename><forename type="middle">S</forename><surname>Dwight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janan</forename><forename type="middle">T</forename><surname>Eppig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature genetics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="29" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Drugbank 5.0: a major update to the drugbank database for 2018</title>
		<author>
			<persName><surname>David S Wishart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yannick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><forename type="middle">C</forename><surname>Feunang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elvis</forename><forename type="middle">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanvir</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Sajed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zinat</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Sayeeda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Identification of disease treatment mechanisms through the multiscale interactome</title>
		<author>
			<persName><forename type="first">Camilo</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title/>
		<author>
			<persName><surname>Pubmed</surname></persName>
		</author>
		<ptr target="https://pubmed.ncbi.nlm.nih.gov/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Biobert: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wonjin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">So</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Domain-specific language model pretraining for biomedical natural language processing</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoto</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.15779</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">What disease does this patient have? a large-scale open domain question answering dataset from medical exams</title>
		<author>
			<persName><forename type="first">Di</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eileen</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nassim</forename><surname>Oufattole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Hung</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanyi</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Szolovits</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Pubmedqa: A dataset for biomedical research question answering</title>
		<author>
			<persName><forename type="first">Qiao</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinghua</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Results of the seventh edition of the bioasq challenge</title>
		<author>
			<persName><forename type="first">Anastasios</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Bougiatiotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Paliouras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">A survey of knowledge-enhanced text generation</title>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaitang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<publisher>ACM Computing Surveys (CSUR</publisher>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Towards controllable biases in language generation</title>
		<author>
			<persName><forename type="first">Emily</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Premkumar</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)-Findings, long</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Ethical and social risks of harm from language models</title>
		<author>
			<persName><forename type="first">Laura</forename><surname>Weidinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Mellor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maribeth</forename><surname>Rauh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Conor</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myra</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mia</forename><surname>Glaese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Borja</forename><surname>Balle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atoosa</forename><surname>Kasirzadeh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.04359</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Realtoxicityprompts: Evaluating neural toxic degeneration in language models</title>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Samuel Gehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of EMNLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Lawyers are dishonest? quantifying representational harms in commonsense knowledge resources</title>
		<author>
			<persName><forename type="first">Ninareh</forename><surname>Mehrabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><surname>Morstatter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Pujara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Galstyan</surname></persName>
		</author>
		<idno>ArXiv, abs/2103.11320</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
