<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The SpiNNaker Project</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Fellow IEEE</roleName><forename type="first">Steve</forename><forename type="middle">B</forename><surname>Furber</surname></persName>
							<email>steve.furber@manchester.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Manchester</orgName>
								<address>
									<postCode>M13 9PL</postCode>
									<settlement>Manchester</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Francesco</forename><surname>Galluppi</surname></persName>
							<email>francesco.galluppi@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Manchester</orgName>
								<address>
									<postCode>M13 9PL</postCode>
									<settlement>Manchester</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Steve</forename><surname>Temple</surname></persName>
							<email>temples@cs.man.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Manchester</orgName>
								<address>
									<postCode>M13 9PL</postCode>
									<settlement>Manchester</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Senior Member IEEE</roleName><forename type="first">Luis</forename><forename type="middle">A</forename><surname>Plana</surname></persName>
							<email>plana@cs.man.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Manchester</orgName>
								<address>
									<postCode>M13 9PL</postCode>
									<settlement>Manchester</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The SpiNNaker Project</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4C4361F8489854BE3D84E36F6729630C</idno>
					<idno type="DOI">10.1109/JPROC.2014.2304638</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Brain modeling</term>
					<term>multicast algorithms</term>
					<term>multipro- cessor interconnection networks</term>
					<term>neural network hardware</term>
					<term>parallel programming</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the design of a massively parallel computer that is suitable for computational neuroscience modeling of large-scale spiking neural networks in biological real time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>THE spiking neural network architecture (SpiNNaker) project is motivated by the grand challenge of understanding how information is represented and processed in the brain <ref type="bibr" target="#b0">[1]</ref>. Most of the frontiers of science are concerned with the very small, such as subatomic particles, or the very large, such as exploring the outer regions of the universe. Yet there remains a great unsolved scientific mystery at a very human scale: how does the brain, an organ that we could readily hold in our hands and observe with the naked eye, perform its role that is so central to all of our lives? ''Wet'' neuroscience has told us a great deal about the basic componentVthe neuronVfrom which the brain is constructed. Brain imaging tells us yet more about how activity moves around the brain as we perform certain mental functions. The former is concerned with individual neurons up to groups of tens or perhaps hundreds; the latter looks at the collective activity of many millions of neurons. But between these scales there are a few orders of magnitude of scale for which there exists no scientific instrument except the computer model, and it is at these intermediate scales, we suggest, that all the interesting information processing takes place.</p><p>Our conclusion is that, if we wish to fully understand how the brain represents and processes information, we need to build computer models to test hypotheses of how the brain works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Neurons and Spikes</head><p>What sort of computer is required for such brain modeling to work?</p><p>The human brain is generally viewed as comprising somewhat under 100 billion neurons, where each neuron is a multiple-input-single-output device.</p><p>There is some debate about the role of the more numerous glial cells that form the structure upon which the neurons build the brain, and, in particular, the role of astrocyte cells in synaptic plasticity <ref type="bibr" target="#b1">[2]</ref>, so any generalpurpose system should aim to accommodate these issues in case they prove to be important.</p><p>Neurons communicate principally through action potentials, or ''spikes.'' These are simply asynchronous impulses where, as a result of the electrochemical regeneration process used to ensure the reliable propagation of these signals along long biological ''wires,'' information is conveyed only in the identity of the neuron that spiked and the time at which it spiked. The height and the width of the impulse are largely invariant at the receiving synapse. This has led to the widespread adoption of the address event representation (AER) encoding of neural activity <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, where the information flow in a network is represented as a time series of neural identifiers.</p><p>There are some notable exceptions to the completeness of the AER view of information flow. Some neurons transport and emit neuromodulators, such as dopamine, that have a global effect on neurons within a neighborhood region; other neurons make direct contact through ''gap'' junctions that make an electrical connection from one neuron to its neighbor. However, in much of the brain, the primary real-time information flow is in the spikes that, in a model, are represented by AER. A general-purpose computer-modeling platform should offer mechanisms to support these other information flows while giving firstclass support to AER ''spikes.''</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Computer Models</head><p>What computer power and architecture are required to support a real-time model of the human brain?</p><p>The simplest estimate of an answer to this question suggests that there are around 10 15 synapses in the brain, with inputs firing at an average rate of 10 1 Hz, and each synaptic event requires perhaps 10 2 instructions to update the state of the postsynaptic neuron and implement any synaptic plasticity algorithm. These figures lead to an estimate of 10 18 operations per second, the performance of an exascale machine. Exascale high-performance computers do not yet exist, though recently the Chinese Tianhe 2 machine has achieved 3 Â 10 16 floating-point operations per second <ref type="bibr" target="#b4">[5]</ref>, so exascale computing is not too far away.</p><p>However, raw computer performance is not the only issue here. The communication patterns in the brain are based on sending very small ''packets'' of information through complex paths to many targets. High-performance computers, on the other hand, are generally optimized for point-to-point communication of large data packets. This mismatch leads to significant inefficiency in the mapping of brain-scale spiking neural networks onto conventional cluster machines and high-performance computers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. SpiNNaker</head><p>The SpiNNaker machine is a computer designed specifically to support the sorts of communication found in the brain. Recognizing the huge computational requirements of the task, SpiNNaker is based on massively parallel computation, and the architecture will accommodate up to a million microprocessor cores, the limit being defined by budget and architectural convenience rather than anything fundamental.</p><p>The key innovation in the SpiNNaker architecture is the communications infrastructure, which is optimized to carry very large numbers of very small packets, in contrast to the conventional cluster and high-performance computer communications system which, as noted above, are optimized for large data packets. Each packet carries a single neural ''spike'' event in a 40-b packet, 32 b of which are the AER identifier of the neuron that spiked and 8 b are management bits identifying the packet type, and such like. (The choice of a 32-b AER identifier is not a fundamental limitation of the architecture, and could be increased in a future implementation to accommodate larger neural models.) The time of the AER spike is implicit; the communications infrastructure can deliver a packet in much less than a millisecond, which is the requirement for real-time neural modeling.</p><p>Although SpiNNaker's design is centered on packetswitched support for AER ''spikes,'' it can also support non-AER information flows through the same communication mechanism delivering discrete (typically 1 ms) updates to continuously variable parameters.</p><p>In order to achieve efficient massively parallel operation, SpiNNaker's design accepts certain compromises, one of which is the requirement for deterministic operation. The asynchronous nature of the communications system leads to nondeterministic ordering of packet reception, and occasionally packets may be dropped to avoid communication deadlock. It is possible to reimpose deterministic operation and lockstep operation to match a conventional sequential model under certain conditions, but this is not the natural or most efficient way to operate the machine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Paper Organization</head><p>This paper is a review of the SpiNNaker project and a tutorial on the use of the machine. The contributions and structure of the paper are as follows.</p><p>We present an overview of the architecture (Section II) and of the hardware implementation (Section III). We present the system software (Section IV), describe the event-driven software model (Section V), the API that supports this (Section VI), and a simple example program that runs on top of the API (Section VII). We present the partitioning and configuration manager (PACMAN, Section VIII) that conceals the physical structure of the machine. Finally, we describe some typical applications that run on the machine (Section IX), our future plans for larger scale machines (Section X), discuss related work (Section XI), and draw our conclusions (Section XII) from our experience with the machine at this stage in its development.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. ARCHITECTURE OVERVIEW</head><p>A detailed description of the architecture of the machine has been presented earlier <ref type="bibr" target="#b5">[6]</ref>, so here we present the key features of the architecture that are germane to what follows (see Fig. <ref type="figure" target="#fig_0">1</ref>). A SpiNNaker machine is a homogeneous 2-D multiple instruction, multiple data array of processing nodes where each node incorporates 18 ARM968 processor cores each with 96 kB of local memory, 128 MB of shared memory, a packet router, and general system support peripherals. Each processor core is a general-purpose 200-MHz 32-b integer processor with no floating-point hardware, so arithmetic is generally implemented as fixed point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Achievable Performance</head><p>Each core can model a few hundred point neuron models, such as leaky integrate and fire or Izhikevich's model, with the order of 1000 input synapses to each neuron. In practice, a number of different constraints may limit the number of neurons a processor core can support in real time, but often the compute budget is dominated by input connectionsVan incoming spike passing through an individual synapseVwhich imposes an upper limit on the (number of neurons) Â (number of inputs per neuron) Â (mean input firing rate). In principle, a processor core can support up to 10 million connections/s, though the current software implementation saturates at about half this throughput, and plastic synapse models reduce it considerably further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Spikes and Packets</head><p>The key innovation in the SpiNNaker architecture is a lightweight multicast packet-routing mechanism that supports the very high connectivity found in biological brains. The mechanism is an extension of conventional AER <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. When the software running on a processor identifies that a neuron should emit a spike, it simply issues a packet that identifies the spiking neuron. The issuing processor has no idea of where that packet will be conveyed toVthat is entirely the responsibility of the routing fabric.</p><p>Each node incorporates a packet router that inspects each packet to look at its source, and routes it accordingly to any subset of its 18 local processors and/or any subset of its six neighbor nodes using multicast transmission (which has been shown to be optimal for neural applications <ref type="bibr" target="#b6">[7]</ref>) in a 2-D triangular mesh. The selected routes are determined by tables in the router that are initialized when the application is loaded into the machine.</p><p>As the packet source identifier is 32 b, it is infeasible to implement full routing tables for every possible source, so a number of optimizations are employed to keep the table sizes reasonable.</p><p>The tables are implemented using content addressable memory (CAM), and entries are required only for those packets that pass through a node. The CAM uses four states: match 0, match 1, match all, and no match. This allows a single CAM entry to route all of those neurons in a population with common routing requirements. Where no CAM entry matches a source identifier, a default routing mechanism allows the packet to pass straight through the node.</p><p>These optimizations allow a routing table with 1024 entries to be sufficient at each node. We will return to the matter of initializing these tables in Section VIII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Processor Disposition</head><p>Each SpiNNaker node selects one of its 18 processor cores to act as ''monitor processor.'' This selection is flexible for fault-tolerance reasons. Once selected, the monitor is assigned an operating system support role. Sixteen of the remaining processors are assigned application support roles, and the 18th processor is held in reserve as a fault-tolerance spare, though on a proportion of nodes, the 18th processor may be faulty as nodes with only 17 functional processors are accepted in production to enhance yield.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. CHIPS, PACKAGES, BOARDS, AND SYSTEMS</head><p>The physical implementation of the SpiNNaker architecture has also been described in detail elsewhere <ref type="bibr" target="#b7">[8]</ref>, so again we will restrict ourselves here to the relevant highlights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Chips and Packages</head><p>Each SpiNNaker node is implemented in a single 19-mm square 300 ball grid array package. The package houses a custom-designed multiprocessor system-on-chip integrated circuit that includes the 18 ARM968 processors, each with its local 32-kB instruction memory and 64-kB data memory, interconnected through a self-timed network on chip to various on-chip shared resources and a second chip, a 128-MB low-power mobile dual-data-rate (DDR) SDRAM. The two chips are stacked onto the package substrate and interconnected using gold wire bonding (Fig. <ref type="figure" target="#fig_1">3</ref>). The aggregate SDRAM bandwidth has been measured to be 900 MB/s <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Boards</head><p>The packages are then assembled onto printed circuit boards (PCBs; see Fig. <ref type="figure" target="#fig_2">2</ref>). The chip-to-chip connections on the PCB are direct wired connections using a self-timed 2-of-7 non-return-to-zero protocol to transmit 4-b symbols with two wire transitions, plus one wire transition for the acknowledge response.</p><p>In principle, these direct connections could be used to build a SpiNNaker machine of arbitrary size, but for practical reasons the machine is constructed from 48-node PCBs, and the PCB-to-PCB connections use high-speed serial links where eight chip-to-chip links are multiplexed through each serial link using Xilinx Spartan6 fieldprogrammable gate arrays (FPGAs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Systems</head><p>SpiNNaker systems of varying sizes can then be assembled from one or more of the 48-node PCBs. There is also a smaller four-node board that is very convenient for training, development, and mobile robotics. The largest machine, incorporating over a million ARM processor cores, will comprise 1200 48-node boards in ten machine room cabinets and will require up to 75 kW of electrical power (peak).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. SPINNAKER SYSTEM SOFTWARE</head><p>SpiNNaker software can be categorized into that which runs on the SpiNNaker system itself and that which runs on other systems, some of which may interact with SpiNNaker. The majority of software that runs on the SpiNNaker chips is written in C. This software can be subdivided into control software (a primitive operating system) and application software which performs the user's computations.</p><p>The primary interface between SpiNNaker systems and the outside world is Ethernet and IP-based protocols. Every SpiNNaker chip has an Ethernet interface and typically one chip per PCB uses this interface. This is used to download code and data to SpiNNaker and to gather results from applications. For some applications, this (100 Mb/s) interface is a bottleneck on getting data to and from SpiNNaker, and we are investigating the use of gigabit links provided by FPGAs on SpiNNaker PCBs to improve this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. SpiNNaker Software</head><p>The control software that runs on SpiNNaker systems is known as the SpiNNaker Control and Monitor Program (SC&amp;MP). The SpiNNaker chips contain primary bootstrap code which allows the loading of code via the Ethernet interface or the interchip links, and this is used to load SC&amp;MP, initially via an Ethernet interface to a single chip. SC&amp;MP is then propagated to the entire system over the interchip links; it runs continuously on the core that has been selected as the monitor processor and provides a range of services to the outside world to allow applications to be loaded on the remaining 16 or 17 application cores on each chip.  A simple packet protocol known as SpiNNaker Datagram Protocol (SDP) is used within the SpiNNaker system. SC&amp;MP acts as a router for SDP packets allowing them to be sent to or from any core in the system and also via Ethernet to external endpoints. This protocol forms the basis for application loading and high-level communication between SpiNNaker chips and/or external machines. Within individual chips, SDP packets are exchanged between cores using a shared-memory interface. Between chips, SDP is transported as sequences of point-to-point packets conveyed by the interchip links. To carry SDP out of the system, the packets are embedded in UDP/IP packets and sent via the Ethernet interface to external endpoints.</p><p>A SpiNNaker ''application'' is a program that runs on one or many of the application cores on a SpiNNaker system. It will typically be written in C and utilize either SDP or multicast packets for its communication needs. Because of the limited code and data size provided by the on-chip memories in SpiNNaker, there is little room for operating system support and so only minimal ancillary code can be loaded along with the application. Each application is linked with a support library known as SpiNNaker Application Runtime Kernel (SARK). SARK provides startup code for the application core to set up the runtime environment for the application. It also provides a library of functions for the application such as memory allocation and interrupt control. SARK also maintains a communications interface with SC&amp;MP running on the monitor processor that allows the application to communicate with and be controlled by other SpiNNaker chips or external systems. Protocols running on top of SDP are used to achieve this functionality.</p><p>An application is built using an ARM cross compiler and linked with SARK and any other runtime libraries that it requires. The output file from the linking stage is converted to a format known as application load and execute (APLX) which is understood by a simple loader which is part of SC&amp;MP. The APLX file can then be downloaded to the SpiNNaker system where it is loaded into the appropriate parts of memory of the relevant application cores by the SC&amp;MP loader.</p><p>Most SpiNNaker applications make use of an event management library known as the Spin1 API. This provides facilities for associating common interrupts with event handling code and for managing queues of events. While the processor is not processing events it is in a low-power sleep mode. This API can be viewed as a software layer between the user's application and the underlying hardware. To facilitate SpiNNaker program development using the API, an emulator has been developed which provides the same set of library calls as the Spin1 API but which runs on a Linux workstation. This allows users without SpiNNaker hardware to develop and debug SpiNNaker applications and to familiarize themselves with the programming model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Host Software</head><p>We refer to the workstation that controls a SpiNNaker system as the ''host.'' A variety of SpiNNaker-related host software has been developed within the project. A number of tools have been developed to download applications to SpiNNaker systems. The ''ybug'' program provides a command line interface for this function and also allows scripted control of the system. A number of application programmer interfaces (APIs) that implement interfaces based on SDP have been developed in C, Perl, and Python. These allow programmed control of a SpiNNaker system to allow applications to be downloaded and controlled and results uploaded.</p><p>In addition, a number of ''visualizer'' applications have been produced which allow the results of SpiNNaker applications to be viewed on the host system. The simplest of these just allows plain text output to be displayed on the host while more sophisticated visualizers <ref type="bibr" target="#b8">[9]</ref> display data in graphical form such as the raster plot of firing spikes in a neural network simulation or the potentials inside a single neuron (Fig. <ref type="figure" target="#fig_3">4</ref>).</p><p>The provision of input data to SpiNNaker applications can also require host software to provide these data. One such application is the ''spike server'' which is used to provide spikes (neural events) in real time to a neural simulation running on SpiNNaker.</p><p>A significant part of the SpiNNaker software effort has been the development of programs that map complex problems onto the SpiNNaker hardware. A typical example is a neural network simulation where individual neurons or groups of neurons have to be allocated to cores in the system and the routing tables set up to allow them to communicate appropriately for the connectivity of the network. The ''PACMAN'' program, which is described in Section VIII, is typical of this class of program <ref type="bibr" target="#b9">[10]</ref>.</p><p>Fig. <ref type="figure" target="#fig_4">5</ref> shows the arrangement of the various software components which make up a SpiNNaker system. Furber et al: The SpiNNaker Project</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EVENT-DRIVEN SOFTWARE MODEL</head><p>The programming model employed on SpiNNaker is that of a real-time event-driven system. The application processors have a base state, which is halted and waiting for an interrupt, contributing to the overall energy efficiency of the system. In the standard neural modeling application, there are three principal events that cause the processor to wake up.</p><p>1) An incoming spike packet. This will usually cause the processor to initiate a direct memory access (DMA) transfer from SDRAM of the synaptic data structures associated with the source of this spike. 2) DMA complete. Once the synaptic data have been transferred, the processor must process the data. 3) One-millisecond timer tick. Each processor has a local timer that marks the passage of time, and each millisecond (typically, the interval is programmable) the processor will compute a further integration step in the neuron dynamics. Of course, these events are asynchronous and unpredictable, so the software running on the processor must be capable of prioritizing the events and handling multiple overlapping requests. This is achieved through the use of a real-time kernel that underpins the event-driven operation of each application processor, and presents a straightforward API to the user, who can build applications on top of the API entirely in C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. SPINNAKER APPLICATION PROGRAMMING INTERFACE</head><p>The SpiNNaker application programming interface (spin1 API) <ref type="bibr" target="#b10">[11]</ref> provides an execution environment that supports a lightweight, event-driven programming model. A central goal of the model is to save energy by keeping the cores in a lowpower state, only responding to events of interest. To this effect, application programs do not control execution flow; they can only indicate the functions, referred to as callbacks, to be executed when specific events, such as the arrival of a packet, the completion of a DMA transfer, or the lapse of a periodic time interval, occur. The callback mechanism is also used to hide the details of the interrupt subsystem, which is handled directly and efficiently by the API.</p><p>Fig. <ref type="figure" target="#fig_5">6</ref> shows the basic architecture of the event-driven framework. Application developers write callback routines that are associated with events of interest and register them with the API at a priority level, which defines them as queueable or non-queueable. When the corresponding event occurs, the scheduler either executes the callback immediately and atomically (in the case of a nonqueueable callback) or places it into a scheduling queue at a position according to its priority (in the case of a queueable callback). When control is returned to the dispatcher (following the completion of a callback) the highest priority queueable callback is executed. Queueable callbacks do not necessarily execute atomically: they may be preempted by non-queueable callbacks if a corresponding event occurs during their execution.</p><p>The dispatcher goes to sleep (in the low-power consumption ''wait for interrupt'' state, where the processor core clock is turned off) when the callback queues are empty and will be awakened by any event. Application developers can designate one non-queueable callback as the preeminent callback, which has the highest priority and can preempt other non-queueable callbacks as well as all queueable ones. The API provides support for callbacks to control entry and exit from critical sections to prevent higher priority callbacks interrupting them at a bad time, e.g., during access to a shared resource.</p><p>This real-time kernel is scalable to very large numbers of processors, but is best suited to relatively simple models running on each processor. Clearly, the system will come to a halt if no events are generated, and real-time performance will be lost if a processor is overwhelmed by incoming events. In practice, careful mapping of a model onto the system can avoid both eventualities. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. EXAMPLE LOW-LEVEL APPLICATION</head><p>As a simple example of a parallel program that runs on top of the SpiNNaker API, here are the key features of a simple example that implements Conway's Life cellular automaton.</p><p>First, the program should include the API calls:</p><p>#include hspin1 a pi:hi</p><p>Then, we need routines to set up the initial state of the automaton and the routing tables. In this case, setting up the routing tables is by far the most complex aspect of the programming task as the Life neighbor connections must be established between processors across chip boundaries. void set up route tables ðuintchip; uintcoreÞf. . .g void init Life state ðuintchip; uintcoreÞf. . .g Now we must define the event-driven callback routines. In this example, the relevant events are timer tick and an incoming packet: void tick callback ðuintticks; uintdummyÞf. . .g void pkt in ðuintkey; uintdataÞf. . .g</p><p>The simulation is started on each processor from c_main. The chip and core addresses are found, then the initialization routines are called:  </p><formula xml:id="formula_0">void c</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. PARTIONING AND CONFIGURATION MANAGER</head><p>The example described in Section VII shows how APIbased applications can set up the simulation parameters, SDRAM content, and routing tables with an algorithmic process. While for simple or highly structured problems this is possible, modeling networks with arbitrary interconnectivity and arbitrary neural types is a problem where a further level of abstraction can be introduced. Configuring a million-core machine, with each core modeling up to a thousand neurons and a million synapses, rapidly becomes an intractable problem: one billion neurons need to be mapped and one trillion synapses need to be routed to implement a user-specified model.</p><p>To solve this problem, we introduce PACMAN <ref type="bibr" target="#b9">[10]</ref>, a software layer that enables users to write their model using a standardized interface, translate it, and run it on SpiNNaker. The software is designed to keep different concerns separated: users interface with the platform through domain-specific, neural languages already present in the scientific milieu, such as PyNN <ref type="bibr" target="#b11">[12]</ref> or Nengo <ref type="bibr" target="#b12">[13]</ref>. PACMAN is the set of algorithms that translate a model into machine-executable code. Such algorithms operate on data representing the network model, information about the system (topology, fault status, etc.), and methods for data structure translation.</p><p>PACMAN maps, routes, and translates network models using populations of neurons and projections between them, rather than single neurons and synapses. This approach reduces the complexity of the algorithms involved in the translation process, by exploiting the hierarchies present in a neural network. This choice is justified by studies on the structure of the central nervous systems, where functionally segregated areas are interconnected by axonal pathways <ref type="bibr" target="#b13">[14]</ref>, and where cortical areas show a remarkably regular laminar structure, with different layers of neurons stereotypically connected in a canonical circuit <ref type="bibr" target="#b14">[15]</ref>. Finally, many neural languages <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> use this abstraction natively, making it a natural choice.</p><p>Using a neural language as a user interface makes the platform more accessible to nonexperts, giving the users a familiar environment to develop models and analyze results, while hiding the complexity of configuring a parallel system and encouraging model sharing across different platforms. The translation process is performed by PACMAN as illustrated in Fig. <ref type="figure" target="#fig_7">7</ref>, which shows the flow of the algorithms used to translate and execute the models (left) and the data representations they work on (right).</p><p>The model is represented in terms of populations and projections in the model view. It is then partitioned, splitting populations, while preserving their interconnectivity structure, accordingly to machine-specific constraints, depending on the neural and synaptic capacity of each core. The model is represented in a digraph-like structure (PACMAN view), and then mapped and routed on a physical machine instance (system view), using the information present in the system library. Finally, the whole model is translated into machine-executable code for each component (ARM cores, SDRAM, routers), using the translation mechanism stored in the model library, loaded onto the system, and executed.</p><p>A simple example network is illustrated in Fig. <ref type="figure" target="#fig_8">8</ref> (left): excitatory and inhibitory populations are recurrently interconnected. The ratio of excitatory to inhibitory neurons is set to 4 : 1 to keep a balance between excitation and inhibition.</p><p>The network can be represented in PyNN <ref type="bibr" target="#b11">[12]</ref>, first by creating the two populations of neurons, for a total of n neurons, with a set of parameters:   to all the neurons in the postsynaptic population with a probability p, weight w, and delay d. All the projections coming from the excitatory population target excitatory synapses; conversely, all the projections coming from the inhibitory population target inhibitory synapses.</p><p>PACMAN automatically partitions and maps the network as illustrated in Fig. <ref type="figure" target="#fig_8">8</ref> (right), which shows an example where the total number of neurons n is 6000, and each core maps 100 neurons. As a result, the model needs to be partitioned into 48 excitatory and 12 inhibitory subgroups, each to be allocated to a single core of a physical machine, with the system library providing the geometry (in this case, a four-chip board) and the functional status of the platform. The model library provides the translation methods for the IF_curr_exp neuron type (a leaky integrate and fire with exponential decaying synapses), its parameters, and its synapses. Fig. <ref type="figure" target="#fig_9">9</ref> shows results of 1 s of simulation in the form of a raster plot, where each dot represents a spike from a neuron (ordinate) in time (abscissa). Red (blue) dots represent spikes from excitatory (inhibitory) neurons; the interconnectivity parameters are set to give rise to the oscillatory activity shown in the figure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. TYPICAL APPLICATIONS</head><p>In this section, we review some scenarios highlighting the flexibility of the SpiNNaker platform, and present an experiment running on a robot equipped with AER sensors and a 48-node SpiNNaker board.</p><p>With the hardware and software infrastructure presented in the previous sections we have simulated networks with up to 250 000 neurons and 80 million synapses in real time on a 48-node SpiNNaker board (as shown in Fig. <ref type="figure" target="#fig_2">2</ref>) within a power budget of 1 W per SpiNNaker package (containing a SpiNNaker chip and a 128-MB SDRAM; see Fig. <ref type="figure" target="#fig_1">3</ref>). In terms of spike delivery (the dominant cost in neural simulations <ref type="bibr" target="#b17">[18]</ref>) and power consumption, these experiments show 1.8 billion connections per second, using a few nanojoules per event and per neuron <ref type="bibr" target="#b18">[19]</ref>, and represent the maximum sustainable throughput of the system with the current software infrastructure.</p><p>Good power efficiency has also been demonstrated in a biologically plausible model of cortical microcircuitry inspired by previous work <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b19">[20]</ref>, comprising 10 000 Izhikevich neurons, replicating spiking dynamics found in the cortex, and 40 million synapses in real time <ref type="bibr" target="#b20">[21]</ref>, while the flexibility of the platform can be used to explore novel algorithms for learning <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Interface With Nengo</head><p>While with PyNN it is possible to define arbitrary network structures, using the neural engineering framework (NEF) <ref type="bibr" target="#b22">[23]</ref>, it is possible to encode functions and dynamical systems in networks of spiking neurons. Using the NEF, it is possible to build complex cognitive architectures such as SPAUN <ref type="bibr" target="#b23">[24]</ref>, a spike-based functional model of the brain that makes comparisons with human neural and behavioral data possible. SpiNNaker has, therefore, been interfaced with Nengo <ref type="bibr" target="#b24">[25]</ref>, the software that implements the NEF, enabling users to create neural networks by specifying the functions to be computed <ref type="bibr" target="#b12">[13]</ref>. Nengo translates the functions into neural circuitry by calculating neuronal and connectivity parameters, while PACMAN distributes and configures the model on the board. Through the use of the NEF, SpiNNaker becomes a ''neural computational box'': input values and vectors are encoded in spiking activity using the NEF principles directly on the SpiNNaker board. The desired computation is performed in real time by spiking neurons, and output values and vectors are decoded from spiking activity. Interfacing with Nengo shows how different front-ends can be interfaced with PACMAN and how flexibly the platform can be programmed with specialized neural kernels, such as the ones performing the NEF encoding and decoding processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Interface With AER sensors</head><p>Biological inspiration is not confined to the exploration of computational architectures and methods, but is also extended to neuromorphic <ref type="bibr" target="#b25">[26]</ref> sensors. Millisecond-precise pulse encoding has been used to explain the ability of the visual system to process information and to recognize complex, dynamical scenes quickly <ref type="bibr" target="#b26">[27]</ref>. With the first observable differences in the temporal lobe starting after 150 ms of the stimulus onset, and with several synaptic stages required to arrive at the infero-temporal cortex (IT, the visual area where object recognition takes place), neurons can emit at most one spike to encode the information, and are believed to encode it in the spike timing <ref type="bibr" target="#b27">[28]</ref>.</p><p>AER sensors can be used to exploit the temporal characteristics of sensory information with event-based approaches. Silicon retinae <ref type="bibr" target="#b28">[29]</ref>- <ref type="bibr" target="#b30">[31]</ref>, for example, take inspiration from their biological counterparts to implement an alternative approach to frame-based image processing on a neuromorphic substrate. Each pixel operates asynchronously, sending an AER message within a few microseconds of a local light intensity change without having to wait for a complete frame to be scanned, resulting in a reduction of latency and redundancy in visual information transmission. For an example showing the benefits of event-based over frame-based systems, see the European Union ''Convolution Address Event Representation (AER) Vision Architecture for Real-Time'' project <ref type="bibr" target="#b31">[32]</ref>.</p><p>These sensors use native event-based processing and AER representation to encode sensory information, and can, therefore, be interconnected directly to SpiNNaker, which acts as an event-based computing platform. In collaboration with the Instituto de Microelectronica de Seville (Sevilla, Spain) we have connected a silicon retina to SpiNNaker using an FPGA <ref type="bibr" target="#b32">[33]</ref>, which translates incoming retinal AER events to the self-timed 2-of-7 protocol used by SpiNNaker interchip links, directly injecting spikes (MC packets) into the packet-switched network fabric. Using this mechanism, the sensor is represented on SpiNNaker as a ''virtual chip.'' At the model level, the silicon retina can be instantiated in PyNN as: pol 0; pol 1 ¼ p:instantiate retinað Þ creating two populations (''pol_0'' and ''pol_1'', one for each ''polarity,'' encoding increasing and decreasing luminance, respectively) where neurons are topographically organized in a 2-D visual field. These populations produce spikes whenever the silicon retina emits an event, and can arbitrarily be interconnected to other populations in the model. PACMAN automatically maps each population to a specific model instantiation, preserving the connectivity information.</p><p>Analogous interfaces with AER sensors have been developed in collaboration with the Institute of Neuroinformatics (Zurich, Switzerland; using the DVS sensor <ref type="bibr" target="#b29">[30]</ref> and the ''silicon cochlea'' <ref type="bibr" target="#b33">[34]</ref>), with the Biology Group at the University of Osaka (Osaka, Japan; using a sensor inspired by the sustained and transient responses of the retina <ref type="bibr" target="#b34">[35]</ref>), and with the Institute of Vision (Paris, France; using the ATIS silicon retina <ref type="bibr" target="#b35">[36]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Integration With Robotic Platforms</head><p>While integration with AER sensors exploits the eventdriven nature of the system, interfacing it with robotic platforms in real environments shows SpiNNaker's realtime characteristics.</p><p>As with AER sensors, the robotic platform becomes available at the model level using PyNN or Nengo, while the system is configured automatically using PACMAN, enabling message transmission to and from the robot and the sensors through a small customized interface board <ref type="bibr" target="#b36">[37]</ref>. The robot is a custom omnidirectional mobile platform, with embedded low-level motor control and elementary sensory systems, developed by the Neuroscientific System Theory group of the Technische Universita ¨t Mu ¨nchen (Munich, Germany). The overall system is a standalone, autonomous, reconfigurable robotic platform with no personal computer in the loop.</p><p>We demonstrate a closed perception-action loop in an example where the robot agent has to discriminate between two different stimuli and move toward the preferred one (a ''þ''), while backing off from the detractor (an ''Â''). This is a small model that uses less than 10% of the resources on the 48-node board, but it serves to illustrate a number of the capabilities of the system.</p><p>The network structure used is represented in Fig. <ref type="figure" target="#fig_10">10</ref>. The two populations representing the different polarities of a 128Â 128 silicon retina are instantiated, as illustrated in Section IX-B. These populations are connected to four different feature maps, representing the result of the convolution between the retinal input and a kernel represented as the white insert in the four feature maps in Fig. <ref type="figure" target="#fig_10">10</ref>, where the black lines represent excitatory connections while the white surround represents inhibitory flanks. This operation, computed in parallel by all feature maps by means of spiking events, is similar to the one performed by the mammalian primary visual cortex, where cells are selectively active accordingly to the stimulus orientation <ref type="bibr" target="#b37">[38]</ref>, as previously done in a model of visual attention running on a four-node SpiNNaker board <ref type="bibr" target="#b32">[33]</ref>. Different feature maps inhibit each other in order to enhance response contrast. The following layer behaves as a local combination of oriented edge detectors, similar to the first layers of the HMAX model, a model of object recognition inspired by the visual cortex <ref type="bibr" target="#b38">[39]</ref>. If the ''þ'' is recognized (as a combination of vertical and horizontal edges), the agent is driven forward toward the preferred stimulus; conversely, if an ''Â'' is detected as a combination of þ=À45 oriented lines, the robot moves backward.</p><p>Robot movements are controlled by the output population, comprising two ''motor'' neurons (one for moving forward and one for moving backward), represented by the two vertical bars in Fig. <ref type="figure" target="#fig_10">10</ref>.</p><p>The retina and the robot are accessible through PyNN, which is also used to describe the rest of the network model, performing different steps of visual processing and orienting its response to the location where a preferred stimulus is detected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X. FUTURE PLANS</head><p>Current SpiNNaker hardware has seen use across the computational neuroscience and neurorobotic communities. All of the major hardware functions required to build larger machines have now been developed and tested, and the remaining tasks to build larger machines are now primarily related to the manufacture of further packages and PCBs.</p><p>A major commitment over the next two years is to deliver a machine with at least half a million processors as a contribution to the European Union Flagship Human Brain Project (HBP), where SpiNNaker will be one of the neuromorphic ''platforms'' offered to the wider HBP community.</p><p>An earlier, less formal, commitment is to demonstrate the capability of SpiNNaker to support a real-time imple-mentation of the University of Waterloo (Waterloo, ON, Canada) SPAUN model <ref type="bibr" target="#b23">[24]</ref>. This is expected to require a system of around 36 48-node SpiNNaker boards, or 30 000 processors, though this estimate should come down with Nengo support for sparse connectivity and reduced firing rates, and will be a solid demonstration of the capability of the SpiNNaker machine as a platform to support large-scale real-time spiking neural models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>XI. RELATED WORK</head><p>While SpiNNaker represents a particular combination of digital many-core computing with a lightweight communications infrastructure tuned to modeling large-scale spiking neural networks in biological real time, there are a number of other designs that take a different approach to achieve similar end goals <ref type="bibr" target="#b39">[40]</ref>. These various approaches can be classified according to whether they use digital or analog technology to model the neurons and synapses, the communications topology employed, and the support for synaptic plasticity.</p><p>Digital models may be implemented on conventional general-purpose computers, including cluster machines and high-performance computers, or on special-purpose hardware such as FPGAs <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, graphics professor units <ref type="bibr" target="#b42">[43]</ref>, or custom silicon <ref type="bibr" target="#b43">[44]</ref>. Analog models <ref type="bibr" target="#b44">[45]</ref> may be subthreshold <ref type="bibr" target="#b45">[46]</ref>, whereupon biological real-time performance is achievable, or above threshold <ref type="bibr" target="#b46">[47]</ref>, where the circuits are likely to be much faster than biological real time. Notable large-scale projects include the following.</p><p>The Stanford Neurogrid <ref type="bibr" target="#b45">[46]</ref> employs subthreshold analog circuits with digital spanning tree AER communications <ref type="bibr" target="#b47">[48]</ref> for real-time neural modeling. Neurogrid can model a million neurons in real time while consuming only 3 W. It combines unicast and multicast digital routing with analog signaling across a local ''diffusion network.'' The IBM neurosynaptic core <ref type="bibr" target="#b48">[49]</ref> employs custom digital circuits to achieve a one-to-one correspondence between the hardware and software simulation models. It is intended to form a generic cognitive subsystem <ref type="bibr" target="#b43">[44]</ref>. It uses AER communication. The Heidelberg HICANN system <ref type="bibr" target="#b46">[47]</ref> employs wafer-scale above threshold analog circuits that operate at 10 4 x biological real time using a twolayer AER protocol, one layer for intrawafer communication and a second layer for interwafer communication. The Cambridge BlueHive system <ref type="bibr" target="#b40">[41]</ref> employs digital circuits on FPGAs to deliver real-time performance. The communication is not pure AER; multicast is implemented using a set of ''fan-out'' messages that carry the destination, weight, and delay. These examples illustrate the diversity of approaches taken to address the problem of modeling large-scale systems of spiking neurons in real time or faster. There are arguments on both sides of the analog/digital divide (for example, energy-efficiency favors analog, whereas flexibility and repeatability favors digital), and on most other design decisions, so the area is still wide open to new ideas, and rather lacking in robust benchmarks that can be used to make quantitative comparisons between alternative approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>XII. CONCLUSION</head><p>The SpiNNaker project has been 15 years since conception and eight years in (funded) execution. Much time and effort has gone into understanding the brain-modeling problem domain and developing the architecture, silicon, and software infrastructure. While the software development will be ongoing, the architecture and silicon are now working reliably and delivering very much as originally anticipated <ref type="bibr" target="#b0">[1]</ref>.</p><p>The process of delivering the potential of the SpiNNaker platform is now underway, and early indications are largely positive. The platform is proving flexible, relatively easy to use (though there is always room for improvement in this dimension), and capable of delivering useful results across a wide range of application areas.</p><p>As the platform is scaled up toward the ultimate million-core machine, new challenges will emerge, particularly in the area of management, application mapping and loading performance, the observability of activity within the machine, and most notably with debugging large-scale models running on the machine. All of these are ongoing areas of research and development, but with help and feedback from a growing (and so far very forgiving) community of users, and secure funding within the HBP alongside a number of other funded projects that will support extensive use of the platform at the University of Manchester [including a European Research Council Advanced Grant and several Engineering and Physical Sciences Research Council (EPSRC)-funded collaborations], we are committed to continued improvement of the capabilities of the platform.</p><p>The time is right to scale up our ambition to understand the information processing principles at work in the brain, and the SpiNNaker platform has been designed to deliver a broad capability to support this ambition. The next five years will be crucial in determining the extent to which we can succeed in delivering a platform with the capabilities required to support the global brain research program. h</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Principal architectural components of a SpiNNaker node.</figDesc><graphic coords="3,57.83,76.77,210.62,257.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Inside a SpiNNaker package. The SpiNNaker chip is mounted on the substrate, then a 128-MB mobile DDR SDRAM is stacked on top of it, and the connections are made inside the package with gold wire bonding. The packaging was carried out by Unisem Europe Ltd.</figDesc><graphic coords="4,301.89,76.81,216.84,143.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. A 48-node SpiNNaker PCB. This circuit board incorporates 48 SpiNNaker packages (center) with a total of 864 ARM968 processor cores, three FPGAs (top) for high-speed inter-PCB communications through serial advanced technology attachment connectors (top left and right), with onboard power regulation (bottom).</figDesc><graphic coords="4,52.31,451.17,229.82,228.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Example output from a SpiNNaker visualizer.</figDesc><graphic coords="5,290.83,76.81,231.96,173.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The various software components running on the host machine, the root node, and other SpiNNaker nodes.</figDesc><graphic coords="6,131.86,75.93,313.32,152.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Event-driven software framework.</figDesc><graphic coords="7,127.90,76.77,313.32,340.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>cell params ¼ f'tau refrac': 5.0, 'v thresh':À50.0, 'v reset':À60.0, 'tau m': 20.0, 'tau syn E': 5.0, 'tau syn I': 10.0, 'v rest':À49.0, 'cm': 0.2} ex ¼ Populationðn À n=5; IF curr exp; cell paramsÞ in ¼ Populationðn=5; IF curr exp, cell paramsÞ The resting potential is located above the threshold potential to induce spontaneous firing in all cells. Populations are interconnected by means of a FixedProbability-Connector, which connects all the neurons in the presynaptic population</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The flow of algorithms (left) and the data representations they work on (right) within PACMAN.</figDesc><graphic coords="8,320.31,75.91,180.28,122.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Example network (left) with one excitatory and one inhibitory population with a size ratio of 4 : 1 is mapped by PACMAN onto 60 processors on four SpiNNaker chips (right).</figDesc><graphic coords="8,296.39,577.26,227.40,122.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Raster plot of the results of running the simulation of the network shown in Fig. 8. Each dot represents one neural spike; red dots are excitatory neurons, and blue dots are inhibitory neurons. Oscillatory activity is visible across the network.</figDesc><graphic coords="9,47.75,516.81,231.74,172.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig.10. Example robotic closed perception-action loop. A ''þ'' is shown to the robot, which extracts and combines the vertical and horizontal lines, moving forward. Gray kernels and dashed lines represent the fact that the pathways for the ''Â'' detection are not activated, as a ''þ'' is presented.</figDesc><graphic coords="11,127.90,76.49,313.32,405.48" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Vol. 102, No. 5, May 2014 | Proceedings of the IEEE 657</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>Furber et al: The SpiNNaker Project Vol. 102, No. 5, May 2014 | Proceedings of the IEEE 661</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>Vol. 102, No. 5, May 2014 | Proceedings of the IEEE 665</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>The SpiNNaker project has benefited from contributions from many people in the team at the University of Manchester, collaborators at the universities of Southampton, Cambridge, and Sheffield, industry partners, and many external collaborators, only some of whom has there been space to mention in the text, but the authors would like to note here the contribution of J. Conradt of the Technische Universita ¨t Mu ¨nchen (Munich, Germany) who developed the robot platform shown in Fig. <ref type="figure">10</ref>. They particularly wish also to acknowledge the benefits accrued from participation in the Capo Caccia and Telluride neuromorphic workshops, and they are grateful to the organizers for the opportunities for collaborations that have emerged from these workshops. The authors would also like to acknowledge the helpful comments and feedback from the anonymous reviewers of the first draft of this paper.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by the Engineering and Physical Sciences Research Council (EPSRC) under GrantEP/G015740/01.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ABOUT THE AUTHORS</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural systems engineering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Furber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Temple</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Soc. Interface</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="193" to="206" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bidirectional coupling between astrocytes and neurons mediates learning and dynamic coordination in the brain: A multiple modeling approach</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Wade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Mcdaid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Crunelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A S</forename><surname>Kelso</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0029445</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">An Analog VLSI System for Stereoscopic Vision</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mahowald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Kluwer</publisher>
			<pubPlace>Boston, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Point-to-point connectivity between neuromorphic chips using address events</title>
		<author>
			<persName><forename type="first">K</forename><surname>Boahen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="416" to="434" />
			<date type="published" when="2000-05">May 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<ptr target="http://top500.org/lists/2013/06/" />
		<title level="m">Top 500 Supercomputers</title>
		<imprint>
			<date type="published" when="2013-06">Jun. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Overview of the SpiNNaker system architecture</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Furber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Plana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Garside</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Painkras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Temple</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2454" to="2467" />
			<date type="published" when="2013-12">Dec. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scalable network-on-chip architecture for configurable neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Vainbrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ginosar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Microprocessors Microsyst</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="152" to="166" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SpiNNaker: A 1 W 18-core system-on-Chip for massively-parallel neural network simulation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Painkras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Plana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Garside</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Temple</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Galluppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Furber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1943" to="1953" />
			<date type="published" when="2013-08">Aug. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Visualising large-scale neural network models in real-time</title>
		<author>
			<persName><forename type="first">C</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Galluppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Rast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Furber</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN.2012.6252490</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. Neural Netw</title>
		<meeting>Int. Joint Conf. Neural Netw<address><addrLine>Brisbane, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-10-15">Jun. 10-15, 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A hierarchical configuration system for a massively parallel neural hardware platform</title>
		<author>
			<persName><forename type="first">F</forename><surname>Galluppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Rast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Plana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Furber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th Conf. Comput. Front</title>
		<meeting>9th Conf. Comput. Front</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Event-driven simulation of arbitrary spiking neural networks on SpiNNaker</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Plana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Galluppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Furber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Information Processing</title>
		<imprint>
			<biblScope unit="volume">7064</biblScope>
			<biblScope unit="page" from="424" to="430" />
			<date type="published" when="2011">2011</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Berlin, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">PyNN: A common interface for neuronal network simulators</title>
		<author>
			<persName><forename type="first">A</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bru ¨derle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eppler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kremkow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pecevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Perrinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yger</surname></persName>
		</author>
		<idno type="DOI">10.3389/neuro.11.011.2008</idno>
	</analytic>
	<monogr>
		<title level="j">Front. Neuroinf</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Python scripting in the Nengo simulator</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tripp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Eliasmith</surname></persName>
		</author>
		<idno type="DOI">10.3389/neuro.11.007.2009</idno>
	</analytic>
	<monogr>
		<title level="j">Front. Neuroinf</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mapping the structural core of human cerebral cortex</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hagmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cammoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gigandet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Meuli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Honey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Wedeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sporns</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pbio.0060159</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS Biol</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2008-07">Jul. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A quantitative map of the circuit of cat primary visual cortex</title>
		<author>
			<persName><forename type="first">T</forename><surname>Binzegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">39</biblScope>
			<biblScope unit="page" from="8441" to="8453" />
			<date type="published" when="2004-09">Sep. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">O</forename><surname>Gewaltig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Diesmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-O</forename><surname>Gewaltig</surname></persName>
		</author>
		<idno type="DOI">10.4249/scholarpedia.1430</idno>
	</analytic>
	<monogr>
		<title level="j">NEST (NEural Simulation Tool)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>Scholarpedia</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Brian: A simulator for spiking neural networks in Python</title>
		<author>
			<persName><forename type="first">D</forename><surname>Goodman</surname></persName>
		</author>
		<idno type="DOI">10.3389/neuro.11.005.2008</idno>
	</analytic>
	<monogr>
		<title level="j">Front. Neuroinf</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient parallel simulation of large-scale neuronal networks on clusters of multiprocessor computers</title>
		<author>
			<persName><forename type="first">H</forename><surname>Plesser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eppler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Diesmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Gewaltig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Euro-Par 2007 Parallel Processing</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4641</biblScope>
			<biblScope unit="page" from="672" to="681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Power analysis of large-scale, real-time neural networks on SpiNNaker</title>
		<author>
			<persName><forename type="first">E</forename><surname>Stromatias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Galluppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Furber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. Neural Netw</title>
		<meeting>Int. Joint Conf. Neural Netw</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1570" to="1577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Large-scale model of mammalian thalamocortical systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Izhikevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Edelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Nat. Acad. Sci. USA</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3593" to="3598" />
			<date type="published" when="2008-03">Mar. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Real-time simulation of detailed cortical microcircuits on SpiNNaker</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Galluppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Rast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Furber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci. Methods</title>
		<imprint>
			<biblScope unit="volume">210</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="110" to="118" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A forecast-based STDP rule suitable for neuromorphic implementation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Galluppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Rast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Furber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="3" to="14" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Eliasmith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Anderson</surname></persName>
		</author>
		<title level="m">Neural Engineering: Computation, Representation, and Dynamics in Neurobiological Systems</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A large-scale model of the functioning brain</title>
		<author>
			<persName><forename type="first">C</forename><surname>Eliasmith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bekolay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dewolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">338</biblScope>
			<biblScope unit="issue">6111</biblScope>
			<biblScope unit="page" from="1202" to="1205" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Real time on-chip implementation of dynamical systems with spiking neurons</title>
		<author>
			<persName><forename type="first">F</forename><surname>Galluppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Furber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Eliasmith</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN.2012.6252706</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. Neural Netw</title>
		<meeting>Int. Joint Conf. Neural Netw</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Analog VLSI and Neural Systems</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Mead</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Addison-Wesley</publisher>
			<pubPlace>Reading, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Speed of processing in the human visual system</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thorpe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fize</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Marlot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">381</biblScope>
			<biblScope unit="page" from="520" to="522" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rate coding versus temporal order coding: What the retinal ganglion cells tell the visual cortex</title>
		<author>
			<persName><forename type="first">R</forename><surname>Van Rullen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thorpe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1255" to="1283" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A silicon model of early visual processing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mahowald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="97" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A 128Â 128 120 dB 15 s latency asynchronous temporal contrast vision sensor</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lichtsteiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Posch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Solid State Circuits</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="566" to="576" />
			<date type="published" when="2008-02">Feb. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A 3.6s latency asynchronous frame-free event-driven dynamic-vision-sensor</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lenero-Bardallo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Serrano-Gotarredona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Linares-Barranco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Solid State Circuits</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1443" to="1455" />
			<date type="published" when="2011-06">Jun. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">CAVIAR: A 45 k neuron, 5 M synapse, 12 G connects/s AER hardware sensory-processing-learning-actuating system for high-speed visual object recognition and tracking</title>
		<author>
			<persName><forename type="first">R</forename><surname>Serrano-Gotarredona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lichtsteiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Linares-Barranco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Paz-Vicente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gomez-Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Camunas-Mesa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rivas-Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hafliger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jimenez-Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Civit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ballcels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Serrano-Gotarredona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Acosta-Jimenez</surname></persName>
		</author>
		<author>
			<persName><surname>Linares-Barranco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1417" to="1438" />
			<date type="published" when="2009-09">Sep. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A real-time, event-driven neuromorphic system for goal-directed attentional selection</title>
		<author>
			<persName><forename type="first">F</forename><surname>Galluppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Brohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Serrano-Gotarredona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carrasco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Linares-Barranco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Furber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Neural Information Processing</title>
		<imprint>
			<biblScope unit="volume">7664</biblScope>
			<biblScope unit="page" from="226" to="233" />
			<date type="published" when="2012">2012</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Berlin, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">AER EAR: A matched silicon cochlea pair with address event representation interface</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Schaik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp. Circuits Syst</title>
		<meeting>IEEE Int. Symp. Circuits Syst</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="4213" to="4216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An analog VLSI chip emulating sustained and transient response channels of the vertebrate retina</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kameda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1405" to="1412" />
			<date type="published" when="2003-09">Sep. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A QVGA 143 dB dynamic range asynchronous address-event PWM dynamic image sensor with lossless pixel-level video compression</title>
		<author>
			<persName><forename type="first">C</forename><surname>Posch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Matolin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wohlgenannt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Solid-State Circuits Conf</title>
		<meeting>Int. Solid-State Circuits Conf</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="400" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Real-time interface board for closed-loop robotic tasks on the SpiNNaker neural computing system</title>
		<author>
			<persName><forename type="first">C</forename><surname>Denk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Llobet-Blandino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Galluppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Plana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Furber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Conradt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Neural Networks and Machine LearningVICANN</title>
		<imprint>
			<biblScope unit="volume">8131</biblScope>
			<biblScope unit="page" from="467" to="474" />
			<date type="published" when="2013">2013. 2013</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Berlin, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Receptive fields, binocular interaction and functional architecture of the cat&apos;s cortex</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Hubel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Wiesel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Physiol</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="page" from="106" to="154" />
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Robust object recognition with cortex-like mechanisms</title>
		<author>
			<persName><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bileschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riesenhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="411" to="426" />
			<date type="published" when="2007-03">Mar. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Challenges for brain emulation: Why is it so difficult?</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cattell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Parker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Intell</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="17" to="31" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">BluehiveVA field-programmable custom computing machine for extreme-scale real-time neural network simulation</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J T</forename><surname>Marsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Markettos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mujumdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 20th Int. Symp. Field-Programmable Custom Comput</title>
		<meeting>IEEE 20th Int. Symp. Field-Programmable Custom Comput</meeting>
		<imprint>
			<publisher>Mach</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="133" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Design of silicon brains in the nano-CMOS era: Spiking neurons, learning synapses and neural architecture optimization</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Cassidy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Georgiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Andreou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="4" to="26" />
			<date type="published" when="2013-06">Jun. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Accelerated simulation of spiking neural networks using GPUs</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Shanahan</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN.2010.5596678</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. Neural Netw</title>
		<meeting>Int. Joint Conf. Neural Netw</meeting>
		<imprint>
			<date type="published" when="2010-07">Jul. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Building block of a programmable neuromorphic substrate: A digital neurosynaptic core</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Merolla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Akopyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Alvarez-Icaza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cassidy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Imam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Risk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Modha</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN.2012.6252637</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf</title>
		<meeting>Int. Joint Conf</meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Neuromorphic silicon neuron circuits</title>
		<author>
			<persName><forename type="first">G</forename><surname>Indiveri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Linares-Barranco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Schaik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Etienne-Cummings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dudek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hafliger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Renaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schemmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cauwenberghs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hynna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Folowosele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saighi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Serrano-Gotarredona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wijekoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Boahen</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnins.2011.00073</idno>
	</analytic>
	<monogr>
		<title level="j">Front. Neurosci</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">23</biblScope>
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Neurotech for neuroscience: Unifying concepts, organizing principles, and emerging tools</title>
		<author>
			<persName><forename type="first">R</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Boahen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grillner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kopell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Olsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci</title>
		<imprint>
			<biblScope unit="page" from="11807" to="11819" />
			<date type="published" when="2007-10">Oct. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A wafer-scale neuromorphic hardware system for large-scale neural modeling,&apos;</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schemmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bruderle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Grubl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Millner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">&apos; in Proc. Int. Symp. Circuits Syst</title>
		<imprint>
			<biblScope unit="page" from="1947" to="1950" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A multicast tree router for multichip neuromorphic systems</title>
		<author>
			<persName><forename type="first">P</forename><surname>Merolla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Bussat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Boahen</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCSI.2013.2284184</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A digital neurosynaptic core using embedded crossbar memory with 45 pj per spike in 45 nm</title>
		<author>
			<persName><forename type="first">P</forename><surname>Merolla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Akopyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Imam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Modha</surname></persName>
		</author>
		<idno type="DOI">10.1109/CICC.2011.6055294</idno>
	</analytic>
	<monogr>
		<title level="s">Proc. Custom Integr. Circuits Conf.</title>
		<imprint>
			<date type="published" when="2011-09">Sep. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
