<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Classifying Memory Access Patterns for Prefetching</title>
				<funder ref="#_xUSfemu">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Grant</forename><surname>Ayers</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Heiner</forename><surname>Litz</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ranganathan</forename><surname>Parthasarathy</surname></persName>
						</author>
						<author>
							<persName><surname>Google</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">UC Santa Cruz Christos Kozyrakis Stanford University</orgName>
								<address>
									<settlement>Google</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Classifying Memory Access Patterns for Prefetching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3373376.3378498</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Prefetching is a well-studied technique for addressing the memory access stall time of contemporary microprocessors. However, despite a large body of related work, the memory access behavior of applications is not well understood, and it remains difficult to predict whether a particular application will benefit from a given prefetcher technique. In this work we propose a novel methodology to classify the memory access patterns of applications, enabling well-informed reasoning about the applicability of a certain prefetcher. Our approach leverages instruction dataflow information to uncover a wide range of access patterns, including arbitrary combinations of offsets and indirection. These combinationsor prefetch kernels-represent reuse, strides, reference locality, and complex address generation. By determining the complexity and frequency of these access patterns, we enable reasoning about prefetcher timeliness and criticality, exposing the limitations of existing prefetchers today. Moreover, using these kernels, we are able to compute the next address for the majority of top-missing instructions, and we propose a software prefetch injection methodology that is able to outperform state-of-the-art hardware prefetchers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation burden</head><p>Von-Neuman architectures suffer from the well-known processor-memory performance gap: While processors have enjoyed exponential increases in performance, memory has scaled in terms of bandwidth and capacity but not in access latency. As a result, the cycle time of modern processors is now two orders of magnitude smaller than the access latency of DRAM. One way computer architects have addressed this problem is by employing deep memory hierarchies with small, low-latency caches. However, given that data set sizes are increasing <ref type="bibr" target="#b27">[28]</ref> and transistor scaling is slowing down <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20]</ref>, we cannot rely solely on cache capacity scaling. Prefetching works around limited cache capacities by speculatively moving data from slow memory into fast caches in advance, so that later demand loads can access the data from those caches with low latency. This can be an efficient technique as it often requires very few hardware resources. However, it relies on an accurate prediction mechanism and sufficient time to prefetch the correct data elements.</p><p>There exists a large body of prior research on prefetchers including stream prefetchers <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44]</ref>, correlation prefetchers <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b44">45]</ref>, and execution-based prefetchers <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b53">54]</ref> which have shown significant performance gains. Surprisingly however, the prefetchers deployed by contemporary microprocessors such as Intel Xeon, are limited to simple, conservative designs such as next-line and stride prefetchers <ref type="bibr" target="#b47">[48]</ref>. This is because, as Session 6B: Memory behavior -Where did I put it?</p><p>ASPLOS <ref type="bibr">'20, March 16-20, 2020</ref>, Lausanne, Switzerland a whole, the prefetcher proposals in the literature do not capture the trifecta of performance, generality, and cost as shown in Figure <ref type="figure" target="#fig_0">1</ref>. Low-cost designs, such as stride prefetchers, are easy to implement but only improve a very small and specific set of access patterns, leaving lots of performance potential on the table. Highly general designs, such as run-ahead prefetchers, promise high performance but come with prohibitive costs such as additional hardware threads or cores. Neither extreme tries to understand the underlying access patterns of the application. Thus, it is still an open problem to design a prefetcher that yields high performance, is general to a wide range of access patterns, and is implementable. As a result, current caches and prefetchers still leave up to 60% of performance on the table in the data center <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>One longstanding challenge for prefetcher design is that computer architects and software developers generally have only a limited understanding of applications' memory access behaviors. This results in a potpourri of prefetcher designs that are excellent for certain types of workloads or kernels but not for others. As such, it is often unclear whether or not an application is amenable to a given prefetching technique, and in many cases a mismatched prefetcher can even reduce performance by creating useless memory traffic and cache evictions. A successful prefetch design that greatly improves generality while minimizing implementation burden must be made aware of the diverse access patterns of complex applications. Even hardware-based prefetchers must move towards this approach.</p><p>To address the challenges above, this paper takes a new approach: Instead of designing a new prefetcher that improves performance for a specific type of access behavior, we develop a novel methodology to understand and classify all of the memory access patterns of applications. Our methodology can be used to evaluate and optimize existing prefetchers, and, crucially, develop new prefetcher techniques. We offer insights and answers to key questions such as the following:</p><p>? What percentage of application cache misses can be handled by a particular prefetcher? ? What are the upper bounds for application miss coverage and performance improvement that a given prefetcher can provide? ? What type of prefetcher capabilities are required to prefetch a certain memory access pattern, or a given percentage of all misses in an application? ? How much opportunity is there for a prefetcher to run ahead and emit timely prefetches for a given cache miss?</p><p>We base our methodology on the observation that memory access patterns are concretely encoded in the application binaries themselves. By extracting these patterns directly, we can avoid the inaccuracy of guessing the patterns a priori, or the overhead of relearning the patterns indirectly. With this dataflow-based approach, we can classify every important miss in an application and reason not only about what type of computation is required to compute the next address, but also how much time there is to prefetch each next miss. Then, taking the application as a whole, we can reason about what type of prefetching techniques would improve performance, and whether they could be implemented in hardware or as custom injected software prefetches.</p><p>In order to focus only on the dataflow paths that are relevant to the application, we leverage both static binary analysis and dynamic profiling information. In particular, we perform a data dependency analysis for all load instructions that cause a significant number of cache misses to determine the sequence of instructions required to compute the delinquent (missed) load address. These instruction sequences are compacted to form prefetch kernels which we can then classify. Such an automated technique for classification and analysis of applications' memory access behaviors provides significant value for entities that run diverse and complex warehouse-scale computer (WSC) applications: First, our technique is scalable and performed entirely offline which enables a comprehensive, automated prefetch analysis for a large number of applications. Second, this approach can accurately predict the utility of prefetching on a per-application basis and hence can filter out applications that are unlikely to benefit from prefetching. Third, our technique can be leveraged to tune exposed prefetcher knobs, such as aggressiveness, without performing a comprehensive search of the configuration space. Finally, dataflow-based memory analysis informs the capabilities that are required for new prefetcher designs to be effective and gives us bounds on what we can expect from them.</p><p>We applied our methodology to a suite of memory intensive SPEC 2006 <ref type="bibr" target="#b21">[22]</ref>, PARSEC <ref type="bibr" target="#b5">[6]</ref> and WSC applications to show the benefits of dataflow-based classification. In contrast to prior work which has shown the benefits of different prefetching techniques, our approach enables automated reasoning about why a particular prefetcher works or doesn't work for an application. Section 5 shows that some applications cannot benefit, even from an oracle prefetcher, and quantifies the expected gains of a particular type of prefetcher. We also provide insights about the complexity of WSC applications and show why optimizing WSC applications is challenging. Finally, focusing on commercially-viable implementations, Section 6 introduces three software based prefetcher designs that leverage dataflow analysis. Our evaluation shows that there exists no simple "one size fits all" prefetcher design and that prefetcher programmability is required to achieve high performance. In particular, by leveraging application-specific prefetcher configurations to account for timeliness and memory access types, we show that dataflow-informed software-based prefetching can reclaim up to 100% of performance lost to stalls in small benchmarks, and up to 44% of lost performance in large WSC workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pattern</head><formula xml:id="formula_0">Recurrence relation R R R Example Notes Constant A n = A n-1 *ptr Delta A n = A n-1 + d streaming, array traversal d = stride Pointer Chase A n = Ld(A n-1 ) next = current-&gt;next</formula><p>Load address is derived from the value of previous load Indirect Delta</p><formula xml:id="formula_1">A n = Ld(B n-1 + d) * (M[i]) Indirect Index A n = Ld(B n-1+c + d) M[N [i]] c = base address of M, d = stride</formula><p>Constant plus offset reference</p><formula xml:id="formula_2">A n = B n + c 1 , B n = Ld(B n-1 + c 2 )</formula><p>Linked list traversal c 1 = data offset c 2 = next pointer offset B n = address of the n th struct Table <ref type="table">1</ref>. Classification of Memory Access Patterns</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Prefetchers track information about past memory accesses in order to predict future accesses. The amount of state being tracked as well as the prefetcher design itself determines the memory access patterns that can be learned by the prefetcher. Efficient prefetchers preload cache lines from slow memory into processor caches before they are used, thus reducing the average memory access time (AMAT). The performance of a prefetcher is determined by two main metrics, coverage and accuracy. Coverage defines the percentage of future memory accesses (or cache misses) a prefetcher can determine, while accuracy is defined as the ratio of prefetched memory elements to those used by future demand loads. Low coverage generally only bounds the potential performance improvement of prefetching, while low accuracy can lead to a slowdown if useless data evicts useful data from a cache or increases memory latency. Other important metrics are prefetch timeliness and cache size. The cache size determines the time period a prefetched memory element remains accessible with low latency before it is evicted. Timeliness refers to the time between a prefetch and the demand load of a cache line. If data is prefetched too late, the memory access latency of a demand load cannot be hidden, while prefetching too early can lead to an eviction of an otherwise accurately-prefetched data element.</p><p>Prior work has shown that the performance improvement delivered by prefetching is highly workload-dependent. For instance, the Canneal application from the PARSEC <ref type="bibr" target="#b5">[6]</ref> benchmark suite suffers from low instructions per cycle (IPC) due to a high AMAT. While Canneal seems to be a perfect candidate to benefit from prefetching, prior work <ref type="bibr" target="#b2">[3]</ref> shows that even complex prefetching schemes hardly improve performance. This paper addresses the question of why specific prefetchers are unable to address a particular workload. Therefore, we first provide a taxonomy of memory access patterns that exist in applications. We then define the operations that are required to prefetch a particular pattern and then develop the methodology to automatically classify the misses of an application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Memory Access Classification</head><p>Applications exhibit a wide variety of memory access patterns, ranging from simple reuse and strides to complex calculations with memory indirection. A prefetcher's performance is thus proportional to the number of cache-misscausing access patterns it can predict. In order to understand the access patterns that impede performance the most, we focus on the load instructions that contribute to the most cache misses. We make the observation that there exists a recurrence relation that defines the difference between two subsequent load addresses of the same program counter (PC). We define a recurrence relation R R R as A n = f (A n-1 ), where A is a memory address, n represents the n th execution of a particular load instruction and f (x) is an arbitrary function. The complexity of f (x) determines the capabilities a prefetcher requires to predict and prefetch a certain future cache miss. Table <ref type="table">1</ref> shows some of the common patterns we observed while analyzing a wide set of applications, ordered by increasing complexity.</p><p>Note that while Table <ref type="table">1</ref> cannot capture all potential access patterns, most accesses we have observed fit into one of these categories or a composition or nesting of them. For instance, double-linked list or tree traversals can be classified as constant plus offset reference.</p><p>Classifying memory accesses according to Table <ref type="table">1</ref> provides the following insights: Given an application and its memory access classification, we can determine the capabilities required by a prefetcher to address a certain percentage of misses. For instance, delta patterns do not contain a load (Ld) and can be predicted by performing an arithmetic operation on address A n to get A n+1 . For architectures that support prefetching certain patterns, we can then quantify the efficacy of their prefetchers. In particular, we can determine the percentage of successful prefetches for each particular class. This classification also enables a better understanding of prefetcher timeliness, since, while achieving timelines for some prefetch patterns is simple, for others it can be complex to impossible. For instance, for the delta pattern we can prefetch</p><formula xml:id="formula_3">A n = A n-k + d * k</formula><p>where k is a multiplicative Session 6B: Memory behavior -Where did I put it?</p><p>ASPLOS <ref type="bibr">'20, March 16-20, 2020</ref>, Lausanne, Switzerland factor that determines prefetcher timeliness (The higher the value of k, the further we prefetch into the future). The same applies to the Indirect Index pattern where future i values are easily predictable and can be used to prefetch arbitrary addresses stored in N . However, for pointer chase, running ahead is difficult as a load needs to be resolved first to predict the next address. If the memory latency given by the load chain is higher than the independent work executed between subsequent delinquent loads, hiding the memory latency is impossible, even with infinite run-ahead. Our approach enables these types of analyses and gives insight about the potential and the actually-achieved performance of a prefetcher technique.</p><p>The classification scheme of Table <ref type="table">1</ref> maps memory accesses to design patterns and data structures and is useful for the human observer. However, these patterns are challenging to leverage for automated processing. Complex applications will often use compositions or variations of these patterns, for instance, a variation of constant plus offset reference is a tree traversal where both children are visited. To address this issue, we formalize our approach to express prefetch patterns as follows: For a given load instruction and its function to compute the next address, f (x) can be expressed as a prefetch kernel which consists of a number of data sources and operations on those sources which generate the delinquent load address. There are three types of data sources: Constants, registers and memory locations. An operation might be any instruction (or micro-op) specified by the ISA of the architecture being analyzed. We list the most-frequently-used operations that our approach leverages for classification in Table <ref type="table">2</ref>. By binning multiple patterns into the same class (e.g. multiple loads are classified as load) this approach becomes general for handling arbitrarily-complex patterns while providing a rich set of analytical capabilities. For instance, it allows us to determine whether a pattern is indirect (uses the value returned by a load) or whether the data sources that feed into loads are constant or depend on a prior execution of the kernel. Furthermore, by analyzing prefetch kernels we can determine the instructions (capabilities) that a prefetcher needs to support to achieve a certain miss coverage of the application. We can also obtain insights about timeliness by analyzing the kernel's complexity and depth of the load chain. In the next section, we will explain how to obtain prefetch kernels from applications in an automated way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Prefetch Kernel Extraction</head><p>Our proposed methodology performs offline analysis of application binaries in combination with traces that contain their instruction sequences and memory load and store addresses. Utilizing dynamic information from traces allows us to focus only on execution paths and misses that matter, and is critical for reducing kernels to their most basic behaviors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Machine Classification Corresponding Pattern</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Constant</head><p>Constant Add Delta Add, Multiply Complex Load, Add Linked List, Indirect Index, ... Load, Add, Multiply Complex Table <ref type="table">2</ref>. Machine Operations and Patterns</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataflow Extraction Overview</head><p>Each miss in the application is caused by an address that was calculated by one or more instructions in the program binary (or libraries). The goal of dataflow analysis is to extract and then classify these address-generating instructions for each miss. As a side effect, the kernels also fully describe how to compute the next memory address accessed by the instruction. Because there is typically an intractable number of dataflow paths in most programs, we leverage dynamic profile information to prune our work to the paths that matter most for prefetching. We start by collecting application traces with Memtrace <ref type="bibr" target="#b6">[7]</ref>. Each trace contains a sequence of instruction program counters (PCs) as well as the memory addresses of all load and store instructions. We also collect a cache miss profile comprised of a ranked list of PCs that cause cache misses. Such a profile can be generated by feeding the traces through a cache simulator, or via performance counters such as Intel's Precise Event-Based Sampling (PEBS) <ref type="bibr" target="#b15">[16]</ref>. It is important to note that a miss profile is specific not only to individual workloads, but also to the machines on which they are run: If we were to classify and then run an application on two machines with different memory hierarchies, the classification might focus on unimportant dataflow graphs. As such, we assume that all dataflow analyses (and any resulting optimizations) are made on a per-application, per-architecture basis.</p><p>The combination of the program and library binaries, a program trace, and a miss profile allows us to rebuild the dataflow graphs for each miss-causing instruction in the application. These graphs can be used for classification as described in Section 5. Furthermore, our kernel-based prefetcher, described in Section 6, is based on these kernels.</p><p>A data dependency graph, or prefetch kernel, fully describes the data (e.g., constants) and computations that are required to form a miss address. The vertices of the graph represent operands, which can be constants, registers, or memory locations. The edges of the graph encode the operations (add, load, assign, etc.) between vertices which form the data dependencies. The graph is directed where the root (sink) vertex is the miss-causing instruction address, and there are one or more source vertices. The depth of the graph defines the critical path length of the kernel. To perform kernel extraction, we search through the instruction trace in execution order for occurrences of misscausing PCs. Each time an important miss-causing PC is found, we form a new dataflow graph that begins at that PC and searches backward in time until the last occurrence of the same PC. This window of execution history can be as small as a few instructions (e.g., a miss PC within a tight loop), or as large as millions of instructions in scale-out WSC workloads like web search. The root node of the graph is the memory address of the miss PC, which may be as simple as a fixed constant, or, in the case of x86, a combination of base, index, and segment registers as well as a scale and displacement. Whatever the case, each component of the address is added to the graph and then itself searched for its own data dependencies by looking even further back in the execution sequence. If a vertex is part of a load instruction, we check if there is a prior store to the same address. If so, we can connect the data dependence from the load to the store since, within a single thread, this is guaranteed to be the source. Load-to-store dependencies are very common, especially in x86 because of frequent register spilling. Our ability to follow dependencies through memory distinguishes our approach from prior work on prefetch-slicing and static-only analysis. For instance, the pointer analysis performed by static approaches to determine data dependencies can be challenging if not impossible whereas our approach can determine dependencies through memory natively.</p><p>We continue building the kernel recursively until all of the source vertices in the graph are terminal. A vertex is terminal if it is a constant (e.g., an immediate, displacement, or scale value), a register that cannot be traced back further (e.g., from a random number source), or any operand that has reached the edge of the instruction sequence (i.e., it has reached the prior occurrence of the miss PC in question). Thus each terminal vertex is a dataflow source, and can be a constant, a register, or a location in memory. Finally, for dataflow sources that change after use (at any point up until the graph root), we add the paths that describe these changes (See Section 4.3).</p><p>We note that our technique determines data dependencies within a sequence, ignoring control flow dependencies. We leverage the fact that the captured dynamic traces already resolve all branches for us and hence we do not need to follow different paths as would be the case for static analysis techniques. Instead, our tool analyses all instruction sequences in between pairs of the same miss PC, and as a result, the tool might discover different data dependency graphs for the same load PC. We track the frequency of occurrence of these different graphs and only utilize those for further processing that are executed frequently. As we will show in the next section, omitting divergent control flow paths enables us to compact prefetch kernels significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Compaction</head><p>At this point, the extracted instruction graph represents the subset of the original application's execution history that is required to compute the load address of a delinquent load. A raw dataflow kernel typically includes a lot of extra and unnecessary operations that make classification difficult. For example, a kernel for a stride access pattern should be as simple as adding a constant to the prior address. However, it may actually contain memory loads and stores caused by register spilling and function calls (e.g., if the base address is passed as a stack parameter). Proper classification of memory access patterns relies on our ability to reduce these graphs to their minimal form, otherwise we would not be able to recognize even simple patterns like strides. To enable compaction, we developed the following techniques for removing dataflow artifacts introduced by the ISA and compiler.</p><p>Store-Load Bypassing In x86, register spilling leads to frequent data movement between registers and the stack in memory, consisting of matching push/pop, or more generally, store/load instruction pairs. While static analysis tools can match push/pop pairs (because the relative memory addresses of stack operations are well-defined), they cannot match general store/load pairs as the memory addresses are unknown at compile time. Our technique leverages dynamic memory traces, matching all store/load pairs and bypassing memory operations by directly connecting store source registers to load destination registers.</p><p>Arithmetic Distribution and Compaction Many dataflow graphs contain operations that ultimately cancel out and should be pruned for proper classification. For example, an immediate value added to a known-zero register (e.g., $0 in MIPS or a register XOR'ed with itself) does not actually require addition as it represents a constant value. Much more complex scenarios arise when, for instance, a fixed base address for an array is passed over the stack. In this case the stack pointer would be identified as a dataflow source, and we have to show that the base address is fixed even though the stack pointer may change frequently in the graph. We can do this by following the pushes and pops of the stack through the dataflow graph showing that they are balanced between subsequent executions of the graph kernel. If they are balanced, the stack pointer is fixed between executions of the kernel and we can then conclude that the base address is also fixed.</p><p>In order to facilitate arithmetic compaction and reduction, we flatten the dataflow graphs by distributing and reducing nested arithmetic operations. For example, if a graph represented the formula (3 + 4) -(6 + 1), it would be flattened to 3+4-6-1 and then simplified to zero and pruned. Arithmetic distribution and compaction is expensive computationally, but we've observed that it can reduce graphs by a factor of 1, 000?, especially for kernels that span wide call stacks.</p><p>Assignment and Zero Pruning Assignments between registers (e.g., mov %rcx, %rdx in x86) occur frequently but are not relevant for classification. All trivial assignments, as well as those revealed by other reductions such as store-load bypassing or distribution, are optimized out of the dataflow graphs. Additionally, many instances of zero values occur either as constants or as computational patterns (such as xor %rax, %rax to clear registers in x86). Removing these vertices often causes large subpaths of the graphs to be pruned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Kernel Extraction Example</head><p>This section provides an example execution of our offline trace analysis tool to generate a prefetch kernel. Figure <ref type="figure">2</ref> shows a code snippet of a linked list traversal that calls a trivial work function at each node. Since each node is allocated on the heap, its memory address is not required to have any relationship to other nodes in the list. In other words, there is no inherent spatial locality between dynamically-allocated heap objects. However, linked list nodes are related to each other through their next pointers, a case of reference locality. Thus a dataflow graph should relate the access of one node to the next with indirection via a load instruction.</p><p>In this program, profiling shows that a cache miss occurs in do_work when the 5 th field of the node is accessed. We can make an important observation here: Cache misses are triggered by the first access to a cache line, the do_work function in this case, and not by the primary pointer chasing instruction node=node?next in simple_chase. As a result, our tool needs to be able to observe data flow dependencies through function calls and memory. Figure <ref type="figure" target="#fig_3">3</ref> shows the execution trace of one iteration of the simple_chase loop, where the last instruction (0x4013fe at the bottom) is the cache miss. The assembly code is in the AT&amp;T syntax of instruction source_operand, destination_operand.</p><p>To start the analysis, consider line 35 of the dynamic trace shown in Figure <ref type="figure" target="#fig_3">3</ref> which shows the miss-causing load instruction 0x4013fe. The tool starts with this instruction and then traverses the trace in backwards direction until either all of the dataflow sources are terminal or it finds the same PC again. The resulting graph is the raw, non-compacted prefetch kernel. We can see that 0x4013fe utilizes the rax register as a source operand to obtain the target memory address. We then search for the next line in the upward direction utilizing rax as a destination register which is the previous line 34. In line 34, we can observe that rax is read from main memory, potentially because of register spilling. Tracing the data flow through memory is challenging as we need to know the effective address used to read from memory. Fortunately, our dynamic traces not only contain instructions but also the effective memory address of each load and store (mov with indirect addressing). Using that additional effective address information (not shown in Figure <ref type="figure" target="#fig_3">3</ref>) we can also follow dependency chains through memory. Note that register spills will be removed from the prefetch-kernel within the optimization pass, however, for now we need to observe dependencies through memory to continue the traversal. As shown by the arrows in Figure <ref type="figure" target="#fig_3">3</ref>, the tool traverses the trace in a backwards direction, tracking all data flow dependencies until it reaches line 12, pop %rbp. Although the tool continues traversal until it hits line 1 (the boundary of the prior loop iteration), it cannot find an earlier instruction that stores the data for rbp, and thus rbp is a dataflow source (as well as all of the constants that were found along the path).</p><p>Even after simplifying the graph with assignment and store/load bypassing, the resulting dataflow kernel graph is still overly complex for a simple linked list access pattern:</p><formula xml:id="formula_4">A n = 0x28 + load(0x38 + load(-0x8 + load(rsp))) (1)</formula><p>This is because simple backwards traversal is insufficient for graph reduction: We also need to analyze if and how source operands are changed by instructions that are not contained in the backwards pass in order to classify the graph. In particular, we now need to perform a forward pass for each discovered source operand. Taking the source instruction pop %rbp from line 12, we determine that the load address (the stack pointer) is constant since the number of push/call instructions is equal to the number of pop/ret instructions and thus the source data register rbp will always be loaded from the same memory location. Next, we determine that the value of rbp does not change from the time it is loaded (line 12) until it is stored (line 30). Then, observing that the source data in rbp is constant, we can eliminate the two inner loads of Equation 1 and arrive at</p><formula xml:id="formula_5">A n = 0x28 + load(0x38 + rax) (<label>2</label></formula><formula xml:id="formula_6">)</formula><p>where rax is the value of rax at line 16  In summary, the dataflow process extracts and reduces dataflow graphs, analyzes source operands for zero pruning, and finally classifies each graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Implementation</head><p>We implemented our dataflow methodology in C++ in approximately 15,000 source lines of code. It is a stand-alone program that accepts an application binary (and libraries), execution trace, and miss profile as inputs, and outputs data classification information as well as software prefetch injection information which we use in Section 6. The graph analysis engine is ISA-agnostic, as all instructions are broken down into very basic micro-ops (so, for example, an x86 push instruction would be decomposed into a register decrement and memory store). However, the tool requires a front-end to convert application binary instructions into the micro-oplike graph nodes. Despite its enormous complexity, we target the x86 instruction set due to its prevalent use in large WSC environments. However, additional ISAs including ARM, POWER, or RISC-V could be supported with considerably less effort. We utilize Intel's XED <ref type="bibr" target="#b23">[24]</ref> library to parse x86 op code classes and source and destination operands. Our tool supports the intricate details of x86 including different register sizes (a 32 bit register can be consumed by a 64 bit instruction in x86), REP-prefixed (repetitive) instructions and conditional instructions, as well as over 100 instruction classes.</p><p>For non-trivial dataflow applications (such as large WSC workloads like web search), we need to be able to process data dependency graphs of millions of vertices and edges and thousands of prefetch kernel instances. Computational efficiency, therefore, is of significant importance in both time and space. We employed a number of algorithmic improvements to reduce the runtime complexity of our tool, including multithreading (using a large shared instruction window pool), decoded instruction caching, hybrid data structures to support efficient allocation layout and insertion/deletion, and other cache and data structure optimizations to reduce memory fragmentation in the heap. With these and other optimizations in place, we can currently process 10 million trace instructions per second (MIPS) for simple applications and 0.1 -5 MIPS for complex workloads, depending on the graph sizes and depths of the applications. Our initial na?ve implementation was over hundred times slower and quickly ran out of memory on commodity server machines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>In this section we utilize our tool to analyze eight memory-bound applications. We chose 471.omnetpp and 462.libquantum from SPEC CPU2006 <ref type="bibr" target="#b21">[22]</ref>, Canneal from PARSEC <ref type="bibr" target="#b5">[6]</ref>, the monte carlo neutron transport algorithm XSBench <ref type="bibr" target="#b46">[47]</ref>, and the high-performance conjugate gradient benchmark XHPCG <ref type="bibr" target="#b13">[14]</ref>. Furthermore, we evaluate three WSC applications <ref type="bibr" target="#b4">[5]</ref> from the Google fleet; A web search leaf node, knowledge graph backend, and an ads matching service. For each of the WSC applications, we collect traces with a representative single-machine loadtest. As in a profile-guided compilation approach, we execute the applications once, instrumented with DynamoRIO's Memtrace tool to generate application traces. The traces are then postprocessed with our dataflow tool described in Section 4 to generate the following prefetch kernel analysis. For small applications, we analyze enough miss instructions to provide at least 95% miss coverage. For complex WSC applications with long miss tails, we currently limit our analysis to the top 200 miss PCs, which provides between 64%-86% miss coverage.</p><p>Session 6B: Memory behavior -Where did I put it? ASPLOS <ref type="bibr">'20, March 16-20, 2020</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Prefetch Kernel Classification</head><p>The access behavior of each miss-causing instruction is characterized by the complexity and type of calculations generating its addresses. Figure <ref type="figure" target="#fig_4">4</ref> shows the dataflow kernel classification for each of our applications. Each kernel is binned into classes similar to those described in Table <ref type="table">2</ref>, and the size of each classification bucket corresponds to the number of kernels that matched it. Because of control flow deviation, multiple kernels may exist for a single delinquent load PC, and each is classified separately. We can draw several insights from Figure <ref type="figure" target="#fig_4">4</ref>. For instance, for 462.libquantum we see that virtually all cache misses can be computed with addition (in this case, a fixed delta), and, therefore, can be prefetched by a simple stride prefetcher (or with software prefetching). The opposite holds for canneal, xsbench, and xhpcg where fewer than 20% of misses correspond to the add or add, shi f t categories prefetchable by stride prefetchers. For these, we can now bound the expected gains of a stride prefetcher in terms of miss coverage.</p><p>We also see that for most applications, computing the next address involves loading data from memory. This is not surprising, since many data structures rely on indirection, but it also reveals quantitatively why traditional hardware prefetchers are ill-equipped for these applications: Indirection typically breaks simple spatial correlations between addresses and these kernel addresses appear random in the access stream. As such, as few as 10% of misses are likely to be covered in apps like xsbench by stride-based prefetching.</p><p>Interestingly, in our large WSC applications, a significant portion of misses (&gt;30%) can be computed without indirection. However, many of these misses have large reuse distances which cause their data to be evicted and thus they aren't amenable to high-degree stride prefetching. Also significant, many of these "simple" kernels still require complex calculations involving many operations as we'll see next. Lastly, the large number of different patterns in WSC applications exceeds typical prefetcher resources such as the number of streams a stride prefetcher can observe.</p><p>In the WSC workloads, some kernels are shown as uncategorized. This is not a limitation of dataflow-based analysis, but rather a consequence of not implementing the long tail of hundreds of x86 instruction encodings and variations needed to fully analyze every kernel. A more complete implementation (or simpler ISA) would properly categorize these slices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Prefetch Kernel Complexity</head><p>Another dimension of access pattern characterization is the number of calculations required to form an address. Figure <ref type="figure" target="#fig_6">5</ref> shows a cumulative distribution function (CDF) of the number of calculations (add, shi f t, multiply, load, etc.) a prefetcher must support (per kernel) to achieve a certain percentage of miss coverage. For simple applications like 462.libquantum, five calculations per kernel covers all misses in the program. On the other hand, nearly 40% of misses in Ads have kernels of more than 10 operations, with some more than 100.</p><p>It is valuable to understand the computational complexity of an application's access patterns, since it informs us about the difficulty of learning patterns indirectly as well as about the storage and computational resources of a hardware prefetcher. Our large WSC application analysis suggests that prefetchers with simple pattern detection will be incapable of covering a majority of cache misses.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Prefetch Kernel Timeliness</head><p>Dataflow analysis gives us accurate address computations, but even accurate prefetches are useless if they can't be completed faster than the program needs them. We can leverage our dynamic profiling information with our compacted prefetch kernels to reason about how much time slack is available for prefetching each miss. Figure <ref type="figure" target="#fig_8">6</ref> shows a prefetch kernel scatter plot, presenting the prefetch kernel latency on the x-axis and the program latency (time between subsequent loads of the miss PC) on the y-axis. Each dot corresponds to one unique prefetch kernel, and the color of the kernel corresponds to its execution frequency. Dots near the blue line (with slope 1) represent prefetch kernels with approximately the same latency as the program. These kernels are challenging to prefetch, sometimes because they are executed in a tight loop (with no unrelated program calculations), because the kernel computations could not be simplified, or because memory loads dominate the latency (e.g., a pure pointer chase). If there is any run-ahead slack at all, then a high prefetch degree can help, where we define prefetch degree d as the number of recursive executions of a prefetch kernel. Applying a prefetch kernel multiple times enables further runahead by prefetching the next d misses of a delinquent load. Otherwise (and especially if these kernels contain chained loads as well), it becomes impossible to run ahead of the main program rendering prefetching useless.</p><p>On the other hand, kernels with larger distances from the blue line are capable of running ahead of the program and are likely to be prefetched in time. In this case, low prefetch degrees are sufficient and, to avoid cache pollution, it can be beneficial to delay injection of the prefetch. By leveraging this data, the prefetch degree, timeliness, and injection sites can be optimized for every prefetched load individually. We show the performance benefit achieved by adjusting per-load parameters in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Prefetcher Design</head><p>We propose a new software prefetcher design based on dataflow analysis. Our approach executes the target binary once to obtain execution traces and miss profiles, performs dataflow analysis, and finally injects prefetches into a new binary, as in automatic feedback-driven optimization (FDO) <ref type="bibr" target="#b8">[9]</ref>. In other words, prefetches are automatically injected by the compiler at the injection sites we specify, and the compiler re-links a new optimized binary. Inserting useful prefetches requires that we address the following questions:</p><p>? What address should be prefetched?</p><p>? Where should prefetch instructions be inserted?</p><p>? How aggressively should we prefetch?</p><p>Prior works on compiler assisted prefetching <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b51">52]</ref> struggle to address these questions. In particular, they either require manual annotation or are significantly limited in the access patterns they can extract from source code (e.g., they can only prefetch simple loops with regular stride accesses). These prior approaches also have limited knowledge about the timeliness of a prefetch and hence have often been ignored in favor of hardware techniques.</p><p>Our dataflow methodology offers new opportunities for software-based prefetching. First, prefetch kernels are capable of expressing arbitrarily-complex formulas to compute the prefetch address, and can utilize multiple source registers and even memory as inputs. Second, by analyzing the distance in instructions between recurrent delinquent load PCs in the trace, we can determine optimal insertion sites for the prefetch instructions. In particular, by leveraging dynamic information such as IPC as well as microarchitectural knowledge of the cache and DRAM latencies, we can compute favorable insertion sites that are timely. We adopt the prefetch dynamic window injection technique <ref type="bibr" target="#b4">[5]</ref> to minimize fan-in and fan-out of our insertion sites. Third, by leveraging additional profiling information contained in the trace, we can tune the prefetch degree (aggressiveness) for each individual delinquent load PC. In particular, for dense loops that contain few instructions unrelated to the delinquent load, high-degree prefetching is generally useful. However, if the loop count is small, then prefetching too far ahead will only pollute the cache. Our prefetcher utilizes Formula (3) below to compute the prefetch degree (D), where LoopCnt is the average number of iterations of a loop that resembles a Session 6B: Memory behavior -Where did I put it?</p><p>ASPLOS <ref type="bibr">'20, March 16-20, 2020</ref>, Lausanne, Switzerland  </p><p>For kernels without loads, the address for a higher-degree prefetch can be computed by applying the kernel repeatedly. However, for complex kernels, loads need to be resolved before reapplying the formula, thus limiting the effectiveness of high-degree prefetching.</p><p>While our technique can also be implemented in hardware, we opted against it because of its high complexity and cost and lower flexibility. Prior works, including run-ahead execution <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref>, precomputation threads <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b53">54]</ref> or helper threads <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b38">39]</ref>, require complex hardware or utilize entire SMT cores for the purpose of prefetching, introducing unrealistic hardware overheads. Due to its complexity, we perform the dataflow analysis offline, once per application. Executing the prefetch kernels at runtime requires the capability to execute a wide range of instructions and hence at least a simple core would be required. As the main program never depends on prefetch kernel instructions, there is significant ILP among the two streams which can be leveraged by superscalar out-of-order processors for efficient execution of the kernels. By injecting kernels in software, we avoid large dedicated on-chip memory which would be required to hold the kernels in hardware. Furthermore, this enables greater flexibility such as configuring the trigger point for the prefetch or the aggressiveness in a kernel-specific way.</p><p>Our technique can be implemented with existing processors, however, there arises one issue. If the prefetch kernels contain load instructions, executing those speculatively can lead to illegal memory accesses causing a segmentation fault. Therefore, we propose one hardware change to existing processors which is the support of a speculative load instruction specmov. The specmov instruction tries to access the effective address in memory and is dropped instead of leading to an exception if the address is unmapped or the page access checks have failed. The dropped load also sets a flag in the condition code register which needs to be checked by a subsequent branch instruction to exit the prefetch kernel prematurely in the case of a misspeculation. As an alternative, prefetch kernels could be executed as part of a transaction such as Intel's TSX <ref type="bibr" target="#b18">[19]</ref>, however, serialization before and after the transaction would significantly reduce ILP between the main program and the independent prefetch kernel without applying additional hardware modifications.</p><p>Our methodology extracts prefetch kernels from execution traces in which all branches have been resolved and hence prefetch kernels do not contain control flow instructions. This enables efficient compaction of the kernels, however, it means that our tool may find multiple kernels for a given delinquent load. For instance, for every loop, the dataflow analysis will at least determine two kernels: One kernel reflects the regular iterations of the loop and another reflects the last iteration where the loop is exited. The dataflow analysis will find both kernels and count the number of executions of each kernel. We only utilize a prefetch kernel if it is executed significantly more frequently than the other kernels of the same PC. If two kernels have the same frequency, we prefetch both. If more than two frequent kernels exist, we ignore them entirely for prefetching. Our experiments have shown that prefetching more than two kernels often leads to cache pollution, offsetting any benefits of prefetching. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Evaluation</head><p>We evaluate our proposed prefetcher technique via simulation and compare it against several baselines. The first baseline reflects a contemporary Intel processor with a simple stream prefetcher that can detect regular strides. Based on this architecture we evaluate compiler-assisted techniques such as prefetching constant prefetch kernels with a fixed degree as well as variable per load degree. Finally, we evaluate a prefetcher that leverages the specmov instruction to also prefetch complex memory access patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Methodology</head><p>We implemented our kernel prefetcher as part of the zsim <ref type="bibr" target="#b42">[43]</ref> simulator which we modified to include a trace-driven execution mode. We collect traces with Dy-namoRIO's <ref type="bibr" target="#b6">[7]</ref> memtrace client, and limit traces to two billion instructions during steady-state execution. We inject the prefetch kernels at the insertion sites determined by dataflow analysis. If the insertion site is part of an inlined function, we insert the kernel at each location. To perform high-degree prefetching, we execute the kernel in a loop to compute the next degree prefetch addresses. In case there exist two frequent kernels for a delinquent load, we insert both kernels as described in Section 6. All instructions executed as part of prefetch kernels are modeled as overhead and not included in IPC improvements.</p><p>The baseline stride prefetcher observes L3 misses and prefetches into the L1 cache. Our software prefetches also target the L1 cache. As zsim implements an inclusive memory hierarchy, this guarantees that lines are also prefetched into the L3 cache. Prefetching is memory-bandwidth-limited and both hardware and software prefetches are dropped whenever the processor consumes more than 90% of the peak memory bandwidth. For the processor we utilize an Intel Haswelllike configuration with the properties shown in Table <ref type="table" target="#tab_2">3</ref>. We limit DRAM bandwidth to 10GB/s per core. We use the same set of applications as used in the study performed in Section 5: 462.libquantum and 471.omnetpp from SPEC 2006 <ref type="bibr" target="#b21">[22]</ref>, Canneal from PARSEC <ref type="bibr" target="#b5">[6]</ref>, XSBench <ref type="bibr" target="#b46">[47]</ref>, XHPCG <ref type="bibr" target="#b13">[14]</ref> and the WSC applications Web Search, Knowledge Graph, and Ads from Google. To show the upper-bound of performance gains that can be obtained with prefetching, we compare each approach against a perfect memory hierarchy in which memory accesses are always served from the L1 cache, including cold misses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Results</head><p>Figure <ref type="figure">7</ref> shows the performance improvement that we measured with our prefetching methodology. We compare a stride prefetcher (red, left) with dataflow-informed prefetchers of simple, load-less kernels and fixed prefetch degrees (df_1, df_2, df_4, df_8) as well as a variable-degree (df_var) dataflow prefetcher that can utilize prefetch kernels with memory loads (df_var_load). Furthermore, we show the theoretical upper bound performance (perfect) where every access hits the L1 cache (gray, right).</p><p>As we showed in Figure <ref type="figure" target="#fig_4">4</ref>, 462.libquantum performs predictable accesses with a constant stride. This causes frequent L3 cache misses that are easy to predict by all prefetchers, however, a high prefetching degree is required to achieve optimal performance. This is challenging for both software and hardware approaches that generally need to be configured to prefetch a fixed degree, but is addressed by our software prefetch techniques that leverage a variable per-load degree. 471.omnetpp, canneal and xsbench exhibit a large fraction of complex kernels that include chained loads. As a result, the stride prefetcher and the software techniques that are limited to prefetching non-load kernels are unable to improve performance significantly, while the prefetching technique leveraging specmov delivers a speedup of 1.38?, 1.14? and 1.9?, respectively.</p><p>As shown in Figure <ref type="figure" target="#fig_8">6</ref>, 471.omnetpp, canneal and xsbench utilize relatively large dataflow kernels, and hence, the gains achieved by prefetching load chains is limited by timeliness. According to Figure <ref type="figure" target="#fig_4">4</ref>, 50% of xhpcg's dataflow kernels are composed of add/multiply operations that do not require loads. The extracted address computation formulas only leverage simple arithmetic, but nevertheless, cannot be learned by the stride prefetcher which can only handle kernels limited to add operators with constant addends. For xhpcg, our tool fails to determine the optimal prefetch degree as a more aggressive degree of 8 delivers higher performance. Nevertheless, for the other applications, variable prefetch degree always performs better than using a fixed degree.</p><p>All three WSC applications showed a wide range of dataflow kernel classes. They also show an order of magnitude higher number of performance-relevant loads and hence are challenging to prefetch. Nevertheless, for Web Search our prefetcher shows 9% IPC gain and for Knowledge Graph an improvement of 6%. These applications have been performance optimized for years and hence limited gains are expected, but nevertheless a 9% IPC gain can save millions of dollars for large WSC providers. Hardware Techniques: Several hardware mechanisms have been proposed <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b53">54]</ref> to prefetch complex memory access patterns that depend on the data and control flow of the executed program. These approaches leverage additional helper threads, running on separate cores or custom hardware to cause cache misses in advance of the main executable, thus prefetching data into caches. These dynamic techniques generally suffer from two disadvantages: First, they introduce high hardware complexity and cost as they require powerful cores to process the helper threads. For timeliness, these helper threads need to be fast and hence they cannot rely on small wimpy cores. Second, dynamic techniques have limited ability to reduce the code footprint of the helper threads using compaction.</p><p>Performing data dependency analysis in hardware requires substantial storage space and hence is limited to short instruction sequences. For example, continuous run-ahead <ref type="bibr" target="#b20">[21]</ref> limits dependency chains to 32 operations. Furthermore, while dependency analysis can filter out independent instructions, implementing the compaction techniques described in Section 4.2 in hardware is challenging. In contrast, our technique performs the dependency analysis offline, avoiding hardware overheads and enabling it to process and compact dependency chains of millions of instructions. Temporal prefetchers including GHB <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b48">49]</ref>, ISB <ref type="bibr" target="#b25">[26]</ref> and MISB <ref type="bibr" target="#b50">[51]</ref> can potentially prefetch arbitrary memory access patterns by storing long sequences of past accesses. However, to provide high performance they require megabytes of expensive on-chip memory. Additionally, they can only prefetch previouslyseen patterns, whereas dataflow kernels compute the next address based on the current state of the system. Software Techniques: Software prefetching <ref type="bibr" target="#b7">[8]</ref> techniques have been well-studied in prior work. Compiler-based techniques <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b45">46]</ref> perform static code analysis to generate prefetch targets. The performance benefits provided by static approaches are limited, as only simple structures such as Singly-Nested Loop Nests (SNLNs) <ref type="bibr" target="#b49">[50]</ref> or regular strides <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b51">52</ref>] can be learned. In contrast, our approach can handle complex dataflows of generic software algorithms and data structures. There exist several works that analyze more complex recursive data structures such as linked-lists at compile time to insert jump pointers <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref> pointing to an element within the same data structure at some distance e.g. several elements ahead in a linked-list. These jump pointers are inserted into the original data structure (e.g. a linked list node) and hence require additional storage as well as source code modifications for initializing jump pointer references whenever an element is inserted into a data structure. Prior works <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b33">34]</ref> leverage dynamic profiling for determining the most useful prefetch candidates, but do not leverage the capabilities of trace-based dataflow analysis to explore timeliness and load classification. Zhang <ref type="bibr" target="#b52">[53]</ref> performs dynamic prefetch optimization based on profiling, however, this work requires an extra thread while running the main application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>In this paper we introduced a new methodology for analyzing memory access patterns of applications for prefetching. Our offline trace-based dataflow analysis provides detailed insights about the memory access types that applications exhibit and how they affect execution performance. Our techniques enable automated classification of memory patterns, and allow us to reason about the effectiveness of a prefetcher for a given application. We applied our approach to propose new software-based prefetcher designs that leverage perload configuration knobs such as prefetcher aggressiveness to achieve performance benefits many times greater than a stride-based baseline with trivial implementation cost. Finally, we anticipate that our technique will be leveraged in future work to develop new software-and hardware-based prefetcher designs. We expect that these new designs will have to be highly configurable, preferably by software, to cover the wide range of access patterns that we quantify in this work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The landscape of prefetcher designs which weighs heavily toward low performance or high cost</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Session 6B :</head><label>6B</label><figDesc>Memory behavior -Where did I put it? ASPLOS'20, March 16-20, 2020, Lausanne, Switzerland</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 3 do_work 9 }Figure 2 .</head><label>392</label><figDesc>Figure 2. Linked List Pseudocode</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Linked List Dynamic Trace</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Memory Access Pattern Classification</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Session 6B :</head><label>6B</label><figDesc>Memory behavior -Where did I put it? ASPLOS'20, March 16-20, 2020, Lausanne,  </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Kernel ops vs. coverage CDF</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(a) 462.libquantum (b) 471.omnetpp (c) canneal (d) xsbench (e) xhpcg (f) Web Search (g) Knowledge Graph (h) Ads</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Prefetch Kernel Timeliness</figDesc><graphic url="image-6.png" coords="10,180.62,181.96,120.96,93.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Session 6B :Figure 7 .</head><label>6B7</label><figDesc>Figure 7. Software Prefetcher IPC Speedup</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>, Lausanne, Switzerland</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Constant</cell><cell>Load, Add, Multiply</cell><cell></cell><cell></cell><cell>Add, Shift</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>12.2%</cell><cell>Add</cell><cell>2.2%</cell><cell></cell><cell></cell><cell>6.8%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3.9% 4.2%</cell><cell>Load, Add</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Add 100.0%</cell><cell></cell><cell></cell><cell>Multiply Load, Add,</cell><cell>77.7%</cell><cell></cell><cell></cell><cell cols="2">Load, Add 97.1%</cell><cell>Multiply Load, Add, 91.7%</cell></row><row><cell></cell><cell cols="2">(a) 462.libquantum</cell><cell></cell><cell cols="4">(b) 471.omnetpp</cell><cell cols="2">(c) canneal</cell><cell cols="2">(d) xsbench</cell></row><row><cell>Load, Add, Multiply</cell><cell>50.4%</cell><cell>49.6%</cell><cell>Add, Multiply</cell><cell cols="2">Load, Add, Multiply Bitwise 8.4% 25.4% Multiply, Load, Add, Uncategorized 14.9%</cell><cell cols="2">Constant 9.8% Add, Multiply Add 3.9% Load, Add 8.1% 24.6%</cell><cell>Load, Add 10.5% Multiply Load, Add, 7.0% Multiply, Bitwise 10.1% Load, Add, 22.1% Uncategorized</cell><cell>Add, Multiply, Bitwise Add, Multiply 5.9% 8.0% Add 27.1% Constant 7.6%</cell><cell>Multiply Load, Add, 5.6% 27.5% Uncategorized Load, Add, Bitwise Multiply, 16.1%</cell><cell>Load, Add 11.2% 27.2% Constant 10.2%</cell><cell>Add</cell></row><row><cell></cell><cell>(e) xhpcg</cell><cell></cell><cell></cell><cell cols="3">(f) Web Search</cell><cell></cell><cell cols="2">(g) Knowledge Graph</cell><cell cols="2">(h) Ads</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>System Configuration</figDesc><table><row><cell>Parameter</cell><cell>Value</cell></row><row><cell>CPU</cell><cell>Intel Haswell E5</cell></row><row><cell>L1 Instruction cache</cell><cell>32 KiB, 8-way</cell></row><row><cell>L2 Unified cache</cell><cell>256 KiB, 8-way</cell></row><row><cell>L3 Unified cache</cell><cell>Shared 2.5 MiB/core 22-way</cell></row><row><cell cols="2">All-core turbo frequency 2.5 GHz</cell></row><row><cell>L3 cache latency</cell><cell>60 cycles (average)</cell></row><row><cell>Memory</cell><cell>DDR4 10GB/s/core 200 cycles</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>ASPLOS'20, March 16-20, 2020, Lausanne, Switzerland  </p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We would like to thank anonymous reviewers for their insightful comments. We would also like to thank our collaborators at Google, the <rs type="institution">Stanford Platform Lab</rs>, and the <rs type="institution">SRC Center for Research on Intelligent Storage and Processingin-memory (CRISP)</rs> for their support. <rs type="person">Heiner Litz</rs>'s research was supported by the <rs type="funder">NSF</rs> grant <rs type="grantNumber">CCF-1823559</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_xUSfemu">
					<idno type="grant-number">CCF-1823559</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Compiler-directed content-aware prefetching for dynamic data structures</title>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Al-Sukhni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Bratt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Connors</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2003 12th International Conference on Parallel Architectures and Compilation Techniques</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="91" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Data prefetching by dependence graph precomputation</title>
		<author>
			<persName><forename type="first">Murali</forename><surname>Annavaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jignesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">S</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><surname>Davidson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 28th Annual International Symposium on Computer Architecture</title>
		<meeting>28th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="52" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Self-contained, accurate precomputation prefetching</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Islam Atta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijayalakshmi</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioana</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Baldini</surname></persName>
		</author>
		<author>
			<persName><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th International Symposium on Microarchitecture</title>
		<meeting>the 48th International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="153" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Memory hierarchy for web search</title>
		<author>
			<persName><forename type="first">Grant</forename><surname>Ayers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung</forename><surname>Ho Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parthasarathy</forename><surname>Ranganathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Asmdb: understanding and mitigating front-end stalls in warehouse-scale computers</title>
		<author>
			<persName><forename type="first">Grant</forename><surname>Ayers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nayana</forename><forename type="middle">Prasad</forename><surname>Nagendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">I</forename><surname>August</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoun Kyu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svilen</forename><surname>Kanev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trivikram</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiner</forename><surname>Litz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tipp</forename><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parthasarathy</forename><surname>Ranganathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th International Symposium on Computer Architecture</title>
		<meeting>the 46th International Symposium on Computer Architecture</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="462" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The parsec benchmark suite: Characterization and architectural implications</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bienia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaswinder Pal</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th international conference on Parallel architectures and compilation techniques</title>
		<meeting>the 17th international conference on Parallel architectures and compilation techniques</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="72" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An infrastructure for adaptive dynamic optimization</title>
		<author>
			<persName><forename type="first">Derek</forename><surname>Bruening</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saman</forename><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Code Generation and Optimization</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003">2003. 2003. 2003</date>
			<biblScope unit="page" from="265" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Software prefetching</title>
		<author>
			<persName><forename type="first">David</forename><surname>Callahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allan</forename><surname>Porterfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="40" to="52" />
			<date type="published" when="1991">1991</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Autofdo: Automatic feedback-directed optimization for warehouse-scale applications</title>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">Xinliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tipp</forename><surname>Moseley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Code Generation and Optimization</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Data access microarchitectures for superscalar processors with compiler-assisted data prefetching</title>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">A</forename><surname>William Y Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pohua P</forename><surname>Mahlke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Mei W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><surname>Hwu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICRO</title>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="69" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pointer cache assisted prefetching</title>
		<author>
			<persName><forename type="first">Jamison</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suleyman</forename><surname>Sair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brad</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dean</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th annual ACM/IEEE international symposium on Microarchitecture</title>
		<meeting>the 35th annual ACM/IEEE international symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="62" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Speculative precomputation: Long-range prefetching of delinquent loads</title>
		<author>
			<persName><forename type="first">Hong</forename><surname>Jamison D Collins</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Dean M Tullsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong-Fong</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">P</forename><surname>Lavery</surname></persName>
		</author>
		<author>
			<persName><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 28th Annual International Symposium on Computer Architecture</title>
		<meeting>28th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="14" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Effectiveness of hardware-based stride and sequential prefetching in shared-memory multiprocessors</title>
		<author>
			<persName><forename type="first">Fredrik</forename><surname>Dahlgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Per</forename><surname>Stenstr?m</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">hpca</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="68" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Hpcg technical specification</title>
		<author>
			<persName><forename type="first">Jack</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Luszczek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heroux</surname></persName>
		</author>
		<idno>SAND2013-8752</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
		<respStmt>
			<orgName>Sandia National Laboratories</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Sandia Report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving data cache performance by pre-executing instructions under a cache miss</title>
		<author>
			<persName><forename type="first">James</forename><surname>Dundas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Mudge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Supercomputing</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="68" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">What can performance counters do for memory subsystem analysis?</title>
		<author>
			<persName><forename type="first">St?phane</forename><surname>Eranian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on memory systems performance and correctness</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dark silicon and the end of multicore scaling</title>
		<author>
			<persName><forename type="first">Emily</forename><surname>Hadi Esmaeilzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renee</forename><surname>Blem</surname></persName>
		</author>
		<author>
			<persName><surname>St</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthikeyan</forename><surname>Amant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Sankaralingam</surname></persName>
		</author>
		<author>
			<persName><surname>Burger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th Annual International Symposium on Computer Architecture, ISCA &apos;11</title>
		<meeting>the 38th Annual International Symposium on Computer Architecture, ISCA &apos;11</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Compiler-directed data prefetching in multiprocessors with memory hierarchies</title>
		<author>
			<persName><forename type="first">Elana</forename><forename type="middle">D</forename><surname>Edward H Gornish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">V</forename><surname>Granston</surname></persName>
		</author>
		<author>
			<persName><surname>Veidenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Supercomputing 25th Anniversary Volume</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="128" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Haswell: The fourth-generation intel core processor</title>
		<author>
			<persName><forename type="first">Per</forename><surname>Hammarlund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><forename type="middle">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atiq</forename><forename type="middle">A</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">L</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Hallnor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Derr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikal</forename><surname>Hunsaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajesh</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="6" to="20" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Toward dark silicon in servers</title>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Hardavellas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Ailamaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="6" to="15" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Continuous runahead: Transparent hardware acceleration for memory intensive workloads</title>
		<author>
			<persName><forename type="first">Milad</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yale</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 49th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">61</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Spec cpu2006 benchmark descriptions</title>
		<author>
			<persName><forename type="first">L</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><surname>Henning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Memory prefetching using adaptive stream detection</title>
		<author>
			<persName><forename type="first">Ibrahim</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Calvin</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 39th Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="397" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Intel x86 encoder decoder (xed</title>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<ptr target="https://github.com/intelxed/xed" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Access map pattern matching for high performance data cache prefetch</title>
		<author>
			<persName><forename type="first">Yasuo</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><surname>Inaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kei</forename><surname>Hiraki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Instruction-Level Parallelism</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Linearizing irregular memory accesses for improved correlated prefetching</title>
		<author>
			<persName><forename type="first">Akanksha</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Calvin</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 46th Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="247" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Prefetching using markov predictors</title>
		<author>
			<persName><forename type="first">Doug</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Grunwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="252" to="263" />
			<date type="published" when="1997">1997</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Profiling a warehouse-scale computer</title>
		<author>
			<persName><forename type="first">Svilen</forename><surname>Kanev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Darago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Hazelwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tipp</forename><surname>Parthasarathy Ranganathan</surname></persName>
		</author>
		<author>
			<persName><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName><surname>Gu-Yeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A case for resource efficient prefetching in multicores</title>
		<author>
			<persName><forename type="first">Muneeb</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Sandberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Hagersten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 43rd International Conference on Parallel Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="101" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Prefetching with helper threads for loosely coupled multiprocessor systems</title>
		<author>
			<persName><forename type="first">Jaejin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhee</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daeseob</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Solihin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1309" to="1324" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The performance of runtime data cache prefetching in a dynamic optimization system</title>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howard</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Chung</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bobbie</forename><surname>Othmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pen-Chung</forename><surname>Yew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong-Yuan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 36th annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">180</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dynamic helper threaded prefetching on the sun ultrasparc cmp processor</title>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Chung</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khoa</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santosh G</forename><surname>Abraham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 38th annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="93" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Compiler-based prefetching for recursive data structures</title>
		<author>
			<persName><forename type="first">Chi-Keung</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><forename type="middle">C</forename><surname>Mowry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGOPS Operating Systems Review</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="222" to="233" />
			<date type="published" when="1996">1996</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Profile-guided post-link stride prefetching</title>
		<author>
			<persName><forename type="first">Chi-Keung</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Muth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harish</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Lowney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th international conference on Supercomputing</title>
		<meeting>the 16th international conference on Supercomputing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="167" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Design and evaluation of a compiler algorithm for prefetching</title>
		<author>
			<persName><forename type="first">Monica</forename><forename type="middle">S</forename><surname>Todd C Mowry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anoop</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">memory</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">110</biblScope>
			<biblScope unit="page">120</biblScope>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Techniques for efficient processing in runahead execution engines</title>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyesoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yale</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="370" to="381" />
			<date type="published" when="2005">2005</date>
			<publisher>IEEE Computer Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Runahead execution: An alternative to very large instruction windows for outof-order processors</title>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yale</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Ninth International Symposium on High-Performance Computer Architecture, 2003. HPCA-9 2003. Proceedings</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="129" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Data cache prefetching using a global history buffer</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">E</forename><surname>Nesbit</surname></persName>
		</author>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Software, IEE Proceedings</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="96" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Runahead threads to improve smt performance</title>
		<author>
			<persName><forename type="first">Tanausu</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Pajuelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Oliverio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateo</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE 14th International Symposium on High Performance Computer Architecture</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="149" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dependence based prefetching for linked data structures</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Moshovos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gurindar</forename><forename type="middle">S</forename><surname>Sohi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGOPS Operating Systems Review</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="115" to="126" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Effective jump-pointer prefetching for linked data structures</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gurindar</forename><forename type="middle">S</forename><surname>Sohi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="111" to="121" />
			<date type="published" when="1999">1999</date>
			<publisher>IEEE Computer Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A decoupled predictor-directed stream prefetching architecture</title>
		<author>
			<persName><forename type="first">Suleyman</forename><surname>Sair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Sherwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brad</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="260" to="276" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Zsim: fast and accurate microarchitectural simulation of thousand-core systems</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Sequential program prefetching in memory hierarchies</title>
		<author>
			<persName><forename type="first">Alan</forename><surname>Jay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Smith</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="7" to="21" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Using a user-level memory thread for correlation prefetching</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Solihin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaejin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josep</forename><surname>Torrellas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 29th Annual International Symposium on Computer Architecture</title>
		<meeting>29th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="171" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A compiler-directed data prefetching scheme for chip multiprocessors</title>
		<author>
			<persName><forename type="first">Seung</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Son</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Mahmut</forename><surname>Kandemir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Karakoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruva</forename><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Sigplan Notices</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="209" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Xsbench-the development and verification of a performance abstraction for monte carlo reactor analysis</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">R</forename><surname>John R Tramm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanzima</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><surname>Schulz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Role of Reactor Physics toward a Sustainable Future (PHYSOR)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Disclosure of hardware prefetcher control on some intel? processors</title>
		<author>
			<persName><surname>Vish Viswanathan</surname></persName>
		</author>
		<ptr target="https://software.intel.com/en-us/articles/disclosure-of-hw-prefetcher-control-on-some-intel-processors" />
		<imprint>
			<date type="published" when="2019-08-09">2019-08-09</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Babak Falsafi, and Andreas Moshovos. Practical off-chip meta-data for temporal memory streaming</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Thomas F Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><surname>Ailamaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE 15th International Symposium on High Performance Computer Architecture</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="79" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">High performance compilers for parallel computing</title>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">Wolfe</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Wolfe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Addison-Wesley Reading</publisher>
			<biblScope unit="volume">102</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Efficient metadata management for irregular data prefetching</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishnendra</forename><surname>Nathella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dam</forename><surname>Sunwoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akanksha</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Calvin</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th International Symposium on Computer Architecture</title>
		<meeting>the 46th International Symposium on Computer Architecture</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="449" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Efficient discovery of regular stride patterns in irregular programs and its use in compiler prefetching</title>
		<author>
			<persName><forename type="first">Youfeng</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGPLAN Notices</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="210" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A self-repairing prefetcher in an event-driven dynamic optimization framework</title>
		<author>
			<persName><forename type="first">Weifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brad</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dean</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Code Generation and Optimization</title>
		<meeting>the International Symposium on Code Generation and Optimization</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="50" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Accelerating and adapting precomputation threads for effcient prefetching</title>
		<author>
			<persName><forename type="first">Weifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dean</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brad</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 IEEE 13th International Symposium on High Performance Computer Architecture</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="85" to="95" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
