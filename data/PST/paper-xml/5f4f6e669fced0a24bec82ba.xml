<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
							<email>jeff.rasley@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Microsoft</forename><surname>Samyam</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rajbhandari</forename><surname>Microsoft</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Microsoft</forename><surname>Yuxiong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">He</forename><surname>Microsoft</surname></persName>
						</author>
						<title level="a" type="main">DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3394486.3406703</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Distributed deep learning, machine learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Explore new techniques in Microsoft's open source library called DeepSpeed, which advances large model training by improving scale, speed, cost, and usability, unlocking the ability to train 100billion-parameter models. DeepSpeed is compatible with PyTorch. One piece of our library, called ZeRO, is a new parallelized optimizer that greatly reduces the resources needed for model and data parallelism while massively increasing the number of parameters that can be trained. Researchers have used these breakthroughs to create Turing Natural Language Generation (Turing-NLG), which at the time of its release was the largest publicly known language model at 17 billion parameters. In addition we will also go over our latest transformer kernel advancements that led the DeepSpeed team to achieve the world fastest BERT pretraining record.</p><p>The Zero Redundancy Optimizer (ZeRO) is a novel memory optimization technology for large-scale distributed deep learning. ZeRO can train deep learning models with over 100 billion parameters on the current generation of GPU clusters at three to five times the throughput of the current best system. It also presents a clear path to training models with trillions of parameters, demonstrating an unprecedented leap in deep learning system technology.</p><p>DeepSpeed brings state-of-the-art training techniques, such as ZeRO, optimized kernels, distributed training, mixed precision, and checkpointing, through lightweight APIs compatible with PyTorch. With just a few lines of code changes to your PyTorch model, you can leverage DeepSpeed to address underlying performance challenges and boost the speed and scale of your training.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">TUTORIAL OUTLINE</head><p>The latest trend in AI is that larger natural language models provide better accuracy; however, larger models are difficult to train because of cost, time, and ease of code integration. With the goal of advancing large model training by improving scale, speed, cost, and usability for model developers across the world, Microsoft made the DeepSpeed library open source in February of 2020. Since then, the DeepSpeed team has been hard at work extending the library to continue pushing the boundaries of scale and speed of deep learning training.</p><p>In this tutorial, the DeepSpeed team will discuss what DeepSpeed is (https://www.deepspeed.ai), how to use it with your existing PyTorch <ref type="bibr" target="#b0">[1]</ref> models, and advancements in the ZeRO optimizer that are central to supporting training of 100-200 billion parameter models and higher. The team will present deep-dive results on how they were able to obtain the world record for fastest BERT training. In addition, other features of DeepSpeed will be discussed such as mixed precision <ref type="bibr" target="#b1">[2]</ref> support to take full advantage of NVIDIA's Volta architecture <ref type="bibr" target="#b2">[3]</ref>.</p><p>DeepSpeed can efficiently train models with 100-200 billion parameters up to 10 times faster than the state-of-the-art via the use of a memory optimization system that we called ZeRO (Zero Redundancy Optimizer) <ref type="bibr" target="#b3">[4]</ref>. ZeRO is a parallelized optimizer that greatly reduces the resources needed for model and data parallelism while massively increasing the number of parameters that can be trained. Researchers used these breakthroughs to create the Turing Natural Language Generation (Turing-NLG), one of the largest publicly known language models at 17 billion parameters.</p><p>DeepSpeed recently obtained the fastest BERT training record of 44 minutes on 1024 NVIDIA V100 GPUs <ref type="bibr" target="#b4">[5]</ref>. This is a 34% improvement over the best published result of 67 minutes <ref type="bibr" target="#b5">[6]</ref> in end-to-end training time to achieve the same accuracy on the same number and generation of GPUs. This improvement does not come at the cost of excessive hardware resources but comes from a result of improved software efficiency. For example, DeepSpeed can attain 64 teraflops of single GPU performance on a NVIDIA V100 GPU, which is over 50% of the hardware peak. Yuxiong He, Ph.D.: Yuxiong He is a research manager at Microsoft. She works on performance optimization of parallel and distributed systems such as machine learning systems, information retrieval systems, data management systems, and large-scale cloud infrastructure. Her work was selected among the best papers for SIGIR, ICDE, WSDM and Middleware. Her research results have been utilized by various Microsoft systems and products, such as Bing, Ads, AzureML and AzureSQL, boosting system performance and capacity. Her recent work focuses on optimizing deep learning systems. She leads DeepCPU and DeepSpeed projects, which strive for order(s)-of-magnitude improvement on speed and scale for DL inference and training. Yuxiong received her PhD in Computer Science from Singapore-MIT Alliance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">TUTORS AND BIOGRAPHIES</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Jeff Rasley, Ph.D.: Jeff Rasley is a researcher at Microsoft with a background in distributed systems and networking. His research is focused broadly on improving the performance and usability of deep learning training. Jeff's research has been used in multiple Microsoft systems and projects such as Bing, Ads, AzureML, and HyperDrive. He is one of the founding developers of the Deep-Speed training library that has resulted in order(s)-of-magnitude speed/scale improvements for DL training. Jeff received his PhD in Computer Science from Brown University. Samyam Rajbhandari, Ph.D.: Samyam Rajbhandari is a researcher at Microsoft with a background in High Performance Computing. He works on developing high performance infrastructures for accelerating large scale deep learning training and inference on parallel and distributed systems. His research results are used in multiple Microsoft systems and products, such as Bing, Ads, AzureML to improve performance and capacity. He developed the core technology in DeepCPU and DeepSpeed resulting in order(s)of-magnitude improvement on speed and scale for DL inference and training. His recent work on memory optimization has enabled training of very large models including the 17.2B Turing-NLG model from Microsoft. Samyam received his PhD in Computer Science from Ohio State University. Olatunji Ruwase, Ph.D.: Olatunji Ruwase is a researcher at Microsoft with a systems background spanning compilers, operating systems, and hardware accelerators. His research results on training, inference, and hyperparameter search for deep learning models are used in multiple Microsoft systems and products, such as Bing, Ads, HyperDrive, and Catapault. He is recently working on systems support and convergence techniques for accelerating large-scale training of deep learning models on distributed GPUs. Tunji received his PhD in Computer Science from Carnegie Mellon University.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alch√©-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Paulius</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonah</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksii</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganesh</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Mixed precision training</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<ptr target="http://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf" />
		<title level="m">NVIDIA Tesla V100 GPU architecture</title>
				<imprint>
			<date type="published" when="2017-04">2017. April-2020</date>
			<biblScope unit="volume">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><surname>Zero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.02054</idno>
		<title level="m">Memory Optimizations Toward Training Trillion Parameter Models</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Microsoft DeepSpeed achieves the fastest BERT training time</title>
		<ptr target="https://www.deepspeed.ai/news/2020/05/27/fastest-bert-training.html" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">NVIDIA Clocks World&apos;s Fastest BERT Training Time</title>
		<author>
			<persName><forename type="first">Shar</forename><surname>Narasimhan</surname></persName>
		</author>
		<ptr target="https://devblogs.nvidia.com/training-bert-with-gpus/" />
		<imprint>
			<date type="published" when="2019-09">2019. September-2019</date>
			<biblScope unit="volume">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
