<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The latest research progress on spectral clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hongjie</forename><surname>Jia</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Shifei</forename><surname>Ding</surname></persName>
							<email>dingsf@cumt.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Xinzheng</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ru</forename><surname>Nie</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Á</forename><forename type="middle">S</forename><surname>Ding</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Á</forename><forename type="middle">X</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Á</forename><forename type="middle">R</forename><surname>Nie</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">University of Mining and Technology</orgName>
								<address>
									<postCode>221116</postCode>
									<settlement>Xuzhou</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Chinese Academy of Science</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Neural Comput &amp; Applic</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">The latest research progress on spectral clustering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">72029B3258A117F89CEFBF1E127E1FC4</idno>
					<idno type="DOI">10.1007/s00521-013-1439-2</idno>
					<note type="submission">Received: 3 January 2013 / Accepted: 23 May 2013 Ó Springer-Verlag London 2013</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Spectral clustering</term>
					<term>Graph theory</term>
					<term>Graph cut</term>
					<term>Laplacian matrix</term>
					<term>Eigen-decomposition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Spectral clustering is a clustering method based on algebraic graph theory. It has aroused extensive attention of academia in recent years, due to its solid theoretical foundation, as well as the good performance of clustering. This paper introduces the basic concepts of graph theory and reviews main matrix representations of the graph, then compares the objective functions of typical graph cut methods and explores the nature of spectral clustering algorithm. We also summarize the latest research achievements of spectral clustering and discuss several key issues in spectral clustering, such as how to construct similarity matrix and Laplacian matrix, how to select eigenvectors, how to determine cluster number, and the applications of spectral clustering. At last, we propose several valuable research directions in light of the deficiencies of spectral clustering algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Clustering is an important research field in data mining. The purpose of clustering is to divide a dataset into natural groups so that data points in the same group are similar while data points in different groups are dissimilar to each other <ref type="bibr" target="#b55">[56]</ref>. Traditional clustering methods, such as k-means algorithm <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b40">41]</ref>, FCM algorithm <ref type="bibr" target="#b20">[21]</ref>, and EM algorithm <ref type="bibr" target="#b12">[13]</ref>, are simple, but lack the ability to handle complex data structures. When the sample space is non-convex, these algorithms are easy to fall into local optimum <ref type="bibr" target="#b28">[29]</ref>.</p><p>In recent years, spectral clustering has aroused more and more attention of academia, due to its good clustering performance and solid theoretical foundation <ref type="bibr" target="#b46">[47]</ref>. Spectral clustering does not make any assumptions on the global structure of the data. It can converge to global optimum and performs well for the sample space of arbitrary shape, especially suitable for non-convex dataset <ref type="bibr" target="#b15">[16]</ref>. The idea of spectral clustering is based on spectral graph theory. It treats data clustering problem as a graph partitioning problem and constructs an undirected weighted graph with each point in the dataset being a vertex and the similarity value between any two points being the weight of the edge connecting the two vertices <ref type="bibr" target="#b7">[8]</ref>. Then, we can decompose the graph into connected components by certain graph cut method and call those components as clusters.</p><p>There are a variety of traditional graph cut methods, such as minimum cut, ratio cut, normalized cut and min/ max cut. The optimal clustering results can be obtained by minimizing or maximizing the objective function of the graph cut methods. However, for various graph cut methods, seeking the optimal solution of the objective function is often NP-hard. With the help of spectral method, the original problem can be solved in polynomial time by relaxing the original discrete optimization problem to the real domain <ref type="bibr" target="#b16">[17]</ref>. In graph partitioning, a point can be considered part belonging to subset A and part belonging to subset B, rather than strictly belonging to one cluster. It can be proved that the classification information of vertices is contained in the eigenvalues and eigenvectors of graph Laplacian matrix. And we can get good clustering results, if we make full use of the classification information during the clustering process <ref type="bibr" target="#b34">[35]</ref>. Spectral clustering can get the relaxation solution of graph cut objective function, which is an approximate optimal solution.</p><p>The earliest study on spectral clustering began in 1973. Donath and Hoffman put forward that graph partitioning can be built based on the eigenvectors of adjacency matrix <ref type="bibr" target="#b17">[18]</ref>. In the same year, Fiedler proved that the bipartition of a graph is closely related to the second eigenvector of Laplacian matrix <ref type="bibr" target="#b22">[23]</ref>. He suggested using this eigenvector to conduct graph partitioning. Hagen and Kahng found the relations among clustering, graph partitioning and the eigenvectors of similarity matrix, and constructed a practical algorithm first <ref type="bibr" target="#b24">[25]</ref>. They proposed ratio cut method in 1992. In 2000, Shi and Malik proposed normalized cut <ref type="bibr" target="#b54">[55]</ref>. This method not only considers the external connections between clusters, but also considers the internal connections within a cluster, so it can produce balanced clustering results. Then, Ding et al. <ref type="bibr" target="#b13">[14]</ref> proposed min/max cut; Ng et al. <ref type="bibr" target="#b50">[51]</ref> proposed classic NJW algorithm. These algorithms are based on matrix spectral theory to classify data points, so they are called spectral clustering. Since 2000, spectral clustering has gradually become a research hotspot of data mining. At present, spectral clustering has been successfully applied to many fields, such as computer vision <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b75">76]</ref>, integrated circuit design <ref type="bibr" target="#b1">[2]</ref>, load balancing <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b26">27]</ref>, biological information <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b51">52]</ref>, and text classification <ref type="bibr" target="#b68">[69]</ref>, etc. Spectral clustering algorithms provide a new idea to solve the problem of clustering and can effectively deal with many practical problems, so their research has great scientific value and application potential.</p><p>This paper is organized as follows: Sect. 2 introduces the basic concepts of algebraic graph theory, compares the objective functions of typical graph cut methods, and explores the nature of spectral clustering algorithm; Sect. 3 summarizes the latest research achievements of spectral clustering and discusses several key issues in spectral clustering, such as how to construct similarity matrix and Laplacian matrix, how to select eigenvectors, how to determine cluster number, and the applications of spectral clustering; finally, several valuable research directions are proposed, in light of the deficiencies of spectral clustering algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Basic concepts of spectral clustering</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Algebraic graph theory</head><p>Graph theory originated in the famous problem of Konigsberg Seven Bridges, which is an important branch of mathematics. It is the study of theories and methods about graphs. Algebraic graph theory is a cross-field combining graph theory, linear algebra, and matrix computation theory. As one of the branches of the graph theory, the research of algebraic graph theory began in the 1850s, aiming to use algebraic methods to study the graph, convert graph characteristics into algebraic characteristics, and then use the algebraic characteristics and algebraic methods to deduce the theorems about graphs <ref type="bibr" target="#b18">[19]</ref>. In fact, the main content of algebraic graph theory is spectrum. Here, spectrum represents the eigenvalues and their multiplicities of matrix. The earliest research on algebraic graph theory is made by Fiedler <ref type="bibr" target="#b22">[23]</ref>. He derived the algebraic criterion about the connectivity of graph. Whether a graph is connected or not can be judged by the second smallest eigenvalue of Laplacian matrix. Later, the eigenvector corresponding to the second smallest eigenvalue is named Fiedler vector, which contains the instruction information about dividing a graph into two parts.</p><p>Adjacency matrix (denoted as A) and Laplacian matrix (denoted as L) are commonly used representations for graph. The adjacency matrix of weighted graph uses real numbers to reflect the different relations between vertices. Laplacian matrix L = D -A, where D is a diagonal matrix, the diagonal values equal to the absolute row sums of A, and the non-diagonal elements are 0. Most spectral clustering algorithms are based on the spectrum of Laplacian matrix to split graphs. There are two kinds of Laplacian matrixes: un-normalized Laplacian matrix (L) and normalized Laplacian matrix. Normalized Laplacian matrix includes symmetric form (denoted as L s ) and random walk form (denoted as L r ). Their expressions are shown in Table <ref type="table" target="#tab_0">1</ref>.</p><p>Mohar introduced some characteristics of un-normalized Laplacian matrix <ref type="bibr" target="#b45">[46]</ref>. Shi and Malik studied the characteristics of normalized Laplacian matrix <ref type="bibr" target="#b54">[55]</ref>. The spectrum of Laplacian matrix provides very useful information for graph partitioning. Based on Fiedler vector, we can  <ref type="bibr" target="#b27">[28]</ref>. While, Luxburg et al. <ref type="bibr" target="#b39">[40]</ref> proved that from the view of statistical consistency theory, normalized Laplacian matrix is superior to unnormalized Laplacian matrix. Table <ref type="table" target="#tab_0">1</ref> also shows the expression of probability transition matrix (denoted as P). Probability transition matrix is essentially the normalized form of similarity matrix. Since the row sum of normalized similarity matrix is 1, the elements in P can be understood as the Markov transition probability. The larger the transition probability between two nodes, the possibility that they belong to the same cluster is greater. The spectrum of probability transition matrix also contains the necessary information to split a graph, but it is slightly different from the spectrum of Laplacian matrix. In probability transition matrix, the eigenvector corresponding to the second largest eigenvalue can indicate the bipartition of a graph, and multiple eigenvectors corresponding to the main eigenvalues can indicate the multi-partition of a graph <ref type="bibr" target="#b42">[43]</ref>.</p><p>Another novel matrix is modularity matrix (denoted as B), which comes from the study of community structure in complex networks <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b33">34]</ref>. It has a clear physical meaning, and its expression is shown in Table <ref type="table" target="#tab_0">1</ref>. In the expression, d represents the column vector, whose elements are the degree of nodes; m represents the total weight of graph edges; the elements of B is the difference between the actual edge number and the desired edge number of pairwise nodes, which also represents the extent that actual edge number beyond desired edge number. Therefore, this matrix leads directly to an objective function that the optimal partition should make the edges of communities (corresponding to ''clusters'') as dense as possible, preferably beyond expected. As for matrix characteristics, modularity matrix and Laplacian matrix have some similarities, such as row sum (column sum) is 0 and 0 is the eigenvalue. But they also have an obvious difference that modularity matrix is not a positive semi-definite matrix, so, some of its eigenvalues may be negative. As for graph partitioning, the bipartition of a network is based on the eigenvector of the largest eigenvalue, and the multi-partition of a network is based on multiple main eigenvectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph cut methods</head><p>Spectral clustering uses the similarity graph to deal with the problem of clustering. Its final purpose is to find a partition of the graph such that the edges between different groups have very low weights, which means that points in different clusters are dissimilar from each other; and the edges within a group have high weights, which means that points within the same cluster are similar to each other. The prototype of graph cut clustering is minimum spanning tree (MST) method <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b71">72]</ref>. MST clustering method is proposed by <ref type="bibr">Zahn [72]</ref>. This algorithm builds the minimum spanning tree by the adjacency matrix of graph, and then removes the edges with large weights from the minimum spanning tree to get a set of connected components. This method is successful in detecting clearly separated clusters, but if the density of nodes is changed, its performance will deteriorate. Another disadvantage is Zahn's research is under the circumstance that the cluster structure (such as separating cluster, contact cluster, density cluster, etc.) is known in advance.</p><p>Cutting a graph means to divide a graph into multiple connected components by removing certain edges, and the sum of weight of the removed edges is called cut. Bames first proposed minimum cut clustering criteria <ref type="bibr" target="#b5">[6]</ref>. Its basic idea is seeking the minimum cut while dividing a graph into k connected sub-graphs. Then Alpert and Yao put forward the spectral method to solve the minimum cut criteria, which laid an important foundation for the later development of spectral clustering <ref type="bibr" target="#b2">[3]</ref>. Wu and Leahy applied minimum cut to image segmentation and based on the maximum network flow theory to calculate the minimum cut <ref type="bibr" target="#b65">[66]</ref>. Minimum cut clustering is successful in some applications of image segmentation, but the biggest problem is it may lead to serious uneven split, such as ''solitary point'' or ''small cluster''. In order to solve this problem, Wei and Cheng proposed ratio cut <ref type="bibr" target="#b64">[65]</ref>, Sarkar and Soundararajan proposed average cut <ref type="bibr" target="#b53">[54]</ref>, Shi and Malik proposed normalized cut <ref type="bibr" target="#b54">[55]</ref>, Ding et al. <ref type="bibr" target="#b13">[14]</ref> proposed Min/Max cut. The expressions of their objective functions are shown in Table <ref type="table" target="#tab_1">2</ref>. These optimization objectives are able to produce more balanced split. Take graph bipartition for example. Assume V is a given set of data points. A represents a subset of V, and B represents V/A. For illustrative purposes, we define the following four terms:</p><p>1. CutðA; BÞ ¼ P i2A;j2B w ij denotes the sum of connection weights between cluster A and B. 2. CutðA; AÞ ¼ P i2A;j2A w ij denotes the sum of connection weights within cluster A. 3. VolðAÞ ¼ P i2A d i denotes the total degrees of the vertices in cluster A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>A j j ¼ n A denotes the number of vertices in cluster A, which is used to describe the size of cluster A.</p><p>The study of complex networks has caused great attention in the past decade. Complex networks possess a rich, multi-scale structure reflecting the dynamical and functional organization of the systems they model <ref type="bibr" target="#b43">[44]</ref>. Newman systematically studied the spectral algorithm for community structure in non-weighted network, weighted network, as well as directed network <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b49">50]</ref>. He uses modularity function to detect communities. Modularity criterion is a novel idea. Take non-weighted graph for example: only if the real proportion of edges in communities is greater than the ''expected'' proportion, the split is considered to be reasonable. The ''expected'' edge number is derived from a random graph model, which is based on the configuration model. This is obviously different from the starting point of traditional graph cut clustering methods. The modularity function is shown in Table <ref type="table" target="#tab_1">2</ref>, where Q represents modularity, m represents the number of edges contained in the graph, k i represents the degree of node i (k j is similar), v i and v j is -1 or 1. v i = v j indicate node i and j belong to different communities; v i = v j indicate node i and j belong to the same community.</p><p>Minimizing ratio cut, average cut, normalized cut, and maximizing modularity are all NP discrete optimization problems. Fortunately, spectral method can provide a loose solution within polynomial time for these optimization problems. Here, ''loose'' means relax the discrete optimization problem to the real number field, and then using some heuristic approach to re-convert it to a discrete solution. The essence of graph partitioning can be summarized as the minimization or maximization problem of matrix trace. To complete the minimizing or maximizing tasks need to rely on spectral clustering algorithm.</p><p>Usually most spectral clustering algorithms are formed of the following three stages: preprocessing, spectral representation and clustering <ref type="bibr" target="#b25">[26]</ref>. First construct the graph and similarity matrix to represent the dataset; then form the associated Laplacian matrix, compute eigenvalues and eigenvectors of the Laplacian matrix, and map each point to a lower-dimensional representation, based on one or more eigenvectors; at last, assign points to two or more classes, based on the new representation. In order to partition a dataset or graph into k (k [ 2) clusters, there are two basic approaches: recursive 2-way partitioning and kway partitioning. The comparison of these two methods is shown in Table <ref type="table" target="#tab_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The latest development of spectral clustering</head><p>Spectral clustering is a large family of grouping methods, and its research is very active in machine learning and data mining, because of the universality, efficiency, and theoretical support of spectral analysis. Next, we will discuss the latest development of spectral clustering from the following aspects: ''construct similarity matrix'', ''form Laplacian matrix'', ''select eigenvectors'', ''the number of clusters'' and ''the applications of spectral clustering''.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Construct similarity matrix</head><p>The key to spectral clustering is to select a good distance measurement, which can well describe the intrinsic structure of data points. Data in the same groups should have similarity and follow space consistency. Similarity measurement is crucial to the performance of spectral clustering <ref type="bibr" target="#b61">[62]</ref>. The Gaussian kernel function is usually adopted as the similarity measure. However, with a fixed scaling parameter r, the similarity between two data points is only determined by their Euclidean distance and is not adaptive to their surroundings. While dealing with complex dataset, the similarity simply based on Euclidean distance cannot reflect the data distribution accurately and in turn resulting in the poor performance of spectral clustering. Zhang et al. <ref type="bibr" target="#b73">[74]</ref> propose a local density adaptive similarity measure-CNN (Common-Near-Neighbor), which uses the local density between two data points to scale the Gaussian kernel function. CNN method is based on the following observation: if two points are distributed in the same cluster, they are in the same region which has a relatively high density. It has an effect of amplifying intracluster similarity thus making the affinity matrix clearly block diagonal. Experimental results show that the spectral clustering algorithm with local density adaptive similarity measure outperforms the traditional spectral clustering algorithm, the path-based spectral clustering algorithm and the self-tuning spectral clustering algorithm.</p><p>Yang et al. <ref type="bibr" target="#b69">[70]</ref> develop a density sensitive distance measure. This measure defines the adjustable line segment length, which can adjust the distance in regions with different density. It squeezes the distances in high density regions while widen them in low density regions. By the distance measure, they design a new similarity function for spectral clustering. Compared with the spectral clustering based on conventional Euclidean distance or Gaussian kernel function, the proposed algorithm with density sensitive similarity measure can obtain desirable clusters with high performance on both synthetic and real life datasets.</p><p>Wang et al. <ref type="bibr" target="#b63">[64]</ref> present spectral multi-manifold clustering (SMMC), based on the analysis that spectral clustering is able to work well when the similarity values of the points belonging to different clusters are relatively low. In their model, the data are assumed to lie on or close to multiple smooth low-dimensional manifolds, where some data manifolds are separated but some are intersecting. Then, local geometric information of the sampled data is incorporated to construct a suitable similarity matrix. Finally, spectral method is applied to this similarity matrix to group the data. SMMC achieves good performance over a broad range of parameter settings and is able to handle intersections, but its robustness remains to be improved.</p><p>In order to better describe the data distribution, Zhang and You propose a random walk based approach to process the Gaussian kernel similarity matrix <ref type="bibr" target="#b74">[75]</ref>. In this method, the pairwise similarity between two data points is not only related to the two points, but also related to their neighbors. Li and Guo develop a new affinity matrix generation method using neighbor relation propagation principle <ref type="bibr" target="#b35">[36]</ref>. The affinity matrix generated can increase the similarity of point pairs that should be in same cluster and can well detect the structure of data. Blekas and Lagaris introduce Newtonian spectral clustering based on Newton's equations of motion <ref type="bibr" target="#b6">[7]</ref>. They build an underlying interaction model for trajectory analysis and employ Newtonian preprocessing to gain valuable affinity information, which can be used to enrich the affinity matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Form Laplacian matrix</head><p>After the similarity matrix is constructed, the next step is forming the corresponding Laplacian matrix according to different graph cut methods. There are three forms of traditional Laplacian matrix, which are, respectively, suitable for different clustering conditions. The selection of graph cut methods and the establishment of Laplacian matrix both have an impact on the performance of spectral clustering algorithms. As a natural nonlinear generalization of graph Laplacian, p-Laplacian has recently been applied to two-class cases. Luo et al. <ref type="bibr" target="#b38">[39]</ref> propose full eigenvector analysis of p-Laplacian and obtain a natural global embedding for multi-class clustering problems, instead of using greedy search strategy implemented by previous researchers. An efficient gradient descend optimization approach is introduced to obtain the p-Laplacian embedding space, which is guaranteed to converge to feasible local solutions. Empirical results suggest that the greedy search method often fails in many real-world applications with non-trivial data structures, but this approach consistently gets robust clustering results and preserves the local smooth manifold structures of real-world data in the embedding space.</p><p>Yang et al. <ref type="bibr" target="#b70">[71]</ref> propose a new image clustering algorithm, referred to as clustering using local discriminant models and global integration (LDMGI). A new Laplacian matrix is learnt in LDMGI by exploiting both manifold structure and local discriminant information. This algorithm constructs a local clique for each data point sampled from a nonlinear manifold, and uses a local discriminant model to evaluate the clustering performance of samples within the local clique. Then a unified objective function is proposed to globally integrate the local models of all the local cliques. Compared with Normalized cut, LDMGI is more robust to algorithmic parameter and is more appealing for the real image clustering applications, in which the algorithmic parameters are generally not available for tuning.</p><p>Most graph Laplacians are base on the Euclidean distance, which does not necessarily reflect the inherent distribution of the data. So Xie et al. <ref type="bibr" target="#b67">[68]</ref> propose a method to directly optimize the normalized graph Laplacian by using pairwise constraints. The learned graph is consistent with equivalence and non-equivalence pairwise relationships, which can better represent the similarity between samples. Meanwhile, this approach automatically determines the scaling parameter during the optimization. The learned normalized Laplacian matrix can be directly applied in spectral clustering and semi-supervised learning algorithms.</p><p>Frederix and Van Barel use linear algebra techniques to solve the eigenvalue problem of a graph Laplacian and propose a novel sparse spectral clustering method <ref type="bibr" target="#b23">[24]</ref>. This method exploits the structure of the Laplacian to construct an approximation, not in terms of a low-rank approximation but in terms of capturing the structure of the matrix. With this approximation, the size of the eigenvalue problem can be reduced. Chen and Feng present a novel k-way spectral clustering algorithm called discriminant cut (Dcut) <ref type="bibr" target="#b9">[10]</ref>. It normalizes the similarity matrix with the corresponding regularized Laplacian matrix. Dcut can reveal the internal relationships among data and produce exciting clustering results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Select eigenvectors</head><p>The eigenvalues and eigenvectors of Laplacian matrix can be obtained by eigen-decomposition. An analysis of the characteristics of eigenspace is carried out which shows that: (a) not every eigenvector of a Laplacian matrix is informative and relevant for clustering; (b) eigenvector selection is critical because using uninformative/irrelevant eigenvectors could lead to poor clustering results; (c) the corresponding eigenvalues cannot be used to select relevant eigenvectors given a realistic dataset. NJW algorithm partitions data using the largest k eigenvectors of the normalized Laplacian matrix derived from the dataset. However, some experiments demonstrate that the top k eigenvectors cannot always detect the structure of the data for real pattern recognition problems. So it is necessary to find a better way to select eigenvectors for spectral clustering.</p><p>Xiang and Gong propose the concept of ''eigenvector relevance'', which differs from previous approaches in that only informative/relevant eigenvectors are employed for determining the number of clusters and performing clustering <ref type="bibr" target="#b66">[67]</ref>. The key element of their algorithm is a simple but effective relevance learning method, which measures the relevance of an eigenvector according to how well it can separate the dataset into different clusters. Experimental results show that this algorithm is able to estimate the cluster number correctly and reveal natural grouping of the input data/patterns even given sparse and noisy data.</p><p>Zhao et al. <ref type="bibr" target="#b76">[77]</ref> propose an eigenvector selection method based on entropy ranking for spectral clustering (ESBER). In this method, first all the eigenvectors are ranked according to their importance on clustering, and then a suitable eigenvector combination is obtained from the ranking list. There are two strategies to select eigenvectors in the ranking list of eigenvectors: one is directly adopting the first k eigenvectors in the ranking list, which are the most important eigenvectors among all the eigenvectors, different to the largest k eigenvectors of NJW method; the other strategy is to search a suitable eigenvector combination among the first k m (k m [ k) eigenvectors in the ranking list, which can reflect the structure of the original data. ESBER method is more robust than NJW method and can obtain satisfying clustering results in most cases.</p><p>Rebagliati and Verri find a fundamental working hypothesis of NJW algorithm, that the optimal partition of k clusters can be obtained from the largest k eigenvectors of matrix L sym , only if the gap between the k-th and the k ? 1-th eigenvalue of L sym is sufficiently large. If gap is small, a perturbation may swap the corresponding eigenvectors and the results can be very different from the optimal ones. So they suggest a weaker working hypothesis: the optimal partition of k clusters can be obtained from a k-dimensional subspace of the first m (m [ k) eigenvectors, where m is a parameter chosen by the user <ref type="bibr" target="#b52">[53]</ref>. The bound of m is based upon the gap between the m ? 1-th and the k-th eigenvalue and ensures stability of the solution. This algorithm is robust to small changes of the eigenvalues, and gives satisfying results on real-world graphs by selecting correct k-dimensional subspaces of the linear span of the first m eigenvectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">The number of clusters</head><p>In most spectral clustering algorithms, the number of clusters must be manually set and it is very sensitive to initialization. How to accurately estimate the number of clusters is one of the major challenges faced by spectral clustering. Some existing approaches attempt to get the optimal cluster number by minimizing some distancebased dissimilarity measure within clusters.</p><p>Wang proposes a novel selection criterion, whose key idea is to select the number of clusters as the one maximizing the clustering stability <ref type="bibr" target="#b60">[61]</ref>. Since maximizing the clustering stability is equivalent to minimizing the clustering instability, they develop an estimation scheme for the clustering instability based on modified cross-validation. The idea of this scheme is to split the data into two training datasets and one validation dataset, where the two training datasets are used to construct two clustering and the validation dataset is used to measure the clustering instability. This selection criterion is applicable to all kinds of clustering algorithms, including distance-based or non-distance-based algorithms. However, the data splitting reduces the sizes of training datasets, so the effectiveness of the cross-validation method is remained to be further studied.</p><p>The concept of clustering stability can measure the robustness of any given clustering algorithm. Inspired by Wang's method <ref type="bibr" target="#b60">[61]</ref>, Fang and Wang develop a new estimation scheme for clustering instability based on the bootstrap, in which the number of clusters is selected so that the corresponding estimated clustering instability is minimized <ref type="bibr" target="#b21">[22]</ref>. The implementation of the bootstrap method is straightforward, and it has a number of advantages. First, the bootstrap samples are of the same size as that of the original data, so the bootstrap method is more efficient. Second, the bootstrap estimate of the clustering instability is the nonparametric maximum likelihood estimate (MLE). Third, the bootstrap method can provide the instability path of a clustering algorithm for any given number of clusters.</p><p>Tepper et al. <ref type="bibr" target="#b56">[57]</ref> introduce a perceptually driven clustering method, in which the number of clusters is automatically determined by setting parameter e that controls the average number of false detections. The detection thresholds are well adapted to accept/reject non-clustered data and can help find the right number of clusters. This method only takes into account inter-point distances and has no random steps. Besides, it is independent from the original data dimensionality, which means that its running time is not affected by an increase in dimensionality. The combination of this method with normalized cuts performs well on both synthetic and real-world datasets and the detected clusters are perceptually significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">The applications of spectral clustering</head><p>Nowadays, spectral clustering has been successfully applied to many areas, such as data analysis, speech separation, video indexing, character recognition, image processing, etc. In such applications, the number of data points to cluster can be enormous. Indeed, a small 256 9 256 gray level image leads to a dataset of 65,536 points, while 4 s of speech sampled at 5 kH leads to more than 20,000 spectrogram samples. In addition, real-world datasets often contain a lot of outliers and noise points with complex data structures <ref type="bibr" target="#b37">[38]</ref>. All these issues should be carefully considered when dealing with the specific clustering problems.</p><p>Bach and Jordan use spectral clustering to solve the problem of speech separation and present a blind separation algorithm, which separates speech mixtures from a single microphone without requiring models of specific speakers <ref type="bibr" target="#b4">[5]</ref>. It works within a time-frequency representation (a spectrogram), and exploits the sparsity of speech signals in this representation. That is, although two speakers might speak simultaneously, there is relatively little overlap in the time-frequency plane if the speakers are different. Thus, speech separation can be formulated as a problem in segmentation in the time-frequency plane. Bach et al. have successfully demixed speech signals from two speakers using this approach.</p><p>Video indexing requires the efficient segmentation of video into scenes. A scene can be regarded as a series of semantically correlated shots. The visual content of each shot can be represented by one or multiple frames, called key-frames. Chasanis et al. <ref type="bibr" target="#b8">[9]</ref> develop a new approach for video scene segmentation. To cluster the shots into groups, they propose an improved spectral clustering method that both estimates the number of clusters and employs the fast global k-means algorithm in the clustering stage after the eigenvector computation of the similarity matrix. The shot similarity is computed based only on visual features and a label is assigned to each shot according to the group that it belongs to. Then, a sequence alignment algorithm is applied Neural Comput &amp; Applic to detect when the pattern of shot labels changes, providing the scene segmentation result. This scene detection method can efficiently summarize the content of each shot and detect most of the scene boundaries accurately, while preserving a good tradeoff between recall and precision.</p><p>Zeng et al. <ref type="bibr" target="#b72">[73]</ref> apply spectral clustering to recognize handwritten numerals and obtain satisfying results. They first select the Zernike moment features of handwritten numerals based on the principles that the distinction degree of inside-cluster features is small and the dividing of the features between clusters is huge; then construct the similarity matrix between handwritten numerals by the similarity measure based on Grey relational analysis and make transitivity transformation to the similarity matrix for better block symmetry after reformation; finally make spectrum decomposition to the Laplacian matrix derived from the reformed similarity matrix, and recognize the handwritten numerals with the eigenvectors corresponding to the second minimal eigenvalue of Laplacian matrix as the spectral features. This algorithm is robust to outliers and its recognition is also very effective.</p><p>Spectral clustering has been broadly used in image segmentation. Liu et al. <ref type="bibr" target="#b36">[37]</ref> take into account the spatial information of the pixels in image and propose a novel non-local spatial spectral clustering algorithm, which is robust to noise and other imaging artifacts. Nowadays, High-Definition (HD) images are widely used in television broadcasting and movies. Segmenting these high resolution images presents a grand challenge because of significant computational demands. Wang and Dong develop a multilevel low-rank approximation-based spectral clustering method, which can effectively segment high resolution images <ref type="bibr" target="#b62">[63]</ref>. They also develop a fast sampling strategy to select sufficient data samples, leading to accurate approximation and segmentation. In order to deal with large images, Tung et al. <ref type="bibr" target="#b57">[58]</ref> propose an enabling scalable spectral clustering algorithm, which combines a blockwise segmentation strategy with stochastic ensemble consensus. The purpose of using stochastic ensemble consensus is to integrate both global and local image characteristics in determining the pixel classifications.</p><p>Ding et al. <ref type="bibr" target="#b14">[15]</ref> focus on controlled islanding problem and use spectral clustering to find a suitable islanding solution for preventing the initiation of wide area blackouts. The objective function used in this controlled islanding algorithm is the minimal power-flow disruption. It is demonstrated that this algorithm is computationally efficient when solving the controlled islanding problem, particularly in the case of a large power system. Adefioye et al. <ref type="bibr" target="#b0">[1]</ref> develop a multi-view spectral clustering algorithm for chemical compound clustering. The tensor-based spectral methods provide chemically appropriate and statistically significant results when attempting to cluster compounds from multiple data sources.</p><p>Experiments show that compounds of extremely different chemotypes clustering together, which can help reveal the internal relations of these compounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and prospect</head><p>Spectral clustering is an elegant and powerful approach for clustering, which has been widely used in many fields. Especially in the graph and network areas, a lot of personalized improved algorithms have emerged. Why spectral clustering attracts so many researchers, there are three most important reasons: firstly, it has a solid theoretical foundation-algebraic graph theory; secondly, for complex cluster structure, it can get a global loose solution; thirdly, it can solve the problem within a polynomial time. However, as a novel clustering method, spectral clustering is still in the development stage, and there are many problems worthy of further study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Semi-supervised spectral clustering</head><p>Traditional spectral clustering is an unsupervised learning that does not take into account the clustering intention of the user. User intention is actually a priori knowledge, also known as the supervised information. The clustering algorithm guided by supervised information is called semi-supervised clustering. Limited priori knowledge can be easily obtained from samples in practice, such as the pairwise constraints of samples. A large number of studies have shown that making full use of priori knowledge in the process of searching clusters can significantly improve the performance of the clustering algorithm <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b31">32]</ref>. Therefore, it will be very meaningful to combine priori knowledge with spectral clustering and carry out the research of semi-supervised spectral clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Fuzzy spectral clustering</head><p>Most of the existing spectral clustering algorithms are hard partition methods. They strictly divide each object into a class and the class of an object is either this or that, which belongs to the scope of classical set theory. In fact, most objects do not have strict properties. These objects are intermediary in forms and classes, suitable for soft division <ref type="bibr" target="#b44">[45]</ref>. The classical set theory often cannot completely solve the classification problems of ambiguity. Fuzzy set theory proposed by Zadeh, provides a powerful tool for this soft division. Applying fuzzy set theory to spectral clustering and studying effective fuzzy spectral clustering algorithms are also very significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Kernel spectral clustering</head><p>Spectral clustering algorithms are mostly based on some similarity measure to classify the samples, so that similar samples are gathered into the same cluster, while dissimilar samples are into different clusters. Thus, the clustering process is mainly dependent on the characteristic difference between the samples. If the distribution of samples is very complex, conventional methods may not be able to get ideal clustering results. Using kernel methods can enlarge the useful features of samples by mapping them to a high dimensional feature space, so that the samples are easier to clustering, and the convergence speed of the algorithm will be accelerated <ref type="bibr" target="#b3">[4]</ref>. Therefore, when dealing with complex clustering problems, the combination of kernel methods and spectral techniques can be considered as a valuable research direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Clustering of large datasets</head><p>Spectral clustering algorithms involve the calculation of eigenvalues and eigenvectors. The underlying eigendecomposition takes cubic time and quadratic space with regard to the dataset size <ref type="bibr" target="#b11">[12]</ref>. These can be reduced by the Nystrom method which samples only a subset of columns from the matrix. However, the manipulation and storage of these sampled columns can still be expensive when the dataset is large. Time and space complexity has become an obstacle for the generalization of spectral clustering in practical applications. So, it is worthy of deep study to explore an effective method to reduce the computation complexity of spectral clustering algorithms, and make them suitable for massive learning problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Spectral clustering ensemble</head><p>Traditional spectral clustering algorithms are sensitive to the scaling parameter and have inherent randomness. In order to overcome these problems, clustering ensemble strategy can be introduced to spectral clustering <ref type="bibr" target="#b30">[31]</ref>. Because it is possible to get better clusters by searching the combination of multiple clustering results. Clustering ensemble can make full use of the results of learning algorithms under different conditions and find the cluster combinations that cannot be obtained by a single clustering algorithm. Thus, the spectral clustering ensemble algorithm is able to improve the quality and stability of the clustering results, with strong robustness to noise, outliers, and sample changes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Graph matrix for spectral clustering</figDesc><table><row><cell>Graph</cell><cell cols="2">Expression Bipartition of a graph Multi-partition</cell></row><row><cell>matrix</cell><cell></cell><cell>of a graph</cell></row><row><cell>Laplacian</cell><cell cols="2">L = D -A Based on fielder vector Based on</cell></row><row><cell>matrix</cell><cell>L r = D -1 ÁL L s = D -1/2 Á LD -1/2</cell><cell>multiple main eigenvectors</cell></row><row><cell>Probability</cell><cell cols="2">P = D -1 ÁA Based on the</cell></row><row><cell>transition</cell><cell></cell><cell>eigenvector of the</cell></row><row><cell>matrix</cell><cell></cell><cell>second largest</cell></row><row><cell></cell><cell></cell><cell>eigenvalue</cell></row><row><cell>Modularity matrix</cell><cell>B ¼ A À dd T 2m</cell><cell>Based on the eigenvector of the</cell></row><row><cell></cell><cell></cell><cell>largest eigenvalue</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Comparison of graph cut methods</figDesc><table><row><cell>References</cell><cell>Graph cut</cell><cell cols="2">Objective function</cell><cell></cell><cell></cell><cell>Advantage</cell><cell>Disadvantage</cell></row><row><cell>Bames [6]</cell><cell>Minimum</cell><cell cols="3">M cut (A, B) = Cut (A, B)</cell><cell></cell><cell>The algorithm is simple and easy</cell><cell>Does not consider the cluster size;</cell></row><row><cell></cell><cell>cut</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>to implement</cell><cell>may lead to serious uneven split</cell></row><row><cell>Wei and Cheng [65]</cell><cell>Ratio cut</cell><cell cols="2">R cut ðA; BÞ ¼ CutðA;BÞ A j jÂ B j j</cell><cell></cell><cell></cell><cell>Introduces the size of clusters, which reduces the possibility of</cell><cell>Only focus on the reducing the similarity between clusters</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>over-split</cell><cell></cell></row><row><cell>Sarkar and Soundararajan</cell><cell cols="3">Average cut A cut ðA; BÞ ¼ CutðA;BÞ A j j</cell><cell cols="2">þ CutðA;BÞ B j j</cell><cell>Can produce more accurate classification</cell><cell>Only take into account the connections between clusters, while</cell></row><row><cell>[54]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ignore the connections within a</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>cluster</cell></row><row><cell>Shi and Malik [55]</cell><cell>Normalized cut</cell><cell cols="4">N cut ðA; BÞ ¼ CutðA;BÞ Vol(AÞ þ CutðA;BÞ Vol(BÞ</cell><cell>Take into account both inter-cluster connections and intra-</cell><cell>The algorithm efficiency is low and unable to deal with the clustering</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>cluster connections</cell><cell>problem of big data</cell></row><row><cell cols="2">Ding et al. [14] Min/Max cut</cell><cell cols="4">M mcut ðA; BÞ ¼ CutðA;BÞ Cut(A;AÞ þ CutðA;BÞ Cut(B;BÞ</cell><cell>Tend to produce balanced clusters and can avoid the</cell><cell>The algorithm complexity is relatively high with slow running</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>clusters containing only a few</cell><cell>speed</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>vertices</cell><cell></cell></row><row><cell>Newman [50]</cell><cell>Modularity</cell><cell>Q ¼ 1 2m</cell><cell>P i;j A i;j À</cell><cell>kikj 2m</cell><cell>vivjþ1 2</cell><cell>Suitable for the partition of complex networks, and can</cell><cell>Does not perform well when there is serious overlap between clusters</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>efficiently find communities</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Comparison of recursive 2-way partitioning and k-way partitioning</figDesc><table><row><cell cols="2">Neural Comput &amp; Applic</cell><cell></cell><cell></cell></row><row><cell>Partition</cell><cell>Basic idea</cell><cell>Advantage</cell><cell>Disadvantage</cell></row><row><cell>method</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Recursive</cell><cell>Divide the graph into two parts by a</cell><cell>The idea of this algorithm is simple and</cell><cell>This method is unstable, and costs a large</cell></row><row><cell>2-way</cell><cell>certain 2-way partitioning algorithm,</cell><cell>easy to realize by programming</cell><cell>computation with low efficiency; it</cell></row><row><cell>partitioning</cell><cell>and then recursively apply the same</cell><cell></cell><cell>only utilizes the information of single</cell></row><row><cell></cell><cell>procedure to the sub-graphs in a</cell><cell></cell><cell>eigenvector (such as Fiedler vector)</cell></row><row><cell></cell><cell>hierarchical way, until the number of</cell><cell></cell><cell></cell></row><row><cell></cell><cell>clusters is enough or the recursive</cell><cell></cell><cell></cell></row><row><cell></cell><cell>conditions are violated</cell><cell></cell><cell></cell></row><row><cell>k-way</cell><cell>First select several main eigenvectors of</cell><cell>Make full use of the information of</cell><cell>The optimization of its objective</cell></row><row><cell>partitioning</cell><cell>Laplacian matrix that contain</cell><cell>multiple eigenvectors; it has less</cell><cell>function is usually more difficult; not</cell></row><row><cell></cell><cell>classification information in a heuristic</cell><cell>computational complexity and the</cell><cell>easy to select the appropriate</cell></row><row><cell></cell><cell>way, and then use these eigenvectors to</cell><cell>clustering results are quite satisfactory</cell><cell>eigenvectors</cell></row><row><cell></cell><cell>map the original data points to a</cell><cell></cell><cell></cell></row><row><cell></cell><cell>spectral space to conduct the clustering</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Neural Comput &amp; Applic</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments This work is supported by the National Key Basic Research Program of China (No.2013CB329502), and the Fundamental Research Funds for the Central Universities (No.2013XK10).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-view spectral clustering and its chemical application</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Adefioye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Moor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int J Comput Biol Drug Des</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="32" to="49" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-way partitioning via geometric embeddings, orderings and dynamic programming</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Alpert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Kahng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Comput-Aaid Des Integr Circuits Syst</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1342" to="1358" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spectral partitioning: the more eigenvectors, the better</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Alpert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd annual ACM/IEEE design automation conference</title>
		<meeting>the 32nd annual ACM/IEEE design automation conference<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="195" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hierarchical kernel spectral clustering</title>
		<author>
			<persName><forename type="first">C</forename><surname>Alzate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jak</forename><surname>Suykens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="21" to="30" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning spectral clustering, with application to speech separation</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Mach Learn Res</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1963" to="2001" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An algorithm for partitioning the nodes of a graph</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Bames</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J Algebraic Discrete Methods</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="541" to="550" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A spectral clustering approach based on Newton&apos;s equations of motion</title>
		<author>
			<persName><forename type="first">K</forename><surname>Blekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">E</forename><surname>Lagaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int J Intell Syst</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="394" to="410" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Survey on spectral clustering algorithms</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Sci</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="14" to="18" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scene detection in videos using shot clustering and sequence alignment</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">T</forename><surname>Chasanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Likas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Galatsanos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Multimed</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="89" to="100" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spectral clustering with discriminant cuts</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl-Based Syst</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="27" to="37" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spectral clustering: a semi-supervised approach</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="229" to="242" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Parallel spectral clustering in distributed systems</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Q</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Patt Anal Mach Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="568" to="586" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rubin</forename><surname>Db</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J R Stat Soc Ser B Stat Methodol</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A min-max cut algorithm for graph partitioning and data clustering</title>
		<author>
			<persName><forename type="first">Chq</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE international conference on data mining (ICDM&apos; 2001)</title>
		<meeting>IEEE international conference on data mining (ICDM&apos; 2001)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="107" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Twostep spectral clustering controlled islanding algorithm</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Gonzalez-Longatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Terzija</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Power Syst</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="75" to="84" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Research of semisupervised spectral clustering algorithm based on pairwise constraints</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">W</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00521-012-1207-8</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Comput Appl</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Research of semi-supervised spectral clustering based on constraints expansion</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput Appl</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">Suppl 1</biblScope>
			<biblScope unit="page" from="405" to="S410" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Lower bounds for the partitioning of graph</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Donath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Hoffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM J Res Dev</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="420" to="425" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Clustering with multi-layer graphs: a spectral perspective</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nefedov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Sig Process</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5820" to="5831" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An improved spectral bisection algorithm and its application to dynamic load balancing</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parallel Comput</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="48" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Well-separated clusters and the optimal fuzzy partitions</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Dunn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Cybern</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="95" to="104" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Selection of the number of clusters via the bootstrap method</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">X</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Stat Data Anal</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="468" to="477" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Algebraic connectivity of graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fiedler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Czechoslov Math J</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="298" to="305" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sparse spectral clustering method based on the incomplete Cholesky decomposition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Frederix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Van</forename><surname>Barel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Comput Appl Math</title>
		<imprint>
			<biblScope unit="volume">237</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="145" to="161" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">New spectral methods for radio cut partitioning and clustering</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Kahng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Comput-aid Des Integr Circuits Syst</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1074" to="1085" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Introduction to spectral clustering</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hamad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Biela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">rd International conference on information and communication technologies: from theory to applications</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="490" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An improved spectral graph partitioning for mapping parallel computations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hendrickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leland</forename><forename type="middle">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J Sci Comput</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="452" to="459" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A unified view of spectral clustering</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Higham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kibble</surname></persName>
		</author>
		<idno>02</idno>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
		<respStmt>
			<orgName>University of Strathclyde Mathematics Research</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Report</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A fast clustering algorithm to cluster very large categorical data sets in data mining</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGMOD workshop on research issues on data mining and knowledge discovery</title>
		<meeting>the SIGMOD workshop on research issues on data mining and knowledge discovery<address><addrLine>Tucson</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="146" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Extensions to the k-means algorithm for clustering large data sets with categorical values</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Min Knowl Discov</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="283" to="304" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bagging-based spectral clustering ensemble selection</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Patt Recogn Lett</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1456" to="1467" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fast semi-supervised clustering with enhanced spectral embedding</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Patt Recogn</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4358" to="4369" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Spectral biclustering of microarray data: coclustering genes and conditions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kluger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Res</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="703" to="716" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Community structure in directed networks</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Leicht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mej</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys Rev Lett</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">118703</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A survey of clustering algorithms based on spectra of graphs</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CAAI Trans Intell Syst</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="405" to="414" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Constructing affinity matrix in spectral clustering based on neighbor propagation</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="125" to="130" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Non-local spatial spectral clustering for image segmentation</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="461" to="471" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fuzzy spectral clustering with robust spatial information for image segmentation</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl Soft Comput</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3636" to="3647" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On the eigenvectors of p-Laplacian</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach Learn</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="51" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Consistency of spectral clustering</title>
		<author>
			<persName><forename type="first">U</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann Stat</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="555" to="586" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 5th Berkeley symposium on mathematical statistics</title>
		<meeting>5th Berkeley symposium on mathematical statistics</meeting>
		<imprint>
			<date type="published" when="1967">1967</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Contour and texture analysis for image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int J Comput Vis</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="27" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>MIT Press</publisher>
			<biblScope unit="page" from="873" to="879" />
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
	<note>Learning segmentation by random walks</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Alignment and integration of complex networks by hypergraph-based spectral clustering</title>
		<author>
			<persName><forename type="first">T</forename><surname>Michoel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nachtergaele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys Rev E</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">56111</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Additive spectral method for fuzzy cluster analysis of similarity data including community structure and affinity matrices</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mirkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nascimento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf Sci</title>
		<imprint>
			<biblScope unit="volume">183</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="16" to="34" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Some applications of Laplace eigenvalues of graphs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mohar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Graph Symmetry Algebraic Methods Appl</title>
		<imprint>
			<biblScope unit="volume">497</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="227" to="275" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Spectral methods for graph clustering: a survey</title>
		<author>
			<persName><forename type="first">Mcv</forename><surname>Nascimento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Acplf</forename><surname>De Carvalho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur J Oper Res</title>
		<imprint>
			<biblScope unit="volume">211</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Analysis of weighted networks</title>
		<author>
			<persName><forename type="first">Mej</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys Rev E</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">56131</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Finding community structure in networks using the eigenvectors of matrices</title>
		<author>
			<persName><forename type="first">Mej</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys Rev E</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">36104</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Modularity and community structure in networks</title>
		<author>
			<persName><forename type="first">Mej</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc Nat Acad Sci US</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="8577" to="8582" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">On spectral clustering: analysis and an algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv Neural Inf Process Syst</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="849" to="856" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Spectral clustering of protein sequences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paccanaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chennubhotla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Casbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucl Acids Res</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1571" to="1580" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Spectral clustering with more than K eigenvectors</title>
		<author>
			<persName><forename type="first">N</forename><surname>Rebagliati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Verri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1391" to="1401" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Supervised learning of large perceptual organization: graph spectral partitioning and learning automata</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Soundararajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Patt Anal Mach Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="504" to="525" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Patt Anal Mach Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Clustering algorithms research</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Softw</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="48" to="61" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Automatically finding clusters in normalized cuts</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tepper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Muse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Almansa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mejail</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Patt Recogn</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1372" to="1386" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Enabling scalable spectral clustering for image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Clausi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Patt Recogn</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4069" to="4076" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Graph theoretical clustering based on limited neighborhood sets</title>
		<author>
			<persName><forename type="first">R</forename><surname>Urquhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="173" to="187" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat Comput</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Consistent selection of the number of clusters via cross validation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="893" to="904" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Density-sensitive spectral clustering</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Electronica Sinica</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1577" to="1581" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Multi-level low-rank approximationbased spectral clustering for image segmentation</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Patt Recogn Lett</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="2206" to="2215" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Spectral clustering on multiple manifolds</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1149" to="1161" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Toward efficient hierarchical designs by ratio cut partitioning</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on CAD</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="298" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">An optimal graph theoretic approach to data clustering: theory and its application to image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Leahy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Patt Anal Mach Intell</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1101" to="1113" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Spectral clustering with eigenvector selection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Patt Recogn</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1012" to="1029" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Toward the optimization of normalized graph Laplacian</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="660" to="666" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A spectral clustering based conference resolution method</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Chin Inf Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="10" to="16" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Spectral clustering with density sensitive similarity function</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl-Based Syst</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="621" to="628" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Image clustering using local discriminant models and global integration</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">T</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2761" to="2773" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Graph-theoretic methods for detecting and describing gestalt clusters</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Zahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Comput</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="68" to="86" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Hand-written numeral recognition based on spectrum clustering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">J</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MIPPR 2011: pattern recognition and computer vision, Proceedings of SPIE</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Local density adaptive similarity measurement for spectral clustering</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Patt Recogn Lett</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="352" to="358" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">An improved spectral clustering algorithm based on random walk</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">Z</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers Comput Sci China</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="268" to="278" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Spectral clustering ensemble applied to SAR image segmentation</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Geosci Rem Sens</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2126" to="2136" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Spectral clustering with eigenvector selection based on entropy ranking</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput &amp; Applic</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">10-12</biblScope>
			<biblScope unit="page" from="1704" to="1717" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>Neurocomputing</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
