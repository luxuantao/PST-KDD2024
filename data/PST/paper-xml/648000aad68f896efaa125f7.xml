<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CHATDB: AUGMENTING LLMS WITH DATABASES AS THEIR SYMBOLIC MEMORY</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-06-06">6 Jun 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chenxu</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Fu</surname></persName>
							<email>fujie@baai.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Beijing Academy of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chenzhuang</forename><surname>Du</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Simian</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hang</forename><surname>Zhao</surname></persName>
							<email>hangzhao@mail.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CHATDB: AUGMENTING LLMS WITH DATABASES AS THEIR SYMBOLIC MEMORY</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-06-06">6 Jun 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2306.03901v1[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large language models (LLMs) with memory are computationally universal <ref type="bibr" target="#b21">(Schuurmans, 2023)</ref>. However, mainstream LLMs are not taking full advantage of memory, and the designs are heavily influenced by biological brains. Due to their approximate nature and proneness to the accumulation of errors, conventional neural memory mechanisms cannot support LLMs to simulate complex reasoning. In this paper, we seek inspiration from modern computer architectures to augment LLMs with symbolic memory for complex multi-hop reasoning. Such a symbolic memory framework is instantiated as an LLM and a set of SQL databases, where the LLM generates SQL instructions to manipulate the SQL databases. We validate the effectiveness of the proposed memory framework on a synthetic dataset requiring complex reasoning. The project website is available at https: //chatdatabase.github.io/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large language models (LLMs), such as <ref type="bibr">GPT-4 (OpenAI, 2023)</ref> and PaLM 2 <ref type="bibr" target="#b1">(Anil et al., 2023)</ref>, have increasingly become an essential component of modern artificial intelligence (AI) systems, revolutionizing our understanding of natural language processing (NLP) and transforming various industries <ref type="bibr" target="#b12">(Hao et al., 2023;</ref><ref type="bibr" target="#b24">Wang et al., 2023)</ref>. While LLMs have made significant strides in understanding and generating contextually relevant responses, they have limitations <ref type="bibr" target="#b5">(Chen et al., 2023)</ref>. One of the main challenges is that multi-turn interactions with language models generate a large number of tokens, which can easily exceed the input token limit of LLMs. For example, GPT-4 (32K) can only handle 32,000 tokens. As the interaction progresses, the LLMs must maintain contextual information (e.g., user inputs, and previous responses) and generate responses based on the accumulated data. Simply concatenating all contextual information and cramming it into LLMs, however, can easily exceed the processing capabilities of LLMs and accumulate errors, causing the model to lose track of the conversation and produce less accurate responses. Some neural memory mechanisms have been explored <ref type="bibr" target="#b28">(Wu et al., 2022a;</ref><ref type="bibr" target="#b14">Khattab et al., 2022;</ref><ref type="bibr" target="#b31">Zhong et al., 2022)</ref> to overcome the limited token input issue of LLMs. The memory components serve as a storage and retrieval system for relevant information from previous interactions. However, augmenting LLMs with conventional neural memory usually leads to difficulties in storing, retrieving, and manipulating historical information in memory, especially for tasks requiring complex multi-hop reasoning. Two main causes are (a) They do not store historical information in a structured form; (b) Their manipulation of the information stored in memory is not symbolic, as they all rely on some vector similarity calculations, which can be inaccurate, thus leading to the accumulation of errors.</p><p>To address the aforementioned issues, we propose using databases as novel symbolic memory for LLMs. The whole framework is named ChatDB. As shown in Figure <ref type="figure">1</ref>, ChatDB consists of two components: an LLM controller and its memory. The LLM controller can be any commonly used LLM <ref type="bibr" target="#b17">(OpenAI, 2023;</ref><ref type="bibr" target="#b23">Touvron et al., 2023;</ref><ref type="bibr">Du et al., 2022;</ref><ref type="bibr" target="#b30">Zeng et al., 2022)</ref> and is responsible for controlling the read and write operations to the memory. The memory of LLMs, which can be symbolic or non-symbolic, or a combination of both, is responsible for storing historical information and providing information when needed to assist the LLM in responding to user input. In ChatDB, we focus on using databases as symbolic memory, which allows for the structured storage of historical information through the execution of a symbolic language, namely SQL statements. These SQL statements are generated by an LLM. Incorporating a database as symbolic memory is particularly useful in scenarios requiring precise recording, modification, querying, deletion, and analysis of historical data. For example, a store manager needs to maintain daily sales records, where using plain text or matrices as memory is unsuitable <ref type="bibr" target="#b5">(Chen et al., 2023)</ref>. However, using a database as external symbolic memory is highly suitable. The database enables accurate operations, including data insertion, deletion, update, and selection, using SQL statements. Thus, employing databases as external symbolic memory ensures precision and efficiency in managing and manipulating historical data, significantly enhancing the performance of LLMs in scenarios that require high accuracy and long-term data recording and processing.</p><p>In the ChatDB framework, we propose the chain-of-memory (CoM) approach to manipulate the external symbolic memory more effectively, thereby further enhancing the reasoning capabilities of LLMs. The chain-of-memory approach transforms user input into a series of intermediate memory operation steps that lead to final results. Through the chain-of-memory approach, a complex problem is decomposed into multiple steps of memory operations, significantly reducing the complexity of problem-solving. In ChatDB, each intermediate step involves one or more SQL statements.</p><p>Our ChatDB makes several contributions to the field of LLMs. Firstly, we propose augmenting LLMs with databases as their external symbolic memory, allowing for structured storage of historical data and enabling symbolic and complex data operations using SQL statements. Secondly, our chain-of-memory approach enables effective memory manipulation by converting user input into multi-step intermediate memory operations, which enhance the performance of ChatDB, enabling it to handle complex, multi-table database interactions with improved accuracy and stability. Finally, our experiments demonstrate that augmenting LLMs with symbolic memory improves multi-hop reasoning capabilities and prevents error accumulation, thereby enabling ChatDB to significantly outperform ChatGPT on a synthetic dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Memory-Augmented Large Language Models (LLMs). LLMs, such as <ref type="bibr">GPT-4 (OpenAI, 2023)</ref> and PaLM 2 <ref type="bibr" target="#b1">(Anil et al., 2023)</ref>, have demonstrated powerful reasoning and decision-making abilities. However, LLMs are often hindered by their limited context window sizes (e.g., GPT-4 can only handle 32K tokens). Memory-augmented LLMs <ref type="bibr">(Wu et al., 2022a,b;</ref><ref type="bibr" target="#b31">Zhong et al., 2022;</ref><ref type="bibr" target="#b16">Lewis et al., 2020;</ref><ref type="bibr" target="#b11">Guu et al., 2020;</ref><ref type="bibr" target="#b18">Park et al., 2023;</ref><ref type="bibr" target="#b14">Khattab et al., 2022;</ref><ref type="bibr" target="#b13">Izacard et al., 2022)</ref> incorporate a memory module that prevents the model from forgetting crucial information and allows it to handle long text inputs that exceed the context window size. Retrieval-augmented in-context learning <ref type="bibr" target="#b14">(Khattab et al., 2022)</ref> uses retrieval models (RM) to retrieve relevant information that can be inserted into the LLM as a prompt. For example, Auto-GPT<ref type="foot" target="#foot_0">3</ref> and Generative Agents <ref type="bibr" target="#b18">(Park et al., 2023)</ref> utilize a memory module to store the text prompt directly, allowing the agent to keep track of its history. The past and current prompts are then input into the LLM for processing. Neural Turing Machines (NMT) <ref type="bibr" target="#b10">(Graves et al., 2014)</ref>, which incorporate the recurrent neural network (RNN) with external trainable memory resources and learn to interact with the memory module with gradient descent. Recurrent Memory Transformer (RMT) <ref type="bibr" target="#b3">(Bulatov et al., 2022)</ref> introduces additional memory tokens to the input and output sequences to store, process and exchange local and global information between segments of long sequences, and then train the model to control both memory operation and sequence representations processing.</p><p>Reasoning with LLMs. LLMs are known to struggle in complex reasoning tasks. Previous methods focus on incorporating specially designed supervisory signals or fine-tuning to enhance the reasoning ability of language models <ref type="bibr" target="#b19">(Pi?kos et al., 2021;</ref><ref type="bibr" target="#b20">Ran et al., 2019;</ref><ref type="bibr" target="#b0">Andor et al., 2019;</ref><ref type="bibr" target="#b8">Cobbe et al., 2021;</ref><ref type="bibr" target="#b6">Chen et al., 2022)</ref>. Recent methods mainly improve the reasoning ability of language models through In-Context Learning <ref type="bibr" target="#b2">(Brown et al., 2020;</ref><ref type="bibr" target="#b15">Lester et al., 2021;</ref><ref type="bibr" target="#b26">Wei et al., 2021</ref><ref type="bibr" target="#b27">Wei et al., , 2022;;</ref><ref type="bibr" target="#b25">Wang et al., 2022)</ref>. The most representative of these is Chain-of-Thought (CoT) <ref type="bibr" target="#b27">(Wei et al., 2022)</ref>, which presents the intermediate reasoning process of solving sample problems to the language model, greatly enhancing its reasoning capabilities.</p><p>LLMs with DBs. LLMs have demonstrated an impressive capability in generating code, including Python code, execution commands for Excel, and Structured Query Language (SQL) for databases <ref type="bibr" target="#b17">(OpenAI, 2023)</ref>. ChatExcel<ref type="foot" target="#foot_1">4</ref> uses LLMs to generate the Excel execution command, simplifying the user interaction process. BINDER <ref type="bibr" target="#b7">(Cheng et al., 2022)</ref> proposes a framework that maps task inputs to executable programs in a programming language (e.g., Python code) bound with an API to call LLMs to perform a wide range of functionalities. SQL-PALM <ref type="bibr" target="#b22">(Sun et al., 2023)</ref> proposes an LLM-based Text-to-SQL model, using the execution-based self-consistent prompting approach, and outperforms previous Text-2-SQL methods by a large margin. While previous works involve databases to some extent, our proposed ChatDB system differs significantly from these methods. In specific, ChatDB views the databases as the external symbolic memory module for the LLM, and then leverages the database for reading and writing essential data information to enhance the reasoning process via chain-of-memory, leading to more accurate reasoning results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ChatDB</head><p>In this section, we first briefly introduce the task definition and setting. We then describe the overall framework of our proposed ChatDB. Finally, we delve into the details of the chain-of-memory approach, which is the main component of ChatDB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task Definition</head><p>Given a user input in natural language and details of existing tables in the database (not required if there are no existing tables), the goal is to manipulate the symbolic memory, i.e., the external database, to fulfill the user's request. For instance, if the user (e.g., a store manager) command is to record, modify, query, and delete specific data, the corresponding SQL operation should be to insert, update, select, and delete the relevant data in the appropriate tables, respectively. These operations often involve multiple tables in the database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Framework Overview</head><p>ChatDB framework consists of three main stages: input processing, chain-of-memory, and response summary, as illustrated in Figure <ref type="figure" target="#fig_1">2</ref>. Algorithm 1 provides a detailed illustration of the entire algorithmic procedure of ChatDB to respond to the user input.</p><p>Input Processing. ChatDB generates a series of intermediate steps to manipulate the symbolic memory by utilizing LLMs if responding to the user input requires the use of memory. Otherwise, we use LLMs directly to generate a reply.</p><p>Chain-of-Memory. ChatDB executes a series of intermediate memory operation steps to interact with symbolic memory. ChatDB manipulates the symbolic memory in sequence according to a series of previously generated SQL statements, including operations such as insert, update, select, delete, etc. The external database executes the corresponding SQL statements, updates the database, and returns the results. It is worth noting that ChatDB decides whether to update the memory operation step based on the results of previous SQL statements before executing this operation. ChatDB executes the next step following the same procedure until all operations on the memory are completed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output</head><p>Step !</p><p>Step "</p><p>Step #</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LLMs LLMs</head><p>SELECT customer_id FROM customers WHERE phone_number = '823451' Response Summary. ChatDB summarizes the final response to the user based on the results of a series of chain-ofmemory steps.</p><formula xml:id="formula_0">+-------------+ | customer_id | +-------------+ | 2 | +-------------+ SELECT sale_id FROM sales WHERE customer_id = 2 AND sale_date = '2023-01-02'; +---------+ | sale_id | +---------+ | 2 | +---------+ DELETE FROM sales WHERE sale_id = 2; delete data successfully. 1 SELECT customer_id FROM customers WHERE phone_number = '823451' +-------------+ | customer_id | +-------------+ | 2 | +-------------+ SELECT sale_id FROM sales WHERE customer_id = 2 AND sale_date = '2023-01-02'; +---------+ | sale_id | +---------+ | 2 | +---------+ DELETE FROM sales WHERE sale_id = 2; delete data successfully. 1 SELECT customer_id FROM customers WHERE phone_number = '823451' +-------------+ | customer_id | +-------------+ | 2 | +-------------+ SELECT sale_id FROM sales WHERE customer_id = 2 AND sale_date = '2023-01-02'; +---------+ | sale_id | +---------+ | 2 | +---------+ DELETE FROM sales WHERE sale_id = 2; delete data successfully. 1 SELECT customer_id FROM customers WHERE phone_number = '823451' +-------------+ | customer_id | +-------------+ | 2 | +-------------+ SELECT sale_id FROM sales WHERE customer_id = 2 AND sale_date = '2023-01-02'; +---------+ | sale_id | +---------+ | 2 | +---------+ DELETE FROM sales WHERE sale_id = 2; delete data successfully. 1 SELECT customer_id FROM customers WHERE phone_number = '823451' +-------------+ | customer_id | +-------------+ | 2 | +-------------+ SELECT sale_id FROM sales WHERE customer_id = 2 AND sale_date = '2023-01-02'; +---------+ | sale_id | +---------+ | 2 | +---------+ DELETE FROM sales WHERE sale_id = 2; delete data successfully. 1 SELECT customer_id FROM customers WHERE phone_number = '823451'; +-------------+ | customer_id | +-------------+ | 2 | +-------------+ SELECT sale_id</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Chain-of-Memory</head><p>Chain-of-thought <ref type="bibr" target="#b27">(Wei et al., 2022)</ref> emphasizes decomposing complex reasoning into a series of intermediate steps.</p><p>Chain-of-memory (CoM) can be seen as a way to augment chain-of-thought by providing a symbolic memory mechanism to support the storage associated with those intermediate steps.</p><p>The purpose of chain-of-memory is to enhance the reasoning capabilities and robustness of LLMs when manipulating symbolic memory. The approach involves converting the user input into a sequence of intermediate memory operations, enabling LLMs to more accurately and effectively manipulate the memory in a symbolic way. The ability to manipulate symbolic memory is particularly valuable for real-world applications that involve complex and accurate interactions with historical data, such as record-keeping and data analysis in a management setting.</p><p>To enhance the performance and stability of our approach, we employ in-context learning <ref type="bibr" target="#b2">(Brown et al., 2020)</ref>, providing prompt exemplars of several sequences of chain-of-memory steps, and chain-of-thought prompting. A robust and accurate chain-of-memory process enables LLMs to better reason over symbolic memory and handle more complex scenarios.</p><p>The advantages of chain-of-memory are twofold. Firstly, it enables LLMs to perform complex database manipulations with greater accuracy, enhancing their multi-hop reasoning ability over symbolic memory. Secondly, by decomposing complex operations into a sequence of intermediate memory operations, the chain-of-memory approach enhances the capability of LLMs when handling complex, multi-table interactions. This approach enables LLMs to handle edge cases and unexpected scenarios better, making it a promising method for real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Comparison with previous memory-augmented LLMs</head><p>In this subsection, we provide a comprehensive comparison between ChatDB and recent approaches that augment Transformer-based language models with memory module. The memory modules of language models proposed in previous work can be broadly categorized into two types. The first type of memory stores context and uses a retrieval model to find content from past interactions that's most relevant to the current conversation, and then uses it as a prompt for the language model <ref type="bibr" target="#b14">(Khattab et al., 2022)</ref>. We refer to this type of memory as prompt-based memory. type of approach utilizes additional memory tokens or memory matrices as memory <ref type="bibr" target="#b3">(Bulatov et al., 2022)</ref>, which we refer to as matrix-based memory. We compare ChatDB with these approaches based on the following aspects:</p><p>1. Memory Format. This aspect relates to the format used for storing memory. ChatDB utilizes databases as its memory. Prompt-based memory <ref type="bibr" target="#b18">(Park et al., 2023)</ref> stores relevant interaction content and/or their corresponding vector embeddings. Matrix-based memory employs additional trainable memory tokens <ref type="bibr" target="#b3">(Bulatov et al., 2022</ref><ref type="bibr" target="#b4">(Bulatov et al., , 2023) )</ref> or trainable memory matrices <ref type="bibr" target="#b10">(Graves et al., 2014)</ref>.</p><p>2. Supported Operations. This aspect refers to the operations supported for manipulating memory. ChatDB supports operations such as insertion, deletion, update, and selection of data in the database memory. The prompt-based memory primarily supports insertion and selection operations but lacks complete support for updates and deletions.</p><p>Matrix-based memory supports reading (selection) and writing (insertion, updating, deletion) operations. However, the exact operations performed by the neural network are not explicitly known.</p><p>3. Memory Storage. This aspect refers to the format in which data is stored in memory, specifically whether it is structured or not. ChatDB stores memory in a structured format using databases, while both the prompt-based memory and matrix-based memory are viewed as semi-structured. The reason is that vector embeddings and memory matrices have specific dimensions and sizes, but each individual dimension does not carry a specific and explicit meaning.</p><p>4. Memory Execution. This aspect focuses on how memory operations are executed, specifically whether they are symbolic or not. ChatDB executes operations on its database memory using SQL, which is a symbolic language, thus making it inherently symbolic. Prompt-based memory performs selections based on a similarity metric using vector embeddings and employs a language encoder to obtain vector embeddings for insertions. Both of these are considered ChatDB non-symbolic executions. In matrix-based memory augmentated LLMs, memory operations are entirely controlled by the neural network, resulting in non-symbolic executions as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>Interpretability. This aspect refers to the degree of interpretability of memory. In ChatDB, memory is stored in a structured and explicit format, and its operations are symbolic, leading to a high level of interpretability. In the prompt-based memory, interpretability is generally limited due to inherent challenges in interpreting vector embeddings.</p><p>For matrix-based memory approach, the degree of interpretability is low since memory is completely controlled by the neural network implicitly.</p><p>6. State Tracking. This aspect refers to whether the memory effectively tracks the current state of the LLMs. In the case of ChatDB, its memory accurately tracks the current state of the LLMs. The fruit shop experiment serves as a demonstration, where after processing each record, ChatDB's database memory is updated to reflect the latest state of the fruit shop. This showcases how ChatDB's memory effectively tracks its current state. Thanks to the symbolic memory execution, ChatDB's memory allows for easy rollback to any desired timestamp, providing greater flexibility and controllability. In matrix-based memory approach, memory is continuously updated and altered by the model itself, enabling it to track the current state of the LLMs. However, the prompt-based memory approach simply stores the historical context and only knows what has happened in the past without a clear understanding of the current state.</p><p>By examining these aspects, we observe the distinctive features and capabilities of ChatDB in comparison to existing approaches. The superiority of ChatDB highlights the advantages of using symbolic memory to augment LLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>In this section, we conduct experiments to evaluate the effectiveness of augmenting LLMs with databases as their symbolic memory. Our experiment results demonstrate that ChatDB significantly outperforms the baseline model ChatGPT, highlighting the advantages of symbolic memory integration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>As previously mentioned, using a database as symbolic memory is particularly suitable for scenarios that require precise recording and processing of historical information, such as various data management scenarios. To adapt to the use case of ChatDB and enable quantitative comparisons with other models, we constructed a synthetic dataset simulating the management of a fruit shop.</p><p>Furthermore, in order to evaluate the performance of the models, we collected a set of 50 questions with annotated standard answers. These questions varied in difficulty, ranging from hard ones that required multi-hop reasoning to easy ones that only required retrieving information from historical data. There are 15 easy questions and 35 hard questions.</p><p>Each question is independently answered by the models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Model Configuration</head><p>ChatDB. The LLM used in ChatDB is ChatGPT (GPT-3.5 Turbo), and the hyperparameter temperature is set to 0. We used the MySQL database as the external symbolic memory.</p><p>Baseline. We use ChatGPT (GPT-3.5 Turbo) as the baseline model with a maximum token length of 4096. Similar to ChatDB, we set the temperature to 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Dataset</head><p>We synthesize a dataset of fruit shop management records, referred to as the "Fruit Shop Dataset". This dataset simulates four common operations in a shop: purchasing, selling, changing price, and goods returns. We ensure that all historical records are valid and do not encounter issues such as negative inventory. We generate 70 records arranged in chronological order, totaling approximately 3.3k tokens, which is within the maximum token length limit of ChatGPT (4096 tokens).</p><p>Why do we limit the token length of the dataset? If the token length of the dataset exceeds the maximum token length of ChatGPT, memory becomes necessary. However, the mainstream memory retrieval methods based on vector embedding are prone to errors. This inevitably leads to a decline in the performance of ChatGPT, which is not desired. Therefore, we deliberately design the token length of the dataset to be within the maximum token length of ChatGPT to avoid using memory and maximize the model's performance. Note that ChatDB's performance is generally unaffected by the token length of the dataset. Thus, if ChatDB outperforms ChatGPT when the dataset is small, it indicates that ChatDB is also superior to memory-augmented ChatGPT when the dataset is large.</p><p>(a) Purchasing goods: the shop purchases goods from suppliers.</p><p>Record: We restocked our store on 2023-01-01 with a new supply of fruits from 'ABC' (abc_sup@example.com, 10080). The purchased quantities include 24 kg cherry, at unit prices of 0.8. (c) Return of goods: customers bring back purchased items to the shop.</p><p>Record: Because the customer returned their purchase, we are required to undo the sales transaction made by customer 'John Doe' (phone: 123-456-7890, email: john.doe@example.com) on 2023-01-05. ChatDB: Begin to interact with the external database (symbolic memory).</p><p>Step1: Find the sale_id for this customer on this date SELECT sale_id FROM sales WHERE customer_id = (SELECT customer_id FROM customers WHERE phone_number = '123-456-7890' AND email = 'john.doe@example.com') AND sale_date = '2023-01-05'; <ref type="figure">--------+</ref> Step2: Get all the fruit_id and quantity_sold for this sale, replace &lt;sale_id&gt; with the results from the previous queries  <ref type="figure">+----------+---------------+  | fruit_id | quantity_sold |  +----------+---------------</ref> <ref type="figure">---------+---------------+</ref> Step3: Increase the stock_quantity for each fruit sold in this sale, replace &lt;quan-tity_sold&gt; &lt;fruit_id&gt; with the results from the previous queries  </p><formula xml:id="formula_1">? ? Database response: +---------+ | sale_id | +---------+ | 9 | +-</formula><formula xml:id="formula_2">+ | 7 | 10 | | 6 | 9 | +-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ChatDB</head><p>Suppose you are a fruit shop manager and good at analyzing history records. The fruit shop newly opened on January 1, 2023. Given the history records for the fruit shop in January 2023, which include customer names, transaction dates, fruit prices, quantities purchased, and whether the items were returned, you need to answer some questions. By default, exclude the sales transactions that have been returned when performing calculations. Here are the historical records of the fruit shop, which are arranged in chronological order based on the occurrence time, surrounded by triple backticks: ``{ records} ``B ased on the history records, answer the question about the fruit shop: {question} Figure <ref type="figure">4</ref>: Prompt for ChatGPT to answer questions in the Fruit Shop Dataset. The placeholders "records" and "question" will be replaced by specific details when it is actually used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Processing records</head><p>For ChatDB, the first step is to initialize the database. We need to generate a reasonable database schema for the specific task scenario and create tables in the database. The generation of the database schema can be done manually or using LLMs. Next, for each record in the dataset, ChatDB processes them one by one. Using the LLM controller, ChatDB manipulates the external database (i.e., symbolic memory) following Algorithm 1. We provide examples of ChatDB's response to the four common operations in the Fruit Shop Dataset, namely purchasing, selling, changing prices, and goods returns, as shown in Figure <ref type="figure" target="#fig_4">3</ref>. It is worth emphasizing that ChatDB processes record one by one, so it is not sensitive to the total number of records. Furthermore, each step of the database operation in ChatDB is symbolic and free from errors. Therefore, in theory, ChatDB can handle an infinite number of historical records without sacrificing performance. However, for ChatGPT or existing memory-augmented LLMs, excessively long historical records can significantly degrade performance. In this experiment, as for the ChatGPT baseline, since the records are not long, we simply treat them as part of the prompt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Answering questions</head><p>When answering questions, ChatDB no longer requires records to be part of the prompt. After processing the records, the information is stored in symbolic memory. Following Algorithm 1, ChatDB utilizes SQL statements to perform a series of database queries (including calculations) in order to answer the question. On the other hand, ChatGPT includes records as part of the prompt and directly asks the question. The prompt template is shown in Figure <ref type="figure">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>The experimental results are presented in Table <ref type="table" target="#tab_2">2</ref>, which clearly demonstrate that ChatDB outperforms ChatGPT with significantly higher accuracy. While ChatGPT is capable of answering easy questions, it falls short in handling hard questions that necessitate multi-hop reasoning and precise calculations. Consequently, ChatGPT exhibits a low accuracy rate for these hard questions. In contrast, ChatDB exhibits a notably high accuracy rate, underscoring the advantages of utilizing a database as symbolic memory. This approach not only prevents error accumulation but also enhances the multi-hop reasoning and precise calculation capabilities of LLMs. We present several examples of the two models answering questions in Figure <ref type="figure">5</ref> for comparison. In all these examples, ChatDB correctly answers the questions while ChatGPT fails. ChatGPT often exhibits errors in calculating the total price of each sale transaction, as observed in Figure <ref type="figure">5</ref>(a). Sometimes, the formulas are correct but the calculations are wrong, while other times, even the formulas are incorrect. In addition, ChatGPT struggles to find all valid sale ChatDB (a) Question: What was the total revenue for January 2023?</p><p>ChatGPT:</p><p>To calculate the total revenue for January 2023, we need to sum up the revenue generated from each sales transaction. We can use the following formula to calculate the revenue for each transaction: revenue = quantity * selling price We also need to exclude the sales transactions that have been returned. Based on the history records, the following sales transactions have been returned:</p><p>- To calculate the revenue for each day, we need to sum up the total sales for that day (excluding any returned items) and multiply by the selling price. We can then compare the revenue for each day to determine which day had the highest revenue.</p><p>Here is the breakdown of sales for each day:</p><p>-January 2: 9 kg apple at 3.8 = 34.2, 4 kg cherry at 1.3 = 5.2, 3 kg cherry at 1.3 = 3.9, 3 kg orange at 2.1 = 6.3, total sales = 49.6 [WRONG]: should be 43.2 -January 4: 2 kg apple at 1.5 = 3, 4 kg pear at 1.6 = 6.4, 7 kg cherry at 1.3 = 9.1, total sales = 18.5 [WRONG]: should be 27.9 -January 5: 5 kg cherry at 1.9 = 9.5, 10 kg orange at 1.8 = 18, 1 kg cherry at 1.9 = 1.9, 1 kg grape at 3.2 = 3.2, total sales = 32.6 [WRONG]: should be 83 -January 6: 3 kg apple at 1.5 = 4.5, 10 kg grape at 3.2 = 32, total sales = 36.5 [WRONG]: should be 29.4 -January 7: 4 kg pear at 1.6 = 6.4, total sales = 6.4 [WRONG]: should be 37.2 -January 8: 6 kg peach at 3 = 18, total sales = 18 [WRONG]: should be 21.6 [...] [WRONG]: most of the daily revenues are calculated incorrectly.</p><p>[STOP]: Because it exceeds the maximum token length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ChatDB:</head><p>Begin to interact with the external database (symbolic memory).</p><p>Step1: Calculate the total revenue for each day in January 2023  <ref type="figure">+------------+---------</ref></p><formula xml:id="formula_3">+ | sale_date | revenue | +------------+---------+ | 2023-01-02 | 43.2 | | 2023-01-04 | 27.9 | | ... | ... | +----------------------+</formula><p>Step2: Find the day with the highest revenue  <ref type="figure">-------------</ref> ChatDB transactions, leading to errors in its answering process. This issue is commonly encountered and evident in all these examples. Furthermore, ChatGPT tends to make sequential errors, resulting in significant error accumulation.</p><formula xml:id="formula_4">--------------+ | total_weight | +--------------+ | 278 | +-</formula><p>In contrast, ChatDB performs quite well in these examples. During the initial processing of records, symbolic operations (i.e., SQL operations) are applied to manipulate the database (i.e., symbolic memory), ensuring that all information is stored in a structured form within the database. When answering questions, ChatDB generates SQL statements to query the database. The three examples demonstrate the effectiveness of ChatDB in solving problems requiring one, two, and three chain-of-memory steps, respectively. We can observe that ChatDB accurately answers the questions, and the execution logic of the chain-of-memory is clear, with each step tightly connected and approaching the ultimate answer.</p><p>From these examples, the advantages of ChatDB are evident in two aspects:</p><p>1. Through the chain-of-memory approach, complex problems are decomposed into multiple steps of memory operations, simplifying the problem's complexity. Each step's result is accurately stored as an intermediate outcome and used in subsequent steps, which greatly assists in complex reasoning.</p><p>2. Symbolic memory enables precise operations and calculations. ChatDB delegates many calculational tasks to the external database by executing SQL statements, ensuring the accuracy of each step and preventing error accumulation.</p><p>In summary, by leveraging external databases as symbolic memory, ChatDB significantly outperforms ChatGPT in this experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we introduce a framework that augments LLMs with symbolic memory in the form of databases. We demonstrate the advantages and capabilities of symbolic memory and chain-of-memory approach in enhancing complex reasoning and preventing error accumulation. By providing a precise storage mechanism for intermediate results, symbolic memory enables accurate and reliable operations. Moreover, the use of symbolic languages, such as SQL, allows symbolic computation and manipulation of stored information. Through experimental evaluation, we observe a significant improvement in performance with ChatDB compared to ChatGPT. The integration of symbolic memory in ChatDB substantially enhances the model's ability to handle various queries and reasoning tasks in management settings. This improvement highlights the benefits and effectiveness of leveraging symbolic memory in LLMs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: ChatDB framework. The red arrow lines represent the process flow of chain-of-memory, indicating the connection between multiple memory operations. The red arrow lines between database tables represent the reference relationships between primary keys and foreign keys, which start from primary keys to foreign keys. Only the first four columns of each table are shown for brevity. This example showcases the process of returning goods purchased on 2023-01-02 by a customer with the phone number 823451.</figDesc><graphic url="image-10.png" coords="4,124.31,184.60,351.60,67.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>SELECT</head><label></label><figDesc>fruit_id, quantity_sold FROM sale_items WHERE sale_id = 9 [&lt;sale_id&gt;]; Database response:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>UPDATEfruits SET stock_quantity = stock_quantity + 10 [&lt;quantity_sold&gt;] WHERE fruit_id = 7 [&lt;fruit_id&gt;]; UPDATE fruits SET stock_quantity = stock_quantity + 9 [&lt;quantity_sold&gt;] WHERE fruit_id = 6 [&lt;fruit_id&gt;]; Step4: Delete the sale items for this sale, replace &lt;sale_id&gt; with the results from the previous queries DELETE FROM sale_items WHERE sale_id = 9 [&lt;sale_id&gt;]; Step5: Delete the sale record, replace &lt;sale_id&gt; with the results from the previous queries DELETE FROM sales WHERE sale_id = 9 [&lt;sale_id&gt;]; (d) Changing price: modify the price of the goods. Record: On 2023-01-05, the sale price of pear in the store was changed to 1.6 dollar per unit. ChatDB: Begin to interact with the external database (symbolic memory). Step1: Update the selling price of pear UPDATE fruits SET selling_price = 1.6 WHERE fruit_name = 'pear';</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Examples showcasing ChatDB's response to the four common operations in the records of the Fruit Shop Dataset. Due to page size limitations, some unnecessary details and ChatDB's summary are omitted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Alice Smith on 2023-01-17 -Bob Smith on 2023-01-05 [...] Therefore, we need to exclude these transactions when calculating the total revenue. Here is the calculation for each sales transaction: -Bob Smith on 2023-01-02: (9 kg * 3.8 USD/kg) + (4 kg * 1.3 USD/kg) = 39.1 USD [WRONG]: calculation error, should be 39.4 -Sue Davis on 2023-01-02: 1 kg * 3.8 USD/kg = 3.8 USD -Bob Smith on 2023-01-05: (5 kg * 1.8 USD/kg) + (2 kg * 3.0 USD/kg) + (6 kg * 2.8 USD/kg) = 47.6 USD [WRONG]: calculation error and this transaction has been returned. [...] [WRONG]: most of the sale transactions are calculated incorrectly. [STOP]: Because it exceeds the maximum token length. ChatDB: Begin to interact with the external database (symbolic memory). Step1: Calculate the total revenue for January 2023 SELECT SUM(total_price) AS total_revenue FROM sales WHERE sale_date &gt;= '2023-01-01' AND sale_date &lt; '2023-02-01'; Question: Which day in January 2023 had the highest revenue? ChatGPT:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>SELECT</head><label></label><figDesc>sale_date, SUM(total_price) AS revenue FROM sales WHERE sale_date BETWEEN '2023-01-01' AND '2023-01-31' GROUP BY sale_date ORDER BY revenue DESC LIMIT 1; Database response: +------------+---------+ | sale_date | revenue | +------------+---------+ | 2023-01-30 | 119.7 | +------------+---------+ (c) Question: What was the average weight of fruit per sales transaction in January 2023? ChatGPT: To calculate the average weight of fruit per sales transaction, we need to first identify all the sales transactions in January 2023 and calculate the total weight of fruit sold in those transactions. Then, we can divide the total weight by the number of sales transactions to get the average weight per transaction. Here are the sales transactions in January 2023: -Bob Smith on 2023-01-02: 9 kg apple, 4 kg cherry -Sue Davis on 2023-01-02: 1 kg apple -Bob Smith on 2023-01-05: 5 kg orange, 2 kg peach, 6 kg pear [WRONG]: this transaction has been returned. -Sue Davis on 2023-01-05: 5 kg pear, 10 kg orange, 1 kg cherry -Alice Smith on 2023-01-05: 5 kg cherry [...] [WRONG]: not find out all valid sale transactions.To calculate the total weight of fruit sold, we can add up the quantities of each fruit sold in the above transactions: -Apples: 16 kg -Cherries: 12 kg -Oranges: 10 kg -Pears: 18 kg -Kiwis: 12 kg -Grapes: 18 kg -Bananas: 27 kg -Strawberries: 7 kg -Peaches: 9 kg -Plums: 8 kg The total weight of fruit sold in January 2023 is 127 kg. [WRONG]: It should be 278. Even according to the calculation above, the total should be 137, not 127. To calculate the average weight per sales transaction, we need to divide the total weight by the number of sales transactions. There are 18 sales transactions in January 2023, [WRONG]: It should be 28 valid sale transactions in total. so the average weight [STOP]: Because it exceeds the maximum token length.ChatDB:Begin to interact with the external database (symbolic memory).Step1: Calculate the total weight of fruit sold in January 2023 SELECT SUM(si.quantity_sold) as total_weight ? FROM sale_items si JOIN sales s ON si.sale_id = s.sale_id WHERE s.sale_date BETWEEN '2023-01-01' AND '2023-01-31'; ? Database response: +</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>+</head><label></label><figDesc>Figure 5: Examples of ChatDB and ChatGPT answering questions. While ChatGPT fails to answer any of the questions correctly, ChatDB successfully answers all of them. The issues with ChatGPT are highlighted in red. Some irrelevant details and ChatDB's summary are omitted for brevity. The gray ellipsis [...] indicates that the response was trimmed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The second Algorithm 1 The algorithm of ChatDB Input: userInput, dataBase Output: reply ? Begin of Input Processing 1: if need manipulate memory to respond to userInput then Comparison with prompt-based memory and matrix-based memory.</figDesc><table><row><cell>2:</cell><cell cols="4">memOps = LLM getSteps (userInput)</cell><cell></cell><cell cols="3">? Generate intermediate steps using LLMs</cell></row><row><cell cols="2">3: else</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4:</cell><cell cols="3">reply = LLM(userInput)</cell><cell></cell><cell></cell><cell cols="3">? Generate reply using LLMs directly</cell></row><row><cell>5:</cell><cell cols="2">return reply</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">6: end if</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">? Begin of Chain-of-Memory</cell></row><row><cell cols="3">7: sqlResults = []</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">8: newMemOps = []</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">9: for each memOp in memOps do</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>10:</cell><cell cols="5">if need update memOp based on sqlResults then</cell><cell></cell><cell></cell></row><row><cell>11:</cell><cell cols="5">newMemOp = LLM updateOperation (memOp, sqlResults)</cell><cell></cell><cell></cell></row><row><cell>12:</cell><cell>else</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>13:</cell><cell cols="3">newMemOp = memOp</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>14:</cell><cell>end if</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>15:</cell><cell cols="5">sqlResult = executeOperation(newMemOp, dataBase)</cell><cell cols="3">? Execute operation on database</cell></row><row><cell>16:</cell><cell cols="4">sqlResults.append(sqlResult)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>17:</cell><cell cols="4">newMemOps.append(newMemOp)</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">18: end for</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">? Begin of Response Summary</cell></row><row><cell>Type</cell><cell></cell><cell>Model</cell><cell>Memory format</cell><cell>Supported operations</cell><cell>Memory storage</cell><cell>Memory execution</cell><cell>Interpretability</cell><cell>State tracking</cell></row><row><cell cols="2">Symbolic</cell><cell>ChatDB</cell><cell>Symbolic storage (e.g., database)</cell><cell>Insert, Delete Update, Select</cell><cell>Structured</cell><cell>Symbolic</cell><cell>High</cell><cell>Yes</cell></row><row><cell cols="2">Prompt-based</cell><cell>Auto-GPT</cell><cell>Content and its vector embeddings</cell><cell>Insert, Select</cell><cell>Semi-Structured</cell><cell>Non-Symbolic</cell><cell>Normal</cell><cell>No</cell></row><row><cell cols="2">Matrix-based</cell><cell>RMT</cell><cell>Memory tokens/ memory matrices</cell><cell>Read, Write</cell><cell>Semi-Structured</cell><cell>Non-Symbolic</cell><cell>Low</cell><cell>Yes</cell></row></table><note><p>19: reply = LLM summary (userInput, newMemOps, sqlResults) ? Summarize the final reply 20: return reply</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The experimental results of answering questions in the Fruit Shop Dataset. There are 50 questions in total, with 15 being easy questions and 35 being hard questions.</figDesc><table><row><cell>Model</cell><cell cols="2">Easy Hard</cell><cell cols="2">All Accuracy</cell></row><row><cell>ChatGPT</cell><cell>10/15</cell><cell cols="2">1/35 11/50</cell><cell>22%</cell></row><row><cell cols="4">ChatDB (ours) 13/15 28/35 41/50</cell><cell>82%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>https://github.com/Significant-Gravitas/Auto-GPT</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>https://chatexcel.com/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Giving bert a calculator: Finding operations and arguments with reading comprehension</title>
		<author>
			<persName><forename type="first">D</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pitler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.00109</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shakeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Taropa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.10403</idno>
		<title level="m">Palm 2 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recurrent memory transformer</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bulatov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kuratov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burtsev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="11079" to="11091" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Bulatov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kuratov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Burtsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.11062</idno>
		<title level="m">Scaling transformer to 1m tokens and beyond with rmt</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Parrish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Padmakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.14279</idno>
		<title level="m">Two failures of self-consistency in the multi-step reasoning of llms</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.12588</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nadkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.02875</idno>
		<title level="m">Binding language models in symbolic languages</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nakano</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.14168</idno>
		<title level="m">Training verifiers to solve math word problems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Glm: General language model pretraining with autoregressive blank infilling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chatdb Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="320" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Retrieval augmented language model pre-training</title>
		<author>
			<persName><forename type="first">K</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine pages</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3929" to="3938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.14992</idno>
		<title level="m">Reasoning with language model is planning with world model</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Few-shot learning with retrieval augmented language models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lomeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dwivedi-Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.03299</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Santhanam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.14024</idno>
		<title level="m">Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Constant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08691</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Retrieval-augmented generation for knowledge-intensive nlp tasks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>K?ttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-T</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rockt?schel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9459" to="9474" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<title level="m">Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>O'brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.03442</idno>
		<title level="m">Generative agents: Interactive simulacra of human behavior</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Measuring and improving bert&apos;s mathematical abilities by predicting the order of reasoning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pi?kos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03921</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Q</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06701</idno>
		<title level="m">Numnet: Machine reading comprehension with numerical reasoning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.04589</idno>
		<title level="m">Memory augmented large language models are computationally universal</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">O</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nakhost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<title level="m">Sql-palm: Improved large language modeladaptation for text-to-sql</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-A</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rozi?re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Azhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<title level="m">Llama: Open and efficient foundation language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Visionllm: Large language model is also an open-ended decoder for vision-centric tasks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.11175</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Self-consistency improves chain of thought reasoning in language models</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.11171</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.01652</idno>
		<title level="m">Finetuned language models are zero-shot learners</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.11903</idno>
		<title level="m">Chain of thought prompting elicits reasoning in large language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Rabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hutchins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.08913</idno>
		<imprint>
			<date type="published" when="2022">2022a</date>
		</imprint>
	</monogr>
	<note type="report_type">Memorizing transformers. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.16773</idno>
		<title level="m">An efficient memory-augmented transformer for knowledge-intensive nlp tasks</title>
		<imprint>
			<date type="published" when="2022">2022b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.02414</idno>
		<title level="m">Glm-130b: An open bilingual pre-trained model</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.12674</idno>
		<title level="m">Training language models with memory augmentation</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
