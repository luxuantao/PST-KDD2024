<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AENet: Learning Deep Audio Features for Video Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Naoya</forename><surname>Takahashi</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Michael</forename><surname>Gygli</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Luc</forename><forename type="middle">Van</forename><surname>Gool</surname></persName>
						</author>
						<title level="a" type="main">AENet: Learning Deep Audio Features for Video Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F747CD500A8FCD4A0BCD6C8194FB49CB</idno>
					<idno type="DOI">10.1109/TMM.2017.2751969</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2751969, IEEE Transactions on Multimedia IEEE TRANSACTION ON MULTIMEDIA 1 This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2751969, IEEE Transactions on Multimedia IEEE TRANSACTION ON MULTIMEDIA</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>convolutional neural network</term>
					<term>audio feature</term>
					<term>large audio event dataset</term>
					<term>large input field</term>
					<term>highlight detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a new deep network for audio event recognition, called AENet. In contrast to speech, sounds coming from audio events may be produced by a wide variety of sources. Furthermore, distinguishing them often requires analyzing an extended time period due to the lack of clear sub-word units that are present in speech. In order to incorporate this long-time frequency structure of audio events, we introduce a convolutional neural network (CNN) operating on a large temporal input. In contrast to previous works this allows us to train an audio event detection system end-to-end. The combination of our network architecture and a novel data augmentation outperforms previous methods for audio event detection by 16%. Furthermore, we perform transfer learning and show that our model learnt generic audio features, similar to the way CNNs learn generic features on vision tasks. In video analysis, combining visual features and traditional audio features such as MFCC typically only leads to marginal improvements. Instead, combining visual features with our AENet features, which can be computed efficiently on a GPU, leads to significant performance improvements on action recognition and video highlight detection. In video highlight detection, our audio features improve the performance by more than 8% over visual features alone.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>A S a vast number of consumer videos have become available, video analysis such as concept classification <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, action recognition [5], <ref type="bibr" target="#b5">[6]</ref> and highlight detection <ref type="bibr" target="#b6">[7]</ref> have become more and more important to retrieve <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> or summarize <ref type="bibr" target="#b10">[11]</ref> videos for efficient browsing. Beside visual information, humans greatly rely on their hearing for scene understanding. For instance, one can determine when a lecture is over, if there is a river nearby, or that a baby is crying somewhere near, only by sound. Audio is clearly one of the key components for video analysis. Many works showed that audio and visual streams contain complementary information <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr">[5]</ref>, e.g. because audio is not limited to the line-of-sight.</p><p>Many efforts have been dedicated to incorporate audio in video analysis by using audio only <ref type="bibr" target="#b2">[3]</ref> or fusing audio and visual information <ref type="bibr">[5]</ref>. In audio-based video analysis, feature extraction remains a fundamental problem. Many types of low level features such as short-time energy, zero crossing This work was mostly done when N.Takahashi was at ETH Zurich. This manuscript greatly extends the work presented at Interspeech 2016 <ref type="bibr" target="#b0">[1]</ref>.</p><p>N. Takahashi is with the System R&amp;D Group, Sony Corporation, Shinagawa-ku, Tokyo, Japan (e-mail: Naoya.Takahashi@sony.com). M. Gygli and L.V. Gool are with the Computer Vision Lab at ETH Zurich, Sternwartstrasse 7, CH-8092 Switzerland (e-mail: gygli@vision.ee.ethz.ch; vangool@vision.ee.ethz.ch).</p><p>Manuscript submitted December 4, 2016.</p><p>rate, pitch, frequency centroid, spectral flax, and Mel Frequency Cepstral Coefficients (MFCC) <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref> have been investigated. These features are very low level or not designed for video analysis, however. For instance, MFCC has originally been designed for automatic speech recognition (ASR), where it attempts to characterize phonemes which last tens to hundreds of milliseconds. While MFCC is often used as an audio feature for the aural detection of events, their audio characteristics differ from those of speech. Such sounds are not always stationary and some audio events could only be detected based on several seconds of sound. Thus, a more discriminative and generic feature set, capturing longer temporal extents, is required to deal with the wide range of sounds occurring in videos.</p><p>Another common method to represent audio signals is the Bag of Audio Words (BoAW) approach <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b3">[4]</ref>, which aggregates frame features such as MFCC into a histogram. BoAW discards the temporal order of the frame level features, thus suffering from considerable information loss.</p><p>Recently, Deep Neural Networks (DNNs) have been very successful at many tasks, including ASR <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>, audio event recognition <ref type="bibr" target="#b0">[1]</ref> and image analysis <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. One advantage of DNNs is their capability to jointly learn feature representations and appropriate classifiers. DNNs have also already been used for video analysis <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, showing promising results. This said, most DNN work on video analysis relies on visual cues only and audio is often not used at all.</p><p>Our work is partially motivated by the success of deep features in vision, e.g. in image <ref type="bibr" target="#b17">[18]</ref> and video <ref type="bibr" target="#b15">[16]</ref> analysis. The features learnt in these networks (activations of the last few layers) have shown to perform well on transfer learning tasks <ref type="bibr" target="#b17">[18]</ref>. Yet, a large and diverse dataset is required so that the learnt features become sufficiently generic and work in a wide range of scenarios. Unfortunately, most existing audio datasets are limited to a specific category, e.g. speech <ref type="bibr" target="#b18">[19]</ref>, music, environmental sounds in offices <ref type="bibr" target="#b19">[20]</ref>).</p><p>There are some datasets for audio event detection such as <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. However, they consist of complex events with multiple sound sources in a class (e.g. the "birthday party" class may contain sounds of voices, hand claps, music and crackers). Features learnt from these datasets are task specific and not useful for generic videos since other classes such as "Wedding ceremony" also would contain the sounds of voices, hand claps or music.</p><p>We generate features dedicated to the more general task of audio event recognition (AER) for video analysis. Therefore, we first created a dataset on which such more general deep audio features can be trained. The dataset consists of various kinds of sound events which may occur in consumer videos. In order to design the classifier, we introduce novel deep convolutional neural network (CNN) architectures with up to 9 layers and a large input field. The large input field allows the networks to directly model several seconds long audio events with time information and be trained end-to-end. The large input field, capturing audio features for video segments, is suitable for video analysis since this is typically conducted on segments several seconds long. Our feature descriptions keep information on the temporal order, something which is lost in most previous approaches <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b22">[23]</ref>.</p><p>In order to train our networks, we further propose a novel data augmentation method, which helps with generalization and boosts the performance significantly. The proposed network architectures show superior performance on AER over BoAW and conventional CNN architectures which typically have up to 3 layers. Finally, we use the learnt networks as feature extractors for video analysis, namely action recognition and video highlight detection. As our experiments confirm, our approach is able to learn generic features that yield a performance superior to that with BoAW and MFCC.</p><p>Our major contributions are as follows. 1) We introduce novel network architectures for AER with up to 9 layers and a large input field which allows the networks to directly model entire audio events and to be trained end-to-end. 2) We propose a data augmentation method which helps to prevent over-fitting. 3) We built an audio event dataset which contains a variety of sound events which may occur in consumer videos.</p><p>The dataset was used to train the networks for generic audio feature extraction. We also make the pre-trained model available so that the research community can easily utilize our proposed features<ref type="foot" target="#foot_0">1</ref> . 4) We conducted experiments on different kinds of consumer video tasks, namely audio event recognition, action recognition and video highlight detection, to show the superior performance and generality of the proposed features. To the best of our knowledge, this is the first work on consumer video highlight detection taking advantage of audio. On all tasks we outperform the state of the art results by leveraging the proposed audio features. A primary version of this work was published as a conference paper <ref type="bibr" target="#b0">[1]</ref>. In this paper, we (i) extend the audio dataset from 28 to 41 classes to learn a more generic and powerful representation for video analysis, (ii) present new experiments using the learnt representation for video analysis tasks and (iii) Show that our features improve performance of action recognition and video highlight detection, compared to using existing audio features such as MFCC.</p><p>The remaining sections of this paper are organized as follows. In section II, related work is reviewed in terms of three aspects: including audio features in video analysis, deep features, and AER. Section III introduces audio feature learning in an AER context, including a new dataset, novel network architectures and a data augmentation strategy. Experimental results for AER, action recognition, and video highlight detection are reported and discussed in Section IV, V and V-F, respectively. Finally, directions for future research and conclusions are presented in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Features for video analysis</head><p>Traditionally, visual video analysis relied on spatio-temporal interest points, described with low-level features such as SIFT, HOG, HOF, etc. <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. Given the current interest in learning deep representations through end-to-end training, several methods using convolutional neural networks (CNN) have been proposed recently. Karpathy et al.introduced a largescale dataset for sports classification in videos <ref type="bibr" target="#b25">[26]</ref>. They investigated ways to improve single frame CNNs by fusing spatial features over multiple frames in time. Wang et al. <ref type="bibr" target="#b26">[27]</ref> combine the trajectory pooling of <ref type="bibr" target="#b24">[25]</ref> with CNN features. The best performance is achieved by combining RGB with motion information obtained through optical flow estimation <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, but this comes at a higher computational cost. A compromise between computational efficiency and performance is offered by C3D <ref type="bibr" target="#b15">[16]</ref>, which uses spatio-temporal 3D convolutions to encode appearance and motion information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Transfer learning</head><p>The success of deep learning is driven, in part, by large datasets such as ImageNet <ref type="bibr">[31]</ref> or Sports1M <ref type="bibr" target="#b25">[26]</ref>. These kinds of datasets are, however, only available in a limited set of research areas. Naturally, the question of whether CNN representations are transferable to other tasks arose <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>. Indeed, as these works have shown, using CNN features trained on ImageNet provides performance improvements in a large array of tasks such as attribute detection or image and object instance retrieval, compared to traditional features <ref type="bibr" target="#b32">[33]</ref>. Several follow-up works have analysed pooling mechanisms for improved domain transfer, e.g. <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>. Video CNN features have also been successfully transferred to other tasks <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b36">[37]</ref>. For this work, we have been inspired by these works and propose deep convolutional features trained on audio event recognition and that are transferable to video analysis. To the best of our knowledge, no other such features exist to date.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Audio Event Recognition</head><p>Our work is also closely related to audio event recognition (AER) since our audio feature learning is based on an AER task.</p><p>Traditional methods for AER apply techniques from ASR directly. For instance, Mel Frequency Cepstral Coefficients (MFCC) were modeled with Gaussian Mixture Models (GMM) or Support Vector Machines (SVM) <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b38">[39]</ref>. Yet, applying standard ASR approaches leads to inferior performance due to differences between speech and non-speech signals. Thus, more discriminative features were developed. Most were hand-crafted and derived from lowlevel descriptors such as MFCC <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b11">[12]</ref>, filter banks <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref> or time-frequency descriptors <ref type="bibr" target="#b42">[43]</ref>. These descriptors are frame-by-frame representations (typically frame length is in the order of tens of ms) and are usually modeled by GMMs to deal with the sounds of entire audio events that normally last seconds at least. Another common method to aggregate frame level descriptors is the Bag of Audio Words (BoAW) approach, followed by an SVM <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b12">[13]</ref>. These models discard the temporal order of the frame level features however, causing considerable information loss. Moreover, methods based on hand-crafted features optimize the feature extraction process and the classification process separately, rather than learning end-to-end.</p><p>Recently, DNN approaches have been shown to achieve superior performance over traditional methods. One advantage of DNNs is their capability to jointly learn feature representations and appropriate classifiers. In <ref type="bibr" target="#b45">[46]</ref>, a fully connected feedforward DNN is built on top of MFCC features. Miquel et al. <ref type="bibr" target="#b46">[47]</ref> utilize a Convolutional Neural Network (CNN) <ref type="bibr" target="#b47">[48]</ref> to extract features from spectrograms. Recurrent Neural Networks are also used on top of low-level features such as MFCCs and fundamental frequency <ref type="bibr" target="#b48">[49]</ref>. These networks are still relatively shallow (e.g. less than 3 layers). The recent success of deeper architectures in image analysis <ref type="bibr" target="#b14">[15]</ref> and ASR <ref type="bibr" target="#b49">[50]</ref> hinges on the availability of large amounts of training data. If a training dataset is small, it is difficult to train deep architectures from scratch in order not to over-fit the training set. Moreover, the networks take only a few frames as input and the complete acoustic events are modeled by Hidden Markov Models (HMM) or simply by calculating the mean of the network outputs, which is too simple to model complicated acoustic event structures.</p><p>Furthermore, these methods are task specific, i.e. the trained networks cannot be used for other tasks. We conclude that there was still a lack a generic way to represent audio signals. Such a generic representation would be very helpful for solving various audio analysis tasks in a unitary way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DEEP AUDIO FEATURE LEARNING A. Large input field</head><p>In ASR, few-frame descriptors are typically concatenated and modeled by a GMM or DNN <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>. This is reasonable since they aim to model sub-word units like phonemes which typically last less than a few hundred ms. The sequence of sub-word units is typically modeled by a HMM. Most works in AER follow similar strategies, where signals lasting from tens to hundreds of ms are modeled first. These small input field representations are then aggregated to model longer signals by HMM, GMM <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref> or a combination of BoAW and SVM <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b12">[13]</ref>. Yet, unlike speech signals, non-speech signals are much more diverse, even within a category, and it is doubtful whether a sub-word approach is suitable for AER. Hence, we decided to design a network architecture that directly models the entire audio event, with signals lasting multiple seconds handled as a single input. This also enables the networks to optimize its parameters end-toend.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Deep Convolutional Network Architecture</head><p>Since we use large inputs, the audio event can occur at any time and last only for a part, as depicted in Table <ref type="table">1</ref>. There the audio event occurs only at the beginning and the end of the input. Therefore, it is not a good idea to model the input with a fully connected DNN since this would induce a very large number of parameters that we could not learn properly. In order to model the large inputs efficiently, we used a CNN <ref type="bibr" target="#b47">[48]</ref> to leverage its translation invariant nature, suitable to model such larger inputs. CNNs have been successfully applied to the audio domain, including AER <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b46">[47]</ref>. The convolution layer has kernels with a small receptive field which are shared across different positions in the input and extract local features. As stacking convolution layers, the receptive field of deeper layer covers larger area of input field. We also apply convolution to the frequency axis to deal with pitch shifts, which are shown to be effective for speech signals <ref type="bibr" target="#b55">[56]</ref>. Our network architecture is inspired by VGG Net <ref type="bibr" target="#b14">[15]</ref>, which obtained the second place in the ImageNet 2014 competition and was successfully applied for ASR <ref type="bibr" target="#b49">[50]</ref>. The main idea of VGG Net is to replace large (typically 9×9) convolutional kernels by a stack of 3×3 kernels without pooling between these layers. Advantages of this architecture are (1) additional nonlinearities, hence more expressive power, and (2) a reduced number of parameters (i.e. one 9×9 convolution layer with C maps has 9 2 C 2 = 81C 2 weights while a three-layer 3×3 convolution stack has 3(3 2 C 2 ) = 27C 2 weights). We have investigated many types of architectures, including the number of layers, pooling sizes, and the number of units in fully connected layers, to adapt the VGG Net of the image domain to AER. As a result, we propose two architectures as outlined in Table <ref type="table" target="#tab_0">I</ref>. Architecture A has 4 convolutional and 3 fully connected layers, while Architecture B has 9 weight layers: 6 convolutional and 3 fully connected. In this table, the convolutional layers are described as conv(input feature maps, output feature maps). All convolutional layers have 3×3 kernels, thus henceforth kernel size is omitted. The convolution stride is fixed to 1. The max-pooling layers are indicated as time × f requency in Table <ref type="table" target="#tab_0">I</ref>. They have a stride equal to the pool size. Note that since the fully connected layers are placed on top of the convolutional and pooling layers, the input size to the fully connected layer is much smaller than that of the input to the CNN, hence it is much easier to train these fully connected layers. All hidden layers except the last fully-connected layer are equipped with the Rectified Linear Unit (ReLU) non-linearity. In contrast to <ref type="bibr" target="#b14">[15]</ref>, we do not apply zero padding before convolution, since the output size of the last pooling layer is still large enough in our case. The networks were trained by minimizing the cross entropy loss L with l 1 regularization using back-propagation:</p><formula xml:id="formula_0">arg min W i,j L(x i j , y i j , W ) + ρ W 1 (1)</formula><p>where x j is the jth input vector, y j is the corresponding class label and W is the set of network parameters, respectively. ρ is a constant parameter which is set to 10 -6 in this work.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Data Augmentation</head><p>Since the proposed CNN architectures have many hidden layers and a large input, the number of parameters is high, as shown in the last row of Table <ref type="table" target="#tab_0">I</ref>. A large number of training data is vital to train such networks. Jaitly et al. <ref type="bibr" target="#b56">[57]</ref> showed that data augmentation based on Vocal Tract Length Perturbation (VTLP) is effective to improve ASR performance. VTLP attempts to alter the vocal tract length during the extraction of descriptors, such as a log filter bank, and perturbs the data in a certain non-linear way. In order to introduce more data variation, we propose a different augmentation technique. For most sounds coming with an event, mixed sounds from the same class also belong to that class, except when the class is differentiated by the number of sound sources. For example, when mixing two different ocean surf sounds, or of breaking glass, or of birds tweeting, the result still belongs to the same class. Given this property we produce augmented sounds by randomly mixing two sounds of a class, with randomly selected timings. In addition to mixing sounds, we further perturb the sound by moderately modifying frequency characteristics of each source sound by boosting/attenuating a particular frequency band to introduce further varieties while keeping the sound recognizable. An augmented data sample s aug is generated from source signals for the same class as the one both s 1 and s 2 belong to, as follows:</p><formula xml:id="formula_1">s aug = αΦ(s 1 (t), ψ 1 ) + (1 -α)Φ(s 2 (t -βT ), ψ 2 ) (2)</formula><p>where α, β ∈ [0, 1) are uniformly distributed random values, T is the maximum delay and Φ(•, ψ) is an equalizing function parametrized by ψ. In this work, we used a second order parametric equalizer parametrized by ψ = (f 0 , g, Q) where</p><formula xml:id="formula_2">f 0 ∈ [100, 6000] is the center frequency, g ∈ [-8, 8] is a gain and Q ∈ [1, 9</formula><p>] is a Q-factor which adjusts the bandwidth of a parametric equalizer. An arbitrary number of such synthetic samples can be obtained by randomly selecting the parameters α, β, ψ for each data augmentation. This data augmentation helps networks to be more robust and thus generalize to moderate fluctuation of the number of sources, loudness, and sound color. We refer to this approach as Equalized Mixture Data Augmentation (EMDA).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Dataset</head><p>In order to learn a discriminative and universal set of audio features, a dataset on which the feature extraction network is trained needs to be carefully designed. If the dataset contains only a small number of audio event classes, the learned features could not be discriminative. Another concern is that the learned features would be too task specific if the target classes are defined at too high a semantic level (e.g. Birthday Party or Repairing an Appliance), as such events would present the system with rather typical mixtures of very different sounds. Therefore, we design the target classes according to the following criteria: 1) The target classes cover as many audio events which may happen in consumer videos as possible, 2) The sound events should be atomic (no composed events) and non-overlapping. As a counterexample, "Birthday Party" may consist of Speech, Cracker Explosions and Applause, so it is not suitable. 3) Then again, the subdivision of event classes should also not made too fine-grained. This will also higher the chance that a sufficiently large number of samples can be collected. For instance, "Church Bell" is better not subdivided further, e.g. in terms of its pitch.</p><p>In order to create such a novel audio event classification database, we harvested samples from Freesound <ref type="bibr" target="#b57">[58]</ref>. This is a repository of audio samples uploaded by users. The database consists of 28 events as described in Table <ref type="table" target="#tab_1">II</ref> <ref type="foot" target="#foot_1">2</ref> . Note that since the sounds in the repository are tagged in free-form style and the words used vary a lot, the harvested sounds contain irrelevant sounds. For instance, a sound tagged 'cat' sometime does not contain a real cat meow, but instead a musical sound produced by a synthesizer. Furthermore sounds were recorded with various devices under various conditions (e.g. some sounds are very noisy and in others the audio event occurs during a short time interval between longer silences). This makes our database more challenging than previous datasets such as <ref type="bibr" target="#b58">[59]</ref>. On the other hand, the realism of our selected sounds helps us to train our networks on sounds similar to those in actual consumer videos. With the above goals in mind we extended this initial freesound dataset, which we introduced in [1]), to 41 classes, including more diverse classes from the RWCP Sound Scene Database <ref type="bibr" target="#b58">[59]</ref>.</p><p>In order to reduce the noisiness of the data, we first normalized the harvested sounds and eliminated silent parts. If a sound was longer than 12 sec, we split the sound into pieces so that the split sounds lasted shorter than 12 sec. All audio samples were converted to 16 kHz sampling rate, 16 bits/sample, mono channel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Multiple Instance Learning</head><p>Since we used web data to build our dataset (see Sec. III-D), the training data is expected to be noisy and to contain outliers. In order to alleviate the negative effects of outliers, we also employed multiple instance learning (MIL) <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b60">[61]</ref>. In MIL, data is organized as bags {X i } and within each bag there are a number of instances {x ij }. Labels {Y i } are provided only at the bag level, while labels of instances {y ij } are unknown. A positive bag means that at least one instance in the bag is positive, while a negative bag means that all instances in the bag are negative. We adapted our CNN architecture for MIL as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. N instances {x 1 , • • • , x N } in a bag are fed to a replicated CNN which shares its parameters. The last softmax layer is replaced with an aggregation layer where the outputs from each network h = {h ij } ∈ R M ×N are aggregated. Here, M is the number of classes. The distribution of class of bag p i is calculated as</p><formula xml:id="formula_3">p i = f (h i1 , h i2 , • • • , h iN )</formula><p>where f () is an aggregation function. In this work, we investigate 2 aggregation functions: max aggregation</p><formula xml:id="formula_4">p i = exp( ĥi ) i exp( ĥi ) (3) ĥi = max j (h ij )<label>(4)</label></formula><p>and Noisy OR aggregation <ref type="bibr" target="#b61">[62]</ref>, Since it is unknown which sample is an outlier, we can not be sure that a bag has at least one positive instance. However, the probability that all instances in a bag are negative exponentially decreases with N , thus the assumption becomes very realistic.</p><formula xml:id="formula_5">p i = 1 - j (1 -p ij )<label>(5)</label></formula><formula xml:id="formula_6">p ij = exp(h ij ) i exp(h ij ) . (<label>6</label></formula><formula xml:id="formula_7">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. ARCHITECTURE VALIDATION AND AUDIO EVENT RECOGNITION</head><p>We first evaluated our proposed deep CNN architectures and data augmentation method on the audio event recognition task. The aim here is to validate the proposed method and find an appropriate network architecture, since we can assume that a network that is more discriminative for the audio event recognition task gives us more discriminative AENet features for the other video analysis tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation details</head><p>Through all experiments, 49 band log-filter banks, logenergy and their delta and delta-delta were used as a low-level descriptor, using 25 ms frames with 10 ms shift, except for the BoAW baseline described in Sec. IV-B. The input patch length was set to 400 frames (i.e. 4 sec). The effects of this length were further investigated in Sec. IV-C. During training, we randomly crop 4 sec for each sample. The networks were trained using mini-batch gradient descent based on back propagation with momentum. The learning rate was initially set to 0.001 and was reduced by a factor of 10 when the training error plateaued. The networks were trained for up to 5 ×10 4 iterations. We applied dropout <ref type="bibr" target="#b62">[63]</ref> to each fullyconnected layer with as keeping probability 0.5. The batch size was set to 128, the momentum to 0.9. For data augmentation we used VTLP and the proposed EMDA. The number of augmented samples is balanced for each class. During testing, 4 sec patches with 50% shift were extracted and used as input to the Neural Networks. The class with the highest probability was considered the detected class. The models were implemented using the Lasagne library <ref type="bibr" target="#b63">[64]</ref>.</p><p>Similar to <ref type="bibr" target="#b54">[55]</ref>, the data was randomly split into training set (75%) and test set (25%). Only the test set was manually checked and irrelevant sounds not containing the target audio event were omitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. State-of-the-art comparison</head><p>In our first set of experiments we compared our proposed deeper CNN architectures to three different state-of-the-art baselines, namely, BoAW <ref type="bibr" target="#b11">[12]</ref>, HMM+DNN/CNN as in <ref type="bibr" target="#b64">[65]</ref>, and a classical DNN/CNN with large input field.</p><p>BoAW We used MFCC with delta and delta-delta as lowlevel descriptor. K-means clustering was applied to generate an audio word code book with 1000 centers. We evaluated both a SVM with a χ 2 kernel and a 4 layer DNN as classifiers. The layer sizes of the DNN classifier were <ref type="bibr">(1024,</ref><ref type="bibr">256,</ref><ref type="bibr">128,</ref><ref type="bibr" target="#b27">28)</ref>. DNN/CNN+HMM We evaluated the DNN-HMM system. The neural network architectures are described in the left 2 columns of Table <ref type="table" target="#tab_0">I</ref>. Both the DNN and CNN models are trained to estimate HMM state posteriors. The HMM topology consists of one state per audio event, and an ergodic architecture in which all states have equal transitions probabilities to all states, as in <ref type="bibr" target="#b46">[47]</ref>. The input patch length for the CNN/DNN is 30 frames with 50% shift. DNN/CNN+Large input field In order to evaluate the effect of using the proposed CNN architectures, we also evaluated the baseline DNN/CNN architectures with the same large field, namely, 400 frame patches. The classification accuracies of these systems -trained with and without data augmentation -are shown in Table <ref type="table" target="#tab_2">III</ref>. Even without data augmentation, the proposed CNN architectures outperform all previous methods. Furthermore, the performance is significantly improved by applying data augmentation, yielding a 12.5% improvement for the architecture. The best result was obtained by the B architecture with data augmentation. It is important to note that the B architecture outperforms the classical DNN/CNN even though it has fewer parameters, as shown in Table <ref type="table" target="#tab_0">I</ref>. This result corroborates the efficiency of deeper CNNs with small kernels for modelling large input fields. This observation coincides with that made in earlier work in computer vision in <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Effectiveness of a large input field</head><p>Our second set of experiments focuses on input field size. We tested our CNN with different patch size 50, 100, 200, 300, 400 frames (i.e. from 0.5 to 4 sec). The B architecture was used for this experiment. As a baseline we evaluated the CNN+HNN system described in Sec. IV-B but using our architecture B, rather than a classical CNN. The performance improvement over the baseline is shown in Fig. <ref type="figure">3</ref>. The result shows that larger input fields improve the performance. Especially the performance with patch length less than 1 sec sharply drops. This proves that modeling long signals directly with a deeper CNN is superior to handling long sequences with HMMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Effectiveness of data augmentation</head><p>We verified the effectiveness of our EMDA data augmentation method in more detail. We evaluated 3 types of data augmentation: EMDA only, VTLP only, and a mixture of EMDA and VTLP (50%, 50%) with different numbers of augmented samples 10k, 20k, 30k, 40k. Fig. <ref type="figure">4</ref> shows that using both EDMA and VTLP always outperforms EDMA or VTLP only. This shows that EDMA and VTLP perturb the original data and thus create new samples in a different way (VTLP changes speed or pitch while EMDA changes number of sources, loudness, and sound color.). Applying both provides a more effective variation of data and helps to train the network to learn a more robust and general model from a limited amount of data.</p><p>1520-9210 (c) 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2751969, IEEE Transactions on Multimedia</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Effects of Multiple Instance Learning</head><p>The A and B architectures with a large input field were adapted to MIL, to handle the noise in the database. The number of parameters were identical since both the max and Noisy OR aggregation methods are parameter-free. The number of instances in a bag was set to 2. We randomly picked 2 instances from the same class during each epoch of the training. Table <ref type="table" target="#tab_3">IV</ref> shows that MIL didn't improve performance in this case. However, MIL with a medium size input field (i.e. 2 sec) performs as good as or even slightly better than single instance learning with a large input field. This is perhaps due to the fact that the MIL took the same size input length (2 sec ×2 instances = 4 sec), while it had fewer parameter. Thus it managed to learn a more robust model. We also tried training the networks with 4 instances in a bag with 2 sec input field. However, we could not observe improved performance compared to using 2 instances. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. VIDEO ANALYSIS USING AENET FEATURES A. Audio Event Net Feature</head><p>Once the network was trained, it can be used as a feature extractor for audio and video analysis tasks. An audio stream is split into clips whose lengths are equal to the length of the network's input field. In our experiments, we took a 2 sec length (200 frame) patch since it did not degrade the performance considerably (Fig. <ref type="figure">3</ref>) but gave us a reasonable time resolution with easily affordable computational complexity. Through our experiment we split audio streams with 50% overlap, although clips can be overlapped with arbitrary length depending on the desired temporal resolution. These clips are fed into the network architecture "A" and activations of the second last fully connected layer are extracted. The activations are then L2 normalized to form audio features. We call these features 'AENet features' from now on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Action recognition</head><p>We evaluated the AENet features on the USF101 dataset <ref type="bibr" target="#b65">[66]</ref>. This dataset consists of 13,320 videos of 101 human action categories, such as Apply Eye Makeup, Blow Dry Hair and Table <ref type="table" target="#tab_0">Tennis</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Baselines</head><p>The AENet features were compared with several baselines: visual only and with two commonly used audio features, namely MFCC and BoAW. Thirteen-dimensional MFCCs and its delta and delta delta were extracted, with 25 ms window with 10 ms shift and averaged for a clip. In order to form BoAW features, MFCC, delta and delta delta were clustered by K-means to obtain 1000 codebook elements. The audio features are then concatenated with the visual features. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Setup</head><p>The AENet features were averaged within a clip. We did not fine-tune the network since our goal is to show the generality of the AENet features. For all experiments, we use a multi-class SVM classifier with a linear kernel for fair comparison. As visual features, we used C3D features <ref type="bibr" target="#b15">[16]</ref> since it is a standard method for fast video analysis. More recent methods, such as <ref type="bibr" target="#b29">[30]</ref> have higher performance, but have a high computational cost, as they rely on optical flow inputs. We observed that half of the videos in the dataset contain no audio. Thus, in order to focus on the effect of the audio features, we used only videos that do contain audio. This resulted in 6837 videos of 51 categories. We used the three split setting provided with this dataset and report the averaged performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Results</head><p>The action recognition accuracy of each feature set are presented in Table <ref type="table" target="#tab_4">V</ref>. The results show that the proposed AENet features significantly outperform all baselines. Using MFCC to encode audio on the other hand, does not lead to any considerable performance gain over visual features only. One difficulty of this dataset could be that the characteristic sounds for certain actions only occur very sparsely or that sounds are very similar, thus making it difficult to characterize sound tracks by averaging or taking the histogram of frame-based MFCC features. AENet features, on the other hand, perform well without fine-tuning. This suggests that AENet learned more discriminative and general audio representations.</p><p>In order to further investigate the effect of AENet features, we show the difference between the confusion matrices when using C3D vs C3D+AENet in Table <ref type="table">5</ref>. Positive diagonal values indicate an improvement in classification accuracy for the corresponding classes, whereas positive values on offdiagonal elements indicate increased mis-classification. The class indices were ordered according to descending accuracy gain. The Fig. <ref type="figure" target="#fig_3">5</ref> shows that the performance was improved or remains the same for most classes by using AENet features. The off-diagonal elements of the confusion matrix difference also show some interesting properties, e.g. the confusion of Playing Dhal (index 8) and Playing Cello (index 10) was descreased by adding the AENet features. This may be due to the clear difference of the cello and Dhal sounds while their visual appearance is sometimes similar: a person holding a brownish object in the middle and moving his hands arround the object. The confusion between Playing Cello and Playing Daf (index 2) , on the other hand, was slightly increased by using AENet features, since both are percussion instruments and the sound from these instruments may be reasonably similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predicted class</head><note type="other">Playing Daf Playing Dhol Playing Cello Brushing Teeth Typing Haircut Table Tennis Shot Field Hockey Penalty</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Video Highlight Detection</head><p>We further investigate the effectiveness of AENet features for finding better highlights in videos. Thereby the goal is to find domain-specific highlight segments <ref type="bibr" target="#b6">[7]</ref> in long consumer videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Dataset</head><p>The dataset consists of 6 domains, skating, gymnastics, dog, parkour, surfing, and skiing. Each domain has about 100 videos with various lengths, harvested from Youtube. The total accumulated time is 1430 minutes. The dataset was split in half for training and testing. Highlights for the training set were automatically obtained by comparing raw and edited pairs of videos. The segments (moments) contained in the edited videos are labeled as highlights while moments only appearing in the raw videos are labeled as non-highlights. See <ref type="bibr" target="#b6">[7]</ref> for more information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Setup</head><p>If a moment contained multiple features, they were averaged within the moment. We used the C3D features for the visual appearance and concatenated then with AENet features. A Hfactor y was estimated by neural networks which had two hidden layers and one output unit. A higher H-factor value indicates highlight moments, while a lower value indicates a non-highlight moment, as in <ref type="bibr" target="#b6">[7]</ref>. Note that the classification model can not be applied since highlights are not comparable among videos: a highlight in one video may be boring compared to a non-highlight moment in other videos. A training objective which only depends on the relative 'highlightness' of moments from a video is more suitable. Therefore, we followed <ref type="bibr" target="#b6">[7]</ref> and used a ranking loss</p><formula xml:id="formula_8">L ranking = i max(1 -y pos i + y neg i )<label>(7)</label></formula><p>where {y pos } and {y neg } are the outputs of the networks for the highlight moments and non-highlight moments of a video. Eq. ( <ref type="formula" target="#formula_8">7</ref>) required the network to score highlight moments higher than non-highlight moments within a video, but does not put constraints on the absolute values of the scores. Since all moments are labeled as a highlight if the moments are included in the edited video, the label tends to be redundant and noisy. To overcome this, we modified the ranking loss by applying the Huber loss [37]</p><formula xml:id="formula_9">L Huber = 1/2L 2 ranking , if L ranking &lt; δ δ(-L ranking + 1/2δ), otherwise<label>(8)</label></formula><p>and further by replacing the ranking loss by a multiple instance ranking loss</p><formula xml:id="formula_10">L miranking = max(1 -max i (y pos i ) + y neg ).<label>(9)</label></formula><p>The Huber loss has a smaller gradient for margin violations, as long as the positive example scores are higher than the negative, which alleviates the effect from ambiguous samples and leads to a more robust model. Eq. ( <ref type="formula" target="#formula_10">9</ref>) takes I highlight moments {y pos i |i = 1, ..., I} and requires only the highest scoring segment among them to rank higher than the negative. It is thus more robust to false positive samples, which exist in the training data, due to the way it was collected <ref type="bibr" target="#b6">[7]</ref>. We used I = 2 in our experiment and the network was trained five times and the scores were averaged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Baselines</head><p>As for action recognition, we consider three baselines, C3D features only, C3D with MFCC, and BoAW. MFCC features were averaged within a moment and the BoAW was calculated for each moment. A DNN based highlight detector was trained in the same manner as the ranking loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J. Results</head><p>The mean average precisions (mAP) of each domain, averaged over all videos on the test set, are presented in figure <ref type="figure" target="#fig_4">6</ref>. For most of the domains, AENet features perform the best or are competitive with the best competing features. The overall performance of AENet features significantly outperforms the baselines, achieving 56.6% mAP which outperforms the current state-of-the-art of <ref type="bibr" target="#b6">[7]</ref>. For skating and surfing, all audio features help to improve performance, probably due to the fact that videos of these domains contain characteristic sounds at highlight moments: when a skater performs some stunt or a surfer starts to surf. For skiing and parkour, AENet features improve performance while some other features do not. In the parkour domain, pulsive sounds such as foot steps which may  occur when a player jumps, typically characterize the highlights. The MFCC features might have failed to capture the characteristics of the foot step sound because of the averaging of the features within the moment. BoAW features could keep the characteristics by taking a histogram, but AENet features are far better to characterize such pulsive sounds. For dog and gymnastics, audio features do not improve performance or even slightly lower it. We observed that many videos in these domains do not contain any sounds which characterize the highlights, but contain constant noise or silence for the entire video. This may cause over-fitting to the training data. We further investigated the effects of loss functions. Table <ref type="table" target="#tab_6">VI</ref> shows the mAPs trained with the ranking loss in Eq. ( <ref type="formula" target="#formula_8">7</ref>), Huber loss in Eq. ( <ref type="formula" target="#formula_9">8</ref>) and the multiple instance ranking loss (MIRank) in Eq. ( <ref type="formula" target="#formula_10">9</ref>). The Huber loss and MIRank both increase the performance by 1.2% and 2.4%, respectively. This shows that more robust loss functions help in this scenario, where the labels are affected by noise and contain false positives.</p><p>Qualitative evaluation: In figure <ref type="figure">7</ref>, 8 and 9, we illustrate some typical examples of highlight detection results for the domains parkour, skating and surfing, i.e. the domains that were most improved by introducing AENet features. The last two rows give examples of highlight videos which were created by taking the moments with the highest H-factors so that the video length would about 20% of original video. In the video of parkour shown in figure <ref type="figure">7</ref>, a higher H-factor was assigned around the moments in which a man was running, jumping and turning a somersault, when we used AENet features, as shown in the second row. On the other hand, the moments which clearly show the man in the video and have less camera motion tend to get a higher H-factor when only visual (C3D) features were used. The AENet features could characterize a footstep sound and therefore detected the highlights more reliably. Figure <ref type="figure">8</ref> illustrates a video with seven jumping scenes. With the AENet features, we can observe peaks in the Hfactor at all jumps, since AENet features effectively capture the sound made by a skater jumping. Without audio information, the highlight detector failed to detect some jumping scenes and tended to pick moments with general motion including camera motion. For surfing, highlight videos created by using AENet features capture the whole sequence from standing on the board up to falling into the sea, while a highlight video created from visual features only sometimes misses some sequence of surfing and contains boring parts showing somebody just floating and waiting for the next wave. By including AENet features, the system really recognizes the difference in sound when somebody is surfing or not, and it detects highlights more reliably. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>We proposed a new, scalable deep CNN architecture to learn a model for entire audio events end-to-end, and outperforming the state-of-the-art on this task. Experimental results showed that deeper networks with smaller filters perform better than previously proposed CNNs and other baselines. We further proposed a data augmentation method that prevents over-fitting and leads to superior performance even when the training data is limited. We used the learned network activations as audio features for video analysis and showed that they generalize well. Using the proposed features led to superior performance on action recognition and video highlight detection, compared to commonly used audio features. We believe that our new audio features will also give similar improvements for other video analysis tasks, such as action localization and temporal video segmentation.Investigating recently proposed architectures such as ResNet will be future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Our deeper CNN models several seconds of audio directly and outputs the posterior probability of classes.</figDesc><graphic coords="4,79.23,56.07,453.55,83.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Architecture of our deeper CNN model adapted to MIL. The softmax layer is replaced with an aggregation layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Fig.3. Performance of our network for different input patch lengths. The plot shows the increase over using a CNN+HMM with a small input field of 30 frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Difference of confusion matrices of C3D+AENet and C3D only. Positive diagonal values indicate a performance improvement for this class, while negative, off-diagonal values indicate that the mis-classification increased. The performance was improved or remains the same for most classes by using AENet features.</figDesc><graphic coords="8,70.72,101.12,228.63,181.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Domain specific highlight detection results. AENet features outperform other audio features with an average improvement of 8.6% over the C3D (visual) features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I</head><label>I</label><figDesc>The architecture of our deeper CNNs. Unless mentioned explicitly convolution layers have 3×3 kernels. The input size of each model is discussed in Sec IV-C.</figDesc><table><row><cell></cell><cell></cell><cell>Baseline</cell><cell cols="2">Proposed CNN</cell></row><row><cell>#Fmap</cell><cell>DNN</cell><cell>Classic CNN</cell><cell>A</cell><cell>B</cell></row><row><cell>64</cell><cell></cell><cell>conv5×5 (3,64)</cell><cell>conv(3,64)</cell><cell>conv(3,64)</cell></row><row><cell></cell><cell></cell><cell>pool 1×3</cell><cell>conv(64,64)</cell><cell>conv(64,64)</cell></row><row><cell></cell><cell></cell><cell>conv5×5(64,64)</cell><cell>pool 1×2</cell><cell>pool 1×2</cell></row><row><cell>128</cell><cell></cell><cell></cell><cell>conv(64,128)</cell><cell>conv(64,128)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>conv(128,128)</cell><cell>conv(128,128)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>pool 2×2</cell><cell>pool 2×2</cell></row><row><cell>256</cell><cell></cell><cell></cell><cell></cell><cell>conv(128,256)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>conv(128,256)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>pool 2×1</cell></row><row><cell>FC</cell><cell>FC4096</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>FC2048</cell><cell>FC1024</cell><cell>FC1024</cell><cell>FC2048</cell></row><row><cell></cell><cell>FC2048</cell><cell>FC1024</cell><cell>FC1024</cell><cell>FC2048</cell></row><row><cell></cell><cell>FC28</cell><cell>FC28</cell><cell>FC28</cell><cell>FC28</cell></row><row><cell></cell><cell></cell><cell cols="2">softmax</cell><cell></cell></row><row><cell cols="2">#param 258×10 6</cell><cell>284×10 6</cell><cell>233×10 6</cell><cell>257×10 6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II</head><label>II</label><figDesc>The statistics of the dataset.</figDesc><table><row><cell>Class</cell><cell>Total</cell><cell># clip</cell><cell>Class</cell><cell>Total</cell><cell># clip</cell></row><row><cell></cell><cell>minutes</cell><cell></cell><cell></cell><cell>minutes</cell><cell></cell></row><row><cell cols="2">Acoustic guitar 23.4</cell><cell>190</cell><cell>Hammer</cell><cell>42.5</cell><cell>240</cell></row><row><cell>Airplane</cell><cell>37.9</cell><cell>198</cell><cell>Helicopter</cell><cell>22.1</cell><cell>111</cell></row><row><cell>Applause</cell><cell>41.6</cell><cell>278</cell><cell>Knock</cell><cell>10.4</cell><cell>108</cell></row><row><cell>Bird</cell><cell>46.3</cell><cell>265</cell><cell>Laughter</cell><cell>24.7</cell><cell>201</cell></row><row><cell>Car</cell><cell>38.5</cell><cell>231</cell><cell cols="2">Mouse click 14.6</cell><cell>96</cell></row><row><cell>Cat</cell><cell>21.3</cell><cell>164</cell><cell cols="2">Ocean surf 42</cell><cell>218</cell></row><row><cell>Child</cell><cell>19.5</cell><cell>115</cell><cell>Rustle</cell><cell>22.8</cell><cell>184</cell></row><row><cell>Church bell</cell><cell>11.8</cell><cell>71</cell><cell>Scream</cell><cell>5.3</cell><cell>59</cell></row><row><cell>Crowd</cell><cell>64.6</cell><cell>328</cell><cell>Speech</cell><cell>18.3</cell><cell>279</cell></row><row><cell>Dog barking</cell><cell>9.2</cell><cell>113</cell><cell>Squeak</cell><cell>19.8</cell><cell>173</cell></row><row><cell>Engine</cell><cell>47.8</cell><cell>263</cell><cell>Tone</cell><cell>14.1</cell><cell>155</cell></row><row><cell>Fireworks</cell><cell>43</cell><cell>271</cell><cell>Violin</cell><cell>16.1</cell><cell>162</cell></row><row><cell>Footstep</cell><cell>70.3</cell><cell>378</cell><cell>Water tap</cell><cell>30.2</cell><cell>208</cell></row><row><cell cols="2">Glass breaking 4.3</cell><cell>86</cell><cell>Whistle</cell><cell>6</cell><cell>78</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Total</cell><cell>768.4</cell><cell>5223</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III</head><label>III</label><figDesc>Accuracy of the deeper CNN and baseline methods, trained with and without data augmentation (%).</figDesc><table><row><cell></cell><cell cols="2">Data augmentation</cell></row><row><cell>Method</cell><cell>without</cell><cell>with</cell></row><row><cell>BoAW+SVM</cell><cell>74.7</cell><cell>79.6</cell></row><row><cell>BoAW+DNN</cell><cell>76.1</cell><cell>80.6</cell></row><row><cell>DNN+HMM</cell><cell>54.6</cell><cell>75.6</cell></row><row><cell>CNN+HMM</cell><cell>67.4</cell><cell>86.1</cell></row><row><cell>DNN+Large input</cell><cell>62.0</cell><cell>77.8</cell></row><row><cell>CNN+Large input</cell><cell>77.6</cell><cell>90.9</cell></row><row><cell>A</cell><cell>77.9</cell><cell>91.7</cell></row><row><cell>B</cell><cell>80.3</cell><cell>92.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV</head><label>IV</label><figDesc>Accuracy of MIL and normal training (%).</figDesc><table><row><cell></cell><cell>Single</cell><cell></cell><cell>MIL</cell><cell></cell></row><row><cell cols="5">Architecture instance Noisy OR Max Max (2sec)</cell></row><row><cell>A</cell><cell>91.7</cell><cell>90.4</cell><cell>92.6</cell><cell>92.9</cell></row><row><cell>B</cell><cell>92.8</cell><cell>91.3</cell><cell>92.4</cell><cell>92.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V</head><label>V</label><figDesc>Accuracy of the deeper CNN and baseline methods, trained with and without data augmentation (%).</figDesc><table><row><cell>Method</cell><cell>accuracy</cell></row><row><cell>C3D</cell><cell>82.2</cell></row><row><cell>C3D+MFCC</cell><cell>82.5</cell></row><row><cell>C3D+BoAW</cell><cell>82.9</cell></row><row><cell>C3D+AENet</cell><cell>85.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>1520-9210 (c) 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2751969, IEEE Transactions on Multimedia</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI</head><label>VI</label><figDesc>Effects of loss function. Mean average precision trained with different loss functions.</figDesc><table><row><cell>Method</cell><cell>mAP</cell></row><row><cell>Sum et al. [7]</cell><cell>53.6</cell></row><row><cell>C3D+AENet ranking loss</cell><cell>53.0</cell></row><row><cell>C3D+AENet Huber loss</cell><cell>54.2</cell></row><row><cell cols="2">C3D+AENet Huber loss MIRank 56.6</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The model is available at https://github.com/znaoya/aenet</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The dataset is available at https://data.vision.ee.ethz.ch/cvl/ae dataset</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1520-9210 (c) 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2751969, IEEE Transactions on Multimedia IEEE TRANSACTION ON MULTIMEDIA</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep Convolutional Neural Networks and Data Augmentation for Acoustic Event Detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Audio content analysis for online audiovisual data segmentation and classification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C Jay</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="441" to="457" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Audio-based semantic concept classification for consumer video</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1406" to="1416" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Detecting semantic concepts in consumer videos using audio</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="2279" to="2283" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Realistic human action recognition with multimodal feature selection and fusion</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics Part A:Systems and Humans</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="875" to="885" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Action and event recognition with fisher vectors on a compact feature set</title>
		<author>
			<persName><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013-12">December 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ranking domain-specific highlights by analyzing edited videos</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Indexing , Filtering , and Retrieval</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Naphade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="151" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A Survey on Visual Content-Based Video Indexing and Retrieval</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics, Part C</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="797" to="819" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Exploring audio semantic concepts for event-based video retrieval</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1360" to="1364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Video summarization by learning submodular mixtures of objectives</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bag-of-Audio-Words Approach for Multimedia Event Classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pancoast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Akbacak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech<address><addrLine>Portland, OR, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improved audio features for largescale multimedia event detection</title>
		<author>
			<persName><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICME</title>
		<meeting>ICME</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning Spatiotemporal Features with 3D Convolutional Networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Beyond Short Snippets: Deep Networks for Video Classification</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-H</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The DARPA speech recognition research database: Specifications and status</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Doddington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Goudie-Marshall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. DARPA Workshop on Speech Recognition</title>
		<meeting>DARPA Workshop on Speech Recognition</meeting>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="93" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The CHIL audiovisual corpus for lecture and meeting analysis inside smart rooms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mostefa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Choukri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Potamianos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Turmo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cristoforetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tobia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pnevmatikakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mylonakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Talantzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bernardin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rochet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page">389407</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">2011 trecvid multimedia event detection evalua-tion plan</title>
		<author>
			<persName><surname>Nist</surname></persName>
		</author>
		<ptr target="http://www.nist.gov/itl/iad/mig/upload/MED11-EvalPlan-V03-20110801a.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Trecvid 2013 an introduction to the goals, tasks, data, eval-uation mechanisms, and metrics</title>
		<author>
			<persName><forename type="first">P</forename><surname>Over</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fiscus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sanders</surname></persName>
		</author>
		<ptr target="http://www-nlpir.nist.gov/projects/tv2013/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Audio-based context recognition</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Eronen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">T</forename><surname>Peltonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Tuomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Klapuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fagerlund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sorsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lorho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huopaniemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="321" to="329" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Action recognition by dense trajectories</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3169" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Action recognition with trajectorypooled deep-convolutional descriptors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Convolutional twostream network fusion for video action recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.06573</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02155</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cnn features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR Workshops</title>
		<meeting>CVPR Workshops</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="806" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-scale orderless pooling of deep convolutional activation features</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="392" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep filter banks for texture recognition and segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Video2gif: Automatic generation of animated gifs from video</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A blind segmentation approach to acoustic event detection based on I-vector</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hautamäki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2282" to="2286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Comparison of sequence discriminant support vector machines for acoustic event classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Temko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Monte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nadeu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="721" to="724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Representing nonspeech audio signals through speech classification models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hertel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mazur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mertins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3441" to="3445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Acoustic event recognition using dominant spectral basis vectors</title>
		<author>
			<persName><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2002" to="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Scalable identification of mixed environmental sounds, recorded from heterogeneous sources</title>
		<author>
			<persName><forename type="first">J</forename><surname>Beltrán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chávez</surname></persName>
		</author>
		<author>
			<persName><surname>Favela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="153" to="160" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Environmental sound recognition with time frequency audio features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1142" to="1158" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Sparse representation with temporal max-smoothing for acoustic event detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kawai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1176" to="1180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Robust sound event classification using LBP-HOG based bag-of-audio-words feature representation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3325" to="3329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Audio-based multimedia event detection with DNNs and Sparse Sampling Categories and Subject Descriptors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bernd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICMR</title>
		<meeting>ICMR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="611" to="614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Exploiting spectro-temporal locality in deep learning based acoustic event detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Espi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kinoshita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nakatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Audio, Speech, and Music Processing</title>
		<imprint>
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="issue">26</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Gradient based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE</title>
		<meeting>of the IEEE</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Audio-based multimedia event detection using deep recurent neural nework</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Yun Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonardo</forename><surname>Neves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="2742" to="2746" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Very deep multilingual convolutional neural networks for LVCSR</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4955" to="4959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Applying convolutional neural networks concepts to hybrid NN-HMM model for Speech Recognition</title>
		<author>
			<persName><forename type="first">O</forename><surname>Abdel-Hamid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Penn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="4277" to="4280" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Realworld acoustic event detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1543" to="1551" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Audio keywords generation for sports video analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transaction on Multimedia Computing, Communications, and Applications</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Robust minimum statistics project coefficients feature for acoustic environment recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="8232" to="8236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Exploring Convolutional Neural Network Structures and Optimization Techniques for Speech Recognition</title>
		<author>
			<persName><forename type="first">O</forename><surname>Abdel-Hamid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3366" to="3370" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Vocal tract length perturbation (VTLP) improves speech recognition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Freesound technical demo</title>
		<author>
			<persName><forename type="first">F</forename><surname>Font</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Roma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 21st ACM international conference on Multimedia</title>
		<meeting>21st ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Acoustical sound database in real environments for sound scene understanding and hands-free speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hiyane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Asano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Language Resources &amp; Evaluation</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="2" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Deep multiple instance learning for image classification and auto-annotation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3460" to="3469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Multiple instance boosting for object detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="1769" to="1775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A tractable inference algorithm for diagnosing multiple diseases</title>
		<author>
			<persName><forename type="first">H</forename><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. UAI</title>
		<meeting>UAI</meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="163" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Lasagne: First release</title>
		<author>
			<persName><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thoma</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.27878</idno>
		<ptr target="http://dx.doi.org/10.5281/zenodo.27878" />
		<imprint>
			<date type="published" when="2015-08">Aug. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Recognition of acoustic events using deep neural networks</title>
		<author>
			<persName><forename type="first">O</forename><surname>Gencoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huttunen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EUSIPCO, no. 1-5 Sept</title>
		<meeting>EUSIPCO, no. 1-5 Sept</meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="506" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">From 2015 to 2016, he worked with Prof. Luc Van Gool at the Computer Vision Lab at ETH Zurich. He is currently a scientific researcher at Sony corporation. His current research activities include audio event recognition, speech recognition, video highlight detection, audio source separation and spatial audio</title>
		<author>
			<persName><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>CRCV-TR-12-01</idno>
		<imprint>
			<date type="published" when="2008">2012. 2008</date>
		</imprint>
		<respStmt>
			<orgName>he joined Sony Corporation, Japan</orgName>
		</respStmt>
	</monogr>
	<note>Naoya Takahashi received M.S.degree from the Waseda University in Applied Physics in 2008</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
