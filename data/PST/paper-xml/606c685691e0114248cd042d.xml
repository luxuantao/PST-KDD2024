<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Empirical Study of Training Self-Supervised Vision Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Kaiming He Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Kaiming He Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">An Empirical Study of Training Self-Supervised Vision Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper does not describe a novel method. Instead, it studies a straightforward, incremental, yet must-know baseline given the recent progress in computer vision: selfsupervised learning for Vision Transformers (ViT). While the training recipes for standard convolutional networks have been highly mature and robust, the recipes for ViT are yet to be built, especially in the self-supervised scenarios where training becomes more challenging. In this work, we go back to basics and investigate the effects of several fundamental components for training self-supervised ViT. We observe that instability is a major issue that degrades accuracy, and it can be hidden by apparently good results. We reveal that these results are indeed partial failure, and they can be improved when training is made more stable. We benchmark ViT results in MoCo v3 and several other selfsupervised frameworks, with ablations in various aspects. We discuss the currently positive evidence as well as challenges and open questions. We hope that this work will provide useful data points and experience for future research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Unsupervised pre-training has revolutionized natural language processing (NLP) <ref type="bibr" target="#b34">[36,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b3">4]</ref>. In computer vision, the un-/self-supervised pre-training paradigms differ from their NLP counterparts in at least two aspects: (i) the learners in NLP are masked auto-encoders, while in vision the recently popular choices are Siamese networks (e.g., <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b6">7]</ref>); (ii) the backbone architectures in NLP are self-attentional Transformers <ref type="bibr" target="#b40">[42]</ref>, while in vision the common choice is convolutional <ref type="bibr" target="#b25">[27]</ref>-yet non-attentionaldeep residual networks (ResNets) <ref type="bibr" target="#b19">[21]</ref>. To complete the big picture of self-supervised learning in vision, and towards closing the gap of pre-training methodology between vision and language, it is of scientific merit to investigate these differences.</p><p>This work focuses on training Transformers with the leading self-supervised frameworks in vision. This investigation is a straightforward extension given the recent progress on Vision Transformers (ViT) <ref type="bibr">[16]</ref>. In contrast to prior works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr">16]</ref> that train self-supervised Transformers with masked auto-encoding, we study the frameworks that are based on Siamese networks, including MoCo <ref type="bibr" target="#b18">[20]</ref> and *: equal contribution. framework model params acc. (%) linear probing: iGPT <ref type="bibr" target="#b8">[9]</ref> iGPT-L 1362M 69.0 iGPT <ref type="bibr" target="#b8">[9]</ref> iGPT-XL 6801M 72.0 MoCo v3</p><p>ViT-B 86M 76.7 MoCo v3</p><p>ViT-L 304M 77. <ref type="bibr" target="#b5">6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MoCo v3</head><p>ViT-H 632M 78.1 MoCo v3</p><p>ViT-BN-H 632M 79.1 MoCo v3</p><p>ViT-BN-L/7 304M 81.0 end-to-end fine-tuning: masked patch pred. <ref type="bibr">[16]</ref> ViT-B 86M 79.9 †</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MoCo v3</head><p>ViT-B 86M 83.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MoCo v3</head><p>ViT-L 304M 84.1</p><p>Table <ref type="table">1</ref>.</p><p>State-of-the-art Self-supervised Transformers in ImageNet classification, evaluated by linear probing (top panel) or end-to-end fine-tuning (bottom panel). Both iGPT <ref type="bibr" target="#b8">[9]</ref> and masked patch prediction <ref type="bibr">[16]</ref> belong to the masked auto-encoding paradigm. MoCo v3 is a contrastive learning method that compares two (224×224) crops. ViT-B, -L, -H are the Vision Transformers proposed in <ref type="bibr">[16]</ref>. ViT-BN is modified with BatchNorm, and "/7" denotes a patch size of 7×7. † : pre-trained in JFT-300M. others <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>Unlike standard convolutional networks whose training practice has been extensively studied thanks to continuous community effort, ViT models are new and their recipes are yet to be established. In this work, we go back to basics and investigate the fundamental components of training deep neural networks: the batch size, learning rate, and optimizer. We find that under various cases, instability is a major issue that impacts self-supervised ViT training.</p><p>Interestingly, we observe that unstable ViT training may not result in catastrophic failure (e.g., divergence); instead, it can cause mild degradation in accuracy (e.g., 1∼3%). Such a degree of degradation may not be noticeable, unless a more stable counterpart is available for comparison. To the best of our knowledge, this phenomena is rare in the literature of training convolutional networks <ref type="foot" target="#foot_0">1</ref> , and we believe this problem and its hidden degradation are worth noticing.</p><p>To demonstrate the possible harm of instability, we investigate a simple trick that can improve stability in practice. Based on an empirical observation on gradient changes, we freeze the patch projection layer in ViT, i.e., we use fixed random patch projection. We empirically show that this trick alleviates the instability issue in several scenarios and consistently increases accuracy.</p><p>We benchmark and ablate self-supervised ViT in a variety of cases. We provide ViT results in several selfsupervised frameworks. We conduct ablations on architecture designs and discuss the implications. Furthermore, we explore scaling up the ViT models, including the non-trivial ViT-Large and ViT-Huge [16] -the latter has 40× more computation than ResNet-50 <ref type="bibr" target="#b19">[21]</ref>. Based on these experimental results, we discuss both the currently positive evidence as well as the challenges and open questions.</p><p>We report that self-supervised Transformers can achieve strong results using a contrastive learning framework, compared against masked auto-encoding (Table <ref type="table">1</ref>). This behavior of Transformers differs from the existing trend in NLP. Moreover, as a promising signal, our bigger self-supervised ViT can achieve better accuracy, unlike the ImageNetsupervised ViT in <ref type="bibr">[16]</ref> whose accuracy degrades if getting bigger. For instance, for the very big ViT-Large, our self-supervised pre-training can outperform its supervised pre-training counterpart for transfer learning in certain cases. This presents a proof-of-concept scenario where selfsupervised pre-training is needed.</p><p>In addition, we report that our self-supervised ViT models have competitive results vs. the big convolutional ResNets in prior art <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">18]</ref>. On one hand, this comparison shows the potential of ViT, especially considering that it achieves these results using relatively "fewer inductive biases" <ref type="bibr">[16]</ref>. On the other hand, we suggest that there could be room for self-supervised ViT models to further improve. As one example, we observe that removing the position embedding in ViT only degrades accuracy by a small margin. This reveals that self-supervised ViT can learn strong representations without the positional inductive bias, but it also implies that the positional information has not been sufficiently exploited.</p><p>In summary, we believe that the evidence, challenges, and open questions in this study are worth knowing, if selfsupervised Transformers will close the gap in pre-training between vision and language. We hope our data points and experience will be useful to push this frontier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Self-supervised visual representation learning. In computer vision, contrastive learning <ref type="bibr" target="#b17">[19]</ref> has become increasingly successful for self-supervised learning, e.g., <ref type="bibr" target="#b42">[44,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b9">10]</ref>. The methodology is to learn representations that attract similar (positive) samples and dispel different (negative) samples. The representations from contrastive self-supervised pre-training can outperform their supervised counterparts in certain tasks <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>Contrastive learning is commonly instantiated as some forms of Siamese networks <ref type="bibr" target="#b2">[3]</ref>. Recently, a series of works <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b12">13]</ref> retain the Siamese architectures but eliminate the requirement of negative samples. The success of these methods suggest that it is of central importance to learn invariant features by matching positive samples.</p><p>Transformers. Transformers <ref type="bibr" target="#b40">[42]</ref> were originally introduced for machine translation and later became a dominant backbone in NLP <ref type="bibr" target="#b34">[36,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b3">4]</ref>. The long-range, selfattentional behavior makes Transformers an effective tool given the non-local, relational nature of languages.</p><p>There have been continuous efforts on generalizing Transformers to computer vision <ref type="bibr" target="#b41">[43,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr">16</ref>]. The recent work on Vision Transformers (ViT) <ref type="bibr">[16]</ref> greatly pushes this frontier. ViT is purely Transformer-based, rather than interlaced with non-degenerated (i.e., non-1×1) convolutions. <ref type="foot" target="#foot_1">2</ref> This largely closes the architectural gap between NLP and vision. ViT achieves compelling accuracy in supervised learning, especially with large-scale data and highcapacity models. Given these properties, we believe ViT is a must-study baseline for self-supervised learning in computer vision.</p><p>Self-supervised Transformers for vision. In pioneering works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr">16]</ref>, training self-supervised Transformers for vision problems in general follows the masked auto-encoding paradigm in NLP <ref type="bibr" target="#b34">[36,</ref><ref type="bibr" target="#b14">15]</ref> (Table <ref type="table">1</ref>). iGPT <ref type="bibr" target="#b8">[9]</ref> masks and reconstructs pixels, and the self-supervised variant of ViT in [16] masks and reconstructs patches. In this work, we focus on training Transformers in the contrastive/Siamese paradigm, in which the loss is not defined for reconstructing the inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MoCo v3</head><p>We introduce a "MoCo v3" framework that facilitates our study. MoCo v3 is an incremental improvement of MoCo v1/2 <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b11">12]</ref>, and we strike for a better balance between simplicity, accuracy, and scalability. The pseudocode of MoCo v3 is in Alg. 1, described next.</p><p>As common practice (e.g., <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b9">10]</ref>), we take two crops for each image under random data augmentation. They are encoded by two encoders, f q and f k , with output vectors q and k. Intuitively, q behaves like a "query" <ref type="bibr" target="#b18">[20]</ref>, and the goal of learning is to retrieve the corresponding "key". This is formulated as minimizing a contrastive loss function <ref type="bibr" target="#b17">[19]</ref>. We adopt the form of InfoNCE <ref type="bibr" target="#b31">[33]</ref>:</p><formula xml:id="formula_0">Lq = − log exp(q•k + /τ ) exp(q•k + /τ ) + k − exp(q•k − /τ ) .<label>(1)</label></formula><p>Here k + is f k 's output on the same image as q, known as q's positive sample. The set {k − } consists of f k 's outputs from other images, known as q's negative samples. τ is a temperature hyper-parameter <ref type="bibr" target="#b42">[44]</ref> for 2 -normalized q, k.</p><p>Following <ref type="bibr" target="#b43">[45,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b9">10]</ref>, in MoCo v3 we use the keys that naturally co-exist in the same batch. We abandon the memory queue <ref type="bibr" target="#b18">[20]</ref>, which we find has diminishing gain if the batch is sufficiently large (e.g., 4096). With this simplification, the contrastive loss in (1) can be implemented by a few lines of code: see ctr(q, k) in Alg. 1. We adopt a symmetrized loss <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b12">13]</ref>: ctr(q1, k2)+ctr(q2, k1).</p><p>Our encoder f q consists of a backbone (e.g., ResNet, ViT), a projection head <ref type="bibr" target="#b9">[10]</ref>, and an extra prediction head <ref type="bibr" target="#b16">[18]</ref>; the encoder f k has the backbone and projection head, but not the prediction head. f k is updated by the movingaverage of f q <ref type="bibr" target="#b18">[20]</ref>, excluding the prediction head.</p><p>As a reference, we examine the MoCo v3 accuracy with ResNet-50 (R50) (detailed in appendix). This table compares the linear probing accuracy in ImageNet: The improvement here is mainly due to the extra prediction head and large-batch (4096) training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Stability of Self-Supervised ViT Training</head><p>In principle, it is straightforward to replace a ResNet backbone with a ViT backbone in the contrastive/Siamese self-supervised frameworks. But in practice, a main challenge we have met is the instability of training.</p><p>We observe that the instability problem can not be simply reflected by accuracy numbers. In fact, as we will show, the training is "apparently good" and provides decent results, even when it is potentially unstable. To reveal the instability, we monitor the kNN curves <ref type="bibr" target="#b42">[44]</ref> (see appendix) during training. In Sec. 4.1, we study how the basic factors influence stability. The curves suggest that the training can be "partially successful", or in other words, "partially failed". In Sec. 4.2, we explore a simple trick that can improve stability. As a result, the accuracy is improved in various cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Empirical Observations on Basic Factors</head><p>Batch size. ViT models in <ref type="bibr">[16]</ref> are by design computationally heavy (see Table <ref type="table">2 and 3</ref>), and large-batch training <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b44">46,</ref><ref type="bibr" target="#b45">47]</ref> is a desirable solution to big ViT models.</p><p>A large batch is also beneficial for accuracy in recent selfsupervised learning methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b6">7]</ref>. Fig. <ref type="figure" target="#fig_0">1</ref> presents the training curves with different batch sizes.</p><p>A batch of 1k and 2k produces reasonably smooth curves, with 71.5% and 72.6% linear probing accuracy. In this regime, the larger batch improves accuracy thanks to more negative samples <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b9">10]</ref>. The curve of a 4k batch becomes noticeably unstable: see the "dips" in Fig. <ref type="figure" target="#fig_0">1</ref>. It has 72.2% linear probing accuracy. Although this seems to be a marginal decrease vs. the 2k batch, its accuracy is harmed by the instability, as we will show in the next subsection.</p><p>The curve of a 6k batch has worse failure patterns (big dips in Fig. <ref type="figure" target="#fig_0">1</ref>). We hypothesize that the training is partially restarted and jumps out of the current local optimum, then seeks a new trajectory. As a consequence, the training does not diverge, but the accuracy depends on how good the local restart is. When this partial failure happens, it still provides an apparently decent result (69.7%). This behavior is harmful to explorative research: unlike catastrophic failure that is easily noticeable, the small degradation can be fully hidden.</p><p>We also find that the mild instability does not result in a noticeably large variation. In many of our ablations, running the same configuration for a second trial often results in a small difference of 0.1∼0.3%. This also makes it difficult to notice the potential degradation caused by instability.</p><p>Learning rate. In practice, the learning rate is often scaled when the batch size increases <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b15">17]</ref>. In all experiments in this paper, we adopt the linear scaling rule <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b15">17]</ref>: we set the learning rate as lr×BatchSize/256, where lr is a "base" learning rate. lr is the hyper-parameter being set <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">18]</ref>. In Fig. <ref type="figure">2</ref> we study the influence of lr. When lr is smaller, the training is more stable, but it is prone to under-fitting. In Fig. <ref type="figure">2</ref>, lr=0.5e-4 has 1.8% worse accuracy than lr=1.0e-4 (70.4 vs. 72.2). In this regime, the accuracy is determined by fitting vs. under-fitting. Training with a larger lr becomes less stable. Fig. <ref type="figure">2</ref> shows that lr=1.5e-4 for this setting has more dips in the curve, and its accuracy is lower. In this regime, the accuracy is determined by stability.</p><p>Optimizer. By default, we use AdamW <ref type="bibr" target="#b28">[30]</ref> as the optimizer, which is the common choice for training ViT models [16, <ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b33">35]</ref>. 3 On the other hand, recent self-supervised methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b6">7]</ref> are based on the LARS optimizer <ref type="bibr" target="#b44">[46]</ref> for large-batch training. In Fig. <ref type="figure">3</ref>, we study the LAMB optimizer <ref type="bibr" target="#b45">[47]</ref>, which is an AdamW-counterpart of LARS.</p><p>Given an appropriate learning rate (lr=5e-4, Fig. <ref type="figure">3</ref>), LAMB achieves slightly better accuracy (72.5%) than AdamW. But the accuracy drops rapidly when lr is larger than the optimal value. LAMB with lr=6e-4 and 8e-4 has 1.6% and 6.0% lower accuracy. Interestingly, the training curves are still smooth, but they degrade gradually in the middle. We hypothesize that although LAMB can avoid sudden change in the gradients, the negative impact of unreliable gradients is accumulated.</p><p>During our exploration, we find that LAMB can achieve comparable accuracy with AdamW, if lr is appropriately chosen. But the sensitivity to lr makes it difficult to ablate 3 In original <ref type="bibr">ViT [16]</ref> in JAX, the weight decay is "AdamW style": different architecture designs without extra lr search. As a result, we opt to use AdamW in other parts of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">A Trick for Improving Stability</head><p>All these experiments suggest that instability is a major issue. Next we describe a simple trick that can improve stability in various cases in our experiments.</p><p>During training, we notice that a sudden change of gradients (a "spike" in Fig. <ref type="figure">4</ref>) causes a "dip" in the training curve, which is as expected. By comparing all layers' gradients, we observe that the gradient spikes happen earlier in the first layer (patch projection), and are delayed by couples of iterations in the last layers (see Fig. <ref type="figure">4</ref>). Based on this observation, we hypothesize that the instability happens earlier in the shallower layers. Motivated by this, we explore freezing the patch projection layer during training. In other words, we use a fixed random patch projection layer to embed the patches, which is not learned. This can be easily done by applying a stop-gradient operation right after this layer.</p><p>Comparisons. In Fig. <ref type="figure">5</ref>  curves. This stability benefits the final accuracy, boosting the accuracy by 1.7% to 73.4% at lr=1.5e-4. The improvement is bigger for a larger lr (0.4%, 0.6%, 1.7%). This comparison confirms that the training instability is a main issue that impacts accuracy. Besides MoCo, we find that other related methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b6">7]</ref> can also be unstable. Fig. <ref type="figure" target="#fig_2">6</ref> presents the training curves of ViT in SimCLR <ref type="bibr" target="#b9">[10]</ref> and BYOL <ref type="bibr" target="#b16">[18]</ref>. Random patch projection improves stability in both SimCLR and BYOL, and increases the accuracy by 0.8% and 1.3%. We also observe the instability issue for SwAV <ref type="bibr" target="#b6">[7]</ref>, in which, however, the loss diverges (NaN) when it is unstable. Random patch projection helps SwAV by enabling a relatively larger lr without diverging, and improves its accuracy from 65.8% to 66.4% when using the largest stable lr. In sum, this trick is effective in all these self-supervised frameworks.</p><p>We have also tried BatchNorm (BN) <ref type="bibr" target="#b22">[24]</ref>, WeightNorm (WN) <ref type="bibr" target="#b37">[39]</ref>, or gradient clip on patch projection. We observe that BN or WN on the learnable patch projection layer does not improve instability, and produces similar results; gradient clip on this layer is useful if given a sufficiently small threshold, which to the extreme becomes freezing the layer.</p><p>Discussions. It is an interesting observation that it is not necessary to train the patch projection layer. For the standard ViT patch size, the patch projection matrix is complete (768-d output for a 3-channel 16×16 patch) or overcomplete. In this case, random projection should be sufficient to preserve the information of the original patches.</p><p>We note that freezing the first layer does not change the architecture, and it actually narrows down the solution space. This indicates that the underlying problem is on optimization. The trick alleviates the issue, but does not solve it. The model can still be unstable if lr is too big. The first layer is unlikely the essential reason for the instability; instead, the issue concerns all layers. The first layer is merely easier to be handled separately, e.g., it is the only non-Transformer layer in the backbone. We hope to see a more fundamental solution in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Implementation Details</head><p>This section describes the details of ViT+MoCo v3. More subtleties are described in the appendix.</p><p>Optimizer. By default we use AdamW <ref type="bibr" target="#b28">[30]</ref> and a batch size of 4096 <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b6">7]</ref>. We search for lr and weight decay wd based on 100-epoch results, and then apply it for longer training. We adopt learning rate warmup <ref type="bibr" target="#b15">[17]</ref> for 40 epochs (as per "warmup of 10k steps", Table <ref type="table">4</ref> in <ref type="bibr">[16]</ref>). This long warmup helps alleviate instability, though all unstable results are already with this warmup. After warmup, lr follows a cosine decay schedule <ref type="bibr" target="#b27">[29]</ref>.</p><p>MLP heads. The projection head <ref type="bibr" target="#b9">[10]</ref> is a 3-layer MLP, following <ref type="bibr" target="#b10">[11]</ref>. The prediction head <ref type="bibr" target="#b16">[18]</ref> is a 2-layer MLP. The hidden layers of both MLPs are 4096-d and are with ReLU <ref type="bibr" target="#b29">[31]</ref>; the output layers of both MLPs are 256-d, without ReLU. In MoCo v3, all layers in both MLPs have BN <ref type="bibr" target="#b21">[23]</ref>, following SimCLR <ref type="bibr" target="#b9">[10]</ref>. The MLP heads of BY-OL/SwAV have different BN designs (see appendix).</p><p>Loss. We scale the contrastive loss in (1) by a constant 2τ (see Alg. 1), following <ref type="bibr" target="#b16">[18]</ref>'s appendix. This scale is redundant because it can be absorbed by adjusting lr and wd. But this scale makes it less sensitive to the τ value when lr and wd are fixed. We set τ =0.2 <ref type="bibr" target="#b11">[12]</ref> as the default.</p><p>ViT architecture. We closely follow the designs in <ref type="bibr">[16]</ref>. The input patch size is 16×16 or 14×14 ('/16' or '/14'), and after projection it results in a sequence of length 196 or 256 for a 224×224 input. Position embeddings are added to the sequence, and we use the sine-cosine variant <ref type="bibr" target="#b40">[42]</ref> in 2-D. This sequence is concatenated with a learnable class token. The sequence is then encoded by a stack of Transformer blocks <ref type="bibr" target="#b40">[42]</ref> following the design in <ref type="bibr">[16]</ref>. The class token after the last block (and after the final LayerNorm <ref type="bibr" target="#b0">[1]</ref>) is treated as the output of the backbone, and is the input to the MLP heads. Linear probing. Following common practice, we evaluate the representation quality by linear probing. After selfsupervised pre-training, we remove the MLP heads and train a supervised linear classifier on frozen features. We use the SGD optimizer, with a batch size of 4096, wd of 0, and sweep lr for each case. We train this supervised classifier for 90 epochs in the ImageNet training set, using only random resized cropping and flipping augmentation. We evaluate single-crop top-1 accuracy in the validation set.  <ref type="table">1</ref> in <ref type="bibr" target="#b10">[11]</ref>, and BYOL results are from Table <ref type="table">1</ref> in <ref type="bibr" target="#b16">[18]</ref>.</p><p>and BERT <ref type="bibr" target="#b14">[15]</ref>). Our MoCo-based ViT has higher accuracy and smaller models than iGPT, under the same linear probing protocol and training data. The mask patch prediction in [16] is pre-trained on JFT-300M and end-to-end fine-tuned in ImageNet, which we append as a reference.</p><p>Our self-supervised ViT models have higher accuracy when the models are bigger. This is in contrast to the supervised results in <ref type="bibr">[16]</ref>, where ViT-L has lower accuracy than ViT-B when pre-trained in ImageNet-1k/21k. Actually, for ViT-L, our self-supervised pre-training with linear probing (77.6%) is better than the supervised counterpart in [16] (76.53%) when trained in ImageNet-1k. <ref type="foot" target="#foot_3">4</ref> These comparisons suggest that self-supervised learning as a tool for generic representation learning is less prone to over-fitting.</p><p>Comparisons with big ResNets. <ref type="foot" target="#foot_4">5</ref> In Fig. <ref type="figure" target="#fig_3">8</ref> we compare with the state-of-the-art big ResNets reported by SimCLR v2 <ref type="bibr" target="#b10">[11]</ref> and BYOL <ref type="bibr" target="#b16">[18]</ref>. We note that both SimCLR v2 and BYOL use a momentum encoder. Our baseline ViT MoCo (the curve of "ViT, MoCo v3") is slightly better than ResNet SimCLR v2 in the small-model regime, but the envelopes become just comparable for larger models. SimCLR v2 with SK-ResNet (Selective Kernel <ref type="bibr" target="#b26">[28]</ref>, a form of attention) has a higher envelope. BYOL also has a higher envelope with wider ResNets (1-4×), and has an outstanding point with a deeper ResNet (R200-2×).</p><p>ter than DeiT's 81.8% at 300 epochs.</p><p>In addition, MoCo v3 has 84.1% with ViT-L when finetuned for only 100 epochs with a drop path rate of 0.5. This short schedule demonstrates the effectiveness of MoCo pretraining. We have also found DeiT-L diverges under its default settings, and a different solution may be needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Transfer Learning</head><p>In Table <ref type="table">6</ref> we evaluate transfer learning. We study the four downstream datasets as in <ref type="bibr">[16]</ref>. We fine-tune the models end-to-end, also following <ref type="bibr">[16]</ref>.</p><p>Our self-supervised ViT has better transfer learning accuracy when the model size increases from ViT-B to ViT-L, yet it gets saturated when increased to ViT-H. As a comparison, the ImageNet-supervised ViT in [16] becomes saturated or overfitted starting at ViT-L. Our self-supervised ViT achieves better results than its ImageNet-supervised counterparts in three of these four datasets.</p><p>The overfitting is more prominent when training the big ViT models from scratch in these small datasets: the accuracy in general decreases with bigger ViT. We also find that the from-scratch ViT results are much worse than their ResNet-counterparts (c.f ., Table <ref type="table">8</ref> in <ref type="bibr" target="#b9">[10]</ref>) in these small datasets. This suggests that if data are not enough, it is difficult for ViT to learn representations in the lack of inductive biases. Self-supervised pre-training can close this gap and largely reduce overfitting in small datasets.</p><p>Finally, we note that with supervised pre-training in bigger datasets (ImageNet-21k or JFT-300M), the ViT results in [16] can be better than ours when transferring to these small datasets. A potential future work is to perform self-supervised pre-training for big ViT models in bigger data. This is analogous to the trajectory of unsupervised pre-training in NLP in the past years <ref type="bibr" target="#b34">[36,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b3">4]</ref>, i.e., both models and datasets are scaled up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We have explored training ViT in the recently popular self-supervised frameworks. Our comparisons concern several aspects, including ViT vs. convolutional networks, supervised vs. self-supervised, and contrastive learning vs. masked auto-encoding. We report positive evidence as well as challenges, open questions, and opportunities. We hope our empirical study will be useful for the community to close the gap of pre-training between vision and language.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Training curves of different batch sizes (MoCo v3, ViT-B/16, 100-epoch ImageNet, AdamW, lr=1.0e-4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .Figure 3 .</head><label>23</label><figDesc>Figure 2. Training curves of different learning rates (MoCo v3, ViT-B/16, 100-epoch ImageNet, AdamW, batch 4096).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Random vs. learned patch projection (ViT-B/16, 100epoch ImageNet, AdamW, batch 4096). Top: SimCLR: lr=2e-4, wd=0.1. Bottom: BYOL: lr=1e-4, wd=0.03.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Comparisons with state-of-the-art big ResNets, presented as parameters-vs.-accuracy trade-off. All entries are pretrained with two 224×224 crops, and are evaluated by linear probing. SimCLR v2 results are from Table1in<ref type="bibr" target="#b10">[11]</ref>, and BYOL results are from Table1in<ref type="bibr" target="#b16">[18]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 MoCo v3: PyTorch-like Pseudocode</figDesc><table><row><cell># f_q: encoder: backbone + proj mlp + pred mlp</cell></row><row><cell># f_k: momentum encoder: backbone + proj mlp</cell></row><row><cell># m: momentum coefficient</cell></row><row><cell># tau: temperature</cell></row><row><cell>for x in loader: # load a minibatch x with N samples</cell></row><row><cell>x1, x2 = aug(x), aug(x) # augmentation</cell></row><row><cell>q1, q2 = f_q(x1), f_q(x2) # queries: [N, C] each</cell></row><row><cell>k1, k2 = f_k(x1), f_k(x2) # keys: [N, C] each</cell></row><row><cell>loss = ctr(q1, k2) + ctr(q2, k1) # symmetrized</cell></row><row><cell>loss.backward()</cell></row><row><cell>update(f_q) # optimizer update: f_q</cell></row><row><cell>f_k = m * f_k + (1-m) * f_q # momentum update: f_k</cell></row><row><cell># contrastive loss</cell></row><row><cell>def ctr(q, k):</cell></row><row><cell>logits = mm(q, k.t()) # [N, N] pairs</cell></row><row><cell>labels = range(N) # positives are in diagonal</cell></row><row><cell>loss = CrossEntropyLoss(logits/tau, labels)</cell></row><row><cell>return 2 * tau * loss</cell></row></table><note>Notes: mm is matrix multiplication. k.t() is k's transpose. The prediction head is excluded from f k (and thus the momentum update).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>We monitor the gradient magnitude, shown as relative values for the layer. A "spike" in the gradient causes a "dip" in the training curve. We observe that a spike happens earlier in the first layer, and are delayed by tens of iterations in the last layers.</figDesc><table><row><cell>∞-norm</cell><cell>first layer last layer</cell><cell></cell><cell></cell></row><row><cell>gradient,</cell><cell></cell><cell></cell><cell></cell></row><row><cell>6000</cell><cell># iter.</cell><cell></cell><cell>7000</cell></row><row><cell>0 50 kNN accuracy Figure 4. 0</cell><cell>epochs</cell><cell cols="2">learned, lr=1.0e-4 random, lr=1.0e-4</cell><cell>100</cell></row><row><cell>50</cell><cell></cell><cell></cell><cell></cell></row><row><cell>kNN accuracy</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">learned, lr=1.5e-4</cell></row><row><cell></cell><cell></cell><cell cols="2">random, lr=1.5e-4</cell></row><row><cell>0</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>epochs</cell><cell></cell><cell></cell><cell>100</cell></row><row><cell>lr, ×10 −4</cell><cell>0.5</cell><cell>1.0</cell><cell>1.5</cell></row><row><cell>learned patch proj.</cell><cell>70.4</cell><cell>72.2</cell><cell>71.7</cell></row><row><cell>random patch proj.</cell><cell>70.8</cell><cell>72.8</cell><cell>73.4</cell></row></table><note>https://github.com/google/flax/blob/master/flax/optim/adam.py Figure 5. Random vs. learned patch projection (MoCo v3, ViT-B/16, 100-epoch ImageNet, AdamW, batch 4096). Top: lr=1.0e-4. Bottom: lr=1.5e-4.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">See also postscript on a related discussion.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">We argue that it is imprecise to simply compare self-attention against "convolutions". Convolutions<ref type="bibr" target="#b25">[27]</ref> by definition have several properties: weight-sharing, locally-connected, translation-equivariant. All projection layers in a self-attention block have all these properties of convolutions, and are equivalent to 1×1 convolutions. The counterpart of self-attention is more appropriately the non-degenerated (e.g.,</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">3×3) convolutions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">Stronger regularization could reduce over-fitting for supervised ViT<ref type="bibr" target="#b39">[41]</ref>, though regularizing the very big ViT-L/H is yet to be explored.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">Transformers<ref type="bibr" target="#b40">[42]</ref> by design consist of residual blocks<ref type="bibr" target="#b19">[21]</ref>, and thus are a form of residual networks. In the literature on "Transformer vs. ResNet", precisely speaking, the term of "ResNet" refers to the specific design that has non-degenerated (e.g., 3×3) convolutions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">We are trying to replace every LN with BN in the ViT backbone. In preliminary experiments, doing so leads to convergence problems.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6">ViT-BN-H/7 is out of memory in our unoptimized implementation.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental Results</head><p>In this section we benchmark and ablate self-supervised ViT. We perform self-supervised training on the 1.28M Im-ageNet training set <ref type="bibr" target="#b13">[14]</ref>, and evaluate by linear probing.</p><p>Table <ref type="table">2</ref> summarizes the ViT configurations we study. ViT-B/L/H follow [16], and ViT-S is similar to that in <ref type="bibr" target="#b39">[41]</ref>. We use ViT-B by default in our ablations.</p><p>Training time. We train our models in TPUs (v3) that are publicly available in Google Cloud Platform (GCP). Table <ref type="table">3</ref> summarizes the training time (per 100 epochs). It takes 2.1 hours training ViT-B for 100 epochs, and our ablations typically take 6.3 hours each (300 epochs). This is a competitive performance, as it enables us to ablate many design decisions. The TPU implemenataion also makes it possible to explore the ViT-H model, which takes 9.8 hours per 100 epochs using 512 TPUs. This is a gigantic scale of training: for the 300-epoch ViT-H, this amounts to ∼625 TPU•days, or ∼1.7 TPU•years of training.</p><p>We also verify our models in GPUs using PyTorch. It takes 24 hours for ViT-B in 128 GPUs (vs. 2.1 hours in 256 TPUs). With an increasing number of devices, we observe that TPUs scale up more favorably than GPUs. While further engineering optimization could speed up our GPU system, we opt to use the TPU system for the ease of research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Self-supervised Learning Frameworks</head><p>We benchmark self-supervised ViT in four frameworks: MoCo v3, SimCLR <ref type="bibr" target="#b9">[10]</ref>, BYOL <ref type="bibr" target="#b16">[18]</ref>, and SwAV <ref type="bibr" target="#b6">[7]</ref>.We use the same random projection trick in all cases. We sweep lr and wd for each individual framework for fair comparisons. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Ablations of ViT + MoCo v3</head><p>Next we ablate the designs of the ViT + MoCo v3 system. We use random patch projection in all ablations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Position embedding. The following table compares the choice of position embedding (our default is sin-cos):</head><p>ViT-B, 300-ep sin-cos learned none linear acc.</p><p>76.5 76.1 74.9</p><p>The learned version works well, but not better than sin-cos. Surprisingly, the model works decently even with no position embedding (74.9%). The capability to encode positions contributes only 1.6%. We believe this data point reveals both strengths and limitations of the current model. On the positive side, it suggests that the model can learn strong representations just by a set of patches, which are fully permutation-invariant. This is analogous to bag-ofwords models <ref type="bibr" target="#b38">[40]</ref>. This model has no positional inductive bias. On the negative side, it also suggests that the model has not made good use of positions, and the gesture of the object contributes relatively little to the representation. We hope this data point will draw attention to future study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class token. The following table ablates the role of the class token [CLS] in ViT:</head><p>ViT The optimal value is m=0.99 (our default). The case of m=0 is analogous to SimCLR (plus the prediction head and stop-gradient on the keys), and its accuracy of 74.3% is similar to SimCLR's (73.9%, Table <ref type="table">4</ref>). The usage of the momentum encoder leads to 2.2% increase.</p><p>Training length. In the following The smaller ViT-S enjoys the benefit of training longer, and improves by 0.9% when extending to 600 epochs. This is similar to the behavior of R50, which was typically trained for 800 epochs <ref type="bibr" target="#b9">[10]</ref>. But the gain of training longer is diminishing on ViT-B. Based on this ablation, we train the bigger ViT-L/H for 300 epochs presented next (Table <ref type="table">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Comparisons with Prior Art</head><p>Self-supervised Transformers. </p><p>ViT-BN-B/7</p><p>ViT-BN-L/7 We notice that this comparison concerns a composition of many choices. As one example, the default ViT backbone in <ref type="bibr">[16]</ref> uses LayerNorm (LN), while the default ResNet <ref type="bibr" target="#b19">[21]</ref> uses BatchNorm (BN). These design choices can lead to a systematic gap. In our preliminary experiments, we explore replacing LN with BN in the ViT backbone's MLP blocks (i.e., excluding self-attention blocks). 6 We simply refer to this as a "ViT-BN" backbone. It leads to ∼1% improvement consistently (see Fig. <ref type="figure">8</ref>).</p><p>In iGPT <ref type="bibr" target="#b8">[9]</ref>, accuracy can be improved by using longer sequences in the pixel domain. Here we explore longer sequences by reducing the patch size to 7×7 ("/7" in Fig. <ref type="figure">8</ref>). This keeps the model size unchanged, but increases FLOPs to ∼6×. It can improve the accuracy by ∼2-3%. The gain of using small patches is also observed in <ref type="bibr" target="#b7">[8]</ref>. MoCo v3 achieves 81.0% with ViT-BN-L/7. 7 As a comparison, the previous best results under the linear probing protocol are 79.8% with SimCLR v2 (SK-ResNet152-3×), and 79.6% with BYOL (ResNet200-2×).</p><p>Discussion. While the bigger self-supervised ViT can achieve better accuracy, the results are saturated. This is unlike the trend in NLP, where bigger Transformers learn better representations (e.g., <ref type="bibr" target="#b3">[4]</ref>). A potential solution is to use more data. The saturation can also be caused by the limited power of the existing instance-based pretext task <ref type="bibr" target="#b42">[44]</ref>. It may be desired to design more difficult pretext tasks.</p><p>Our self-supervised ViT models are competitive with the big convolutional ResNets. It suggests that ViT can learn strong representations with "fewer inductive biases" <ref type="bibr">[16]</ref>. However, we also find that the accuracy only decreases by a bit even if removing the only positional inductive bias (position embedding), suggesting that in our method ViT relies less on positional information than convolutional networks.</p><p>End-to-end fine-tuning. Table <ref type="table">5</ref> reports end-to-end finetuning results. We use the DeiT codebase <ref type="bibr" target="#b39">[41]</ref> and all its default settings unless specified. MoCo v3 achieves 83.2% with ViT-B under 150-epoch fine-tuning, substantially bet-</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Layer normalization</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Signature verification using a &quot;Siamese&quot; time delay neural network</title>
		<author>
			<persName><forename type="first">Jane</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Säckinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roopak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ilya Sutskever, and Dario Amodei</title>
				<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gcnet: Non-local networks meet squeeze-excitation networks and beyond</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">End-toend object detection with Transformers</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2006">2020. 1, 2, 3, 4, 5, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14294</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2008">2020. 1, 2, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2008">2020. 1, 2, 3, 4, 5, 6, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploring simple Siamese representation learning</title>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2007">2021. 2, 3, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina Toutanova ; Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2008">2019. 1, 2, 7, 8 [16. 2021. 1, 2, 3, 4, 5, 6, 7, 8</date>
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training ImageNet in 1 hour</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent -a new approach to self-supervised learning</title>
		<author>
			<persName><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaohan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2007">2020. 1, 2, 3, 4, 5, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2008">2016. 1, 2, 6, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Batch renormalization: Towards reducing minibatch dependence in batch-normalized models</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech Report</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">One weird trick for parallelizing convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.5997</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donnie</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Selective kernel networks</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">SGDR: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
				<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Omkar M Parkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2018. 1, 2, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Stand-alone self-attention in vision models</title>
		<author>
			<persName><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Video Google: a text retrieval approach to object matching in videos</title>
		<author>
			<persName><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Hugo Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2008">2020. 4, 6, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2007">2017. 1, 2, 5, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Abhinav Gupta, and Kaiming He. Non-local neural networks</title>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unsupervised embedding learning via invariant and spreading instance feature</title>
		<author>
			<persName><forename type="first">Mang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Large batch training of convolutional networks</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03888</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Large batch optimization for deep learning: Training bert in 76 minutes</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sashank</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Hseu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinadh</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Exploring self-attention for image recognition</title>
		<author>
			<persName><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
