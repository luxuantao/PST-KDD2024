<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WiFinger: Talk to Your Smart Devices with Finger-grained Gesture</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hong</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianxin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liusheng</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">WiFinger: Talk to Your Smart Devices with Finger-grained Gesture</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">37CC0422915F9EB9ACF33B8ADCD3E564</idno>
					<idno type="DOI">10.1145/2971648.2971738</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Wireless</term>
					<term>Micro-motion Recognition</term>
					<term>Channel State Information H.5.2 Information Interfaces and Presentation: User Interfaces</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent literatures, WiFi signals have been widely used to "sense" people's locations and activities. Researchers have exploited the characteristics of wireless signals to "hear" people's talk and "see" keystrokes by human users. Inspired by the excellent work of relevant scholars, we turn to explore the field of human-computer interaction using finger-grained gestures under WiFi environment. In this paper, we present Wi-Finger -the first solution using ubiquitous wireless signals to achieve number text input in WiFi devices. We implement a prototype of WiFinger on a commercial Wi-Fi infrastructure. Our scheme is based on the key intuition that while performing a certain gesture, the fingers of a user move in a unique formation and direction and thus generate a unique pattern in the time series of Channel State Information (CSI) values. WiFinger is deigned to recognize a set of finger-grained gestures, which are further used to realize continuous text input in off-the-shelf WiFi devices. As the results show, WiFinger achieves up to 90.4% average classification accuracy for recognizing 9 digits finger-grained gestures from American Sign Language (ASL), and its average accuracy for single individual number text input in desktop reaches 82.67% within 90 digits.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>The pervasiveness of WiFi signals spurs a surge in relevant research, including motion detection, gesture recognition, localization. Multiple advances have been made both in terms of accuracy and granularity. In particular, recent literatures in keystroke recognition <ref type="bibr" target="#b0">[1]</ref> and mouth motion <ref type="bibr" target="#b1">[2]</ref> detection have demonstrated the viability of micro-movement detection with high accuracy using fine-grained radio reflections.</p><p>The research community has studied various methods to classical gesture recognition, which can be classified into three categories: audio and radio based approaches, computer vision based approaches and wearable sensors based approaches. Audio and radio based approaches like SoundWave <ref type="bibr" target="#b2">[3]</ref> and radio tomography technologies show a tendency of depending on large scale signal changes, they are not desirable for identifying micro-movement gestures. Vision based approaches have the fundamental limitations of requiring line of sight with enough lighting, and the infrared spectrum of sunlight often interferes with systems that rely on infrared, e.g., LeapMotion <ref type="bibr" target="#b3">[4]</ref> and Kinect <ref type="bibr" target="#b4">[5]</ref>. Hence, such systems are not appropriate for outdoor use. Wearable sensors based approaches need wearable sensing devices that attach to the users' hand or body, which probably cause inconvenience in some occasions such as when a user is swimming or bathing, and it may incur extra cost.</p><p>Recently, studies "see" target movements by detecting and analyzing fine-grained radio reflection which overcomes the limitations of the above mentioned approaches. These Wi-Fi signal based human motion recognition systems, such as WiSee <ref type="bibr" target="#b5">[6]</ref>, Wi-Vi <ref type="bibr" target="#b6">[7]</ref>, WiKey <ref type="bibr" target="#b0">[1]</ref> and WiHear <ref type="bibr" target="#b1">[2]</ref>, have been proposed based on the observation that different human motions cause different multi-path distortions in WiFi signals. WiSee uses Universal Software Radio Peripheral (USRP) to capture the Orthogonal Frequency Division Multiplexing (OFDM) signals and extract Doppler shift in signals reflected by movement of body or limbs to recognize nine gestures. Wi-Vi applies virtual multi-antenna technology to identify signal fluctuations caused by body movement. However, WiSee and WiVi focus on recognizing a set of coarse-grained gestures or motions, such as kicking, punching, stepping forward/backward. Additionally, they rely on sophisticated signals which are extracted from software radios, but not readily available in Commercial Off-The-Shelf (COTS) IEEE 802.11 devices. WiKey and WiHear employ COTS WiFi devices, one for continuously sending signals, and one for continuously receiving signals. They are both using in specific application scenarios such as hearing people's talk, detecting human's keystrokes. And both of them are not specialized in fine-grained gesture recognition. Melgarejo et al. proposed a fine-grained gestures recognition scheme which utilized sophisticated WARP v3 board equipped with two RE14P directional patch antennas, however, this scheme needs specialized devices to extract detailed RF signatures <ref type="bibr" target="#b7">[8]</ref>. In our scheme, we focus on finer-grained micro-movement gestures recognition and achieve accurate text input on computer or mobile devices to realize human-computer interaction. Our scheme extracts Channel State Information (CSI) values from Intel 5300 (iwl5300) 802.11n Network Interface Card (NIC) by simply modifying the driver as in <ref type="bibr" target="#b8">[9]</ref>.</p><p>In this paper, we show the potential to identify finer-grained gestures by using feature information that is easily available on COTS wireless devices. The key insight is that while performing a certain finer-grained micro motion, the fingers of a user move in a unique pattern in the time series of CSI values which we call CSI waveform, for that gesture. In this paper, we target gestures of 9 digits finer-grained gestures from the standard ASL, as shown in Figure <ref type="figure" target="#fig_0">1</ref>. Those gestures only involve finger movements and no hand or body movements. The unique finger movements of each gesture introduce relatively unique multipath distortions in WiFi signals and this uniqueness can be exploited to recognize gestures.</p><p>To translate the above high-level ideas into a working system entails a variety of technical challenges: (1) How does WiFinger detect and capture the subtle signal changes caused by finger movements of target user? (2) How does WiFinger analyze the tiny radio reflections and extract distinguishable features? (3) How does WiFinger compare shape features of any two finer-grained micro-movement gestures? (4) Will WiFinger be robust against unpredictable variances(e.g., different users may perform the same gesture in slightly differently ways)?</p><p>This paper addresses the above challenges, and prototypes the WiFi signals based finer-grained gesture recognition system called WiFinger using two WiFi devices, a transmitter (e.g., a router), and a receiver (e.g., a laptop). The transmitter continuously emits signals to the receiver. The target user performs finger gestures between the transmitter and the receiver.</p><p>Our main contributions are summarized as follows: • Accurate text input using WiFinger in WiFi devices: To evaluate the effectiveness of WiFinger, 10 users are volunteered to provide training and testing data for our scheme. Users 2-10 perform 9 digits finger-grained gesture with 35 instances per gesture. To test the impact from the number of training set, we ask user 1 performs each finger-grained gesture 70 times. Finally, we get 3465 finger-grained gesture instances in our scheme. After that, we randomly generate 20 sequences of digits using matlab, there are 18 random numbers from 1 to 9 in each sequence. All users select 5 sequences to perform in WiFinger. As is demonstrated by the results, WiFinger achieves average recognition accuracy of 90.4% for each user, and its average accuracy of recognizing continuously number text input reaches 82.67%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BACKGROUND</head><p>Prior work on gesture recognition can be generally categorized into two groups: device-based and device-free. Devicebased gesture recognition techniques including audio or radio based <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>, vision based <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref> and sensors based <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>. They need extra hardware devices which might lead to high costs, and they are also constrained by the outside environment such as the ambient noise, light condition, etc. Device-free motion recognition techniques recognize human gestures using the variations of wireless signals.  <ref type="bibr" target="#b7">[8]</ref>. In contrast to all these schemes above, WiFinger does not need SDR or any special purpose devices, we utilize COTS WiFi NICs to recognize fine-grained finger gestures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CHANNEL STATE INFORMATION</head><p>In wireless communications, channel state information refers to known channel properties of a communication link. It depicts how a signal propagates from the transmitter to the receiver and represents the combined effect of scattering, fading and power decay with distance. Modern WiFi devices that support IEEE 802.11n/ac standards typically consist of multiple transmitting (TX) and receiving (RX) antennas and thus support Multiple-Input Multiple-Output (MIMO). In OFDM system, the channel between each pair of TX and RX comprises of multiple subcarriers and the narrow band flat-fading channel with multiple TX-RX antennas is modeled as</p><formula xml:id="formula_0">y = H × x + n,<label>(1)</label></formula><p>where y is the received vector, x is the transmitted vector, n is the noise vector and H is the complex valued Channel Frequency Response (CFR). Let N t and N r denote the number of TX and RX antennas respectively, and N c is the number </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WIFINGER OVERVIEW</head><p>WiFinger is a wireless system that utilizes commercial WiFi devices to achieve human-computer interaction by recognizing people's finger-grained gestures. Figure <ref type="figure" target="#fig_1">2</ref> illustrates the framework of WiFinger, which can be divided into 3 parts.</p><p>For the first part, a transmitter and a receiver comprise the CSI sampling unit. The transmitter is a wireless Access Point (AP), e.g., a router or a laptop serving as a hot spot, and the receiver can be a smart device, e.g., a laptop or a mobile phone. In WiFinger, the transmitter has 1 directional antenna and the receiver is equiped with 3 (other than 1) omnidirectional antennas to achieve higher Signal Noise Ratio (S-NR) as well as higher data rate <ref type="bibr" target="#b37">[38]</ref>.</p><p>While the system is working, the transmitter continuously sends wireless packets to the receiver. These packets were further sampled at the receiver side to extract CSI values. To ensure finger-grained motions recognition, we set the sampling rate to be F s = 2000 packets/s. From each sampled packet, a CSI matrix H is extracted, which has a size of 30 × 1 × 3, as mentioned in the previous section. Although we can obtain 1×3 CSI streams at the same time, only the CSI stream 1-1 is selected, thus H can be simplified to a 30 × 1 dimensional matrix. The impact of multiple streams will be covered in the Discussion and Limitation section. The selected stream C contains a series of CSI matrices and is rep-</p><formula xml:id="formula_1">resented as C = [H t1 , H t2 , • • • , H t L ] = [c 1 , c 2 , • • • , c 30 ] T</formula><p>with size 30 × L, where c i (i = 1, 2, ..., 30) represents different subcarriers.</p><p>In the second part, WiFinger minimizes the interference of environment noise by applying various de-noising methods to the original CSI stream. Then we use a novel gesture detection method to eliminate the "silent" part of CSI stream, and arrange gestures as separated finger gesture profiles.</p><p>The third part serves as a gesture recognizer. From each gesture profile, a special designed feature vector is extracted. The recognition of gestures is achieved by applying k-Nearest-Neighbours (kNN) algorithm, where the distance between two feature vectors is measured with Dynamic Time Warping (DTW). Besides, we compress the feature vectors with Discrete Wavelet Transformation (DWT) to reduce the computational complexity of DTW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SIGNAL PREPROCESSING</head><p>The original CSI stream extracted from commodity WiFi NICs are inherently noisy, and its subcarriers usually have different noise levels, thus we process the CSI values of different subcarriers independently. To refine the CSI stream for finer-grained gestures recognition, WiFinger firstly removes the impracticable outliers of each subcarrier, then passes them through a low-pass filter to remove the high-frequency noise. Although strict low-pass filter can obviously remove noise, the noise within passband cannot be eliminated thoroughly. Thus, further process is necessary. We show that a weighed moving average method reduces these in-band noises efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Outlier Removal</head><p>Because of the internal state transition such as transmission power changes and transmission rate adaptation, there are some burst noises in the selected CSI stream. Figure <ref type="figure" target="#fig_4">3(a)</ref> shows the CSI waveform of two consecutive finger gestures of No.1. It can be observed that there are some abrupt fluctuations in the waveform. Obviously, these outliers are not induced by finger movements. Given that outliers probably affect finger motion detection, WiFinger utilizes Hampel identifier <ref type="bibr" target="#b38">[39]</ref> to eliminate these outliers. It declares all points out of the interval [μγ × σ, μ + γ × σ] as outliers, where μ and σ are the median and median absolute deviation of the data sequence, γ varies in different situations and the most widely used value is 3. Figure <ref type="figure" target="#fig_4">3</ref>(b) plots the CSI waveform after removing outliers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Low-pass Filtering</head><p>In WiFinger, the speed of human finger gestures is low, and the signal changes caused by finger motions lie at the low end of the frequency spectrum while the noise induced by hardware imperfection, e.g., carrier frequency offset, and channel propagation, e.g., shadowing, has a relatively high frequency. Butterworth low-pass filter is a natural choice which has maximum flat frequency response in the pass band and roll off towards zero in the stop band, hence ensures the fidelity of signals in target frequency range and removing out-band noise greatly. Therefore, we apply the filter on the received samples to eliminate the out-of-band interference. We experimentally observe that the frequency of the variations in CSI  stream due to finger gestures is often within 1 ∼ 60 Hz. We set the output of the filter with a cut-off frequency of 60 Hz when applied to the CSI values in Figure <ref type="figure" target="#fig_4">3</ref>(b), with sampling rate F s = 2000 packets/s. The filtered waveform is shown in Figure <ref type="figure" target="#fig_4">3(c)</ref>. Obviously, Butterworth low-pass filter successfully removes most high frequency noise. However, the noise cannot be completely eliminated, hence, we have to do further processing towards the CSI stream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Weighted Moving Average</head><p>Figure <ref type="figure" target="#fig_4">3</ref>(c) shows the CSI stream of two consecutive gestures, which is extracted from the third subcarrier of finger gesture No.1. It can be observed that the CSI waveform is still noisy for gesture extraction. WiFinger further introduces weighted moving average method to the filtered CSI stream. Specifically, as for the CSI values of the</p><formula xml:id="formula_2">3 th sub- carrier c 3 = [h t1 , h t2 , • • • , h t L ],</formula><p>the CSI value at time t i is averaged by previous m values. The latest CSI value has the highest weight m, thus</p><formula xml:id="formula_3">h ti = h ti = 1 m + (m -1) + • • • + 1 • [m • h ti + (m -1) • h ti-1 + . . . + h ti-m+1 ],<label>(2)</label></formula><p>where h ti is the averaged new value at time t i . The value of m decides in what degree the current value is related to the historical records <ref type="bibr" target="#b25">[26]</ref>. Figure <ref type="figure" target="#fig_4">3</ref>(d) shows the weighted moving average waveform. We empirically set m = 30 in WiFinger to achieve preferable denoising results. In the following, the processed CSI stream is utilized to extract gestures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GESTURE EXTRACTION</head><p>We show how WiFinger extracts the CSI stream for each finger gestures. The main effects of human finger motions on the received CSI stream are either rising edges, falling edges, or pausing. These variations are critical for detecting finger gestures and the uniqueness of different variation patterns are exploited to classify finger motions. To detect the starting and finishing points of a gesture, we split the processed CSI stream into segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gesture Detection</head><p>To detect the finger gestures from the CSI stream, we refer to <ref type="bibr" target="#b39">[40]</ref> and design an adaptive method to detect the starting and finishing points of finger gestures. We explain the steps to get sign indicator, and then, we dynamically get the threshold from sign indicators which is further applied to extract finger gestures. The extraction process is arranged as the following four steps.</p><p>1. Preprocessing: Firstly, WiFinger removes the Direct Current (DC) component of every subcarrier by subtracting the corresponding constant offsets from the CSI stream.The constant offsets can be calculated through a long-term averaging over different subcarriers. Then WiFinger cuts the CSI stream C into bins with a sliding window. The window size is 500 and the step length is set as 400. Each bin is a matrix of size 30 × 500, represented as M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Correlation estimation:</head><p>WiFinger calculates the correlation matrix as M T × M. The dimension of the correlation matrix equals the number of CSI subcarriers extracted from Intel 5300 NIC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Sign indicator calculation:</head><p>WiFinger calculates the eigenvectors and eigenvalues of the correlation matrix base on a key observation. Namely, when there is no gesture or motion in the experimental environment, the second eigenvector q 2 varies randomly over neighboring subcarriers. However, when a finger gesture happens, the CSI subcarriers become correlated, q 2 varies smoothly and its mean of first difference δ q 2 = 1</p><p>Nc-1 Nc l=2 |q 2 (l)q 2 (l -1)| becomes smaller, where N c represents the number of CSI subcarriers and |q 2 (l)q 2 (l -1)| represents the difference in coefficients for neighboring subcarriers. Besides, the occurrence of gestures results in a higher variance E{h 2 2 } of the principal component h 2 . Thus we define E{h 2 2 }/δ q 2 , as a sign indicator to indicate the occurrence of finger motions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Smoothing:</head><p>As is shown in Figure <ref type="figure" target="#fig_3">4(a)</ref>, there are some abrupt changes in the waveform of sign indicator which are likely to lead to fallacious results of detection. Therefore, we apply a 5-point median filter to the set of sign indicator to facilitate the detection of finger motions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Finger Gesture Profile Extraction</head><p>Considering the characteristics of sign indicator values calculated above, we empirically select the value of the third quartile of the sign indicator values as a threshold to automatically detect and extract finger gestures. The threshold can be dynamically adjusted in accordance with variation of CSI values in real time. Figure <ref type="figure" target="#fig_3">4</ref>(a) and 4(b) plot the sign indicator before and after applying 5-point median filter. We observe that 5-point median filter facilitates finger gesture extraction in WiFinger.</p><p>In </p><formula xml:id="formula_4">-T b , t e 1 + T b , t s 2 -T b , t e 2 + T b , • • • t s n -T b , t e n + T b }.</formula><p>The CSI values between each pair of starting and finishing points are mentioned as finger gesture profiles, represented as </p><formula xml:id="formula_5">P i = [H t s i -T b , • • • H t e i +T b ](i = 1, 2, • • • n). P i</formula><formula xml:id="formula_6">p i (i = 1, 2, • • • n).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FEATURE EXTRACTION</head><p>After finger gesture profile extraction, WiFinger obtains motion profiles for different finger gestures. Figure <ref type="figure">5</ref>(a) to 5(c) show the CSI values variation in 30 subcarriers of digits 1, 4, 7 in ASL. According to characteristics depicted by these time series patterns of different finger gestures, we find that simply utilizing a single subcarrier or several subcarriers cannot fully represent the distinguishable features of each finger motion. The finger motion features distribute in all 30 subcarriers, Wi-Finger must integrate those distributed features to identify different finger motions. Hence WiFinger combines 30 subcarriers by averaging every 6 subcarriers, and then WiFinger concatenates them to form a synthetic waveform, mentioned as a feature vector high computational costs. WiFinger compresses the feature vectors by utilizing DWT. The advantage of DWT are twofolds: 1) It compresses the original signal while preserving both time and frequency domain information. This attribute facilitates the signal analysis and reduces the computational cost in the process of classification. 2) In WiFinger, the finger gestures are finer-grained movement, DWT realizes multiscale analysis towards finger gesture profiles. Owing to the lengths of finger gesture features are not equal, Euclidean distance is not suitable in this scenario. To distinguish the minute difference among finger movements, WiFinger exploits DTW to calculate the distance between two finger gesture features.</p><formula xml:id="formula_7">F = [f 1∼6 , f 7∼12 , f 13∼18 , f 19∼24 , f 25∼30 ], where f k∼l = 1 l-k+1 l i=k p i .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discrete Wavelet Transformation</head><p>The classical Fourier transform and short-time Fourier transform do not provide multiple resolution in time and frequency, which is an important characteristic for analyzing transient signals containing both high and low frequency components. The Discrete Wavelet Transformation overcomes those limitations by employing analyzing functions that are local both in time and frequency. The DWT is defined by the following equation:</p><formula xml:id="formula_8">W (j, k) = j k x(k)2 -j/2 ψ(2 -j n -k) (3)</formula><p>where ψ(t) is a time function with finite energy and fast decay called the mother wavelet. The DWT analysis can be performed using a fast, pyramidal algorithm related to multirate filterbanks <ref type="bibr" target="#b40">[41]</ref>. In the pyramidal algorithm the signal is analyzed at different frequency bands with different resolution by decomposing the signal into a coarse approximation coefficients and detail coefficients. The coarse approximation coefficients is then further decomposed using the same wavelet decomposition step. The complete binary tree in the decomposition process can be shown in Figure <ref type="figure">6</ref>.</p><p>The efficacy of wavelet transformation depends on selecting appropriate wavelet basis. In WiFinger, several wavelet families have been tested such as Daubechies, Coiflets, Symlets. Due to the classification performance, the Daubechies D4 coefficient wavelet family is selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamic Time Warping</head><p>WiFinger utilizes kNN classifier to recognize different finger gestures. Traditional kNN classifier adopts Euclidean distance as the measurement criteria between samples. Considering the feature vectors of gestures might not share the same length, we use DTW to calculate the distances among features. In contrast to Euclidean distance, DTW provides intuitive distance between two waveform and can be resilient to signal distortion or shift. DTW distance is the Euclidean distance of the optimal warping path between two waveforms calculated under boundary conditions and local path constraints <ref type="bibr" target="#b41">[42]</ref>. The objective of DTW is to compare two (timedependent) series  features, and searches for the class majority label among k nearest neighbors of the corresponding finger gesture feature.</p><formula xml:id="formula_9">X = (x 1 , x 2 , • • • , x n ) of length n ∈ N + and Y = (y 1 , y 2 , • • • , y m ) of length m ∈ N + .</formula><formula xml:id="formula_10">X[n] L[n] H[n] 2 2 Level 1 coefficients L[n] H[n] 2 2 Level 2 coefficients Level 3 coefficients L[n] H[n]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IMPLEMENTATION &amp; EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hardware Setup</head><p>In the experiment, we use one 3.2 GHz dual core CPU, 4G-B ROM desktop, which is equipped with Intel 5300 NIC, as the receiver, and its operating system is Ubuntu 12.04. A TP-LINK TLWDR4300 wireless router connecting to a laptop with a cable is deployed as the transmitter, and the router is set as a AP. The AP possesses one DB-Link directional antenna and operates in IEEE 802.11n AP mode at 5.745 GHz (channel 149). The receiver has 3 omni-directional antennas and its firmware is modified as in <ref type="bibr" target="#b42">[43]</ref> to report CSI to upper layers. In addition, the laptop also serves as a Monitor Point (MP) and connects to the receiver by SSH, which realizes remote control to minimize the interferences from other operators. The receiver and the AP are posited on cabinets at the same height. The distance between the receiver and the AP is 50 cm, and the user performs finger gestures in the middle of Line-Of-Sight (LOS) path between TX and RX antennas. We set the directional antenna facing the receiver, which enhances the received signals to capture the subtle signal changes caused by finger gestures of the target user. Figure <ref type="figure">7</ref> illustrates the experimental setup of WiFinger. During the measurement campaign, the MP continuously sends packets with a high data rate of about 2000 packets/s towards receiver using PKTGEN tool through the WiFi router. Setting a higher sending packets frequency leads to a higher sampling rate of CSI, which ensures the time resolution of CSI values for capturing the subtle changes in CSI stream and maximizes the details of different finger motions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Controlled Gesturing</head><p>To accurately measure the CSI variations caused by finger motions during data collection, we ask users to sit on a fixed chair aside the middle of LOS path between transceivers. For the sake of avoiding the interferences caused by the movement of other human body parts, we instruct the users not to move their heads or other body parts in significant scale. Users perform the gestures naturally while keeping the interval time of gestures between 1 to 2 seconds to make the starting and finishing points of finger motions easily to be identified. When the user performs finger gestures, his/her hand remains in the fixed position continuously and only the  fingers move. Figure <ref type="figure" target="#fig_8">8</ref> shows how the user performs gesture "3" and gesture "5" successively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Collection</head><p>We collect training and testing data from 10 users to feed the classification algorithm. Those users are college students who volunteered to collect experiment data for WiFinger. We give them a brief introduction about the finger gestures digit 1 to 9 in ASL. Users 2-10 perform these gestures each for 35 times under our environment to build the training data sets. To evaluate the effects of training samples in different size. We collect 70 instances for each of the 9 finger-grained gestures from user 1. The users perform the gestures sequences naturally with an average time interval at 2 seconds. We totally acquire 3465 instances for WiFinger. After that, we randomly generate 20 sequences using matlab with 18 digits per sequence, and each user chooses 5 sequences to perform. Wi-Finger selects the CSI stream of the above instances, and processes the original CSI values following three steps: signal preprocessing, gesture extraction and then classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Automatic Gesture Extraction Accuracy</head><p>We evaluate the accuracy of the finger motion detection algorithm in WiFinger. The detection accuracy is defined as the ratio of total number of correctly extracted finger gesture profiles to the total number of finger motions performed by each user. Figure <ref type="figure" target="#fig_9">9</ref> plots the percentage of correctly extracted finger motions of 9 digits from 10 users.</p><p>As the figure shows, the detection accuracy varies among users. This is caused by the habits of performing finger motions. Different users probably perform the same gesture in slightly different ways or orientations. The behaviors of performing finger motions affect the detection rates of WiFinger. For example, we observed that user 2 and user 7 separately have average gesture detection ratio of 76% and 81%. We also find that these two users sometimes cannot fully stretch their fingers in some finger gestures. For instance, when they perform finger gesture 3 and 9, their fingers stretch in small </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gesture Recognition Accuracy</head><p>Here, we elaborate the recognition accuracy of finger gesture in WiFinger. We evaluate WiFinger with two kinds of experiments. In the first experiment, we separately feed the userspecific kNN classifiers using 35 profiles per finger gesture of 10 users. In all these experiments, we set k = 5, and then we perform 10-fold cross validation on the finger gesture features in each classifier to acquire the recognition accuracy of each finger gesture. In the second experiment, we test each classifier using corresponding three randomly selected 18 digit sequences performed by each user. As the results show, Wi-Finger provides a recognition accuracy more than 90% for the continuously random 18 digit sequences. Next, we give detailed explanations for the evaluation process of WiFinger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Average Recognition Accuracy</head><p>We evaluate the recognition accuracy of WiFinger using average recognition accuracy per user and average recognition accuracy per finger gesture. For each user, we calculate the mean value of the total recognition accuracy of 9 digits finger gestures. And we also get the average accuracy per finger gesture of all 10 users. Figure <ref type="figure" target="#fig_0">10</ref> shows the average recognition accuracy per gesture. We observe that the recognition accuracies of gesture 3 and 9 are around 85% which are relatively lower than the other gestures. This is because we set the initial gesture as a fist, and gesture 3 and 9 need to open several consecutive fingers by a large margin. Some users hardly perform these gestures with a fully opened palm, hence lead to a relatively low recognition accuracy. However, the overall finger gesture recognition accuracy is still more than 90%.  result shows that the recognition accuracy of user 5 is around 80%, this is related to his behaviors when he performs finger gestures. The sitting posture and the behaviors of performing gestures will affect the recognition accuracy in our scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impacts of Training Set Size</head><p>To evaluate the impact of the training set size. We increase the number of training samples per finger gesture from 35 to 70. And we perform 10-fold cross validation towards the 9 finger gestures when 35 and 70 training samples are used per gesture. Figure <ref type="figure" target="#fig_1">12</ref> plots the recognition accuracy for user 1 with two different training set size. We observe that the average recognition accuracy increased from 93.37% to 96.25% for user 1 with the number of training samples increasing from 35 to 70.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number Text Input Using WiFinger</head><p>To evaluate the accuracy of continuous text input in WiFinger, we collected CSI values for the randomly selected 18 digits sequences performed by different users as mentioned before. We use the dataset of individual finger gesture features as training set, and the extracted finger gesture features from the corresponding three randomly selected 18 digit sequences of the same user as test data to feed the classifier. As is shown by Figure <ref type="figure" target="#fig_4">13</ref>. WiFinger achieves average recognition accuracy of 82.67% for all finger gesture sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DISCUSSTION AND LIMITATIONS</head><p>In our current experimental environment such as laboratory and dormitory, WiFinger runs well under a relatively stable  environment with only two occupants, a target user and a system controller. The accuracy of WiFinger is related to the variations of environment such as human moving around the target user, the orientation and distance between transmitter and receiver. The following illustrations are some discussions of WiFinger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Differences among TX-RX Antenna Pairs</head><p>Antennas separated by more than half a wavelength (&gt; 2.6 cm at 5.745 GHz) have independent fading channels <ref type="bibr" target="#b37">[38]</ref>.</p><p>In WiFinger, the distances between each two RX antennas are greater than 2.6 cm, and we totally obtain 1 × 3 CSI streams.</p><p>To understand the differences among TX-RX antenna pairs, we analyzed all 3 CSI streams in our experiment. We have the following observation: (1) Figure <ref type="figure" target="#fig_3">14</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interference with Multipath Reflection</head><p>In our experiment, the variations in the environment may affect the accuracy of WiFinger, such as human body motions and moving objects around the target user. WiFinger is designed only for single target user with a system administrator who guides the target user during the data collection.  two environments. We observe that they almost have approximate patterns in the two different environments. Moreover, owing to the directivity of our TX antenna, a irrelevant person who walks or twists his body out of the coverage range has little interference on WiFinger. Hence, WiFinger is resilient to the static reflections or some marginal interferences. However, when another person moves around the target user, his activities can disturb or even cover the minute CSI fluctuations induced by finger gestures, which leads to unsatisfactory results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Devices Positioning</head><p>WiFinger applies DB-Link directional antenna operating in the 5GHz frequency band. We test the classification results under different distances of transceivers with the target user sitting aside in the middle between TX and RX antennas. We put the receiver at a fixed position and adjust the position of the transmitter. Figure <ref type="figure" target="#fig_19">16</ref>(a), 16(b) and 16(c) show the waveforms of finger gesture No.2 at different distances between transceivers. We find that the detected finger gesture patterns get weaker with the distance between transceivers increasing. And when the distance exceeds 1m, the patterns caused by finger gestures in CSI stream nearly disappeared. Therefore, in order to capture obvious and stable finger gesture patterns in CSI stream, we should place transceivers at a relatively close distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Different Frequency Band</head><p>Due to the large number of devices operating at 2.4 GHz in our surrounding environment, 2.4 GHz frequency band is crowded and more vulnerable to be interfered.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CSI Sampling Rate</head><p>Sampling rate is extremely important for finer-grained gesture recognition, because the changes caused by finger motions are subtle. The accuracies of finger motion detection and extraction both rely on a high time resolution of CSI values. Higher sampling rate means more CSI values between the starting and finishing point of each gesture, which contains more information in CSI waveform, hence increases the finger motion extraction and classification accuracy. To verify this assumption, we also test WiFinger at sampling rate of 500 packets/s, and ask user 1 to perform 35 times per finger gesture. We find that the periods of finger gestures in collected CSI streams are obviously shorter. Therefore, we adjust the parameters in WiFinger such as segment window length, sliding step length and guard interval. We perform 10-fold cross validation on the data obtained. After evaluation, the average recognition accuracy with a sampling rate of 500 packets/s is 80.95%. However, the average recognition accuracy is 93.38% when the sampling rate reaches 2000 packets/s. Thus, CSI sampling rate is especially important for finer-grained gesture recognition in WiFi environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context-based Error Correction</head><p>For the the environment and users that WiFinger has been trained on, it achieves an average accuracy of more than 90%. Currently, WiFinger enables the recognition and number text input using 9 digit finger gestures from ASL. In future work, we prepare to enlarge the finger gesture set and achieve text input of any characters or sentences with high accuracy. We note that the recognition accuracy could be further improved by utilizing context-based error correction. This idea is inspired by widely used context-aware approaches in automatic speech recognition <ref type="bibr" target="#b46">[47]</ref>. For example, when the target user continuously performs finger gestures to achieve continuous text input using WiFinger. When WiFinger detects a wrong word like "gestuer", WiFinger can automatically distinguish and recognize "gesture" instead of "gestuer" based on the context. This method reduces the recognition mistakes and facilitates the recognition of gestures in WiFinger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONCLUSION</head><p>In this paper, we present WiFinger, a novel system that enables WiFi signals to realize continuously number text input in WiFi devices by recognizing finger-grained gestures from American Sign Language. WiFinger exploits the ubiquitous WiFi signals to sense finger-grained gesture, and it does not need any specialized hardware devices. Hence it can be extended easily to commercial Wi-Fi products. The preliminary results show that WiFinger achieves high recognition accuracy with an average recognition accuracy 90.4% per user. WiFinger collects channel state information available in the existing WiFi devices. We benefit from the observation that different human activities, for example finger gestures, lead to a number of unique CSI time-series patterns. Thus, we are inspired to explore the potential of sensing finger-grained gestures under wireless environment. Compared with other gesture recognition schemes, WiFinger is a non-intrusive and device-free solutions in finger-grained gesture recognition. Given that our scheme is able to identify subtle changes caused by tiny movement of human body, therefore, it can be extended and improved to wide areas such as fine-grained gesture recognition, human activities detection, or any motion recognition scenarios.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Part of finger gestures [12] that WiFinger can detect and recognize.</figDesc><graphic coords="2,333.11,62.30,219.15,104.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Framework of WiFinger</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>After 5-point median filtering</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Finger gestures detected by sign indicator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The procedure of signal preprocessing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>has a size of N c ×l, where N c = 30 represents the amount of CSI subcarriers, and l = t e it s i + 2 • T b represents gesture duration. The profiles of different subcarriers is represented as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 ( 7 Figure 5 .</head><label>575</label><figDesc>Figure 5. The features of different finger gestures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>2 2Figure 6 .Figure 7 .</head><label>267</label><figDesc>Figure 6. Discrete wavelet transformation structure</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. The way of user performing finger gestures.</figDesc><graphic coords="7,325.19,62.30,235.10,74.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Finger gesture extraction accuracy per gesture for users 1-10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 .Figure 11 .</head><label>1011</label><figDesc>Figure 10. Recognition accuracy per gesture for users 1-10</figDesc><graphic coords="8,324.55,62.30,236.59,137.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Figure 11 plots the average recognition accuracy per user. As the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 .Figure 13 .</head><label>1213</label><figDesc>Figure 12. The average accuracy for user 1 using different instances per gesture</figDesc><graphic coords="8,344.39,232.70,215.05,104.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 14 .Figure 15 .</head><label>1415</label><figDesc>Figure 14. Finger gesture No.3 in two different multipath environment.</figDesc><graphic coords="9,67.73,184.16,96.43,59.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>(a), Figure 15(a) and Figure 15(b) illustrate the CSI variations caused by the same finger gesture No.3 in all 3 CSI streams, respectively, and they are dramatically different. (2) It improves finger gesture recognition by integrating features resulted from finger gesture in all 3 CSI streams, because combining all 3 streams can obtain more detailed and distinguishable features for finger gestures. (3) However, it absolutely increases the time and space complexity during DTW. WiFinger finds a trade-off between accuracy and computational complexity and selects only one stream to recognize finger gestures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>Figure 14(a) and14(b)  show the CSI stream patterns of two different environments in the same 5 GHz band. Environment 1 is our original experimental environment. In addition, we add two chairs and a cabinet around the receiver to form Environment 2. Thus, there are different multipath reflectors in these</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 17 .</head><label>17</label><figDesc>Figure 17. Features comparison at 2.4 GHz and 5 GHz.</figDesc><graphic coords="9,67.55,69.32,97.03,56.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 16 .</head><label>16</label><figDesc>Figure 16. Different distances between transceivers.</figDesc><graphic coords="10,238.79,69.56,143.35,60.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Han et al.  proposed WiFall that detects a single human activity of falling<ref type="bibr" target="#b25">[26]</ref>. Zou et al. proposed Electronic Frog Eye that counts the number of people in a crowd using CSI values by treating the people reflecting the WiFi signals as "virtual antennas"<ref type="bibr" target="#b28">[29]</ref>. Wang et al. proposed E-eyes that exploits CSI values for recognizing household activities such as washing dishes and taking a shower<ref type="bibr" target="#b26">[27]</ref>.Zhou et al.  proposed to use CSI to detect the presence of a person in an indoor environment<ref type="bibr" target="#b27">[28]</ref>. Wang et al. proposed WiHear which employs specialized directional antennas to obtain CSI changes caused by lip movement for recognizing spoken words<ref type="bibr" target="#b1">[2]</ref>. Ali et al. proposed WiKey that uses CSI values obtained from COTS to recognize keystrokes<ref type="bibr" target="#b0">[1]</ref>. These splendid research specialized in specific application scenarios which do not contain continuous text input using CSI characteristics. Therefore, we propose a scheme which provides a solution to achieve text input using finger-grained gestures from standard ASL under WiFi environment.Specialized Devices Based. Software Defined Radio (SDR) is a special purpose equipment that provides sophisticated signal features. Researchers have put forward various schemes that utilize SDRs to transmit and receive custom modulated signals for human activity recognition<ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>.Pu et al.  proposed WiSee that uses a special purpose receiver design on USRPs to extract small Doppler shifts from OFDM WiFi transmissions to recognize human gestures<ref type="bibr" target="#b5">[6]</ref>. Kellogg et al. presented AllSee that employs specialized analog envelop detector circuit for recognizing gestures within a distance of up to 2.5 feet using backscatter signals from RFID or TV transmissions<ref type="bibr" target="#b33">[34]</ref>.Adib et al.  </figDesc><table><row><cell>including lying down, crawling, standing and walking and achieved over 80% recognition accuracy for these four activities. Abdelnasser et al. proposed WiGest which leverages RSS values in WiFi signal strength to sense in-air hand gestures around the us-er's mobile device [25]. WiGest can achieve a classification accuracy of 96% for the application actions. According to the studies by former researchers, RSS values are not suitable for recognizing fine-grained motions such as gestures in stan-dard ASL because RSS values only provide coarse-grained information about channel variations and do not contain fine-grained information about small scale fading and multi-path effects caused by micro-movements. CSI Based. Recently CSI values obtained from COTS Wi-Fi NICs (such as Intel 5300 and Atheros 9390) are widely applied to recognize human activities [2, 26, 27, 28, 29, 30] and localize objects [31, 32, 33]. designed WiTrack and WiTrack2.0 that apply specially designed frequency modu-lated carrier wave radio frontend to track human movements behind a wall [36]. Recently, Chen et al. built an SDR based custom receiver design to track keystrokes using wireless sig-nals [37]. Melgarejo et al. proposed a fine-grained gesture recognition scheme using sophisticated WARP v3 board e-quipped with two RE14P directional patch antennas</cell></row></table><note><p>It does not need any additional hardware devices. Researchers have proposed various solutions which can be grouped into three categories: RSS Based. Sigg et al. used USRPs which are specialized hardware devices to capture Received Signal Strength (RSS) values from WiFi signals [23, 24]. They utilized RSS values of WiFi signals to recognize four activities</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>the process of gesture extraction, WiFinger sets a guard interval T b on both side of the estimated finger gesture profile. The algorithm compares the values in the set of sign indicator with threshold to obtain their intersections as the candidates of starting or finishing points of finger gestures. For example, says the candidate set is Then the starting points should minus T b and the finishing points plus T b . The purpose of guard interval is to better obtain the complete finger gesture profiles. Hence the candidate set turns into {t s 1</figDesc><table><row><cell>{t s 1 , t e 1 , t s 2 , t e 2 , • • • t s n , t e n }.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>These sequences may be discrete signals (time series) or, more generally, feature sequences sampled at equidistant points in time. DTW can handle waveforms with different lengths and calculates distance by recovering optimal alignments between the two time series. WiFinger feeds the finger gesture features to kNN classifier and obtains a decision. The kNN classifier uses DTW to calculate the distance between finger gesture</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="250" xml:id="foot_0"><p>UBICOMP'16, SEPTEMBER 12-16, 2016, HEIDELBERG, GERMANY   </p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We would like to thank the anonymous reviewers for their insightful comments for improving the quality of the paper. We thank the volunteers in our lab who helped us in collecting the finger gesture dataset. This work was supported by the National Natural Science Foundation of China (No. 61572456) and the Natural Science Foundation of Jiangsu Province of China (No. BK20151241).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Keystroke recognition using wifi signals</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shahzad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of ACM MobiCom</title>
		<meeting>of ACM MobiCom</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="90" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">We can hear you with wi-fi</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of ACM MobiCom</title>
		<meeting>of ACM MobiCom</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="593" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Soundwave: using the doppler effect to sense gestures</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>of the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1911" to="1914" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">Leap Motion</orgName>
		</author>
		<ptr target="https://www.leapmotion.com" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Microsoft</forename><surname>Kinect</surname></persName>
		</author>
		<ptr target="http://www.roborealm.com/help/MicrosoftKinect.php" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Whole-home gesture recognition using wireless signals</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gollakota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of ACM MobiCom</title>
		<meeting>of ACM MobiCom</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="27" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">See through walls with wifi</title>
		<author>
			<persName><forename type="first">F</forename><surname>Adib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Katabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of ACM SIGCOMM</title>
		<meeting>of ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="75" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Leveraging directional antenna capabilities for fine-grained gesture recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Melgarejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of ACM UbiComp</title>
		<meeting>of ACM UbiComp</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="541" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Tool release: Gathering 802.11n traces with channel state information</title>
		<author>
			<persName><forename type="first">D</forename><surname>Halperin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sheth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wetherall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGCOMM Computer Communication Review</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="53" to="53" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A survey of glove-based input</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Sturman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zeltzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="30" to="39" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A survey of glove-based systems and their applications</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dipietro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Sabatini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="461" to="482" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Finger</forename><surname>Gesture</surname></persName>
		</author>
		<ptr target="http://www.lifeprint.com/dictionary.htm" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Device-free and device-bound activity recognition using radio signal strength</title>
		<author>
			<persName><forename type="first">M</forename><surname>Scholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Beigl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of the 4th Augmented Human International Conference</title>
		<meeting>of the 4th Augmented Human International Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="100" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sonar-based measurement of user presence and attention</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Tarzia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Dinda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Memik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of ACM UbiComp</title>
		<meeting>of ACM UbiComp</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="89" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Radio tomographic imaging and tracking of stationary and moving people via kernel distance</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Patwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkatasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of the 12th international conference on Information processing in sensor networks</title>
		<meeting>of the 12th international conference on Information processing in sensor networks</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="229" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Real-time american sign language recognition from video using hidden markov models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Starner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Motion-Based Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="227" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Vision-based hand-gesture applications</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kölsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Edan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="60" to="71" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visual tracking of high dof articulated structures: an application to human hand tracking</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision ECCV&apos;94</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="35" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Towards using embedded magnetic field sensor for around mobile device 3d interaction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ketabdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Roshandel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Kamer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of the 12th international conference on Human computer interaction with mobile devices and services</title>
		<meeting>of the 12th international conference on Human computer interaction with mobile devices and services</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="153" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Magnetic signatures in air for mobile devices</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ketabdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Moghadam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Naderi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Roshandel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of the 14th international conference on Human-computer interaction with mobile devices and services companion</title>
		<meeting>of the 14th international conference on Human-computer interaction with mobile devices and services companion</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="185" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Using mobile phones to write in air</title>
		<author>
			<persName><forename type="first">S</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Constandache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gaonkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Caves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Deruyter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of ACM Mobisys</title>
		<meeting>of ACM Mobisys</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="15" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">E-gesture: a collaborative architecture for energy-efficient gesture recognition with hand-worn sensor and mobile devices</title>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of ACM SenSys</title>
		<meeting>of ACM SenSys</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="260" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Leveraging rf-channel fluctuation for activity recognition: Active and passive systems, continuous and rssi-based signal features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sigg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Buesching</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of International Conference on Advances in Mobile Computing &amp; Multimedia</title>
		<meeting>of International Conference on Advances in Mobile Computing &amp; Multimedia</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">43</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rf-sensing of activities from non-cooperative subjects in device-free recognition systems using ambient and local signals. Mobile Computing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sigg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Scholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Beigl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="907" to="920" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A ubiquitous wifi-based gesture recognition system</title>
		<author>
			<persName><forename type="first">H</forename><surname>Abdelnasser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Youssef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Harras</surname></persName>
		</author>
		<author>
			<persName><surname>Wigest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Communications (INFOCOM), 2015 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1472" to="1480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Device-free fall detection by wireless networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><surname>Wifall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of IEEE INFOCOM</title>
		<meeting>of IEEE INFOCOM</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="271" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">E-eyes: device-free location-oriented activity identification using fine-grained wifi signatures</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gruteser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of ACM MobiCom</title>
		<meeting>of ACM MobiCom</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="617" to="628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Towards omnidirectional passive human detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shangguan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of IEEE INFOCOM</title>
		<meeting>of IEEE INFOCOM</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3057" to="3065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Electronic frog eye: Counting crowd using wifi</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of IEEE INFOCOM</title>
		<meeting>of IEEE INFOCOM</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="361" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Wi-fi gesture recognition on existing devices</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nandakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kellogg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gollakota</surname></persName>
		</author>
		<idno>CoRR abs/1411.5394</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Avoiding multipath to revive inbuilding wifi localization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Congdon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of the 11th annual international conference on Mobile systems, applications, and services</title>
		<meeting>of the 11th annual international conference on Mobile systems, applications, and services</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="249" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fifs: Fine-grained indoor fingerprinting system</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Ni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Communications and Networks (ICCCN), 2012 21st International Conference on</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">From rssi to csi: Indoor localization via channel response</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bringing gesture recognition to all devices</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kellogg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Talla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gollakota</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th USENIX Symposium on Networked Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="303" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Human gait classification using microdoppler time-frequency signal representations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lyonnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ioana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Amin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Radar Conference</title>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
			<biblScope unit="page" from="915" to="919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">3d tracking via body radio reflections</title>
		<author>
			<persName><forename type="first">F</forename><surname>Adib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kabelac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Katabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th USENIX Symposium on Networked Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="317" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Tracking keystrokes using wireless signals</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Yenamandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of the 13th Annual International Conference on Mobile Systems, Applications, and Services</title>
		<meeting>of the 13th Annual International Conference on Mobile Systems, Applications, and Services</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="31" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Two antennas are better than one: A measurement study of 802</title>
		<author>
			<persName><forename type="first">D</forename><surname>Halperin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sheth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wetherall</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The identification of multiple outliers</title>
		<author>
			<persName><forename type="first">L</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Gather</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="782" to="792" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Understanding and modeling of wifi signal based human activity recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shahzad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of ACM MobiCom</title>
		<meeting>of ACM MobiCom</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="65" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A theory for multiresolution signal decomposition: the wavelet representation. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="674" to="693" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Information retrieval for music and motion</title>
		<author>
			<persName><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
	<note>Dynamic time warping</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Predictable 802.11 packet delivery from wireless channel measurements</title>
		<author>
			<persName><forename type="first">D</forename><surname>Halperin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sheth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wetherall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of ACM SIGCOMM</title>
		<meeting>of ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="159" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Wireless LAN medium access control (MAC) and physical layer (PHY) specifications</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">C S L M S</forename><surname>Committee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Std</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Next Generation Wireless LANS: 802.11 n and 802.11 ac</title>
		<author>
			<persName><forename type="first">E</forename><surname>Perahia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stacey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Spotlight synthetic aperture radar: Signal processing algorithms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Quegan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal for Numerical Methods in Engineering</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="1498" to="1517" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A survey on context-aware systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Baldauf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dustdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rosenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Ad Hoc and Ubiquitous Computing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="263" to="277" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
