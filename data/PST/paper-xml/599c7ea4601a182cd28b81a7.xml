<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Maximizing Cache Performance Under Uncertainty</title>
				<funder>
					<orgName type="full">Qatar Computing Research Institute</orgName>
				</funder>
				<funder ref="#_tBEnNX8">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Nathan</forename><surname>Beckmann</surname></persName>
							<email>beckmann@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Daniel Sanchez Carnegie Mellon University</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Maximizing Cache Performance Under Uncertainty</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/HPCA.2017.43</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Much prior work has studied cache replacement, but a large gap remains between theory and practice. The design of many practical policies is guided by the optimal policy, Belady's MIN. However, MIN assumes perfect knowledge of the future that is unavailable in practice, and the obvious generalizations of MIN are suboptimal with imperfect information. What, then, is the right metric for practical cache replacement?</p><p>We propose that practical policies should replace lines based on their economic value added (EVA), the difference of their expected hits from the average. Drawing on the theory of Markov decision processes, we discuss why this metric maximizes the cache's hit rate. We present an inexpensive implementation of EVA and evaluate it exhaustively. EVA outperforms several prior policies and saves area at iso-performance. These results show that formalizing cache replacement yields practical benefits. * This work was done while the author was at MIT. lacking a theoretical foundation, it is unclear if any are taking the right approach. Each policy performs well on particular programs, yet no policy dominates overall, suggesting that these policies are not making the best use of available information.</p><p>This paper seeks to bridge theory and practice. We take a principled approach that builds on insights from recent empirical designs. First, we show that policies should replace candidates by their economic value added (EVA); i.e., how many more hits one expects from each candidate vs. the average candidate. Second, we design a practical implementation of this policy and show it outperforms existing policies.</p><p>Contributions: Contributions: Contributions: Contributions: This paper contributes the following:</p><p>? We discuss the two main tradeoffs in cache replacement: hit probability and cache space, and describe how EVA reconciles them in a single, intuitive metric. ? We show that EVA maximizes the cache hit rate by drawing on Markov decision process (MDP) theory. ? We present a practical implementation of EVA, which we have synthesized in a 65 nm commercial process. EVA adds 1% area on a 1 MB cache vs. SHiP. Our implementation is the first adaptive policy that does not require set sampling or auxiliary tag monitors. ? We evaluate EVA against prior high-performance policies on SPEC CPU2006 and OMP2012 over many cache sizes. EVA reduces LLC misses over these policies at equal area, closing 57% of the gap from random replacement to MIN vs. 47% for SHiP <ref type="bibr" target="#b38">[39]</ref>, 41% for DRRIP <ref type="bibr" target="#b16">[17]</ref>, and 42% for PDP <ref type="bibr" target="#b13">[14]</ref>.</p><p>Fewer misses translate into large area savings-EVA matches SHiP's performance with gmean 8% less total cache area. These contributions show that formalizing cache replacement yields practical benefits. EVA blends theory and practice to maximize upon available information and outperform many recent empirical policies. Beyond our particular design, we expect our analysis will prove useful in the design of future high-performance policies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Road map:</head><p>Road map: Road map: Road map: Sec. II reviews prior approaches to cache replacement, and Sec. III discusses the key constraints that practical policies face. Sec. IV presents EVA and Sec. V sketches its theoretical justification. Sec. VI presents a simple implementation, which Sec. VII evaluates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND</head><p>Practical replacement policies must cope with uncertainty, never knowing precisely when a candidate will be referenced. The challenge is uncertainty itself, since with complete information the optimal policy is simple: evict the candidate that is referenced furthest in the future (MIN [10,<ref type="bibr" target="#b24">25]</ref>).</p><p>Broadly speaking, prior work has taken two approaches to replacement under uncertainty. On the one hand, designers</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Last-level caches consume significant resources, often over 50% of chip area <ref type="bibr" target="#b23">[24]</ref>, so it is crucial to manage them efficiently. Prior work has approached cache replacement from both theoretical and practical standpoints. Unfortunately, there is a large gap between theory and practice.</p><p>From a theoretical standpoint, the optimal policy is Belady's MIN <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25]</ref>, which evicts the candidate referenced furthest in the future. But MIN needs perfect knowledge of the future, which makes it impractical. In practice, policies must cope with uncertainty, never knowing exactly when candidates will be referenced. Theory-based policies account for uncertainty by using a simplified, statistical model of the reference stream in which the optimal policy can be solved for.</p><p>Unfortunately, these policies generally perform poorly compared to empirical designs. The key challenge faced by theoretical policies is choosing their underlying statistical model. This model should capture enough information about the access stream to make good replacement decisions, yet must be simple enough to analyze. For example, some prior work uses the independent reference model (IRM), which assumes that candidates are referenced independently with static, known probabilities. In this model, the optimal policy is to evict the candidate with the lowest reference probability, i.e., LFU <ref type="bibr" target="#b1">[2]</ref>. Though useful in other areas (e.g., web caches <ref type="bibr" target="#b3">[4]</ref>), the IRM is inadequate for processor caches because it assumes that reference probabilities do not change over time.</p><p>Instead, replacement policies for processor caches are designed empirically, using heuristics based on observations of common-case access patterns <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b38">39]</ref>. We observe that, unlike the IRM, these policies do not assume static reference probabilities. Instead, they exploit dynamic behavior through various mechanisms. While often effective, high-performance policies employ many different heuristics and, can develop a probabilistic model of how programs reference memory and then solve for the optimal replacement policy within this reference model. On the other hand, designers can observe programs' behaviors and find best-effort heuristics that perform well on common access patterns.</p><p>These two approaches are complementary: theory yields insight into how to approach replacement, which is used in practice to design policies that perform well on real applications at low overhead. For example, the most common approach to replacement under uncertainty is to predict when candidates will be referenced and evict the candidate that is predicted to be referenced furthest in the future. This longstanding approach takes inspiration from theory (i.e., <ref type="bibr">MIN)</ref> and has been implemented in recent empirical policies <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20]</ref>. (However, Sec. III shows that this strategy is suboptimal.)</p><p>Yet despite the evident synergy between theory and practice, the vast majority of research has been on the empirical side. We believe that, given the diminishing performance improvements of recent empirical policies, theoretical approaches deserve a second look. Replacement in theory: Replacement in theory: Replacement in theory: Replacement in theory: The challenge for theoretical policies is to define a reference model that is simple enough to solve for the optimal policy, yet accurate enough that this policy is effective. In 1971, Aho et al. <ref type="bibr" target="#b1">[2]</ref> studied page replacement within the independent reference model (IRM), which assumes that pages are accessed non-uniformly with known probabilities. They model cache replacement as a Markov decision process, and show that the optimal policy is to evict the page with the lowest reference probability, i.e., LFU. Though the IRM ignores temporal locality, and recent work recognizes this shortcoming, it nevertheless remains the de facto model <ref type="bibr" target="#b36">[37]</ref>.</p><p>However, the IRM is an especially poor reference model for processor caches. Unlike web caches, which are accessed by thousands of independent users, processor caches are accessed by relatively few threads. As a result, their references tend to be tightly correlated and exhibit complex, dynamic behaviors. It is this time-varying complexity, which must be captured to achieve good performance, that the IRM discards.</p><p>For example, consider a program that scans repeatedly over a 100 K-line array. Since each address is accessed once every 100 K accesses, each has the same "reference probability". Thus the IRM-optimal policy, which evicts the candidate with the lowest reference probability, cannot distinguish among lines and would replace them at random. In fact, the optimal replacement policy is to protect a fraction of the array so that some lines age long enough to hit. Doing so can significantly outperform random replacement, e.g., achieving 80% hit rate with a 80 K-line cache. But protection works only because of dynamic behavior: reference probabilities are not static, but change in correlated and predictable patterns. The independent reference model does not capture this common behavior.</p><p>More recently, a large body of work <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b32">33]</ref> has studied cache behavior under more complex memory reference models. (Garetto et al. <ref type="bibr" target="#b14">[15,</ref><ref type="bibr">?VI]</ref> summarize recent work that models caches in distributed systems.) However, since these reference models are difficult to analyze, prior work focuses on comparing policies (e.g., LRU vs. k-LRU) or tuning their parameters, not on finding policies that truly maximize upon available information.</p><p>Since the behavior of real programs is not captured in analytically tractable models, empirical work has tended to ignore theory and focus on best-effort heuristics instead. Replacement in practice: Replacement in practice: Replacement in practice: Replacement in practice: Many high-performance policies try to emulate MIN through various heuristics. DIP <ref type="bibr" target="#b29">[30]</ref> avoids thrashing by inserting most lines at low priority, and detects when it is advantageous to do so. SDBP <ref type="bibr" target="#b20">[21]</ref> and PRP <ref type="bibr" target="#b10">[11]</ref> predict which lines are unlikely to be reused. RRIP <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b38">39]</ref> and IbRDP <ref type="bibr" target="#b26">[27]</ref> try to predict candidates' times until reference. PDP <ref type="bibr" target="#b13">[14]</ref> protects lines from eviction for a fixed number of accesses. IRGD <ref type="bibr" target="#b34">[35]</ref> computes a statistical cost function from sampled reuse distance histograms. And Hawkeye <ref type="bibr" target="#b15">[16]</ref> emulates MIN's past decisions. Without a theoretical foundation, it is unclear if any of these policies takes the right approach. Indeed, no policy dominates across benchmarks (Sec. VII), suggesting that they are not making the best use of available information.</p><p>We observe two relevant trends in recent research. First, most empirical policies exploit dynamic behavior to select a victim, most commonly by using the recency and frequency heuristics <ref type="bibr" target="#b27">[28]</ref>. Most policies employ some form of recency, favoring candidates that were referenced recently: e.g., LRU uses recency alone, and RRIP <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b38">39]</ref> predicts a longer time until reference for older candidates. Similarly, a few policies that do not assume recency still base their policy on when a candidate was last referenced: PDP <ref type="bibr" target="#b13">[14]</ref> protects candidates until a certain age; and IRGD <ref type="bibr" target="#b34">[35]</ref> uses a heuristic function of ages. Another common way policies expoit dynamic behavior is through frequency, favoring candidates that were previously reused: e.g., LFU uses frequency alone, and "scan-resistant" policies like ARC <ref type="bibr" target="#b25">[26]</ref> and SRRIP <ref type="bibr" target="#b16">[17]</ref> favor candidates that have been reused at least once.</p><p>Second, recent high-performance policies adapt themselves to the access stream to varying degrees. DIP <ref type="bibr" target="#b29">[30]</ref> detects thrashing with set dueling, and thereafter inserts most lines at LRU to prevent thrashing. DRRIP <ref type="bibr" target="#b16">[17]</ref> inserts lines at medium priority, promoting them only upon reuse, and avoids thrashing using the same mechanism as DIP. SHiP <ref type="bibr" target="#b38">[39]</ref> extends DRRIP by adapting the insertion priority based on the memory address, PC, or instruction sequence. Likewise, SDBP <ref type="bibr" target="#b20">[21]</ref>, PRP <ref type="bibr" target="#b10">[11]</ref>, and Hawkeye <ref type="bibr" target="#b15">[16]</ref> learn the behavior of different PCs. And PDP <ref type="bibr" target="#b13">[14]</ref> and IRGD <ref type="bibr" target="#b34">[35]</ref> use auxiliary monitors to profile the access pattern and periodically recompute their policy.</p><p>These two trends show that (i) candidates reveal important information over time, and (ii) policies should learn from this information by adapting themselves to the access pattern. But these policies do not make the best use of the information they capture, and prior theory does not suggest the right policy.</p><p>We use planning theory to design a practical policy that addresses these issues. EVA is intuitive and inexpensive to implement. In contrast to most empirical policies, EVA does not explicitly encode particular heuristics (e.g., recency or frequency). Rather, it is a general approach that aims to make the best use of limited information, so that prior heuristics arise naturally when appropriate (Sec. IV).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. REPLACEMENT UNDER UNCERTAINTY</head><p>Given the many approaches taken in prior work, we start from first principles. All replacement policies have the same goal and face the same constraints: they try to maximize the cache's hit rate with limited cache space. We can develop the intuition behind EVA by precisely characterizing these tradeoffs.</p><p>We are interested in where hits come from and how cache space is used over time. To analyze this, we break up cache space over time into lifetimes, the idle periods between hits and evictions. In Fig. <ref type="figure" target="#fig_6">1</ref>, a line comes into the cache at access t, hits at access t+4, and is evicted at t+10. This gives two lifetimes, as shown. We further define a line's age as the number of accesses since it was last referenced. So in Fig. <ref type="figure" target="#fig_6">1</ref>, the line's first lifetime ends in a hit at age 4 (there are 4 accesses from t to t + 4), and the second lifetime ends in an eviction at age 6 (there are 6 accesses from t + 4 to t + 10). Note how age resets to zero upon each reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inserted</head><p>Hit Evicted ? @ t ? @ t + 4 ? @ t + 10 1st Lifetime 2nd Lifetime . . . Age:</p><formula xml:id="formula_0">1 2 3 4|0 1 2 3 4 5 6|0</formula><p>Fig. <ref type="figure" target="#fig_6">1</ref>: Lifetimes and ages for a single cache line over time (increasing left-to-right). Time is measured in accesses.</p><p>Lifetimes and ages let us precisely describe the tradeoffs in cache replacement. We introduce two random variables, H and L, which give the probability of hits and lifetimes of different ages. That is, P[H = 5] is the probability that a line will hit at age 5, and P[L = 5] is the probability that its lifetime will end at age 5 (i.e., in either a hit or eviction at age 5). Probability is a natural way to reason about the inherent uncertainty facing all practical replacement policies.</p><p>Policies try to maximize the cache's hit rate, which necessarily equals the average line's hit probability:</p><formula xml:id="formula_1">Cache hit rate = P[hit] = ? a=1 P[H = a] ,<label>(1)</label></formula><p>but are constrained by limited cache space. Specifically, the average lifetime equals the cache size, N :</p><formula xml:id="formula_2">N = E[L] = ? a=1 a ? P[L = a]<label>(2)</label></formula><p>Eq. 2 is essentially Little's Law with an arrival rate of one, since time is measured in accesses (see <ref type="bibr">[5, ?B.3]</ref> for its derivation).</p><p>Comparing these two equations, we see that hits are equally beneficial irrespective of their age, yet the cost in space increases in proportion to age (the factor of a in Eq. 2). So to maximize the cache's hit rate, the replacement policy must attempt to both maximize hit probability and limit how long lines spend in the cache.</p><p>But how should policies balance these competing, incommensurable objectives? With perfect information, MIN is a simple policy that achieves both ends. Unfortunately, MIN does not easily generalize under uncertainty: obvious generalizations like evicting the candidate with the highest expected time until reference <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b34">35]</ref> or the lowest expected hit probability <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b20">21]</ref> are inadequate, since they only account for one side of the tradeoff. Example: Example: Example: Example: Inspired by MIN, several recent policies predict time until reference and evict the candidate with the longest one <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b34">35]</ref>. A simple counterexample shows why predictions of time until reference are inadequate.</p><p>Suppose the replacement policy has to choose between two candidates: A is referenced immediately with probability 9/10, and in 100 accesses with probability 1/10; and B is always referenced every two accesses. (See [5, ?B.2] for an example of how such situations could arise in practice.) In this case, the best choice is to evict B, betting that A will hit immediately, and then evict A if it does not. Doing so yields an expected hit rate of 9/10, since every access to A has a 9/10 probability of hitting. In contrast, evicting A yields an expected hit rate of 1/2, since accesses to B produce one hit every two accesses. Yet A's expected time until reference is 1 ? 9/10 + 100 ? 1/10 = 10.9, and B's is 2. Thus, according to their predicted time until reference, A should be evicted. This is wrong.</p><p>Predictions fail because they ignore the possibility of future evictions. When behavior is uncertain and changes over time, the replacement policy can learn more about candidates as they age. This means it can afford to gamble that candidates will hit quickly and evict them if they do not, say, by keeping A for one access to see if it hits. But simple generalizations of MIN ignore this insight, e.g., expected time until reference is unduly influenced by large reuse distances that will never be reached in the cache. In this example, it is skewed by reuse distance 100, but the optimal policy never keeps lines this long, so it is wrong for it to influence replacement decisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EVA REPLACEMENT POLICY</head><p>Since it is inadequate to consider either hit probability or time until reference alone, we must find some way to reconcile them in a single metric. In general, the optimal metric should satisfy three properties: (i) it considers only future behavior, since the past is a sunk cost; (ii) it prefers candidates that are more likely to hit; and (iii) it penalizes candidates that take longer to hit. These properties both maximize the hit probability and minimize time spent in the cache, as desired.</p><p>We achieve these properties by viewing time spent in the cache as forgone hits, i.e., as the opportunity cost of retaining lines. We thus rank candidates by their economic value added (EVA), or how many hits the candidate yields over the "average candidate". EVA is essentially a cost-benefit analysis about whether a candidate's odds of hitting are worth the cache space it will consume.</p><p>This section describes EVA and gives a motivating example. Sec. V discusses why EVA maximizes the hit rate. Intuition: Intuition: Intuition: Intuition: EVA views each replacement candidate as an investment, trying to retain the candidates that yield the highest profit (measured in hits). First, EVA rewards each candidate for its expected future hits. Then, since cache space is a scarce resource, EVA needs to account for how much space each candidate will consume. EVA does so by "charging" each candidate for the time it will spend in the cache. Specifically, EVA charges candidates at a rate of a single line's average hit rate (i.e., the cache's hit rate divided by its size), since this is the long-run opportunity cost of consuming cache space. Altogether, the EVA for a candidate is: EVA = Expected hits -Cache hit rate Cache size ? Expected time</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Computing EVA</head><p>We compute EVA using conditional probability and the random variables H and L introduced above. (Sec. VI discusses how to efficiently sample these distributions.) To begin, we compute EVA from candidates' ages, using the time since last reference to inform EVA's decisions. Inferring EVA from candidates' ages roughly corresponds to the recency heuristic (Sec. II), except that EVA does not assume recently used candidates are more valuable (see example below).</p><p>We compute EVA from the candidate's expected remaining lifetime and its hit probability in that lifetime. Let r(a) be the expected hit probability (reward, or benefit) and c(a) the forgone hits (cost). Then from the equation above, EVA at age a is just their difference, EVA(a) = r(a)c(a).</p><p>The reward r(a) is the expected number of hits for a line of age a, i.e., its hit probability. This is basic conditional probability, where age a restricts the sample space to lifetimes at least a accesses long:</p><formula xml:id="formula_3">r(a) = P hit|age a = P[H &gt; a] P[L &gt; a]<label>(3)</label></formula><p>Likewise, the forgone hits c(a) is the expected number of hits that could be obtained from replacing the candidate, which increases in proportion to the candidate's remaining lifetime. Each access would yield, on average, hits equal the cache's hit rate h divided by its size N :</p><formula xml:id="formula_4">c(a) = h N ? E L -a|age a = h N ? ? x=1 x ? P[L = a + x] P[L &gt; a]<label>(4)</label></formula><p>To summarize, we select a victim by comparing each candidate's EVA and evict the candidate with the lowest EVA. Our implementation does not compute EVA during replacement. Instead, it infrequently ranks ages by computing their EVA. Moreover, Eqs. 3 and 4 at age a each require just a few arithmetic operations to compute from age a + 1, so computing EVA is inexpensive. Sec. VI gives the full details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Example</head><p>To see how EVA works, consider an example application that scans alternating over two arrays of different sizes, called the 'small' and 'big' arrays. The cache size is such that the small array fits in the cache, but the big array does not. Specifically, the arrays take 16 K and 128 K lines, and the cache has 64 K lines. Fig. <ref type="figure">2</ref> shows the resulting distribution of reuse distance, i.e., the number of references between accesses to the same address. (Reuse distances are twice the size of arrays because each receives half of accesses.) Without further information about candidates, this distribution is all the replacement policy has to choose among candidates. What is the right policy? Replacement policy: Replacement policy: Replacement policy: Replacement policy: In this example, we want to cache the small array and some part of the big array. Fig. <ref type="figure">3</ref> shows how EVA ranks candidates to achieve this. Fig. <ref type="figure">3</ref> shows a candidate's EVA (y-axis) vs. its age (x-axis). Initially, at age zero, candidates have decent EVA because half the time they are part of the small array and will hit quickly. To be precise, at age zero a candidate's EVA is zero, since we have not yet learned anything about the candidate. Zero is not a low EVA value; it indicates average behavior.</p><p>A candidate's EVA increases with age as it gets closer to (possibly) hitting in the small array. This is because the hit probability does not change, but its expected remaining lifetime shrinks in proportion to age. The result is a line increasing from age zero up to the reuse distance of the small array ("Cache small array" in Fig. <ref type="figure">3</ref>). The replacement policy will thus evict MRU among ages in this range, since younger lines have lower EVA. This is the optimal policy for a scanning pattern.</p><p>However, if a candidate's age increases beyond the small array, then its EVA plummets because the policy has learned it must be an access to the big array ("Evict after small array" in Fig. <ref type="figure">3</ref>). These candidates have many more accesses to go before hitting (i.e., a long expected remaining lifetime), so their EVA is negative-they take so long to hit that they are not worth the investment. The replacement policy will thus preferentially evict candidates with ages in this range, i.e., accesses to the big array that still have a long time to hit.</p><p>Finally, EVA gradually increases with age as a candidate gets closer to hitting in the big array, until eventually old candidates have the highest EVA among all candidates ("Cache big array eventually" in Fig. <ref type="figure">3</ref>). Their EVA is larger than young candidates because their hit probability is larger, since some young candidates are accesses to the big array that are later evicted. The replacement policy will thus protect older candidates from eviction, fitting as much of the big array as possible. Performance: Performance: Performance: Performance: Fig. <ref type="figure">4</ref> shows the performance of a cache using this policy, namely its distribution of hits and evictions at different ages. All accesses to the small array hit, but only a fraction of those to the big array hit. Evictions are concentrated after the small array, since these ages have the lowest EVA in Fig. <ref type="figure">3</ref>. This is as desired: the cache cannot fit both arrays, and the policy must wait until after the small array to learn whether an access is to the big or small array. Fig. <ref type="figure">4</ref>: The cache holds the entire small array and part of the big array (hit rate: 64%). Uncertainty wastes some cache space, since evictions must occur after the small array.</p><p>Uncertainty comes at a cost, since these evictions waste space. Ideally, a cache with 64 K lines could hold the 16 K lines of the small array plus 48 K lines of the big array, giving a hit rate of 1/2 ? 16 K/16 K + 1/2 ? 48 K/128 K = 69%. But with uncertainty, some space must be spent learning whether accesses are to the small or big array. This wastes space proportional to the size of the small array, so given the information in Fig. <ref type="figure">2</ref>, Eq. 2 implies that the maximum achievable hit rate on this example is 64%. EVA achieves this performance, maximizing upon the available information.</p><p>In contrast, no "protecting distance" in PDP <ref type="bibr" target="#b13">[14]</ref> can do so. Protecting the small array gives a hit rate of 50%, and protecting the big array wastes so much cache space that it actually lowers the hit rate to 44%. Summary: Summary: Summary: Summary: This example shows how EVA changes over time, adapting its policy to the access pattern. Our contribution is identifying hit probability and expected lifetime as the key tradeoffs in cache replacement, and reconciling them in a single metric.</p><p>For simplicity, we have so far assumed that information is limited to that revealed by aging. We now discuss how EVA uses more information to make better decisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. EVA with classification</head><p>The main challenge that replacement policies face is uncertainty about candidates' future behavior. One common technique to reduce uncertainty is to break candidates into multiple classes and tune the policy to each class. Classification is widely used in prior work, e.g., policies classify by reuse, thread, PC, etc. (Sec. II).</p><p>Since EVA is based on conditional probability, EVA naturally supports classification by conditioning on a candidate's class. (Indeed, EVA thus far simply classifies candidates by age.) This formal probabilistic approach lets EVA specialize its policy to each class without any a priori preferences among classes. Example: Example: Example: Example: We can see how EVA incorporates classification by revisiting the above example. In this application, we should differentiate accesses to the big and small arrays. This lets us rank the candidates of each class separately, as illustrated in Fig. <ref type="figure" target="#fig_2">5</ref>. Classification changes the ranks at small ages, as the replacement policy no longer needs to wait to learn about candidates. This has two implications: (i) accesses to the small array become more valuable because they are certain to hit quickly, and (ii) accesses to the big array have low EVA at age zero, since we now know immediately that they will take many accesses to hit. These differences let the replacement policy evict candidates more quickly, reducing waste from evictions and thereby caching more of the big array. In this example classification gives perfect information, but generally this is not the case. Classification improves cache performance, as shown in Fig. <ref type="figure">6</ref>. There are now more hits to the big array, and the hit rate improves to 69%, which is optimal for this cache size and access pattern. This performance improvement is possible because evictions now occur very early, so minimum cache space is wasted on them. Fig. <ref type="figure">6</ref>: With classification, the cache holds the entire small array and more of the big array (hit rate: 69%). Evictions occur immediately, freeing space for more hits.</p><p>To illustrate classification in EVA, we now show how EVA incorporates the frequency heuristic by distinguishing candidates that have been reused at least once, similar to prior scan-resistant policies (Sec. II). Classification by reuse: Classification by reuse: Classification by reuse: Classification by reuse: We divide the cache into two classes: lines that have hit at least once, and newly inserted lines that have not. We use this simple scheme because it has proven effective in prior work <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26]</ref>. (In the above example, the "reused class" is the small array and the fraction of the big array that fits.) However, unlike prior work, EVA does not assume an a priori preference for reused candidates, and EVA supports arbitrary classification schemes (e.g., our technical report <ref type="bibr" target="#b5">[6]</ref> classifies candidates by size in compressed caches).</p><p>Until now, we have considered only the current lifetime when computing expected future hits, as we lacked further information. When distinguishing among classes, this is insufficient, and we must consider all future lifetimes since classes often behave differently over the long run.</p><p>The details depend on the classification scheme. When classifying by reuse, if a lifetime ends in a hit, then the next will be a reused lifetime (by definition). Otherwise, if it ends in an eviction, then the next will be a non-reused lifetime (again by definition). Fig. <ref type="figure" target="#fig_4">7</ref> illustrates classification by reuse; we want to compute EVA over all future hits and misses, starting from a line of age a and class C on the left. Details: Details: Details: Details: We denote the EVA of reused lines as EVA R (a), and the EVA of non-reused lines as EVA NR (a). We further condition EVA's terms with a line's class. For example, the reward for a reused line is:</p><formula xml:id="formula_5">r R (a) = P H &gt; a|reused P L &gt; a|reused<label>(5)</label></formula><p>Compared to Eq. 3, the only difference is the added condition of "reused". Forgone hits (c R (a)) and non-reused lines (r NR (a), c NR (a)) are similarly conditioned. We refer to class C whenever either R or NR apply. EVA for a single lifetime is unchanged: EVA C lifetime (a) = r C (a)c C (a), but now future lifetimes cannot be ignored.</p><p>We can express long-run EVA for any age as a function of the long-run EVA of reused and non-reused lines at age zero, following Fig. <ref type="figure" target="#fig_4">7</ref>. If the hit rate of class C at age a is h C (a), then the corresponding, long-run EVA is:</p><formula xml:id="formula_6">EVA C (a) = EVA C lifetime (a) (Current lifetime) +h C (a) ? EVA R (0) (Hits ? Reused) +(1 -h C (a)) ? EVA NR (0) (Misses ? Non-reused)</formula><p>Finally, the average access's EVA-i.e, the "average difference from the average"-is zero by definition, so:</p><formula xml:id="formula_7">0 = h ? EVA R (0) + (1 -h) ? EVA NR (0)</formula><p>Solving these equations reveals a simple relationship between each class's EVA and the EVA of reused lines:</p><formula xml:id="formula_8">EVA C (a) = EVA C lifetime (a) + h C (a)-h 1-h R (0) ? EVA R lifetime (0)<label>(6)</label></formula><p>The second term essentially says that the more hits class C produces vs. the average, the more it will behave like a reused line in the future. Summary: Summary: Summary: Summary: Classification is a common way to reduce uncertainty. EVA naturally supports classification through conditional probability, automatically specializing its policy to each class. Though conceptually simple, we must account for long-term differences between classes to accurately compute EVA. We focus on classification by reuse for concreteness, but these ideas apply to other classification schemes as well. One must simply express how lines transition between classes and then solve for EVA. For example, our technical report <ref type="bibr" target="#b5">[6]</ref> classifies candidates in a compressed cache by their size.</p><p>EVA takes a different approach to classification than most recent work. Several recent policies break accesses into many classes, often using the requesting PC, and then adapt their policy to each class <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b38">39]</ref>. However, with thousands of classes, these policies are restricted to simple decisions for each class (e.g., a single bit-is the class is valuable or not?), and they rely on a baseline policy (e.g., random, LRU, or RRIP) once candidates are inserted. In contrast, EVA ranks ages at fine granularity, but this restricts EVA to use fewer classes (e.g., just two with classification by reuse). An important area of future work is to explore the best balance of these approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. WHY EVA IS THE RIGHT METRIC</head><p>The previous section described and motivated EVA. This section discusses why EVA is the right metric to maximize cache performance under uncertainty. We first present a na?ve metric that intuitively maximizes the hit rate, but show that it unfortunately cannot capture long-run behavior. Fortunately, prior work in Markov decision processes (MDPs) has encountered and solved this problem, and we show how EVA adapts the MDP solution to cache replacement. Na?ve metric: Na?ve metric: Na?ve metric: Na?ve metric: We want a replacement metric that maximizes the cache's hit rate. With perfect knowledge, MIN achieves this by greedily "buying hits as cheaply as possible," i.e. by keeping the candidates that are referenced in the fewest accesses. Stated another way, MIN retains the candidates that get the most hitsper-access. Therefore, a simple metric that might generalize MIN is to predict each candidate's hits-per-access, or hit rate:</p><formula xml:id="formula_9">E[hit rate] = lim T ??</formula><p>Expected hits after T accesses T</p><p>and retain the candidates with the highest hit rate. Intuitively, keeping these candidates over many replacements tends to maximize the cache's hit rate.</p><p>The problem:</p><p>The problem:</p><p>The problem:</p><p>The problem: Eq. 7 suffices when considering a single lifetime, but it cannot account for candidates' long-run behavior over many lifetimes: To estimate the future hit rate, we must compute expected hits over arbitrarily many future accesses. However, as cache lines are replaced many times, they tend to converge to average behavior because their replacements are generally unrelated to their original contents. Hence, all candidates' hit rates converge in the limit. In fact, all candidates' hit rates are identical: solving Eq. 7 as T ? ? yields h/N , the cache's per-line hit rate, for all ages and classes.</p><p>In other words, Eq. 7 loses its discriminating power over long time horizons, degenerating to random replacement. So while estimating candidates' hit rates is intuitive, this approach is fundamentally flawed as a replacement metric. The solution in a nutshell: The solution in a nutshell: The solution in a nutshell: The solution in a nutshell: EVA sidesteps this problem by changing the question. Instead of asking which candidate gets a higher hit rate?, EVA asks which candidate gets more hits?</p><p>The idea is that Eq. 7 compares the long-run hit rates of retaining different candidates:</p><formula xml:id="formula_11">lim T ?? hits 1 (T ) T ? &gt; lim T ?? hits 2 (T ) T ,<label>(8)</label></formula><p>but unfortunately both sides of the comparison converge to the same quantity. We can avoid this problem by simply multiplying by T and subtracting both sides:</p><formula xml:id="formula_12">lim T ?? hits 1 (T ) -hits 2 (T ) ? &gt; 0 (9)</formula><p>This equation is equivalent to the original, but measures small differences in long-run behavior that would otherwise disappear in the limit. These metrics are are useful in different contexts. When rates are unequal in the limit, Eq. 8 is well-behaved, but Eq. 9 diverges to ?. Eq. 8 is thus the appropriate metric. However, when rates are the same in the limit, Eq. 8 cannot discriminate, but Eq. 9 can. Hence, Eq. 9 is the appropriate metric. Since hit rates are equal in the limit, it makes sense to follow Eq. 9 and rank candidates by which gets more hits in the limit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The solution in detail:</head><p>The solution in detail:</p><p>The solution in detail: The solution in detail: We are not the first to encounter this optimization problem or devise the above solution, which has been previously proposed in the context of Markov decision processes (MDPs). MDPs model decision-making under uncertainty by extending Markov chains to have actions in each state. Each action yields an associated reward. 1  A large body of prior work has studied policies to achieve different objectives and proved conditions of optimality. (For a comprehensive treatment, see Puterman <ref type="bibr" target="#b28">[29]</ref>.) In our case, the reward is cache hits, the actions are accesses (some of which invoke the replacement policy), and we care about maximizing the average reward (i.e., hit rate). When studying the optimal policy for such MDPs, researchers ran into exactly the same problem as above: all actions yield the same average reward in the limit. To solve this problem, they introduce a new quantity called the bias, which is similar to Eq. 9 above.</p><p>The bias is defined as the expected difference in total reward from the current state vs. the average. In other words, after T actions from the current state, one would expect some total reward. Meanwhile, averaging across states, the MDP would have yielded a reward equal to its average reward times T . The bias is just the difference between these two quantities in the limit:</p><formula xml:id="formula_13">Bias = lim T ?? Reward after T actions -Avg. reward ? T</formula><p>This bias is thus similar to Eq. 9, but it compares actions against the average rather than against each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Actions ?</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reward</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? Bias</head><p>Reward after T Avg. reward ? T Fig. <ref type="figure">8</ref>: Bias is the long-run difference between the total reward and the average reward.</p><p>Fig. <ref type="figure">8</ref> illustrates the bias. The y-axis shows total reward, and the x-axis shows time (measured in actions). The solid line shows the expected total reward starting from the current state; the dashed line shows the expected total reward averaged over all states. The expected reward fluctuates initially, as returns from the current state differ from the average, but eventually settles to yield the average reward 1 For interested readers, our technical report <ref type="bibr" target="#b5">[6]</ref> presents a detailed cache replacement MDP using a reference model from our recent work <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. in the limit. The distance between these two lines in the limit is the bias, generally a finite but non-zero number.</p><p>Maximizing the bias provably maximizes the average reward <ref type="bibr">[29, ?8.4</ref>]. Thus, to maximize the cache's hit rate, we should try to maximize the bias among replacement candidates. From theory to practice: From theory to practice: From theory to practice: From theory to practice: EVA does exactly this. Bias and EVA are directly analogous:</p><formula xml:id="formula_14">Bias =lim T ?? Reward after T -Avg. reward ? T EVA =</formula><p>Expected hits -Hit rate Cache size ? Time Sec. IV-A gives the bias over a single lifetime: the reward is the hit probability, the average reward is a single line's hit rate, and the time horizon T is the remaining lifetime. Sec. IV-C gives the bias over all future lifetimes (i.e., as T ? ?), as illustrated in Fig. <ref type="figure" target="#fig_4">7</ref>. MDP theory tells us how to combine these terms into a metric that maximizes the cache's hit rate.</p><p>Two critical points deserve emphasis: Optimality: Optimality: Optimality: Optimality: First, maximizing the bias provably maximizes the average reward. It is not a heuristic. MDP theory provides metrics that allow greedy, local decisions to provably optimize global outcomes <ref type="bibr" target="#b28">[29]</ref>. Maximizing the bias is equivalent to maximizing the hit rate, except that it is well-behaved in the limit and so lets us model long-run behavior. Generality: Generality: Generality: Generality: Second, because maximizing the bias reconciles hit probability and cache space, which we have argued are the fundamental constraints (Sec. III), it gives broad optimality conditions. For example, both perfect information (MIN) and the IRM <ref type="bibr" target="#b1">[2]</ref> are special cases.</p><p>EVA is easy to extend to many different contexts. For instance, with small modifications EVA can model a compressed cache, where candidates have different sizes and policies based on time until reference-even MIN!-are clearly suboptimal <ref type="bibr" target="#b5">[6]</ref>.</p><p>Convergence: Convergence: Convergence: Convergence: MDPs are solved using iterative algorithms that are guaranteed to converge to an optimal policy within arbitrary precision. In particular, policy iteration <ref type="bibr" target="#b22">[23]</ref> alternates between computing the expected total reward for each state and updating the policy. Our implementation takes an analogous approach, alternatively monitoring the cache ("computing rewards") and updating ranks.</p><p>EVA can converge to changes in behavior because it is neutral towards candidates it knows nothing about-their EVA is zero. Since zero represents average behavior, other, below-average candidates will be evicted preferentially. EVA thus naturally explores the behavior of these "unknown candidates", and a few tend to age long enough to discover any changes in their behavior. If even a few of these candidates hit, conditional probability gives them large EVA because so few candidates survive to these ages. This creates a virtuous circle, as favoring these candidates leads to further hits.</p><p>Nevertheless, with classification, it is possible to construct cases where EVA gets stuck evicting one class. This pathology could be fixed by rarely tagging some candidates as "explorers" that are not evicted until they reach the maximum age. However, we find that EVA performs well in practice and do not consider this mechanism further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. IMPLEMENTATION</head><p>Our implementation is shown in Fig. <ref type="figure" target="#fig_5">9</ref>: (i) A small table, called the eviction priority array, ranks candidates to select a victim. (ii) Counters record the age distribution of hits of evictions. And, infrequently, (iii) a lightweight software runtime computes EVA from these counters and updates the eviction priorities. This implementation requires trivial hardware: narrow comparisons and increments plus a small amount of state. All of the analytical complexity is pushed to software, where updates add small overheads. This hybrid design is broadly similar to prior work in cache partitioning <ref type="bibr" target="#b6">[7]</ref>; however, we also describe a hardware-only implementation in Sec. VI-D.</p><p>Unlike prior policies, EVA does not devote a fraction of sets to monitoring alternative policies (cf. <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b29">30]</ref>), nor does it require auxiliary tags to monitor properties independent of the replacement policy (cf. <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b34">35]</ref>). As a result, our implementation makes full use of the entire cache and eliminates overheads from monitors. EVA also does not need to forward the requesting PC to the LLC, which, though widely used in prior work, complicates the core-cache interface and adds network bandwidth.</p><p>A. Hardware operations Aging: Aging: Aging: Aging: We use per-set, coarsened ages <ref type="bibr" target="#b13">[14]</ref>. Each cache line has a k-bit age, and each set has a j-bit counter that is incremented upon an access to the set (k = 7 and j = 4 in our evaluation). When a set's counter reaches a value A, it is reset and every age in the set is incremented until saturating at 2 k -1. Ranking: Ranking: Ranking: Ranking: To rank candidates cheaply, we use a small eviction priority array. We use each candidate's age to index into the priority array, and evict the candidate with the highest eviction priority. We set priorities such that if age a 1 is ranked higher than age a 2 , then EVA(a 1 ) &lt; EVA(a 2 ) (as described below). To ensure that lines are eventually evicted, saturated ages always have the highest eviction priority.</p><p>To work with classification, we add a reused bit to each cache tag, and use two priority arrays to store the priorities of reused and non-reused lines. Eviction priorities require 2 k+1 ? (k + 1) bits, or 256 B with k = 7.</p><p>The eviction priority array is dual ported to support peak memory bandwidth. With 16 ways, we can sustain one eviction every 8 cycles, for 19.2 GBps per LLC bank. Inputs: hitCtrs, evictionCtrs -event counters, A -age granularity, N -cache size Returns: rank -eviction priorities for all ages and classes 1: for a ? 2 k to 1 : Compute hit rates from counters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2:</head><p>for c ? {NR, R} :</p><formula xml:id="formula_15">3: hitsc += hitCtrs[c, a] 4: eventsc += hitCtrs[c, a] + evictionCtrs[c, a] 5: h R [a] ? hitsR/eventsR 6:</formula><p>h NR [a] ? hitsNR/eventsNR for a ? 2 k to 1 :</p><formula xml:id="formula_16">12:</formula><p>expectedLifetime += events 13:</p><p>eva[c, a] ? (hits -perAccessCost ? expectedLifetime)/events 14:</p><p>hits += hitCtrs[c, a]</p><p>15:</p><formula xml:id="formula_17">events += hitCtrs[c, a] + evictionCtrs[c, a] 16: evaReused ? eva[R, 1]/(1 -h R [0])</formula><p>Differentiate classes. 17: for c ? {NR, R} :</p><formula xml:id="formula_18">18:</formula><p>for a ? 2 k to 1 :</p><formula xml:id="formula_19">19: eva[c, a] += (h C [a] -h) ? evaReused 20: order ? ARGSORT(eva)</formula><p>Finally, rank ages by EVA. 21: for i ? 1 to 2 k+1 :</p><formula xml:id="formula_20">22: rank[order[i]] ? 2 k+1 -i 23: return rank</formula><p>Event counters: Event counters: Event counters: Event counters: To sample the hit and lifetime distributions, we add two arrays of 16-bit counters (2 k ? 16 b = 256 B per array) that record the age histograms of hits and evictions. When a line hits or is evicted, the cache controller increments the counter in the corresponding array. These counters are periodically read to update the eviction priorities. To support classification, there are two arrays for both reused and non-reused lines, or 1 KB total with k = 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Software updates</head><p>The eviction priority array is a versatile mechanism that can implement many policies, specifically any ranking function of ages <ref type="bibr" target="#b8">[9]</ref>, such as random, LRU, PDP <ref type="bibr" target="#b13">[14]</ref>, IRGD <ref type="bibr" target="#b34">[35]</ref>, etc. We set priorities to implement EVA.</p><p>Periodically (every 256 K accesses), an OS runtime computes EVA for active applications. First, we read the hit and lifetime distributions (H and L) from the counters, and average them with prior values to maintain a history of application behavior. (We use an exponential moving average with coefficient 0.8.)</p><p>We compute EVA in a small number of arithmetic operations per age, and sort the result to find the eviction priority for each age and class. Algorithm 1 gives an efficient procedure to compute Eq. 6. Updates occur in four passes over ages. In the first pass, we compute hit rates h R , h NR , and h by summing counters. Second, we compute per-lifetime EVA incrementally in five additions, one multiplication, and one division per age. Third, classification adds one more addition and multiplication per age. Finally, we sort EVA to find the final eviction priorities. Our C++ implementation takes just 123 lines of code (excluding debugging and comments), incurs negligible runtime overheads (see below), and can be found online at http://people.csail.mit.edu/sanchez.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Overheads</head><p>Ranking and counters: Ranking and counters: Ranking and counters: Ranking and counters: Our implementation adds 1 KB for counters and 256 B for priority arrays. We have synthesized our design in a commercial process at 65 nm at 2 GHz. We lack access to an SRAM compiler, so we use CACTI 5.3 <ref type="bibr" target="#b35">[36]</ref>   for all SRAMs (using register files instead of SRAM makes the circuit 4? larger). Table <ref type="table" target="#tab_2">I</ref> shows the area and energy for each component at 65 nm; absolute numbers should be scaled to reflect more recent technology nodes. We compute overhead relative to a 1 MB LLC using area from a 65 nm Intel E6750 <ref type="bibr" target="#b12">[13]</ref> and energy from CACTI. Overheads are small, totaling 0.2% area and 1.0% energy. Total leakage power is 2 mW. Even with one LLC access every 10 cycles, EVA adds just 7 mW at 65 nm, or 0.01% of the E6750's 65 W TDP <ref type="bibr" target="#b12">[13]</ref>. Software updates: Software updates: Software updates: Software updates: Updates complete in a few tens of K cycles and run every 256 K LLC accesses (i.e., several M cycles). Specifically, with k = 7 age bits, updates take 43 K cycles on an Intel Xeon E5-2670. Because updates are infrequent, the runtime and energy overhead is negligible: conservatively assuming that updates are on the critical path, we observe that updates take an average 0.1% of system cycles and a maximum of 0.3% on SPEC CPU2006 apps.</p><p>We consider these overheads negligible and choose k = 7, but EVA lets designers trade off overheads and performance. For example, choosing k = 5 reduces software overheads by three-quarters while sacrificing little performance (Fig. <ref type="figure" target="#fig_12">14</ref>). Tags: Tags: Tags: Tags: Since the new components introduced by EVA add negligible overheads, the main overhead is additional tag state. Our implementation uses 8 bits per tag (vs. 2 bits for SHiP). This is roughly 1% area overhead, 0.5% energy overhead, and 20 mW leakage power.</p><p>Our evaluation shows that EVA is competitive with prior policies when using fewer age bits. But we use larger tags because doing so produces a more area-efficient design. Unlike prior policies, EVA's performance steadily improves with more tag bits (Fig. <ref type="figure" target="#fig_12">14</ref>). EVA trades off larger tags for improved performance, making better use of the 99% of cache area not devoted to replacement, and thus saves area at iso-performance. Complexity: Complexity: Complexity: Complexity: A common concern with analytical techniques like EVA is their perceived complexity. However, we should be careful to distinguish between conceptual complexity and implementation complexity. In hardware, EVA adds only narrow increments and comparisons; in software, EVA adds a short, low-overhead reconfiguration procedure (Algorithm 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Alternative hardware-only implementation</head><p>Performing updates in software instead of hardware provides several benefits. Most importantly, software updates reduce implementation complexity, since EVA's implementation is otherwise trivial. Software updates may also be preferable to integrate EVA with other system objectives (e.g., cache partitioning <ref type="bibr" target="#b37">[38]</ref>), or on systems with dedicated OS cores (e.g., the Kalray MPPA-256 <ref type="bibr" target="#b11">[12]</ref> or Fujitsu Sparc64 XIfx <ref type="bibr" target="#b39">[40]</ref>).</p><p>However, if software updates are undesirable, we have also implemented and synthesized a custom microcontroller that performs updates in hardware. Our microcontroller computes Cores Westmere-like OOO [32]   EVA using a single adder and a small ROM microprogram. We use fixed-point arithmetic, requiring 2 k+1 ? 32 bits to store the results plus seven 32-bit registers, or 1052 B with k = 7. We have also implemented a small FSM to compute the eviction priorities, adding 2 ? 2 k+1 ? (k + 1) bits, or 512 B. This microcontroller adds small overheads (Table <ref type="table" target="#tab_2">I</ref>)-1.5 KB of state and simple logic-and is off the critical path. It was fully implemented in Verilog by a non-expert in one week, and its complexity is low compared to microcontrollers shipping in commercial processors (e.g., Intel Turbo Boost <ref type="bibr" target="#b30">[31]</ref>). So although we believe software updates are a simpler design, EVA can be implemented entirely in hardware if desired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. EVALUATION</head><p>We now evaluate EVA over diverse benchmark suites and configurations. We show that EVA performs consistently well across benchmarks, outperforms existing policies, closes the gap with MIN, and saves area at iso-performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Methodology</head><p>We use zsim <ref type="bibr" target="#b31">[32]</ref> to simulate systems with 1 and 8 OOO cores with parameters shown in Table <ref type="table" target="#tab_4">II</ref>. We simulate LLCs from 1 to 8 MB. The single-core chip runs single-threaded SPEC CPU2006 apps, while the 8-core chip runs multi-threaded apps from SPEC OMP2012. Our results hold across different LLC sizes, benchmarks (e.g., PBBS <ref type="bibr" target="#b33">[34]</ref>), and with a stream prefetcher validated against real Westmere systems <ref type="bibr" target="#b31">[32]</ref>. Policies: Policies: Policies: Policies: We evaluate how well policies use information by comparing against random and MIN; these policies represent the extremes of no information and perfect information, respectively. We further compare EVA with LRU, RRIP variants (DRRIP and SHiP), and PDP, which are implemented as proposed. We sweep configurations for each policy and select the one that is most area-efficient at iso-performance. DRRIP uses M = 2 bits per tag and = 1/32 <ref type="bibr" target="#b16">[17]</ref>. SHiP uses M = 2 bits and PC signatures with idealized, large history counter tables <ref type="bibr" target="#b38">[39]</ref>. DRRIP is only presented in text because it performs similarly to SHiP, but occasionally slightly worse. These policies' performance degrades with larger M (Fig. <ref type="figure" target="#fig_12">14</ref>). PDP uses an idealized implementation with large timestamps. Area: Area: Area: Area: Except where clearly noted, our evaluation compares policies' performance against their total cache area at 65 nm, including all replacement overheads. We evaluate performance and area because the power overheads from replacement are negligible-e.g., EVA's are 0.01% of TDP (Sec. VI-C). Area and performance are thus the main design constraints.</p><p>For each LLC size, we use CACTI to model data and tag area. We use 45 tag bits for address and coherence state. We add Area (mm<ref type="foot" target="#foot_0">2</ref> @ 65nm) Area (mm 2 @ 65nm) Area (mm 2 @ 65nm) Area (mm 2 @ 65nm) Area (mm 2 @ 65nm)  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Single-threaded results</head><p>Fig. <ref type="figure" target="#fig_6">10</ref> plots MPKI vs. cache area for ten representative, memory-intensive SPEC CPU2006 apps. Each point on each curve represents increasing LLC sizes from 1 to 8 MB. First note that the total cache area at the same LLC size (i.e., points along x-axis) is hard to distinguish across policies. This is because replacement overheads are small-less than 2% of total cache area.</p><p>In most cases, MIN outperforms all practical policies by a large margin. Excluding MIN, some apps are insensitive to replacement policy. On others, random replacement and LRU perform similarly; e.g., mcf and libquantum. In fact, random often outperforms LRU. EVA performs consistently well: EVA performs consistently well: EVA performs consistently well: EVA performs consistently well: SHiP and PDP improve performance by correcting LRU's flaws on particular access patterns. Both perform well on libquantum (a scanning benchmark), sphinx3, and xalancbmk. However, their performance varies considerably across apps. For example, SHiP performs particularly well on perlbench, mcf, and cactusADM. PDP performs particularly well on GemsFDTD and lbm, where SHiP exhibits pathologies and performs similar to random replacement.</p><p>EVA matches or outperforms SHiP and PDP on most apps and cache sizes. This is because EVA generally maximizes upon available information, so the right replacement strategies naturally emerge where appropriate. As a result, EVA successfully captures the benefits of SHiP and PDP within a common framework, and sometimes outperforms both. Since EVA performs consistently well, and SHiP and PDP do not, EVA achieves the lowest MPKI of all policies on average.</p><p>The cases where EVA performs slightly worse arise for two reasons. First, in some cases (e.g., mcf at 1 MB), the access pattern changes significantly between policy updates. EVA can take several updates to adapt to the new pattern, during which performance suffers. But in most cases the access pattern changes slowly, and EVA performs well. Second, our implementation coarsens ages, which can cause small performance variability for some apps (e.g., libquantum). EVA edges closer to optimal replacement: EVA edges closer to optimal replacement: EVA edges closer to optimal replacement: EVA edges closer to optimal replacement: Fig. <ref type="figure" target="#fig_9">11</ref> compares the practical policies against MIN, showing the average MPKI gap over MIN across the most memory-intensive SPEC CPU2006 apps 2 -i.e., each policy's MPKI minus MIN's at equal area. One would expect a practical policy to fall somewhere between random replacement (no information) and MIN (perfect information). But LRU actually performs worse than random at many sizes because private caches strip out most temporal locality before it reaches the LLC, leaving scanning patterns that are pathological in LRU. In contrast, both SHiP and PDP significantly outperform random replacement. Finally, EVA performs best. On average, EVA closes 57% of the random-MIN MPKI gap. In comparison, DRRIP (not shown) closes 41%, SHiP 47%, PDP 42%, and LRU -9%.  EVA saves cache space: EVA saves cache space: EVA saves cache space: EVA saves cache space: Because EVA improves performance, it needs less cache space than other policies to achieve a given level of performance. Fig. <ref type="figure" target="#fig_6">12</ref> shows the iso-MPKI total cache area of each policy, i.e., the area required to match random replacement's average MPKI for different LLC sizes (lower is better). For example, a 21.5 mm 2 EVA cache achieves the same MPKI as a 4 MB cache using random replacement, whereas SHiP needs 23.6 mm 2 to match this performance. EVA is the most area efficient over the full range. On average, EVA saves 8% total cache area over SHiP, the best practical alternative. However, note that MIN saves 35% over EVA, so there is still room for improvement, though some performance gap is unavoidable due to the costs of uncertainty (Sec. IV-B). EVA achieves the best end-to-end performance: EVA achieves the best end-to-end performance: EVA achieves the best end-to-end performance: EVA achieves the best end-to-end performance: Fig. <ref type="figure" target="#fig_6">13</ref> shows the IPC speedups over random replacement at 35 mm 2 , the area of a 4 MB LLC with random replacement. Only benchmarks that are sensitive to replacement are shown, i.e., benchmarks whose IPC changes by at least 1% under some policy.</p><p>EVA achieves consistently good speedups across apps, whereas prior policies do not. SHiP performs poorly on xalancbmk, sphinx3, and lbm, and PDP performs poorly on mcf and cactusADM. Consequently, EVA achieves the best speedup overall. Gmean speedups on sensitive apps (those shown) are for EVA 8.5%, DRRIP (not shown) 6.7%, SHiP 6.8%, PDP 4.5%, and LRU -2.3%. EVA makes good use of additional state: EVA makes good use of additional state: EVA makes good use of additional state: EVA makes good use of additional state: Fig. <ref type="figure" target="#fig_12">14</ref> sweeps the number of tag bits for different policies and plots their average MPKI at 4 MB. (This experiment is not iso-area.) The figure shows the best configuration on the right; EVA and PDP use idealized, large timestamps. Prior policies achieve peak performance with 2 or 3 bits, after which their performance flattens or even degrades.</p><p>Unlike prior policies, EVA's performance improves steadily Replacement Tag Bits with more state, and its peak performance exceeds prior policies by a good margin. With 2 bits, EVA performs better than PDP, similar to DRRIP, and slightly worse than SHiP. Comparing the best configurations, EVA's improvement over SHiP is 1.8? greater than SHiP's improvement over DRRIP. EVA with 8 b tags performs as well as an idealized implementation, yet still adds small overheads. These overheads more than pay for themselves, saving area at iso-performance (Fig. <ref type="figure" target="#fig_6">12</ref>). With very few age bits, no single choice of age granularity A works well for all applications. To make EVA perform well with few bits, software adapts the age granularity using a simple heuristic: if more than 10% of hits and evictions occur at the maximum age, then increase A by one; otherwise, if less than 10% of hits occur in the second half of ages, then decrease A by one. We find this heuristic rapidly converges to the right age granularity across all evaluated applications. Only Fig. <ref type="figure" target="#fig_12">14</ref> uses this heuristic, and it is disabled for all other results. Since EVA is most area-efficient with larger tags, the design we advocate (8 b tags) does not employ this heuristic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Multi-threaded results</head><p>Fig. <ref type="figure" target="#fig_15">15</ref> extends our evaluation to multi-threaded apps from SPEC OMP2012. Working set sizes vary considerably, so we consider LLCs from 1 to 32 MB, with area shown in log scale Area (mm 2 @ 65nm) Area (mm 2 @ 65nm) on the x-axis. All qualitative claims from single-threaded apps hold for multi-threaded apps. Many apps are streaming or access the LLC infrequently; we discuss four representative apps from the remainder. As in single-threaded apps, SHiP and PDP improve performance on different apps. SHiP outperforms PDP on some apps (e.g., nab), and both perform well on others (e.g., smithwa). Unlike in single-threaded apps, however, DRRIP and threadaware DRRIP (TADRRIP) outperform SHiP. This difference is largely due to a single benchmark: smithwa at 8 MB.</p><p>EVA performs well in nearly all cases and achieves the highest speedup. On the 7 OMP2012 apps that are sensitive to replacement (Fig. <ref type="figure" target="#fig_15">15</ref> plus md, botsspar, and kd), the gmean speedup over random for EVA is 4.5%, DRRIP (not shown) 2.7%, TA-DRRIP 2.9%, SHiP 2.3%, PDP 2.5%, and LRU 0.8%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>The key challenge faced by practical replacement policies is how to cope with uncertainty. Simple approaches like predicting time until reference are flawed. We have argued for replacement by economic value added (EVA), starting from first principles and drawing from prior planning theory. We further showed that EVA can be implemented with trivial hardware, and that it outperforms existing high-performance policies nearly uniformly on single-and multi-threaded benchmarks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :Fig. 3 :</head><label>23</label><figDesc>Fig. 2: Example reuse distance distribution of an application scanning over two arrays. The big array does not fit in cache.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Fig.5: Classification reduces uncertainty. Now, the replacement policy need not wait until after the small array to evict.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: With classification, we must consider behavior in future lifetimes to compute EVA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: An efficient implementation of EVA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Algorithm 1 .</head><label>1</label><figDesc>Algorithm to compute EVA and update ranks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 :</head><label>11</label><figDesc>Fig. 11: MPKI above MIN for each policy on SPEC CPU2006.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 :Fig. 13 :</head><label>1213</label><figDesc>Fig. 12: Iso-MPKI cache area: avg MPKI equal to random at different LLC sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 14 :</head><label>14</label><figDesc>Fig. 14: Avg MPKI for different policies at 4 MB vs. tag overheads (lower is better).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 15 :</head><label>15</label><figDesc>Fig. 15: MPKI vs. total cache area across sizes (1 MB, 2MB, 4 MB, 8 MB, 16 MB, and 32 MB) for different policies on selected SPEC OMP2012 apps. (Lower is better.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I :</head><label>I</label><figDesc>Implementation overheads at 65 nm.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>at 4.2 GHz; 1 (ST) or 8 (MT) L1 caches 32 KB, 8-way set-assoc, split D/I, 1-cycle L2 caches Private, 256 KB, 8-way set-assoc, inclusive, 7-cycle</figDesc><table><row><cell>L3 cache</cell><cell>Shared, 1 MB-8 MB, non-inclusive, 27-cycle; 16-way, hashed set-assoc</cell></row><row><cell cols="2">Coherence MESI, 64 B lines, no silent drops; seq. consistency</cell></row><row><cell>Memory</cell><cell>DDR-1333 MHz, 2 ranks/channel, 1 (ST) or 2 (MT) channels</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II :</head><label>II</label><figDesc>Configuration of the simulated systems for single-(ST) and multi-threaded (MT) experiments.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Misses per thousand instructions (MPKI) vs. total cache area across sizes (1 MB-8 MB) for MIN, random, LRU, SHiP, PDP, and EVA on selected memory-intensive SPEC CPU2006 benchmarks. (Lower is better.) replacement tag bits and other overheads taken from prior work. Whenever unclear, we use favorable numbers for other policies: DRRIP and SHiP add 2 bits/tag, and SHiP adds 1.875 KB for tables (0.1 mm 2 ). PDP adds 3 bits/tag and 10 K NAND gates (0.02 mm 2 ). LRU uses 8 bits/tag. Random adds no overhead. Since MIN is our upper bound, we also grant it zero overhead. Finally, EVA adds 8 bits/tag and 0.04 mm 2 (Sec. VI).</figDesc><table><row><cell>MIN</cell><cell>Random</cell><cell>LRU</cell><cell>SHiP</cell><cell>PDP</cell><cell>EVA</cell></row><row><cell cols="3">Fig. 10: Workloads: Workloads: Workloads: Workloads: We execute SPEC CPU2006 apps for 10 B instruc-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">tions after fast-forwarding 10 B instructions. Since IPC is not</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">a valid measure of work in multi-threaded workloads [3], we</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">instrument SPEC OMP2012 apps with heartbeats that denote</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">application-level work. Each completes a region of interest</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">(ROI) with heartbeats equal to those completed in 1 B cycles</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">with an 8 MB, LRU LLC (excluding initialization).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Metrics: Metrics: Metrics: Metrics: We report misses per thousand instructions (MPKI)</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">and end-to-end performance; for multi-threaded apps, we report</cell><cell></cell><cell></cell><cell></cell></row></table><note><p>MPKI by normalizing misses by the instructions executed on an 8 MB, LRU LLC. EVA's performance results include the time spent in software updates, which is negligible (Sec. VI-C).</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>All with ?3 L2 MPKI; Fig. 10 plus bzip2, gcc, milc, gromacs, leslie3d, gobmk, soplex, calculix, omnetpp, and astar.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>We thank <rs type="person">David Karger</rs> for pointing us towards Markov decision processes. We thank <rs type="person">Harshad Kasture</rs>, <rs type="person">Po-An Tsai</rs>, <rs type="person">Joel Emer</rs>, <rs type="person">Guowei Zhang</rs>, <rs type="person">Suvinay Subramanian</rs>, <rs type="person">Mark Jeffrey</rs>, <rs type="person">Anurag Mukkara</rs>, <rs type="person">Mor Harchol-Balter</rs>, <rs type="person">Haoxian Chen</rs>, and the anonymous reviewers for their helpful feedback. This work was supported in part by <rs type="funder">NSF</rs> grant <rs type="grantNumber">CCF-1318384</rs> and a grant from the <rs type="funder">Qatar Computing Research Institute</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_tBEnNX8">
					<idno type="grant-number">CCF-1318384</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Analyzing the performance of LRU caches under non-stationary traffic patterns</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Traverso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Giaccone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Leonardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Niccolini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.4909v1[cs.NI</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Principles of optimal page replacement</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Denning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">IPC considered harmful for multiprocessor workloads</title>
		<author>
			<persName><forename type="first">A</forename><surname>Alameldeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Optimal replacement policies for nonuniform cache objects with optional eviction</title>
		<author>
			<persName><forename type="first">O</forename><surname>Bahat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INFOCOM</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Design and analysis of spatially-partitioned shared caches</title>
		<author>
			<persName><forename type="first">N</forename><surname>Beckmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Bridging theory and practice in cache replacement</title>
		<author>
			<persName><forename type="first">N</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sanchez</surname></persName>
		</author>
		<idno>MIT- CSAIL-TR-2015-034</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology,</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Talus: A simple way to remove cliffs in cache performance</title>
		<author>
			<persName><forename type="first">N</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sanchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HPCA-21</title>
		<meeting>HPCA-21</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cache calculus: Modeling cache performance through differential equations</title>
		<author>
			<persName><forename type="first">N</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sanchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CAL</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Modeling cache performance beyond lru</title>
		<author>
			<persName><forename type="first">N</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sanchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HPCA-22</title>
		<meeting>HPCA-22</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A study of replacement algorithms for a virtual-storage computer</title>
		<author>
			<persName><forename type="first">L</forename><surname>Belady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Sys. J</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reuse distance-based probabilistic cache replacement</title>
		<author>
			<persName><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Aamodt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TACO</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A clustered manycore processor architecture for embedded and accelerated applications</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>De Dinechin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ayrignac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-E</forename><surname>Beaucamps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Couvert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ganne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>De Massas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HPEC</title>
		<meeting>HPEC</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Inside the CORE microarchitecture</title>
		<author>
			<persName><forename type="first">J</forename><surname>Doweck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>in HotChips-18</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving cache management policies using dynamic reuse distances</title>
		<author>
			<persName><forename type="first">N</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cammarota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Veidenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MICRO-45</title>
		<meeting>MICRO-45</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A unified approach to the performance analysis of caching systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Garetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Leonardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Martina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOMPECS</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Back to the future: Leveraging Belady&apos;s algorithm for improved cache replacement</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISCA-43</title>
		<meeting>ISCA-43</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">High performance cache replacement using re-reference interval prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Theobald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Steely</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISCA-37</title>
		<meeting>ISCA-37</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Least-recently-used caching with dependent requests</title>
		<author>
			<persName><forename type="first">P</forename><surname>Jelenkovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radovanovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theor. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">326</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Insertion and promotion for tree-based pseudolru lastlevel caches</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jim?nez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MICRO-46</title>
		<meeting>MICRO-46</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cache replacement based on reuse-distance prediction</title>
		<author>
			<persName><forename type="first">G</forename><surname>Keramidas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Petoumenos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaxiras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCD</title>
		<meeting>ICCD</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sampling dead block prediction for last-level caches</title>
		<author>
			<persName><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jim?nez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MICRO-43</title>
		<meeting>MICRO-43</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Decoupled dynamic cache segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jimenez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HPCA-18</title>
		<meeting>HPCA-18</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Planning with Markov decision processes: An AI perspective</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kolobov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Artificial Intelligence and Machine Learning</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Westmere: A family of 32nm IA processors</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kurd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhamidipati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mozak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nemani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISSCC</title>
		<meeting>ISSCC</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Evaluation techniques for storage hierarchies</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mattson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gecsei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Slutz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Traiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Sys. J</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">ARC: A self-tuning, low overhead replacement cache</title>
		<author>
			<persName><forename type="first">N</forename><surname>Megiddo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Modha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. FAST</title>
		<meeting>FAST</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Instruction-based reuse-distance prediction for effective cache management</title>
		<author>
			<persName><forename type="first">P</forename><surname>Petoumenos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Keramidas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaxiras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SAMOS</title>
		<meeting>SAMOS</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A survey of web cache replacement strategies</title>
		<author>
			<persName><forename type="first">S</forename><surname>Podlipnig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>B?sz?rmenyi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Markov decision processes: discrete stochastic dynamic programming</title>
		<author>
			<persName><forename type="first">M</forename><surname>Puterman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adaptive insertion policies for high performance caching</title>
		<author>
			<persName><forename type="first">M</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Patt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Steely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISCA-34</title>
		<meeting>ISCA-34</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Power-management architecture of the intel microarchitecture code-named sandy bridge</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rotem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Naveh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ananthakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rajwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Weissmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">ZSim: fast and accurate microarchitectural simulation of thousand-core systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISCA-40</title>
		<meeting>ISCA-40</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Reuse-based online models for caches</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGMETRICS Performance Evaluation Review</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Brief announcement: the problem based benchmark suite</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Blelloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fineman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Simhadri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPAA</title>
		<meeting>SPAA</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Inter-reference gap distribution replacement: an improved replacement algorithm for set-associative caches</title>
		<author>
			<persName><forename type="first">M</forename><surname>Takagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hiraki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICS-18</title>
		<meeting>ICS-18</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">S</forename><surname>Thoziyoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Muralimanohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<idno>HPL-2008-20</idno>
	</analytic>
	<monogr>
		<title level="j">HP Labs</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Temporal locality in today&apos;s content caching: why it matters and how to model it</title>
		<author>
			<persName><forename type="first">S</forename><surname>Traverso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Garetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Giaccone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Leonardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Niccolini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGCOMM</title>
		<meeting>SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Futility scaling: High-associativity cache partitioning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MICRO-47</title>
		<meeting>MICRO-47</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">SHiP: signature-based hit predictor for high performance caching</title>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hasenplaugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Steely</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MICRO-44</title>
		<meeting>MICRO-44</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">SPARC64 XIfx: Fujitsu&apos;s Next Generation Processor for HPC</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yoshida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hondou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tabata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kiyota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kojima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
