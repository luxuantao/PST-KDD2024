<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Modelling Uncertainty in Deep Learning for Camera Relocalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-02-18">18 Feb 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Alex</forename><surname>Kendall</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
						</author>
						<title level="a" type="main">Modelling Uncertainty in Deep Learning for Camera Relocalization</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-02-18">18 Feb 2016</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1509.05909v2[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a robust and real-time monocular six degree of freedom visual relocalization system. We use a Bayesian convolutional neural network to regress the 6-DOF camera pose from a single RGB image. It is trained in an end-to-end manner with no need of additional engineering or graph optimisation. The algorithm can operate indoors and outdoors in real time, taking under 6ms to compute. It obtains approximately 2m and 6 • accuracy for very large scale outdoor scenes and 0.5m and 10 • accuracy indoors. Using a Bayesian convolutional neural network implementation we obtain an estimate of the model's relocalization uncertainty and improve state of the art localization accuracy on a large scale outdoor dataset. We leverage the uncertainty measure to estimate metric relocalization error and to detect the presence or absence of the scene in the input image. We show that the model's uncertainty is caused by images being dissimilar to the training dataset in either pose or appearance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. MOTIVATION</head><p>Modern Simultaneous Localization and Mapping (SLAM) systems perform well for a number of applications such as augmented reality and domestic robotics <ref type="bibr" target="#b0">[1]</ref>. However, we are yet to see their wide spread use in the wild because of their inability to cope with large viewpoint or appearance changes. The point landmarks used by visual SLAM, such as SIFT <ref type="bibr" target="#b1">[2]</ref> or ORB <ref type="bibr" target="#b2">[3]</ref>, are not able to create a representation which is sufficiently robust to these challenging scenarios. Dense SLAM systems <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> attempt to estimate camera motion directly from the pixels, but are also fragile in these situations. Their assumption of small changes between frames breaks with large viewpoint or appearance changes. In addition to these short comings, both these metric SLAM frameworks require a good initial pose estimate to be able to track the camera's pose continuously and are ineffective at relocalization.</p><p>A complimentary system, appearance based SLAM, can relocalize to a coarse pose estimate, but is constrained to classifying the scene among a limited number of discrete locations. This has been achieved using SIFT <ref type="bibr" target="#b1">[2]</ref> features with FAB-MAP <ref type="bibr" target="#b5">[6]</ref> or using convolutional neural network features <ref type="bibr" target="#b6">[7]</ref>.</p><p>In <ref type="bibr" target="#b7">[8]</ref>, we introduced a new framework for localization, PoseNet, which overcomes many limitations of these current systems. It removes the need for separate mechanisms for appearance based relocalization and metric pose estimation. Furthermore it does not need to store key frames, or establish frame to frame correspondence. It does this by mapping monocular images to a high dimensional space which is</p><p>The authors are with the Department of Engineering, University of Cambridge, UK. Contact: agk34@cam.ac.uk</p><p>The web demo, code and dataset can be viewed on the project webpage mi.eng.cam.ac.uk/projects/relocalisation/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6-DOF Camera Pose with Uncertainty</head><p>Single RGB Input Image</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stochastic Pose Samples from Bayesian Convolutional Network</head><p>Fig. <ref type="figure" target="#fig_2">1</ref>: Bayesian convolutional network 6-DOF pose regressor. Our system takes a single RGB image as input and outputs the 6-DOF camera pose with a measure of model uncertainty. The system operates in under 6ms per image on a GPU, requiring &lt; 50MB memory and can relocalize within approximately 2m and 6 • for large outdoor scenes spanning 50, 000m 2 . For an online demonstration, please see mi.eng.cam.ac.uk/projects/relocalisation/ linear in pose and robust to annoyance variables. We can then regress the full 6-DOF camera pose from this representation. This allows us to regress camera pose from the image directly, without the need of tracking or landmark matching.</p><p>The main contribution of this paper is extending this framework to a Bayesian model which is able to determine the uncertainty of localization. Our Bayesian convolutional neural network requires no additional memory, and can relocalize in under 6ms per frame on a GPU. By leveraging this probabilistic approach, we achieve an improvement on PoseNet's performance on Cambridge Landmarks, a very large urban relocalization dataset, and 7 Scenes, a challenging indoor relocalization dataset. Furthermore, our approach qualitatively improves the system by producing a measure of model uncertainty. We leverage this uncertainty value to estimate:</p><p>• metric relocalization error for both position and orientation, • the confidence of modelling the data (detect if the scene is actually present in the input image). Understanding model uncertainty is an important feature for a localization system. A non-Bayesian system which outputs point estimates does not interpret if the model is making sensible predictions or just guessing at random. By measuring uncertainty we can understand with what confidence we can trust the prediction.</p><p>Secondly, it is easy to imagine visually similar landmarks and we need to be able to understand with what confidence can we trust the result. For example, there are many examples of buildings with visually ambiguous structures, such as window, which are tessellated along a wall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Visual SLAM <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b8">[9]</ref>, and relocalization research <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, has largely focused on registering viewpoints using point-based landmark features, such as SIFT <ref type="bibr" target="#b1">[2]</ref> or ORB <ref type="bibr" target="#b2">[3]</ref>. The problem with using local point-based features, is that they are not robust to many real-life scenarios. Tracking loss typically fails with rapid motion, large changes in viewpoint, or significant appearance changes.</p><p>Convolutional neural networks have recently been shown to be extremely powerful for generating image representations for classification <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. Their hierarchical structure has been shown to build representations from an object level. These higher level contours and shape cues <ref type="bibr" target="#b13">[14]</ref> have been shown to be increasingly invariant to object instantiation parameters. However their use for regression has been predominantly limited to object detection <ref type="bibr" target="#b12">[13]</ref> and human pose regression <ref type="bibr" target="#b14">[15]</ref>. Our approach <ref type="bibr" target="#b7">[8]</ref> leverages convolutional network's advanced high level representations to regress the full 3D camera pose. We show in <ref type="bibr" target="#b7">[8]</ref> that using these highlevel features allows us to localize with unknown camera intrinsics, different weather, silhouette lighting and other challenging scenarios.</p><p>A learning approach to relocalization has also been proposed previously using random forests to regress Scene Coordinate labels <ref type="bibr" target="#b15">[16]</ref>. It was extended with a probabilistic approach in <ref type="bibr" target="#b16">[17]</ref>. However this algorithm has not been demonstrated beyond a scale of a few m 3 and requires RGB-D images to generate the scene coordinate label, in practice constraining its use to indoor scenes.</p><p>Although many of the modern SLAM algorithms do not consider localization uncertainty <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b10">[11]</ref>, previous probabilistic algorithms have been proposed. Bayesian approaches include extended Kalman filters and particle filter approaches such as FastSLAM <ref type="bibr" target="#b17">[18]</ref>. However these approaches estimate uncertainty from sensor noise models, not the uncertainty of the model to represent the data. Our proposed framework does not assume any input noise but measures the model uncertainty for localization.</p><p>Neural networks which consider uncertainty are known as Bayesian neural networks <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. They offer a probabilistic interpretation of deep learning models by inferring distributions over the networks weights. They are often very computationally expensive, increasing the number of model parameters without increasing model capacity significantly. Performing inference in Bayesian neural networks is a difficult task, and approximations to the model posterior are often used, such as variational inference <ref type="bibr" target="#b20">[21]</ref>.</p><p>On the other hand, the already significant parameterization of convolutional network architectures leaves them particularly susceptible to over-fitting without large amounts of training data. A technique known as dropout is commonly used as a regularizer in convolutional neural networks to prevent overfitting and coadaption of features <ref type="bibr" target="#b21">[22]</ref>. During training with stochastic gradient descent, dropout randomly removes connections within a network. By doing this it samples from a number of thinned networks with reduced width. At test time, standard dropout approximates the effect of averaging the predictions of all these thinnned networks by using the weights of the unthinned network. This can be thought of as sampling from a distribution over models.</p><p>Recent work <ref type="bibr" target="#b22">[23]</ref> has extended dropout to approximate Bayesian inference over the network's weights. Gal and Ghahramani <ref type="bibr" target="#b23">[24]</ref> show that dropout can be used at test time to impose a Bernoulli distribution over the convolutional net filter's weights, without requiring any additional model parameters. This is achieved by sampling the network with randomly dropped out connections at test time. We can consider these as Monte Carlo samples which sample from the posterior distribution of models. This is significantly different to the 'probabilities' obtained from a softmax classifier in classification networks. The softmax function provides relative probabilities between the class labels, but not an overall measure of the model's uncertainty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MODEL FOR DEEP REGRESSION OF CAMERA POSE</head><p>For clarity, we briefly review PoseNet which we proposed in <ref type="bibr" target="#b7">[8]</ref> to learn camera pose with deep regression. We use a modified GoogLeNet architecture <ref type="bibr" target="#b12">[13]</ref> which is a 23 layer deep convolutional neural network. We replace the classifiers with pose regressors, representing 3D pose with position x and orientation q, expressed as a quaternion. We train the network to regress this 7 dimensional vector with the following Euclidean loss function:</p><formula xml:id="formula_0">loss(I) = x − x 2 + β q − q q 2 (1)</formula><p>The parameter β is tuned to optimally learn both position and orientation simultaneously. We train the model end-toend with stochastic gradient descent. We show in <ref type="bibr" target="#b7">[8]</ref> that with transfer learning we can train the system using only a few dozen training examples.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. MODELLING LOCALIZATION UNCERTAINTY</head><p>Gal and Ghahramani <ref type="bibr" target="#b23">[24]</ref> have shown a theoretical relationship between dropout <ref type="bibr" target="#b21">[22]</ref> and approximate inference in a Gaussian process. We can consider sampling with dropout as a way of getting samples from the posterior distribution of models. They link this approximate model to variational inference in Bayesian convolutional neural networks with Bernoulli distributions. We leverage this method to obtain probabilistic inference of our pose regression model, forming a Bayesian PoseNet.</p><p>Firstly, we briefly summarise the method to obtain a Bayesian convolutional neural network introduced by <ref type="bibr" target="#b23">[24]</ref>. We are interested in finding the posterior distribution over the convolutional weights, W, given our observed training data X and labels Y.</p><formula xml:id="formula_1">p(W | X, Y)<label>(2)</label></formula><p>In general, this posterior distribution is not tractable, we must use some learning method to approximate the distribution of these weights <ref type="bibr" target="#b18">[19]</ref>. We use variational inference to approximate it <ref type="bibr" target="#b20">[21]</ref>. This technique allows us to learn the distribution over the network's weights, q(W), by minimising the Kullback-Leibler (KL) divergence between this approximating distribution and full posterior.</p><formula xml:id="formula_2">KL(q(W) || p(W | X, Y))<label>(3)</label></formula><p>Where the approximating variational distribution q(W i ) for every layer i is defined as:</p><formula xml:id="formula_3">b i,j ∼ Bernoulli(p i ) for j = 1, ..., K i−1 W i = M i diag(b i )<label>(4)</label></formula><p>with b i vectors of Bernoulli distributed random variables and variational parameters M i we obtain the approximate model of the Gaussian process in <ref type="bibr" target="#b23">[24]</ref>. The dropout probabilities, p i , could be optimised. However we leave them at the standard probability of dropping a connection as 50%, i.e. p i = 0.5 <ref type="bibr" target="#b21">[22]</ref>. <ref type="bibr" target="#b23">[24]</ref> shows that minimising the Euclidean loss objective function in the pose regression network has the effect of minimising the Kullback-Leibler divergence term. Therefore training the network with stochastic gradient descent will encourage the model to learn a distribution of weights which explains the data well while preventing over-fitting.</p><p>As a result of this dropout interpretation of Bayesian convolutional neural networks, a dropout layer should be added after every convolutional layer in the network. However in practice this is not the case as is explained in section IV-C. Using standard libraries such as <ref type="bibr" target="#b24">[25]</ref> we can obtain an efficient implementation of a Bernoulli Bayesian convolutional neural network. At test time we perform inference by averaging stochastic samples from the dropout network.</p><p>Therefore the final algorithm for the probabilistic pose net is as follows: for each weight in network do samples ← evaluate network 7: end for 8: compute pose as average of all samples 9: compute uncertainty as a function of the samples' variance Output: 6-DOF pose, uncertainty</p><p>We also explored the possibility of using dense sliding window evaluation of the convolutional pose regressor over the input image to obtain a distribution of poses. This was done by taking 224 × 224 crops at regular intervals from the 455 × 256 pixel input image. This is equivalent to the densely evaluated PoseNet introduced in section <ref type="bibr" target="#b7">[8]</ref>. The variance of these pose samples also correlates with localization error, however not as strongly as sampling from a Bernoulli distribution over the weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Estimating Uncertainty</head><p>We can evaluate the posterior pose distribution from the Bayesian convolutional network by integrating with Monte Carlo sampling. Figure <ref type="figure" target="#fig_1">2</ref> shows a plot of Monte Carlo samples from the output of the posterior network in blue. Additionally, in green we show the output from the first auxiliary pose regressor from the GoogLeNet architecture (see Figure <ref type="figure" target="#fig_4">3</ref> of <ref type="bibr" target="#b12">[13]</ref>). This output regresses pose from the representation after the inception (sub-net) layer 3. This result is at a much shallower depth and provides an insight as to what the network learns with additional depth. A similar result can be observed for the quaternion samples for the rotational component of pose.</p><p>For the full network's output (blue) we obtain a distribution that appears to be drawn from both an isotropic and single-modal Gaussian. The network appears to be very certain about the specific pose. By sampling with dropout over the distribution of models we observe some isotropic scatter around a single pose estimate.</p><p>At a shallower depth, with the first auxiliary pose regressor (green), the results are multi-modal. This is especially true for visually ambiguous images such as (c) in Figure <ref type="figure" target="#fig_1">2</ref>. The window in image (c) is repeated along the face of St Mary's Church. Using dropout to sample the distribution of models at this shallower depth produces distributions which have components drawn from multiple pose hypotheses. This suggests that this extra depth in the network is able to learn a representation that is sufficiently discriminative to distinguish visually similar landmarks.</p><p>Therefore, we fit a unimodal Gaussian to the samples from the network's final pose regressor. We treat the mean of these samples as the point estimate for pose. For an uncertainty measurement we take the trace of the unimodal Gaussian's covariance matrix. We have found the trace to be an effective scalar measure of uncertainty. The trace is a sum of the eigenvalues, which is rotationally invariant and represents the uncertainty that the Gaussian contains effectively. Figure <ref type="figure">8</ref> empirically shows this uncertainty measure is strongly correlated with metric error in relocalization.</p><p>We also considered using the determinant, which is a measure of the Gaussian's volume. However the determinant takes the product of the eigenvalues which means that if some are large and others are small then the resulting value will be small. This was the case as the resulting Gaussian often had a strong elongated component to it, as can be observed in Figure <ref type="figure" target="#fig_1">2</ref>. We found that using the determinant resulted in a numerically poorer measure of uncertainty.</p><p>We tried other models which accounted for multi-modality in the data: • taking the geometric median instead of the mean as a point prediction, • fitting a mixture of Gaussians model to the data using the Dirichlet process <ref type="bibr" target="#b25">[26]</ref>,</p><p>• clustering the samples using k-means and taking the mean of the largest cluster. However we found that all of these methods produced poorer localization uncertainty than fitting a single unimodal Gaussian to the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Creating a Comparable Uncertainty Statistic</head><p>In order to compare the uncertainty values we obtained from a model, we propose the following method to create a normalized measure, or Z-score. This is an uncertainty value which is able to be directly compared between models.</p><p>To achieve this, firstly we evaluate the test dataset and record the predicted camera poses and associated uncertainties for each scene. Typical distribution of uncertainty results for the Street scene can be viewed in Figure <ref type="figure" target="#fig_4">3</ref>. Examining this distribution, we chose to model it with a Gamma distribution for three reasons; it requires only two parameters, the distribution is constrained to strictly positive values only and is right skewed.</p><p>Obtaining an estimate for the distribution of model uncertainty values for images from a scene's test set allows us to evaluate where a new image's uncertainty values sit compared to the population. We can now assign a percentile to both the translational and rotational uncertainty values by using the cumulative distribution function for the Gamma distribution. We treat this percentile as a Z-score and generate this from a separate distribution for both the translational and rotational uncertainties, as well as separately for each scene.</p><p>Figure <ref type="figure" target="#fig_5">4</ref> shows that the rotational and translational uncertainties are highly correlated. We can therefore compute an overall localization uncertainty by averaging the Z-score for translational and rotational uncertainty. This gives us a final single percentile score which we assign as the confidence of the pose prediction for a given model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Architecture</head><p>To obtain a fully Bayesian model we should perform dropout sampling after every convolutional layer. However  we found in practice this was not empirically optimal. In <ref type="bibr" target="#b7">[8]</ref> we discovered that fine tuning from pretrained filters trained on a large scale dataset such as Places <ref type="bibr" target="#b26">[27]</ref> enhanced localization accuracy significantly. This is again true with the probabilistic network. However these pretrained filters were trained without the use of dropout.</p><p>Finetuning from weights pretrained on the Places <ref type="bibr" target="#b26">[27]</ref> dataset, we experimented with adding dropout throughout the model at different depths. We observe a performance drop in localization when using the fully Bayesian convolutional neural network. Using dropout after every convolutional layer throughout the network acted as too strong a regularizer and degraded performance by approximately 10% . We obtained the optimal result when we included it only before convolutions which had randomly initialized weights. Therefore we add dropout after inception (sub-net) layer 9 and after the fully connected layers in the pose regressor.</p><p>Yosinski et al. <ref type="bibr" target="#b27">[28]</ref> argues that transferring weights can cause performance to drop in two situations. Firstly when the representation is too specific. However this is unlikely to be the case as we found the weights could successfully generalize to the new task <ref type="bibr" target="#b7">[8]</ref>. The second explanation was that features may co-adapt fragilely and that transferring them breaks these co-adaptions. We believe this may be the case. The local minima that the weights were optimised to without dropout requires complex co-adaptions that are not able to optimise to a network with the same performance when using dropout.</p><p>We did not experiment with changing the dropout probability, or attempt to optimise this hyperparameter. We leave this to future work.</p><p>With this architecture we can then sample from the probabilistic model at test time to obtain an estimate of pose. We can improve localization performance by averaging the Monte Carlo dropout samples <ref type="bibr" target="#b23">[24]</ref>. Figure <ref type="figure" target="#fig_6">5</ref> gives empirical results suggesting that 40 samples are enough to achieve convergence of Monte Carlo samples. We show that less than five samples are typically required to surpass the performance of using a single pose regression convolutional net. After approximately 40 samples no more increase in localization accuracy is observed. and densely evaluated PoseNet (red) <ref type="bibr" target="#b7">[8]</ref>. This shows that Monte Carlo sampling provides significant improvement over both these point estimate models after a couple of samples. Monte Carlo sampling converges after around 40 samples and no more significant improvement is observed with more samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS</head><p>We evaluate the performance of the Bayesian convolutional neural network pose regressor on the localization dataset, Cambridge Landmarks, which was introduced in <ref type="bibr" target="#b7">[8]</ref>. Additionally we present results on an indoor relocalization dataset, 7 Scenes <ref type="bibr" target="#b15">[16]</ref>. Table <ref type="table">6</ref> presents the experimental results of localization accuracy, averaging 100 Monte Carlo dropout samples from the probabilistic PoseNet. We compare this to PoseNet introduced in <ref type="bibr" target="#b7">[8]</ref> and to a nearest neighbour baseline <ref type="bibr" target="#b7">[8]</ref> which finds the nearest pose from the training set in feature vector space. We also compare to the SCORE Forest algorithm <ref type="bibr" target="#b15">[16]</ref> which is state-of-the-art for relocalization with depth data, however the need for RGB-D data often constrains it to indoor use.</p><p>The results in Table <ref type="table">6</ref> show that using Monte Carlo dropout <ref type="bibr" target="#b23">[24]</ref> results in a considerable improvement in localization accuracy over PoseNet <ref type="bibr" target="#b7">[8]</ref>. Allowing the model to take into account the uncertainty of model selection, by placing a Bernoulli distribution over the weights, results in more accurate localization. The Monte Carlo samples allow us to obtain estimates of poses probabilistically over the distribution of models. The mean of these samples results in a more accurate solution.</p><p>Figure <ref type="figure" target="#fig_7">7</ref> shows a cumulative histogram of errors for two scenes. This shows that our probabilistic PoseNet performs consistently better than the non-probabilistic PoseNet for all error thresholds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Uncertainty as an Estimate of Error</head><p>Figure <ref type="figure">8</ref> shows that the uncertainty estimate is very strongly correlated with metric relocalization error. This shows that we can use the uncertainty estimate to predict relocalization error. The plot shows that this relationship is linear for both translational and rotational uncertainty. However the proportionality gradient between error and uncertainty varies significantly between scenes.</p><p>Figure <ref type="figure" target="#fig_5">4</ref> shows that metric error and uncertainty values are correlated between rotational and translational values.  <ref type="bibr" target="#b15">16]</ref> datasets. We compare the performance of the probabilistic PoseNet to PoseNet and a nearest neighbour baseline <ref type="bibr" target="#b7">[8]</ref>. Additionally we compare to SCORE Forests <ref type="bibr" target="#b15">[16]</ref> which requires depth input, limiting it to indoor scenes. The performance of the uncertainty model is shown with 100 Monte Carlo dropout samples. In addition to the qualitative improvement of obtaining an uncertainty metric, we also observe an improvement in relocalization accuracy of up to 10% over Dense PoseNet. supports the assumptions in our method of generating an overall uncertainty estimate as an 'average' of these normalized values. We observe relocalization error and uncertainty are strongly correlated between both position and orientation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Uncertainty as a Landmark Detector</head><p>We show that the uncertainty metric can also be used to determine if the image is from the scene or landmark that the pose regressor was trained on. For a given scene in a dataset we test each image on all of the models. We then compute the uncertainty metric using the normalization method proposed in Section IV-B. The image should have the lowest uncertainty value with the model which was trained on the scene that the image was taken from.</p><p>In Figure <ref type="figure">9</ref> we present a confusion matrix showing this result for the Cambridge Landmarks and 7 Scenes datasets. We exclude the Street scene as it contains many of the landmarks in the other scenes. We show the confusion matrix when using the combined normalized uncertainty. We observed that combining the rotation and translation metrics often provides a superior and more robust error metric.</p><p>Note that the network has not been trained to classify the landmark it is observing. This is obtained as a by-product of the probabilistic architecture. If the convolutional net was trained to classify landmarks we are confident that it would perform significantly better. The purpose of this was to validate that the uncertainty measurement can reflect whether or not the network is trained on what it is presented with. The results show that the system can not only estimate the accuracy of the prediction, but also correctly identify when the landmark is not present at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. What Makes the Model Uncertain About a Pose?</head><p>An initial hypothesis may be that test images which are far from training examples give very uncertain results, because they are more unknown to the network. To study this we plotted, for each test image in a scene, the uncertainty against the Euclidean distance between the test image and the nearest training image. This plot showed a very slight increasing trend but was not sufficiently clear to draw any conclusions. However Euclidean distance to the nearest training image is not a comprehensive measure of similarity to the training set. It excludes other variables such as orientation, weather, pedestrian activity and lighting.</p><p>PoseNet produces a 2048 dimensional feature vector (see Section 3.2 of <ref type="bibr" target="#b7">[8]</ref>). This feature vector contains a high dimensional representation of instantiation parameters in the scene, such as weather, lighting, and object pose. Therefore we use this feature vector as a representation of the image. To compute similarity between two images, we evaluate the pose regressor's feature vector for each image and take the Euclidean distance between each feature vector. Therefore we can use this as a measure of similarity between a test image and the dataset's training image by finding the distance to the nearest neighbour training image in this feature space. This is the same measure used to compute the nearest neighbour results in Table <ref type="table">6</ref>.</p><p>Figure <ref type="figure" target="#fig_8">10</ref> shows a plot of model uncertainty against this distance for all test images in the Street scene. The strong relationship indicates that the model is more uncertain for images which are less similar (in this localization feature space) to those in the training dataset.</p><p>The points which deviate from this trend, with larger uncertainty values, are typically the difficult images to localize. Some examples are shown in Figure <ref type="figure" target="#fig_9">11</ref> These images have challenges such as heavy vehicle occlusion or strong silhouette lighting which result in inaccurate and uncertain prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. System Efficiency</head><p>We now compare the performance of our probabilistic PoseNet to our non-probabilistic PoseNet introduced in <ref type="bibr" target="#b7">[8]</ref>. The probabilistic approach shares all the same benefits of PoseNet <ref type="bibr" target="#b7">[8]</ref>, being scalable as its memory and computational requirements do not scale with map size or training data. Introducing dropout uncertainty does not require any more parametrisation and the weight file remains constant at 50 MB. This is still much more efficient than the gigabytes required for metric relocalization systems with point features <ref type="bibr" target="#b10">[11]</ref>. This shows that the uncertainty metric is able to recognize correctly the landmark that it was trained to relocalize from. The network outputs large model uncertainty when it is presented with an unknown scene. The average scene detection accuracy is approximately 78% for Cambridge Landmarks. The indoor dataset is a far more challenging problem, as many scenes are very visually ambiguous. For example the pumpkin scene is the same room as the kitchen, with a different arrangement. Despite this, our system still performs modestly with 52% accuracy. Drawing stochastic samples comes at a small additional time cost. As Figure <ref type="figure" target="#fig_6">5</ref> shows, the optimal samples to take is approximately 40 as any more samples than this does not significantly improve performance. When operating on a parallel processor, such as a GPU, this extra computation is manageable by treating it as a mini-batch of operations. Only the final two fully connected layers need to be sampled, as the first 21 convolutional layers are deterministic. For example, on an NVIDIA Titan X, computing pose by averaging 40 Monte Carlo dropout samples takes 5.4ms while 128 samples takes 6ms. For comparison, a single PoseNet evaluation takes 5ms per image. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) King's College (b) St Mary's Church (c) St Mary's Church.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: 3D scatter plots of Monte Carlo pose samples from the Bayesian convolutional neural network (top row) from an input image (bottom row) from the posterior distribution. We show typical examples from two scenes (a,b) and a visually ambiguous example (c).In green are the results from the first auxiliary pose regressor and in blue are samples from the final pose regressor. It shows that the auxiliary pose predictions (from the shallower sub-net) are typically multimodal however results from the final regressor are unimodal.</figDesc><graphic url="image-7.png" coords="3,274.50,145.05,63.00,63.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1</head><label>1</label><figDesc>Probabilistic PoseNet Input: image, learned weights W, number of samples 1: for sample = 1 to number of samples do 2: set network's weights to learned values 3:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Histograms of uncertainty values from the testing images in the Street scene. In red we show the Gamma distribution used to model these populations. The Gamma distribution is a reasonable fit of the positively distributed, right skewed data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 :</head><label>4</label><figDesc>Fig.4: Plot of translational uncertainty against rotational uncertainty for test images in the St Mary's Church and King's College scene and for all scenes. This shows that the model uncertainty values are very strongly correlated for both rotation and translation. This suggests that we can form a single uncertainty value which represents the overall model uncertainty.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Localization accuracy in the Street scene for different number of Monte Carlo samples. Results are averaged over 8 repetitions, with 1 standard deviation error bars shown. Horizontal lines are shown representing the performance of PoseNet (green)and densely evaluated PoseNet (red)<ref type="bibr" target="#b7">[8]</ref>. This shows that Monte Carlo sampling provides significant improvement over both these point estimate models after a couple of samples. Monte Carlo sampling converges after around 40 samples and no more significant improvement is observed with more samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Localization accuracy for both position and orientation as a cumulative histogram of errors for the entire test set. This shows that our probabilistic PoseNet performs consistently better than the non-probabilistic PoseNet for all error thresholds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 :</head><label>10</label><figDesc>Fig. 10: Uncertainty value for test images in the Street scene, plotted against Euclidean distance to the nearest neighbour training image feature vector. The feature vector is a 2048 dimensional vector obtained from the final layer in PoseNet before the pose regression. This shows that having similar training examples lowers model uncertainty in test images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 :</head><label>11</label><figDesc>Fig. 11: Images with the largest uncertainty values and largest localization errors. All of these images contain one of the following situations causing difficult and uncertain localization: strong occlusion from vehicles, pedestrians or other objects, motion blur, are taken from an area at the edge of the scene or are distant from a training example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Plot of translational and rotational errors against their respective estimated uncertainty for test images in the King's College scene and for all scenes. These plots show that the uncertainty is correlated with error and provides a good estimate of metric relocalization error. It also shows that the scale of uncertainty values that each model learns varies significantly, suggesting they should be normalized for each model, as proposed in Section IV-B.</figDesc><table><row><cell></cell><cell>12</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.36</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Positional Uncertainty</cell><cell>3 4 5 6 7 8 9 10 11</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Rotational Uncertainty</cell><cell cols="2">0.22 0.24 0.26 0.28 0.3 0.32 0.34</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0 2</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell><cell>12</cell><cell>14</cell><cell>16</cell><cell>18</cell><cell></cell><cell></cell><cell>0 0.2</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">Positional Error [m]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">Rotational Error [degrees]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">(a) King's College</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Street</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Street</cell></row><row><cell>Positional Uncertainty</cell><cell>2 4 6 8 10 12 14 16 18</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">King's College Great Court St Mary's Church Old Hospital Shop Facade</cell><cell cols="2">Rotational Uncertainty</cell><cell>0.3 0.4 0.5 0.6 0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">King's College Great Court St Mary's Church Old Hospital Shop Facade</cell></row><row><cell></cell><cell>0 0</cell><cell></cell><cell>5</cell><cell></cell><cell>10</cell><cell></cell><cell cols="2">15</cell><cell></cell><cell>20</cell><cell></cell><cell></cell><cell>0 0.2</cell><cell></cell><cell>10</cell><cell>20</cell><cell></cell><cell>30</cell><cell></cell><cell>40</cell><cell>50</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">Positional Error [m]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">Rotational Error [degrees]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">(b) All Scenes</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Fig. 8:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Scene recognition confusion matrices. For each dataset (row) we computed the Z-score for both rotation and translation uncertainties. Dataset images were classified to the model (column) with the lowest uncertainty. Note that the Street scene is excluded as it contains many of the other landmarks in Cambridge Landmarks.</figDesc><table><row><cell></cell><cell cols="2">King's College</cell><cell>St Mary's</cell><cell></cell><cell>Old Hospital</cell><cell cols="2">Shop Facade</cell></row><row><cell>King's College</cell><cell>75.22</cell><cell></cell><cell>5.54</cell><cell></cell><cell>11.66</cell><cell></cell><cell>7.58</cell></row><row><cell>St Mary's</cell><cell>2.45</cell><cell></cell><cell>79.43</cell><cell></cell><cell>5.28</cell><cell></cell><cell>12.83</cell></row><row><cell>Old Hospital</cell><cell>11.54</cell><cell></cell><cell>0</cell><cell></cell><cell>80.22</cell><cell></cell><cell>8.24</cell></row><row><cell>Shop Facade</cell><cell>4.85</cell><cell></cell><cell>4.85</cell><cell></cell><cell>14.56</cell><cell></cell><cell>75.73</cell></row><row><cell cols="8">(a) Confusion matrix for Cambridge Landmarks dataset</cell></row><row><cell></cell><cell>Chess</cell><cell>Fire</cell><cell>Heads</cell><cell cols="4">Office Pumpkin Kitchen Stairs</cell></row><row><cell>Chess</cell><cell>57.5</cell><cell>10.5</cell><cell>1.6</cell><cell>9.4</cell><cell>7.2</cell><cell>6.9</cell><cell>6.9</cell></row><row><cell>Fire</cell><cell>0.4</cell><cell>59.8</cell><cell>6.8</cell><cell>1.6</cell><cell>1.7</cell><cell>9.9</cell><cell>19.8</cell></row><row><cell>Heads</cell><cell>5.9</cell><cell>2.8</cell><cell>52.4</cell><cell>14</cell><cell>3.7</cell><cell>7.8</cell><cell>13.4</cell></row><row><cell>Office</cell><cell>12.2</cell><cell>11.4</cell><cell>6.2</cell><cell>42.4</cell><cell>6.6</cell><cell>14.9</cell><cell>6.3</cell></row><row><cell>Pumpkin</cell><cell>13.1</cell><cell>11.2</cell><cell>6.6</cell><cell>2.1</cell><cell>45.6</cell><cell>5.4</cell><cell>16</cell></row><row><cell>Red Kitchen</cell><cell>7.9</cell><cell>5.2</cell><cell>9.3</cell><cell>5.1</cell><cell>6.5</cell><cell>57.9</cell><cell>8.1</cell></row><row><cell>Stairs</cell><cell>11.1</cell><cell>4.5</cell><cell>2.8</cell><cell>11.1</cell><cell>18.4</cell><cell>1.9</cell><cell>50.2</cell></row><row><cell></cell><cell cols="6">(b) Confusion matrix for 7 Scenes dataset</cell><cell></cell></row><row><cell>Fig. 9:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>We show how to successfully apply an uncertainty framework to the convolutional neural network pose regressor, PoseNet. This improved PoseNet's real time relocalization accuracy for indoor and outdoor scenes. We do this by averaging Monte Carlo dropout samples from the posterior Bernoulli distribution of the Bayesian convolutional network's weights. This is requires no extra parametrisation.</p><p>Furthermore we show the trace of these sample's covariance matrix provides an appropriate model uncertainty estimate. We show that this uncertainty estimate accurately reflects the metric relocalization error and can be used to detect the presence of a previously observed landmark. We present evidence that shows the model is more uncertain about images which are dissimilar to the training examples.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Parallel tracking and mapping for small ar workspaces. In Mixed and Augmented Reality</title>
		<author>
			<persName><forename type="first">Georg</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMAR 2007. 6th IEEE and ACM International Symposium on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page" from="225" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><surname>David G Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Orb: an efficient alternative to sift or surf</title>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Rublee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2011 IEEE International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2564" to="2571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">DTAM: Dense tracking and mapping in real-time</title>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Richard A Newcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Lovegrove</surname></persName>
		</author>
		<author>
			<persName><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2011 IEEE International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2320" to="2327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">LSD-SLAM: Large-scale direct monocular slam</title>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Schöps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="834" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">FAB-MAP: Probabilistic localization and mapping in the space of appearance</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="647" to="665" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Place recognition with convnet landmarks: Viewpoint-robust, condition-robust, trainingfree</title>
		<author>
			<persName><forename type="first">Niko</forename><surname>Sunderhauf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sareh</forename><surname>Shirazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feras</forename><surname>Dayoub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Pepperell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Upcroft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Milford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Robotics: Science and Systems XII</title>
				<meeting>Robotics: Science and Systems XII</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional networks for real-time 6-dof camera relocalization</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
				<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Orb-slam: a versatile and accurate monocular slam system</title>
		<author>
			<persName><forename type="first">Raul</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">D</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName><surname>Tardos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Robotics</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Coarse-to-fine vision-based localization by indexing scale-invariant features</title>
		<author>
			<persName><forename type="first">Junqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongbin</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on</title>
				<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="413" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Worldwide pose estimation using 3d point clouds</title>
		<author>
			<persName><forename type="first">Yunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Huttenlocher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2012</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="15" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scene coordinate regression forests for camera relocalization in RGB-D images</title>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2930" to="2937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploiting uncertainty in regression forests for accurate camera relocalization</title>
		<author>
			<persName><forename type="first">Julien</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4400" to="4408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Probabilistic robotics</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfram</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Transforming neural-net output levels to probability distributions</title>
		<author>
			<persName><forename type="first">John</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 3. Citeseer</title>
				<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A practical bayesian framework for backpropagation networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="448" to="472" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Practical variational inference for neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2348" to="2356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02142</idno>
		<title level="m">Dropout as a Bayesian approximation: Representing model uncertainty in deep learning</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Bayesian convolutional neural networks with bernoulli approximate variational inference</title>
		<author>
			<persName><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02158</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><surname>Caffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Variational inference for dirichlet process mixtures</title>
		<author>
			<persName><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bayesian analysis</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="121" to="143" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="487" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
