<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automated Depression Analysis Using Convolutional Neural Networks from Speech</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-05-28">May 28, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lang</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">NPU-VUB joint AVSP Research Lab</orgName>
								<orgName type="institution">Northwestern Polytechnical University (NPU)</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cui</forename><surname>Cao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Moscow Institute of Arts</orgName>
								<orgName type="institution" key="instit2">Weinan Normal University</orgName>
								<address>
									<settlement>Weinan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Automated Depression Analysis Using Convolutional Neural Networks from Speech</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-05-28">May 28, 2018</date>
						</imprint>
					</monogr>
					<idno type="MD5">9C7AD15F8FF422AD45B0B6FD4D5B5130</idno>
					<idno type="DOI">10.1016/j.jbi.2018.05.007</idno>
					<note type="submission">Received Date: 6 November 2017 Revised Date: 25 April 2018 Accepted Date: 12 May 2018 Preprint submitted to Elsevier</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Depression</term>
					<term>Automatic diagnosis</term>
					<term>Median Robust extended Local Binary Patterns(MRELBP)</term>
					<term>Speech processing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To help clinicians to efficiently diagnose the severity of a person's depression, the affective computing community and the artificial intelligence field have shown a growing interest in designing automated systems. The speech features have useful information for the diagnosis of depression. However, manually designing and domain knowledge are still important for the selection of the feature, which makes the process labor consuming and subjective. In recent years, deep-learned features based on neural networks have shown superior performance to hand-crafted features in various areas. In this paper, to overcome the difficulties mentioned above, we propose a combination of hand-crafted and deep-learned features which can effectively measure the severity of depression from speech. In the proposed method, Deep Convolutional Neural Networks (DCNN) are firstly built to learn deep-learned features from spectrograms and raw speech waveforms. Then we manually extract the state-of-the-art texture descriptors named median robust extended local binary patterns (MRELBP) from spectrograms. To capture the complementary information within the handcrafted features and deep-learned features, we propose joint fine-tuning layers to combine the raw and spectrogram DCNN to boost the depression recognition performance. Moreover, to address the problems with small samples, a data augmentation method was proposed. Experiments conducted on AVEC2013 and AVEC2014 depression databases show that our approach is robust and effective for the diagnosis of depression when compared to state-of-the-art audio-based methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Depression and anxiety disorders are highly prevalent worldwide, which have placed undue burden on individuals, families, and society. Studies suggest that effective treatments for depression can be aided by the detection of the problems at its early stages. According to the World Health Organization (WHO), depression will become the fourth most mental disorder by 2020 <ref type="bibr" target="#b0">[1]</ref>.</p><p>Depression is often difficult to diagnose because it manifests itself in different ways. The assessment methodologies for its diagnosis rely on subjective patient self-report or clinical judgments of symptom severity <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. The Hamilton Rating Scale for Depression (HAMD) <ref type="bibr" target="#b3">[4]</ref> is currently the standard for depression severity estimation. It is worth noting that, evaluations by clinicians vary depending on their expertise and the used diagnosis methods, such as Diagnostic and Statistical Manual of Mental Disorders (DSM-IV) <ref type="bibr" target="#b4">[5]</ref>, the Quick Inventory of Depressive Symptoms-Self Report (QIDS) <ref type="bibr" target="#b5">[6]</ref>, the Beck Depression Inventory (BDI) <ref type="bibr" target="#b6">[7]</ref>, the 10-item Montgomery-Asberg Depression Rating Scale (MADRS) <ref type="bibr" target="#b7">[8]</ref>, the 9-item Patient Health Questionnaire (PHQ-9) <ref type="bibr" target="#b8">[9]</ref>, and the PHQ-8 <ref type="bibr" target="#b9">[10]</ref>.</p><p>In recent years, some machine learning methods have been proposed utilizing audio cues for depression analysis <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>.</p><p>Meanwhile, there is a wealth of research, which suggests that voice patterns have a close relationship with emotion and stress <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref>. In <ref type="bibr" target="#b19">[20]</ref>, the author suggested that the analysis of voice patterns can be divided into three primary categories, including prosodics, the vocal tract, and the glottal source. Although hand-crafted features have been proven to obtain better perfor-mance for estimating depression severity. However, there are some limitations of handcrafted features for depression scale prediction. First, to design hand-crafted features requires a lot of effort (i.e., domain knowledge, labor and time, etc.). For example, Mel Frequency Cepstral Coefficients (MFCCs) are widely used in automatic speech and speaker recognition tasks.</p><p>However, if we designed hand-crafted features like MFCCs, we should have task-specific knowledge of depression and to acquire such knowledge is time-consuming. Second, hand-crafted features may lose some useful information related to depression patterns. Specifically, some patterns of depression implied in the audiovisual signals cannot be well mined. Moreover, the concept of the designed features relies on people's subjective assumptions. Finally, it is difficult to select an appropriate toolkit to extract the features. Various available toolkits are widely used to extract low-level features, such as openSMILE <ref type="bibr" target="#b20">[21]</ref>, COVAREP <ref type="bibr" target="#b21">[22]</ref>, SPTK <ref type="bibr" target="#b22">[23]</ref>, KALDI <ref type="bibr" target="#b23">[24]</ref>, YAAFE <ref type="bibr" target="#b24">[25]</ref>,</p><p>and OpenEAR <ref type="bibr" target="#b25">[26]</ref>. Each existing toolbox is generally the result of a single laboratory's work. Different researchers considered features from their own perspective. There is no unified standard defining which feature is most useful for depression analysis. A more in-depth understanding of the deep learning methods the reader is referred to <ref type="bibr" target="#b26">[27]</ref>. Among these different deep learning representations, Convolutional Neural Networks has been widely used to achieve state-of-the-art performance in many communities <ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref>. Moreover, it has been proved fairly efficient in texture classification scenario. In <ref type="bibr" target="#b30">[31]</ref>, the authors proved that the CNN-based method matched the state-of-theart for the dataset with macroscopic images, and outperformed the best-published results on the microscopic images. The performance of proposed CNN architecture also surpass exist tex-ture descriptors for forest species recognition. To the best of our knowledge, deep-learned features from spectrogram for depression recognition has not yet been explored. Accordingly, in this work we explore how the depression severity prediction can benefit from the adoption of CNN in learning spectrogram patterns of the speech.</p><p>From the machine learning perspective, depression analysis can be considered as a regression or classification problem (e.g., in AVEC2013 <ref type="bibr" target="#b13">[14]</ref> and AVEC2014 <ref type="bibr" target="#b14">[15]</ref> depression sub-challenges). Our goal is to predict the depression score called Beck Depression Inventory-II (BDI-II) of a subject from recorded audio.</p><p>In summary, the main contributions of this work can be summarized as follows. First, we develop an automated framework, which can effectively capture the vocal information for measuring the depression severity. Second, we find that complementary characteristics is existed between hand-crafted features and deep learned features for estimating the depression severity. Third, we propose a combination of the hand-crafted and the deep-learned features to effectively measure the severity of depression from speech. Finally, to address the problems with small samples, a data augmentation method was proposed. To the best of our knowledge, in our proposed approach, it is the first time that the deep learning technology is employed for depression diagnosis.</p><p>The remainder of this paper is organized as follows. Section 2 briefly discusses previous works on audio-based depression analysis and recognition. Section 3 provides more implementation details about the proposed framework. Section 4 introduces the dataset and experimental results. Conclusions and future challenges are discussed in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Various depression recognition approaches have been proposed in the Depression Recognition Sub-Challenge (DSC) of the Audio-Visual Emotion Challenge and Workshop (AVEC2013, AVEC2014, AVEC2016 <ref type="bibr" target="#b31">[32]</ref>, AVEC2017 <ref type="bibr" target="#b32">[33]</ref>).</p><p>Regression methods have been developed using the AVEC2013 and AVEC2014 data sets, and classification approaches considered the AVEC2016 and AVEC2017 data. In this work, we make use of the AVEC2013 and AVEC2014 data sets. Detailed description of the database can be referred to Section 4.1 and 4.2. In our research, we focus on the recorded audio for the diagnosis of depression. In the following section, we briefly describe the competitive audio-based methods for measuring the depression severity. 1   For AVEC2013 depression recognition <ref type="bibr" target="#b13">[14]</ref> In the AVEC2013 depression challenge, Williamson et al. <ref type="bibr" target="#b12">[13]</ref> adopted the combination of eigenvalue spectra and coordination features to analyze the relationship between the vocal behaviors and the depression scales. With the coordinationand phoneme-rate-based features, they designed a Gaussian staircase regression system to predict the BDI-II scores for each audio data. PCA is also used for dimension reduction. Finally, the authors provided the minimum performance on the test sets with root mean square error (RMSE) of 7.42 and mean absolute error (MAE) of 5.75.</p><p>In <ref type="bibr" target="#b33">[34]</ref>, Moore et al. explored prosodics, the vocal tract, and parameters extracted directly from the glottal waveform to discriminate the depressed speech. They extracted about 200 1 Some of following works also used the video cues, while we only focus on the audio cues. prosodics, vocal tract, and glottal waveform measures from the depression database and translated them into 2000 statistics for study.</p><p>In <ref type="bibr" target="#b34">[35]</ref>, Nicholas et al. provided a comprehensive and exhaustive conclusion about the assessment and diagnosis of the depression and the suicide. They reviewed the important characteristics of paralinguistic speech affected by depression and suicide. They analyzed the patterns which were used in classification and regression issues. Finally, they provided an in-depth discussion about the current limitations and challenges.</p><p>In <ref type="bibr" target="#b35">[36]</ref> and <ref type="bibr" target="#b15">[16]</ref>, the authors investigated the relation be- In <ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b46">[47]</ref>, all of them use the audio feature provided by the AVEC2013 depression sub-challenge. In <ref type="bibr" target="#b47">[48]</ref>, they also explored a number of features, (1) estimated articulatory trajectories during speech production, (2) acoustic characteristics, and</p><p>(3) acoustic-phonetic characteristics and (4) prosodic features.</p><p>They are used and compared with different models to predict the Beck depression rating scale, such as support vector regression (SVR), a Gaussian backend, and decision trees.</p><p>In <ref type="bibr" target="#b48">[49]</ref>, Williamson et al. explored the interrelationships and complementary characteristics by extracting features from the speech source, system, and prosody. They fused the different feature domain to obtain a better performance. Finally, they combined Gaussian staircase regression with Extreme Learning Machine (ELM) classifiers, and get a test RMSE of 8.12.</p><p>For the AVEC2016 <ref type="bibr" target="#b31">[32]</ref> and AVEC2017 <ref type="bibr" target="#b32">[33]</ref> depression subchallenge, the organizers provided the audio, video, and transcript files, but did not provide the original video clips. For the audio features of both in AVEC2016 and AVEC2017, they used COVAREP (v1.3.2), a freely available open source Matlab toolbox for speech analyses <ref type="bibr" target="#b21">[22]</ref>. Prosodic, voice quality, and spectral features were extracted by the COVAREP toolkit from the audio signals. In <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>, all of them used the audio features provided by the AVEC2016 organizers. However, the baseline audio features does not include all of the features considered as useful for depression prediction (e.g., jitter, shimmer, etc.). Therefore, the authors in <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>, also extracted another useful audio feature for depression recognition. In <ref type="bibr" target="#b52">[53]</ref>, they extracted spectral features, lower vocal tract physiology features, and loudness variation features, obtaining relatively better results for depression prediction. In <ref type="bibr" target="#b53">[54]</ref>, they extracted extended spectral and prosodic features, teager energy cepstral coefficients, session-level acoustic features, and phoneme-based features. They obtained F1 scores of 0.63 and 0.89 for depressed and not depressed classes respectively.</p><p>From the literature review we can see that hand-crafted audio features have limitations in diagnosing depression. Specially, hand-craft audio features are extracted by different toolboxes from the perspective of different researchers. To overcome these limitations, we explore a more robust representation for depression analysis, which could better capture valuable information from the vocal cues. That is to say, we propose a new approach based on the deep learning networks, for automatic estimation the severity of the depression scale. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Hand crafted based Feature Extraction</head><p>For hand-crafted features, two different kinds of descriptors were adopted. The first one is the Median Robust Extended Local Binary Patterns (MRELBP), a novel descriptor for texture classification <ref type="bibr" target="#b55">[56]</ref>. However, its application for depression recognition has yet not been explored. In this work, we apply MRELBP on spectrogram to extract textural features. The other one is the audio features extracted by openSMILE toolkit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Audio Features</head><p>The 2268 baseline audio features of AVEC2013 <ref type="bibr" target="#b13">[14]</ref> and AVEC2014 <ref type="bibr" target="#b14">[15]</ref> adopt the extended Geneva Minimalistic Acoustic Parameter Set (eGeMAPS) containing functionals on the 38 low-level descriptors (LLDs), which are extracted with the openSMILE toolkit <ref type="bibr" target="#b20">[21]</ref>. These LLDs cover the spectral, cepstral, prosodic and voice quality information. 38 low-level descriptors (LLDs) are shown in Table <ref type="table" target="#tab_0">1</ref>.</p><p>To capture the valuable pattern of the depression, we try different strategies to segment the audio features. In our experiment, we use overlapping fixed length segments shifting forward at a rate of 1 second, while the size of the windows is 20 seconds, which can capture slow changing, long range characteristics.</p><p>Details for functionals can be found in <ref type="bibr" target="#b13">[14]</ref>. For the audio features, 42 functionals on 32 energy and spectral related lowlevel descriptors (LLD), 32 functionals on 6 voicing related LLD, 19 functionals on 6 delta coefficients of the voicing related LLD, and 10 voiced/unvoiced durational features, resulting in 2268 feature vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Median Robust Extended Local Binary Patterns (MRELBP)</head><p>The LBP operator characterizes the spatial structure of a local image patch by encoding the differences between the pixel value of the central point and those of its neighbors, considering only the signs to form a binary pattern. Formally, given  Voicing related ( <ref type="formula">6</ref>) F0 (sub-harmonic summation, followed by Viterbi smoothing), probability of voicing, jitter, shimmer (local), jitter (delta: "jitter of jitter"), logarithmic Harmonics-to-Noise Ratio (logHNR) a pixel x c in the image, the basic LBP response is calculated by comparing its value with those of its P neighboring pixels {x R,P,n } P-1 n=0 , evenly distributed on a circle of radius R centered on x c : We describe our methods in detail below.</p><formula xml:id="formula_0">LBP R,P (x c ) = P-1 n=0 s(x R,P,n -x c )2 n s(x) =              1, if x ≥ 0 0, if x &lt; 0<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Deep learned audio features</head><p>For the deep-learned audio features, we feed the frame-level raw waveform into the first CNN convolutional layer to learn a filter-bank representation which was equivalent to filter kernels in a time-frequency representation. In this method, if the raw waveform is filtered by the first strided convolutional layer, the output feature map will have the same as the spectrogram.</p><p>Specifically, the parameters of the first convolutional layer (i.e., stride, filter length, and number of filters) corresponded to the parameters of spectrogram (i.e., mel-size, window size, and number of mel-bands, respectively).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Deep learned texture features</head><p>In this section, we describe the details for the deep-learned texture features, which were different from the deep-learned audio features.</p><p>Even original images (1), flipped images (1), rotated images with six angles, and their flipped versions <ref type="bibr" target="#b11">(12)</ref>. In total, we obtained 42 times more data samples to train the model. After the above augmentation process, this makes the model robust for learning a lot of parameters of the input images.</p><p>The DCNN architecture used in our work has been proved</p><p>effective to perform well on other tasks such as object recognition, action recognition, etc. It repeatedly adopts convolutional layers with 64 filters followed by max-pooling layers, inspired by <ref type="bibr" target="#b30">[31]</ref>. The architecture is illustrated in Figure . 2. To improve the computational efficiency and boost the recognition accuracy, we resize the spectrogram image into 128×128.</p><p>In the following, we describe the CNN architecture with parameters. In our work, the input image size is 128×128. The convolutional layers have trainable filters (feature maps), which were applied across the entire image. The definition of the layers consisted of the filter size and stride, which was the distance between the applications of the filters. If the stride size is smaller than the filter size, the overlapped windows can be adopted for the filters. To learn the optimal hyperparameters, we conducted several experiments and obtained a filter size of 5×5 with stride size of 1.</p><p>For the pooling layers, we aimed to implement a non-linear down sampling function for dimensionality reduction and thus achieve translation invariance. In our study, we used different kernels and strides to carry out experiment. We found that the window size 2×2 with stride 2 get the best performance. Similar to <ref type="bibr" target="#b30">[31]</ref>, the fully-connected layers connect, all the neurons of one layer with that of the next one. In our work, depression severity measurement is considered as a regression problem from the point of machine learning view. Therefore, Euclidean loss was used as the loss function of the network, which was suitable for regression. Mathematically, the Euclidean loss function E computes the sum of squared differences of its two inputs, which can be written as:</p><formula xml:id="formula_1">E = 1 2N N i=1 ŷi -y i 2 (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where N is the number of samples, ŷi denotes the output from the network, and y i represents the ground truth (BDI-II score).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Joint Fine-Tuning Method</head><p>In our approach, the Raw-DCNN and the Spectrogram-DCNN are able to predict BDI-II scores separately. To capture the complementary information within the two used models, we propose joint fine-tuning method to boost the recognition performance. Specifically, four fully connected layers are concatenated as feature layers in both the raw and spectrogram networks. Euclidean loss function is still used for regression in our task. In the training process, the four DCNNs are trained separately, and then the joint fine-tuning is created using the architecture with joint tuning layers, as shown in Figure . 1.</p><p>Meanwhile, the dropout method is adopted for reducing overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Evaluation</head><p>In this section, the datasets used for the experiments are introduced firstly in Section 4.1 and Section 4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">AVEC2014 Depression Database</head><p>AVEC2014 corpus <ref type="bibr" target="#b14">[15]</ref> was a subset of the AVEC2013 corpus. The AVEC2014 corpus consisted of recordings of 2 different human-computer interaction tasks. Each of the tasks was supplied as separate recordings. In total, the corpus includes 300 videos with the duration ranging from 6 seconds to 4 minutes. The two tasks included a reading task and a spontaneous speech task, which are described below:</p><p>• Northwind -Participants read aloud an excerpt of the fable "Die Sonne und der Wind" (The North Wind and the Sun).</p><p>(German).</p><p>• Freeform -Participants respond to one of a number of questions such as: "What is your favorite dish?" or discuss a sad childhood memory (German).  <ref type="table">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experimental Setup and Evaluation Measures</head><p>In this sub-section, we describe the experimental setup and evaluation measures in detail.  encoding scheme <ref type="bibr" target="#b56">[57]</ref> for LBP and MRELBP. For MRELBP, the authors <ref type="bibr" target="#b55">[56]</ref> have proved that the uniform encoding scheme can obtain the striking texture classification accuracy.</p><p>For the DCNN architecture, the networks are trained with stochastic gradient using caffe deep learning toolbox <ref type="bibr" target="#b57">[58]</ref> with a batch size 32. For both of the Raw-DCNN and Spectrogram-DCNN, the training procedure starts from scratch. Euclidean loss is considered as the loss function for regression. The number of iterations for Raw model and Spectrogram model were set to 200,000 and 400,000, respectively. The parameters of the two deep networks are selected by experience and followed recommendation in another works <ref type="bibr" target="#b58">[59]</ref>. The learning rate was set to 10 -3 reduced by polynomial with gamma equals to 0.5. The momentum was set to 0.9 with weight decay equals to 0.0002.</p><p>The shape of the raw audio input is 265216. All experiments were conducted using NVIDIA Quadro K2200 with 4G memory.</p><p>In our experiments, the joint tuning layers are designed as two fully connected layers with 512 and 256 hidden units, respectively. To use the advantage of the deep-learned model and hand-crafted model, we proposed an integration method for Raw-DCNN and Spectrogram-DCNN using a joint fine-tuning method, which achieves better results than the two models. In the joint fine-tuning procedure, we retrained the top layers, and freezed other layers of the two trained networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Evaluation Metric</head><p>The depression severity recognition performance is assessed in terms of mean absolute error (MAE) and root mean square error (RMSE) between the prediction and reported BDI-II values.</p><p>The MAE was computed as:</p><formula xml:id="formula_3">MAE = 1 N N i=1 |y i -ỹi |<label>(3)</label></formula><p>And the RMSE was computed as:</p><formula xml:id="formula_4">RMS E = 1 N N i=1 (y i -ỹi ) 2<label>(4)</label></formula><p>where N denotes the number of data samples, y i is the ground truth and ỹi represents the predicted value of i-th sample, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Experimental Results</head><p>In the following, we first compare the performance of LBP feature with the MRELBP feature. Then, we compare the performance of hand-crafted features with that of deep-learned features for depression scale prediction. Finally, we compare our results to the ones from other state-of-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1.">Performance of single models</head><p>The performances of depression recognition on AVEC2013 and AVEC2014 databases are shown in Table <ref type="table" target="#tab_1">3</ref> and<ref type="table" target="#tab_2">4</ref>, respectively. In our work, we first described the results using single models without any joint tuning procedure. Table <ref type="table" target="#tab_1">3</ref> shown that the deep-learned features obtained the better results with   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3.">Overall performance by Joint Tuning</head><p>In our research, we also conducted the experiments by using joint tuning method. The results are shown in the last row in Tables <ref type="table" target="#tab_6">5</ref> and<ref type="table" target="#tab_4">6</ref> on AVEC2013 and AVEC2014, respectively. The results implied that the proposed joint tuning method performance was improved when employing both the handcrafted and deep learned models. During the training process, the performance is measured by Euclidean loss in the joint fine-tuning process. Fig. <ref type="figure" target="#fig_11">3</ref> shows convergences of the MSE loss during joint fine-tuning for final depression scale prediction.</p><p>In addition, we also combine the AVEC2013 and AVEC2014 as a single database to predict the depression scale. As shown in Table <ref type="table" target="#tab_5">7</ref>, the results after combining the two databases are the MAE of 8.1901 and the RMSE of 9.8874. A potential reason for this is: the new enlarged database has more data samples for training and the DCNN models can better predict the depression scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.4.">Comparison with Previous Works</head><p>In Table <ref type="table" target="#tab_7">8</ref> and Table <ref type="table" target="#tab_8">9</ref>, we compare our depression recognition results. Using the proposed approach, we combined hand-crafted features and deep-learned features, to state-of-theart results using other audio features, for the AVEC2013 and AVEC2014 databases, respectively. The indicated results were similar to those reported by previously cited studies. We should note here that these results have also been obtained on the combined dataset of Freeform and Northwind. As shown in Table 8 and 9, our approach provided better results than most of the state-of-the-art research that has been conducted. To make a fair comparison with other works, we use the training, validation, and test set provided by the database providers. The new augmented samples are generated from the original training, development and testing set. In our work, we applied the same data augmentation approach on the three datasets.</p><p>As shown in Table <ref type="table" target="#tab_7">8</ref>, it is clearly demonstrated that the proposed method outperforms all the other methods but one (Williamson et al. <ref type="bibr" target="#b12">[13]</ref>). In <ref type="bibr" target="#b12">[13]</ref>, the authors adopted a feature space to capture useful information based on the eigenvalue spectra coordination features and combined them with a feature set involving average phonetic durations, i.e., phoneticbased speaking rates. While in Table <ref type="table" target="#tab_7">8</ref>, the proposed method surpasses all the methods except one (Kachele et al. <ref type="bibr" target="#b44">[45]</ref>). In In Figure <ref type="figure" target="#fig_13">4</ref>, we report our results on the AVEC2014 dataset compared to reported state-of-the-art results using both audio (A) and video (V) features. As they can be seen using only audio features, our methods provided comparable results to multimodal approaches for depression recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Works</head><p>Depression is a serious psychological disorder. Computer aided technologies have been investigated to assist psychologists in the assessment of depression levels. To improve the accuracy of automatic depression recognition from speech signals, we proposed a new method based on deep learning and traditional method, which we employed to overcome the difficulties caused by designing hand-crafted features for depression recognition. In the proposed method, we use the raw and  rior performance compared with other audio-based methods for depression recognition. In our future work, we will explore more powerful regression models further improve the racy of depression</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Recently, deep learning has been successfully applied to various communities. Both theories and experiments have shown that deep learning can learn a lot of valuable information from the audiovisual signals. The deep learning method has several variants, such as single Layer Learning models, Probabilistic Models, Auto-Encoders and Convolutional Neural Networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>, researchers have used audio baseline features extracted by using the freely available open-source Emotion and Affect Recognition (openEAR) [26] toolkit's feature extraction backend openS-MILE [21]. The audio feature set consists of 2268 features, including 32 energy and spectral related low-level descriptors (LLD) x 42 functionals, 6 voicing related LLD x 32 functionals, 32 delta coefficients of the energy/spectral LLD x 19 functionals, 6 delta coefficients of the voicing related LLD x 19 functionals, and 10 voiced/unvoiced durational features. In order to capture the dynamic, long-range characteristics, the authors segment the audio clips with fixed length segments (3 seconds), which shift at one second. Finally, Support Vector Regression(SVR) is used for learning and predicting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>tween vocal prosody and change in depression severity over time. They presented three hypotheses: 1) Naive listeners can distinguish the depressed participants and health controls from vocal recordings; 2) The quantitative features of vocal prosody can capture changes from the diagnosis of the depression; 3) Interpersonal relationships can also occurred in the severity of depression estimation procedure. Finally, they validated the hypotheses by experiments. The results showed that the analysis of vocal prosody is a valuable tool for depression analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Feature</head><label></label><figDesc>design or feature extraction plays an important role in depression analysis tasks. In this work we combine handcrafted features with deep-learned features for estimating the severity of depression. First, for hand-crafted features, we extract the Low Level Descriptors (LLD) from the raw audio clips and Median Robust extended Local Binary Patterns (MRELBP) features from the spectrograms of audio. Second, we use DCNN to directly learn the deep-learned features from the raw audio and spectrogram images. Finally, we describe the proposed joint fine-tuning method to combine the four streams for the final depression prediction. The proposed framework for automatic depression recognition is given in Figure. 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of the proposed method for depression recognition using deep neural networks. The Raw-DCNN (Top) takes raw audio signals and low level descriptors (LLD) as input, while the Spectrogram-DCNN (Bottom) uses texture features as input. The red box in Figure 1 is Hand-Crafted features. Other two arrows are Deep-Learned features. The predicted depression score is computed by aggregating or averaging the individual predictions per frame from four DCNNs.</figDesc><graphic coords="6,79.24,193.10,123.61,70.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Recently,</head><label></label><figDesc>Liu et al. [56]  proposed the Median Robust Extended Local Binary Pattern (MRELBP), where the individual pixel intensities in Eq. 1 were replaced by a median filter response φ() to maximize the robustness of the representation to noise. Different from the traditional LBP, MRELBP compared regional image medians rather than raw image intensities, which can capture both microstructure and macrostructure texture information. For a more detail understanding of the methods, we refer the reader to<ref type="bibr" target="#b55">[56]</ref>.3.2. Deep Learning based Features ExtractionAs DCNN has shown its advantages in learning the patterns of feature, we adopt it to learn the valuable characteristic information implied in the audiovisual signals. Our deep-learned features are extracted by using two different models. The first deep network extracts deep-learned audio features from framelevel raw waveforms, while the other deep network model di-rectly learns feature representations from spectrogram images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>as a commonly used neural network technology, CNN has its own limitations. First, CNN can not process the highresolution images. Second, it requires a lot of samples for training. Specifically, CNN can learn a large number of parameters through the training procedure based on the amount of the training data. To overcome the limitations mentioned above, we first segment the audio clips with different size. Several authors studied the appropriate length of segments for extracting reliable audio features. In<ref type="bibr" target="#b13">[14]</ref>,<ref type="bibr" target="#b14">[15]</ref> the authors proposed 20s to capture slow changing and long range characteristics. For the extraction of vocal patterns using CNN, we first conducted experiments using segment of 6s, and 20s. Then we proposed a data augmentation method to tackle with the small samples of the training data. First, ∆ and ∆∆ features were extracted from the frequency domain of spectrograms. Second, following the above-mentioned step, the whole spectrograms image sequences were horizontally flipped. We rotated each image by each angle in -15 • ,-10 • ,-5 • , 5 • , 10 • , 15 • . Finally, we receptively obtained 14 times more data than the original images, ∆, ∆∆:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>2 .</head><label>2</label><figDesc>In Section 4.3, we briefly detail the experimental setup. Finally, the experimental results are provided in Section 4.4. 4.1. AVEC2013 Depression Database AVEC2013 [14] used a subset of the audio-visual depressive language corpus (AVDLC), which included 340 video clips of 292 subjects, performing a Human-Computer Interaction task while being recorded by a webcam and a microphone. The AVEC2013 database consist of 14 different tasks which were Power Point guided: e.g., sustained vowel phonation, sustained loud vowel phonation, and sustained smiling vowel phonation; speaking out loud while solving a task; Counting from 1 to 10, etc. The subjects were recorded between one and four times, with a period of two weeks between the measurements. The average age was 31.5 years with a range of 18 to 63 years. The length of each recording varied from 20 to 50 minutes, with an average duration of 25 minutes per recording. The 16-bit audio was recorded at a sampling rate of 41KHz. The videos, with frames of 640×480 pixels and 24-bits per pixel, were recorded at 30 frames per second. For the depression sub-challenge, there were 150 videos from 82 subjects. The recordings were split into three partitions: a training, development, and test set of 50 recordings each. The depression levels were labeled per clip using Beck Depression Inventory-II (BDI-II). Final BDI-II scores range from 0-63 (0-13 no or minimal depression; 14-19 mild depression; 20 -28 moderate depression; 29 -63 severe depression).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Each recording was labeled with BDI-II severity of depression. For AVEC 2014 depression sub-challenge, every task was split into three partitions: a training, development, and test set of 50 recordings each. In our experiments, we combined the training sets of the Northwind dataset and Freeform dataset to train the models, the development sets to verify the performance, and the test sets to test the models. Therefore, the training, development, and test set have 100 recordings, respectively (Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The deep Convolutional Neural Network architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The training MSE loss decreases and converges during joint finetuning for final depression scale prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>[ 45 ]</head><label>45</label><figDesc>, Kachele et al. propose an approach based on abstract meta information about individual subjects and also prototypical task and label dependent templates to infer the respective emotional states. They obtained better results in the depression challenge task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: AVEC2014 -Comparison with techniques of depression recognition using audio (A) and visual (V) features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note><p>38 low-level descriptors Energy&amp;Spectral (32) loudness (auditory model based), zero crossing rate, energy in bands from 250-650Hz, 1 kHz-4 kHz, 25%, 50%, 75%, and 90% spectral roll-off points spectral flux, entropy, variance, skewness, kurtosis, psychoacousitc sharpness, harmonicity, flatness, MFCC 1-16</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Performance of hand-crafted and deep-learned features on the development set and test set of AVEC2013.</figDesc><table><row><cell>Partition</cell><cell>Methods</cell><cell></cell><cell>RMSE MAE</cell></row><row><cell></cell><cell></cell><cell>LBP</cell><cell>9.3507 7.7314</cell></row><row><cell></cell><cell>Hand Crafted Model</cell><cell cols="2">MRELBP 9.1673 7.5455</cell></row><row><cell>Dev.</cell><cell></cell><cell>LLD</cell><cell>9.3154 7.6502</cell></row><row><cell></cell><cell></cell><cell cols="2">Wavform 9.3896 7.8184</cell></row><row><cell></cell><cell>Deep Learned Model</cell><cell></cell></row><row><cell></cell><cell cols="3">Spectrogram 9.1129 7.5371</cell></row><row><cell></cell><cell></cell><cell>LBP</cell><cell>10.9312 9.2443</cell></row><row><cell></cell><cell>Hand Crafted Model</cell><cell cols="2">MRELBP 10.5611 8.6580</cell></row><row><cell>Test</cell><cell></cell><cell>LLD</cell><cell>10.6418 8.8935</cell></row><row><cell></cell><cell></cell><cell cols="2">Wavform 11.0983 9.4484</cell></row><row><cell></cell><cell>Deep Learned Model</cell><cell></cell></row><row><cell></cell><cell cols="3">Spectrogram 10.4561 8.4832</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Performance of hand-crafted and deep-learned features on the development set and test set of AVEC2014.</figDesc><table><row><cell>Partition</cell><cell>Methods</cell><cell></cell><cell>RMSE MAE</cell></row><row><cell></cell><cell></cell><cell>LBP</cell><cell>9.3478 7.5699</cell></row><row><cell></cell><cell>Hand Crafted Model</cell><cell cols="2">MRELBP 9.1523 7.5026</cell></row><row><cell>Dev.</cell><cell></cell><cell>LLD</cell><cell>9.3000 7.5514</cell></row><row><cell></cell><cell></cell><cell cols="2">Wavform 9.3770 7.8813</cell></row><row><cell></cell><cell>Deep Learned Model</cell><cell></cell></row><row><cell></cell><cell cols="3">Spectrogram 9.1100 7.4969</cell></row><row><cell></cell><cell></cell><cell>LBP</cell><cell>10.8211 8.7489</cell></row><row><cell></cell><cell>Hand Crafted Model</cell><cell cols="2">MRELBP 10.4618 8.6420</cell></row><row><cell>Test</cell><cell></cell><cell>LLD</cell><cell>10.5648 8.6800</cell></row><row><cell></cell><cell></cell><cell cols="2">Wavform 10.9014 8.7810</cell></row><row><cell></cell><cell>Deep Learned Model</cell><cell></cell></row><row><cell></cell><cell cols="3">Spectrogram 10.4413 8.6014</cell></row><row><cell cols="4">MAE 8.4832 and RMSE 10.4561 on the test set. In compar-</cell></row><row><cell cols="4">ison with the performance of AVEC2013, AVEC2014 obtains</cell></row><row><cell cols="4">better results. As shown in Table 4, the deep-learned features</cell></row><row><cell cols="4">also obtained the better performances with MAE 8.6014 and</cell></row><row><cell cols="4">RMSE 10.4413 on the test set. These results showed that the</cell></row><row><cell cols="4">deep learned model was important for depression severity pre-</cell></row><row><cell cols="4">diction, and the spectrogram DCNN can represent the charac-</cell></row><row><cell cols="4">teristics of depression. Moreover, the deep learned model could</cell></row><row><cell cols="4">reduce some effort to design and find the suitable hand-crafted</cell></row><row><cell cols="3">features for depression scale prediction.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Overall Performance on the development set and test set of AVEC2013.</figDesc><table><row><cell>Partition</cell><cell>Methods</cell><cell>RMSE MAE</cell></row><row><cell></cell><cell>Hand &amp; Deep (Ave.)</cell><cell>9.1001 7.4456</cell></row><row><cell>Dev.</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Hand &amp; Deep (Joint Tuning) 9.0000 7.4210</cell></row><row><cell></cell><cell>Hand &amp; Deep (Ave.)</cell><cell>10.2261 8.2323</cell></row><row><cell>Test</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Hand &amp; Deep (Joint Tuning) 10.0012 8.2012</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Overall Performance on the development set and test set of AVEC2014.</figDesc><table><row><cell>Partition</cell><cell>Methods</cell><cell>RMSE MAE</cell></row><row><cell></cell><cell>Hand &amp; Deep (Ave.)</cell><cell>9.0089 7.4213</cell></row><row><cell>Dev.</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Hand&amp;Deep (Joint Tuning) 9.0001 7.4211</cell></row><row><cell></cell><cell>Hand &amp; Deep (Ave.)</cell><cell>10.1284 8.2204</cell></row><row><cell>Test</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Hand &amp; Deep (Joint Tuning) 9.9998 8.1919</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Overall Performance on the development set and test set (AVEC2013+AVEC2014).</figDesc><table><row><cell>Partition</cell><cell>Methods</cell><cell>RMSE MAE</cell></row><row><cell></cell><cell>Hand &amp; Deep (Ave.)</cell><cell>8.9971 7.4200</cell></row><row><cell>Dev.</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Hand &amp; Deep (Joint Tuning) 8.8920 7.4118</cell></row><row><cell></cell><cell>Hand &amp; Deep (Ave.)</cell><cell>10.0009 8.2323</cell></row><row><cell>Test</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Hand &amp; Deep (Joint Tuning) 9.8874 8.1901</cell></row><row><cell cols="3">4.4.2. Overall performance by fusing the individual models</cell></row><row><cell cols="3">In our experiments, to capture the complementary informa-</cell></row><row><cell cols="3">tion with the deep-learned features using DCNN and the hand-</cell></row><row><cell cols="3">crafted features, we calculate the performance by fusing the</cell></row><row><cell cols="3">hand-crafted features and deep-learned features. The results</cell></row><row><cell cols="3">are shown in the first row in Table 5 and 6 for AVEC2013 and</cell></row><row><cell cols="3">AVEC2014, respectively. It can be seen from the table that,</cell></row><row><cell cols="3">when averaging is adopted, the RMSE and MAE obtained are</cell></row><row><cell cols="3">10.2261 and 8.2323 on the AVEC2013 database, respectively.</cell></row><row><cell cols="3">On AVEC2014, the MAE of 8.2204 and RMSE of 10.1284 are</cell></row><row><cell cols="3">obtained. The results showed that by fusing the hand-crafted</cell></row><row><cell cols="3">and the deep-learned model, the overall performance can be</cell></row></table><note><p>improved than adopting the single model which means the necessity of using both hand-crafted and deep-learned features for depression scale prediction.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc>illustrates that the results after joint tuning the models were MAE 8.2012 and RMSE 10.0012 on the AVEC2013 database. While on the AVEC2014 database, as shown in Table6, the best results were MAE of 8.1919, and RMSE of 9.9998.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>AVEC2013 -Comparison to state-of-the-art results. Note that the listed results use audio data only.</figDesc><table><row><cell>Partition</cell><cell>Methods</cell><cell>RMSE</cell><cell>MAE</cell></row><row><cell></cell><cell>Baseline [14]</cell><cell>10.75</cell><cell>8.66</cell></row><row><cell></cell><cell>Meng et al. [38]</cell><cell>8.82</cell><cell>7.09</cell></row><row><cell></cell><cell>Williamson et al. [13]</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>Dev.</cell><cell>Ours</cell><cell cols="2">9.0000 7.4210</cell></row><row><cell></cell><cell>Baseline [14]</cell><cell>14.12</cell><cell>10.35</cell></row><row><cell></cell><cell>Meng et al. [38]</cell><cell>11.19</cell><cell>9.14</cell></row><row><cell></cell><cell>Williamson et al. [13]</cell><cell>7.42</cell><cell>5.75</cell></row><row><cell>Test</cell><cell>Ours</cell><cell cols="2">10.0012 8.2012</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>AVEC2014 -comparison to state-of-the-art results. Note that the listed results use audio data only.</figDesc><table><row><cell>Partition</cell><cell>Methods</cell><cell>RMSE</cell><cell>MAE</cell></row><row><cell></cell><cell>Baseline [15]</cell><cell>11.52</cell><cell>8.93</cell></row><row><cell></cell><cell>Jain et al. [41]</cell><cell>11.51</cell><cell>9.75</cell></row><row><cell></cell><cell>Jan et al. [40]</cell><cell>10.69</cell><cell>8.92</cell></row><row><cell>Dev.</cell><cell>Senoussaoui et al. [46]</cell><cell>10.09</cell><cell>7.41</cell></row><row><cell></cell><cell>Parez et al. [43]</cell><cell>9.79</cell><cell>7.75</cell></row><row><cell></cell><cell>Kachele et al. [45]</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell></cell><cell>Mitra et al. [48]</cell><cell>7.71</cell><cell>6.10</cell></row><row><cell></cell><cell>Ours</cell><cell cols="2">9.0001 7.4211</cell></row><row><cell></cell><cell>Baseline [15]</cell><cell cols="2">12.567 10.036</cell></row><row><cell></cell><cell>Jain et al. [41]</cell><cell>10.25</cell><cell>8.40</cell></row><row><cell></cell><cell>Jan et al. [40]</cell><cell>11.30</cell><cell>9.10</cell></row><row><cell>Test</cell><cell>Senoussaoui et al. [46]</cell><cell>12.71</cell><cell>9.82</cell></row><row><cell></cell><cell>Parez et al. [43]</cell><cell>11.92</cell><cell>9.36</cell></row><row><cell></cell><cell>Kachele et al. [45]</cell><cell>9.18</cell><cell>7.10</cell></row><row><cell></cell><cell>Mitra et al. [48]</cell><cell>11.10</cell><cell>8.83</cell></row><row><cell></cell><cell>Ours</cell><cell cols="2">9.9998 8.1919</cell></row><row><cell cols="4">spectrogram DCNN to model the characteristic information of</cell></row><row><cell cols="4">depression. Moreover, we also proposed to adopt joint tun-</cell></row><row><cell cols="4">ing layers, to combine the raw and spectrogram DCNN, which</cell></row></table><note><p>can improve the performance of depression recognition. Experimental results on two depression dataset, AVEC2013 and AVEC2014, have demonstrated that our approach obtain supe-</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment This work is supported by the Shaanxi Provincial International Science and Technology Collaboration Project (grant 2017KW-ZD-14), the National Natural Science Foundation of China (grant 61273265), the VUB Interdisciplinary Research Program through the EMO-App project, and the program of China Scholarship Council (CSC) (No. 201606290171).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of interest</head><p>No potential conflict of interest relevant to this article was reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>*Graphical Abstract</head><p> A framework for automatic diagnosis of depression from speech is proposed.  The combination of complementary information between deep-learned features and hand-crafted features can effectively measure the depression severity.  Useful characteristic of depression can be learned by Deep Convolutional Neural Networks (DCNN) from speech.  Help to the clinician when designing features related to depression.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The global burden of disease: 2004 update</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mathers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Fat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Boerma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>World Health Organization</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Albrecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Herrick</surname></persName>
		</author>
		<title level="m">100 Questions &amp; Answers About Depression</title>
		<imprint>
			<publisher>Jones &amp; Bartlett Learning</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Voice acoustic measures of depression severity and treatment response collected via interactive voice response (ivr) technology</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Mundt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Cannizzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chappie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Geralts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neurolinguistics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="50" to="64" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A rating scale for depression</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neurology Neurosurgery and Psychiatry</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="56C" to="62" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Diagnostic and Statistical Manual of Mental Disorders</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bogduk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>American Psychiatric Association</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The 16-item quick inventory of depressive symptomatology (qids), clinician rating (qids-c), and selfreport (qids-sr): a psychometric evaluation in patients with chronic major depression</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Ibrahim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Psychiatry</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="573" to="583" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Comparison of beck depression inventories-ia and-ii in psychiatric outpatients</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Steer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Ranieri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality Assessment</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="588" to="597" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A new depression scale designed to be sensitive to change</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Montgomery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Asberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The British journal of psychiatry</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="382" to="389" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The phq-9: a new depression diagnostic and severity measure</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kroenke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Spitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychiatric annals</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="509" to="515" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The phq-8 as a measure of current depression in the general population</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kroenke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Strine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Spitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Mokdad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of affective disorders</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="163" to="173" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Influence of acoustic low-level descriptors in the detection of clinical depression in adolescents</title>
		<author>
			<persName><forename type="first">L.-S</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maddage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sheeber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics Speech and Signal Processing (ICASSP), 2010 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="5154" to="5157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">An investigation of depressed speech detection: Features and normalization</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Epps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Breakspear</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Interspeech</publisher>
			<biblScope unit="page" from="2997" to="3000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Vocal biomarkers of depression based on motor incoordination</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Quatieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Helfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Horwitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Mehta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd ACM international workshop on Audio/visual emotion challenge</title>
		<meeting>the 3rd ACM international workshop on Audio/visual emotion challenge</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Avec 2013: the continuous audio/visual emotion and depression recognition challenge</title>
		<author>
			<persName><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bilakhia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schnieder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd ACM international workshop on audio/visual emotion challenge</title>
		<meeting>the 3rd ACM international workshop on audio/visual emotion challenge</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Avec 2014: 3d dimensional affect and depression recognition challenge</title>
		<author>
			<persName><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Almaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krajewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge</title>
		<meeting>the 4th International Workshop on Audio/Visual Emotion Challenge</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Detecting depression severity from vocal prosody</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fairbairn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="142" to="150" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Evidence for the independent function of intonation contour type, voice quality, and f 0 range in signaling speaker affect</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Ladd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Silverman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tolkmitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="435" to="444" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Vocal affect expression: a review and a model for future research</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological bulletin</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">143</biblScope>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Vocal cues in emotion encoding and decoding</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Banse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Wallbott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Goldbeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Motivation and emotion</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="148" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Objectively measurable descriptors of speech</title>
		<author>
			<persName><forename type="first">B</forename><surname>Necioglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dept. Electr. Comp. Eng., Georgia Inst. Technol., Atlanta</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<pubPlace>GA</pubPlace>
		</imprint>
	</monogr>
	<note>Ph.D. thesis. Ph. D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recent developments in opensmile, the munich open-source multimedia feature extractor</title>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM international conference on Multimedia</title>
		<meeting>the 21st ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="835" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Covarepa collaborative voice analysis repository for speech technologies</title>
		<author>
			<persName><forename type="first">G</forename><surname>Degottex</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Drugman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Raitio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="960" to="964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Imai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tokuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Masuko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Koishida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sako</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<title level="m">Speech signal processing toolkit (sptk)</title>
		<imprint/>
	</monogr>
	<note>version 3.3 (2009</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The kaldi speech recognition toolkit, in: IEEE 2011 workshop on automatic speech recognition and understanding, no. EPFL-CONF-192584</title>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>IEEE Signal Processing Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Yaafe, an easy to use and efficient audio feature extraction software</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Essid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Prado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Richard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>ISMIR</publisher>
			<biblScope unit="page" from="441" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Openearintroducing the munich opensource emotion and affect recognition toolkit</title>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wöllmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Affective computing and intelligent interaction and workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
	<note>ACII 2009. 3rd international conference on</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks, in: Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning and transferring midlevel image representations using convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1717" to="1724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for endto-end speech recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="4845" to="4849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Forest species recognition using deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Hafemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cavalin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (ICPR), 2014 22nd International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1103" to="1107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Avec 2016: Depression, mood, and emotion recognition workshop and challenge</title>
		<author>
			<persName><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gratch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ringeval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lalanne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Torres</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Stratou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge</title>
		<meeting>the 6th International Workshop on Audio/Visual Emotion Challenge</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Avec 2017-real-life depression, and a ect recognition workshop and challenge</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ringeval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gratch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mozgai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Critical analysis of the impact of glottal features in the classification of clinical depression in speech</title>
		<author>
			<persName><forename type="first">I</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Elliot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Clements</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Peifer</surname></persName>
		</author>
		<author>
			<persName><surname>Weisser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on bio-medical engineering</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="96" to="107" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A review of depression and suicide risk assessment using speech analysis</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krajewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schnieder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Epps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Quatieri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="10" to="49" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Detecting depression from facial actions and vocal prosody</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Kruez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Padilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Affective Computing and Intelligent Interaction and Workshops</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Diagnosis of depression by behavioural signals: a multimodal approach</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sethu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Epps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd ACM international workshop on Audio/visual emotion challenge</title>
		<meeting>the 3rd ACM international workshop on Audio/visual emotion challenge</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Depression recognition based on dynamic facial and vocal expression features using partial least square regression</title>
		<author>
			<persName><forename type="first">H</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ai-Shuraifi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd ACM international workshop on Audio/visual emotion challenge</title>
		<meeting>the 3rd ACM international workshop on Audio/visual emotion challenge</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="21" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Audiovisual three-level fusion for continuous estimation of russell&apos;s emotion circumplex</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sánchez-Lozano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lopez-Otero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Docio-Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Argones-Rúa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Alba-Castro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd ACM international workshop on Audio/visual emotion challenge</title>
		<meeting>the 3rd ACM international workshop on Audio/visual emotion challenge</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="31" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Automatic depression scale prediction using facial expression dynamics and regression</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">F A</forename><surname>Gaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Turabzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge</title>
		<meeting>the 4th International Workshop on Audio/Visual Emotion Challenge</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="73" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Depression estimation using audiovisual features and fisher vector encoding</title>
		<author>
			<persName><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge</title>
		<meeting>the 4th International Workshop on Audio/Visual Emotion Challenge</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="87" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Emotion recognition and depression diagnosis by acoustic and visual features: A multimodal approach</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sidorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Minker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge</title>
		<meeting>the 4th International Workshop on Audio/Visual Emotion Challenge</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="81" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fusing affective dimensions and audio-visual features from segmented video for depression recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Villasenor-Pineda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Et</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge</title>
		<meeting>the 4th International Workshop on Audio/Visual Emotion Challenge</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="49" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Predicting affective dimensions based on self assessed depression severity</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>INTERSPEECH</publisher>
			<biblScope unit="page" from="1427" to="1431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Inferring depression and affect from application dependent meta knowledge</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kächele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schwenker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge</title>
		<meeting>the 4th International Workshop on Audio/Visual Emotion Challenge</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Model fusion for multimodal depression classification and level detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Senoussaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sarria-Paja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Falk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge</title>
		<meeting>the 4th International Workshop on Audio/Visual Emotion Challenge</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="57" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multimodal prediction of affective dimensions and depression in human-computer interactions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Malandrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Segbroeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Potamianos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge</title>
		<meeting>the 4th International Workshop on Audio/Visual Emotion Challenge</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The sri avec-2014 evaluation system</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shriberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mclaren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kathol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Richey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vergyri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Graciarena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge</title>
		<meeting>the 4th International Workshop on Audio/Visual Emotion Challenge</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="93" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Vocal and facial biomarkers of depression based on motor incoordination and timing</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Quatieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Helfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ciccarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Mehta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge</title>
		<meeting>the 4th International Workshop on Audio/Visual Emotion Challenge</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Decision tree based depression classification from audio video and language information</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Oveneke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sahli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge</title>
		<meeting>the 6th International Workshop on Audio/Visual Emotion Challenge</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Depaudionet: An efficient deep model for audio based depression classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge</title>
		<meeting>the 6th International Workshop on Audio/Visual Emotion Challenge</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="35" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Depression assessment by fusing high and low level features from audio, video, and text</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pampouchidou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Simantiraki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fazlollahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pediaditis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Manousos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roniotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Giannakakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Meriaudeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Simos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Marias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge</title>
		<meeting>the 6th International Workshop on Audio/Visual Emotion Challenge</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Detecting depression using vocal, facial and semantic communication cues</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Godoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schwarzentruber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Khorrami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-T</forename><surname>Kung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dagli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Quatieri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge</title>
		<meeting>the 6th International Workshop on Audio/Visual Emotion Challenge</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="11" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Georgiou, Multimodal and multiresolution depression detection from speech and facial landmark features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nasir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Shivakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nallan Chakravarthula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge</title>
		<meeting>the 6th International Workshop on Audio/Visual Emotion Challenge</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Staircase regression in oa rvm, data selection and gender dependency in avec</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Stasak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Wataraka</forename><surname>Gamage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sethu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Epps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge</title>
		<meeting>the 6th International Workshop on Audio/Visual Emotion Challenge</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="19" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Median robust extended local binary pattern for texture classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Fieguth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1368" to="1381" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Dynamic texture recognition using local binary patterns with an application to facial expressions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Automated depression diagnosis based on deep networks to encode facial appearance and dynamics</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
