<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast Extra Gradient Methods for Smooth Structured Nonconvex-Nonconcave Minimax Problems</title>
				<funder ref="#_AgnmZrw">
					<orgName type="full">Samsung Science and Technology Foundation</orgName>
				</funder>
				<funder ref="#_GTqeGz9">
					<orgName type="full">Korea government (MSIT)</orgName>
				</funder>
				<funder>
					<orgName type="full">National Research Foundation of Korea</orgName>
					<orgName type="abbreviated">NRF</orgName>
				</funder>
				<funder>
					<orgName type="full">POSCO Science Fellowship of POSCO TJ Park Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sucheol</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematical Sciences KAIST</orgName>
								<address>
									<settlement>Daejeon</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Donghwan</forename><surname>Kim</surname></persName>
							<email>donghwankim@kaist.ac.kr</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Mathematical Sciences KAIST</orgName>
								<address>
									<settlement>Daejeon</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fast Extra Gradient Methods for Smooth Structured Nonconvex-Nonconcave Minimax Problems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modern minimax problems, such as generative adversarial network and adversarial training, are often under a nonconvex-nonconcave setting, and developing an efficient method for such setting is of interest. Recently, two variants of the extragradient (EG) method are studied in that direction. First, a two-time-scale variant of the EG, named EG+, was proposed under a smooth structured nonconvexnonconcave setting, with a slow O(1/k) rate on the squared gradient norm, where k denotes the number of iterations. Second, another variant of EG with an anchoring technique, named extra anchored gradient (EAG), was studied under a smooth convex-concave setting, yielding a fast O(1/k 2 ) rate on the squared gradient norm. Built upon EG+ and EAG, this paper proposes a two-time-scale EG with anchoring, named fast extragradient (FEG), that has a fast O(1/k 2 ) rate on the squared gradient norm for smooth structured nonconvex-nonconcave problems; the corresponding saddle-gradient operator satisfies the negative comonotonicity condition. This paper further develops its backtracking line-search version, named FEG-A, for the case where the problem parameters are not available. The stochastic analysis of FEG is also provided.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, nonconvex-nonconcave minimax problems have received an increased attention in the optimization community and the machine learning community due to their applications to generative adversarial network <ref type="bibr" target="#b9">[10]</ref> and adversarial training <ref type="bibr" target="#b26">[27]</ref>. In this paper, we consider a smooth structured nonconvex-nonconcave minimax problem:</p><formula xml:id="formula_0">min x?R dx max y?R dy f (x, y),<label>(1)</label></formula><p>where f : R dx ? R dy ? R is smooth and is possibly nonconvex in x for fixed y, and possibly nonconcave in y for fixed x; the saddle-gradient operator F := (? x f, -? y f ) satisfies the negative comonotonicity <ref type="bibr" target="#b0">[1]</ref>. We construct an efficient (first-order) method, using a saddle gradient operator F for finding a first-order stationary point of the problem <ref type="bibr" target="#b0">(1)</ref>.</p><p>So far little is known under the nonconvex-nonconcave setting, compared to the convex-concave setting. Recent works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44]</ref> studied extragradient-type methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b38">39]</ref> for minimax problems under various structured nonconvex-nonconcave settings. In other words, they consider various non-monotone conditions on F , such as the Minty variational inequality (MVI) condition <ref type="bibr" target="#b3">[4]</ref>, the weak MVI condition <ref type="bibr" target="#b6">[7]</ref>, and the negative comonotonicity <ref type="bibr" target="#b0">[1]</ref>. 1 Among them, this 35th Conference on Neural Information Processing Systems (NeurIPS 2021).</p><p>paper focuses on the negative comonotonicity condition for a Lipschitz continuous F . To the best of our knowledge, the following two-time-scale variant of the extragradient method, named EG+:</p><formula xml:id="formula_1">z k+1/2 = z k - ? k ? F z k , z k+1 = z k -? k F z k+1/2 , (EG+)</formula><p>is the only known (explicit)<ref type="foot" target="#foot_0">2</ref> method, using F , that converges under the considered setting<ref type="foot" target="#foot_1">3</ref>  <ref type="bibr" target="#b6">[7]</ref>, where z k := (x k , y k ). The EG+, however, has a slow O(1/k) rate on the squared gradient norm. Note that a similar two-time-scale approach has been found to stabilize the stochastic extragradient method with unbounded noise variance <ref type="bibr" target="#b13">[14]</ref>.</p><p>Meanwhile, under the smooth convex-concave setting, recent works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b42">43]</ref> suggest that Halpern-type <ref type="bibr" target="#b11">[12]</ref> (or anchoring) methods, performing a convex combination of an initial point z 0 and the last updated point z k at each iteration, has a fast O(1/k 2 ) rate in terms of the squared gradient norm. In particular, <ref type="bibr" target="#b42">[43]</ref> developed the following anchoring variant of the extragradient method, named extra anchored gradient (EAG):</p><formula xml:id="formula_2">z k+1/2 = z k + ? k (z 0 -z k ) -? k F z k , z k+1 = z k + ? k (z 0 -z k ) -? k F z k+1/2 . (EAG)</formula><p>This is the first (explicit) method with a fast O(1/k 2 ) rate on the squared gradient norm, when F satisfies both the Lipschitz continuity and the monotonicity. <ref type="bibr" target="#b42">[43]</ref> also showed that such O(1/k 2 ) rate is optimal for first-order methods using a Lipschitz continuous and monotone F .</p><p>Built upon both EG+ and EAG, this paper studies the following class of two-time-scale anchored extragradient methods, named fast extragradient (FEG):</p><formula xml:id="formula_3">z k+1/2 = z k + ? k (z 0 -z k ) -(1 -? k )(? k + 2? k )F z k , z k+1 = z k + ? k (z 0 -z k ) -? k F z k+1/2 -(1 -? k )2? k F z k . (Class FEG)</formula><p>Note that (Class FEG) reuses the F z k term in the z k+1 update, unlike the standard extragradienttype methods, which we found essential for handling the negative comonotonicity condition. We leave further understanding the use of F z k and the formulation of (Class FEG) as future work. The proposed FEG method (with appropriately chosen step coefficients ? k , ? k and ? k discussed later) has an O(1/k 2 ) rate on the squared gradient norm, under the Lipschitz continuity and the negative comonotonicity conditions on F . To the best of our knowledge, this is the first accelerated method under the nonconvex-nonconcave setting. The FEG also has value under the smooth convex-concave setting. First, when F is Lipschitz continuous and monotone, the rate bound of FEG is about 27/4 times smaller than that of EAG. Also note that the rate bound of FEG is only about four times larger than the O(1/k 2 ) lower complexity bound of first-order methods under such setting <ref type="bibr" target="#b42">[43]</ref>, further closing the gap between the lower and upper complexity bounds. Second, when F is cocoercive, FEG has a rate faster than that of a version of Halpern iteration <ref type="bibr" target="#b11">[12]</ref> in <ref type="bibr" target="#b5">[6]</ref>.</p><p>We also develop an adaptive variant of FEG, named FEG-A, which updates its parameters, ? k and ? k in (Class FEG), adaptively using a backtracking line-search <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b30">31]</ref>. FEG requires the knowledge of the two problem parameters for the Lipschitz continuity and the comonotonicity of F . However, those global parameters can be conservative, and in practice, they are even usually unknown. For such cases, the FEG-A adaptively and locally estimates the problem parameters, while preserving the fast rate O(1/k 2 ) on the squared gradient norm for smooth structured nonconvex-nonconcave minimax problems.</p><p>Lastly, we study a stochastic version of FEG, named S-FEG, which uses an unbiased stochastic estimate of F z, i.e., F z = F z + ?, instead of F z in FEG, where ? denotes a stochastic noise. For a Lipschitz continuous and monotone F , we provide a convergence analysis in terms of the expected squared gradient norm. In specific, we show that the S-FEG is stable with a rate O(1/k 2 ) + O( ), when the noise variance decreases in the order of O( /k), while being unstable otherwise due to error accumulation. This is similar to the convergence behavior of a stochastic version of Nesterov's fast gradient method <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>, observed in <ref type="bibr" target="#b4">[5]</ref>, for smooth convex minimization.</p><p>Our main contributions are summarized as follows.</p><p>? We propose the FEG method that has an accelerated convergence rate O(1/k 2 ) on the squared gradient norm for smooth structured nonconvex-nonconcave minimax problems. ? We present that the FEG method has a rate faster than those of the EAG and the Halpern iteration for smooth convex-concave problems. ? We construct a backtracking line-search version of FEG, named FEG-A, for the case where the Lipschitz constant and comonotonicity parameters of F are unavailable.</p><p>? We analyze a stochastic version of FEG, named S-FEG, for smooth convex-concave problems.</p><p>2 Related work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Methods for convex-concave minimax problems</head><p>The extragradient method <ref type="bibr" target="#b18">[19]</ref> is one of the widely used methods for solving smooth convex-concave minimax problems (see, e.g., <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44]</ref> for its extensions and applications). In terms of the duality gap, max y ?Y f (x, y ) -min x ?X f (x , y), where X and Y are compact <ref type="foot" target="#foot_2">4</ref> domains, the ergodic iterate of the extragradient-type methods <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b36">37]</ref> have an O(1/k) rate. Such O(1/k) rate on the duality gap is order-optimal for the first-order methods <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b37">38]</ref>, leaving no room for improvement. On the other hand, the last iterate of the extragradient method has a slower O(1/ ? k) rate on the duality gap, under an additional assumption that F has a Lipschitz derivative <ref type="bibr" target="#b8">[9]</ref>. In terms of the squared gradient norm, F z 2 , the best iterate of the extragradient-type methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b38">39]</ref> have an O(1/k) rate <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43]</ref>. The last iterate of the extragradient method also has a rate O(1/k), when F is further assumed to have a Lipschitz derivative <ref type="bibr" target="#b8">[9]</ref>. Unlike the duality gap, the O(1/k) rate on the squared gradient norm is not optimal <ref type="bibr" target="#b42">[43]</ref>. From now on throughout this paper, we mainly study and compare the convergence rates on the squared gradient norm, which still has room for improvement in convex-concave problems, and has meaning for nonconvex-nonconcave minimax problems, unlike the duality gap.</p><p>Recently, <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b42">43]</ref> found that Halpern-type <ref type="bibr" target="#b11">[12]</ref> (or anchoring) methods yield a fast O(1/k 2 ) rate in terms of the squared gradient norm for minimax problems. <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21]</ref> showed that the (implicit) Halpern iteration <ref type="bibr" target="#b11">[12]</ref> with appropriately chosen step coefficients has an O(1/k 2 ) rate on the squared norm of a monotone F . Then, for a cocoercive F , an (explicit) version of the Halpern iteration was studied in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17]</ref> that has the same fast rate. In addition, <ref type="bibr" target="#b5">[6]</ref> constructed a double-loop version of the Halpern iteration for a Lipschitz continuous and monotone F , which has a rate ?(1/k 2 ) on the squared gradient norm, slower than the rate O(1/k 2 ). While this is promising compared to the O(1/k) rate of the extragradient methods on the squared gradient norm <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43]</ref>, the computational complexity due to its double-loop nature and a relatively slow rate remained a problem. Very recently, <ref type="bibr" target="#b42">[43]</ref> proposed the extra anchored gradient (EAG) method, which is the first (explicit) method with a fast O(1/k 2 ) rate for smooth convex-concave minimax problems, i.e., for Lipschitz continuous and monotone operators. In addition, <ref type="bibr" target="#b42">[43]</ref> proved that the EAG is order-optimal by showing that the lower complexity bound of first-order methods is ?(1/k 2 ).</p><formula xml:id="formula_4">Cocoercive ? Monotone ? Negative comonotone ? ? MVI ? Weak MVI</formula><p>Figure <ref type="figure">1</ref>: Relations between the conditions on F .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Methods for nonconvex-nonconcave minimax problems</head><p>Some recent literature considered relaxing the monotonicity condition of the saddle gradient operator to tackle modern nonconvex-nonconcave minimax problems. For example, the Minty variational inequality (MVI) condition, i.e., there exists <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24]</ref>. This condition is also studied under the name, the coherence, in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44]</ref>. Moreover, <ref type="bibr" target="#b6">[7]</ref> considered a weaker condition, named the weak MVI condition, i.e., for some ? &lt; 0, there exists z * ? Z * (F ) satisfying</p><formula xml:id="formula_5">z * ? Z * (F ) satisfying F z, z -z * ? 0 for all z ? R d where Z * (F ) := {z * ? R d : F z * = 0}, is studied in</formula><formula xml:id="formula_6">F z, z -z * ? ? F z 2 for all z ? R d .</formula><p>The weak MVI condition is implied by the negative comonotonicity <ref type="bibr" target="#b0">[1]</ref> or, equivalently, the (positive) cohypomonotonicity <ref type="bibr" target="#b2">[3]</ref>. The comonotonicity will be further discussed in the upcoming section.</p><p>For L-Lipschitz continuous F , <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b41">42]</ref> showed that the extragradient-type methods have an O(1/k) rate on the squared gradient norm under the MVI condition, and <ref type="bibr" target="#b6">[7]</ref> developed the (EG+) method under the weak MVI condition (and thus under the negative comonotonicty), which also has an O(1/k) rate on the squared gradient norm. To the best of our knowledge, there is no known accelerated method for the nonconvex-nonconcave setting; our proposed FEG method is the first method to have a fast O(1/k 2 ) rate under the nonconvex-nonconcave setting. The convergence rates of the existing methods and the FEG on the squared gradient norm are summarized in Table <ref type="table" target="#tab_0">1</ref>. </p><formula xml:id="formula_7">O(1/k) O(1/k) O(1/k) EG+ [7] O(1/k) O(1/k) O(1/k) O(1/k) O(1/k) Accelerated Halpern [12, 6] O(1/k 2 ) ?(1/k 2 ) EAG [43] O(1/k 2 ) O(1/k 2 ) FEG (this paper) O(1/k 2 ) O(1/k 2 ) O(1/k 2 )</formula><p>3 Preliminaries</p><p>The followings are the two main assumptions for the saddle gradient operator F of the smooth structured nonconvex-nonconcave problem <ref type="bibr" target="#b0">(1)</ref>. Under such assumptions, we develop efficient methods that find a first-order stationary point</p><formula xml:id="formula_8">z * ? Z * (F ) where Z * (F ) := {z * ? R d : F z * = 0}.</formula><p>Assumption 1 (L-Lipschitz continuity). For some L ? (0, ?), F satisfies</p><formula xml:id="formula_9">F z -F z ? L z -z , ?z, z ? R d . Assumption 2 (?-Comonotonicity). For some ? ? -1 2L , ? , F satisfies F z -F z , z -z ? ? F z -F z 2 , ?z, z ? R d .</formula><p>The ?-comonotonicity consists of three cases depending on the choice of ?; the negative comonotonicity when ? &lt; 0, the monotonicity when ? = 0, and the cocoercivity when ? &gt; 0. The negative comonotonicity is weaker than the other two, and is the main focus of this paper. The following is an examplary nonconvex-nonconcave condition that is stronger than the negative comonotonicity <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref>. Example 1. Let f be twice continuously differentiable and ?-weakly-convex-weakly-concave. Further assume that f satisfies</p><formula xml:id="formula_10">? 2 xx f + ? 2 xy f (?I -? 2 yy f ) -1 ? 2 yx f ?I,<label>(2)</label></formula><formula xml:id="formula_11">-? 2 yy f + ? 2 yx f (?I + ? 2 xx f ) -1 ? 2</formula><p>xy f ?I, for some ? ? 0 and ? &gt; ?, named ? ? 0-interaction dominant condition in <ref type="bibr" target="#b10">[11]</ref>. Then, the saddle gradient of f satisfies the -1 ? -negative comonotonicity. (See Appendix A.1.) For any ?-weaklyconvex-weakly-concave function, the condition (2) holds with ? = -? &lt; 0. Its extreme case is</p><formula xml:id="formula_12">f (x, y) = -? 2 x 2 + ? 2 y 2</formula><p>, where there is no interaction between x and y. On the other hand, when the the second terms in the left-hand side of (2) are sufficently positive definite, a nonconvex-nonconave function satisfies the condition (2) with a nonnegative ?. In specific, the ? ? 0-interaction dominant condition is satisfied when the interaction term of Hessian ? 2 xy f is dominating any negative curvature in Hessians ? 2 xx f and -? 2 yy f <ref type="bibr" target="#b10">[11]</ref>.</p><p>We next present our proposed FEG, and illustrate that the FEG outperforms existing methods such as EG+, EAG, and the Halpern iteration, for each three comonoticity case, respectively.</p><p>4 Fast extragradient (FEG) method for Lipschitz continuous and comonotone operators</p><p>This section considers an instance of (Class FEG) with</p><formula xml:id="formula_13">? k = 1 L , ? k = 1 k+1</formula><p>, and ? k = ? for all k ? 0. The resulting method, named FEG, is illustrated in Algorithm 1, which has an O(1/k 2 ) fast rate with respect to the squared gradient norm, in Theorem 4.1. The proof of Theorem 4.1 is provided in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Fast extragradient (FEG) method</head><formula xml:id="formula_14">Input: z 0 ? R d , L ? (0, ?), ? ? -1 2L , ? for k = 0, 1, . . . do z k+1/2 = z k + 1 k + 1 (z 0 -z k ) -1 - 1 k + 1 1 L + 2? F z k z k+1 = z k + 1 k + 1 (z 0 -z k ) - 1 L F z k+1/2 -1 - 1 k + 1 2?F z k .</formula><p>end for Theorem 4.1. For the L-Lipschitz continuous and ?-comonotone operator F with ? &gt; -1 2L and for any z * ? Z * (F ), the sequence {z k } k?0 generated by FEG satisfies, for all k ? 1,</p><formula xml:id="formula_15">F z k 2 ? 4 z 0 -z * 2 1 L + 2? 2 k 2 . (<label>3</label></formula><formula xml:id="formula_16">)</formula><p>The following example shows that the bound (3) of the FEG is exact for ? = 0 and k = 4l + 2. The bound (3) is not known to be exact in general, and we leave finding the exact bound as future work.</p><p>Example 2. Let f : R ? R ? R be f (x, y) = Lxy. Its saddle gradient operator and solution are F (x, y) = (Ly, -Lx) and z * = (0, 0), respectively. For the initial point z 0 = (x 0 , y 0 ) = (1, 0), the sequence {z k } k?0 generated by FEG satisfies z 4l+2 = 0, 1 2l+1 for all l ? 0. Hence,</p><formula xml:id="formula_17">F z 4l+2 2 = L 2 (2l+1) 2 = 4L 2 z0-z * 2 (4l+2) 2</formula><p>for all l ? 0. (See Appendix B.1.)</p><p>We next compare the rate bound (3) with existing analyses for the three cases -1 2L &lt; ? &lt; 0, ? = 0, and ? &gt; 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison to EG+ under the negative comonotonicity (? &lt; 0)</head><p>Under the negative comonotonicity with -1 8L &lt; ? &lt; 0, the (EG+) method with ? k = 1 2L and ? = 1 2 has an O(1/k) rate on the squared gradient norm. To the best of our knowledge, this is the best known rate, and the FEG has a faster O(1/k 2 ) rate with a wider region of convergence -1 2L &lt; ? &lt; 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison to EAG under the monotonicity (? = 0)</head><p>For an L-Lipschitz continuous and monotone operator F , <ref type="bibr" target="#b42">[43]</ref> proposed two EAG methods, named EAG-C and EAG-V, with same ? k = 1 k+2 but with different choices of ? k . EAG-C sets ? k to be a constant 1 8L for all k ? 0 in (EAG), and has a large constant 260 in its convergence rate, </p><formula xml:id="formula_18">F z k 2 ? 260L 2 z0-z * 2 (k+1) 2</formula><p>for all k ? 0. On the other hand, while EAG-V requires a complicated recursive update for {? k },</p><formula xml:id="formula_19">? k+1 = ? k 1-? 2 k L 2 1 -(k+2) 2 (k+1)(k+3) ? 2 k L 2</formula><p>for all k ? 0, with ? 0 = 0.618 L , its rate has a smaller constant 27.</p><p>The FEG takes a constant ? k = 1 L , unlike EAG-V, but has an even smaller constant 4 in its convergence rate</p><formula xml:id="formula_20">F z k 2 ? 4L 2 z0-z * 2 k 2</formula><p>for ? = 0. Therefore, the FEG with ? = 0 has about 260/4-times and 27/4-times faster convergence rate compared to those of EAG-C and EAG-V, respectively. Furthermore, the rate bound of FEG with ? = 0 is only about 4-times larger than the lower complexity bound of first-order methods under the considered setting <ref type="bibr" target="#b42">[43]</ref>, reducing the gap between the lower and upper complexity bounds from 27 to 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison to the Halpern iteration under the cocoercivity (? &gt; 0)</head><p>For a ?-cocoercive operator F , an (explicit) version of Halpern iteration <ref type="bibr" target="#b11">[12]</ref>, studied in <ref type="bibr" target="#b5">[6]</ref>, has a fast rate,</p><formula xml:id="formula_21">F z k 2 ? z0-z * 2 ? 2 k 2</formula><p>. Note that while the ?-cocoercivity implies the 1 ? -Lipschitz continuity, there is case where the ?-cocoercive (and thus Lipschitz continuous) operator has a Lipschitz constant</p><formula xml:id="formula_22">L smaller than 1 ? . Since L ? 1 ? , the FEG has a rate F z k 2 ? 4 z0-z * 2 (1/L+2?) 2 k 2 = 4 z0-z * 2 9? 2 k 2</formula><p>that is faster than that of Halpern iteration. However, if we take into account that the FEG requires computing the saddle gradient twice per iteration, unlike Halpern iteration studied in <ref type="bibr" target="#b5">[6]</ref>, the FEG method has a slower rate in terms of the number of gradient computations. If we narrow down to the case L &lt; 1 2? , the FEG has a faster rate,</p><formula xml:id="formula_23">F z k 2 ? 4 z0-z * 2 (1/L+2?) 2 k 2 &lt; z0-z * 2 4? 2 k 2 .</formula><p>For such case, the FEG has a rate faster than that of the Halpern iteration, even in terms of the number of gradient computations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Toy example</head><p>We performed a toy experiment on a simple quadratic function,</p><formula xml:id="formula_24">f (x, y) = ?L 2 2 x 2 +L 1 -? 2 L 2 xy- ?L 2</formula><p>2 y 2 , which has an L-Lipschitz continuous and ?-comonotone saddle gradient. For the case ? = -1 3L and L = 1, Figure <ref type="figure">2</ref> illustrates that the FEG converges with an accelerated rate whereas EG+, EAG-C, EAG-V, and the (explicit) version of Halpern iteration <ref type="bibr" target="#b5">[6]</ref> diverge. This example presents that the existing guarantees on convergence and acceleration of the aforementioned methods under the convex-concave setting do not generalize to the nonconvex-nonconcave setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">FEG with backtracking line-search</head><p>The FEG requires the knowledge of the two global parameters L and ? for Lipschitz continuity and comonotonicity, respectively. Those global parameters are often difficult to compute in practice and can be locally conservative. To handle these two disadvantages, we employ the backtracking line-search technique <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b30">31]</ref> in FEG. We adaptively decrease the two step size parameters, ? and ?, to satisfy the both conditions, the local 1 ? -Lipschitz continuity and the ?-? 2 -comonotonicity. 5 A pseudocode of the resulting method, named FEG-A, is illustrated in Algorithm 2. For a detailed description of the FEG-A, see Algorithm 4 in Appendix C.1.</p><p>Algorithm 2 Fast extragradient method with adaptive step size (FEG-A)</p><formula xml:id="formula_25">Input: z 0 ? R d , ? -1 ? (max{0, -2?}, ?), ? 0 ? (0, ?), ? ? (0, 1) Find the smallest nonnegative integer i 0 such that ? = z 0 -? -1 (1 -?) i0 F z 0 satisfies F ? - F z 0 ? 1 ?-1(1-?) i 0 ? -z 0 . ? 0 = ? -1 (1 -?) i0 , z 1 = z 0 -? 0 F z 0 . for k = 1, 2, . . . do i k = j k = 0.</formula><p>Increase each i k and j k one by one until</p><formula xml:id="formula_26">?k+1/2 = z k + 1 k + 1 (z 0 -z k ) -1 - 1 k + 1 ? k-1 (1 -?) j k F z k and ?k+1 = z k + 1 k + 1 (z 0 -z k ) -? k-1 (1 -?) i k F z k+1/2 -1 - 1 k + 1 (? k-1 (1 -?) j k -? k-1 (1 -?) i k )F z k satisfy both conditions, F ?k+1 -F ?k+1/2 ? 1 ? k-1 (1 -?) i k ?k+1 -?k+1/2 and F ?k+1 -F z k , ?k+1 -z k ? ? k-1 (1 -?) j k -? k-1 (1 -?) i k 2 F ?k+1 -F z k 2 . ? k = ? k-1 (1 -?) i k , ? k = ? k-1 (1 -?) j k , z k+1 = ?k+1 . end for</formula><p>The following lemma shows that each of the nonincreasing sequences {? k } k?0 and {? k } k?0 of the FEG-A has a positive lower bound, and thus FEG-A is well-defined <ref type="foot" target="#foot_3">6</ref> , under the condition ? &gt; -? k 2 . This condition for ? can be weaker than the condition ? &gt; -1 2L of FEG, since the local Lipschitz parameter 1 ? k can be smaller than L. This is another benefit of using a backtracking line-search in FEG, over the standard FEG. Lemma 5.1. For the L-Lipschitz and ?-comonotone operator F and a given constant ? ? (0, 1), the step size ? k of FEG-A is lower bounded by a positive value ? := min ? -1 , 1-? L for all k ? 0, and if ? &gt; -? k 2 , the step size ? k is lower bounded by a positive value min ? 0 , (1 -?) ? k + 2? for all k ? 1.</p><p>The FEG-A method also has the following O(1/k 2 ) rate with respect to the squared gradient norm in Theorem 5.1, when ? &gt; -? k 2 . The proof is provided in Section 7 and Appendix C.3. Theorem 5.1. For the L-Lipschitz and ?-comonotone operator F and for any z * ? Z * (F ), the sequence {z k } k?0 generated by FEG-A satisfies</p><formula xml:id="formula_27">F z k 2 ? 4 z 0 -z * 2 ((k -1)? k + ? k + 2?) 2 for all k ? 1, if ? &gt; -? k 2 .</formula><p>This rate bound of FEG-A reduces to that of FEG in Theorem 4.1, when we choose ? -1 = 1 L and ? 0 = 1 L + 2? for FEG-A. 5 In specific, ? and ? locally estimate 1 L and 1 L + 2?, respectively. One could have directly estimate ?, instead of 1 L + 2?, but this complicates the line-search process to handle both positive and negative values of ?, unlike our choice of ? in FEG-A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">FEG under stochastic setting</head><p>When exactly computing F z is expensive in practice, one usually instead consider its stochastic estimate for computational efficiency (see, e.g., <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44]</ref>). This section also considers using a stochastic oracle in FEG for smooth convex-concave problems. In specific, this section assumes that we only have access to a noisy saddle gradient oracle,</p><formula xml:id="formula_28">F z k/2 = F z k/2 + ? k/2 , where {? k/2 } k?0 are independent random variables satisfying E[? k/2 ] = 0 and E[ ? k/2 2 ] = ? 2 k/2</formula><p>for all k ? 0. Under this setting, we study a stochastic first-order method, named stochastic fast extragradient (S-FEG) method, illustrated in Algorithm 3.</p><p>Algorithm 3 Stochastic fast extragradient (S-FEG) method</p><formula xml:id="formula_29">Input: z 0 ? R d , L ? (0, ?).</formula><p>for k = 0, 1, . . . do</p><formula xml:id="formula_30">z k+1/2 = z k + 1 k + 1 (z 0 -z k ) -1 - 1 k + 1 1 L F z k z k+1 = z k + 1 k + 1 (z 0 -z k ) - 1 L F z k+1/2</formula><p>end for</p><p>The following theorem provides an upper bound of the expected squared gradient norm for the S-FEG.</p><p>(See Appendix D.3 for the proof.) Theorem 6.1.</p><formula xml:id="formula_31">Let F z k/2 = F z k/2 + ? k/2 , where {? k/2 } k?0 are independent random variables satisfying E[? k/2 ] = 0 and E[ ? k/2 2 ] = ? 2 k/2 for all k ? 0.</formula><p>Then, for the L-Lipschitz continuous and monotone operator F and for any z * ? Z * (F ), the sequence {z k } k?0 generated by S-FEG satisfies</p><formula xml:id="formula_32">E[ F z k 2 ] ? 4L 2 z 0 -z * 2 k 2 + 6 k 2 ? 2 0 + k-1 l=1 (l 2 ? 2 l + (l + 1) 2 ? 2 l+1/2 )<label>(4)</label></formula><p>for all k ? 1. Furthermore, if ? 2 0 ? 6 , ? 2 k ? 6k and ? 2 k+1/2 ? 6(k+1) for all k ? 1, then the bound (4) reduces to</p><formula xml:id="formula_33">E[ F z k 2 ] ? 4L 2 z 0 -z * 2 k 2 + for all k ? 1.</formula><p>Here, we needed the noise variance ? 2 k/2 to decrease in the order of O(1/k) so that the stochastic error of the S-FEG does not accumulate. Otherwise, if ? 2 k/2 is a constant for all k, the error accumulates with rate O(k). In short, the S-FEG will suffer from error accumulation, unless the stochastic error decreases with rate O(1/k). Such error accumulation behavior also appears in a stochastic version of Nesterov's fast gradient method <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref> for smooth convex minimization <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8]</ref>. Similar to <ref type="bibr" target="#b4">[5]</ref>, we believe that adjusting the step coefficients of the S-FEG can make the S-FEG become relatively stable even with a constant noise, which we leave as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Convergence analysis with nonincreasing potential lemma</head><p>We analyze FEG and FEG-A by finding a nonincreasing potential function in a form</p><formula xml:id="formula_34">V k = a k F z k 2 -b k F z k , z 0 -z k in the lemma below.</formula><p>We provide a similar potential lemma for S-FEG in Appendix D.2. The convergence analyses of EAG and Halpern iteration are also based on such potential function <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b42">43]</ref>. Lemma 7.1. Let {z k } k?0 be the sequence generated by (Class FEG) with {? k } k?0 , {? k } k?0 ,</p><formula xml:id="formula_35">{L k } k?0 ? (0, ?) and {? k } k?0 ? R, satisfying ? 0 ? (0, ?), ? k ? 0, 1 L k , ? 0 = 1, {? k } k?1 ? (0, 1) for all k ? 1, and (1 -? k+1 ) 2? k+1 (? k+1 + 2? k+1 ) -? k+1 ? 1 2? k (? k + 2? k ) -? k</formula><p>for all k ? 0. Assume that the following conditions are satisfied.</p><formula xml:id="formula_36">F z 1 -F z 0 ? L 0 z 1 -z 0 F z k+1 -F z k+1/2 ? L k z k+1 -z k+1/2 for all k ? 1, F z k+1 -F z k , z k+1 -z k ? ? k F z k+1 -F z k 2 for all k ? 1.</formula><p>Then the potential function</p><formula xml:id="formula_37">V k = a k F z k 2 -b k F z k , z 0 -z k with a 0 = ?0(L 2 0 ? 2 0 -1) 2 , b 0 = 0, b 1 = 1, a k = b k (1 -? k ) 2? k (? k + 2? k ) -b k ? k and b k+1 = b k 1 -? k for all k ? 1 satisfies V k ? V k-1 for all k ? 1.</formula><p>Based on the above potential lemma, we next provide a convergence analysis of FEG. The analyses for the convergence rate of FEG-A and S-FEG, i.e., the proofs of Theorem 5.1 and Theorem 6.1, are similar to that of FEG and are provided in Appendix C.3 and Appendix D.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Convergence analysis for FEG</head><p>Proof of Theorem 4.1. Recall that FEG is equivalent to (Class FEG) with ? k = 1 L , ? k = 1 k+1 , and ? k = ?. It is straightforward to verify that the given {? k } k?0 and {? k } k?0 satisfy the conditions in Lemma 7.1 with L k = L for all k ? 0. Since</p><formula xml:id="formula_38">a k = b k (1 -? k ) 2? k (? k + 2? k ) -b k ? k = k 2 2 1 L + 2? -k? and b k = 1 1 -? k-1 b k-1 = k-1 i=1 1 1 -? i b 1 = k, Lemma 7.1 implies that 0 = V 0 ? V k = k 2 2 1 L + 2? -k? F z k 2 -k F z k , z 0 -z k . Therefore, k 2 2 1 L + 2? F z k 2 ? k F z k , z 0 -z k +k? F z k 2 = k F z k , z 0 -z * +k F z k , z * -z k +k? F z k 2 ? k F z k , z 0 -z * (? ?-comonotonicity of F ) ? k F z k z 0 -z * .</formula><p>The desired result follows directly by dividing both sides by k</p><formula xml:id="formula_39">2 2 1 L + 2? F z k .</formula><p>8 Discussion: first-order methods for Lipschitz continuous operators Throughout this paper, we studied and constructed efficient methods in a class of first-order methods:</p><formula xml:id="formula_40">z k ? z 0 + span{F z 0 , ? ? ? , F z k }</formula><p>denoted by A, for smooth structured nonconvex-nonconcave problems. We observed that all existing first-order methods, including the FEG, required an additional condition, such as the negative comonoticity, on a Lipschitz continuous F to guarantee convergence. One would then be curious whether or not there exists an (efficient) method in class A that guarantees convergence without any additional condition on a Lipschitz continuous F . Unfortunately, the following lemma states that there exists a worst-case<ref type="foot" target="#foot_4">7</ref> smooth example that none of the methods in A can find its stationary point. The corresponding smooth function is illustrated in Figure <ref type="figure">3</ref>. (</p><formula xml:id="formula_41">)<label>5</label></formula><p>Its saddle-gradient operator F is L-Lipschitz continuous but not comonotone. 8 Then, the sequence {z k } k?0 generated by any first-order method in class A with z 0 = (0, 0) satisfies F z k 2 = 2LR for all k ? 0.</p><p>Proof. F satisfies F (x, y) = (-? LR, -? LR) whenever x = y. Hence, for all sequences {z k } k?0 satisfying z 0 = (0, 0) and z k ? z 0 + span{F z 0 , ? ? ? , F z k } for all k ? 0, we have that {z k } k?0 ? {z = (x, y) ? R 2 |x = y}; thus, F z k 2 = 2LR for all k ? 0.</p><p>The lemma implies that one should consider a class of methods, other than the class A, to guarantee finding a stationary point of any smooth problem, which we leave as future work. We also leave finding additional conditions for a Lipschitz continuous F , weaker than the weak MVI condition and the negative comonotonicity (with ? &gt; -1 2L ), which guarantee convergence or its accelerated rate, respectively, as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>This paper proposed a two-time-scale and anchored extragradient method, named FEG, for smooth structured nonconvex-nonconcave problems. The proposed FEG has an accelerated O(1/k 2 ) rate, with respect to the squared gradient norm, for the Lipschitz continuous and negative comonotone operators for the first time. The FEG also has value for smooth convex-concave problems, compared to existing works. We further studied its backtracking line-search version, named FEG-A, for the smooth structured nonconvex-nonconcave problems and studied its stochastic version, named S-FEG, for smooth convex-concave problems. We leave extending this work to stochastic, composite, or more general nonconvex-nonconcave setting and applying to more realistic problems as future work. 8 Let z = x, x + R L and w = (0, 0). Since F z = (0, 0) and F w = (-? LR, -? LR), we get F z -F w, zw = 2 ? LRx + R and F z -F w 2 = 2LR, which implies that ? = -? in the comonotonicity condition as x ? -?.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :x 2 + 2 ? 2 3 xy + 1 6 y 2 .</head><label>2222</label><figDesc>Figure 2: Numerical result with f (x, y) = -1 6 x 2 + 2 ? 2 3 xy + 1 6 y 2 . The dashed line represents the theoretical bound (3) of FEG.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 : 2</head><label>32</label><figDesc>Figure 3: A smooth worst-case example f (x, y) (5) with L = R = 1 for first-order methods. any sequence {z k } k?0 generated by a first-order method in class A starting from (0, 0) is contained in the line x = y.</figDesc><graphic url="image-1.png" coords="10,205.76,125.42,126.72,95.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of the convergence rates of the existing extragradient-type methods and the FEG, with respect to the squared gradient norm, for smooth structured minimax problems, under various assumptions on the Lipschitz continuous saddle gradient operator F .</figDesc><table><row><cell></cell><cell>Method</cell><cell>Convex-concave</cell><cell>Nonconvex-nonconcave</cell></row><row><cell></cell><cell></cell><cell cols="2">Cocoercive Monotone Negative comonotone MVI Weak MVI</cell></row><row><cell>Normal</cell><cell>EG [4, 42]</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>A proximal point method converges under the negative comonotonicity<ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18]</ref>, but such implicit method is not preferable over explicit methods in practice due to its implicit nature.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>The EG+ was originally shown to work under the weak MVI condition of F , which is weaker than the negative comonotonicity.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>The convergence analysis on the duality gap of the extragradient type methods are generalized under the unbounded domain assumption in<ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>This requires one to chooses ?-1 strictly greater than the unknown value -2? when ? &lt; 0.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4"><p><ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">20]</ref> also introduce worst-case minimax examples that existing methods cannot find a stationary point. A key difference from our example is that their saddle-gradient operators are not Lipschitz continuous. In addition, the considered classes of methods in<ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">20]</ref> exclude EG+ and FEG, unlike the class A.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments and Disclosure of Funding</head><p>This work was supported in part by the <rs type="funder">National Research Foundation of Korea (NRF)</rs> grant funded by the <rs type="funder">Korea government (MSIT)</rs> (No. <rs type="grantNumber">2019R1A5A1028324</rs>), the <rs type="funder">POSCO Science Fellowship of POSCO TJ Park Foundation</rs>, and the <rs type="funder">Samsung Science and Technology Foundation</rs> (No. <rs type="grantNumber">SSTF-BA2101-02</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_GTqeGz9">
					<idno type="grant-number">2019R1A5A1028324</idno>
				</org>
				<org type="funding" xml:id="_AgnmZrw">
					<idno type="grant-number">SSTF-BA2101-02</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generalized monotone operators and their averaged resolvents</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Bauschke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Moursi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">189</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="74" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A fast iterative shrinkage-thresholding algorithm for linear inverse problems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Teboulle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Imaging Sci</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="183" to="202" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Proximal methods for cohypomonotone operators</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Combettes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pennanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Control Optim</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="731" to="742" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the convergence properties of non-Euclidean extragradient methods for variational inequalities with generalized monotone operators</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Optimization and Applications</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="277" to="310" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Stochastic first order methods in smooth convex optimization</title>
		<author>
			<persName><forename type="first">O</forename><surname>Devolder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Halpern iteration for near-optimal and parameter-free monotone inclusion and strong solutions to variational inequalities</title>
		<author>
			<persName><forename type="first">J</forename><surname>Diakonikolas</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1428" to="1451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Efficient methods for structured nonconvexnonconcave min-max optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Diakonikolas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Daskalakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2746" to="2754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Accelerated gradient methods for nonconvex nonlinear and stochastic programming</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ghadimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">156</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="99" />
			<date type="published" when="2016-03">March 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Last iterate is slower than averaged iterate in smooth convex-concave saddle point problems</title>
		<author>
			<persName><forename type="first">N</forename><surname>Golowich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pattathil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Daskalakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ozdaglar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory (COLT)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Info. Proc. Sys</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The landscape of the proximal point method for nonconvex-nonconcave minimax optimization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Grimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Worah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mirrokni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fixed points of nonexpanding maps</title>
		<author>
			<persName><forename type="first">B</forename><surname>Halpern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bull. Amer. Math. Soc</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="957" to="961" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On the convergence of single-call stochastic extra-gradient methods</title>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Iutzeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mertikopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Info. Proc. Sys</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Explore aggressively, update conservatively: Stochastic extragradient methods with variable stepsize scaling</title>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Iutzeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mertikopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Info. Proc. Sys</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The limits of min-max optimization algorithms: convergence to spurious non-critical sets</title>
		<author>
			<persName><forename type="first">Y.-P</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mertikopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cevher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Conf. Mach. Learn</title>
		<meeting>Intl. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Solving variational inequalities with stochastic mirror-prox algorithm</title>
		<author>
			<persName><forename type="first">A</forename><surname>Juditsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nemirovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tauvel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stochastic Systems</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="58" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Accelerated proximal point method for maximally monotone operators</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">190</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="87" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On the proximal point algorithm and its Halpern-type variant for generalized monotone operators in Hilbert space</title>
		<author>
			<persName><forename type="first">U</forename><surname>Kohlenbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optimization Letters</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An extragradient method for finding saddle points and other problems</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Korpelevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ekonomika i Mateaticheskie Metody</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="747" to="756" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On the impossibility of global convergence in multi-loss optimization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Letcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Conf. on Learning Representations</title>
		<meeting>Intl. Conf. on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On the convergence rate of the Halpern-iteration</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optimization Letters</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="405" to="418" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Towards better understanding of adaptive gradient algorithms in generative adversarial nets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">First-order convergence theory for weakly-convexweakly-concave min-max problems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rafique</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learning Res</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">169</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Golden ratio algorithms for variational inequalities</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Malitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">184</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="383" to="410" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A first-order primal-dual algorithm with linesearch</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Malitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="411" to="432" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Optimistic mirror descent in saddle-point problems: going the extra (gradient) mile</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mertikopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lecouat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zenati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-S</forename><surname>Foo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Piliouras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Conf. on Learning Representations</title>
		<meeting>Intl. Conf. on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmdit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Conf. on Learning Representations</title>
		<meeting>Intl. Conf. on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A unified analysis of extra-gradient and optimistic gradient methods for saddle point problems: Proximal point approach</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mokhtari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ozdaglar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pattathil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Conf. Artificial Intelligence and Stat. (AISTATS)</title>
		<meeting>Intl. Conf. Artificial Intelligence and Stat. (AISTATS)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On the complexity of the hybrid proximal extragradient method for the iterates and the ergodic mean</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D C</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">F</forename><surname>Svaiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2755" to="2787" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Complexity of variants of Tseng&apos;s modified FB splitting and Korpelevich&apos;s methods for hemivariational inequalities with applications to saddle-point and convex optimization problems</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D C</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">F</forename><surname>Svaiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1688" to="1720" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Convex-concave backtracking for inertial bregman proximal gradient algorithms in nonconvex optimization</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Mukkamala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sabach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Mathematics of Data Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="658" to="682" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Prox-method with rate of convergence O(1/t) for variational inequalities with Lipschitz continuous monotone operators and smooth convex-concave saddle point problems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nemirovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="229" to="251" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Robust stochastic approximation approach to stochastic programming</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nemirovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Juditsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1574" to="1609" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Problem complexity and method efficiency in optimization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nemirovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yudin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A method for unconstrained convex minimization problem with the rate of convergence O(1/k 2 )</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dokl. Akad. Nauk. USSR</title>
		<imprint>
			<biblScope unit="volume">269</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="543" to="547" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Smooth minimization of non-smooth functions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="127" to="152" />
			<date type="published" when="2005-05">May 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dual extrapolation and its applications to solving variational inequalities and related problems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="319" to="344" />
			<date type="published" when="2007-03">March 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Lower complexity bounds of first-order methods for convex-concave bilinear saddle-point problems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">185</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A modification of the Arrow-Hurwicz method for search of saddle points</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Popov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical notes of the Academy of Sciences of the USSR</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="845" to="848" />
			<date type="published" when="1980-11">November 1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">ODE analysis of stochastic gradient methods with optimism and anchoring for minimax problems and GANs</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905">2019. arxiv 1905.10899</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A hybrid approximate extragradient-proximal point algorithm using the enlargement of a maximal monotone operator</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Solodov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">F</forename><surname>Svaiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Set-Valued Analysis</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="323" to="345" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Optimistic dual extrapolation for coherent non-monotone variational inequalities</title>
		<author>
			<persName><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Info. Proc. Sys</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Accelerated algorithms for smooth convex-concave minimax problems with O(1/k 2 ) rate on squared gradient norm</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Ryu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Conf. Mach. Learn</title>
		<meeting>Intl. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Stochastic mirror descent in variationally coherent optimization problems</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mertikopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bambos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Glynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Info. Proc. Sys</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
