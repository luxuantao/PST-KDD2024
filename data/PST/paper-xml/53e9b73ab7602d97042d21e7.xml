<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Test Collection Based Evaluation of Information Retrieval Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mark</forename><forename type="middle">Sanderson</forename><surname>Contents</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The Information School</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<settlement>Sheffield</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Mark</forename><surname>Sanderson</surname></persName>
							<email>m.sanderson@shef.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">The Information School</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<settlement>Sheffield</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Test Collection Based Evaluation of Information Retrieval Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B1304B651E9B2EDB817EED1605941B98</idno>
					<idno type="DOI">10.1561/1500000009</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>2</term>
					<term>1 Cleverdon&apos;s Cranfield Collection 2</term>
					<term>2 Evaluating Boolean Retrieval Systems on a Test Collection 2</term>
					<term>3 Evaluating over a Document Ranking 2</term>
					<term>4 Challenging the Assumptions in the Early Collections 2</term>
					<term>5 Assessor Consistency 2</term>
					<term>6 The Practical Challenges of Creating and Using Test Collections 3 TREC and Its Ad Hoc Track 3</term>
					<term>1 Building an Ad Hoc Test Collection 3</term>
					<term>2 Classic TREC Ad hoc Measures 3</term>
					<term>3 The Other TREC Tracks and Uses of TREC Collections 3</term>
					<term>4 Other Evaluation Exercises 3</term>
					<term>5 TREC&apos;s Run Collection 3</term>
					<term>6 TREC Ad Hoc: A Great Success with Some Qualifications 3</term>
					<term>7 Conclusion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Use of test collections and evaluation measures to assess the effectiveness of information retrieval systems has its origins in work dating back to the early 1950s. Across the nearly 60 years since that work started, use of test collections is a de facto standard of evaluation. This monograph surveys the research conducted and explains the methods and measures devised for evaluation of retrieval systems, including a detailed look at the use of statistical significance testing in retrieval experimentation. This monograph reviews more recent examinations of the validity of the test collection approach and evaluation measures as well as outlining trends in current research exploiting query logs and live labs. At its core, the modern-day test collection is little different from the structures that the pioneering researchers in the 1950s and 1960s conceived of. This tutorial and review shows that despite its age, this long-standing evaluation method is still a highly valued tool for retrieval research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>An examination of the opening pages of a number of Information Retrieval (IR) books reveals that each author defines the topic of IR in different ways. Some say that IR is simply a field concerned with organizing information <ref type="bibr" target="#b210">[210]</ref>; and others emphasize the range of different materials that need to be searched <ref type="bibr" target="#b287">[286]</ref>. While others stress the contrast between the strong structure and typing of a database (DB) system with the lack of structure in the objects typically searched in IR <ref type="bibr" target="#b262">[262,</ref><ref type="bibr" target="#b244">244]</ref>. Across all of these definitions, there is a constant, IR systems have to deal with incomplete or underspecified information in the form of the queries issued by users. The IR systems receiving such queries need to fill in the gaps of the users' underspecified query.</p><p>For example, a user typing "nuclear waste dumping" into the search engine of an academic repository is probably looking for multiple documents describing this topic in detail, he/she probably prefers to see documents from reputable sources, but all he/she enters into the search engine are three words. Users querying on a web search engine for "BBC" are probably looking for the official home page of the corporation, yet they fully expect the search engine to infer that specific information request from the three letters entered. The fact that the content being searched is typically unstructured and its components (i.e., words) can have multiple senses, and different words can be used to express the same concept, merely adds to the challenge of locating relevant items. In contrast to a DB system, whose search outputs are deterministic, the accuracy of an IR system's output cannot be predicted with any confidence prior to a search being conducted; consequently, empirical evaluation has always been a critical component of Information Retrieval. <ref type="foot" target="#foot_0">1</ref>The typical interaction between a user and an IR system has the user submitting a query to the system, which returns a ranked list of objects that hopefully have some degree of relevance to the user's request with the most relevant at the top of the list. The success of such an interaction is affected by many factors, the range of which has long been considered. For example, Cleverdon and Keen <ref type="bibr">[61, p. 4</ref>] described five.</p><p>(1) "The ability of the system to present all relevant documents <ref type="bibr" target="#b1">(2)</ref> The ability of the system to withhold non-relevant documents <ref type="bibr">(3)</ref> The interval between the demand being made and the answer being given (i.e., time) (4) The physical form of the output (i.e., presentation) <ref type="bibr" target="#b4">(5)</ref> The effort, intellectual or physical, demanded of the user (i.e., effort)."</p><p>To this list one could add many others, e.g.:</p><p>• the ability of the user at specifying their need;</p><p>• the interplay of the components of which the search algorithm is composed; • the type of user information need;</p><p>• the number of relevant documents in the collection being searched; • the types of documents in the collection;</p><p>• the context in which the user's query was issued; and • the eventual use for the information being sought.</p><p>Evaluation of IR systems is a broad topic covering many areas including information-seeking behavior usability of the system's interface; its broader contextual use; the compute efficiency, cost, and resource needs of search engines. A strong focus of IR research has been on measuring the effectiveness of an IR system: determining the relevance of items, retrieved by a search engine, relative to a user's information need.</p><p>The vast majority of published IR research assessed effectiveness using a resource known as a test collection used in conjunction with evaluation measures. Such is the importance of test collections that at the time of writing, there are many conferences and meetings devoted purely to their use: including three international conferences, TREC, CLEF, and NTCIR, which together have run more than 30 times since the early 1990s. This research focus is not just a feature of the past two decades but part of a longer tradition which was motivated by the creation and sharing of testing environments in the previous three decades, which itself was inspired by innovative work conducted in the 1950s. The classic components of a test collection are as follows:</p><p>• a collection of documents; each document is given a unique identifier, a docid; • a set of topics (also referred to as queries); each given a query id (qid); and • a set of relevance judgments (often referred to as qrelsquery relevance set) composed of a list of qid/docid pairs, detailing the relevance of documents to topics.</p><p>In the possession of an appropriate test collection, an IR developer or researcher simply loads the documents into their system and in a batch process, submits the topics to the system one-by-one. The list of the docids retrieved for each of the topics is concatenated into a set, known as a run. Then the content of the run is examined to determine which of the documents retrieved were present in the qrels and which were not. Finally, an evaluation measure is used to quantify the effectiveness of that run.</p><p>Together, the collection and chosen evaluation measure provide a simulation of users of a searching system in an operational setting. Using test collections, researchers can assess a retrieval system in isolation helping locate points of failure, but more commonly, collections are used to compare the effectiveness of multiple retrieval systems. Either rival systems are compared with each other, or different configurations of the same system are contrasted. Such determinations, by implication, predict how well the retrieval systems will perform relative to each other if they were deployed in the operational setting simulated by the test collection.</p><p>A key innovation in the IR academic community was the early recognition of the importance of building and crucially sharing test collections. <ref type="foot" target="#foot_1">2</ref> Through sharing, others benefited from the initial (substantial) effort put into the creation of a test collection by re-using it in other experiments. Groups evaluating their own IR systems on a shared collection could make meaningful comparisons with published results tested on the same collection. Shared test collections provided a focus for many international collaborative research exercises. Experiments using them constituted the main methodology for validating new retrieval approaches. In short, test collections are a catalyst for research in the IR community.</p><p>Although there has been a steady stream of research in evaluation methods, there has been little survey of literature covering test collection based evaluation. Salton's evaluation section <ref type="bibr" target="#b210">[210,</ref><ref type="bibr">Section 5</ref>] is one such document; a chapter in Van Rijsbergen's book <ref type="bibr" target="#b262">[262]</ref> another; Spärck Jones's edited articles on IR experiments <ref type="bibr" target="#b242">[242]</ref> a third. Since those works, no broad surveys of evaluation appear to have been written; though Hearst has recently written about usability evaluation in IR <ref type="bibr" target="#b115">[116,</ref><ref type="bibr">Section 3]</ref>. The sections on evaluation in recent IR books provided the essential details on how to conduct evaluation, rather than reviewed past work. There are notable publications addressing particular aspects of evaluation: Voorhees and Harman's book detailed the history of the TREC evaluation exercise and outlined evaluation methods used <ref type="bibr" target="#b281">[280]</ref>; a special issue of Information Processing and Management reflected the state of IR evaluation in 1992 <ref type="bibr" target="#b97">[98]</ref>; another special issue in the Journal of the American Society for Information Science provided a later perspective <ref type="bibr" target="#b253">[253]</ref>. More recently, Robertson published his personal view on the history of IR evaluation <ref type="bibr" target="#b199">[199]</ref>. However, there remains a gap in the literature, which this monograph attempts to fill.</p><p>Using test collections to assess the effectiveness of IR systems is itself a broad area covering a wide range of document types and forms of retrieval. IR systems were built to search over text, music, speech, images, video, chemical structures, etc. For this monograph, we focus on evaluation of retrieval from documents that are searched by their text content and similarly queried by text; although, many of the methods described are applicable to other forms of IR.</p><p>Since the initial steps of search evaluation in the 1950s, test collections and evaluation measures were developed and adapted to reflect the changing priorities and needs of IR researchers. Often changes in test collection design caused changes in evaluation measures and vice versa. Therefore, the work in these two distinct areas of study are described together and laid out in a chronological order. The research is grouped into three periods, which are defined relative to the highly important evaluation exercise, TREC.</p><p>• Early 1950s-early1990s, Section 2: the initial development of test collections and measures. In this time, test collection content was mostly composed of catalogue information about academic papers or later the full-text of newspaper articles. The evaluation measures commonly used by researchers were strongly focused on high recall search: finding as many relevant items as possible. • Early 1990s-early 2000s, Section 3: the "TREC ad hoc" period. Scale and standardization of evaluation were strong themes of this decade. The IR research community collaborated to build a relatively small number of large test collections mainly composed of news articles. Evaluation was still focused on high recall search. • Early 2000s-present, Section 4: the post ad hoc period (for want of a better name). Reflecting the growing diversity in application of search technologies and the ever-growing scale of collections being searched, evaluation research in this time showed a diversification of content and search task along with an increasing range of evaluation measures that reflected user's more common preference for finding a small number of relevant items. Run data gathered by TREC and other similar exercises fostered of a new form of evaluation research in this period: studying test collection methodologies. This research is covered in Section 6.</p><p>The one exception to the ordering can be found in the section on the use of significance testing. Apart from a recent book <ref type="bibr" target="#b73">[74]</ref>, little has been written on the use of significance in IR evaluation and relatively little research has been conducted; consequently, I chose to describe research in this area, in Section 5, more as a tutorial than a survey.</p><p>Such an ordering means that descriptions of or references to evaluation measures are spread throughout the document. Therefore, we provide an index at the conclusion of this work to aid in their location.</p><p>Note, unless explicitly stated otherwise, the original versions of all work cited in this document were obtained and read by the author.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Initial Development of Test Collections</head><p>The genesis of IR evaluation is generally seen as starting with the work of Cleverdon and his Cranfield collections, built in the early 1960s. However, he and others were working on retrieval evaluation for most of the 1950s. In his article looking back over his career, Cleverdon <ref type="bibr" target="#b59">[60]</ref> stated that along with a collaborator, Thorne, he created a small test collection in 1953. The intention was to test the effectiveness of librarians at locating documents indexed by different library cataloguing systems when faced with information requests from library users. This work was first described by Thorne two years after it was completed <ref type="bibr" target="#b257">[257]</ref>.</p><p>Thorne described the motivation for conducting this testing in terms that have a strong a resonance with the motivations of IR researchers today. "the author has found the need for a 'yardstick' to assist in assessing a particular system's merits . . . the arguments of librarians would be more fertile if there were quantitative assessments of efficiency of various cataloguing systems in various libraries". In describing their methodology for testing, Thorne stated "Suppose the questions put to the catalogue [from users] are entered in a log, and 100 test questions are prepared which are believed to represent typically such a log. If the test questions are based on material known to be included in the collection, they can then be used to assess the catalogue's probability of success".</p><p>The paper listed 50 statements of information need that were used to assess a series of library cataloguing systems. Thorne and Cleverdon tests were essentially a form of known item searching. To illustrate, the following is an information need taken from Appendix C of the paper: "The pressure distributions over the nose of a body of revolution of fineness ratio 6 for angles of attack 0 • to 8 • at high subsonic Mach number (RN &gt; 4 × 10 • )." This request was generated by the authors from a document known to be catalogued in a library. Assessments were based not only on success in finding the known item, but also consideration of the costs of implementing the cataloguing system. Note in Salton's writings, this early test collection was often referred to as Cranfield I (though Cleverdon called a different collection by that name).</p><p>In the same year of Cleverdon and Thorne's early efforts, Gull (who published the work in 1956, <ref type="bibr" target="#b96">[97]</ref>) also reported building a form of test collection. Composed of 15,000 catalogue entries, the collection was built to compare two library cataloguing systems, each built by a separate group. In total, 98 queries (called requests by <ref type="bibr">Gull)</ref> were created and searchers from each group worked to locate as many relevant documents for these requests as possible. Each group formed its own relevance judgments independently, which proved to be problematic, as they discovered that their judgments were quite different from each other based on different interpretations of the queries. Gull stated that one group took a more liberal view of relevance than the other. (Cleverdon stated in <ref type="bibr" target="#b59">[60]</ref>, that after seeing the problems created by independently formed qrels, he decided to centralize relevance judgments for his collections.)</p><p>In the 1950s, computers started to be used for searching of library catalogues. An early mention of "machines" being involved in IR was by Kent et al. <ref type="bibr" target="#b154">[155]</ref>, who proposed an evaluation methodology that they called "a framework of reference for analyzing the performance of an IR system". The framework described was similar to a modern test collection. Maron et al. <ref type="bibr" target="#b172">[173]</ref> as part of their work in experimenting with probabilistic indexing described a form of evaluation using a collection of 110 documents and 40 queries. Fels <ref type="bibr" target="#b83">[84]</ref> detailed a methodology for testing retrieval effectiveness proposed by Mooers <ref type="bibr" target="#b181">[181]</ref>. Bryant <ref type="bibr" target="#b34">[35]</ref> briefly described the work by Borko <ref type="bibr" target="#b28">[29]</ref> who, according to Bryant, constructed a test collection composed of 612 abstracts. In an appendix of his evaluation survey paper Robertson <ref type="bibr" target="#b195">[195]</ref> details a number of other early tests; see also the books from Lancaster <ref type="bibr" target="#b158">[159]</ref> and from Spärck Jones <ref type="bibr" target="#b242">[242]</ref> for more on the early developments in IR evaluation.</p><p>Given that the very first uses of computers for searching only date back to the late 1940s,<ref type="foot" target="#foot_2">1</ref> evaluation of searching systems was clearly an early and important priority for IR researchers. These works, however, are little remembered by today's researchers due to the detailed construction of a test collection that Cleverdon started in the late 1950s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Cleverdon's Cranfield Collection</head><p>In his reflective piece, Cleverdon cited an editorial from American Documentation <ref type="foot" target="#foot_3">2</ref> (now renamed JASIST) stating that "evaluation of all experimental results is essential in rating the efficiency of [IR] systems". Cleverdon argued that it wasn't good enough for groups to evaluate their own systems, an independently run evaluation was needed. Consequently, he was funded to test four competing indexing approaches on a collection composed of 18,000 papers <ref type="bibr" target="#b56">[57]</ref>. The papers were manually indexed using each of the four classification methodologies. Once the indexes were built, the papers were searched with 1,200 "search questions". The questions were designed to retrieve one of the collection papers; if that paper was retrieved, the search was considered a success. The collection became known as Cranfield I. Cleverdon reported on the results of his comparison of the four methods and from the experience of this collection, decided to develop Cranfield II.</p><p>Cleverdon felt that the relatively large size of Cranfield I was not important in ensuring that measurements were reliable. Therefore, the new collection was composed of 1,400 "documents" (titles, author names, and abstracts) derived from the references listed in around 200 recent research papers. The authors of those papers were contacted and asked to write a question that summarized the problem their paper addressed, these became the collection topics. The authors were also asked to rate each reference in their paper on a scale of 1-5 on how relevant the reference was to the stated question and if possible to provide additional references. Cleverdon's students checked all documents against all questions and contacted the authors of each question asking them if they considered any additional documents found to be relevant. All this work resulted in a collection comprising 1,400 documents, 221 topics, and a set of complete variable level relevance judgments.</p><p>Cleverdon was not alone in creating test collections, Salton instigated the creation of a series of test collections: collectively known as the SMART collections (named after the experimental retrieval system that Salton and his students built). In 1968, along with Lesk <ref type="bibr" target="#b163">[164]</ref>, he described research using two collections, the ADI, a collection of short academic papers, and the IRE-3 collection composed of the abstracts of computer science publications. Later, Salton and Yu <ref type="bibr" target="#b215">[215]</ref> described two more: Time and MEDLARS, the first is composed of 425 full-text articles from Time magazine; the second composed of 450 abstracts of medical literature. Note, this MEDLARS collection is different from the test collection with the same name built by Lancaster <ref type="bibr" target="#b157">[158]</ref> who in an extensive evaluation of the MEDLARS system created a test collection from 410 actual search requests submitted to the system. Another popular test collection was the NPL created by Vaswani and Cameron <ref type="bibr" target="#b263">[263]</ref>. To illustrate the scale of these collections, a number of the more commonly used are detailed in the following table. For details on others, see Spärck Jones and Van Rijsbergen's survey <ref type="bibr" target="#b246">[246]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Evaluating Boolean Retrieval Systems on a Test Collection</head><p>With the creation of test collections came the need for effectiveness measures. Many early IR systems produced Boolean output: an unordered set of documents matching a user's query; evaluation measures were defined to assess this form of output. Kent et al. <ref type="bibr" target="#b154">[155]</ref>, listed what they considered to be the important quantities to be calculated in Boolean search:</p><p>• n -the number of documents in a collection;</p><p>• m -the number of documents retrieved;</p><p>• w -the number that are both relevant and retrieved; and </p><formula xml:id="formula_0">= a a + b Recall = a a + c Fallout = b b + d</formula><p>Where precision measures the fraction of retrieved documents that are relevant, recall measures the fraction of relevant documents retrieved and fallout measures the fraction of non-relevant documents retrieved. Although commonly described in IR text books, fallout is by far the least used in published IR research.</p><p>Of all the measures that were proposed, two -precision and recall -dominated evaluation from the start. Reporting on his 1953 test collection work, Gull <ref type="bibr" target="#b96">[97]</ref> appeared to be the first to describe recall, measuring competing systems by dividing "actual retrieval" by "optimum retrieval". Precision and recall were first described together by Kent et al. <ref type="bibr" target="#b154">[155]</ref>. In their paper, precision was referred to as a "pertinence factor"; recall was called "recall factor". Kent et al. stated that neither factor could be used on its own; both measures had to be taken into account to determine effectiveness of a retrieval system. Cleverdon, who called the precision and recall measures, respectively, "relevance ratio" and "recall ratio" described an inverse relationship between the two <ref type="bibr">[58, pp. 71-72]</ref>, showing that if one issued Boolean searches that precisely targeted relevant documents and avoided the retrieval of nonrelevant, precision would likely be high, but recall would be low. If a query could be broadened in some way to improve recall, the almost inevitable consequence was that more non-relevant documents would  be returned, causing precision to drop. Using the Cranfield II test collection, he graphed recall/precision data points corresponding to the different Boolean queries; the graph is reproduced in Figure <ref type="figure">2</ref>.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Summarizing Precision and Recall to a Single Value</head><p>A great deal of evaluation research addressed the question of how to conflate the two measures into a single value. Van Rijsbergen <ref type="bibr" target="#b261">[261]</ref> surveyed a range of such measures. Later in his book, he proposed a measure, which is one minus the weighted harmonic mean of recall and precision, which he called e. Although this measure was sometimes used <ref type="bibr" target="#b72">[73]</ref>, the weighted harmonic mean was more extensively used in IR literature; it is commonly referred to as f , and is defined as follows.</p><formula xml:id="formula_1">f = 1 α 1 P + (1 -α) 1 R .</formula><p>The tuning constant α is used to indicate a preference for how much influence precision or recall has on the value of f . It is common for α to be set to 0.5, which then allows f to be defined as:</p><formula xml:id="formula_2">f = 1 1 2 1 R + 1 P or the equivalent form f = 2 × P × R P + R .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Evaluating over a Document Ranking</head><p>The measures described so far work over an unordered set of retrieved documents as would be returned by a Boolean IR system. The development of ranked retrieval -see for example, Maron et al., <ref type="bibr" target="#b172">[173]</ref> -required changes in evaluation as the balance of relevant and non-relevant documents varied over the ranking. For any query that was a reasonable reflection of a user's information need, the retrieved documents that matched the query well tended to be mostly relevant and ranked highly. Relevant documents in the collection that matched the query less well appeared further down the ranking mixed in with progressively greater numbers of non-relevant documents. Swets <ref type="bibr" target="#b249">[249]</ref> formally described this situation. He suggested that for any given document ranking, the proportion of relevant to non-relevant documents could be described by two distributions: one for the relevant documents and one for the non-relevant. Swets did not have search output to work with, and so could only speculate on the shape of the distributions: he initially suggested that they would both be normal, though later described other possibilities <ref type="bibr" target="#b250">[250]</ref>. Bookstein <ref type="bibr" target="#b27">[28]</ref> described potential problems with Swets's model with normal distributions in place. Much later, researchers such as Manmatha, et al. <ref type="bibr" target="#b170">[171]</ref> analyzed large sets of ranks and confirmed Swets's formalisms. They found that relevant documents adhered to a normal distribution and the non-relevant followed a negative exponential. Graphs illustrating the distributions of two retrieval systems are shown in Figure <ref type="figure">2</ref>.2. High scoring documents (on the right of the graphs) are all or nearly all relevant, but for documents that match the query progressively less well (moving to the left), the balance of relevant to non-relevant shifts to a greater proportion of non-relevant being retrieved. The exact nature of the balance and the way that it changes across a rank for a particular retrieval system will depend on that system's effectiveness: a good system will produce a ranking that has a strong separation between the distributions (e.g., graph on right of Figure <ref type="figure">2</ref>.2) and a poor system the opposite (e.g., graph on the left), Swets suggested that this approach could form the basis of an evaluation measure for IR systems, however, the idea was not taken up by the community, who instead choose to focus on adapting precision and recall to ranked retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Plotting Precision on a Graph</head><p>An early popular approach to evaluation of ranked retrieval was to graph precision values, measured over the document ranking averaged across multiple topics. However, as can be seen from the example in Figure <ref type="figure">2</ref>.3, plotting recall and precision computed at each relevant document for the ranks of two topics results in a scatter plot of discreet points that before being averaged need to be transformed to a pair of continuous functions using interpolation.</p><p>Many methods of interpolation were considered by researchers: Cleverdon and Keen <ref type="bibr" target="#b60">[61]</ref> defined one; Keen discussed others <ref type="bibr">[151, p. 90</ref>]. In the end, one of Keen suggestion's was commonly adopted, which Keen named semi-Cranfield, sometimes also called neo-Cleverdon Williamson et al. <ref type="bibr" target="#b286">[285]</ref>. Here, the interpolated value of precision measured at any recall level (r i ) was defined to be the highest precision measured at any level (r ) greater than or equal to r i . <ref type="bibr">Manning</ref>  In the first topic, there are five relevant documents, two of which were not retrieved; in the second topic, there are three relevant, all of which were retrieved in the top ten of the ranking.</p><p>formulated Keen's description thus<ref type="foot" target="#foot_6">5</ref> :</p><formula xml:id="formula_3">P interp (r i ) = max r r i P (r ).</formula><p>The result of the interpolation method on the points of the two topics in Figure <ref type="figure">2</ref>.3 can be seen in the graph in Figure <ref type="figure">2</ref>.4. The values of precision for each function were averaged at a series of pre-chosen recall levels, commonly eleven levels from 0% to 100%; although researchers also used ten levels (dropping the 0%), three levels (25%, 50%, 100%) and twenty-one recall levels (0%, 5%, 10%, 15%, . . . , 95%, 100%). The resulting graph of precision averaged across both topics is shown in Figure <ref type="figure">2</ref>.5. Note, it is common to draw such a graph with a simple interpolated line drawn between the averaged points. By measuring the precision of every relevant document including those that were not retrieved (implied by measuring precision at recall 100%), there was an assumption in the design of this measure that users were interested in achieving such a high level of recall.</p><p>Precision at each of the standard recall levels can itself be averaged and is referred to as interpolated average precision or sometimes npoint average precision, where n is the number of recall levels. For example, from the graph in Figure <ref type="figure">2</ref>.5 one could compute 11-point average precision.</p><p>Recall precision graphs were a common form of reporting effectiveness: in his book, Salton mentioned little else; in Van Rijsbergen's evaluation section, <ref type="bibr" target="#b262">[262]</ref>, much space was devoted to the ways of computing such graphs.</p><p>At first glance, it might appear that the interpolation was an unusual choice as it ensured that the continuous function could only be a flat or a monotonically decreasing line. Williamson et al. <ref type="bibr" target="#b286">[285]</ref> stated that it was chosen as the standard used by the SMART retrieval system <ref type="bibr" target="#b211">[211]</ref>. Ten years later, and it would appear quite independently, Van Rijsbergen also declared Keen's interpolation as the most appropriate to use <ref type="bibr" target="#b262">[262,</ref><ref type="bibr">Section 7]</ref>. Both Van Rijsbergen and Williamson stated that they preferred this method over others as it was a conservative interpolation that did not inflate the values of precision for a topic. For topic 1 in Figure <ref type="figure">2</ref>.4, this would appear to be the case; however, for topic 2, the interpolation would appear to be anything but conservative.</p><p>Keen appeared to explain this feature of the interpolation by stating that it computed "the theoretical maximum performance that a user could achieve". Van Rijsbergen, in a later personal communication stated that his reasoning for choosing the interpolation was one of normalization against potential errors. The general trend of precision recall graphs was that of a monotonically decreasing line, the increasing precision of topic 2 went against that trend and so should be viewed as an error to be normalized. Van Rijsbergen also stated that at the time, retrieval systems ranked documents based on similarity scores with a coarse granularity. Often, sets of documents were assigned exactly the same score. The order that such documents were ranked by was commonly the order the documents were stored in the IR system; Cooper <ref type="bibr" target="#b64">[65]</ref> described this form ranking as a weak ordering. Van Rijsbergen's concern was that these blocks of weakly ordered documents caused many of the increases in precision seen for topics. Consequently, he chose the interpolation function as it would normalize such increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Another Early Evaluation Measure</head><p>Cooper was interested in evaluating ranked retrieval using a single evaluation measure, but was not convinced that finding every relevant document was always the priority of searchers. In his 1968 publication, he stated "most measures do not take into account a crucial variable: the amount of material relevant to [the user's] query which the user actually needs"; he went onto say "the importance of including user needs as a variable in a performance measure seems to have been largely overlooked". He proposed a measure called Expected Search Length (ESL) <ref type="bibr" target="#b64">[65]</ref>, which determined the amount of a ranking that had to be observed by a searcher in order to locate a pre-specified quantity of relevant documents. Cooper was aware that the ranked retrieval algorithms of the time commonly sorted retrieved documents into weak orderings with large numbers of documents being given the same score. Consequently he ensured the measure provided an effectiveness score that accounted for these blocks of equally retrieved documents. Although relatively un-used by researchers at the time, ESL was later influential, most notably in the cumulative gain family of measures from Järvelin et al. described in Section 4.2.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Challenging the Assumptions in the Early Collections</head><p>There were a number of common themes to the test collections created in the first few decades of IR research, particularly regarding topics and the definition of relevance. Topics tended to be sentence length statements that mimicked the types of information request issued to librarians. Although not explicitly stated in the literature at the time, there was an assumption that users would query a computerbased IR system in the same way they used the service of a librarian: with a written detailed natural language information request. Early on some researchers pointed out that the assumption was often wrong: Fairthorne stated that users could issue a query that was "an exceedingly ambiguous phrase" <ref type="bibr" target="#b82">[83]</ref>. However, this disparity between test collection queries and actual user queries wasn't addressed for some considerable time (see Section 4.1 for more detail).</p><p>From the initial testing work of Cleverdon and Thorne and of Gull, relevance was assumed to be a form of topical relevance, where a user would judge a document as relevant to an information need if its content either partially or wholly addressed the need. Further, search engine users were assumed to be people who would want to find as much relevant information on a particular topic as possible.</p><p>Although this straightforward notion of relevance persisted in the test collections that were built, many researchers were aware early on of the potential limitations of these assumptions. Verhoeff et al. <ref type="bibr" target="#b264">[264]</ref> stated that it was highly likely that different users could issue the same query but consider different documents as relevant. They proposed that retrieved documents should be ranked based on the probability of a population of users judging those documents as relevant: the more users who considered a particular document relevant, the higher its rank. Goffman, then considered the interdependence of relevance, pointing out that a document may not be viewed as relevant if retrieved documents containing the same information were previously seen by the user <ref type="bibr" target="#b94">[95]</ref>. Such pioneering views on the importance of considering diversity in relevance and redundancy of information was only taken up in earnest much later; see Section 4.1.2.</p><p>Cooper <ref type="bibr" target="#b65">[66]</ref> proposed that there should be a distinction between topical relevance (in his paper he referred to this as logical relevance) and what he called utility. He stated that ultimately an IR system needs to be useful and while it is possible to conceive of systems that retrieve a wide range of documents that have some level of topical relevance to an information need, the more important question to ask was which of those documents were actually useful to the user? Cooper pointed out that the credibility of a source or the recency of a document might be important factors in determining the utility of a relevant document to a user. Later <ref type="bibr" target="#b66">[67]</ref> he suggested that writing style or even a human assessment of document quality could be a factor in utility. See Saracevic for a broader survey <ref type="bibr" target="#b224">[224]</ref> of relevance research.</p><p>With hindsight, it can be seen that such suggestions were important features to consider when designing both test collections and search engines. However, it was many decades before such ideas were put into practice and in the intervening period the challenges made to the core assumptions of test collections were largely ignored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Assessor Consistency</head><p>One of the primary concerns about test collection formation was that the relevance assessments made to form qrels were subjective judgments made by individuals rather than objectively true decisions. An early critic of test collections, Faithorne <ref type="bibr" target="#b82">[83]</ref> argued for relevance judgments made by groups rather than individuals. Katter stated "A recurring finding from studies involving relevance judgments is that the inter-and intra-judge reliability of relevance judgments is not very high" <ref type="bibr" target="#b144">[145]</ref>. With such low levels of agreement, the concern was that effectiveness scores calculated from test collections using a single set of judgments were not accurate or representative. See also <ref type="bibr">Burgin [41]</ref> who detailed past studies on the wide range of influences shown to affect assessment, including the order and way in which documents are presented, the definitions of relevance used, instructions given to assessors, the experience of assessors, etc.</p><p>Lesk and Salton <ref type="bibr" target="#b214">[214]</ref> studied the question of assessor consistency by gathering pairs of relevance judgments for a test collection composed of 1,268 abstracts and 48 queries: judgments from the creator of the query were paired with those of a "subject expert" who assessed documents independently. The researchers evaluated three configurations of a search engine using different combinations of the paired judgments, determining which configuration was the best. The conclusion of their work was that regardless of the judgments used, the ranking of the different versions of the engine always came out the same. Lesk and Salton analyzed the reasons for this consistency and found that although on average, assessor consistency was low; the disparity between assessors was largely to be found for lower ranked documents. They stated that the reason for this result was that top-ranked documents tended to be most similar to the query, therefore judgments about such documents were easier to make. Most of the effectiveness measures used to assess search engines were more influenced by the rank position of top-ranked documents, therefore the ranking of the three configurations tested was consistent across the different qrel sets.</p><p>A similar experiment was conducted by Cleverdon <ref type="bibr" target="#b58">[59]</ref> who worked with the Cranfield II collection and its multiple relevance judgments. Like Lesk and Salton, Cleverdon found variations in assessments but also found that they did not impact on the ranking of different configurations of a retrieval system. Burgin <ref type="bibr" target="#b40">[41]</ref> also looked at a collection with multiple relevance judgments and again confirmed the Lesk and Salton result, though he cited one work (that we have been unable to locate) that was said to show variations across assessors could impact on ranking of systems. Harman <ref type="bibr" target="#b99">[100]</ref> mentioned briefly an assessor consistency experiment that she reported showed an 80% overlap in relevance judgments.</p><p>As a final footnote to this section, it is worth noting that the work here focused on absolute judgments of relevance. Rees and Schultz <ref type="bibr" target="#b193">[193]</ref>, examined the consistency of users at making judgments of documents relative to each other. In this test, they reported "It is evident that the Judgmental Groups agree almost perfectly with respect to the relative ordering of the Documents." (p. 185). This early important observation was noted by a number of other researchers, but little work on capturing or exploiting relative judgments was reported until recently, see Section 6.3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">The Practical Challenges of Creating and Using Test Collections</head><p>In the early years of IR research, there were a series of practical challenges that faced test collection builders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.1">The Challenge of Obtaining Documents</head><p>The only digitized materials widely available for collection construction were catalogue information about document collections. It would appear that obtaining large quantities of full-text was virtually impossible. The only early test collection with complete documents was the Time collection built by Salton's group. It would appear that Salton got students at his institution to transcribe news articles from copies of the actual magazine. The issues used were preserved by researchers at NIST in the United States and are pictured in Figure <ref type="figure">2</ref>.6. Later, Salton and Yu implied another collection, (MEDLARS) was also created through manual transcription <ref type="bibr" target="#b215">[215]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.2">Distribution of Collections</head><p>Although IR researchers were keen to share test collections, the practicalities of sharing could be challenging. Although in the 1970s and 1980s many institutions were connected to each other via some form of network, across the world, a range of different protocols were used to transfer data. Broad adoption of the Internet's TCP/IP beyond North America did not occur until the late 1980s. Removable storage devices such as magnetic tapes were a data transfer option, but a range of formats existed and often only a few devices to read each format were found in an organization such as a University. Because of these obstacles, sharing of collections between research groups was patchy and ad hoc.</p><p>No broadly applicable solution to distribution was found until the early 1990s, when the first large-scale distribution of test collections was achieved with the creation of the Virginia Disc One: a CDROM containing many of the commonly used test collections <ref type="bibr" target="#b85">[86]</ref>. Several hundred copies of the discs were distributed world-wide. <ref type="foot" target="#foot_7">6</ref> With the increased ubiquity of networks and data transfer protocols, by the early 1990s, networked-based distribution of collections started; an early example of which is the University of Glasgow IR group's test collections web page, created in 1994 by Sanderson. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.3">The Challenge of Scale -Limited by qrels</head><p>Commercial IR systems were by the early 1960s searching the subject keywords of several tens of thousands of documents <ref type="bibr" target="#b77">[78]</ref>. By the mid-1970s, it is recounted that searching hundreds of thousands of documents was routine <ref type="bibr" target="#b22">[23]</ref>, yet the test collections of the time were orders of magnitude smaller. A key reason for this appears to be researchers' insistence on knowing the total number of relevant documents for a query.</p><p>Cleverdon, when building Cranfield II, employed people to manually scan the full collection for all relevant documents for all topics. Others investigated less resource intensive approaches. Kent et al. <ref type="bibr" target="#b154">[155]</ref> and later Bornstein <ref type="bibr" target="#b31">[32]</ref> proposed collection sampling to locate some of the missing relevant and estimate the numbers remaining unfound. Maron et al. <ref type="bibr">[173, p. 79</ref>] described a method of using multiple queries generated by searchers to create a set of retrieved documents that were then assessed for relevance, this approach was also suggested by Salton <ref type="bibr">[210, p. 294]</ref>. Lancaster <ref type="bibr" target="#b157">[158]</ref> used subject experts to both search and draw on their knowledge of existing documents to build up what he called the "recall base". Fels <ref type="bibr" target="#b83">[84]</ref> stated that Mooers <ref type="bibr" target="#b180">[180]</ref> proposed creating a form of known item search test collection. The methodology, which Fels tested, involved randomly sampling documents from a collection and creating topics that would be highly relevant, relevant or irrelevant to the sampled document. These topics would then be issued to a searching system and tests of success or failure would be determined by the presence or absence of the known items.</p><p>Despite a plethora of suggestions listed above, however, there appears to be little evidence of researchers actually trying these suggestions to build bigger test collections. Neither does there appear to be a willingness to give up on the notion of finding all relevant documents as advised by Cooper <ref type="bibr" target="#b66">[67]</ref>. The conclusion amongst IR researchers at the time was that a way had to be found to produce larger test collections while at the same time locate as many relevant documents as possible.</p><p>Spärck-Jones and Van Rijsbergen proposed <ref type="bibr" target="#b245">[245]</ref> a methodology for building larger test collections (what they referred to as an ideal test collection). Their proposal was motivated by concern that existing test collections were not only small but often carelessly built and/or inappropriately used by researchers (p. 3 of their report). They proposed the creation of one or more large test collections built using well-founded principles and distributed to the community by a common organization. Addressing the problem of assessors not being able to judge every document in large collections they proposed a solution using a technique they referred to as pooling.</p><p>Spärck-Jones and Van Rijsbergen suggested that for a particular topic, assessors judge the documents retrieved by "independent searches using any available information and device" <ref type="bibr">[245, p. 13]</ref>. Pooling would create a small subset of documents containing a sufficiently representative sample of relevant documents. The relationship of pool size and its impact on the accuracy of comparisons between retrieval systems was analyzed later in some detail by Spärck-Jones and Bates <ref type="bibr">[244, p. A31]</ref>. In order to manage the number of relevance judgments needing to be made, the later report also described random sampling from a pool (see pp. <ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref>.</p><p>The impact of the ideal test collection report was initially limited. Some further small collections were built using exhaustive relevance judgments, such as the CISI <ref type="bibr" target="#b84">[85]</ref> and the LISA collections <ref type="bibr" target="#b75">[76]</ref>. Some collections were built using pooling but did not appear to be aware of Spärck-Jones and Van Rijsbergen's work. Katzer built the INSPEC test collection, composed of 12,684 abstracts <ref type="bibr" target="#b145">[146,</ref><ref type="bibr" target="#b146">147]</ref>. Katzer stated that 84 topics were created for the collection and relevance judgments made on a pool of documents composed of the union of seven searches conducted by search intermediaries. Later Salton augmented the pool with the ranked document lists of two retrieval systems configured to use different ranking algorithms <ref type="bibr">[213, p. 1030</ref>]. Although earlier evaluation was conducted using approaches similar to pooling, such as Lancaster's MEDLAR tests, INSPEC appears to be the first shared test collection built using pooling. Fox described another collection, the CACM, composed of 3,204 documents where Katzer et al.'s seven search variations were used to build a pool for the collection <ref type="bibr" target="#b84">[85]</ref>. <ref type="foot" target="#foot_9">8</ref> Fuhr and Knorzs built a 300 query, 15,000 document collection with pooling <ref type="bibr" target="#b90">[91]</ref>. Blair and Maron <ref type="bibr" target="#b24">[25]</ref> later Blair <ref type="bibr" target="#b23">[24]</ref>, constructed a test collection for estimating the true recall of a Boolean search engine, using a series of broad searches to locate as many relevant documents as possible. None of this work cited Spärck-Jones and Van Rijsbergen.</p><p>Details of these somewhat larger collections are provided in the following table. By contrast, commercial search engines were by that time routinely searching multi-million document collections and calls were made by the industry for the research community to start testing on larger data sets <ref type="bibr" target="#b159">[160]</ref>. A few research groups obtained such collections: researchers in Glasgow used 130,000 newspaper articles for testing a new interface to a ranked retrieval system <ref type="bibr" target="#b218">[218]</ref>; IR researchers at NIST conducted experiments on a gigabyte collection composed of 40,000 large documents <ref type="bibr" target="#b106">[107]</ref>; and Hersh et al. <ref type="bibr" target="#b117">[118]</ref>  Spärck-Jones and Van Rijsbergen's ideal test collection report is often cited for its introduction of the idea of pooling, however, the researchers had more ambitious goals. On page 2 of the report can be found a series of recommendations for the IR research community:</p><p>(1) "that an ideal test collection be set up to facilitate and promote research;</p><p>(2) that the collection be of sufficient size to constitute an adequate test bed for experiments relevant to modern IR systems. . . (3) that the collection(s) be set up by a special purpose project carried out by an experienced worker, called the Builder; (4) that the collection(s) be maintained in a well-designed and documented machine form and distributed to users, by a Curator; (5) that the curating (sic) project be encouraged to, promote research via the ideal collection(s), and also via the common use of other collection(s) acquired from independent projects."</p><p>This vision of larger test collections built by a curating project that fostered their use in research was finally realized in the formation of TREC.</p><p>In 1990, the US government agency DARPA funded the National Institute of Standards and Technology (NIST) to build a large test collection to be used in the evaluation of a text research project, TIP-STER. In 1991, NIST proposed that this collection be made available to the wider research community through a program called TRECthe Text REtrieval Conference. The annual evaluation event started in November, 1992. Operating on an annual cycle, the multiple goals of TREC were to:</p><p>• create test collections for a set of retrieval tasks; • promote as widely as possible research in those tasks; and • organize a conference for participating researchers to meet and disseminate their research work using TREC collections.</p><p>In the early years, TREC organizers annually created gigabytesized test collections, each with 50 topics and a set of qrels built using pooling, see Voorhees and Harman <ref type="bibr" target="#b281">[280]</ref> for a detailed history of the exercise. As can be seen in the overlap between the TREC goals and the Spärck-Jones and Van Rijsbergen recommendations, the ideal test collection document appeared to have influenced the construction of TREC, however, its initiators, headed up by Harman, still had to instantiate them. Key to making TREC a success was their solution to gathering the independent searches that Spärck-Jones and Van Rijsbergen described.</p><p>Harman and her colleagues appear to be the first to realize that if the documents and topics of a collection were distributed for little or no cost, a large number of groups would be willing to load that data into their search systems and submit runs back to TREC to form a pool, all for no cost to TREC. TREC would use assessors to judge the pool. The effectiveness of each run would then be measured and reported back to the groups. Finally, TREC would hold a conference where an overall ranking of runs would be published and participating groups would meet to present work and interact. It was hoped that a slight competitive element would emerge between groups to produce the best possible runs for the pool.</p><p>The benefits of the "TREC approach" were that research groups got access to new test collections; and at the conference, compared their methods against others. The benefits of the approach to TREC were that their chosen area of IR research became a focus of interest among the research community. The benefit of the approach to all was that new test collections were formed annually for all of the IR community to use. Other fields of Human Language Technology used such collaborative/competitive approaches before TREC: e.g., the Message Understanding Conference <ref type="bibr" target="#b95">[96]</ref>. However, the continued running of TREC, now approaching its third decade, is a testament to the particular success of the approach Harman and her colleagues applied to IR.</p><p>TRECs had a profound influence on all aspects of evaluation, from the formatting of test collection documents, topics, and qrels, through the types of information needs and relevance judgments made, to the precise definition of evaluation measures used. In particular, the first eight years of TREC, when the ad hoc track was run, established the norms on which a great deal of other TREC and broader IR evaluation work was based. Consequently that period in TREC is described here in some detail. The section starts with an explanation of how each of the three components of a test collection was created followed by a detailing of some of the tasks that TREC chose to focus on in its early years. Finally, the evaluation measures used are explained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Building an Ad Hoc Test Collection</head><p>The TREC ad hoc collections were built with the searching task of an information analyst in mind: a person who was given topics to search for on behalf of someone else Harman <ref type="bibr" target="#b104">[105]</ref>. The topic given to them was well described and the analyst was expected to locate as much relevant material as possible. The topics for the ad hoc track were created by members of the TREC document assessment team at a rate of 50 per year. The numbers and exact procedures for forming the topics varied over the eight years of TREC ad hoc, Voorhees and Harman <ref type="bibr">[280, p. 28]</ref>. However, certain aspects of the method remained constant. The creators of the topics would create a set of candidate topics, these were then trialed by searching on the ad hoc collections to estimate how many relevant documents each topic would return. Topics with too many or too few relevant documents were rejected <ref type="bibr" target="#b98">[99,</ref><ref type="bibr" target="#b279">278]</ref>.</p><p>TREC topics were structured to provide a detailed statement of the information need that lay behind the query, which was intended to ensure that the topic was fully understood. The topics were formatted into an XML-like scheme (Figure <ref type="figure" target="#fig_4">3</ref>.1), the structure of which varied over the years, but its main components were:</p><p>• a topic id or number; • a short title, which could be viewed as the type of query that might be submitted to a search engine; • a description of the information need written in no more than one sentence; and • a narrative that provided a more complete description of what documents the searcher would consider as relevant.</p><p>Obtaining large quantities of text to build a collection involved persuading the copyright owners of a large corpus of material to allow their content to be used. Through connections with news publishers, TREC organizers obtained US and UK newspaper and magazine articles, as &lt;narr&gt; Narrative: The impact can be positive or negative or qualitative. It may include the expansion or shrinkage of markets or manufacturing volume or an influence on the methods or strategies of the U.S. textile industry. "Textile industry" includes the production or purchase of raw materials; basic processing techniques such as dyeing, spinning, knitting, or weaving; the manufacture and marketing of finished goods; and also research in the textile field. well as US government documents. TREC standardized the gathered documents in a similar XML scheme as used in the topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt;/top&gt;</head><p>The documents and topics were sent out to participating groups who were given a limited time to generate and return a series of runs. Each run contained a maximum of 1,000 top-ranked documents retrieved for each of the TREC topics. The top n documents (most often n = 100, or more recently 50) from each run were merged into the pool to be judged. In order to make pool judgment tractable, TREC organizers sometimes had to limit the number of runs that contributed to the pool. In such situations, participating groups nominated a subset of submitted runs to be assessed.</p><p>TREC defined two types of run:</p><p>• automatic runs, defined as runs where no manual intervention took place between the submission of topics to a group's retrieval system and the outputting of the run. • manual runs, where any amount of human intervention in the generation of search output was allowed. For some manual runs, the list of documents submitted was a concatenation of the best results from multiple queries. For details of how individual manual runs were created, see Voorhees and Harman's overview of one of the years of TREC (e.g., <ref type="bibr" target="#b98">[99,</ref><ref type="bibr" target="#b99">100,</ref><ref type="bibr" target="#b100">101,</ref><ref type="bibr" target="#b278">277,</ref><ref type="bibr" target="#b279">278]</ref>). Although such runs appeared to have limited scientific value, TREC organizers encouraged their submission as they were found to be rich sources of relevant documents for the pool. Kuriyama et al. <ref type="bibr" target="#b155">[156]</ref>, showed the importance of manual searching in effective pool formation.</p><p>In order to be seen to be fair to all participants, TREC assessors viewed all top n documents in the pool; documents were sorted by docid so that the rank ordering of documents did not impact on the assessment. TREC organizers tried to ensure that the creator of the topic was also the assessor of its qrels. Unlike a number of earlier test collections, which had degrees of relevance, in TREC, assessors made a binary relevance judgment. They were instructed that "a document is judged relevant if any piece of it is relevant (regardless of how small the piece is in relation to the rest of the document)", <ref type="foot" target="#foot_11">1</ref> which resulted in a liberal view of what documents were viewed as relevant.</p><p>TREC, particularly, its ad hoc collections continue to have a profound impact on the academic IR community. The collections are used extensively: a search on a well-known scholarly search engine (conducted in May 2010) revealed that the phrase "TREC collection" occurred in nearly 1,210 papers; in a small survey conducted for this paper, examining 40 of the 60 papers in ACM SIGIR 2004, 28 of the papers used TREC collections (70%), 17 of which used at least one of the ad hoc collections (43%). These multi-gigabyte data sets became the de facto standard on which many new ideas were tested.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Classic TREC Ad hoc Measures</head><p>TREC was not only influential on test collections used by researchers but also on the effectiveness measures used. Although many measures were calculated by TREC organizers, three, MAP, R-precision, and MRR are commonly used in the broader community. Precision measured at a fixed rank, P (n), although used before TREC was another measure adopted by the evaluation exercise and an important result comparing the properties of MAP and P (n) was described for the first time at TREC meetings. The measures are detailed here. Following the chronological ordering of this review, more recent ad hoc measures are described later in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Average Precision</head><p>In the first year of the exercise, TREC organizers calculated 11-point average precision using Keen's interpolation function. However, perhaps because weak orderings of rankings were less common by the early 1990s, this was soon replaced by a non-interpolated version. The first reference to this measure was in Harman <ref type="bibr" target="#b98">[99]</ref>, where the measure was called non-interpolated average precision (AP). It is defined as follows:</p><formula xml:id="formula_4">AP = N rn=1 (P (rn) × rel(rn)) R .</formula><p>Here, N is the number of documents retrieved, rn is the rank number; rel(rn) returns either 1 or 0 depending on the relevance of the document at rn; P(rn) is the precision measured at rank rn; and R is the total number of relevant documents for this particular topic. Simply, the measure calculates precision at the rank position of each relevant document and takes the average. Note, by summing over the N and dividing by R, in effect, precision is measured as being zero for any unretrieved relevant documents. This measure is similar to normalized precision <ref type="bibr">[210, p. 290]</ref>.</p><p>If one calculates AP for each of a set of topics and takes the mean of those average precision values, the resulting calculation is known as Mean Average Precision (MAP). Harman's original definition was published with a mistake, replacing the denominator R, with the number of relevant documents retrieved; a journal version of the paper contained the same error <ref type="bibr" target="#b101">[102]</ref>. Voorhees appeared to be the first to describe the measure as mean average precision <ref type="bibr" target="#b267">[267]</ref>, though it took several years for MAP to become its universally accepted name. MAP became one of the primary measures used in many evaluation exercises as well as a large quantity of published IR research.</p><p>Note that, interpolated average precision (described in Section 2.3.1.) was often in older literature referred to as average precision or AP, which can cause confusion for the modern reader. Occasionally, more recent papers and books appear to use interpolated AP where it would appear that the authors were unaware of the existence of the more established non-interpolated version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Measuring Precision at a Fixed Ranking</head><p>A common option for measuring precision is to decide that a user will choose only to examine a fixed number of retrieved results and calculate precision at that rank position.</p><formula xml:id="formula_5">P (n) = r(n) n ,</formula><p>where r(n) is the number of relevant items retrieved in the top n. The choice of n is often influenced by the manner of their display, P <ref type="bibr" target="#b9">(10)</ref> being the commonest version. Sometimes the measure P( <ref type="formula">1</ref>) is used and referred to as the Winner Takes All (WTA) measure. Precision measured at a fixed rank has long been described in the literature. Both Salton <ref type="bibr" target="#b210">[210]</ref>, and Van Rijsbergen <ref type="bibr" target="#b262">[262]</ref> mentioned calculating precision at a fixed rank. However, both described the measure in the context of producing graphs of precision over a range of ranks. Neither described the measure in the way it was used later on: a single value measured at one point in the ranking. Note P(n) ignores the rank position of relevant documents retrieved above the cutoff and ignores all relevant below. Also if a topic has fewer than n relevant documents in the collection being searched, P(n) for that topic will always be &lt; 1. However, there is little evidence these features of the measures are problematic.</p><p>However, there is one feature that is worth noting, the importance of which was described in one of the later years of TREC. Computing precision at a fixed rank ignores the total number of relevant documents there are for a topic. This number can affect the expected value of precision calculated at a fixed rank n. To illustrate, if we calculate P(10) on the ranks resulting from retrieval on two topics, one with 1,000 relevant documents, the other with 10. For both topics, the balance of relevant and non-relevant documents will change across the resulting ranks. However, in the first topic, it should be relatively straightforward for a retrieval system to place a great many relevant documents in the top 10. For the second topic, the chances are that there will be fewer easy to retrieve relevant documents; consequently, it will be harder for a retrieval system to fill the top 10 with just relevant. In other words, IR systems are likely to get a high P(10) on the first topic and a low P(10) on the second. An improved system finding one more relevant documents for the first topic will score the same increase in P(10) as another system that finds one more relevant for the second topic, even though locating the extra relevant for the second topic was most likely algorithmically harder to achieve.</p><p>When evaluating within a particular test collection there does not appear to be any published evidence that this feature of the measure causes problems. However, there have been evaluations across two test collections, where differences in measurement arose. The effect was highlighted during the running of the Very Large Collection (VLC) track of TREC-6 <ref type="bibr" target="#b114">[115]</ref>. Participating groups applied their retrieval systems to a 20 GB ad hoc collection and a 2 GB subset. Effectiveness was measured using P <ref type="bibr" target="#b19">(20)</ref>. Across all seven participating groups, P(20) was higher for searches on the 20 GB collection than on the subset; on average 39% higher. Hawking and Thistlewaite noted the increase, but did not study it. The following year the track was re-run, using a larger 100 GB collection along with 1% and 10% subsets, a similar increase in P(20) was noted <ref type="bibr" target="#b111">[112]</ref>.</p><p>Reasons for the increase were discussed at the TREC meeting. Consequently, Hawking and Robertson <ref type="bibr" target="#b113">[114]</ref> studied the results in detail, postulating a number of hypotheses. They concluded that the core reason was that searching on a larger collection resulted in there being more relevant documents per topic and a consequent increase in the number of such documents that could be highly ranked. As a means of final confirmation, Hawking and Robertson examined the effectiveness of the systems participating in the VLC track using MAP, which measures precision across all retrieved and un-retrieved relevant documents. With this measure, no increase in effectiveness was observed.</p><p>Given that P(20) behaved completely differently from MAP, one might ask, is one measure better than the other? Hull discussed the qualities of the two approaches <ref type="bibr" target="#b124">[125]</ref>. He pointed out that the answer to which is the best, depends on how users are going to use a search engine. If the user is (like most web searchers) focused on obtaining a few relevant documents and examined only the first page of results, then a fixed rank version of precision seems more appropriate. If a search engine was able to increase the size of the collection it retrieved over, such users would view the resulting increase in relevant documents in the first page of a search engine as a clear improvement.</p><p>The situation would be different if the users of the system were, for example, patent searchers whose goal was to locate every relevant document in the collection. When the collection being searched was increased in size, such searchers would probably value the growth in the total number of relevant documents, but they might not view the engine as having improved because across all the relevant documents viewed by such a thorough searcher, the proportion of non-relevant to relevant would be unchanged.</p><p>Here, the rank cutoff version of precision appears to be the better choice in most situations. As will be seen later in Section 6.4, it would be rash to assume one version is always better than another: when the measures are compared in other contexts or used for alternate purposes, different conclusions are often drawn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">R-precision</head><p>Instead of calculating precision over the same fixed rank for a set of topics, one could use a different cut off for each topic; R-precision uses this approach. It is calculated as P(R), where R is the total number of relevant documents in the collection for a particular topic. Precision is calculated at the rank position where a perfect retrieval system would have retrieved all known relevant documents, a more consistent recall level than a fixed rank. The measure was first used in TREC-2 ([99], Appendix A).</p><p>Note that at the point R, the number of relevant documents ranked below R will always equal the number of non-relevant documents ranked above R, which has led others to refer to R as the equivalence number and called R-precision missed@equivalent <ref type="bibr" target="#b189">[189]</ref>. Note also that all forms of AP and, as pointed out by Aslam et al. <ref type="bibr" target="#b13">[14]</ref>, R-precision approximates the area under a recall precision graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Searching for a Single Relevant Document</head><p>Known item search describes retrieval based on topics that have one relevant document in the collection being searched. It was first described in Thorne's original test collection paper <ref type="bibr" target="#b257">[257]</ref>. Mean Reciprocal Rank (MRR) was created by Kantor and Voorhees 2 <ref type="bibr" target="#b142">[143]</ref> to assess such retrieval. The measure calculates the reciprocal of the rank of the first relevant document in a ranking.</p><p>The Reciprocal Rank (RR) calculated over the four example rankings shown in Figure <ref type="figure" target="#fig_4">3</ref>.2, is respectively, 1, 0.5, 0.5, and 0. Note how the measure is particularly sensitive to small changes in the location of the relevant document at top ranks: the RR for the second example is half of that of the first. Conversely, the measure is insensitive to large difference in low rank. Because any other relevant documents in a ranking are ignored, the RR is the same for the second and third examples. The MRR has been used in some evaluations, for example, it was used in a known-item search task in TREC <ref type="bibr" target="#b280">[279]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5">Standardizing Measure Calculation</head><p>The organizers of TREC recognized that another important role it could play was to act as a supplier of a standard tool to calculate the effectiveness of a retrieval run. Thus it did with the public release of trec eval: an application that, given a run and a set of qrels, calculates an extensive range of effectiveness measures over the run. The tool is used by many research groups and is generally viewed as holding 2 See also Kantor and Voorhees <ref type="bibr" target="#b143">[144]</ref> for a more complete description of the work.</p><formula xml:id="formula_6">Rank Rel Rank Rel Rank Rel Rank Rel 1 1 1 0 1 0 1 0 2 0 2 1 2 1 2 0 3 0 3 1 3 0 3 0 4 0<label>4</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The Other TREC Tracks and Uses of TREC Collections 285</head><p>the definitive definition of many of the measures used by the IR community.<ref type="foot" target="#foot_13">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The Other TREC Tracks and Uses of TREC Collections</head><p>Although the test collections and other associated data resulting from TREC ad hoc is the strongest legacy of TREC in the 1990s, test collections for many other documents types were developed at the same time. Voorhees and Harman <ref type="bibr">[280, pp. 8-9]</ref> detailed both the names and nature of them in their book, some of the more significant tracks focused on:</p><p>• categorizing and/or retrieving streamed text, as addressed in the routing and filtering tracks <ref type="bibr" target="#b200">[200]</ref>; • medical scholarly articles in the TREC-based genomics collection where matching to variants of gene names became a part of the search task <ref type="bibr" target="#b118">[119]</ref>; • search across languages with English queries retrieving Spanish and Chinese documents, as covered in the cross language search tracks <ref type="bibr" target="#b230">[230]</ref>; and • retrieval of noisy channel data, output by OCR and speech recognizer systems, addressed in the Confusion <ref type="bibr" target="#b143">[144]</ref> and Spoken Document Retrieval tracks <ref type="bibr" target="#b91">[92]</ref>. • Later, within the field of distributed IR, groupings of TREC ad hoc collections were established into a set of commonly used collections by that research community. Shokouhi and Zobel <ref type="bibr" target="#b229">[229]</ref> detail six such collated collections and the researchers who initially built them.</p><p>Some tracks established their own evaluation measures or methods -see the measures in the filtering track overview <ref type="bibr" target="#b200">[200]</ref> or the differing methods of the interactive track <ref type="bibr" target="#b186">[186]</ref>. However, across the majority of the TREC tracks in this period, the type of search task established in the ad hoc track was highly influential on the topic design, definition of relevance, evaluation measures, and pooling methodologies used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Other Evaluation Exercises</head><p>The success of TREC inspired many others to start similar evaluation exercises:</p><p>• CLEF<ref type="foot" target="#foot_14">4</ref> -The annual Cross Language Evaluation Forum focuses on search across European languages; though in recent years it diversified into other languages including Persian and some of the languages of the Indian sub-continent <ref type="bibr" target="#b32">[33]</ref>. Search of other objects such as images has also been addressed: imageCLEF <ref type="bibr" target="#b63">[64]</ref>. • NTCIR -The NII Test Collection for IR Systems is an evaluation exercise held every 18 months in Japan. NTCIR has focused on cross-language search for Asian languages such as Japanese, Chinese, and Korean. A particular focus was on patent search <ref type="bibr" target="#b140">[141]</ref>. The first NTCIR evaluation exercise used a collection of the title and abstracts of several hundred thousand scholarly articles <ref type="bibr" target="#b141">[142]</ref>. • TDT -Topic Detection and Tracking was an exercise examining the automatic detection and tracking of important emerging stories in streaming text <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• INEX -The INitiative for the Evaluation of XML Retrieval</head><p>examines the retrieval of semi-structured documents, in particular focusing on retrieval of document fragments <ref type="bibr" target="#b156">[157]</ref>. • TRECVID -an evaluation exercise focused on video retrieval <ref type="bibr" target="#b232">[232]</ref>. • A number of other smaller and/or newer evaluation exercises were created, a number of which presented their work at the First International Workshop on Evaluating Information Access <ref type="bibr" target="#b219">[219]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">TREC's Run Collection</head><p>In addition to the test collections, topics and qrel sets generated each year by TREC, the runs (ranked lists of the documents retrieved for each topic) submitted by participating groups for each track were also archived. In the ad hoc track for TRECs 2-8, nearly 500 runs were archived. As will be seen in Section 6, this archive opened up new opportunities for research examining the impact of different evaluation measures and for exploring the effectiveness of test collection formation methodologies. Other evaluation exercises also archived their run data, see Sakai <ref type="bibr" target="#b206">[206]</ref> for use of NTCIR runs and Sanderson et al. <ref type="bibr" target="#b221">[221]</ref> for an example of use of runs from CLEF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">TREC Ad Hoc: A Great Success with Some Qualifications</head><p>TREC and its spin-off evaluations had a profoundly positive impact: providing large-scale test collections, a pooling method, evaluation measures, and other data sets to a research community that up to the formation of TREC did not appear to have the appetite or resources to build its own. The collections, particularly those from the ad hoc track, are extensively used. Ten years after the track stopped, it is still common to see the collections exploited in high impact research. The inaugural running in 2008 of the Indian languages evaluation exercise, FIRE (Forum for IR Evaluation) used collections and a topic design strongly influenced by the TREC ad hoc paradigm <ref type="bibr" target="#b169">[170]</ref>. Because the ad hoc collections and methodologies continue to be widely used, it is worth reviewing some of the methods employed in those early years, focusing on collections, topics, and relevance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.1">Collections</head><p>The documents of test collections in this period were commonly newspaper articles. This tradition started with Salton's TIME collection, but was carried on by TREC and later other evaluation campaigns. TREC started in 1992. In late 1993, public web search engines were being created <ref type="bibr">[154, p. 152]</ref>; by the summer of 1994, a large number were in existence including the relatively well-known Lycos and Excite. However, TREC and the broader academic IR research community maintained their focus on search from almost nothing but newspaper and government documents for much of the 1990s. Using this form of content had a strong influence on the type of topics that were created for the test collections, which in itself limited the range of search tasks that were addressed by IR researchers in the 1990s. Some have criticized TREC organizers for not moving more quickly to the study of web search. However, it is worth considering at least one of the contributing factors to this situation. The building of web and particularly enterprise collections for use in a large-scale evaluation is a difficult legal and privacy challenge. A web crawler has no automatic way of knowing the copyright status of the pages it is downloading. Just because an item is placed on a freely accessible web page does not mean the creator of that page is giving permission to others to copy and redistribute it. Although now, precedent has established that crawling and storing most web content is unlikely to be a copyright violation, in the 1990s this was not as clear. TREC took a cautious legal approach to such matters, which was undoubtedly a factor in the delay in adapting to Web tasks.<ref type="foot" target="#foot_15">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.2">Topic Selection</head><p>As a general rule one would expect the components of a test collection to be a sample of the documents and queries typically submitted to an IR system. When creating their original testing environment, Thorne <ref type="bibr" target="#b257">[257]</ref> suggested sampling from a log of queries for example. The processes used for topic selection in the ad hoc track of TREC were intended to obtain a representative sample. However, there was no log of existing searches on the document collections being built, therefore, topics had to be created. As described in Section 3.1, certain topics with too few or too many relevant documents were avoided; the later because of concerns that relevance assessors would be overwhelmed with documents to assess. However, by focusing only on topics that had a middling number of relevant documents associated with them, there was a danger of introducing bias into the topic sets.</p><p>Although TREC topics had a title, which mimicked a short user query, groups commonly submitted runs that were based on a combination of the topic's title and description fields. However, Rose and Stevens <ref type="bibr" target="#b204">[204]</ref> described research based on query logs of web search engines showing that 53% of queries consisted of just one word; far shorter than the TREC topic titles of the time. As a reaction to this, the length of topic titles became shorter in subsequent years of TREC and participating groups were encouraged to submit runs based only on titles; as detailed by Voorhees and Harman <ref type="bibr">[280, p. 39]</ref>. Nevertheless many still used the full-text of the topics in experiments despite their apparent lack of realism.</p><p>It is notable that while the length of topics was addressed, ambiguity was consciously avoided: in the TREC-5 overview, it was stated that "Candidate topics were . . . rejected if they seemed ambiguous" and this approach persisted for many years, not just by TREC, but by most other evaluation campaigns. Attempts at building such a collection in the interactive track of TREC were made in the 1990s <ref type="bibr" target="#b187">[187]</ref>, however, the methodologies used there were not picked up by others in TREC or the wider search evaluation community until much later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.3">Binary Relevance</head><p>For many of TRECs early years, the focus was on topics locating as much information as possible even to the point of seeking documents with only a single relevant sentence. Sormunen <ref type="bibr" target="#b240">[240]</ref> re-examined TREC relevance judgments for 38 topics from TREC-7 and 8 using three relevance grades: not relevant, marginally relevant and highly relevant. He reported that about 50% of the relevant documents were marginally relevant and questioned if it was right for these commonly used test collections to be so strongly composed of this form of relevant document. While giving a talk about test collections and the TREC definition of relevance in 2000, a member of the audience who, at the time, was working for part of the UK intelligence community claimed (to me) that TREC's definition was the same as the one used by intelligence analysts in his organization. It would appear that this liberal definition of relevance was an appropriate definition for the information analyst task that TREC ad hoc was originally created for. Whether it was an appropriate definition for many of the information-seeking tasks carried out by other users is perhaps open to question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.4">Summing Up</head><p>It is worth reiterating that these qualifications are in the context of a highly successful on-going evaluation exercise. It is in many ways because of the success of TREC that the issues are highlighted here. TREC test collections particularly ad hoc were not only used but also imitated. Although as will be seen in the next section, TREC organizers and many others moved to address the problems listed here, there is a danger that other test collection creators sometimes too faithfully reproduced the early TREC approach. The user of any test collection would do well to examine overview documentation to understand fully the way the collection they intend to experiment on was built.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Conclusion</head><p>In this section, the initial development of large-scale test collections using a pooling approach for building qrels was described and the measures used to assess effectiveness were described. The innovation primarily from TREC of keeping run data and encouraging its use for a new form of experimentation was also described, before finally detailing some of the concerns over ad hoc test collection design.</p><p>Although ad hoc-style test collections continue to be used and created, from the early 2000s, development of new test collections started with different document content, addressing new tasks, and beginning to employ novel evaluation measures. One of the motivations for this was a realization that TREC's ad hoc design had limitations.</p><p>An example of this appeared when some unusual results emerged from an early TREC web test collection: <ref type="bibr" target="#b109">[110,</ref><ref type="bibr" target="#b110">111]</ref>. Although the collection content was different from classic ad hoc collections, the topics were similar to those used in the past. One striking result from these collections was that link-based methods, such as PageRank, appeared to provide little or no value. Broder pointed out that many web queries were different from the classic information seeking view of search <ref type="bibr" target="#b33">[34]</ref>. So-called navigational queries where users seek a home page did benefit from link-based methods, but they weren't present in the existing TREC web collections. Consequently, the organizers introduced different types of topics into subsequent collections, focusing particularly on locating named home pages. This was later generalized into the socalled topic distillation task: finding a series of home pages relevant to a particular topic.</p><p>It was now clear that testing different searching applications required different document collections and different types of searching task. In this section, the tasks and collections that were introduced in this post ad hoc period are described, followed by details of the new measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">New Tasks, New Collections</head><p>At the same time that the TREC web track was being developed, the Question Answering (QA) track was created. Here the search task was for a QA system to interpret an inputted question and locate passages within documents that answered the question <ref type="bibr" target="#b269">[269]</ref>. Searching for passages within documents was also explored in other QA tracks run in other evaluation exercises <ref type="bibr" target="#b168">[169]</ref> as well as the novelty track of TREC <ref type="bibr" target="#b102">[103]</ref>. Another evaluation exercise to explore search of document parts was INEX (INitiative for the Evaluation of XML Retrieval), where search of different collections of XML data was examined <ref type="bibr" target="#b89">[90]</ref>. Here the focus was on evaluating searching systems for not only their ability to retrieve relevant structured documents, but also to locate the best point in a document's structure where a user could start reading the relevant part of the document.</p><p>Search of new web-based content was examined in the blog track of TREC, where several hundred thousand blog feeds were crawled to form collections composed of millions of postings. Here the search tasks examined a form of topic distillation to locate relevant feeds addressing a particular topic. Blog topics were sampled from a blog search engine log. In addition, detection of the opinions expressed in blogs became part of the search task <ref type="bibr" target="#b185">[185]</ref>.</p><p>Search tasks associated with organizations were explored in the Enterprise track, which examined search of email discussion threads as well as using a multi-faceted collection of documents from an organization to create a search task for location of experts in particular topics <ref type="bibr" target="#b69">[70]</ref>. Multimodal search tasks started to be explored first off in the video track of TREC -TRECVid <ref type="bibr" target="#b233">[233,</ref><ref type="bibr" target="#b231">231]</ref> -and then in the image searching track of CLEF -ImageCLEF <ref type="bibr" target="#b63">[64]</ref>. In both cases, topics were specified as a combination of text and examples of the media sought.</p><p>New types of high recall search were also examined. In 2002, NTCIR started a long-term examination of patent retrieval <ref type="bibr" target="#b128">[129]</ref>, considering a range of different search tasks. In this domain, location of all relevant past material was important. Aspects of patent search were more recently taken up by CLEF evaluation exercise <ref type="bibr" target="#b202">[202]</ref>. Search supporting e-discovery was examined in the TREC legal track <ref type="bibr" target="#b21">[22]</ref>, another area of IR where the users (e.g., lawyers) wish to find all relevant items.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Moving Beyond Binary Relevance</head><p>From the start of test collection development, there were collections with multiple levels of relevance, Cleverdon's Cranfield I collection <ref type="bibr" target="#b57">[58]</ref> had ternary judgments: relevant, partially relevant or not relevant. However, the qrels of most ad hoc style test collections used binary relevance judgments; see Kando et al. <ref type="bibr" target="#b141">[142]</ref> and Oard et al. <ref type="bibr" target="#b184">[184]</ref>, as notable exceptions. This situation changed as researchers started to report that retrieval techniques that worked well for retrieving highly relevant documents were different from the methods that worked well at retrieving all known relevant documents <ref type="bibr" target="#b271">[271]</ref>.</p><p>There was also a realization that degrees of relevance were commonly being used in the internal test collections of web search companies. To illustrate, White and Morris <ref type="bibr">[284, p. 256</ref>] mentioned a form of test collection within Microsoft with relevance judgments "assigned on a six-point scale by trained human judges". Carterette and Jones <ref type="bibr" target="#b49">[50]</ref> described a collection within Yahoo! used for advertising retrieval with five levels of relevance ("Perfect, Excellent, Good, Fair, and Bad"); and Huffman and Hochster <ref type="bibr" target="#b123">[124]</ref> described work in Google where assessors judged relevance of retrieved documents on a "graduated scale". Note, although the dates of these publications are more recent, they are the best examples found of the companies revealing some aspects of their testing work; it is thought very likely that this approach to relevance has been used for sometime in search companies.</p><p>Consequently, degrees of relevance become more common in test collections. For example, a ternary scheme was used in the web track of TREC <ref type="bibr" target="#b70">[71]</ref>; degrees of relevance were used in TREC's Enterprise track as well as the Blog track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Diversity</head><p>As described at the start of Section 2, the origins of IR test collections lay in methods developed for measuring the effectiveness of library cataloguing systems, where users wrote detailed information requests for librarians who would do the actual searching. However, Verhoeff et al. <ref type="bibr" target="#b264">[264]</ref>, Fairthorne <ref type="bibr" target="#b82">[83]</ref> and Goffman <ref type="bibr" target="#b94">[95]</ref>, respectively, pointed out that users' definitions of relevance were diverse, queries were commonly ambiguous, and that the relevance of a document to a query could be strongly influenced by the documents already retrieved. However, test collections topics continued to follow the tradition of being detailed unambiguous statements of an information need for which one view of relevance was defined and the relevance of a document was assessed independent of other documents.</p><p>An early attempt to create topics with multiple distinct notions of relevance was the interactive track of TREC, which over several years, built a collection composed of 20 topics, each with relevance judgments that addressed multiple aspects <ref type="bibr" target="#b187">[187]</ref>. This collection was widely used in diversity research. Following on from this, the novelty track of TREC <ref type="bibr" target="#b102">[103]</ref> and the QA track <ref type="bibr" target="#b272">[272]</ref> both encouraged the building of systems that retrieved fragments of documents (sentences for the novelty track, so-called nuggets for QA) that were both relevant and had not previously been seen. More recently, a collection addressing search of ambiguous person names was created <ref type="bibr" target="#b10">[11]</ref>. Both Clarke et al. and Sanderson et al. described re-using existing test collections for diversity research: Clarke re-using a TREC QA collection <ref type="bibr" target="#b54">[55]</ref>; Sanderson, an image search collection <ref type="bibr" target="#b221">[221]</ref>. Liu et al. <ref type="bibr" target="#b166">[167]</ref> described building a web test collection of ambiguous queries, where they examined search engine effectiveness against different levels of ambiguity in the queries.</p><p>This relatively small number of test collections addressing diversity is likely to change in the coming years, as web, blog, and image collections were used in the 2009 runs of TREC and CLEF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Post Ad hoc Measures</head><p>With the new collections, tasks and relevance judgments, came a need for new effectiveness measures. Note, only the prominent measures are described here, others exist, see Demartini and Mizzaro for a tabulation of a great many <ref type="bibr" target="#b76">[77]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Grades of Relevance</head><p>Measures such as Precision can only be used with binary judgments, though using a threshold, one can map n-ary judgments to a binary scheme. Cleverdon used such a mapping in his early test collection work (the two sets of points in the graph in Figure <ref type="figure">2</ref>.1 reflect results calculated with different thresholds). Researchers were aware that commonly used evaluation measures failed to consider the degrees of relevance and suggested alternatives: Pollock <ref type="bibr" target="#b191">[191]</ref> conducted notable early work in this area, aspects of which are described later in this sub-section.</p><p>Assuming that one can transform relevance judgments on documents to numerical values, Järvelin and Kekäläinen <ref type="bibr" target="#b131">[132]</ref> proposed a suite of measures that evaluated the effectiveness of a retrieval system regardless of the number of levels of relevance. Their simplest measure, Cumulative Gain (CG), is the sum of relevance values (rel) measured in the top n retrieved documents. Note, Cooper's ESL measure <ref type="bibr" target="#b64">[65]</ref> operated in a similar way, though the number of non-relevant documents, instead of relevant, was counted.</p><formula xml:id="formula_7">CG(n) = n i=1 rel(i).</formula><p>Examining some example rankings: in the left hand rank in Figure <ref type="figure">4</ref>.1, CG(5) = 6. Because CG ignores the rank of documents we see that this is also the value of CG in the poorer rank on the right in Figure <ref type="figure">4</ref>.1. However, Discounted Cumulative Gain(DCG), where the relevance values are discounted progressively as one moves down the document ranking, used a log-based discount function to simulate users valuing highly ranked relevant documents over the lower ranked. and the DCG scores for each rank position. We see that DCG(5) of the left hand rank in Figure <ref type="figure">4</ref>.2 is 4.7 and 3.6 in the right hand poorer rank. In a follow-up paper Järvelin and Kekäläinen <ref type="bibr" target="#b132">[133]</ref> added a third measure, normalized DCG (nDCG). Here DCG was normalized against an ideal ordering of the relevant documents, IDCG, see Figure <ref type="figure">4</ref>.3.</p><formula xml:id="formula_8">DCG(n) = rel(1) + n i=2 rel(i) log b (i)</formula><formula xml:id="formula_9">nDCG(n) = DCG(n) IDCG(n) .</formula><p>The value of nDCG ranges between 0 and 1. The nDCG(5) of the left and right rankings in Figure <ref type="figure">4</ref>.2 are 4.7/5.1 = 0.92 and 3.6/5.1 = 0.71. Note, Pollock worked on graded relevance, he proposed a measure that computed normalized cumulative gain <ref type="bibr" target="#b191">[191]</ref>. Al-Maskari et al. <ref type="bibr">[3]</ref> pointed out that in certain circumstances nDCG can produce unexpected results. To illustrate, for both rankings in Figure <ref type="figure">4</ref>.4, there are only three known relevant documents, though the topic on the left has three highly relevant documents, the other has three partially relevant documents. For both topics the rankings are ideal, so the nDCG (DCG ÷ IDCG) in both cases is 1, which is perhaps a counterintuitive result. Burges et al. <ref type="bibr" target="#b39">[40]</ref> described a version of nDCG, for which the DCG component more strongly emphasized the high ranking of the most relevant documents:</p><formula xml:id="formula_10">DCG(n) = n i=1 2 rel(i) -1 log(1 + i) .</formula><p>A series of other graded relevance measures were proposed in recent times: Sakai <ref type="bibr" target="#b205">[205]</ref> detailed and compared the properties of a number of them including Average Weighted Precision (AWP) and Q-measure. The measures were used by NTCIR organizers as graded relevance was a common feature of the test collections produced by that evaluation exercise. Järvelin and Kekäläinen reviewed many other proposed measures <ref type="bibr" target="#b132">[133]</ref>. Moffat and Zobel <ref type="bibr" target="#b179">[179]</ref> argued that the log-based discount function in DCG was not the best model of users' behavior when browsing a ranked list of documents. They constructed a model based on the probability p that a user progresses from one document in the ranking to the next. A high p models a persistent searcher; a low p models a fleeting one. The probability was incorporated into a geometric discount function forming the Rank-Biased Precision (RBP) measure</p><formula xml:id="formula_11">RBP = (1 -p) • d i=1 rel(i) • p i-1 ,</formula><p>where d was the depth of rank one wished to compute the measure over.</p><p>Although it is rarely discussed in the literature, when one has grades of relevance in a set of qrels, one could view measuring the effectiveness of a retrieval system as a comparison of ranked lists: the retrieved ranking compared with the qrels ranked by their relevance. This idea was suggested by Pollack <ref type="bibr" target="#b191">[191]</ref>. Joachims implemented the idea using Kendall's τ <ref type="bibr" target="#b152">[153]</ref> to measure the correlation between the two ranks so as to obtain a measure of effectiveness <ref type="bibr" target="#b136">[137]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Managing Unjudged Documents</head><p>When pooling approaches were introduced into test collection formation, the relevance judgments that accompanied the collections were composed of lists of documents that were judged relevant or not relevant. There was, however a third class of document: the unjudged, documents not in the pool. An early mention of unjudged documents can be found in Hersh et al. <ref type="bibr" target="#b117">[118]</ref>, where the researchers stated that they chose to ignore such documents when calculating effectiveness measures. A more common approach was to assume that unjudged documents were not relevant. Büttcher et al. <ref type="bibr" target="#b42">[43]</ref> pointed out that in many situations this is a sensible approach.</p><p>However, Buckley and Voorhees <ref type="bibr" target="#b37">[38]</ref> stated that there are other situations where unjudged documents were a potential problem. They were concerned that the size of pools relative to the size of collections was reducing as test collections grew. A related concern was that some test collections were created from sets of documents that were subsequently updated. If no new judging was done after an update, the pool would effectively reduce in size relative to the collection. Therefore, Buckley and Voorhees considered if a new evaluation measure could be devised that better estimated the effectiveness of a system when there were a large number of unjudged documents. They devised BPref, so-called as it "uses binary relevance judgments to define the preference relation". It is defined as follows:</p><formula xml:id="formula_12">BPref = 1 R r 1 - |n ranked higher than r| min(R, N ) ,</formula><p>where R is the number of documents judged relevant for a particular topic; N is the number of documents judged not relevant; r is a relevant retrieved document; and n is a member of the first R irrelevant retrieved documents. The measure considers a bounded number of judged nonrelevant documents, determining the fraction of these documents that are ranked higher than r. The numerator captures relevance in terms of preference (n ranked higher than r). Although not the first measure to consider preference -see Frei and Schäuble's usefulness measure <ref type="bibr" target="#b87">[88]</ref> -BPref is the first commonly used measure to which preference judgments could be easily applied.</p><p>In their 2004 paper, Buckley and Voorhees stated that the formulation of BPref was arrived at empirically after a number of experiments. Note the definition shown is from Soboroff's paper <ref type="bibr" target="#b237">[237]</ref>. It supersedes the original definition for BPref and its variation BPref-10 and is the default definition of BPref used in recent years by TREC and its trec eval system (from version 8 onwards). However, such is the popularity of the earlier paper the previous definitions persist in the research community.</p><p>Buckley and Voorhees devised a series of simulated experiments comparing the stability of BPref with P(10), R-precision, or MAP. They stated that BPref's greater stability was due to its ignoring the increasing numbers of unjudged documents, when the other measures treated these documents as not relevant.</p><p>Soon after, Yilmaz and Aslam <ref type="bibr" target="#b290">[289]</ref> described a number of alternative effectiveness measures also built to handle un-judged documents including induced AP (indAP) and inferred AP (infAP). In Bpref, Buckley and Voorhees's aim was to create a measure that mimicked MAP as closely as possible, which was also Yilmaz and Aslam's aim. Unlike BPref, however, their two AP measures were more directly related to the formulation of MAP and in the presence of complete relevance information, resulted in the same score as MAP. Their second measure, infAP is the more widely used of the two and is described here in more detail.</p><p>Yilmaz and Aslam split the unjudged documents of a run into two sets: based on whether the documents would or would not have contributed to the test collection's pool had the run been used to build the collection. For the later set, the unjudged documents were assumed to be non-relevant; for the former set, infAP calculated the proportion of judged relevant and non-relevant documents in the document ranking for that topic and assumed that this proportion was the probability that unjudged documents were relevant. For example, in most of the TREC test collections, pools were formed from the top 100 documents of each submitted run. For such a collection, infAP measured at rank position k would be formulated as follows:</p><formula xml:id="formula_13">infAP (k) = 1 R r 1 k + (k -1) k |d100| k -1 • |rel| + ε (|rel| + |nonrel| + 2ε)</formula><p>.</p><p>Here the definitions of R and r are the same as BPref; |d100| is the number of judged documents found above rank k plus the number of unjudged documents above rank k that would have contributed to the document pool; |rel| is the number of documents above rank k that are judged relevant; and |nonrel | is the number above k that were judged not relevant and ε is a smoothing constant.</p><p>In similar stability experiments to those conducted by Buckley and Voorhees, Yilmaz and Aslam showed that all of their new measures, in particular infAP, were substantially more stable than BPref. A number of evaluation campaigns adopted infAP using it in conjunction with pool sampling to streamline their relevance assessment process: TRECVID started in 2006 <ref type="bibr" target="#b188">[188]</ref> as did the TREC Terabyte track <ref type="bibr" target="#b41">[42]</ref>. For more on pool sampling see Section 6.3.1.</p><p>These were not the only example of such effectiveness measures, a tranche of similar measures were proposed and further analyses conducted. Sakai <ref type="bibr" target="#b207">[207]</ref> tested a number of alternatives including one that considered graded judgments (RPref). Büttcher et al. <ref type="bibr" target="#b42">[43]</ref> described their measure, RankEff, which inferred the relevance of an unjudged document based on its textual similarity to judged documents. A number of variants directly inspired by infAP were described in the literature, statAP, which is used in the Million Query track of TREC is probably the best known <ref type="bibr" target="#b50">[51]</ref>. Bompada et al. <ref type="bibr" target="#b26">[27]</ref> compared BPref, infAP, and nDCG under a wide range of situations where qrels were incomplete. They found nDCG (which simply ignores unjudged documents) to be the most stable measure. See also Sakai and Kando <ref type="bibr" target="#b209">[209]</ref> for another detailed comparison of these types of measures.</p><p>Probably because it was the first such measure, BPref started to be used quite widely in the IR research community, however, given more recent research questioning its stability compared to alternatives, this popularity may be brief. There is not yet a sufficiency of definitive work on which alternative is best.</p><p>A different approach to dealing with unjudged documents was suggested by a number of researchers: assessing potential error in a measure. Baillie et al. <ref type="bibr" target="#b19">[20]</ref> proposed that the number of unjudged documents retrieved should be considered when making comparisons between runs. They found that if the number of unjudged between such runs was different, there was a danger that comparisons were unreliable. Moffat and Zobel similarly examined error rates when comparing systems on collections with unjudged documents <ref type="bibr" target="#b179">[179]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Relevance Applied to Parts of Documents</head><p>Most IR evaluation focuses on retrieval of whole documents. It is to those whole units that judgments of relevance or non-relevance are applied. In early IR research, the "documents" being retrieved tended to be at most abstract-sized texts. However, as full-sized documents started to be retrieved, passage-based retrieval was increasingly studied and effective means of its evaluation was considered. Initial work <ref type="bibr" target="#b116">[117,</ref><ref type="bibr" target="#b212">212]</ref> focused on using passage retrieval to improve document retrieval, which meant that existing document test collections could be used unaltered. In passage retrieval, the aim was to identify accurately the passages of a document that were relevant to a user's request. Consequently, an adapted form of test collection was built with more detailed information in the qrels on the location of relevant passages.</p><p>Passage retrieval was part of the tasks in the TREC HARD track and was an integral part of the INEX evaluations of XML retrieval. A broad range of evaluation measures were developed within HARD <ref type="bibr" target="#b282">[281]</ref> and INEX <ref type="bibr" target="#b148">[149,</ref><ref type="bibr" target="#b147">148,</ref><ref type="bibr" target="#b139">140]</ref>. Many of the measures were extensions of existing approaches to evaluation of document retrieval, such as precision, recall, MAP, and DCG. Many of the measures were task specific and no single measure emerged that is used more than others or used beyond the evaluation exercises that created it. Here we illustrate the working of one of these measures taken from Kamps et al. <ref type="bibr" target="#b139">[140]</ref>. Here, document passages (called parts in INEX) p r were ranked at positions r to collectively form a ranking retrieved in response to a query q. Precision at rank r was defined as:</p><formula xml:id="formula_14">P (r) = r i=1 rsize(p i ) r i=1 size(p i ) ,</formula><p>where rsize(p i ) was the length of any segment of the document part that was judged relevant and size(p i ) was the total number of characters in p i . Recall at rank r was:</p><formula xml:id="formula_15">R(r) = r i=1 rsize(p i ) T rel(q) ,</formula><p>where Trel (q) was the total quantity of relevant text in the collection for the query q. Kamps et al. <ref type="bibr" target="#b139">[140]</ref> went on to describe an interpolated version of P to allow precision recall graphs to be plotted and an MAPlike measure MAiP to be calculated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Dependence and Diversity in Rankings</head><p>All the evaluation measures described so far assumed that users judged the relevance of a document independent of all other documents. Many measures discounted the importance of documents retrieved lower down the ranking, but the level of discount was always determined by rank and not the quantity of relevant documents already retrieved. Measures that took a dependent view of relevance were developed in the context of diversity and novelty. For diversity, coverage of different aspects of relevance in rankings and individual documents was a priority. For novelty, the goal was to score higher IR systems that prevented repetition of the same relevant content in a ranking. The initial work in measuring the effectiveness of diverse retrieval systems appears to be from the TREC-6 interactive track <ref type="bibr" target="#b186">[186]</ref>. Documents relevant to the collection topics were expected to be relevant to one or more distinct aspects, 1 which Over stated were "roughly one of many possible answers to a question which the topic in effect poses". When assessing runs submitted to the track, aspectual precision and aspectual recall were calculated. Respectively, Over defined them as "the fraction of the submitted documents which contain one or more aspects" and "the fraction of total aspects . . . for the topic that are covered by the submitted documents". As defined, aspectual precision appears to be the same as precision.</p><p>Zhai et al. <ref type="bibr" target="#b294">[293]</ref> defined a diversity-specific version of precision and formalized Over's definition of aspectual recall, choosing to instead to call it sub-topic recall (S-recall). Considering a topic with n A subtopics and a ranking of documents, d 1 , . . . , d m , S-recall calculated the percentage of subtopics retrieved by the documents up to rank position K:</p><formula xml:id="formula_16">S-recall(K) = K i=1 s(d i ) n A .</formula><p>Here, s(d i ) was the number of subtopics covered in d i . The measure gave a higher score to runs that covered the largest number of subtopics. Several years later, Chen and Karger <ref type="bibr" target="#b52">[53]</ref> proposed the measure k-call(n), which counted if at least one relevant document was retrieved by rank n. By measuring effectiveness in this way, IR system designers looking to optimize for this measure would be motivated to produce systems with diverse rankings. Both measures ignored the rank of retrieved documents. Clarke et al. <ref type="bibr" target="#b54">[55]</ref> proposed an adaptation of nDCG (Section 4.2.1) called α-nDCG, which included this aspect in a diversity measure. The researchers re-defined the function rel(i) from nDCG as:</p><formula xml:id="formula_17">rel(i) = m k=1 J(d i , k)(1 -α) r k,i-1 ,</formula><p>where m is the number of distinct nuggets (the researchers' term for aspects or subtopics), n 1 , . . . , n m , relevant to a particular topic;</p><formula xml:id="formula_18">J(d i , k) = 1 if an assessor judged that document d i contained nugget n k ; r k,i-1 = i-1 j=1 J(d j , k)</formula><p>was the number of documents ranked before document d i that were judged to contain nugget n k ; and the constant α represented the probability that the user of the retrieval system observed prior relevant documents. Note, if α was set to zero and the number of distinct nuggets m = 1, the measure reverted to standard DCG. Clarke et al. <ref type="bibr" target="#b55">[56]</ref> also created the NRBP diversity metric based on Rank-Biased Precision (RBP). Agrawal et al. <ref type="bibr" target="#b1">[2]</ref> pointed out that some of the sub-topics of a query could be more popular or important than others. They found sources of information to estimate a user's probable intended meaning when entering ambiguous topics. To assess their system, they adapted a number of conventional measures (nDCG, MAP, MRR) to handle diversity and to be intent aware.</p><p>Chapelle et al. <ref type="bibr" target="#b51">[52]</ref> described Expected Reciprocal Rank (ERR). While a version of the measure that deals with diversity was described, ERR could also be used simply to promote novelty in search. The measure was defined as follows:</p><formula xml:id="formula_19">ERR = n r=1 1 r r-1 i=1 (1 -R i )R r ,</formula><p>where n was the number of documents in the ranking and R i was the probability that the document at rank i was relevant. At each rank position, r, the probability that a user missed each of the relevant documents retrieved higher up the ranking (1 -R i ) was used as a discount on the impact that R r had on the final score.</p><p>The α-nDCG and intent aware precision measures were used in a recent TREC diversity evaluation track <ref type="bibr" target="#b53">[54]</ref>; and cluster recall was used in ImageCLEF evaluations <ref type="bibr" target="#b9">[10]</ref>. A consensus on a common diversity effectiveness measure is yet to emerge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Are All Topics Equal?</head><p>Commonly, when an evaluation measure is defined in the literature, its formula is presented as a calculation over a single topic. It is assumed that when summarizing the values computed across a set Q of test collection topics, the arithmetic mean of the values is taken.</p><p>Alternatives have been discussed and occasionally tried. Cooper <ref type="bibr" target="#b64">[65]</ref> suggested the use of the geometric mean and a weighted average when aggregating scores across queries for his ESL measure, but settled on the arithmetic mean. Later, Voorhees described GMAP, which uses the geometric mean of AP scores <ref type="bibr" target="#b273">[273]</ref>. Robertson described the formulation of GMAP <ref type="bibr" target="#b198">[198]</ref> and suggested a more computationally tractable version of the formula, which took the arithmetic mean of the log values of AP, Robertson referred to this as AL, this formula produced the same ranking of runs as GMAP though with different values. The two approaches to calculating geometric mean (GMAP, AL) of average precision (AP) values computed over a set of topics Q are as follows:</p><formula xml:id="formula_20">GMAP (Q) = |Q| |Q| k=1 [AP (Q k ) + ε] AL(Q) = 1 |Q| |Q| k=1 ln(AP (Q k ) + ε).</formula><p>Note, if AP = 0 for any topic, GMAP becomes zero and AL undefined, therefore, Robertson added a small value ε to avoid such problems.</p><p>Robertson discussed this measure in some detail pointing out that using geometric mean emphasized improvements in topics that had a low AP score. As Robertson stated. . . This property of GMAP caused it to be created for use in the Robust Track of TREC <ref type="bibr" target="#b273">[273]</ref>, where there was a particular focus on so-called poorly performing topics. Beyond the robust track, it is little used. Whether the method is a more effective averaging approach than the arithmetic mean is yet to be determined. The quality of GMAP to emphasize some changes in topics over others was explored using alternate approaches. Given a large historical set of run scores for the topics of a given test collection, one can compute the score of a new run in relation to the historical scores, determining on a per topic basis if the new run is better or worse than the past runs. Webber et al. <ref type="bibr" target="#b283">[282]</ref> proposed such an approach, employing a methodology used in human testing called score standardization also known as z-score standardization. For each topic (t) in a collection, the score of a new run s is computed as m st . The mean (µ t ) and standard deviation (σ t ) of the scores for the topic is computed from the historical data and a standardized score (m st ) for s is computed as follows:</p><formula xml:id="formula_21">m st = m st -µ t σ t .</formula><p>It was long assumed that the topics of a test collection contributed equally to the measuring of the effectiveness of a search engine. Bodoff and Li <ref type="bibr" target="#b25">[26]</ref> used Classical Test Theory (CTT) to examine how TREC ad hoc topics ranked the runs submitted to particular years of TREC.</p><p>They showed how CTT identified potential outlier topics that ranked runs differently from the majority in a collection. The implication from Bodoff and Li's work was that these outliers were potentially, introducing noise into the test collection. Whether such topics were noise or important outliers was not examined by them. The same year, Mizzaro and Robertson <ref type="bibr" target="#b177">[177]</ref> reported a study on TREC ad hoc run data looking to find redundant topics in test collections: topics that ranked runs similarly. Mizzaro and Robertson stated that such topics could be eliminated from a test collection and that one "could do reliable system evaluation on a much smaller set of topics". However, they only found this small set of topics through an exhaustive search of all possible combination of topics using the run data from TREC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Summing Up</head><p>In the decade following the development of ad hoc test collections, IR evaluation research explored an increasingly wide range of collection types and search tasks. There was a re-discovery of evaluation ideas and practices described in the past, including use of logs to source topics, gathering, and measurement of graded relevance judgments and increasing consideration of diversity in search results. There was also an exploration of relatively new topics such as management of unjudged documents. As shown in this section, these topics produced an extensive body of work. However, this was only one strand of evaluation research; work in studying statistical significance was addressed; as well as a more introspective examination of the methods that underpin the creation and use of test collections also became a major part of evaluation research. These two topics are described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Beyond the Mean: Comparison and Significance</head><p>Whichever evaluation measure one uses, the effectiveness of one run will almost always be compared to another, often considering the absolute or relative difference between the runs. However, simply considering the ∆ between two values can hide important detail, which is illustrated with an examination of three runs: a, b, c. <ref type="foot" target="#foot_17">1</ref> The MAP for the three runs is 0.281, 0.324, and 0.373, respectively. With similar sized gaps between the runs, one might view the comparisons to be revealing similar differences. However, if one graphs a &amp; b and b &amp; c -plotting the AP scores across each of the 50 topics used in the collection -a more complex picture is revealed; see Figures 5.1 and 5.2. The order of topics in both graphs is sorted by the topic effectiveness scores of the b run.</p><p>It can be seen that there is great variation in AP ranging from 0.01 to 0.87. In Figure <ref type="figure">5</ref>.1, the effectiveness of a follows a similar pattern and has a similar range of scores. Harman and Buckley <ref type="bibr" target="#b105">[106]</ref> reported on a detailed study of run comparisons and stated that for most runs, the relatively similar performance on topics shown here is typical. Having the worse run, in this case a, being the same or a little better for some topics (16 of the 50 topics here) is also common. The absolute difference between a and b is 4.3% and relative difference is 15.4%. Both the difference in scores and an examination of the graph in Figure <ref type="figure">5</ref>.1, would lead most to agree that in this case system b is better.</p><p>Harman <ref type="bibr" target="#b103">[104]</ref> highlighted the comparison shown in Figure <ref type="figure">5</ref>.2 where it is arguably harder to determine the better run, yet the absolute and relative differences between b and c are 4.9% and 15.1% respectively, similar to the two runs above. While Harman and Buckley's work <ref type="bibr" target="#b105">[106]</ref> showed that most run comparisons were like the case in Figure <ref type="figure">5</ref>.1, situations such as those found in Figure <ref type="figure">5</ref>.2 are not exceptional. Therefore, it is necessary to examine more than the single value effectiveness measure calculated for a particular run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Significance Tests</head><p>A common approach to more fully understanding a difference ∆ measured between two runs is to use one or more significance tests. The tests estimate the probability p of observing a difference at least as large as ∆ given that a so-called null hypothesis (H 0 ) is true. In the context of IR experiments, H 0 states that the systems producing the two runs under examination have effectively the same retrieval characteristics and that any difference between the runs occurred by random chance. The convention when using such tests is that if it is found that p is below a certain threshold -typically either 0.05 or 0.01 -it is concluded that H 0 is unlikely and consequently should be rejected. Although it is not universally agreed upon, the common interpretation of rejecting H 0 is to conclude that an alternate hypothesis, H 1 is true. This hypothesis states that the two IR systems have different retrieval characteristics, leading the experimenter to conclude that a significant difference was observed. The exact nature of H 1 depends on whether a one-or a two-tailed test is chosen. This topic is discussed below in Section 5.1.2.</p><p>The tests are not infallible and can make errors, which have been classified into Type I and Type II errors. Type I errors are false positives: leading the experimenter to incorrectly reject H 0 and conclude that H 1 is true; Type II errors are false negatives: leading the experimenter to incorrectly conclude that they cannot reject the null hypothesis. In IR parlance, Type I measures the precision of the test, Type II measures its recall. Different significance tests tend to produce a different balance between these two errors. For example, the sign test is known for its high number of Type II errors whereas the t-test is known for producing Type I.</p><p>The creators of the tests make assumptions on the underlying data being examined and it is important for experimenters to be aware of a test's assumptions before applying it. Tests that have fewer assumptions tend to generate more Type II errors and are said to have less power. So-called non-parametric tests, fit this profile, the best known in IR research are the Wilcoxon signed-ranks test and the Sign test. More powerful tests generate fewer Type II errors but make more assumptions about the data being tested. If such tests, known as parametric tests, are applied to data in violation of such assumptions Type I errors can result. The best-known parametric test used in IR is the t-test.</p><p>Assuming each run is conducted on the same topics in a test collection, the significance test is usually operated as a paired test. If, less commonly, the runs being compared use different sets of topics an independent or two-sample version of the tests can be used. Although there is a wide range of tests available to the IR researcher, the three mentioned so far are the most often used. Lesk <ref type="bibr" target="#b161">[162]</ref> discussed use of the sign and the t-test in IR experiments and Ide used the Wilcoxon and t-test to examine the significance of retrieval results <ref type="bibr" target="#b126">[127]</ref>.</p><p>At the same time, Saracevic urged caution on the use of a wide range of statistical tests. He found that no parametric statistical test could be used with confidence on data emanating from effectiveness evaluations because such "data does not satisfy the rigid assumptions under which such tests are run" <ref type="bibr">[223, p. 13</ref>]. In addition, he pointed out that conditions set for use with non-parametric tests were also likely violated by the data output from test collection evaluations. Saracevic did not suggest that these problems should be viewed as being the end of the matter, instead he called for studies on the applicability of statistical tests in IR evaluation. Later Van Rijsbergen detailed the problems with using these test <ref type="bibr" target="#b262">[262,</ref><ref type="bibr">Section 7]</ref>. For the t-test, results needed to be drawn from a "normally distributed population", which Van Rijsbergen stated did not occur with the output of test collection based retrieval experiments. For Wilcoxon and Sign, he pointed out these tests could only be used if data was drawn from continuous distributions; yet retrieval experiments produce discrete distributions. Despite the violations, Van Rijsbergen suggested "conservative" use of the Sign test.</p><p>In later work, Hull <ref type="bibr" target="#b124">[125]</ref> suggested that such prudence was most likely excessive. Hull argued that with "sufficient data, discrete measures are often well-approximated by continuous distributions". Further, he stated that "the t-test assumes that the error follows the normal distribution, but it often performs well even when this assumption is violated ". Hull went on to discuss the properties of a range of significance tests, including the three examined by Van Rijsbergen as well as variations of the ANOVA and Friedman tests. Robertson had earlier conducted a theoretical analysis of a set of tests <ref type="bibr" target="#b196">[196]</ref>, estimating the topic set sizes needed in order to obtain statistical significance in retrieval experiments. He examined the Mann-Whitney U-test, the Chi-squared and the t-test. However, neither Hull nor Robertson empirically tested their comparisons.</p><p>Empirical examinations of the tests appeared to have been first conducted by Keen <ref type="bibr" target="#b151">[152]</ref> who described a small-scale comparison of the Sign and Wilcoxon tests, the results of which showed the tests gave "a similar picture". Keen stated that Wilcoxon tended to indicate significance more readily. Zobel described a study of which of three significance tests was the best suited for IR experiments <ref type="bibr" target="#b296">[295]</ref>. Splitting the topics of a test collection in half: one-half used as a mini test collection; the other as a simulation of an operational setting, he tested the paired t-test, the paired Wilcoxon's signed rank test and ANOVA (his method is described in detail in Section 6.4). Zobel concluded that use of all three tests resulted in accurate prediction of which was the best run, though he expressed a preference for the Wilcoxon test "given its reliability and greater power".</p><p>Later, Sanderson and Zobel <ref type="bibr" target="#b222">[222]</ref> using Zobel's <ref type="bibr" target="#b296">[295]</ref> methodology examined the t-test, Wilcoxon and Sign tests. Their conclusions were that use of the t-test and the Wilcoxon test allowed for more accurate prediction over the sign test of which run was better. However, the differences between the t-test and Wilcoxon test were small. They also pointed out that even if one observed a significant difference between two runs based on a small number of topics (≤ 25), one should not be confident that the same observed ordering of runs will be seen in the operational setting. Using a variant methodology, Cormack and Lynam <ref type="bibr" target="#b67">[68]</ref> reported significant differences resulting from the t-test failing to be observed in an operational setting particularly for topics with a small number of relevant documents (≤ 5).</p><p>Savoy <ref type="bibr" target="#b226">[226]</ref> proposed use of the bootstrap test, 2 as assumptions of normality, or continuous distributions are not required with this test. Despite Savoy's promotion, the test was little used by the IR community until more recent times, when Cormack and Lynam <ref type="bibr" target="#b67">[68]</ref>, Sakai <ref type="bibr" target="#b206">[206]</ref> and Jensen <ref type="bibr" target="#b133">[134]</ref> applied the test. Sakai <ref type="bibr" target="#b208">[208]</ref> provided a detailed explanation of how two forms of the test can be used.</p><p>A comparison of a number of such newer significance tests was conducted by Smucker et al. <ref type="bibr" target="#b235">[235]</ref> who compared the Wilcoxon, sign, and t-tests with the bootstrap and the randomization or permutation test. Like the bootstrap, almost no properties of a data set must hold before the randomization test can be applied. Smucker et al. compared the values of p obtained across every possible pairing of runs submitted to 6 years of TREC's ad hoc track. They found the t-test, bootstrap and randomization produced similar p values across the pairs, with the Wilcoxon and sign tests producing quite different p values. Smucker et al. stated that the Wilcoxon and sign tests were simpler versions of the randomization test and so argued that in comparison with the randomization test, the different sets of p-values were indicative of errors in the two non-parametric tests, leading the researchers to argue that use of the Wilcoxon and sign should cease. Although their experiments did not show any difference between the remaining three tests, they argued from references to past work that the randomization test was likely to be the best to use in test collection experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Not Using Significance</head><p>Spärck Jones <ref type="bibr" target="#b241">[241]</ref> suggested a simple approach to determining the significance of comparisons stating that "in the absence of significance tests, performance differences of less than 5% must be disregarded ". Voorhees and Buckley <ref type="bibr" target="#b277">[276]</ref> later clarified that Spärck Jones was referring to was an absolute percentage difference. Given the example of the two TREC runs graphed above, with an absolute difference of 4.9%, one might be tempted to agree with this view. She went on to state that she would "broadly characterize performance differences, assumed significant, as noticeable if the difference is of the order of 5-10%, and as material if it is more than 10% ". Use of simple tests like this are rarely reported in the research literature.</p><p>Voorhees and Buckley <ref type="bibr" target="#b277">[276]</ref> when comparing effectiveness measures (see Section 6.4) chose to treat Spärck Jones's threshold as a form of simple significance test: requiring a 5% difference between runs. The question that Voorhees and Buckley addressed was how many topics were needed in a test collection in order for a 5% difference (measured in the collection) to accurately predict which of a pair of runs was the better in the operational setting. They found that around 50 topics were needed before one could be confident that a 5% absolute difference measured between two runs on a test collection would predict which was the better run in an operational setting. Their result was in stark contrast to Zobel's 1998 work who found that using 25 topics in conjunction with the common significance tests was more than sufficient <ref type="bibr" target="#b296">[295]</ref>. Given that the methodologies and data sets used between the two works were similar, it would suggest that, just measuring the magnitude of difference in effectiveness scores has limited utility.</p><p>Another work of note is that of Frei and Schäuble <ref type="bibr" target="#b87">[88]</ref>, who proposed a new evaluation measure that as part of its calculation, computed an error probability to indicate the stability of the value measured. The measure, usefulness, was never widely adopted, although, another feature of the work, that it used relative relevance judgments, proved to be influential on others later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">One or Two-Tail Tests?</head><p>So far H 0 has been described, but H 1 has not. There are two types of hypothesis that can be chosen for H 1 , which correspond to different types of test: a one-and a two-tailed test (also known as a one-or twosided test). In a two-tailed test, H 1 states that the two systems under examination are not equal, e.g., from the runs in Figure <ref type="figure">5</ref>.1, H 1 would state that system a does not have the same retrieval characteristics as system b. Comparing a and b, a two-tailed t-test of H 0 returns p = 0.002; the Wilcoxon signed-rank test returns p = 0.004; and the sign test p = 0.015. Assuming a 5% threshold, regardless of which test was used, the experimenter would reject the null hypothesis and consider the difference between a and b to be significant.</p><p>Since IR experiments are often concerned with determining if a new type of IR system is better than an existing baseline, experimenters sometimes use a form of significance test that focuses only on the question of difference in one direction between two runs: this is the one-tailed test. Here the experimenter predicts before conducting the experiment that one of the systems will be better than the other and sets H 1 to reflect that prediction. Taking this time the comparison from Figure <ref type="figure">5</ref>.2, if system b is a baseline and system c is a new system under test, the experimenter sets H 1 to predict that c &gt; b. A one-tailed t-test returns p = 0.036; the Wilcoxon p = 0.026; and the sign test, p = 0.102. <ref type="foot" target="#foot_19">3</ref> Despite the lack of significance in the sign test, most experimenters would consider the improvement of c over b as significant. The one-tailed test is recommended for use in IR experiments by Van Rijsbergen [262, Section 7] and more recently by Croft et al. <ref type="bibr" target="#b73">[74]</ref>, however, it is worth noting that in some areas of experimental science the one-tailed test is viewed as almost always inappropriate <ref type="bibr">[8, p. 171]</ref>.</p><p>The one-tailed version of a significance test has a p value that is half that of the two-tailed version, which makes it a tempting choice for experimenters as its use doubles the chance of finding significance. Note for example that all of the two-tailed tests comparing b and c would have failed to reject H 0 . However, if using the one-tailed test, it is important to understand what its use entails. If from Figure <ref type="figure">5</ref>.1 an experimenter had incorrectly predicted that system a &gt; baseline b and chose to use a one-tailed test; and upon discovering that a &lt; b, the experimenter would have to conclude that they had failed to reject H 0 . In other words, the experimenter would be obliged to report that a and b had the same retrieval characteristics, no matter how much worse a was compared to b; to many a strange conclusion to draw. The experimenter could of course conduct the one-tailed test in the opposite direction, but this second test could only be conducted on a new data set.</p><p>Recalling the example in Figure <ref type="figure">5</ref>.2, an abuse of significance tests would arise if an experimenter decided to use a two-tailed test, when comparing c and b, found no significance and so switched to a one-tailed test in the favorable direction in order to search out significance.</p><p>The choice of a one-or two-tailed test needs to be made before analyzing the data of an experiment and not after. If you are not sure of the direction of difference you wish to test for when comparing two systems, a two-tailed test is the appropriate choice. If you are certain that you only wish to test for a difference in one pre-selected direction, the onetailed test can be used. It is important that the experimenter always states which "tailed version" they used when describing their work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Consider the Data in More Detail</head><p>It is also always worth remembering that although the use of significance tests can help the IR researcher better understand the difference ∆ between two runs, the tests are not oracles, they are merely a generic statistical tool constructed for the purpose of estimating the probability p of observing at least ∆ if the null hypothesis, H 0 , is true. The value of p is calculated on the sample of topics, documents, and relevance judgments in the test collection. If that test set is a representative sample of the broader population of queries, documents and judgments made by users in an operational setting, then the conclusions on whether H 0 can be rejected or not should apply to the population. If, however, as is often the case, the sample is not representative, then conclusions drawn may be unreliable. Sanderson and Zobel <ref type="bibr" target="#b222">[222]</ref> showed a number of examples where a range of significance tests produced p values ≤ 0.05 for a pair of runs on one sample test collection, but for the pair using a different sample collection produced p values &gt; 0.05. Voorhees, using larger topic sets <ref type="bibr" target="#b276">[275]</ref> went further, occasionally finding examples where one system was better than another, but on a different topic set, the system ordering was swapped and in both cases the differences were significant.</p><p>Even if the experimenter compareds two runs using the collection and found p ≤ 0.05, there remains the question, is the result practically significant? Comparing the runs in Figure <ref type="figure">5</ref>.2, if b was a baseline system already installed at an organization, even though c is significantly better (according to a one-tailed test), the manager of the existing baseline system might argue it is questionable if users would welcome the new system c given that it is notably worse than b on 10 of the 50 topics (20%). In a different setting, a manager might conclude that c is worth installing because it appears to improve substantially on topics that b performed very poorly on, but only reduces somewhat b's top performing topics and those reductions would be acceptable to his/her users. Such issues, which could be critical in deciding the value of one system over another, are not addressed by significance tests and can only be answered by a more detailed understanding of the uses and users of an IR system.</p><p>It is also important to consider the magnitude of difference between two systems: i.e., if a significant difference is substantial enough. In an operational setting, as Cooper pointed out <ref type="bibr" target="#b66">[67]</ref>, a better system might require more compute resource than the baseline and the benefits of the new might not outweigh the disadvantages of the additional resource needed. Alternatively, experimentation comparing a new system with a baseline might fail to reject H 0 . However, if the new system uses fewer resources than the baseline, the new system could still be the better choice. The question of what constitutes a sufficiently large improvement in retrieval effectiveness continues to be examined in some detail by the IR community, details of which can be found in Section 6.5.</p><p>A significance test provides a binary decision on whether there is something of note in data or not. On finding significance, researchers may not feel it is necessary to examine their data further. Perhaps of more concern is researchers who fail to find significance, may not look further at their data, which may prevent them from learning what had gone wrong and/or how to fix any problem. Webber et al. explored this situation, pointing out that one possible explanation for failing to reject H 0 is that the experimental setup did not have sufficient statistical power for such a difference to be reliably observed: i.e., a Type II error occurred <ref type="bibr" target="#b284">[283]</ref>. The researchers described the means of measuring such power in test collections and detailed a method for incrementally adding topics to a test collection until the required power was achieved in order to avoid such errors.</p><p>While significance tests are without doubt a popular statistical data analysis method, it is worth remembering that many statisticians feel the tests are overused and that their use discourages researchers from examining their data in more detail. As pointed out by Gigerenzer, a wealth of other statistical analysis methods exists to allow different forms of analysis to be conducted <ref type="bibr" target="#b93">[94]</ref>. One popular alternative is the confidence interval (CI) which can be used to compute an interval around a value, commonly displayed in graphs using an error bar. If when comparing two values, the error bars don't overlap, a researcher can state that the difference between the values is of note. In some scientific fields, confidence intervals have replaced significance tests to become the default method for analyzing experimental data. It would appear they were chosen because their use encouraged more analysis of the properties of the data, than significance testing does. Confidence intervals are sometimes used in IR literature; in describing statMAP, Carterette et al. defined how to compute CIs over that measure <ref type="bibr" target="#b47">[48]</ref>. Cormack and Lynam <ref type="bibr" target="#b67">[68]</ref> described how to calculate an interval on MAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Examining the Test Collection Methodologies and Measures</head><p>Cleverdon's report of his design and use of the Cranfield test collection <ref type="bibr" target="#b57">[58]</ref> -with its set of documents, topics, and qrels -concluded a decade of preliminary work in testing IR systems and established a methodology for evaluation that, now approaching its 6 th decade, continues to be widely used. The changes in computer technology in that time have been profound, causing IR systems to transform from slow searchers of limited collections to engines capable of searching billions of documents across different media, genres, and languages. With such enormous change, it is striking that the test collection methodology has altered little over that time.</p><p>In the decade of research conducted after the development of ad hoc test collections, there was a wide ranging examination of all aspects of test collection methodology, helped greatly by the run data sets produced by TREC and NTCIR. Exploitation of these sets prompted a re-examination of the impact of assessor consistency on measurements made on test collections; an exploration of pooling including consideration of effective use of relevance assessors' time; determining which were the best topics to use in a test collection; establishing which is the best evaluation measure; and perhaps most importantly, determining if test collections actually predict how users will use an IR system. The work is now described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Re-checking Assessor Consistency</head><p>As highlighted in Section 2.5, an early concern of some researchers was the potential of inconsistent relevance assessments resulting in poor quality qrel sets, which could mean that measurements made on test collections could be inaccurate. Early on, both Lesk and Salton as well as Cleverdon tested the potential for such error and found that variations in assessments although high did not affect the relative ranking of runs. Voorhees conducted a larger scale study using TRECs 4 and 6 run data <ref type="bibr" target="#b268">[268]</ref>, for which multiple relevance assessments, qrels, were available.</p><p>Voorhees established a method of correlating ranks of runs that became widely used by many other IR researchers. The method involved ranking the runs submitted to the two TRECs using different qrel sets. Voorhees measured the correlation between a rank of runs using one qrel set and the same runs ranked using a different qrel set. A high degree of correlation meant the qrel sets ranked IR systems similarly; and low correlation implied that each qrel set was ranking runs differently.</p><p>In order to determine the similarity between ranks, Voorhees used Kendall's Tau (τ ) <ref type="bibr" target="#b152">[153]</ref>. For any two rankings of the same n set of items, τ is a linear function of the number of pairs of items which are in different orders in the two rankings. This function is constrained such that τ = 1 if the two rankings are in identical order, and τ = -1 if the order is reversed. There are a number of τ variations, we illustrate the one originally defined by Kendall. It is as follows:</p><formula xml:id="formula_22">τ = 2(n C -n D ) n(n -1) .</formula><p>Here n C is the number of pairs in the two rankings that are in the same order (i.e., concordant) and n D is the number of pairs that are in different order (i.e., discordant). Note that every possible pair of items in the rankings (i.e., there are n(n -1) such pairings) is compared when calculating n C and n D . If one was using τ to measure the correlation between two rankings of ten runs from TREC and the two rankings were identical, then n C = 45, n D = 0, and τ = 1.0. If there was a swap anywhere in one ranking of two adjacent items, then n C = 44, n D = 1, and τ = 0.96. If the swap was between the first item and the last item of one ranking, then n C = 28, n D = 17, and τ = 0.24. When considering τ calculated over a set of rankings, a common question that is asked is at what level of correlation can one view two rankings as effectively equivalent? Voorhees suggested τ ≥ 0.9 as such a threshold <ref type="bibr" target="#b270">[270]</ref>, which was adopted by a number of subsequent researchers. 1 Using Kendall's τ Voorhees found the rankings of runs, though not identical, were very similar, leading Voorhees to conclude that variations in assessment did not impact noticeably on retrieval effectiveness. The second part of Lesk and Salton's work on examining the consistency of judgments against the rank of the documents being judged was also repeated in later years: both Sanderson <ref type="bibr" target="#b216">[216]</ref> and later Voorhees <ref type="bibr" target="#b270">[270]</ref> using different TREC data sets, showed that inter-assessor agreement was higher for top-ranked documents.</p><p>In the experiments conducted up to this point, the assessors used to generate different qrels were all assumed to be capable of judging the relevance of the documents. In later work based on TREC Enterprise track data, Bailey et al. <ref type="bibr" target="#b18">[19]</ref> drew from different sets of assessors based on their knowledge about the test collection topics. The assessors were classed as gold, silver, and bronze judges. Gold and silver were subject experts with the gold judges having a more intimate knowledge of the data set being searched. The bronze judgments were made by the participants in the TREC track: presumably motivated, but nonsubject experts. Like the previous results, Bailey et al. showed that the 1 Sanderson and Soboroff <ref type="bibr" target="#b220">[220]</ref> pointed out that the items in a ranking are sorted by a score and the range of the scores of the items in a list, impacts on the value of τ . They showed that if the range is large, there is a greater likelihood of finding high τ correlation scores. This quality is common to all rank correlation measures and makes use of absolute thresholds difficult. Another criticism of τ is that it measures correlation equally across a ranking and for many IR tasks, correlation in the top part of a ranking (i.e., the runs of the best performing systems) is generally more important than the bottom. Yilmaz et al. <ref type="bibr" target="#b291">[290]</ref> produced a new correlation coefficient τap that addresses this failing. They also cite a number of other proposed τ variants. Carterette has also described an alternative to τ <ref type="bibr" target="#b45">[46]</ref>. See also <ref type="bibr">Melucci [175]</ref> on concerns about use of τ in IR experiments.</p><p>qrels from the gold and silver judges produced similar rankings of runs. However, the rankings from gold and bronze judges were different. These works show that while test collections are more resilient to assessor variation than was originally feared, there are limits to this resilience and the appropriateness of the assessors used to make judgments needs to be carefully considered when forming qrels.</p><p>It is often thought that differences in assessment are an indication of some sort of human error. However, Chen and Karger <ref type="bibr" target="#b52">[53]</ref> showed that one can view the differences as simply two distinct, but valid, interpretations of what constitutes a relevant document. They used the TREC-4 and 6 multiple assessments to test a retrieval system that returned diverse search results. Viewing the two sets of assessments as representing two legitimate interpretations of relevance for the topics in TRECs 4 and 6, Chen and Karger showed that supporting diversity ensured that more documents were retrieved that satisfied at least one of the assessors. For more on diversity evaluation, see Section 4.1.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Does Pooling Build an Unbiased Sample?</head><p>The aim of pooling is to locate an unbiased sample of the relevant documents in a large test collection, as made clear by Spärck Jones <ref type="bibr" target="#b243">[243]</ref>. She was confident in the validity of pooling in part due to earlier work by Harman <ref type="bibr" target="#b101">[102]</ref> and later Zobel <ref type="bibr" target="#b296">[295]</ref> who tried to estimate the quantity and impact of relevant documents missing from TREC pools. In the early years of the TREC ad hoc collections, pools were formed from the union of the top 100 documents retrieved by each submitted run for each topic. Harman examined a pool formed by the documents in ranks 101-200 for a sample of the runs and topics in TREC-2 and all the runs and topics in TREC-3. She reported that a further 11% of relevant documents were discovered in the TREC-2 pool and a further 21% in TREC-3. Harman stated that "These levels of completeness [in the pools] are quite acceptable for this type of evaluation". Zobel examined the relationship of the number of relevant documents found (n) to the depth of rank used to form a pool (p) and found the relationship to match well to the following power law distribution:</p><formula xml:id="formula_23">n = Cp s -1,</formula><p>where C and s were constants. So strong was the fit that Zobel felt confident to extrapolate the curve beyond the depth 100 pools of TREC. Extrapolating what n would be when p = 500, he stated that the number of extra relevant documents would be double that found when p = 100. The prospect of so many unfound relevant documents caused Zobel to consider if it was possible for runs to retrieve the majority of the unknown relevant and consequently receive an unfairly low effectiveness score. He, therefore, explored how the contributing runs to TREC would have been ranked if they had not contributed to the formation of the pool. This was achieved by, in-turn, removing from the pool the relevant documents unique to a particular run, re-forming the reduced qrels and then comparing a ranking of all runs with the original run rank. Zobel found relatively small changes in the way that left out runs were ranked, the pools appeared to be an unbiased sample. Zobel stated the result boded well for the reusability of test collections.</p><p>More recently, Büttcher et al. <ref type="bibr" target="#b42">[43]</ref> explored two adaptations of Zobel's "leave one run out" experiment. They pointed out that it was common for a research group to submit multiple runs to TREC, leaving a single run out, as Zobel had done, was perhaps not the best of simulations as other runs from the same group would have been left in. Therefore, borrowing a technique described by Voorhees and Harman <ref type="bibr" target="#b278">[277]</ref> the researchers used a "leave one group out" approach.</p><p>Testing their work on the TREC 2006 Terabyte track runs and qrels, the researchers found that removing a whole group made a relatively small difference to the way runs from the held out group were ranked. In a second test, the researchers held out relevant documents uniquely found in manual runs from the pools. The manual runs are generally the richest source of relevant documents. By leaving them out Büttcher et al. were attempting to simulate a situation where after a test collection was formed, a new substantially better run was tested on the collection. Examining how well these runs were ranked by the reduced pools, Büttcher et al. reported that the runs were ranked somewhat differently compared to when the full TREC pools were used.</p><p>Such a result was undoubtedly concerning. However, the extent that it represented a problem for experiments is yet to be fully determined. There would not appear to be much in the way of evidence that users of test collections have found new retrieval systems poorly scored, though admittedly few researchers actually analyze for such potential problems. There appears in the literature to be just one example of a run that if it had not contributed to the pool of a test collection, it would have been poorly scored on that collection. So unusual was this particular run that it was studied in some depth by Buckley et al. <ref type="bibr" target="#b35">[36]</ref>. They found that within the pool of runs used to form the test collection was a particular bias: most relevant documents contained at least one word from the title of the test collection topics. However, the errant run contained many documents that were both relevant and did not contain words in the topic title.</p><p>Buckley et al. studied this unusual run and the properties of the collection to try to better understand the causes of this anomaly. It would appear that the size of the collection was an important factor. It is normal for many of the relevant documents in a test collection to contain words from the title of a topic and it is also to be expected that such documents would be highly ranked. The number of such documents is finite. In the smaller test collections it would appear that the size of the pools assessed was larger than the number of relevant documents containing terms from the topic title. However, in the collection under study, the pool size was not large enough to encompass all the relevant documents containing a topic title word and to also find enough of the relevant documents without.</p><p>Whether this one example was an outlier or an indicator of a broader problem with current approaches to pooling in large test collections is yet to be determined.</p><p>Examining a different aspect of potential bias in pooling, Azzopardi and Vinay <ref type="bibr" target="#b16">[17]</ref> studied if within large collections there are documents that are almost never retrieved by any search engine. Loading large collections into a conventional ranked retrieval system, they ran hundreds of thousands of queries on the collection. The queries were either single word terms that occurred &gt;5 times in the collection, and bigrams that occurred &gt;20 times. The researchers' aim was to understand if through all those queries there were documents in the collections that persistently failed to be highly ranked. Their conclusions were that a notable number of such documents existed in these collections. Such a conclusion could be of concern since pools are built through querying. However, Azzopardi et al. did not examine the relevance of the poorly retrieved documents. It is unclear at the moment if this probable bias is important with respect to locating a representative sample of relevant documents.</p><p>It would appear that despite concerns of some in the IR community that pooling risks the creation of test collections a biased sample of qrels, studies have largely shown such concerns are unfounded. However, the effort required to build pools is substantial and as described in Section 6.3, attempts are being made to produce smaller pools, which might introduce new forms of bias. This is a topic, therefore, that is likely to be returned to in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Building Pools Efficiently</head><p>Reflecting on the first eight years of TREC, Voorhees and Harman <ref type="bibr" target="#b279">[278]</ref> detailed that the average number of documents assessed per topic in the ad hoc tracks of TREC was 1,464 (averaged from data in Table <ref type="table">4</ref> of that paper). With 50 topics per year, approximately 73,000 judgments were made each year. At a rate of two judgments per minute -Voorhees and Harman [279, Section 2.1.3] estimate -8 hours of work per day, judging the TREC ad hoc pool each year took just over 75 person days. This was the limit of human resource TREC organizers were able to supply; similar limits applied to other large evaluation exercises such as CLEF or NTCIR.</p><p>Assessors in evaluation exercises tend to be used in a relatively straightforward and similar manner. Here we highlight the way that assessors were used in TREC ad hoc. When groups submitted a run to TREC, the top 100 documents for each topic in the run were extracted and merged into a pool for each topic and sorted by document id. The assessor for a topic was generally the person who created that topic. They examined every document from every system in the pool for that topic. This straightforward approach to examining a pool was justified by Voorhees and Harman <ref type="bibr" target="#b279">[278]</ref> and later by Soboroff and Robertson <ref type="bibr" target="#b239">[239]</ref> by stating that it was important for the pools to contain a set of documents that were not biased in any way toward one particular retrieval strategy or a particular type of document. This was judged vital so that the qrels could be used not only to fairly determine the relative effectiveness of runs submitted to TREC, but also that later users of the test collections could be confident that the effectiveness of a new retrieval strategy will be accurately and fairly measured.</p><p>A number of researchers examined other ways of selecting or sampling parts of a pool to judge so as to use fewer human assessments. The approaches are grouped here into examinations of the way that pools are scanned; research assessing if pool depth or topic breadth is more important; approaches to solve the assessment resource problem by distributing assessment; an examination of an approach that opened the possibility of avoiding use of assessors at all; and finally exploitation of existing data to simulate assessments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Scanning the Pools in Different Ways</head><p>Zobel <ref type="bibr" target="#b296">[295]</ref> pointed out that some topics in a test collection will have more relevant documents in them than others and suggested that as topics were being assessed for pools, the number of relevant found so far could be noted and for those topics richer in relevant documents, more assessor effort could be focused on examining a larger pool for them.</p><p>In the same year that Zobel's made his suggestion, Cormack et al. <ref type="bibr" target="#b68">[69]</ref> proposed a number of alternate strategies. In a similar vein to Zobel's ideas of focusing assessor effort on the richest sources of relevant documents, they pointed out that the runs from certain retrieval systems contributing to the pool contained many more relevant documents than others. They proposed focusing assessor effort onto those runs richer in relevant documents. This approach they called local move to front (MTF) pooling. The researchers also tested an approach that included Zobel's ideas: prioritizing assessment of both the most fruitful runs and the most fruitful topics, which they called global MTF pooling. The authors tested the approaches and showed that they could assess 10% of the full TREC pool and produce a qrel set that ranked runs in an almost identical manner to the ranking achieved using the TREC baseline pool. Global MTF appeared to be more effective than local MTF. See also <ref type="bibr">Moffat et al. [178]</ref> for related work.</p><p>Cormack et al.'s paper contained one other approach to building qrels, which they called Interactive Search and Judge (ISJ). Here they proposed an alternative role for the relevance assessor, instead of judging a long list of documents, the assessor would search the test collection, issuing multiple queries, noting down relevant documents found and searching until they could find no more relevant for a particular topic. Cormack et al. reported that a group of ISJ assessors was given the task of locating relevant documents for one of the years of TREC. In just over <ref type="bibr" target="#b12">13</ref> person days (compared to TREC's 75 person days) a qrel set was formed that was shown (through experimentation) to be of comparable quality to that produced by TREC.</p><p>The work was a re-examination of the approach to pooling used by Katzer et al. <ref type="bibr" target="#b145">[146]</ref> and earlier still by Lancaster <ref type="bibr" target="#b157">[158]</ref>. TREC implicitly uses ISJ through its encouragement for manual runs to be submitted to its tracks <ref type="bibr" target="#b217">[217]</ref>. It has also been used explicitly by a number of evaluation campaigns such as CLEF <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b92">93]</ref> and NTCIR <ref type="bibr" target="#b155">[156]</ref>. Recognizing that TREC assessors preferred to assess rather than search, Soboroff and Robertson <ref type="bibr" target="#b239">[239]</ref> described an alternative approach to ISJ where relevant documents located by assessors were used as positive examples for a relevance feedback system to locate more items for assessment. Soboroff reported that the approach had worked well in reducing assessor time and effort. Oard et al. <ref type="bibr" target="#b184">[184]</ref>, detailed using this technique, which called they search-guided assessment, to build a test collection.</p><p>Carterette et al. <ref type="bibr" target="#b46">[47]</ref> pointed out that certain documents in the pool were better at distinguishing runs from each other and these should be targeted for assessment. For example, if one compared two runs using P <ref type="bibr" target="#b9">(10)</ref> and all that one wished to determine was which run was better, only the top ten documents needed to be examined and any documents in common could be left unjudged. Through such targeting, the researchers took eight runs from six different searching systems retrieving across 60 topics and in 6 hours of assessor time were able to produce a ranking of those runs that with 90% confidence was the same as the ranking produced by a baseline TREC top 100 pooling approach.</p><p>Another approach to reducing assessment is sampling of pools, an analysis of which was first described by Spärck Jones and Bates <ref type="bibr">[244, pp. 20-21]</ref>. Aslam et al. <ref type="bibr" target="#b12">[13]</ref> described experiments with pool sampling showing that one could sample as little as 4% of a TREC ad hoc pool and still produce accurate results. Lewis <ref type="bibr" target="#b164">[165]</ref> used stratified sampling to the pools of the TREC filtering track. More recently this approach was exploited in the TREC million query track <ref type="bibr" target="#b50">[51]</ref> and the legal track <ref type="bibr" target="#b21">[22]</ref>. When sampling pools, the number of unjudged documents is likely to increase, consequently, the measures described in Section 4.2.2, tend to be used, particularly, infAP and statAP.</p><p>When compared to the original approach used to form qrels for the TREC ad hoc collections, it would appear that at least some of the methods described here could be used with confidence. For some of the newer approaches involving more extreme forms of sampling, although experimental results have shown the methods to be reliable when tested on historical data, there is still the question of how reuseable such collections will be in experiments run in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Narrow and Deep, or Wide and Shallow?</head><p>One question considered by test collection creators is should assessors be focused on judging deeply the pools of a small number of topics, or the shallow pools of a larger number of topics? The convention was to limit the number of topics and examine their pools in some detail. For many years in TREC, the top 100 retrieved documents of submitted runs were assessed, though in recent years this was reduced to the top 50. Sanderson and Zobel <ref type="bibr" target="#b222">[222]</ref> speculated on the number of topics that could be assessed if a smaller part of a run was drawn into a pool. They estimated that if only the top 10 of each run was assessed, a test collection with between 357 and 454 topics could be created using the same amount of assessor effort as with 50 topics examined to depth 100. They also pointed out that the top part of runs generally have a greater density of relevant documents, consequently such a strategy would in all likelihood find between 1.7 and 3.6 times more relevant documents than a conventional pooling approach.</p><p>Bodoff and Li <ref type="bibr" target="#b25">[26]</ref> pointed out that sources of score variation in test collection based evaluation can be attributed to different IR systems, different topics, and to interactions between systems and topics. The researchers analyzed TREC run data using Generalizability Theory to identify where the main source of variation was, concluding that topics were the highest source. This led the researchers to conclude that building test collections with more topics was a priority. Webber et al. <ref type="bibr" target="#b284">[283]</ref> applied their analysis of statistical power in test collection based experiments to also study this question. They found that greater statistical power would result from a wide and shallow approach to pooling.</p><p>At around this time, publications from the commercial search engine community were produced showing that internal test collections had substantially larger numbers of topics than existed in publically available ones: White and Morris <ref type="bibr" target="#b285">[284]</ref> mentioned a collection at Microsoft with 10,680 "query statements"; Carterette and Jones <ref type="bibr" target="#b48">[49]</ref> described a collection in Yahoo! with 2,021 queries; at the same company Chapelle et al. <ref type="bibr" target="#b51">[52]</ref> mentioned several internal collections, one with over 16,000 queries, judged to depth five.</p><p>In reaction to this, the academic community looked to build its own collections with many more topics. Carterette et al. <ref type="bibr" target="#b50">[51]</ref> described work on the so-called Million Query Track, which, using both Carterette et al.'s just-in-time approach and the pool sampling methods associated with statAP, created a test collection with 1,755 topics. With their larger data set Carterette et al. were able to empirically test the proposal by Sanderson and Zobel that "wide and shallow" was better that "deep and narrow". In their data set Carterette et al. found that 250 topics with 20 judgments per topic were the most costeffective in terms of minimizing assessor effort and maximizing accuracy in ranking runs. Voorhees expressed concern that wide and shallow test collections might not be as reusable as ones built using the deep and narrow approach <ref type="bibr" target="#b275">[274]</ref>. However, Carterette <ref type="bibr" target="#b44">[45]</ref> provided evidence that such collections were more reusable than was perhaps previously thought.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.3">Distributing Assessment</head><p>The relevance assessors are generally paid by the organizers of evaluation campaigns. There have been attempts to reduce or remove such costs. Both INEX and the enterprise track of TREC explored making relevance assessment a necessary part of groups being able to participate in the campaign. There is relatively little research on the accuracy of such "coerced" judgments. However, Bailey et al.'s work <ref type="bibr" target="#b18">[19]</ref> on gold, silver, and bronze judgments suggested that such an approach to gathering judgments was not without its risks.</p><p>Another potentially large source of human assessors can be found through crowd sourcing. Alonso et al. <ref type="bibr" target="#b6">[7]</ref> described their use of the Amazon Mechanical Turk system to obtain relevance judgments. Mechanical Turk is a market place where workers are paid small amounts of money (typically ranging from 1 cent to $2) to conduct short run tasks, called HITS. The tasks on offer in the market place include writing short reviews, adding metadata to images, and judging documents for relevance. Because the workers on systems like Mechanical Turk are anonymous, it is hard to know the motivation of those conducting the task. It is reasonable to assume that some workers will attempt to earn money for little or no work. Alonso et al. described their attempts to ensure that the anonymous workers chosen were motivated and appropriate for the task.</p><p>Work in this area is still relatively novel and the success of such approaches requires more study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.4">Absolute vs. Relative/Preference Judgments</head><p>Virtually every test collection built has gathered its qrels using absolute judgments: asking an assessor to determine a document's relevance relative to a topic independent of other documents seen. Researchers have sometimes asked if these so-called absolute judgments are the most reliable approach to gathering qrels, suggesting instead that relative or preference judgments made between pairs of documents are sought instead. Some research addressed this question. In the context of concern about consistency of assessor judgments, Rees and Schultz <ref type="bibr" target="#b193">[193]</ref> stated "It is evident that the Judgmental Groups agree almost perfectly with respect to the relative ordering of the Documents." (p. 185). In contrast, Katter reported on an experiment the results of which showed that absolute judgments were more reliable <ref type="bibr" target="#b144">[145]</ref>.</p><p>A concern with relative judgments was that gathering a complete set required showing assessors every possible pairing of documents under consideration, an O(n 2 ) problem. Rorvig conducted experiments examining the tractability of building test collections with relative judgments <ref type="bibr" target="#b203">[203]</ref>. His results indicated that collections could be built from such judgments as preference judgments appeared to be transitive, which meant that some preferences could be reliably inferred, substantially cutting the number of judgments needing to be assessed. He proposed a methodology for building a test collection. More recently, Carterette et al. <ref type="bibr" target="#b47">[48]</ref> showed that relative judgments drawn from users produced more reliable results than absolute. They also found that 99% of judgments gathered were transitive and went on to build on Rorvig's methods for reducing the number of preference judgments that needed to be made.</p><p>There does not as yet appear to have been a public test collection built from relative judgments. As will be seen in Section 7.2, however, deriving relevance judgments using preference from query logs is increasingly common.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.5">Don't Use Assessors at All?</head><p>Soboroff et al. <ref type="bibr" target="#b238">[238]</ref> examined the possibility of not using human input of any kind when creating relevance assessments. They hypothesized that judgments could be generated simply by randomly selecting commonly retrieved documents from the pool of runs used to form a test collection. The researchers examined the way in which runs submitted to several years of TREC ad hoc were ranked using such automatically generated qrels and compared the ranking using the standard qrels of TREC. They found that the two rankings were correlated relatively well. However, the most effective runs were ranked as poor or middle performing runs by the automated qrels. Consequently, the approach was judged to not work.</p><p>Aslam et al. <ref type="bibr" target="#b11">[12]</ref> later pointed out that the method was ranking runs by their similarity to each other; those runs retrieving the most popular documents amongst the set of runs were ranked highest. The best runs retrieved relevant documents that few other runs found, such documents were not in the automatic qrels, consequently the best runs were scored poorly. Soboroff et al.'s method was explored further, Wu and Crestani <ref type="bibr" target="#b288">[287]</ref>; Shang and Li <ref type="bibr" target="#b227">[227]</ref>; Can et al. <ref type="bibr" target="#b43">[44]</ref> and later Nuray and Can <ref type="bibr" target="#b183">[183]</ref>. As yet, no success has been found in fixing the important failing in Soboroff et al.'s approach.</p><p>Others suggested automatically creating both qrels and topics. A recent exploration of this area was described by Azzopardi et al. <ref type="bibr" target="#b15">[16]</ref> who examined means of creating known item topics for a range of collections and languages; they were inspired by earlier work from Tague and Nelson <ref type="bibr" target="#b252">[252]</ref>. The work demonstrated the potential for this approach, but the authors acknowledged that more investigation was needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.6">Exploiting Structure or Other Data Sets</head><p>Although pooling was the predominant means of forming qrels, alternative approaches to seeking relevant documents were tried. What follows is a list of some proposals.</p><p>• Sheridan et al. <ref type="bibr" target="#b228">[228]</ref> described building a small spoken document test collection of broadcast news items. To make relevance assessments easier, queries are referred to events that occurred on a specific date. This allowed the researchers to concentrate assessment to items broadcast on or after the date of the event. • Using pre-existing manual organization of documents has been used on a number of occasions. Harmandas et al. <ref type="bibr" target="#b107">[108]</ref> described the building of a web image test collection where assessors were encouraged to use the topical classifications present on many websites to locate relevant items. Haveliwala et al. <ref type="bibr" target="#b108">[109]</ref> used a similar approach using the topical grouping of the Open Directory website to locate relevant related documents. • Cleverdon building his Cranfield II collection composed of scientific papers used references in papers to identify potentially relevant material. This approach was further developed by Ritchie et al. <ref type="bibr" target="#b194">[194]</ref>.</p><p>• When working in the enterprise web search domain, Hawking et al. <ref type="bibr" target="#b112">[113]</ref> described using the sitemaps of a website (a page that maps out for users the location of important pages on a website) as a source of relevance judgments for known item searching. The known item being the identified page, the query for that item being a title extracted from the sitemap page. • Amitay et al. <ref type="bibr" target="#b8">[9]</ref> proposed trels. For each topic in a test collection it was proposed to manually form a set of words that defined what was and was not a relevant document. Once a stable set of trels was formed, unjudged documents were assessed against the trels to determine relevance. Amitay and her collaborators showed trels to be successful in a simulation on TREC data. This approach was also used to build reusable question answering test collections, see Lin and Katz <ref type="bibr" target="#b165">[166]</ref>. • Jensen et al. <ref type="bibr" target="#b134">[135]</ref>, in the context of web search, tested the combining of manual relevance judgments with judgments mined from a website taxonomy, such as DMOZ. They were able to show that the additional judgments improved evaluation accuracy. • In the field of personalized search, use of bookmark or URL tagging data has been used as an approximation to relevance judgments in a personalized searching system. See for example, Xu et al. <ref type="bibr" target="#b289">[288]</ref>. • An ever increasing body of work has examined the use of search engine logs to help determine relevance of items. Morita and Shinoda <ref type="bibr" target="#b182">[182]</ref> explored using the time that a retrieved item was viewed as a way to infer the relevance of the item. More common was the use of click data in logs to determine relevance, e.g., Fox et al. <ref type="bibr" target="#b86">[87]</ref>. So much log data is being generated particularly within large web search engines, that there is extensive research in analyzing log data and exploiting it. A description of the work in this area is described in Section 7.</p><p>With the exception of using query logs, none of the methods described has been as thoroughly tested as pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Which is the Best Effectiveness Measure?</head><p>Perhaps surprisingly, for a research field that so values evaluation, it would appear that for many decades there was no quantitative research into the relative merits of different effectiveness measures. This was rectified in recent years through two forms of study: calculating the correlation between evaluation measures and assessing the stability of measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.1">Correlating Measures</head><p>Tague-Sutcliffe and Blustein <ref type="bibr" target="#b254">[254]</ref> were the first to quantitatively compare evaluation measures, establishing a methodology that became the standard for most subsequent research. Taking archived runs TREC, Tague-Sutcliffe and Blustein used different precision-based evaluation measures to each rank the runs. Correlations measured between the ranks showed strong similarities across the measures. The researchers concluded that there was little value in calculating different precisionbased measures. However, more recent investigations, e.g., Buckley and Voorhees <ref type="bibr" target="#b38">[39]</ref> and Thom and Scholer <ref type="bibr" target="#b255">[255]</ref>, showed that high precision measures, such as P <ref type="bibr" target="#b9">(10)</ref> and P(1), correlated less well with measures such as MAP or R-precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.2">Measuring Measure Stability</head><p>Zobel <ref type="bibr" target="#b296">[295]</ref> devised a method to test the predictive power of evaluation measures. The core role of a test collection is to determine which retrieved method will produce the best effectiveness when used in an operational setting. Zobel simulated this testing and operational setup by splitting the topics of a test collection in half: one-half was treated as a mini test collection, the other half was a simulation of the operational setting. Using TREC-5 run data, Zobel took pairs of runs and determined which was the best on the mini collection and then measured if the winning run was still the best in the operational setting. If it was, then a correct prediction was made using the reduced collection, if the pairs had swapped order, the result measured on the collection was a mistake. Using this swap method, Zobel determined which of four precision based the measures produced better predictions. He reported that although P <ref type="bibr" target="#b9">(10)</ref> and P (100) were worse at predicting than 11 point interpolated AP, in his judgment, the difference between the measures was too small to be of concern.</p><p>Using an alternative method, Buckley and Voorhees exploited the TREC-8 query track test collection <ref type="bibr" target="#b36">[37]</ref>, which had 21 so-called query sets: manually generated variations of each of the 50 topics in the collection. Each of the sets was run against a range of different retrieval systems resulting in 9 runs for each of the 21 sets. Buckley and Voorhees sought an evaluation measure that ranked the runs consistently over the query sets that also produced the smallest number of ties. They reported that measures such as MAP, R-precision, and P(1000) were the most stable; P(10) and P(1) the least. Buckley and Voorhees judged MAP to have the best balance between high stability and few ties.</p><p>In a separate study Voorhees and Buckley <ref type="bibr" target="#b277">[276]</ref> applied Zobel's swap method to a wide range of TREC test collections and again confirmed that rank cutoff measures like P <ref type="bibr" target="#b9">(10)</ref> were less accurate at predicting the effectiveness of runs than measures like MAP. One possible reason for the difference between Zobel's ambivalent and Voorhees and Buckley's more emphatic conclusions about a measure like P <ref type="bibr" target="#b9">(10)</ref> was that Zobel used his measures in conjunction with a significance test, Voorhees and Buckley did not.</p><p>Sanderson and Zobel <ref type="bibr" target="#b222">[222]</ref> pointed out that when comparing two measures, such as, MAP and P <ref type="bibr" target="#b9">(10)</ref>, the effort required to judge the relevant documents for MAP was substantially higher than that required to assess P <ref type="bibr" target="#b9">(10)</ref>; where only the top 10 documents from each run need be examined. Analyzing nine years of TREC data, Sanderson and Zobel showed that P <ref type="bibr" target="#b9">(10)</ref> required between 11% and 14% of the assessor effort required to calculate MAP. The researchers concluded that P(10) was far more stable than MAP per equal quantity of assessor effort. If the qrels of a test collection already exist, then Sanderson and Zobel's point on the value of P(10) over MAP was not important. If one was evaluating retrieval systems without a test collection, where assessors still had to judge the relevance of documents, then consideration of assessor effort was critical.</p><p>In contrast to most papers suggesting that MAP produces stable ranks of runs, Soboroff <ref type="bibr" target="#b236">[236]</ref> used the swap method on a test collection with a small number of relevant documents per topic: the TREC 2003 topic distillation collection. He found that P(10) was noticeably more stable than R-precision and MAP. Soboroff also showed that MRR can be stable when used in a collection with a large number of topics (≥ 100). Further means of testing stability were described by Bodoff and Li <ref type="bibr" target="#b25">[26]</ref> using Cronbach's alpha (a statistic that measures co-variance); and Sakai <ref type="bibr" target="#b208">[208]</ref> who used the bootstrap test to count statistical significance between pairs of runs when measured with a particular evaluation measure.</p><p>It is worth remembering that the work on measure stability, while valuable, has its limitations. An "evaluation measure" could be created that ranks the runs of different retrieval systems by an alphabetical sorting of the run's name: e.g., a run labeled "Inquery" would be ranked higher than a run labeled "Okapi", which would be ranked higher than "Terrier". Under every stability test described here, this useless measure is perfectly stable; Sakai's significance count methodology would result in the maximum number of observable significant differences, and the Cronbach's alpha approach would show perfect co-variance.</p><p>Ignoring questions of stability, Aslam et al. <ref type="bibr" target="#b14">[15]</ref> used a maximum entropy-based method to explore the degree to which an evaluation measure predicted the distribution of relevant and non-relevant documents across a retrieved list. Essentially, in this work the aim was to understand how well the single value from an evaluation measure summarized the distribution of relevant and non-relevant documents. Aslam et al. found that average precision was the better measure compared to R-precision and precision measured at fixed rank.</p><p>In this section, the assessment of measures was achieved by comparing a relatively simple property of each measure against some ideal. In the following section, the outputs of test collections and evaluation measures were compared with models of user searching behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Do Test Collections or Measures Predict User Behavior?</head><p>A series of experiments were conducted to measure how well predictions made using test collections or evaluation measures correlated with a range of user behaviors when searching on systems under test. Results from this work are contradictory; the research described here is broken into those that concluded that little or no correlation existed, those that showed some link and those that showed a stronger link. Finally the apparent contradictions between these sets of work are discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.1">Little Correlation Found</head><p>In testing the impact of a searching system on user behavior, one can choose to measure effectiveness scores of users searching on an operational system and look for correlations between the scores and some aspect of user behavior or an outcome from the search. A number of studies took this approach. Tagliacozzo <ref type="bibr" target="#b251">[251]</ref> showed that 18% of ∼900 surveyed MEDLINE (a medical literature search engine) users did not appear to be satisfied with search results despite them containing a large number of relevant retrieved documents. Su <ref type="bibr" target="#b247">[247]</ref> attempted to correlate many measures of IR performance with user satisfaction. She found that precision did not correlate significantly with satisfaction and examined this issue in more detail later <ref type="bibr" target="#b248">[248]</ref>. Hersh et al. <ref type="bibr" target="#b119">[120]</ref> examined medical students' ability to answer clinical questions after searching on MEDLINE. Expert assessors were used to calculate recall and precision of the students' search outputs looking for correlations between these measures and the scores students attained for the questions. The researchers reported no correlation. Similar work was conducted by Huuskonen and Vakkari <ref type="bibr" target="#b125">[126]</ref> producing similar negative results. Hersh et al. <ref type="bibr" target="#b121">[122]</ref> were the first to try to correlate test collectionbased results with user behavior. They examined a pair of IR systems measured as significantly different on a small test collection; when subjects used one of the pair of systems, no significant difference in user behavior was observed. This experiment was repeated on another small collection with the same perhaps surprising conclusion <ref type="bibr" target="#b258">[258]</ref>. See also a more detailed examination of the experiments <ref type="bibr" target="#b259">[259]</ref>. Using a method of artificially creating ranked document lists each with a different level of MAP, Turpin and Scholer <ref type="bibr" target="#b260">[260]</ref> described a larger experiment that showed some small significant differences in user behavior when there were large differences in MAP between the artificial ranks.</p><p>Smith and Kautor <ref type="bibr" target="#b234">[234]</ref> engaged 36 users to each search 12 information gathering topics on two versions of a web search engine: one the normal searching system, the other a version of the engine which displayed results starting from rank 300, presumably much worse. No significant difference in user success in finding relevant items was observed. Smith et al. reported that users adapted to the poorer system by issuing more queries; this change appeared to mitigate the smaller number of relevant documents retrieved in each search.</p><p>To many researchers, the totality of this work highlighted the artificiality of test collections. Ingwersen and Järvelin <ref type="bibr">[128, p. 234</ref>] provided a detailed survey of past work that outlined the limitations of what an experimental result on a test collection can tell the researcher. The collective results from these works were viewed by some as strong evidence that there was a problem with the test collection methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.2">Some Correlation Found</head><p>Allan et al. <ref type="bibr" target="#b5">[6]</ref> studied the problem of locating relevant text fragments, called facets. The researchers created artificial document rankings displaying fragments and links to full document texts. The rankings were formed starting by randomly degrading a perfect ranking. Users were asked to identify within the rankings, sections of documents that were relevant to a topic. Subjects were given many hours to complete the task. Allan et al. measured the time users took to complete their task, their error rate, and their facet recall. Unlike previous work, the researchers found a correlation between user behavior and test collection-based evaluation measures.</p><p>Huffman and Hochster <ref type="bibr" target="#b123">[124]</ref> addressed the question of how effectively a test collection can be used to predict user satisfaction. They described getting two sets of assessors to judge the search results of 200 queries: the first assessors judged the relevance of the top three results; and the second set of assessors judged user satisfaction with the overall results. The researchers reported finding a correlation between DCG measured on the relevance judgments and user satisfaction.</p><p>Al-Maskari et al. <ref type="bibr">[3]</ref> conducted a small study measuring correlations between user satisfaction measures and different evaluation measures based on examinations of Google searches. She showed that there was a strong correlation between user rankings of results and the ranking produced by the evaluation measures she tested. She found that Cumulative Gain (CG) correlated better with user measures than P(10), DCG, and nDCG. Later, she and others used a test collection to select a pair of retrieval systems that had noticeably different effectiveness scores on a particular topic <ref type="bibr" target="#b3">[4]</ref>. They then measured how well groups of users performed on those two systems for that topic. Fifty-six users searched from a selection of 56 TREC topics. The researchers showed a correlation between test collection experiments and user behavior, though they noted that user satisfaction was harder to predict than more objective measures such as the number of relevant documents saved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.3">Strong Correlation Found</head><p>When conducting an analysis of click log data, Joachims claimed that "It appears that users click on the (relatively) most promising links . . . independent of their absolute relevance" <ref type="bibr" target="#b135">[136]</ref>. He described experimental results showing that users, given different versions of an IR system, clicked at almost the same average rank position, despite there being differences in the effectiveness of the three versions. Joachims highlighted Rees and Schultz's <ref type="bibr" target="#b193">[193]</ref> past work on relative relevance judgments and proposed an alternative approach for measuring user interaction with different systems. His suggestion was to interleave the outputs of the different systems into a single ranking and observe if users tended to click more on results from one ranking over another. The results of this preference-based experiment showed that users chose the results from the better ranking in a statistically significantly measurable way. This work was repeated later by Radlinski et al. <ref type="bibr" target="#b192">[192]</ref>, showing the same results.</p><p>Inspired by Joachims, Thomas and Hawking <ref type="bibr" target="#b256">[256]</ref> presented a different preference methodology that allowed users to express a preference for not only the ranking of a retrieval system but potentially its interface as well. In their methodology, two versions of a search engine result were presented side-by-side to users. Users could query the two engines and interact with them as normal. Thomas et al. presented in the two panels, the top 10 results of Google and the presumably worse Google results in ranks 21-30. The researchers observed a clear statistically significant preference for the results from the top ranks over the lower-ranked results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.4">Discussion</head><p>The work showing little correlation might lead some to question the value of test collections; however, it is notable that many of the studies in the opening sub-section failed to show statistical significance in the user-based tests. A lack of significance can mean that there is no measurable difference or it can mean that the experiment was not powerful enough to allow such differences to be measured (see <ref type="bibr">Section 5)</ref>. The challenge of accurately measuring users was pointed out by Voorhees <ref type="bibr" target="#b275">[274]</ref> who suggested that the experiments, such as those from Hersh and Turpin et al., concluding failure in test collections may in fact have failed to measure their users' behavior accurately enough. Perhaps the strongest conclusions to draw from these collective works is that faced with a poor search, or worse a poor IR system, users either make do with the documents they are shown or they work around the system to manage to achieve their search task successfully. The last tranche of studies contrasts with the former as user's performance was assessed in a relative instead of an absolute way. From that work, it would appear that given a choice between two systems, users prefer to use the better system as a source of retrieval results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Conclusions</head><p>This section examined in some detail the range of research that tested many aspects of the test collection method. Assessor consistency was re-examined and was generally found to be un-problematic. Pooling was found to produce a sample of relevant documents to effectively rank runs. Means of building collections more efficiently were proposed and a number of those methods adopted. Evaluation measures were examined in detail and the importance of selecting the right measure for the right task was highlighted. Finally, the consistency with which test collection results predicted user behavior on operational system was examined. Perhaps the simplest conclusion to draw here is that measuring users accurately requires care.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Alternate Needs and Data Sources for Evaluation</head><p>As shown in Section 6, over the past decade, a detailed examination of the construction and use of test collections was conducted that by and large found the long-standing evaluation methodology to be a valid approach to measuring the effectiveness of IR systems. However, during that period, the needs of at least part of the IR research community changed and at the same time, new potential sources of information about the relevance of documents became more accessible to researchers. In this section, the new need is described and the data sets created for it are outlined. Also two new evaluation data sources are introduced. As much of this work is beyond the scope of a test collection review article, it is described here briefly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Learning to Rank</head><p>Test collections and evaluation measures are commonly used for the purposes of comparison: deciding if one approach or one retrieval system is better than another. However, there is a related use, retrieval function optimization. The ranking functions of IR systems are increasingly complex, containing a wide range of parameters for which optimal settings need to be found. A common approach to finding such values is to use a machine learning approach known as Learning To Rank (LTR). The study of LTR has its origins in the late 1980s <ref type="bibr" target="#b88">[89]</ref>; see <ref type="bibr" target="#b167">[168]</ref> for other early LTR papers. Although a resurgence of interest started around 2005, from the point of view of evaluation, work is still in its infancy. There are two key evaluation areas to consider: data sets and evaluation measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1">Data Sets</head><p>As with any machine learning approach, data is needed to train an LTR retrieval function, which is then tested on a separate data set. In LTR, the data are generally composed of the classic components of a test collection: documents, topics, and qrels. It is notable that in his pioneering work, Fuhr stated that a key concern was the approach "needs large samples of relevance feedback data for its application", by which he meant training and testing data. Fuhr used a test collection with &gt; 240 usable topics <ref type="bibr" target="#b90">[91]</ref>. The first shared LTR data set was the LETOR benchmark <ref type="bibr" target="#b167">[168]</ref>. It was composed of two existing IR test collections: OHSUMED and TREC web, which together had a similar number of topics to Fuhr's earlier collection. A series of features were extracted from all relevant and top-ranked documents in relation to each topic in the data set. Machine learning groups who were not interested in extracting such features from the documents could simply apply the features to their learning algorithms. The collection quickly became a standard for use in LTR experiments. However, it is relatively new and recent publications have suggested that certain biases exist within it <ref type="bibr" target="#b176">[176]</ref>. It is likely that adjustments to LETOR to correct these biases will arise as will the creation of new LTR collections. However, it is not clear if adapting existing IR test collections will produce large enough data sets for the LTR community. Web search companies, such as Yahoo!, have released custom built LTR data sets<ref type="foot" target="#foot_20">1</ref> ; exploiting sources such as query logs to build data sets are an active area of research, which are described in Section 7.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2">Evaluation Measures</head><p>An LTR function is trained with respect to a particular evaluation measure. Liu et al. <ref type="bibr" target="#b167">[168]</ref> described training using a series of common measures: P(n), nDCG, and MAP. It was assumed that the measure to use when optimizing was the one that reflected, most accurately, a model of the user in the operational setting one is optimizing for. Recent research, however, from Yilmaz and Robertson <ref type="bibr" target="#b292">[291]</ref> showed that measures that make the greatest use of available training data can in fact be the better measure to employ. For example, although one might argue that P( <ref type="formula">10</ref>) is a more accurate model of a typical casual search engine user. If one optimizes on that measure, relevance information from only the top 10 documents will be used. If instead, one optimizes on MAP, relevance information from across the document ranking will be used. Yilmaz and Robertson showed that LTR systems trained on MAP and tested on P(10) produced better rank optimization than systems trained and tested on P <ref type="bibr" target="#b9">(10)</ref>.</p><p>Because the range of parameters in a retrieval functions can be very large, it is impossible to exhaustively explore every possible combination. In order to optimize an LTR system effectively, techniques drawn from the machine learning community, such as gradient ascent, are used. However, Robertson and Zaragoza <ref type="bibr" target="#b201">[201]</ref> showed that the current suite of existing evaluation measures are not ideal for use with gradient ascent and related learning techniques. In their paper they argued that new measures need to be built to ensure that optimization can be achieved more successfully. This is likely to be an area that will come to factor more significantly in future evaluation surveys.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Query Logs -Modeling Users</head><p>For as long as automated searching systems existed, logs of activities on those systems were gathered and studied. An early example is Meister and Sullivan <ref type="bibr" target="#b174">[174]</ref>, who, studying the NASA/RECON citation search engine, examined both the volume of searches and the number of retrieved items that were viewed. Inductive studies of user behavior as recorded in query logs continued from that time, growing considerably with the introduction of web search engines and the selective release of large public data sets from them; see <ref type="bibr" target="#b129">[130]</ref> for an overview of that research. Such use of logs in this way was influential in IR researchers' understanding of user behavior, from the shortness of query length to the prevalence of spelling mistakes. Section 6.3.6 briefly mentioned research exploiting data in logs to help generate conventional test collections: Fox et al. <ref type="bibr" target="#b86">[87]</ref> showed that it was possible to use clicks as indicators of relevance. However, more recent research showed that in order to use such data, noise and bias needed to be removed. Noise was introduced to logs by automated programs repeatedly querying a search engine either to gather information or to try to deliberately spam the search engine in some way. Simple methods for identifying the activities of information gathering systems were found to be relatively straightforward: Jansen et al., for example, removed search sessions that had &gt; 100 queries <ref type="bibr" target="#b130">[131]</ref>. Detecting spam data, which will be engineered to be as similar to user interactions as possible, is harder to spot. Description of that work is beyond the scope of this monograph.</p><p>Bias in the query logs arises from the way that users interact with search engines. Joachims et al. <ref type="bibr" target="#b137">[138]</ref> identified two forms of user bias, what they called trust bias (in other publications, this was called presentation bias) and quality bias. Trust bias was given its name due to users' willingness to trust the search engine to find the most relevant item in the top ranks. Joachims demonstrated the strength of this bias by manipulating search results, deliberately placing non-relevant documents in top ranked positions and showing that users still commonly clicked on the top position. With the second form of bias, Joachims showed that when the overall quality of search results was poor, users appeared willing to click on less relevant documents.</p><p>Joachims et al.'s conclusions were that extracting absolute relevance judgments from a query log was hard and as an alternative, proposed that relative or preference judgments should be extracted. For example, if a user clicked on the item in the 2 nd rank position but not the 1 st , one would infer that the item at rank 2 was more relevant than the item at rank 1. Joachims later showed that how such preference judgments were used in LTR <ref type="bibr" target="#b138">[139]</ref>, as did Zheng et al. <ref type="bibr" target="#b295">[294]</ref>.</p><p>Agichtein et al. <ref type="bibr" target="#b0">[1]</ref> used query logs to learn how to customize search results for individual users. They removed trust bias from query logs by building a model of the typical bias toward certain rank positions and then subtracted that bias from the query and click log data of the user under study. This work was notable as it was one of the first to use the technique of click prediction. Here the researchers split the query log into two parts. They trained their system to a particular user (in this case using the first 75% of the log) and then used the system to predict which result that user would click on for the queries they submitted in the remaining part of the log, thus determining if the user model was accurate See also Piwowarski et al. <ref type="bibr" target="#b190">[190]</ref> for further work in this area.</p><p>Joachim's observations of bias in user clicks were an initial attempt to model user behavior when examining search results. A series of models were subsequently proposed and tested on extensive collections of search log data often using click prediction. Craswell et al. <ref type="bibr" target="#b71">[72]</ref> showed how modeling behavior simply based on document rank was not ideal.</p><p>They introduced what they referred to as a cascade model where the probability of a click on a search result was dependent on the probability of the current result being relevant and of the higher ranked results not being relevant. See Dupret and Piwowaeski <ref type="bibr" target="#b79">[80]</ref> and Chapelle et al. <ref type="bibr" target="#b51">[52]</ref> for further extension to and testing of the cascade model.</p><p>Query logs were also used to validate evaluation measures. Chapelle et al. <ref type="bibr" target="#b51">[52]</ref> compared a range of evaluation measures, using a combination of assessor-based relevance judgments and click data from a large query log.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Live Labs</head><p>The involvement of users in evaluation of IR systems has long been advocated and conducted as is recorded and promoted in the works of Ingwersen and Järvelin <ref type="bibr" target="#b127">[128]</ref>, Saracevic <ref type="bibr" target="#b225">[225]</ref>, and Borlund <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>.</p><p>A key limiting factor in the experimental methods promoted by such researchers is the challenge of finding a sufficiency of users. Given there are now very large numbers of people who have high speed access to the Internet, new forms of search evaluation are possible, using what has sometimes been called live labs. This rather broad term covers a range of experimental methodologies, which we outline here.</p><p>An early example was the work of Dumais et al. <ref type="bibr" target="#b78">[79]</ref> who as part of the testing of their desktop search engine, Stuff I've Seen, deployed a working version of the system, which was installed by 234 users (employees of a large organization) who used the system as their desktop search tool. The search engine was instrumented to log certain information about user interaction, which enabled the researchers to understand how the system was used and how often. Unbeknownst to the employees, the researchers randomly deployed different versions of the search interface and using the logs were able to determine how the versions affected searcher behavior. This approach of deploying software to willing volunteers/users was used by others, e.g., <ref type="bibr" target="#b74">[75]</ref>.</p><p>When working with services accessed over a network, such as a search engine, it is possible to make changes to the searching system at regular intervals without the users of the engine to have to install any updates. Such activity was described at a conference panel by Cutting <ref type="bibr" target="#b162">[163]</ref> where he stated that several updates to commercial search engines in a single day was not an unusual occurrence. After each change, search logs could be examined to observe any change in user behavior. Joachims appeared to be the first to publish on this topic (see Section 6.5.3), describing a methodology for measuring user preferences for two different versions of a search engine. A key part of Joachims' approach was that users were unaware they were being given a choice between two different searching systems. <ref type="foot" target="#foot_22">2</ref> In a later paper working with Radlinski et al. <ref type="bibr" target="#b192">[192]</ref>, Joachims, deployed this methodology to a popular academic paper searching system for a month and was able to observe user behavior for over 20,000 queries.</p><p>A number of IR researchers were inspired by von Ahn's ESP game <ref type="bibr" target="#b266">[266]</ref>, where users label images as part of their activities while playing a multi-user game. Clough et al., <ref type="bibr" target="#b61">[62]</ref> keen to study cross language searching created an image finding system built on top of Flickr and through user interactions with the game were able to study interaction. Kazai et al. <ref type="bibr" target="#b149">[150]</ref> created a game that involved players making relevance judgments on documents.</p><p>While enticing as approaches to creating large-scale evaluations or data sets for evaluation, all three methods are challenging to implement. The first requires the software being deployed to be of a high standard before users will willingly engage with it for a long period of time. The second method requires the experimenter to have access to a popular search engine so as to manipulate its results. The third requires highquality software to be developed where the game play is enticing enough for a sufficient number of people to participate.</p><p>There is, however, another approach, as mentioned in Section 6.3.3, it is possible, using services like Mechanical Turk, to pay people to conduct short-run tasks. The small amount of money they are willing to work for means many people can be employed. The example task described in the earlier section was that of judging the relevance of a document for the purposes of building a test collection, however, the potential range of tasks is broader than this: annotating corpora, seeking user opinion of search interfaces, and comparing result rankings are just some of the possibilities. Exploiting systems like Mechanical Turk for IR research is relatively new with little research to review as yet. There are challenges to using such services, but nevertheless, the service is likely to be increasingly used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>This monograph presented a brief history of the development of test collection-based evaluation from the earliest works through to the highly influential TREC exercises. Next a series of prominent evaluation measures were described and research testing the properties of those measures was detailed. The need for and use of significance tests in IR experiments was outlined next. One can see that the One might expect for the research community to discover flaws in such a long-standing methodology. A great deal of research conducted in the past decade has tried specifically to determine if such flaws exist. However, the results of the research are some new evaluation measures; some useful alternatives to the means by which test collections are built; but ultimately the research has validated the test collection approach. The components of a test collection -a set of documents, a set of topics, and a list of qrels -while a somewhat artificial construct remains at the core of experimental validation of new methodologies in IR. It is clear that query logs offer a means of constructing noisy though vast testing sets that are particularly helpful in new lines of IR research such as LTR. However, it is likely that this approach will not be a replacement, instead offering a complementary methodology to the long standing and proven approach of measuring the effectiveness of an IR system on a test collection. and Ray. Tefko Saracevic kindly scanned in some pages from one of his early IR reports. Peter Willett and Micheline Beaulieu's personal collection of books, preprints, and technical reports was an invaluable resource also. Finally, my efforts to locate old articles were helped by the long tradition of IR research conducted in my department in Sheffield, which meant that in a dark, lower basement corner of the University of Sheffield Western Bank Library, a wealth of 1940s, 1950s, and 1960s journals, books, proceedings and reports lay in wait for me and my photocopy card.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 . 2</head><label>22</label><figDesc>Fig.2.2 Representations of the balance between relevant and non-relevant documents across a document ranking. The graph on the left represents a less effective retrieval system than the graph on the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 . 6</head><label>26</label><figDesc>Fig. 2.6 The copies of TIME magazine, the articles of which were manually transcribed to form the TIME test collection.</figDesc><graphic coords="26,241.02,141.26,131.04,98.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>7</head><label>7</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>&lt;top&gt; &lt;num&gt; Number: 200 &lt;title&gt;</head><label>200</label><figDesc>Topic: Impact of foreign textile imports on U.S. textile industry &lt;desc&gt; Description: Document must report on how the importation of foreign textiles or textile products has influenced or impacted on the U.S. textile industry.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3.1 Example TREC ad hoc topic.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 . 2</head><label>32</label><figDesc>Fig. 3.2 Four example documents ranks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>"</head><label></label><figDesc>GMAP treats a change in AP from 0.05 to 0.1 as having the same value as a change from 0.25 to 0.5. MAP would equate the former with a change from 0.25 to 0.3, regarding a change from 0.25 to 0.5 as five times larger."</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 . 1 Fig. 5 . 2</head><label>5152</label><figDesc>Fig. 5.1 Topic-by-topic comparison of two TREC-8 runs based on average precision scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>IR community still uses the model for evaluation initiated by the pioneering work of Thorne, Cleverdon, and Gull in the early 1950s and consolidated by Cleverdon's Cranfield collections of the early 1960s. Most of the evaluation measures used by the community are closely related to the measures created by Gull and Kent et al. in the 1950s. The commonest significance tests used in research papers today are the same as those used by IR researchers in the late 1960s.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>7 Alternate Needs and Data Sources for Evaluation</head><label></label><figDesc></figDesc><table><row><cell>4.1 New Tasks, New Collections</cell></row><row><cell>4.2 Post Ad hoc Measures</cell></row><row><cell>4.3 Are All Topics Equal?</cell></row><row><cell>4.4 Summing Up</cell></row><row><cell>5 Beyond the Mean: Comparison and Significance</cell></row><row><cell>5.1 Significance Tests</cell></row><row><cell>6 Examining the Test Collection</cell></row><row><cell>Methodologies and Measures</cell></row><row><cell>6.1 Re-checking Assessor Consistency</cell></row><row><cell>6.2 Does Pooling Build an Unbiased Sample?</cell></row><row><cell>6.3 Building Pools Efficiently</cell></row><row><cell>6.4 Which is the Best Effectiveness Measure?</cell></row><row><cell>6.5 Do Test Collections or Measures Predict</cell></row><row><cell>User Behavior?</cell></row><row><cell>6.6 Conclusions</cell></row><row><cell>7.1 Learning to Rank</cell></row><row><cell>7.2 Query Logs -Modeling Users</cell></row><row><cell>7.3 Live Labs</cell></row><row><cell>8 Conclusions</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>• x -the total number of documents in the collection that are relevant. Inspired by work from Vickery [265, p. 174], Cleverdon and Keen [61, p. 34] produced a contingency table of all possible quantities that could be calculated. The table is reproduced below including Kent et al.'s original labels.</figDesc><table><row><cell></cell><cell cols="2">Relevant Not-relevant</cell><cell></cell></row><row><cell>Retrieved</cell><cell>a(w)</cell><cell>b</cell><cell>a+ b(m)</cell></row><row><cell cols="2">Not retrieved c</cell><cell>d</cell><cell>c+ d</cell></row><row><cell></cell><cell cols="2">a + c(x) b + d</cell><cell>a+ b + c + d(n)</cell></row></table><note><p>Both Kent et al. and Cleverdon and Keen listed measures that could be created out of combinations of the table's cells. The three that are probably the best known are Precision</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>et al.<ref type="bibr" target="#b171">[172]</ref> Fig.2.3 Recall and Precision calculated and plotted for ranks resulting from two topics.</figDesc><table><row><cell>Rank</cell><cell>Rel</cell><cell>Pr</cell><cell>Rcl</cell><cell>Rank</cell><cell>Rel</cell><cell>Pr</cell><cell>Rcl</cell><cell></cell><cell>100%</cell></row><row><cell>1</cell><cell cols="3">1 100% 20%</cell><cell>1</cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell>90%</cell></row><row><cell>2</cell><cell>0</cell><cell></cell><cell></cell><cell>2</cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell>80%</cell></row><row><cell>3</cell><cell>1</cell><cell cols="2">67% 40%</cell><cell>3</cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell>70%</cell></row><row><cell>4 5 6</cell><cell>0 1 0</cell><cell cols="2">60% 60%</cell><cell>4 5 6</cell><cell cols="3">0 0 1 1 7 % 3 3 %</cell><cell>Precision</cell><cell>40% 50% 60%</cell><cell>Topic 1</cell></row><row><cell>7</cell><cell>0</cell><cell></cell><cell></cell><cell>7</cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell>30%</cell><cell>Topic 2</cell></row><row><cell>8 9</cell><cell>0 0</cell><cell></cell><cell></cell><cell>8 9</cell><cell cols="3">0 1 2 2 % 6 7 %</cell><cell></cell><cell>0% 10% 20%</cell></row><row><cell>10</cell><cell>0</cell><cell></cell><cell></cell><cell>10</cell><cell cols="3">1 30% 100%</cell><cell></cell><cell>0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%</cell></row><row><cell>∞ ∞</cell><cell>10 1</cell><cell cols="2">% 80% 0% 100%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Recall</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Järvelin and Kekäläinen suggested setting b to 2. The ranks in Figure 4.2 show the values of the discount function (in the denominator)</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Rank</cell><cell cols="2">Rel</cell><cell>Rank</cell><cell>Rel</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>2</cell><cell></cell><cell>1</cell><cell>1</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>2</cell><cell>1</cell><cell></cell><cell>2</cell><cell>0</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>3</cell><cell>2</cell><cell></cell><cell>3</cell><cell>2</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>4</cell><cell>0</cell><cell></cell><cell>4</cell><cell>1</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>5</cell><cell>1</cell><cell></cell><cell>5</cell><cell>2</cell><cell></cell><cell></cell></row><row><cell cols="4">Fig. 4.1 Two document rankings.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Rank Rel Disc Rel/Disc DCG</cell><cell cols="5">Rank Rel Disc Rel/Disc DCG</cell></row><row><cell>1</cell><cell>2</cell><cell>1.00</cell><cell>2.0</cell><cell cols="2">2.0</cell><cell>1</cell><cell>1</cell><cell>1.00</cell><cell>1.0</cell><cell>1.0</cell></row><row><cell>2</cell><cell>1</cell><cell>1.00</cell><cell>1.0</cell><cell cols="2">3.0</cell><cell>2</cell><cell>0</cell><cell>1.00</cell><cell>0.0</cell><cell>1.0</cell></row><row><cell>3</cell><cell>2</cell><cell>1.58</cell><cell>1.3</cell><cell cols="2">4.3</cell><cell>3</cell><cell>2</cell><cell>1.58</cell><cell>1.3</cell><cell>2.3</cell></row><row><cell>4</cell><cell>0</cell><cell>2.00</cell><cell>0.0</cell><cell cols="2">4.3</cell><cell>4</cell><cell>1</cell><cell>2.00</cell><cell>0.5</cell><cell>2.8</cell></row><row><cell>5</cell><cell>1</cell><cell>2.32</cell><cell>0.4</cell><cell cols="2">4.7</cell><cell>5</cell><cell>2</cell><cell>2.32</cell><cell>0.9</cell><cell>3.6</cell></row><row><cell cols="7">Fig. 4.2 Document rankings with discount values.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">Rank Rel Disc Rel/Disc IDCG</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>2</cell><cell>1.00</cell><cell>2.0</cell><cell cols="2">2.0</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>2</cell><cell>2</cell><cell>1.00</cell><cell>2.0</cell><cell cols="2">4.0</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>3</cell><cell>1</cell><cell>1.58</cell><cell>0.6</cell><cell cols="2">4.6</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>4</cell><cell>1</cell><cell>2.00</cell><cell>0.5</cell><cell cols="2">5.1</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>5</cell><cell>0</cell><cell>2.32</cell><cell>0.0</cell><cell cols="2">5.1</cell><cell></cell></row><row><cell cols="6">Fig. 4.3 Perfect ordering of relevant documents.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Rankings from two different topics that result in the same nDCG.</figDesc><table><row><cell cols="5">Rank Rel Disc Rel/Disc DCG IDCG</cell><cell cols="5">Rank Rel Disc Rel/Disc DCG IDCG</cell></row><row><cell>1</cell><cell>2 1.00</cell><cell>2.0</cell><cell>6.7</cell><cell>6.7</cell><cell>1</cell><cell>1 1.00</cell><cell>1.0</cell><cell>4.6</cell><cell>4.6</cell></row><row><cell>2</cell><cell>2 1.00</cell><cell>2.0</cell><cell>8.7</cell><cell>8.7</cell><cell>2</cell><cell>1 1.00</cell><cell>1.0</cell><cell>5.6</cell><cell>5.6</cell></row><row><cell>3</cell><cell>2 1.58</cell><cell>1.3</cell><cell>10.0</cell><cell>10.0</cell><cell>3</cell><cell>1 1.58</cell><cell>0.6</cell><cell>6.3</cell><cell>6.3</cell></row><row><cell>4</cell><cell>0 2.00</cell><cell>0.0</cell><cell>10.0</cell><cell>10.0</cell><cell>4</cell><cell>0 2.00</cell><cell>0.0</cell><cell>6.3</cell><cell>6.3</cell></row><row><cell>5</cell><cell>0 2.32</cell><cell>0.0</cell><cell>10.0</cell><cell>10.0</cell><cell>5</cell><cell>0 2.32</cell><cell>0.0</cell><cell>6.3</cell><cell>6.3</cell></row><row><cell>Fig. 4.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>This is not to say that researchers haven't tried to devise non-empirical approaches, such as building theoretical models of IR systems. However, Robertson<ref type="bibr" target="#b197">[197]</ref> points out that a theory of IR that would allow one to predict performance without evaluation remains elusive.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Indeed, it would appear that the academic IR community is one of the first in the Human Language Technologies (HLT) discipline of computer science to create and share common testing environments. Many other areas of HLT, such as summarization, or word sense disambiguation did not start building such shared testing resources until the 1990s.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_2"><p>Holmstrom described a "machine called the Univac" capable of searching for text references associated with a subject code. The code and text were stored on a magnetic steel tape. Holmstrom stated that the machine could process "at the rate of 120 words per minute"<ref type="bibr" target="#b122">[123]</ref>. Note, the UNIVAC isn't generally thought to have come into existence until 1951, the date when the first machine was sold, Holmstrom presumably saw or was told about a pre-production version. See also Mooers -creator of the term information retrieval -for further historical references to mechanical searching devices of the early twentieth century<ref type="bibr" target="#b181">[181]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_3"><p><ref type="bibr" target="#b1">2</ref> 1955, "The Truth, The Whole Truth. . . " American Documentation. Vol. 6 p. 58; it was not possible to locate this editorial.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_4"><p>Year is either the year when the document describing the collection was published or the year of the first reference to use of the collection.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_5"><p>Note, the axes and their labels are changed here from the way that Cleverdon drew the graph, so as to reflect the modern convention in presenting such data.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_6"><p>Note, this interpolation measure is sometimes mistakenly thought to be the maximum precision measured between the recall levels r i and r i+1 . See Harmandas et al.<ref type="bibr" target="#b107">[108]</ref> and Baeza-Yates and Ribeiro-Neto<ref type="bibr" target="#b17">[18,</ref> Section 3]  as examples of researchers who made this error.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_7"><p>A personal communication to the author.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_8"><p>http://ir.dcs.gla.ac.uk/resources/test collections/ (accessed April 2010).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_9"><p>Note, the literature is a little confused on how the CACM collection's relevance judgments were formed. A later article briefly mentioned the CACM stating that all documents were examined for relevance[213, p. 1030].</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_10"><p>Year is either the year when the document describing the collection was published or the year of the first reference to use of the collection.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_11"><p>http://trec.nist.gov/data/reljudge eng.html (accessed April</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_12"><p>2010).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_13"><p>There does not appear to be any paper or technical report describing trec eval. It can be downloaded from the following URL: http://trec.nist.gov/trec eval/ (accessed April 2010).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_14"><p>Pronounced "clay", from the French word for key.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_15"><p>One might view such caution as an inhibition to research. However, ignoring such concerns is not without risk: in a personal communication with the author, the head of a research group who were regularly crawling blogs (respecting conventions and protocols) hit problems when a blog owner went to the press claiming the researchers were spying, causing the head significant work in placating his University and the blog owner. A better-known example was the release of query logs from AOL in 2006. Although the logs were anonymized, sufficient session information was preserved to allow some people to be identified<ref type="bibr" target="#b20">[21]</ref>. The consequence of this privacy failure was the sacking of two employees and the resignation of a company executive<ref type="bibr" target="#b293">[292]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_16"><p>In later TRECs aspects were called instances<ref type="bibr" target="#b120">[121]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_17"><p>The runs are from TREC-8, INQ604, ok8alx, and CL99XT. The later is a manual run, which probably explains the more erratic difference between it and the others in Figure5.2.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_18"><p>Savoy cites Efron and Tibshirani<ref type="bibr" target="#b80">[81,</ref><ref type="bibr" target="#b81">82]</ref>; Léger et al.<ref type="bibr" target="#b160">[161]</ref> as the originators of the test.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_19"><p>It is also worth noting in the c and b comparison how the Wilcoxon and t-tests produced p values below the 0.05 threshold but the sign test did not. The former tests were more influenced by the substantial improvements of c over b in some topics. The sign test ignored the size of a difference; considering only the sign of the difference.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_20"><p>http://learningtorankchallenge.yahoo.com/ (accessed April</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_21"><p><ref type="bibr" target="#b25">26,</ref> 2010).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_22"><p>Cooper [67]  proposed an evaluation methodology where experimenters would go to the site where an IR system was being used and observe a random sample of users conducting their search tasks on either an existing system or a new trial system. The users would not know which system they were being shown.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Index</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>I am most grateful to Paul Clough and Peter Willett for their helpful comments while preparing this monograph and two of my students Azzah Al-Maskari and Shahram Sedghi, both of whom were working on evaluation topics and whose work and conversations were particularly stimulating while writing. In addition, Evangelos Kanoulas, Ian Soboroff, Chris Buckley, and Ellen Voorhees at TREC were continually helpful in charting out more recent developments in evaluation and being willing to discuss some of the finer (and admittedly quite nerdy) points of IR evaluation. Stefan Rüger, Peter Bath, and Andrew Holmes were a tremendous help in guiding my understanding of significance tests. Finally, I wish to thank the reviewers for their detailed and invaluable comments after examining the earlier versions of this monograph.</p><p>Obtaining primary sources in the early history of IR evaluation research would have been much harder had it not been for the invaluable and timely help of Donna Harman, whose formation of a digital library of the key IR texts made access to the old Cornell and Cranfield reports trivially easy. In addition Bill Maron and Keith van Rijsbergen helped me obtain copies of the early reports from Maron, Kuhns,</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Index</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning user interaction models for predicting web search result preferences</title>
		<author>
			<persName><forename type="first">E</forename><surname>Agichtein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ragno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Diversifying search results</title>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gollapudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Halverson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ieong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Second ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="5" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The relationship between IR effectiveness measures and user satisfaction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Al-Maskari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="773" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The good and the bad system: Does the test collection predict users&apos; effectiveness?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Al-Maskari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Airio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="59" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Topic Detection and Tracking: Event-based Information Organization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">The Kluwer International Series on Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2002">2002</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>st ed.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">When will information retrieval be &quot;good enough&quot;?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="433" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Crowdsourcing for relevance evaluation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="9" to="15" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Practical Statistics for Medical Research</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Altman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Chapman &amp; Hall/ CRC</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>st ed.</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scaling IR-system Evaluation using Term Relevance Sets</title>
		<author>
			<persName><forename type="first">E</forename><surname>Amitay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lempel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Soffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information retrieval</title>
		<meeting>the 27th Annual International ACM SIGIR Conference on Research and Development in Information retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="10" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Overview of the Image-CLEFphoto 2008 photographic retrieval task</title>
		<author>
			<persName><forename type="first">T</forename><surname>Arni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grubinger</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-04447-262</idno>
	</analytic>
	<monogr>
		<title level="m">Evaluating Systems for Multilingual and Multimodal Information Access</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="volume">5706</biblScope>
			<biblScope unit="page" from="500" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Web people search: Results of the first evaluation and the plan for the second</title>
		<author>
			<persName><forename type="first">J</forename><surname>Artiles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sekine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on World Wide Web</title>
		<meeting>the 17th International Conference on World Wide Web<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1071" to="1072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A unified model for metasearch, pooling, and system evaluation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pavlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Savell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth International Conference on Information and Knowledge Management</title>
		<meeting>the Twelfth International Conference on Information and Knowledge Management<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="484" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A statistical method for system evaluation using incomplete judgments</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pavlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="541" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A geometric interpretation of r-precision and its correlation with average precision</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pavlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="573" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The maximum entropy method for analyzing retrieval measures</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pavlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Building simulated queries for known-item topics: An analysis using six European languages</title>
		<author>
			<persName><forename type="first">L</forename><surname>Azzopardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="455" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Retrievability: An evaluation measure for higher order information access tasks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Azzopardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vinay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM Conference on Information and Knowledge Management</title>
		<meeting>the 17th ACM Conference on Information and Knowledge Management<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="561" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
		<title level="m">Modern Information Retrieval</title>
		<imprint>
			<publisher>Addison Wesley</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Relevance assessment: Are judges exchangeable and does it matter</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="667" to="674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A retrieval evaluation methodology for incomplete relevance assessments</title>
		<author>
			<persName><forename type="first">M</forename><surname>Baillie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Azzopardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ruthven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4425</biblScope>
			<biblScope unit="page" from="271" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A face is exposed for AOL Searcher No. 4417749</title>
		<author>
			<persName><forename type="first">M</forename><surname>Barbaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zeller</surname><genName>Jr</genName></persName>
		</author>
		<ptr target="http://www.nytimes.com/2006/08/09/technology/09aol.html" />
	</analytic>
	<monogr>
		<title level="j">The New York Times</title>
		<imprint>
			<date type="published" when="2006-08-09">August 9 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">TREC-2006 legal track overview</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC 2006) Proceedings, NIST Special Publication</title>
		<meeting><address><addrLine>Gaithersburg, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">500</biblScope>
			<biblScope unit="page" from="79" to="98" />
		</imprint>
		<respStmt>
			<orgName>Department of Commerce, National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
	<note>The Fifteenth Text REtrieval Conference</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Online before the internet, Part 1: Early pioneers tell their stories</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bjørner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Ardito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Searcher: The Magazine for Database Professionals</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">STAIRS redux: Thoughts on the STAIRS evaluation, ten years after</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Blair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="22" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An evaluation of retrieval effectiveness for a full-text document-retrieval system</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Blair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Maron</surname></persName>
		</author>
		<idno type="DOI">10.1145/3166.3197</idno>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="289" to="299" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Test theory for assessing IR test collections</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bodoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="367" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On the robustness of relevance measures with incomplete judgments</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bompada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shenoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 30th annual international ACM SIGIR conference on Research and development in information retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="359" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">When the most &quot;pertinent&quot; document should not be retrieved -An analysis of the Swets model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bookstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Processing &amp; Management</title>
		<imprint>
			<date type="published" when="1977">1977</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="377" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Borko</surname></persName>
		</author>
		<idno>Sp-909/000/00</idno>
		<title level="m">Evaluating The: Effectiveness of Information Retrieval Systems</title>
		<meeting><address><addrLine>Santa Monica, California</addrLine></address></meeting>
		<imprint>
			<publisher>Systems Development Corporation</publisher>
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The concept of relevance in IR</title>
		<author>
			<persName><forename type="first">P</forename><surname>Borlund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="913" to="925" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The development of a method for the evaluation of interactive information retrieval systems</title>
		<author>
			<persName><forename type="first">P</forename><surname>Borlund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ingwersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="225" to="250" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A paradigm for a retrieval effectiveness experiment</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bornstein</surname></persName>
		</author>
		<idno type="DOI">10.1002/asi.5090120403</idno>
		<imprint>
			<date type="published" when="1961">1961</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="254" to="259" />
		</imprint>
	</monogr>
	<note>American Documentation</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cross-language evaluation forum: Objectives, results, achievements</title>
		<author>
			<persName><forename type="first">M</forename><surname>Braschler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="7" to="31" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A taxonomy of web search</title>
		<author>
			<persName><forename type="first">A</forename><surname>Broder</surname></persName>
		</author>
		<idno type="DOI">10.1145/792550.792552</idno>
	</analytic>
	<monogr>
		<title level="m">SIGIR Forum</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Progress towards evaluation of information retrieval systems</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Bryant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Retrieval Among Examining Patent Offices: 4th Annual Meeting of the Committee for International Cooperation in Information Retrieval among Examining Patent Offices (ICIREPAT)</title>
		<imprint>
			<publisher>Macmillan</publisher>
			<date type="published" when="1966">1966</date>
			<biblScope unit="page" from="362" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Bias and the limits of pooling for large collections</title>
		<author>
			<persName><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dimmick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="491" to="508" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Evaluating evaluation measure stability</title>
		<author>
			<persName><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Retrieval evaluation with incomplete information</title>
		<author>
			<persName><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Retrieval system evaluation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC: Experiment and Evaluation in Information Retrieval</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="53" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning to rank using gradient descent</title>
		<author>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Renshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lazier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Deeds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hullender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Machine Learning</title>
		<meeting>the 22nd International Conference on Machine Learning<address><addrLine>Bonn, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Variations in relevance judgments and the evaluation of retrieval performance</title>
		<author>
			<persName><forename type="first">R</forename><surname>Burgin</surname></persName>
		</author>
		<idno type="DOI">10.1016/0306-4573(92)90031-T</idno>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="619" to="627" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The TREC 2006 terabyte track</title>
		<author>
			<persName><forename type="first">S</forename><surname>Büttcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Text Retrieval Conference (TREC 2006)</title>
		<meeting>the Fifteenth Text Retrieval Conference (TREC 2006)<address><addrLine>Maryland, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Gaithersburg</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">500</biblScope>
			<biblScope unit="page" from="128" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Reliable information retrieval evaluation with incomplete and biased judgements</title>
		<author>
			<persName><forename type="first">S</forename><surname>Büttcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C K</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="63" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Automatic performance evaluation of Web search engines</title>
		<author>
			<persName><forename type="first">F</forename><surname>Can</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nuray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Sevdik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="495" to="514" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Robust test collections for retrieval evaluation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="55" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">On rank correlation and the distance between rankings</title>
		<author>
			<persName><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="436" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Minimal test collections for retrieval evaluation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sitaraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="268" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Here or There</title>
		<author>
			<persName><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chickering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-78646-75</idno>
		<ptr target="http://dx.doi.org/10.1007/978-3-540-78646-75" />
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="16" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Evaluating search engines by modeling the relationship between relevance and clicks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Annual Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>the 21st Annual Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="217" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Evaluating search engines by modeling the relationship between relevance and clicks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="217" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Evaluation over thousands of queries</title>
		<author>
			<persName><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pavlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="651" to="658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Expected reciprocal rank for graded relevance</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metlzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grinspan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM conference on Information and knowledge management</title>
		<meeting>the 18th ACM conference on Information and knowledge management<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="621" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Less is more: Probabilistic models for retrieving fewer relevant documents</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Karger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="429" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>Preliminary report on the TREC 2009 Web track,&quot; Working notes of the proceedings of TREC 2009</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Novelty and diversity in information retrieval evaluation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kolla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ashkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Büttcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mackinnon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="659" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">An effectiveness measure for ambiguous and underspecified queries</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kolla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vechtomova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval Theory: Second International Conference on the Theory of Information Retrieval</title>
		<meeting><address><addrLine>Cambridge, UK; New York Inc</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2009-09-10">2009. September 10-12, 2009. 2009</date>
			<biblScope unit="page" from="188" to="199" />
		</imprint>
	</monogr>
	<note>Proceedings</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">The evaluation of systems used in information retrieval (1958: Washington)</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Cleverdon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Scientific Information -Two Volumes</title>
		<meeting>the International Conference on Scientific Information -Two Volumes<address><addrLine>Washington</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1959">1959</date>
			<biblScope unit="page" from="687" to="698" />
		</imprint>
		<respStmt>
			<orgName>National Academy of Sciences, National Research Council</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Cleverdon</surname></persName>
		</author>
		<title level="m">Report on the Testing and Analysis of an Investigation Into the Comparative Efficiency of Indexing Systems. ASLIB Cranfield Research Project</title>
		<meeting><address><addrLine>Cranfield, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">The Effect of Variations in Relevance Assessments in Comparative Experimental Tests of Index Languages</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Cleverdon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970">1970</date>
			<publisher>Cranfield Institute of Technology</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Cranfield Library Report</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The significance of the cranfield tests on index languages</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Cleverdon</surname></persName>
		</author>
		<idno type="DOI">10.1145/122860.122861</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 14th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Chicago, Illinois, United States; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Factors Affecting the Performance of Indexing Systems</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Cleverdon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Keen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1966">1966</date>
			<publisher>C. Cleverdon</publisher>
			<biblScope unit="page" from="37" to="59" />
			<pubPlace>Bedford, UK</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Cranfield Research Project</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Large-scale interactive evaluation of multilingual information access systems -The iCLEF flickr challenge</title>
		<author>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Karlgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Artiles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Peinado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop on Novel Methodologies for Evaluation in Information Retrieval</title>
		<meeting>Workshop on Novel Methodologies for Evaluation in Information Retrieval<address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="33" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">The CLEF 2005 cross-language image retrieval track</title>
		<author>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Accessing Multilingual Information Repositories</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">4022</biblScope>
			<biblScope unit="page" from="535" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">The CLEF cross language image retrieval track (ImageCLEF) 2004</title>
		<author>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture notes in Computer Science</title>
		<imprint>
			<biblScope unit="page" from="243" to="251" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Expected search length: A single measure of retrieval effectiveness based on the weak ordering action of retrieval systems</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Cooper</surname></persName>
		</author>
		<idno type="DOI">10.1002/asi.5090190108</idno>
		<imprint>
			<date type="published" when="1968">1968</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="30" to="41" />
		</imprint>
	</monogr>
	<note>American Documentation</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A definition of relevance for information retrieval</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information storage and retrieval</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="37" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">On selecting a measure of retrieval effectiveness</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Statistical precision of information retrieval evaluation</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Lynam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="533" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Efficient construction of large test collections</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Overview of the trec-2005 enterprise track</title>
		<author>
			<persName><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Text Retrieval Conference (TREC 2005)</title>
		<meeting>the Fourteenth Text Retrieval Conference (TREC 2005)<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Overview of the TREC 2002 Web track</title>
		<author>
			<persName><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh Text Retrieval Conference (TREC-2002), NIST Special Publication 500-251</title>
		<meeting><address><addrLine>Gaithersburg, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="86" to="95" />
		</imprint>
		<respStmt>
			<orgName>Department of Commerce, National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">An experimental comparison of click position-bias models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Zoeter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramsey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international conference on Web search and web data mining</title>
		<meeting>the international conference on Web search and web data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="87" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">A file organization for cluster-based retrieval</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Annual International ACM SIGIR Conference on Information Storage and Retrieval</title>
		<meeting>the 1st Annual International ACM SIGIR Conference on Information Storage and Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1978">1978</date>
			<biblScope unit="page" from="65" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Search Engines: Information Retrieval in Practice</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Addison Wesley</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>st ed.</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Fast, flexible filtering with phlat</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cutrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sarin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="261" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">A Document Test Collection for Use in Information Retrieval Research</title>
		<author>
			<persName><forename type="first">A</forename><surname>Davies</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
		</imprint>
		<respStmt>
			<orgName>Information Studies. University of Sheffield</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Dissertation). Department of</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">A classification of IR effectiveness metrics</title>
		<author>
			<persName><forename type="first">G</forename><surname>Demartini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mizzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">3936</biblScope>
			<biblScope unit="page" from="488" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Index manipulation and abstract retrieval by computer</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Dennis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Dovel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Documentation</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="234" to="242" />
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Stuff I&apos;ve seen: A system for personal information retrieval and re-use</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cutrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Cadiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jancke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sarin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Robbins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="72" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">A user browsing model to predict search engine click data from past observations</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dupret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Piwowarski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="331" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Bootstrap methods for standard errors, confidence intervals, and other measures of statistical accuracy</title>
		<author>
			<persName><forename type="first">B</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="54" to="77" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">An introduction to the bootstrap</title>
		<author>
			<persName><forename type="first">B</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Monographs on Statistics and Applied Probability</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="1" to="177" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Implications of test procedures</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Fairthorne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Retrieval in Action</title>
		<meeting><address><addrLine>Cleveland, Ohio, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Western Reserve UP</publisher>
			<date type="published" when="1963">1963</date>
			<biblScope unit="page" from="109" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Evaluation of the performance of an information-retrieval system by modified Mooers plan</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Fels</surname></persName>
		</author>
		<idno type="DOI">10.1002/asi.5090140105</idno>
		<imprint>
			<date type="published" when="1963">1963</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="28" to="34" />
		</imprint>
	</monogr>
	<note>American Documentation</note>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Characterization of two new experimental collections in computer and information science containing textual and bibliographic concepts</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<ptr target="http://techreports.library.cornell.edu:8081/Dienst/UI/1.0/Display/cul.cs/TR83-561" />
		<imprint>
			<date type="published" when="1983">1983</date>
		</imprint>
		<respStmt>
			<orgName>Computer Science Technical Reports). Cornell University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<title level="m">Virginia Disc One</title>
		<meeting><address><addrLine>Blacksburg, VA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Produced by Nimbus Records</publisher>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Evaluating implicit measures to improve web search</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Karnawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mydland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="147" to="168" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Determining the effectiveness of retrieval algorithms</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Frei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schäuble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing and Management: An International Journal</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="153" to="164" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Optimum polynomial retrieval functions based on the probability ranking principle</title>
		<author>
			<persName><forename type="first">N</forename><surname>Fuhr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="183" to="204" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">INEX: INitiative for the Evaluation of XML retrieval</title>
		<author>
			<persName><forename type="first">N</forename><surname>Fuhr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gövert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kazai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lalmas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGIR 2002 Workshop on XML and Information Retrieval</title>
		<meeting>the SIGIR 2002 Workshop on XML and Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Retrieval test evaluation of a rule based automatic indexing (AIR/PHYS)</title>
		<author>
			<persName><forename type="first">N</forename><surname>Fuhr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Knorz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 7th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>UK</addrLine></address></meeting>
		<imprint>
			<publisher>British Computer Society Swindon</publisher>
			<date type="published" when="1984">1984</date>
			<biblScope unit="page" from="391" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">The TREC spoken document retrieval track: A success story</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Garofolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G P</forename><surname>Auzanne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Text Retrieval Conference (TREC-8)</title>
		<meeting>the Eighth Text Retrieval Conference (TREC-8)<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="107" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Geo-CLEF: The CLEF 2005 cross-language geographic information retrieval track overview</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Joho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Petras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Accessing Multilingual Information Repositories</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">4022</biblScope>
			<biblScope unit="page" from="908" to="919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Mindless statistics</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gigerenzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Socio-Economics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="587" to="606" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">On relevance as a measure</title>
		<author>
			<persName><forename type="first">W</forename><surname>Goffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Storage and Retrieval</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="201" to="203" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Message Understanding Conference-6: A brief history</title>
		<author>
			<persName><forename type="first">R</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sundheim</surname></persName>
		</author>
		<ptr target="http://portal.acm.org/citation.cfm?id=992628.992709" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference on Computational Linguistics</title>
		<meeting>the 16th Conference on Computational Linguistics<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="466" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Seven years of work on the organization of materials in the special library</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Gull</surname></persName>
		</author>
		<idno type="DOI">10.1002/asi.5090070408</idno>
	</analytic>
	<monogr>
		<title level="j">American Documentation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="320" to="329" />
			<date type="published" when="1956">1956</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Evaluation issues in information retrieval</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="439" to="440" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Overview of the second text retrieval conference (TREC-2)</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIST Special Publication. Presented at the Second Text Retrieval Conference (TREC 2)</title>
		<meeting><address><addrLine>Gaithersburg, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
		<respStmt>
			<orgName>Department of Commerce, National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Overview of the third text retrieval conference (TREC-3)</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Third Text Retrieval Conference (TREC-3)</title>
		<meeting><address><addrLine>Gaithersburg, MD, USA; Gaithersburg, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
		<respStmt>
			<orgName>Department of Commerce, National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Overview of the fourth text retrieval conference (TREC-4)</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Forth Text Retrieval Conference (TREC-4)</title>
		<meeting><address><addrLine>Gaithersburg, MD, USA; Gaithersburg, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
		<respStmt>
			<orgName>Department of Commerce, National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Overview of the second text retrieval conference (TREC-2)</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="271" to="289" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Overview of the TREC 2002 novelty track</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh Text Retrieval Conference (TREC 2002)</title>
		<meeting>the Eleventh Text Retrieval Conference (TREC 2002)<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="46" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Some Interesting Unsolved Problems in Information Retrieval</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</author>
		<ptr target="http://www.clsp.jhu.edu/ws02/preworkshop/lectureharman.shtml" />
	</analytic>
	<monogr>
		<title level="m">Presented at the Center for Language and Speech Processing</title>
		<meeting><address><addrLine>Barton Hall Baltimore, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-07-02">2002. July 2 2002</date>
		</imprint>
	</monogr>
	<note>The Johns Hopkins University 3400 North Charles Street</note>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">The TREC ad hoc experiments</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC: Experiment and Evaluation in Information Retrieval (Digital Libraries and Electronic Publishing)</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="79" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">The NRRC reliable information access (RIA) workshop</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="528" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Retrieving records from a gigabyte of text on a minicomputer using statistical ranking</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Candela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="581" to="589" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Image retrieval by hypertext links</title>
		<author>
			<persName><forename type="first">V</forename><surname>Harmandas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Dunlop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 20th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="296" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Evaluating strategies for similarity search on the web</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Haveliwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on World Wide Web</title>
		<meeting>the 11th International Conference on World Wide Web<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="432" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Overview of the TREC-9 Web track</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ninth Text Retrieval Conference (TREC-9)</title>
		<meeting><address><addrLine>Gaithersburg, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="87" to="102" />
		</imprint>
		<respStmt>
			<orgName>Department of Commerce, National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
	<note>NIST Special Publication</note>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">ACSys TREC-8 Experiments</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth Text Retrieval Conference (TREC-8)</title>
		<title level="s">NIST Special Publication</title>
		<meeting><address><addrLine>Gaithersburg, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="307" to="316" />
		</imprint>
		<respStmt>
			<orgName>Department of Commerce, National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Overview of TREC-7 very large collection track</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Thistlewaite</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Seventh Text Retrieval Conference (TREC-7)</title>
		<meeting><address><addrLine>Gaithersburg, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="91" to="104" />
		</imprint>
		<respStmt>
			<orgName>Department of Commerce, National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">How valuable is external link evidence when searching enterprise Webs?</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Crimmins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Australasian database conference</title>
		<meeting>the 15th Australasian database conference<address><addrLine>Darlinghurst, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Australian Computer Society, Inc</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="77" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">On collection size and retrieval effectiveness</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="99" to="105" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Overview of TREC-6 very large collection track</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Thistlewaite</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Sixth Text Retrieval Conference (TREC-6)</title>
		<meeting><address><addrLine>Gaithersburg, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="93" to="106" />
		</imprint>
		<respStmt>
			<orgName>Department of Commerce, National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
		<title level="m">Search User Interfaces</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>st ed.</note>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Subtopic structuring for full-length document access</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Plaunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="59" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">OHSUMED: An interactive retrieval evaluation and new large test collection for research</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Leone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hickam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag New York, Inc</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="192" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">TREC 2006 genomics track overview</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Rekapalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Fifteenth Text Retrieval Conference</title>
		<meeting><address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="52" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Factors associated with success in searching MEDLINE and applying evidence to answer clinical questions</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Crabtree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Hickam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sacherek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tidmarsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mosbaek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">TREC-9 Interactive Track Report</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Over</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the Ninth Text REtrieval Conference (TREC-9)</title>
		<meeting>the Ninth Text REtrieval Conference (TREC-9)<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>NTIS</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="41" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Do batch and user evaluations give the same results?</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Turpin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sacherek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Olson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Opening plenary session</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Holmstrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Royal Society Scientific Information Conference</title>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>Royal Society</publisher>
			<date type="published" when="1948-07-02">21 June-2 July 1948. 1948</date>
		</imprint>
	</monogr>
	<note>Section III</note>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">How well does result relevance predict session satisfaction?</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Huffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hochster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="567" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Using statistical testing in the evaluation of retrieval experiments</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="329" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Students&apos; search process and outcome in Medline in writing an essay for a class on evidence-based medicine</title>
		<author>
			<persName><forename type="first">S</forename><surname>Huuskonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vakkari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="287" to="303" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">New experiments in relevance feedback</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Report ISR-14 to the National Science Foundation</title>
		<imprint>
			<date type="published" when="1968">1968</date>
		</imprint>
		<respStmt>
			<orgName>Cornell University, Department of Computer Science</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<monogr>
		<title level="m" type="main">The Turn: Integration of Information Seeking and Retrieval in Context</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ingwersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Järvelin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Overview of patent retrieval task at NTCIR-3</title>
		<author>
			<persName><forename type="first">M</forename><surname>Iwayama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fujii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Takano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the third NTCIR Workshop on Research in Information Retrieval, Automatic Text Summarization and Question Answering</title>
		<meeting>the third NTCIR Workshop on Research in Information Retrieval, Automatic Text Summarization and Question Answering</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">How are we searching the World Wide Web? A comparison of nine search engine transaction logs</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Spink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="248" to="263" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Web searcher interaction with the Dogpile.com metasearch engine</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Spink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Koshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="744" to="744" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">IR evaluation methods for retrieving highly relevant documents</title>
		<author>
			<persName><forename type="first">K</forename><surname>Järvelin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kekäläinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Cumulated gain-based evaluation of IR techniques</title>
		<author>
			<persName><forename type="first">K</forename><surname>Järvelin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kekäläinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="422" to="446" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<monogr>
		<title level="m" type="main">Repeatable Evaluation of Information Retrieval Effectiveness In Dynamic Environments</title>
		<author>
			<persName><forename type="first">E</forename><surname>Jensen</surname></persName>
		</author>
		<ptr target="http://ir.iit.edu/∼ej/jensenphdthesis.pdf" />
		<imprint>
			<date type="published" when="2006-05">May 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Repeatable evaluation of search services in dynamic environments</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Beitzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Frieder</surname></persName>
		</author>
		<idno type="DOI">10.1145/1292591.1292592</idno>
	</analytic>
	<monogr>
		<title level="m">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Evaluating retrieval performance using clickthrough data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGIR Workshop on Mathematical/Formal Methods in Information Retrieval</title>
		<meeting>the SIGIR Workshop on Mathematical/Formal Methods in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="12" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Optimizing search engines using clickthrough data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="133" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Accurately interpreting clickthrough data as implicit feedback</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Granka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hembrooke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="154" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Search engines that learn from implicit feedback</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Radlinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="page" from="34" to="40" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<monogr>
		<title level="m" type="main">INEX 2007 evaluation measures</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pehcevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kazai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lalmas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-85902-42</idno>
		<ptr target="http://dx.doi.org/10.1007/978-3-540-85902-42" />
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="24" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Evaluation of information access technologies at the NTCIR workshop</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Comparative Evaluation of Multilingual Information Access Systems. 4th Workshop of the Cross-Language Evaluation Forum, CLEF</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="29" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Overview of IR tasks at the first NTCIR workshop</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kuriyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nozue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Eguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hidaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First NTCIR Workshop on Research in Japanese Text Retrieval and Term Recognition</title>
		<meeting>the First NTCIR Workshop on Research in Japanese Text Retrieval and Term Recognition</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="11" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">The TREC-5 Confusion Track</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kantor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Fifth Text Retrieval Conference (TREC-5)</title>
		<meeting><address><addrLine>Gaithersburg, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
		<respStmt>
			<orgName>Department of Commerce, National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<monogr>
		<title level="m" type="main">The TREC-5 Confusion Track: Comparing Retrieval Methods for Scanned Text</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Kantor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="165" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">The influence of scale form on relevance judgments</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Katter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Storage and Retrieval</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">A study of the overlap among document representations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Katzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Mcgill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Tessier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Frakes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dasgupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Technology: Research and Development</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="261" to="274" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<monogr>
		<title level="m" type="main">A Study of the Impact of Representations in Information Retrieval Systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Katzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Tessier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Frakes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dasgupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981">1981</date>
			<publisher>School of Information Studies</publisher>
			<pubPlace>Syracuse, New York</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Syracuse University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">INEX 2005 evaluation measures</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kazai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lalmas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in XML Information Retrieval and Evaluation</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">3977</biblScope>
			<biblScope unit="page" from="16" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">The overlap problem in contentoriented XML retrieval evaluation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kazai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lalmas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="72" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Towards methods for the collective gathering and quality control of relevance assessments</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kazai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Milic-Frayling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Costello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="452" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Evaluation parameters</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Keen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Report ISR-13 to the National Science Foundation</title>
		<imprint>
			<date type="published" when="1967">1967</date>
		</imprint>
		<respStmt>
			<orgName>Cornell University, Department of Computer Science</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Presenting results of experimental retrieval comparisons</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Keen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Processing and Management</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="491" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">A new measure of rank correlation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Kendall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="81" to="93" />
			<date type="published" when="1938">1938</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Kent</surname></persName>
		</author>
		<title level="m">Encyclopedia of Library and Information Science</title>
		<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<monogr>
		<title level="m" type="main">Machine literature searching VIII. Operational criteria for designing information retrieval systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">U</forename><surname>Luehrs</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Perry</surname></persName>
		</author>
		<idno type="DOI">10.1002/asi.5090060209</idno>
		<imprint>
			<date type="published" when="1955">1955</date>
			<publisher>American Documentation</publisher>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="93" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Pooling for a largescale test collection: An analysis of the search results from the first NTCIR workshop</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kuriyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nozue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Eguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="59" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Evaluating XML retrieval effectiveness at INEX</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lalmas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tombros</surname></persName>
		</author>
		<idno type="DOI">10.1145/1273221.1273225</idno>
	</analytic>
	<monogr>
		<title level="m">SIGIR Forum</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="40" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">W</forename><surname>Lancaster</surname></persName>
		</author>
		<title level="m">Evaluation of the MEDLARS Demand Search Service. (No. PB-178-660</title>
		<meeting><address><addrLine>Springfield, VA</addrLine></address></meeting>
		<imprint>
			<publisher>Clearinghouse for Federal Scientific and Technical Information</publisher>
			<date type="published" when="1968">1968</date>
			<biblScope unit="volume">22151</biblScope>
			<biblScope unit="page">278</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">W</forename><surname>Lancaster</surname></persName>
		</author>
		<title level="m">Information Retrieval Systems Characteristics, Testing, and Evaluation</title>
		<imprint>
			<publisher>John Wiley &amp; Sons, Inc</publisher>
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">On the difficulties of applying the results of information retrieval research to aid in the searching of large scientific databases</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ledwith</surname></persName>
		</author>
		<idno type="DOI">10.1016/0306-4573</idno>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">90003</biblScope>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Bootstrap technology and applications</title>
		<author>
			<persName><forename type="first">C</forename><surname>Léger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Politis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="378" to="398" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">SIG -The significance programs for testing the evaluation output</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Lesk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Report ISR-12 to the National Science Foundation</title>
		<imprint>
			<date type="published" when="1966">1966</date>
		</imprint>
		<respStmt>
			<orgName>Cornell University, Department of Computer Science</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Real life information retrieval (panel): Commercial search engines</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Lesk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cutting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Noreault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Koll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 20th annual international ACM SIGIR conference on Research and development in information retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page">333</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Relevance assessments and retrieval system evaluation*1</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Lesk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<idno type="DOI">10.1016/0020-0271(68)90029-6</idno>
	</analytic>
	<monogr>
		<title level="j">Information Storage and Retrieval</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="343" to="359" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">The TREC-5 filtering track</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Fifth Text Retrieval Conference (TREC-5)</title>
		<meeting><address><addrLine>Gaithersburg, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="75" to="96" />
		</imprint>
		<respStmt>
			<orgName>Department of Commerce, National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">Building a reusable test collection for question answering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="851" to="861" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Building a test collection for evaluating search result diversity: A preliminary study</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGIR 2009 Workshop on the Future of IR Evaluation</title>
		<meeting>the SIGIR 2009 Workshop on the Future of IR Evaluation</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="31" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Letor: Benchmark dataset for research on learning to rank for information retrieval</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR 2007 Workshop on Learning to Rank for Information Retrieval</title>
		<meeting>SIGIR 2007 Workshop on Learning to Rank for Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Overview of the CLEF 2004 multilingual question answering track</title>
		<author>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vallin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Erbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">3491</biblScope>
			<biblScope unit="page">371</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Text collections for FIRE</title>
		<author>
			<persName><forename type="first">P</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bandyopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maiti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="699" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Modeling score distributions for combining the outputs of search engines</title>
		<author>
			<persName><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="267" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<title level="m">Introduction to Information Retrieval</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kuhns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ray</surname></persName>
		</author>
		<title level="m">Probabilistic Indexing: A Statistical Technique for Document Identification and Retrieval</title>
		<imprint/>
	</monogr>
	<note type="report_type">Technical Memorandum</note>
</biblStruct>

<biblStruct xml:id="b173">
	<monogr>
		<title level="m" type="main">Data Systems Project Office</title>
		<imprint>
			<date type="published" when="1959">1959</date>
			<publisher>Thompson Ramo Wooldridge Inc</publisher>
			<biblScope unit="page">91</biblScope>
			<pubPlace>Los Angeles, California</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">Evaluation of User Reactions to a Prototype On-line Information Retrieval System</title>
		<author>
			<persName><forename type="first">D</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Prepared under Contract No. NASw-1369 by Bunker-Ramo Corporation</title>
		<meeting><address><addrLine>Canoga Park, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
	<note>No. NASA CR-918</note>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">On rank correlation in information retrieval evaluation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Melucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="33" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">Selection bias in the LETOR datasets</title>
		<author>
			<persName><forename type="first">T</forename><surname>Minka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR Workshop on Learning to Rank for Information Retrieval</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="48" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Hits hits TREC: exploring IR evaluation results with network analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mizzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="479" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">Strategic system comparisons via targeted relevance judgments</title>
		<author>
			<persName><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Webber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="375" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">Rank-biased precision for measurement of retrieval effectiveness</title>
		<author>
			<persName><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<monogr>
		<title level="m" type="main">The Intensive Sample Test for the Objective Evaluation of the Performance of Information Retrieval System</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Mooers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1959">1959</date>
			<publisher>Zator Corporation</publisher>
			<biblScope unit="page">20</biblScope>
			<pubPlace>Cambridge, Massachusetts</pubPlace>
		</imprint>
	</monogr>
	<note>No. ZTB-132</note>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">The next twenty years in information retrieval: Some goals and predictions</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Mooers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Papers Presented at the Western Joint Computer Conference</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1959">1959</date>
			<biblScope unit="page" from="81" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">Information filtering based on user behavior analysis and best match text retrieval</title>
		<author>
			<persName><forename type="first">M</forename><surname>Morita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shinoda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Annual international ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 17th Annual international ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag New York, Inc</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="272" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">Automatic ranking of information retrieval systems using data fusion</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nuray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Can</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="595" to="614" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">Building an information retrieval test collection for spontaneous conversational speech</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Soergel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">Overview of the TREC-2006 blog track</title>
		<author>
			<persName><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Text Retrieval Conference</title>
		<meeting>the Fifteenth Text Retrieval Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<publisher>TREC</publisher>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="page" from="17" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">TREC-6 interactive report</title>
		<author>
			<persName><forename type="first">P</forename><surname>Over</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixth Text Retrieval Conference (TREC-6)</title>
		<meeting>the sixth Text Retrieval Conference (TREC-6)<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">500</biblScope>
			<biblScope unit="page" from="73" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">The TREC interactive track: An annotated bibliography</title>
		<author>
			<persName><forename type="first">P</forename><surname>Over</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="369" to="381" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">TRECVID 2006-An overview</title>
		<author>
			<persName><forename type="first">P</forename><surname>Over</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ianeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Valencia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the TREC Video Retrieval Evaluation Notebook Papers</title>
		<meeting>the TREC Video Retrieval Evaluation Notebook Papers</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<analytic>
		<title level="a" type="main">Comparison of methods for searching protein sequence databases</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Pearson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Protein Science: A Publication of the Protein Society</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">1145</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<analytic>
		<title level="a" type="main">Mining user web search activity with layered bayesian networks or how to capture a click in its context</title>
		<author>
			<persName><forename type="first">B</forename><surname>Piwowarski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dupret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Second ACM International Conference on Web Search and Data Mining<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="162" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<analytic>
		<title level="a" type="main">Measures for the comparison of information retrieval systems</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Pollock</surname></persName>
		</author>
		<idno type="DOI">10.1002/asi.5090190406</idno>
	</analytic>
	<monogr>
		<title level="j">American Documentation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="387" to="397" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main">How does clickthrough data reflect retrieval quality?</title>
		<author>
			<persName><forename type="first">F</forename><surname>Radlinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kurup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the 17th ACM Conference on Information and Knowledge Management</title>
		<meeting>eeding of the 17th ACM Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main">A Field Experimental Approach to the Study of Relevance Assessments in Relation to Document Searching</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Schultz</surname></persName>
		</author>
		<idno>HC $3.00</idno>
	</analytic>
	<monogr>
		<title level="m">Appendices. Clearinghouse for Federal Scientific and Technical Information</title>
		<meeting><address><addrLine>Springfield, VA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1967-10">October 1967</date>
			<biblScope unit="volume">II</biblScope>
			<biblScope unit="page">22151</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<analytic>
		<title level="a" type="main">Creating a test collection for citationbased IR experiments</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Teufel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics</title>
		<meeting>the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics<address><addrLine>Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="391" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<analytic>
		<title level="a" type="main">The parametric description of retrieval tests: Part II: Overall measures</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="93" to="107" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">On sample sizes for non-matched-pair IR experiments</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing and Management: An International Journal</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="739" to="753" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<analytic>
		<title level="a" type="main">Salton award lecture on theoretical argument in information retrieval</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<idno type="DOI">10.1145/373593.373597</idno>
	</analytic>
	<monogr>
		<title level="j">SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<analytic>
		<title level="a" type="main">On GMAP: And other transformations</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 15th ACM International Conference on Information and Knowledge Management<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="78" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<analytic>
		<title level="a" type="main">On the history of evaluation in IR</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<idno type="DOI">10.1177/0165551507086989</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Information Science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="439" to="456" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b200">
	<analytic>
		<title level="a" type="main">The TREC-9 filtering track final report</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Hull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Text REtrieval Conference</title>
		<meeting>the Ninth Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<publisher>NTIS</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="25" to="40" />
		</imprint>
	</monogr>
	<note>TREC-2001</note>
</biblStruct>

<biblStruct xml:id="b201">
	<analytic>
		<title level="a" type="main">On rank-based effectiveness measures and optimization</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="321" to="339" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b202">
	<analytic>
		<title level="a" type="main">CLEF-IP 2009: Retrieval experiments in the intellectual property domain</title>
		<author>
			<persName><forename type="first">G</forename><surname>Roda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tait</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Piroi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zenz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Working Notes for the CLEF 2009 Workshop</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b203">
	<analytic>
		<title level="a" type="main">The simple scalability of documents</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Rorvig</surname></persName>
		</author>
		<idno type="DOI">10.1002/(SICI)1097-4571(199012)41:8&lt;590::AID-ASI5&gt;3.0.CO;2-T</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="590" to="598" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b204">
	<analytic>
		<title level="a" type="main">V-twin: A lightweight engine for interactive use</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stevens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Text Retrieval Conference (TREC-5)</title>
		<meeting>the Fifth Text Retrieval Conference (TREC-5)</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="279" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b205">
	<analytic>
		<title level="a" type="main">New performance metrics based on multigrade relevance: Their application to question answering</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sakai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NTCIR-4 Proceedings</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b206">
	<analytic>
		<title level="a" type="main">Evaluating evaluation metrics based on the bootstrap</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sakai</surname></persName>
		</author>
		<idno type="DOI">10.1145/1148170.1148261</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Seattle, Washington, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="525" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b207">
	<analytic>
		<title level="a" type="main">Alternatives to bpref</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sakai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="71" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b208">
	<analytic>
		<title level="a" type="main">Evaluating information retrieval metrics based on bootstrap hypothesis tests</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sakai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Media Technologies</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1062" to="1079" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b209">
	<analytic>
		<title level="a" type="main">On information retrieval metrics designed for evaluation with incomplete relevance assessments</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kando</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10791-008-9059-7</idno>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="447" to="470" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b210">
	<monogr>
		<title level="m" type="main">Automatic Information Organization and Retrieval</title>
		<author>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968">1968</date>
			<publisher>McGraw Hill Text</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b211">
	<analytic>
		<title level="a" type="main">The Smart Retrieval System</title>
		<author>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Experiments in Automatic Document Processing</title>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b212">
	<analytic>
		<title level="a" type="main">Approaches to passage retrieval in full text information systems</title>
		<author>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="49" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b213">
	<analytic>
		<title level="a" type="main">Extended Boolean information retrieval</title>
		<author>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1145/182.358466</idno>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1022" to="1036" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b214">
	<analytic>
		<title level="a" type="main">Computer evaluation of indexing and text processing</title>
		<author>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Lesk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="8" to="36" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b215">
	<analytic>
		<title level="a" type="main">On the construction of effective vocabularies for information retrieval</title>
		<author>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1973 Meeting on Programming Languages and Information Retrieval Table of Contents</title>
		<meeting>the 1973 Meeting on Programming Languages and Information Retrieval Table of Contents<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1973">1973</date>
			<biblScope unit="page" from="48" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b216">
	<analytic>
		<title level="a" type="main">Accurate user directed summarization from existing tools</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Conference on Information and Knowledge Management</title>
		<meeting>the Seventh International Conference on Information and Knowledge Management<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="45" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b217">
	<analytic>
		<title level="a" type="main">Forming test collections with no system pooling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Joho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b218">
	<analytic>
		<title level="a" type="main">NRT: News retrieval tool</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Publishing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="205" to="217" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b219">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evia</forename></persName>
		</author>
		<title level="m">The First International Workshop on Evaluating Information Access</title>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b220">
	<analytic>
		<title level="a" type="main">Problems with Kendall&apos;s tau</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="839" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b221">
	<analytic>
		<title level="a" type="main">What else is there? Search diversity examined</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Arni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st European Conference on IR Research on Advances in Information Retrieval</title>
		<meeting>the 31st European Conference on IR Research on Advances in Information Retrieval</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="562" to="569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b222">
	<analytic>
		<title level="a" type="main">Information retrieval system evaluation: Effort, sensitivity, and reliability</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="162" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b223">
	<analytic>
		<title level="a" type="main">An Inquiry into Testing of Information Retrieval Systems: Part II: Analysis of Results</title>
		<author>
			<persName><forename type="first">T</forename><surname>Saracevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Comparative Systems Laboratory: Final Technical Report</title>
		<imprint>
			<date type="published" when="1968">1968</date>
		</imprint>
		<respStmt>
			<orgName>Center for Documentation and Communication Research, School of Library Science, Case Western Reserve University</orgName>
		</respStmt>
	</monogr>
	<note>CSL:TR-FINAL-II)</note>
</biblStruct>

<biblStruct xml:id="b224">
	<analytic>
		<title level="a" type="main">RELEVANCE: A review of and a framework for the thinking on the notion in information science</title>
		<author>
			<persName><forename type="first">T</forename><surname>Saracevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="143" to="165" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b225">
	<analytic>
		<title level="a" type="main">Evaluation of evaluation in information retrieval</title>
		<author>
			<persName><forename type="first">T</forename><surname>Saracevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="138" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b226">
	<analytic>
		<title level="a" type="main">Statistical inference in retrieval effectiveness evaluation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="495" to="512" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b227">
	<analytic>
		<title level="a" type="main">Precision evaluation of search engines</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1019679624079</idno>
	</analytic>
	<monogr>
		<title level="j">World Wide Web</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="159" to="173" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b228">
	<analytic>
		<title level="a" type="main">Cross-language speech retrieval: Establishing a baseline performance</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sheridan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wechsler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schäuble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 20th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="99" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b229">
	<analytic>
		<title level="a" type="main">Robust result merging using sample-based score estimates</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shokouhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b230">
	<analytic>
		<title level="a" type="main">Spanish and Chinese document retrieval in TREC-5</title>
		<author>
			<persName><forename type="first">A</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wilkinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Text Retrieval Conference (TREC-5)</title>
		<meeting>the Fifth Text Retrieval Conference (TREC-5)<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="57" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b231">
	<analytic>
		<title level="a" type="main">TRECVID-An overview</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Over</surname></persName>
		</author>
		<ptr target="http://www-nlpir.nist.gov/projects/tvpubs/tvpapers03/tv3overview.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the TRECVID 2003 Conference</title>
		<meeting>the TRECVID 2003 Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b232">
	<analytic>
		<title level="a" type="main">Evaluation campaigns and TRECVid</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Over</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th ACM International Workshop on Multimedia Information Retrieval</title>
		<meeting>the 8th ACM International Workshop on Multimedia Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="321" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b233">
	<analytic>
		<title level="a" type="main">The TREC-2001 video track report</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Over</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Taban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Text REtrieval Conference (TREC-2001)</title>
		<meeting>the Tenth Text REtrieval Conference (TREC-2001)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="52" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b234">
	<analytic>
		<title level="a" type="main">User adaptation: Good results from poor systems</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Kantor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="147" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b235">
	<analytic>
		<title level="a" type="main">A comparison of statistical significance tests for information retrieval evaluation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management</title>
		<meeting>the Sixteenth ACM Conference on Conference on Information and Knowledge Management<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="623" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b236">
	<analytic>
		<title level="a" type="main">On evaluating web search with very few relevant documents</title>
		<author>
			<persName><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="530" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b237">
	<analytic>
		<title level="a" type="main">Dynamic test collections: Measuring search effectiveness on the live web</title>
		<author>
			<persName><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="276" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b238">
	<analytic>
		<title level="a" type="main">Ranking retrieval systems without relevance judgments</title>
		<author>
			<persName><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nicholas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cahan</surname></persName>
		</author>
		<idno type="DOI">10.1145/383952.383961</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New Orleans, Louisiana, United States</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="66" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b239">
	<analytic>
		<title level="a" type="main">Building a filtering test collection for TREC 2002</title>
		<author>
			<persName><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="243" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b240">
	<analytic>
		<title level="a" type="main">Liberal relevance criteria of TREC-: Counting on negligible documents?</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sormunen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="324" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b241">
	<analytic>
		<title level="a" type="main">Automatic indexing</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Spärck</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="393" to="432" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b242">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Spärck</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval Experiment. Butterworth-Heinemann Ltd</title>
		<imprint>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b243">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Spärck</forename><surname>Jones</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0306-4573(02)00026-2</idno>
		<title level="m">Letter to the editor,&quot; Information Processing &amp; Management</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="156" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b244">
	<monogr>
		<title level="m" type="main">Report on a design study for the &apos;ideal&apos; information retrieval test collection</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Spärck</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Bates</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977">1977</date>
		</imprint>
		<respStmt>
			<orgName>Computer Laboratory, University of Cambridge</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">British Library Research and Development Report</note>
</biblStruct>

<biblStruct xml:id="b245">
	<monogr>
		<title level="m" type="main">Report on the need for and the provision of an &apos;ideal&apos; information retrieval test collection</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Spärck</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<biblScope unit="page">43</biblScope>
		</imprint>
		<respStmt>
			<orgName>Computer Laboratory, University of Cambridge</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">British Library Research and Development Report</note>
</biblStruct>

<biblStruct xml:id="b246">
	<analytic>
		<title level="a" type="main">Information retrieval test collections</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Spärck</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
		<idno type="DOI">10.1108/eb026616</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="75" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b247">
	<analytic>
		<title level="a" type="main">Evaluation measures for interactive information retrieval</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Processing &amp; Management</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="503" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b248">
	<analytic>
		<title level="a" type="main">The relevance of recall and precision in user evaluation</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="207" to="217" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b249">
	<analytic>
		<title level="a" type="main">Information retrieval systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Swets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="issue">3577</biblScope>
			<biblScope unit="page" from="245" to="250" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b250">
	<analytic>
		<title level="a" type="main">Effectiveness of information retrieval methods</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Swets</surname></persName>
		</author>
		<idno type="DOI">10.1002/asi.4630200110</idno>
	</analytic>
	<monogr>
		<title level="j">American Documentation</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="72" to="89" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b251">
	<analytic>
		<title level="a" type="main">Estimating the satisfaction of information users</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tagliacozzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the Medical Library Association</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="243" to="249" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b252">
	<analytic>
		<title level="a" type="main">Simulation of user judgments in bibliographic retrieval systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Tague</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Nelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Annual International ACM SIGIR Conference on Information Storage and Retrieval: Theoretical Issues in Information Retrieval</title>
		<meeting>the 4th Annual International ACM SIGIR Conference on Information Storage and Retrieval: Theoretical Issues in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1981">1981</date>
			<biblScope unit="page" from="66" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b253">
	<analytic>
		<title level="a" type="main">Some perspectives on the evaluation of information retrieval systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Tague-Sutcliffe</surname></persName>
		</author>
		<idno type="DOI">10.1002/(SICI)1097-4571(199601)47:1&lt;1</idno>
		<idno>AID- ASI1&gt;3.0.CO;2-3</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="3" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b254">
	<analytic>
		<title level="a" type="main">A statistical analysis of the TREC-3 data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Tague-Sutcliffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Blustein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Third Text Retrieval Conference (TREC-3)</title>
		<meeting><address><addrLine>Gaithersburg, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="385" to="398" />
		</imprint>
		<respStmt>
			<orgName>Department of Commerce, National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b255">
	<analytic>
		<title level="a" type="main">A comparison of evaluation measures given how users perform on search tasks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Thom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Scholer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Presented at the Proceedings of the Twelfth Australasian Document Computing Symposium</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="56" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b256">
	<analytic>
		<title level="a" type="main">Evaluation by comparing result sets in context</title>
		<author>
			<persName><forename type="first">P</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 15th ACM International Conference on Information and Knowledge Management<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="94" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b257">
	<analytic>
		<title level="a" type="main">The efficiency of subject catalogues and the cost of information searches</title>
		<author>
			<persName><forename type="first">R</forename><surname>Thorne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="130" to="148" />
			<date type="published" when="1955">1955</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b258">
	<analytic>
		<title level="a" type="main">Why batch and user evaluations do not give the same results</title>
		<author>
			<persName><forename type="first">A</forename><surname>Turpin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="225" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b259">
	<analytic>
		<title level="a" type="main">User interface effects in past batch versus user experiments</title>
		<author>
			<persName><forename type="first">A</forename><surname>Turpin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="431" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b260">
	<analytic>
		<title level="a" type="main">User performance versus precision measures for simple search tasks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Turpin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Scholer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="11" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b261">
	<analytic>
		<title level="a" type="main">Foundation of evaluation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="365" to="373" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b262">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval. Butterworth-Heinemann Ltd</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b263">
	<analytic>
		<title level="a" type="main">The National Physical Laboratory Experiments in Statistical Word Associations and Their Use in Document Indexing And Retrieval</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K T</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Cameron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">National Physical Lab</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page">171</biblScope>
			<date type="published" when="1970">1970</date>
			<pubPlace>Teddington (Great Britain</pubPlace>
		</imprint>
		<respStmt>
			<orgName>National Physical Laboratory Computer Science Division-Publications</orgName>
		</respStmt>
	</monogr>
	<note>COM.SCI.</note>
</biblStruct>

<biblStruct xml:id="b264">
	<analytic>
		<title level="a" type="main">Inefficiency of the use of Boolean functions for information retrieval systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Verhoeff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Goffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Belzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="557" to="558" />
			<date type="published" when="1961">1961</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b265">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Vickery</surname></persName>
		</author>
		<title level="m">On Retrieval System Theory. Butterworths</title>
		<imprint>
			<date type="published" when="1965">1965</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b266">
	<analytic>
		<title level="a" type="main">Labeling images with a computer game</title>
		<author>
			<persName><forename type="first">L</forename><surname>Von Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dabbish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="319" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b267">
	<analytic>
		<title level="a" type="main">On expanding query vectors with lexically related words</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Second Text Retrieval Conference (TREC 2), NIST Special Publication 500-215</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="223" to="231" />
		</imprint>
		<respStmt>
			<orgName>Department of Commerce, National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b268">
	<analytic>
		<title level="a" type="main">Variations in relevance judgments and the measurement of retrieval effectiveness</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in information retrieval</title>
		<meeting>the 21st Annual International ACM SIGIR Conference on Research and Development in information retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b269">
	<analytic>
		<title level="a" type="main">The TREC-8 question answering track report</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Text Retrieval Conference (TREC-8)</title>
		<meeting>the Eighth Text Retrieval Conference (TREC-8)<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="77" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b270">
	<analytic>
		<title level="a" type="main">Variations in relevance judgments and the measurement of retrieval effectiveness</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="697" to="716" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b271">
	<analytic>
		<title level="a" type="main">Evaluation by highly relevant documents</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New Orleans, Louisiana, United States. New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="74" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b272">
	<analytic>
		<title level="a" type="main">Overview of the TREC 2003 question answering track</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth Text REtrieval Conference (TREC 2003)</title>
		<meeting>the Twelfth Text REtrieval Conference (TREC 2003)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">142</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b273">
	<analytic>
		<title level="a" type="main">Overview of the TREC 2004 robust retrieval track</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirteenth Text Retrieval Conference (TREC 2004)</title>
		<imprint>
			<publisher>NIST Special Publication</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b274">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Gaithersburg</surname></persName>
		</author>
		<author>
			<persName><surname>Usa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>Department of Commerce, National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b275">
	<analytic>
		<title level="a" type="main">On test collections for adaptive information retrieval</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Processing and Management</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b276">
	<analytic>
		<title level="a" type="main">Topic set size redux</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="806" to="807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b277">
	<analytic>
		<title level="a" type="main">The effect of topic set size on retrieval experiment error</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="316" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b278">
	<analytic>
		<title level="a" type="main">Overview of the seventh text retrieval conference</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Text Retrieval Conference (TREC-7)</title>
		<meeting>the Seventh Text Retrieval Conference (TREC-7)</meeting>
		<imprint>
			<publisher>NIST Special Publication</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="1" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b279">
	<analytic>
		<title level="a" type="main">Overview of the eighth text retrieval conference (TREC-8)</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eighth Text Retrieval Conference (TREC-8)</title>
		<meeting><address><addrLine>Gaithersburg, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="1" to="24" />
		</imprint>
		<respStmt>
			<orgName>Department of Commerce, National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b280">
	<analytic>
		<title level="a" type="main">Overview of TREC 2001</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIST Special Publication 500-250</title>
		<imprint>
			<publisher>Government Printing Office</publisher>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
	<note>Tenth Text Retrieval Conference</note>
</biblStruct>

<biblStruct xml:id="b281">
	<monogr>
		<title level="m" type="main">TREC: Experiment and Evaluation in Information Retrieval</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
	<note>illustrated ed.</note>
</biblStruct>

<biblStruct xml:id="b282">
	<analytic>
		<author>
			<persName><forename type="first">C</forename><surname>Wade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<idno>IR-396</idno>
	</analytic>
	<monogr>
		<title level="m">Passage Retrieval and Evaluation</title>
		<title level="s">CIIR Technical Report</title>
		<meeting><address><addrLine>Amherst, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts, Amherst Center for Intelligent Information Retrieval</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b283">
	<analytic>
		<title level="a" type="main">Score standardization for intercollection comparison of retrieval systems</title>
		<author>
			<persName><forename type="first">W</forename><surname>Webber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="51" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b284">
	<analytic>
		<title level="a" type="main">Statistical power in retrieval experimentation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Webber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM Conference on Information and Knowledge Management</title>
		<meeting>the 17th ACM Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="571" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b285">
	<analytic>
		<title level="a" type="main">Investigating the querying and browsing behavior of advanced search engine users</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="255" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b286">
	<analytic>
		<title level="a" type="main">The Cornell implementation of the SMART system</title>
		<author>
			<persName><forename type="first">D</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Lesk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The SMART Retrieval System: Experiments in Automatic Document Processing</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</editor>
		<meeting><address><addrLine>Englewood Cliffs, New Jersey</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall, Inc</publisher>
			<date type="published" when="1971">1971</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b287">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Managing</forename><surname>Gigabytes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b288">
	<analytic>
		<title level="a" type="main">Methods for ranking information retrieval systems without relevance judgments</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 ACM symposium on Applied computing</title>
		<meeting>the 2003 ACM symposium on Applied computing<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="811" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b289">
	<analytic>
		<title level="a" type="main">Exploring folksonomy for personalized search</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="155" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b290">
	<analytic>
		<title level="a" type="main">Estimating average precision with incomplete and imperfect judgments</title>
		<author>
			<persName><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 15th ACM International Conference on Information and Knowledge Management<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="102" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b291">
	<analytic>
		<title level="a" type="main">A new rank correlation coefficient for information retrieval</title>
		<author>
			<persName><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="587" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b292">
	<analytic>
		<title level="a" type="main">On the choice of effectiveness measures for learning to rank</title>
		<author>
			<persName><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning to Rank for Information Retrieval. Workshop in Conjunction with the ACM SIGIR Conference on Information Retrieval</title>
		<meeting><address><addrLine>Boston, MA, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b293">
	<analytic>
		<title level="a" type="main">AOL Moves to Increase Privacy on Search Queries</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zeller</surname><genName>Jr</genName></persName>
		</author>
		<ptr target="http://www.nytimes.com/2006/08/22/technology/22aol.html" />
	</analytic>
	<monogr>
		<title level="j">The New York Times</title>
		<imprint>
			<date type="published" when="2006-08-22">August 22 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b294">
	<analytic>
		<title level="a" type="main">Beyond independent relevance: Methods and evaluation metrics for subtopic retrieval</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 26th annual international ACM SIGIR conference on Research and development in information retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="10" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b295">
	<analytic>
		<title level="a" type="main">A regression framework for learning ranking functions using relative relevance judgments</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="287" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b296">
	<analytic>
		<title level="a" type="main">How reliable are the results of large-scale information retrieval experiments?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="307" to="314" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
