<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mining Latent Structures for Multimedia Recommendation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-04-19">19 Apr 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jinghao</forename><surname>Zhang</surname></persName>
							<email>jinghao.zhang@cripac.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
							<email>yanqiao.zhu@cripac.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
							<email>qiang.liu@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
							<email>shu.wu@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuhui</forename><surname>Wang</surname></persName>
							<email>shuhui.wang@vipl.ict.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Computer Science and Technology</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
							<email>wangliang@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Mining Latent Structures for Multimedia Recommendation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-04-19">19 Apr 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/1122445.1122456</idno>
					<idno type="arXiv">arXiv:2104.09036v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multimedia recommendation</term>
					<term>latent structure learning</term>
					<term>graph neural networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multimedia content is of predominance in the modern Web era. Investigating how users interact with multimodal items is a continuing concern within the rapid development of recommender systems. The majority of previous work focuses on modeling user-item interactions with multimodal features included as side information. However, this scheme is not well-designed for multimedia recommendation. Specifically, only collaborative item-item relationships are implicitly modeled through high-order item-user-item relations. Considering that items are associated with rich contents in multiple modalities, we argue that the latent item-item structures underlying these multimodal contents could be beneficial for learning better item representations and further boosting recommendation. To this end, we propose a LATent sTructure mining method for multImodal reCommEndation, which we term LATTICE for brevity. To be specific, in the proposed LATTICE model, we devise a novel modality-aware structure learning layer, which learns item-item structures for each modality and aggregates multiple modalities to obtain latent item graphs. Based on the learned latent graphs, we perform graph convolutions to explicitly inject high-order item affinities into item representations. These enriched item representations can then be plugged into existing collaborative filtering methods to make more accurate recommendations. Extensive experiments on three real-world datasets demonstrate the superiority of our method over state-of-the-art multimedia recommendation methods and validate the efficacy of mining latent item-item relationships from multimodal features.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>Collaborative relation Semantic relation</p><formula xml:id="formula_0">u 1 u 2</formula><p>Figure <ref type="figure">1</ref>: A toy example of recommendation with two types of item relations. In this paper, we argue that semantic structures mined from multimodal features are helpful for comprehensively discovering candidate items supplementary to collaborative signals in traditional work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>With the rapid development of Internet, recommender systems have become an indispensable tool to help users find relevant information. Nowadays, users are easily accessible to large amounts of online information represented in multiple modalities, including images, texts, videos, etc. Recent years have witnessed growing research interests on multimedia recommendation, which aims to predict whether a user will interact with an item with multimodal contents. Focused on exploiting abundant user-item interactions, collaborative filtering (CF) serves as a foundation of personalized recommender systems, which encodes users and items into low-dimensional dense vectors and recommendations can be made based on these embeddings <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b31">32]</ref>. Traditional work on multimedia recommendation, e.g., VBPR <ref type="bibr" target="#b11">[12]</ref>, DeepStyle <ref type="bibr" target="#b22">[23]</ref>, and ACF <ref type="bibr" target="#b2">[3]</ref>, extends the vanilla CF framework by incorporating multimodal contents as side information in addition to item representations. However, as these methods only focus on direct user-item interactions, their expressiveness is confined.</p><p>Inspired by the success of graph neural networks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b33">34]</ref>, Wang et al. <ref type="bibr" target="#b34">[35]</ref> propose to model user-item relationships as bipartite graphs and inject high-order interactions into the embedding process to learn better representations. These graph-based recommender systems <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42]</ref> achieve great success and obtain state-of-the-art performance. Recently, many attempts have been made to integrate multimodal contents into graph-based recommendation systems. MMGCN <ref type="bibr" target="#b37">[38]</ref> constructs modality-specific user-item interaction graphs to model user preferences specific to each modality. Following MMGCN, GRCN <ref type="bibr" target="#b36">[37]</ref>  Visual structure &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " J c h Z 3 9 R 6 p C R 9 h / I q V z J 8 4 l R P j E 8 = " &gt;</p><formula xml:id="formula_1">A A A C X H i c b V B N T w I x E C 3 r F 6 I I a u L F y 0 Z i 4 s G Q X e P X E e P F I y a g J r C S b h m g o e 1 u 2 l m F b P g l X v V H e f G 3 2 A V i B H 1 J k 5 c 3 M 5 0 3 L 4 w F N + h 5 n z l n Z X V t f S O / W d j a L u 6 U y r t 7 D y Z K N I M m i 0 S k n 0 J q Q H A F T e Q o 4 C n W Q G U o 4 D E c 3 m b 1 x x f Q h k e q g e M Y A k n 7 i v c 4 o 2 i l T r n U D m V 6 M 3 l u I 4 w w f Z l 0 y h W v 6 k 3 h / i X + n F T I H P X O b u 6 i 3 Y 1 Y I k E h E 9 S Y l u / F G K R U I 2 c C J o V 2 Y i C m b E j 7 0 L J U U Q k m S K f O J + 6 x V b p u L 9 L 2 K X S n 6 u + J l E p j x j K 0 n Z L i w C z X M v G / W i v B 3 n W Q c h U n C I r N F v U S 4 W L k Z j G 4 X a 6 B o R h b Q p n m 1 q v L B l R T h j a s h S 2 h X L y h 4 Q d p Z j b 7 d q F R 8 B D s 0 W r p 5 h 8 5 S B W 8 4 m h q u V C w Q f v L s f 4 l D 2 d V / 7 L q 3 5 9 X a q f z y P P k k B y R E + K T K 1 I j d 6 R O m o S R h L y R d / K R + 3 J W n S 2 n O G t 1 c v O Z f b I A 5 + A b f d a 3 0 g = = &lt; / l a t e x i t &gt; A v</formula><p>Latent structure Collaborative filtering (MF, NGCF, …) Learning objective &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M j s n u o h s l / / p A / 9 e t 2 t 8 d g S L i y</p><formula xml:id="formula_2">E = " &gt; A A A C Y 3 i c b V D L S s N A F J 3 G V 2 1 9 1 A d u R A g W w V V J x N d S d O P C R R X 7 g D a U y f R W h 0 4 m Y e Z G W 2 J + x q 3 + k B / g f z i J R W z 1 w s D h 3 H v n n n P 8 S H C N j v N R s O b m F x a X i s u l 8 s r q 2 n p l Y 7 O p w 1 g x a L B Q h K r t U w 2 C S 2 g g R w H t S A E N f A E t f 3 i V 9 V t P o D Q P 5 T 2 O I / A C + i D 5 g D O K h u p V d r o B x U d G R X K T 9 r o I I 0 w u 6 3 d p r 1 J 1 a k 5 e 9 l / g T k C V T K r e 2 y i c d P s h i w O Q y A T V u u M 6 E X o J V c i Z g L T U j T V E l A 3 p A 3 Q M l D Q A 7 S W 5 g d Q + M E z f H o T K P I l 2 z v 7 e S G i g 9 T j w z W Q m V 8 / 2 M v K / X i f G w b m X c B n F C J J 9 H x r E w s b Q z t K w + 1 w B Q z E 2 g D L F j V a b P V J F G Z r M p q 7 4 w b S H e 9 d L M r H Z t 1 O D g v t g T M s Z z z + 0 l 0 h 4 x l E u u V Q y Q b u z s f 4 F z a O a e 1 p z b 4 + r F 8 4 k 8 i L Z J f v k k L j k j F y Q a 1 I n D c L I C 3 k l b + S 9 8 G m V r U 1 r + 3 v U K k x 2 t s h U W X t f h t 2 6 o Q = = &lt; / l a t e x i t &gt; L BPR ? &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D Z v K 2 A w U e d P v B 0 i M d 9 4 p 7 u l l p F k = " &gt; A A A C U X i c b V B N S 8 N A E J 3 E r 1 q / W j 1 6 C R b B g 5 S k + H W s e P G o Y K v Q B t l s p + 3 a 3 U 3 Y 3 a g l 9 D 9 4 1 b / l y Z / i z U 0 N Y q s D C 4 8 3 M z v v v S j h T B v f / 3 D c h c W l 5 Z X S a n l t f W N z q 1 L d b u s 4 V R R b N O a x u o u I R s 4 k t g w z H O 8 S h U R E H G + j 0 U X e v 3 1 E p V k s b 8 w 4 w V C Q g W R 9 R o m x V L s b i e x 8 c l + p + X V / W t 5 f E B S g B k V d 3 V e d 4 2 4 v p q l A a S g n W n c C P z F h R p R h l O O k 3 E 0 1 J o S O y A A 7 F k o i U I f Z V O 7 E 2 7 d M z + v H y j 5 p v C n 7 e y M j Q u u x i O y k I G a o 5 3 s 5 + V + v k 5 r + W Z g x m a Q G J f 0 + 1 E + 5 Z 2 I v 9 + 7 1 m E J q + N g C Q h W z W j 0 6 J I p Q Y x O a u R K J W Q 8 3 Q Z j l Y v N v Z w Y 5 i 9 C a l n O e f + g w k / h k n q e S y 2 U b d D A f 6 1 / Q b t S D k 3 r j + q j W P C w i L 8 E u 7 M E B B H A K T b i E K 2 g B h Q d 4 g V d 4 c 9 6 d T x d c 9 3 v U d Y q d H Z g p d + 0 L L m e 0 W g = = &lt; / l a t e x i t &gt; A Figure 2:</formula><p>The overall framework of our proposed LATTICE model. We first learn modality-aware item graphs and aggregate multiple modalities in an adaptive manner. Based on the mined latent graphs, we conduct graph convolutions to inject high-order item relationships into item embeddings, which are then combined with downstream CF methods to make recommendations.</p><p>features to refine user-item interaction graphs by identifying the false-positive feedback and prunes the corresponding noisy edges.</p><p>Despite their effectiveness, previous attempts fail to explicitly model item relationships, which have been proved to be important in recommender systems <ref type="bibr" target="#b29">[30]</ref>. Specifically, the majority of previous work concentrates on modeling user-item interactions by constructing better interaction graphs or designing sophisticated user-item aggregation strategies, following the traditional CF paradigm. Therefore, only collaborative item relationships are implicitly discovered through modeling high-order item-user-item co-occurrences, which potentially leads to a gap to the genuine item-item relations that carry semantic relationships. Taking Figure <ref type="figure">1</ref> as an example, existing methods will recommend the shirt ( ) for 𝑢 2 according to collaborative relationships, since shirts ( ), hats ( ), and pants ( ) all interacted with 𝑢 1 . However, previous work may not be able to recommend coats ( ) to 𝑢 2 , which are visually similar to shirts. Considering that items are associated with rich multimodal features, the latent connections underlying multimodal contents should facilitate learning better item representations and assist the recommender systems to comprehensively discover candidate items.</p><p>Towards this end, we propose a novel LATent sTructure mining scheme for multImodal reCommEndation, LATTICE for brevity. As shown in Figure <ref type="figure">2</ref>, the proposed LATTICE consists of three key components. We first develop a novel modality-aware structure learning layer, which learns modality-aware item structures from multimodal features and aggregates modality-aware item graphs to form latent multimodal item graphs. After that, we perform graph convolutions on the learned latent graphs to explicitly consider item relationships. The resulting item representations are thus infused with high-order item relationships, which will be added into the output item embeddings of CF models. Please kindly note that distinct from previous work that leverages raw features of multimodal contents as side information in addition to ID embeddings, in our work, multimodal features are only used to learn graph structures, and graph convolutions are employed on ID embeddings to directly model item-item affinities.</p><p>The LATTICE model enjoys two additional benefits. Firstly, previous work that leverages multimodal features to model user-item interactions faces the cold-start problem, where limited user-item interactions are provided. Our work, on the contrary, mines latent graph structures from multimodal features. Even with limited interactions, items will get similar feedbacks from relevant neighbors through neighborhood aggregation. Secondly, unlike previous attempts which design sophisticated user-item aggregation strategies, LATTICE is agnostic to downstream CF methods. Therefore, it could be served as a play-and-plug module for existing recommender models.</p><p>To sum up, the main contribution of this work is threefold.</p><p>• We highlight the importance of explicitly exploiting item relationships in multimedia recommendation, which are helpful for discovering comprehensive candidate items.</p><p>• We propose a novel framework for multimedia recommendation to mine the latent structures beneath multimodal features, which can be supplemented with collaborative signals modeled by traditional CF methods.</p><p>• We perform extensive experiments on three public datasets to validate the effectiveness of our proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">THE PROPOSED METHOD</head><p>In this section, we first formulate the problem and introduce our model in detail. As illustrated in Figure <ref type="figure">2</ref>, there are three main components in our proposed framework: (1) a modality-aware graph structure learning layer that learns item graph structures from multimodal features and fuses multimodal graphs, (2) graph convolutional layers that learn the embeddings by injecting item-item affinities, and (3) downstream CF methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preliminaries</head><p>Let U, I denote the set of users and items. Each user 𝑢 ∈ U is associated with a set of items I 𝑢 with positive feedbacks which indicate the preference score 𝑦 𝑢𝑖 = 1 for 𝑖 ∈ I 𝑢 . 𝒙 𝑢 , 𝒙 𝑖 ∈ R 𝑑 is the input ID embedding of 𝑢 and 𝑖, respectively, where 𝑑 is the embedding dimension. Beside user-item interactions, multimodal features are offered as content information of items. We denote the modality features of item 𝑖 as 𝒆 𝑚 𝑖 ∈ R 𝑑 𝑚 , where 𝑑 𝑚 denotes the dimension of the features, 𝑚 ∈ M is the modality, and M is the set of modalities. The purpose of multimodal recommendation is to accurately predict users' preferences by ranking items for each user according to predicted preference scores ŷ𝑢𝑖 . In this paper, we consider visual and textual modalities: M = {v, t}. Our method is not fixed to the two modalities and multiple modalities can be involved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Modality-aware Latent Structure Learning</head><p>Multimodal features provide rich and meaningful content information of items, while existing methods only utilize multimodal features as side information for each item, ignoring the important semantic relationships of items underlying features. In this section, we introduce how to discover the underlying latent graph structure of item graphs in order to learn better item representations. To be specific, we first construct initial 𝑘-nearest-neighbor (𝑘NN) modality-aware item graphs by utilizing raw multimodal features. After that, we learn the latent graph structures from transformed, high-level features based on the initial graph. Finally, we integrate the latent graphs from multiple modalities in an adaptive way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2.1</head><p>Constructing initial 𝑘NN modality-aware graphs. We first construct initial 𝑘NN modality-aware graph 𝑺 𝑚 by using raw features for each modality 𝑚. Based on the hypothesis that similar items are more likely to interact than dissimilar items <ref type="bibr" target="#b25">[26]</ref>, we quantify the semantic relationship between two items by their similarity. Common options for node similarity measurement include cosine similarity <ref type="bibr" target="#b35">[36]</ref>, kernel-based functions <ref type="bibr" target="#b20">[21]</ref>, and attention mechanisms <ref type="bibr" target="#b4">[5]</ref>. Our method is agnostic to similarity measurements, and we opt to the simple and parameter-free cosine similarity in this paper, where the similarity matrix 𝑺 𝑚 ∈ R 𝑁 ×𝑁 is computed by</p><formula xml:id="formula_3">𝑺 𝑚 𝑖 𝑗 = (𝒆 𝑚 𝑖 ) ⊤ 𝒆 𝑚 𝑗 ∥𝒆 𝑚 𝑖 ∥∥𝒆 𝑚 𝑗 ∥ .<label>(1)</label></formula><p>Typically, the graph adjacency matrix is supposed to be nonnegative but 𝑺 𝑖 𝑗 ranges between [−1, 1]. Thus, we suppress its negative entries to zeros. Moreover, common graph structures are much more sparse other than a fully-connected graph, which is not only computationally expensive but also might introduce noisy, unimportant edges <ref type="bibr" target="#b4">[5]</ref>. For this purpose, we conduct a 𝑘NN sparsification <ref type="bibr" target="#b1">[2]</ref> on the dense graph: for each item 𝑖, we only keep edges with top-𝑘 confidence scores:</p><formula xml:id="formula_4">𝑺 𝑚 𝑖 𝑗 = 𝑺 𝑚 𝑖 𝑗 , 𝑺 𝑚 𝑖 𝑗 ∈ top-𝑘 (𝑺 𝑚 𝑖 ), 0, otherwise,<label>(2)</label></formula><p>where 𝑺 𝑚 is a sparsified, directed graph adjacency matrix. To alleviate the exploding or vanishing gradient problem <ref type="bibr" target="#b19">[20]</ref>, we normalize the adjacency matrix as:</p><formula xml:id="formula_5">𝑺 𝑚 = (𝑫 𝑚 ) − 1 2 𝑺 𝑚 (𝑫 𝑚 ) − 1 2 ,<label>(3)</label></formula><p>where 𝑫 𝑚 ∈ R 𝑁 ×𝑁 is the diagonal degree matrix of 𝑺 𝑚 and 𝑫 𝑚 𝑖𝑖 = 𝑗 𝑺 𝑚 𝑖 𝑗 . 2.2.2 Learning latent structures. Although we have obtained the modality-aware initial graph structures 𝑺 𝑚 by utilizing raw multimodal features, they may not be ideal for the recommendation task. This is because the raw multimodal features are often noisy or even incomplete due to the inevitably error-prone data measurement or collection. Additionally, initial graphs are constructed from the original multimodal features, which may not reflect the genuine graph structures after feature extraction and transformation. To this end, we propose to dynamically learn the graph structures by transformed, high-level multimodal features and combine learned structures with initial structures.</p><p>Firstly, we transform raw modality features into high-level features 𝒆 𝑚 𝑖 :</p><formula xml:id="formula_6">𝒆 𝑚 𝑖 = 𝑾 𝑚 𝒆 𝑚 𝑖 + 𝒃 𝑚 ,<label>(4)</label></formula><p>where 𝑾 𝑚 ∈ R 𝑑 ′ ×𝑑 𝑚 and 𝒃 𝑚 ∈ R 𝑑 ′ denote the trainable transformation matrix and bias vector, respectively. 𝑑 ′ is the dimension of high-level feature vector 𝒆 𝑚 𝑖 . We then dynamically infer the graph structures utilizing 𝒆 𝑚 𝑖 , repeating the process in Eqs. (1, 2, 3) and obtain the adjacency matrix 𝑨 𝑚 .</p><p>Although the initial graph could be noisy, it typically still carries rich and useful information regarding item graph structures. Also, drastic change of adjacency matrix will lead to unstable training. To keep rich information of initial item graph and stabilize the training process, we add a skip connection that combines the learned graph with the initial graph:</p><formula xml:id="formula_7">𝑨 𝑚 = 𝜆 𝑺 𝑚 + (1 − 𝜆) 𝑨 𝑚 ,<label>(5)</label></formula><p>where 𝜆 ∈ (0, 1) is the coefficient of skip connection that controls the amount of information from the initial structure. The obtained 𝑨 𝑚 is the final graph adjacency matrix representing latent structures for modality 𝑚. It is worth mentioning that both 𝑺 𝑚 and 𝑨 𝑚 are sparsified and normalized matrices, thus the final adjacency matrix 𝑨 𝑚 is also sparsified and normalized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2.3</head><p>Aggregating multimodal latent graphs. After we obtained modality-aware adjacency matrix 𝑨 𝑚 for each modality 𝑚 ∈ M, in this section, we explore how to fuse different modalities to compute the final latent structures. Since users usually focus on different modalities in different scenarios, for example, one may pay more attention to visual modality when selecting clothes, while focusing more on textual information when picking books, we intro learnable weights to assign different importance scores to modality-specific graphs in an adaptive way:</p><formula xml:id="formula_8">𝑨 = |M | ∑︁ 𝑚=0 𝛼 𝑚 𝑨 𝑚 ,<label>(6)</label></formula><p>where 𝛼 𝑚 is the importance score of modality 𝑚 and 𝑨 ∈ R 𝑁 ×𝑁 is the graph that represents multimodal item relationships. We apply the softmax function to keep the adjacency matrix 𝑨 normalized, such that</p><formula xml:id="formula_9">|M | 𝑚=0 𝛼 𝑚 = 1.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Graph Convolutions</head><p>After obtained the latent structures, we then perform graph convolution operation to learn better item representations by injecting item-item affinities into the embedding process. Graph convolutional operation can be treated as message propagating and aggregation. Through propagating the item representations from its neighbors, one item can aggregate the information within the first-order neighborhood. Furthermore, by stacking multiple graph convolutional layers, the high-order item-item relationships are captured and aggregated. Following He et al. <ref type="bibr" target="#b12">[13]</ref>, Wu et al. <ref type="bibr" target="#b38">[39]</ref>, we employ simple message passing and aggregation without feature transformation and non-linear activation which is effective and computationally efficiently. In the 𝑙-th layer, the message passing and aggregation could be formulated as:</p><formula xml:id="formula_10">𝒉 (𝑙) 𝑖 = ∑︁ 𝑗 ∈N (𝑖) 𝑨 𝑖 𝑗 𝒉 (𝑙−1) 𝑗 ,<label>(7)</label></formula><p>where N (𝑖) is the neighbor items and 𝒉</p><formula xml:id="formula_11">(𝑙)</formula><p>𝑖 ∈ R 𝑑 is the 𝑙-th layer item representation of item 𝑖. We set the input item representation</p><formula xml:id="formula_12">𝒉 (0)</formula><p>𝑖 as its corresponding ID embedding vector 𝒙 𝑖 . We utilize ID embeddings of items as input representations rather than multimodal features, since we employ graph convolutions in order to directly capture item-item affinities. After stacking 𝐿 layers, 𝒉 (𝐿) 𝑖 encodes the high-order item-item relationships that are constructed by multimodal information and thus can benefit the downstream CF methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Combining with Collaborative Filtering</head><p>Different from previous attempts which design sophisticated useritem aggregation strategies, LATTICE learns item representations from multimodal features and then combine with downstream CF methods that model user-item interactions. It is flexible and could be served as a play-and-plug module for any CF methods.</p><p>We denote the output user and item embedding from CF methods as 𝒙 𝑢 , 𝒙 𝑖 ∈ R 𝑑 and simply add normalized item embedding 𝒉 (𝐿) 𝑖 learned by item graph with the output item embedding of CF methods:</p><formula xml:id="formula_13">𝒙 𝑖 = 𝒙 𝑖 + 𝒉 (𝐿) 𝑖 ∥𝒉 (𝐿) 𝑖 ∥ 2 . (<label>8</label></formula><formula xml:id="formula_14">)</formula><p>We then compute the user-item preference score by</p><formula xml:id="formula_15">ŷ𝑢𝑖 = 𝒙 ⊤ 𝑢 𝒙 𝑖 .<label>(9)</label></formula><p>We conduct experiments in Section 3.3 on different downstream CF methods. Furthermore, the play-and-plug paradigm separates the usage of multimodal features with the usage of user-item interactions, thus alleviating the problem of cold-start, where tailed items are only interacted with few users or even never interacted with users. We learn latent structures for items and the tailed items can aggregate information from their learned neighbors. We conduct experiments in cold-start settings in Section 3.2 which proves the effectiveness of this play-and-plug paradigm. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Optimization</head><p>We adopt Bayesian Personalized Ranking (BPR) loss <ref type="bibr" target="#b28">[29]</ref> to conduct the pair-wise ranking, which encourages the prediction of an observed entry to be higher than its unobserved counterparts:</p><formula xml:id="formula_16">L BPR = − ∑︁ 𝑢 ∈U ∑︁ 𝑖 ∈I 𝑢 ∑︁ 𝑗∉I 𝑢 ln 𝜎 ŷ𝑢𝑖 − ŷ𝑢 𝑗 ,<label>(10)</label></formula><p>where I 𝑢 indicates the observed items associated with user 𝑢 and (𝑢, 𝑖, 𝑗) denotes the pairwise training triples where 𝑖 ∈ I 𝑢 is the positive item and 𝑗 ∉ I 𝑢 is the negative item sampled from unobserved interactions. 𝜎 (•) is the sigmoid function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS</head><p>In this section, we conduct experiments on three widely used realworld datasets to answer the following research questions:</p><p>• RQ1: How does our model perform compared with the stateof-the-art multi-modal recommendation methods and other CF methods in both warm-start and cold-start settings?</p><p>• RQ2: How effective are the item graph structures learned from multimodal features?</p><p>• RQ3: How sensitive is our model with different hyper-parameter settings?</p><p>• RQ4: What knowledge does our model learn from multimodal features?</p><p>3.1 Experiments Settings Clothing, Shoes and Jewelry, Sports and Outdoors, and Baby, which we refer to as Clothing, Sports, and Baby in brief. The statistics of these three datasets are summarized in Table <ref type="table" target="#tab_0">1</ref>. Amazon dataset includes visual modality and textual modality. We use the 4,096dimensional visual features that have been extracted and published <ref type="foot" target="#foot_0">1</ref> .</p><p>For the textual modality, we extract sentence embeddings as textual features by concatenating the title, descriptions, categories, and brand of one item and utilize sentence-transformers <ref type="bibr" target="#b27">[28]</ref> to obtain 1,024-dimensional sentence embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Baselines.</head><p>To evaluate the effectiveness of our proposed model, we compare it with several state-of-the-art recommendation models. These baselines fall into two groups: CF methods (i.e. MF, NGCF, LightGCN) and content-aware methods (i.e. VBPR, MMGCN, GRCN).</p><p>• MF <ref type="bibr" target="#b28">[29]</ref> optimizes Matrix Factorization using the Bayesian personalized ranking (BPR) loss, which exploits the useritem direct interactions only as the target value of interaction function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Clothing Sports Baby R@20 P@20 NDCG@20 R@20 P@20 NDCG@20 R@20 P@20 NDCG@20 MF 0. Table <ref type="table">2</ref>: Performance comparison of our LATTICE with different baselines in terms of Recall@20 (R@20), Precision@20 (P@20), and NDCG@20. The best performance is highlighted in bold and the second to best is highlighted by underlines. Improv. indicates relative improvements over the best baseline in percentage. All improvements all significant with 𝑝-value ≤ 0.05.</p><p>• NGCF <ref type="bibr" target="#b34">[35]</ref> explicitly models user-item interactions by a bipartite graph. By leveraging graph convolutional operations, it allows the embeddings of users and items interact with each other to harvest the collaborative signals as well as high-order connectivity signals.</p><p>• LightGCN <ref type="bibr" target="#b12">[13]</ref> argues the unnecessarily complicated design of GCNs (i.e. feature transformation and nonlinear activation) for recommendation systems and proposes a light model which only consists of two essential components: light graph convolution and layer combination.</p><p>• VBPR <ref type="bibr" target="#b11">[12]</ref>: Based upon the BPR model, it integrates the visual features and ID embeddings of each item as its representation and feed them into Matrix Factorization framework.</p><p>In our experiments, we concatenate multi-modal features as the content information to predict the interactions between users and items.</p><p>• MMGCN <ref type="bibr" target="#b37">[38]</ref> is one of the state-of-the-art multimodal recommendation methods, which constructs modal-specific graphs and refines modal-specific representations for users and items. Tt aggregates all model-specific representations to obtain the representations of users or items for prediction.</p><p>• GRCN <ref type="bibr" target="#b36">[37]</ref> is also one of the state-of-the-arts multimodal recommendation methods. It refines user-item interaction graph by identifying the false-positive feedback and prunes the corresponding noisy edges in the interaction graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Evaluation protocols.</head><p>We conduct experiments in both warmstart and cold-start settings.</p><p>Warm-start settings. For each dataset, we select 80% of historical interactions of each user to constitute the training set, 10% for validation set and the remaining 10% for testing set. For each observed user-item interaction, we treat it as a positive pair, and then conduct the negative sampling strategy to pair them with one negative item that the user does not interact before.</p><p>Cold-start settings. We randomly select 20% items and remove all user-item interaction pairs associated with these items from training set and divided half into validation and half into testing sets. In other words, these items are entirely unseen in the training set.</p><p>We adopt three widely-used metrics to evaluate the performance of preference ranking: Recall@𝑘, NDCG@𝑘, and Precision@𝑘. By default, we set 𝑘 = 20 and report the averaged metrics for all users in the testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Implementation details.</head><p>We implemente our method in Py-Torch <ref type="bibr" target="#b26">[27]</ref> and the embedding dimension 𝑑 is fixed to 64 for all models to ensure fair comparison. We optimize all models with the Adam <ref type="bibr" target="#b18">[19]</ref> optimizer, where the batch size is fixed at 1024. We use the Xavier initializer <ref type="bibr" target="#b8">[9]</ref> to initialize the model parameters. The optimal hyper-parameters were determined via grid search on the validation set: the learning rate is tuned amongst {0.0001, 0.0005, 0.001, 0.005}, the coefficient of ℓ 2 normalization is searched in {10 −5 , 10 −4 , 10 −3 , 10 −2 }, and the dropout ratio in {0.0, 0.1, • • • , 0.8}. Besides, we stop training if Recall@20 on the validation set does not increase for 10 successive epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Performance Comparison (RQ1)</head><p>We start by comparing the performance of all methods, and then explore how the item graph structures learned from multimodal features alleviate the cold-start problem. In this subsection, we combine our method with LightGCN as downstream CF method, and will also conduct experiments with different CF methods in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Overall performance. Table 2 reports the performance comparison results, from which we can observe:</head><p>• Our method significantly outperforms both CF methods and content-aware methods, verifying the effectiveness of our method. Specifically, our method improves over the strongest baselines in terms of Recall@20 by 12.5%, 9.8%, and 9.9% in Clothing, Sports, and Baby, respectively. This indicates our proposed method is well-designed for multimodal recommendation by discovering underlying item-item relationships from multimodal features.</p><p>• Compared with CF methods, content-aware methods yield better performance overall, which indicates that multimodal Model Clothing Sports Baby R@20 P@20 NDCG@20 R@20 P@20 NDCG@20 R@20 P@20 NDCG@ features provide rich content information about items, and can boost recommendations. GRCN outperforms other baselines in three datasets since it discovers and prunes falsepositive edges in user-item interaction graphs. In spite of the sophisticated designed mechanisms, GRCN is suboptimal compared with LATTICE, which verifies the importance of explicitly capturing item-item relationships.</p><p>• Additionally, existing content-aware methods are highly dependent on the representativeness of multimodal features and thus obtain fluctuating performances over different datasets.</p><p>For Clothing dataset where visual features are very important in revealing item attributes <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b22">23]</ref>, VBPR, MMGCN, and GRCN outperform all CF methods, including the powerful LightGCN. For the other two datasets where multimodal features may not directly reveal item attributes, contentaware methods obtain relatively small improvements. The performances of VBPR and MMGCN are even inferior to CF method LightGCN. Different from existing content-aware methods, we discover the latent item relationships underlying multimodal features instead of directly using them as side information. The latent item relationships are less dependent on the representativeness of multimodal features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.2</head><p>Performance in cold-start settings. The cold-start problem remains a prominent challenge in recommendation systems <ref type="bibr" target="#b30">[31]</ref>. Multimodal features of items provide rich content information, which can be exploited to alleviate cold-start problem. We conduct cold start experiments and compare with representative baselines. Figure <ref type="figure" target="#fig_3">3</ref> reports the performance comparison results, from which we can observe:</p><p>• LATTICE can alleviate cold-start problem and outperforms all baselines in three datasets. We learn item graphs from NDCG@20 (%) .</p><p>NDCG@20 (%) multimodal features, along which even unseen item can aggregate information from its learned neighbors.</p><formula xml:id="formula_18">MF LightGCN VBPR GRCN LATTICE-LightGCN</formula><p>• CF methods MF and LightGCN obtain poor performance under cold-start settings. CF methods only leverage users' feedbacks to predict the interactions between users and items. Although these methods may work well for items with sufficient feedbacks, they cannot help in cold-start settings, since no user preference information is available to form any basis for recommendations.</p><p>• Content-aware methods outperform CF methods overall, which indicates the content information provided by multimodal features benefits recommendation with unseen items.</p><p>In particular, content information can help bridge the gap from existing items to unseen items. However, some existing content-aware methods such as GRCN, although performs well in warm-start settings, obtains poor performance in cold-start settings. GRCN utilizes multimodal features on user-item interaction bipartite graphs, which is also heavily dependent on user-item interactions. For unseen items, they never interacted with users and become isolated nodes in the user-item graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Studies (RQ2)</head><p>In this subsection, we combine LATTICE with three common-used CF methods, i.e. MF, NGCF, and LightGCN to validate the effectiveness of our proposed method. For each CF method, we have three other variants: one is combined with our original method, employs graph convolutions on ID embeddings as described in Section 2.3, named LATTICE-CF; the second is LATTICE/feats-CF, which employs graph convolutions on multimodal features instead; the another is named CF+feats, which uses transformed multimodal features to replace the item representations learned from item graphs in Eq. ( <ref type="formula" target="#formula_13">8</ref>). Table <ref type="table" target="#tab_2">3</ref> summarizes the performance, from which we have the following observations:</p><p>• Our method significantly and consistently outperforms original CF methods and other two variants with all three CF methods, which verifies the effectiveness of discovering latent structures and the flexibility of our plug-in paradigm. Specifically, we obtain 17.6% average improvements over the "CF+feats" variants, which directly utilize multimodal features as side information of items.</p><p>• Based on the learned item graph structures, LATTICE/feats-CF employs graph convolutions on multimodal features. Our original method LATTICE-CF utilizes the same learned structures but employ graph convolutions on item ID embeddings, which aims to directly model item affinities. The improvements between two variants validate the effectiveness explicitly modeling item affinities. Multimodal features are used to bridge semantic relationships between items, which is important but not explicitly considered by existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Sensitivity Analysis (RQ3)</head><p>Since the graph structure learning layer plays a pivotal role in our method, in this subsection, we conduct sensitive analysis with different hyperparameters on graph structure learning layers. Firstly, we investigate performance of LATTICE-LightGCN with respect tor different 𝑘 value of 𝑘-NN sparsification operation since 𝑘 value is important which determines the number of neighbors of each item, and thus controls the information propagated from neighbors. Secondly, we discuss how the skip connection coefficient 𝜆 affects the performance which controls the amount of information from initial topology.</p><p>3.4.1 Impact of varied 𝑘 values. Figure <ref type="figure" target="#fig_5">4</ref> reports the performance comparison. 𝑘 = 0 means no item relationships are included and the model is degenerated to LightGCN. We have the following observations:</p><p>• Our method gains significant improvement between 𝑘 = 0 and 𝑘 = 10, which validates the rationality of item relationships mined from multimodal features. Even if only a small part of the neighbors are included, we can obtain better item representations by aggregating meaningful and important information, and thus boost the recommendation performance.  • Furthermore, the performance first grows as 𝑘 becomes larger, which verifies the effectiveness of information aggregation along item-item graphs since more neighbors bring more meaningful information that helps to make more accurate recommendations.</p><p>• The trend, however, declines when 𝑘 continues to increase, since there may exist many unimportant neighbors that inevitably introduce noisy to the information propagation. This demonstrates the necessity of conducting 𝑘NN sparsification on the dense graph. We have the following observations:</p><p>• When we set 𝜆 = 0, the model obtain poor performance. It only learns graph structure from the transformed high-level features, completely updating the adjacency matrix every time, resulting in fluctuating training process.</p><p>• The performance first grows as 𝜆 becomes larger, validating the importance of initial structures constructed by raw multimodal features. However, it begins to deteriorate when 𝜆 continues to increase, since raw features are often noisy due to the inevitably error-prone data measurement or collection process.</p><p>• Overall, there are no apparent sharp rises and falls, indicating that our method is not that sensitive to the selection of 𝜆. Notably, all performances exceed the baselines in Table <ref type="table">2</ref>, proving the effectiveness of item graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Case Studies (RQ4)</head><p>We select several representative items and visualize the initial textual-aware adjacency matrix, initial visual-aware matrix, initial fusion matrix (average on two modality graphs) and final learned adjacency matrix of them in Figure <ref type="figure" target="#fig_7">5</ref> for the better understanding of our proposed model. Due to the inevitably error-prone data measurement or limitation of feature extractors, raw multimodal features are often noisy. We can observe that almost all the items including sportswears and casual sweaters are clustered together in the initial visual-aware graph, since the visual features can not capture the details in the images and all the items are similar overall, which results in that the initial fusion graph is also noisy. However, the adjacency matrix learned by our proposed method can denoise initial item relationships by removing unimportant edges. We can observe that only significant item relationships are reserved, such as the relationships with casual sweaters (in the last row) and relationships among sportswears (in the third row).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK 4.1 Multimodal Recommendation</head><p>Collaborative filtering (CF) based methods have achieved great success in recommendation systems, which leverage users' feedbacks (such as clicks and purchase) to predict the preference of users and make recommendations. However, CF-based methods fail on sparse data with limited user-item interactions and many few viewed tail items. To address the problem of sparse data, it is important to exploit other information besides the user-item interactions. Multimodal recommendation systems extends the recommendation approach by considering massive multimedia content information of items, which have been successfully applied to many applications, such as e-commerce, instant video platforms and social media platforms <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b32">33]</ref>. For example, VBPR <ref type="bibr" target="#b11">[12]</ref> extended Matrix Factorization by incorporating visual features extracted from product images to improve the performance. DVBPR <ref type="bibr" target="#b16">[17]</ref> attempted to jointly train the image representation as well as the parameters in a recommender model. Sherlock <ref type="bibr" target="#b9">[10]</ref> incorporates categorical information for recommendation based on visual features. DeepStyle <ref type="bibr" target="#b22">[23]</ref> disentangled category information from visual representations for learning style features of items and sensing preferences of users. ACF <ref type="bibr" target="#b2">[3]</ref> introduced item-level and component-level attention model for inferring the underlying users' preferences encoded in the implicit user feedbacks. VECF <ref type="bibr" target="#b3">[4]</ref> modeled users' various attentions on different image regions and reviews. MV-RNN <ref type="bibr" target="#b5">[6]</ref> uses multimodal features for sequential recommendation in a recurrent framework.</p><p>Recently, due to its effectiveness in modeling graph-structure data, Graph Neural Networks (GNNs) have been introduced into recommendation systems <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b39">40]</ref> as well as multimodal recommendation systems <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>. MMGCN <ref type="bibr" target="#b37">[38]</ref> constructed modal-specific graph and refine modal-specific representations for users and items. GRCN <ref type="bibr" target="#b36">[37]</ref> refined user-item interaction graph by identifying the false-positive feedback and prunes the corresponding noisy edges in the interaction graph. In essence, the above methods directly utilize multimodal features as side information of each item. In our model, we step further by discovering fine-grained item-item relationships from multimodal features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Deep Graph Structure Learning</head><p>Although GNNs have shown great power on analyzing graphstructured data, most GNN methods are highly sensitive to the quality of graph structures and usually require a perfect graph structure that are hard to construct in real-world applications <ref type="bibr" target="#b6">[7]</ref>. Since GNNs recursively aggregate information from neighborhoods of one node to compute its node embedding, such an iterative mechanism has cascading effects -small noise in a graph will be propagated to neighboring nodes, affecting the embeddings of many others <ref type="bibr" target="#b43">[44]</ref>. Additionally, there also exist many real-world applications where initial graph structures are not available. Recently, considerable literature has arisen around the central theme of Graph Structure Learning (GSL), which targets at jointly learning an optimized graph structure and corresponding representations. There are three categories of GSL methods: metric learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b35">36]</ref>, probabilistic modeling <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b42">43]</ref>, and direct optimization approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b40">41]</ref>. For example, IDGL <ref type="bibr" target="#b4">[5]</ref> casts the graph learning problem into a similarity metric learning problem and leverage adaptive graph regularization for controlling the quality of the learned graph; DGM <ref type="bibr" target="#b17">[18]</ref> predicts a probabilistic graph, allowing a discrete graph to be sampled accordingly in order to be used in any graph convolutional operator.</p><p>In personalized recommendation, although user-item interactions can be formulated bipartite graph naturally, item-item relations remain rarely explored. To model item relationships explicitly, we employ metric learning approaches to represent edge weights as a distance measure between two end nodes, which is fit for multimedia recommendation since rich content information can be included to measure the semantic relationship between two items.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we have proposed the latent structure mining method (LATTICE) for multimodal recommendation, which leverages graph structure learning to discover latent item relationships underlying multimodal features. In particular, we first devise a modality-aware graph structure learning layer that learns item graph structures from multimodal features and fuses multimodal graphs. Along the learned graph structures, one item can receive informative highorder affinities from its neighbors by graph convolutions. Finally, we combine our model with downstream CF methods to make recommendations. Empirical results on three public datasets demonstrate the effectiveness of our proposed model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " M Q c x z z t v K i q o e m P D 4 + E u x a s 2 v e w = " &gt;A A A C X H i c b V B N S w M x E E 3 X q r W 1 t i p 4 8 b J Y B A 9 S d s W v Y 8 W L R 4 V W h X Y t 2 X T a B p P s k s y q Z d l f 4 l V / l B d / i 9 l a x F Y f B B 5 v Z j J v X h g L b t D z P g r O U n F 5 Z b W 0 V q 6 s V z d q 9 c 2 t W x M l m k G H R S L S 9 y E 1 I L i C D n I U c B 9 r o D I U c B c + X u b 1 u y f Q h k e q j Z M Y A k l H i g 8 5 o 2 i l f r 3 W C 2 V 6 k T 3 0 E F 4 w x a x f b 3 h N b w r 3 L / F n p E F m u O 5 v F k 5 6 g 4 g l E h Q y Q Y 3 p + l 6 M Q U o 1 c i Y g K / c S A z F l j 3 Q E X U s V l W C C d O o 8 c / e t M n C H k b Z P o T t V f 0 + k V B o z k a H t l B T H Z r G W i / / V u g k O z 4 O U q z h B U O x 7 0 T A R L k Z u H o M 7 4 B o Y i o k l l G l u v b p s T D V l a M O a 2 x L K + R v a f p D m Z v N v 5 x o F D 8 E e r R Z u / p G D V M E z vk w t l 8 s 2 a H 8 x 1 r / k 9 q j p n z b 9 m + N G 6 3 A W e Y n s k j 1 y Q H x y R l r k i l y T D m E k I a / k j b w X P p 2 i U 3 G q 3 6 1 O Y T a z T e b g 7 H w B e g S 3 0 A = = &lt; / l a t e x i t &gt; A t</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>3. 1 . 1</head><label>11</label><figDesc>Datasets. We conduct experiments on three categories of widely used Amazon dataset introduced by McAuley et al.<ref type="bibr" target="#b24">[25]</ref>:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: of our method with different baselines in cold-start settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Varied 𝜆 for Baby</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performances comparison over different hyperparameters settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>3. 4 . 2</head><label>42</label><figDesc>Impact of varied coefficients 𝜆. Figure4reports the performance comparison. 𝜆 = 0 means only consider the graph structure learned by transformed high-level multimodal features; 𝜆 = 1 means only consider the structure generated by raw multimodal features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Visualizing the adjacency of latent structures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the datasets</figDesc><table><row><cell cols="3">Dataset #Users #Items #Interactions Density</cell></row><row><cell>Clothing 39,387 23,033 Sports 35,598 18,357 Baby 19,445 7,050</cell><cell>237,488 256,308 139,110</cell><cell>0.00026 0.00039 0.00101</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance of our proposed LATTICE on top of different downstream collaborative filtering (CF) methods. Improv. indicates relative improvements in percentage over the base CF model with multimodal features (CF+feats).</figDesc><table><row><cell>20</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">http://jmcauley.ucsd.edu/data/amazon/links.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1">arXiv.org (March 2021). arXiv:2103.03036v1 [cs.LG]</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Recommender Systems: The Textbook</title>
		<author>
			<persName><forename type="first">C</forename><surname>Charu</surname></persName>
		</author>
		<author>
			<persName><surname>Aggarwal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast Approximate kNN Graph Construction for High Dimensional Data via Recursive Lanczos Bisection</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yousef</forename><surname>Haw-Ren Fang</surname></persName>
		</author>
		<author>
			<persName><surname>Saad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1989" to="2012" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Attentive Collaborative Filtering: Multimedia Recommendation with Item-and Component-Level Attention</title>
		<author>
			<persName><forename type="first">Jingyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="335" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Personalized Fashion Recommendation with Visual Explanations based on Multimodal Attention Network: Towards Visually Explainable Recommendation</title>
		<author>
			<persName><forename type="first">Hanxiong</forename><surname>Xu Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongteng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyuan</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="765" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Iterative Deep Graph Learning for Graph Neural Networks: Better and Robust Node Embeddings</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Zaki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In NeurIPS. 19314-19326</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">MV-RNN: A Multi-view Recurrent Neural Network for Sequential Recommendation</title>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="317" to="331" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning Discrete Structures for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Franceschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="1972">2019. 1972-1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploring Structure-Adaptive Graph Learning for Robust Semi-Supervised Classification</title>
		<author>
			<persName><forename type="first">Xiang</forename><forename type="middle">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongming</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Understanding the Difficulty of Training Deep Feedforward Neural Networks</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sherlock: Sparse Hierarchical Embeddings for Visually-aware One-class Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunbin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ups and Downs: Modeling the Visual Evolution of Fashion Trends with One-Class Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="507" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">VBPR: Visual Bayesian Personalized Ranking from Implicit Feedback</title>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="144" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong-Dong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Neural Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<idno>WWW. 173-182</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hierarchical Graph Convolutional Networks for Semi-supervised Node Classification</title>
		<author>
			<persName><forename type="first">Fenyu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4532" to="4539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Graph Structure Learning for Robust Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaorui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianfeng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="66" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visually-Aware Fashion Recommendation and Design with Generative Image Models</title>
		<author>
			<persName><forename type="first">Wang-Cheng</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="207" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Differentiable Graph Module (DGM) for Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Anees</forename><surname>Kazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Cosmo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.04999v3[cs.LG]</idno>
		<imprint>
			<date type="published" when="2020-02">2020. Feb. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adaptive Graph Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Ruoyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feiyun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3546" to="3553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hierarchical Fashion Graph Network for Personalized Outfit Recommendation</title>
		<author>
			<persName><forename type="first">Xingchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="159" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">DeepStyle: Learning User Preferences for Visual Recommendation</title>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="841" to="844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to Drop: Robust Graph Neural Network via Topological Denoising</title>
		<author>
			<persName><forename type="first">Dongsheng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenchao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingchao</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="779" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Image-Based Recommendations on Styles and Substitutes</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Targett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
	<note>Qinfeng Shi, and Anton van den Hengel</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Birds of a Feather: Homophily in Social Networks</title>
		<author>
			<persName><forename type="first">Lynn</forename><surname>Miller Mcpherson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Smith-Lovin</surname></persName>
		</author>
		<author>
			<persName><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Sociol</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="415" to="444" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3980" to="3990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">BPR: Bayesian Personalized Ranking from Implicit Feedback</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Item-based Collaborative Filtering Recommendation Algorithms</title>
		<author>
			<persName><forename type="first">George</forename><surname>Badrul Munir Sarwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">A</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="285" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Methods and Metrics for Cold-Start Recommendations</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">I</forename><surname>Schein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandrin</forename><surname>Popescul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lyle</forename><forename type="middle">H</forename><surname>Ungar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Pennock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="253" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A Survey of Collaborative Filtering Techniques</title>
		<author>
			<persName><forename type="first">Xiaoyuan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taghi</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">421425</biblScope>
			<biblScope unit="page" from="1" to="421425" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning Visual Clothing Style with Heterogeneous Dyadic Co-occurrences</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balazs</forename><surname>Kovacs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kavita</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<idno>ICCV. 4642-4650</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Neural Graph Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">AM-GCN: Adaptive Multi-channel Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meiqi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1243" to="1253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Graph-Refined Convolutional Network for Multimedia Recommendation with Implicit Feedback</title>
		<author>
			<persName><forename type="first">Yinwei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3451" to="3459" />
		</imprint>
	</monogr>
	<note>Xiangnan He, and Tat-Seng Chua</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">MMGCN: Multi-modal Graph Convolution Network for Personalized Recommendation of Micro-video</title>
		<author>
			<persName><forename type="first">Yinwei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1437" to="1445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Simplifying Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><surname>Holanda De</surname></persName>
		</author>
		<author>
			<persName><surname>Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Christopher Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Session-based Recommendation with Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuyuan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="346" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Topology Optimization based Graph Convolutional Network</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zesheng</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanfang</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4054" to="4061" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">TAGNN: Target Attentive Graph Neural Networks for Session-based Recommendation</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1921" to="1924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Robust Graph Representation Learning via Neural Sparsification</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongjin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingchao</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenchao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno>ICML. 11458-11468</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinghao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">Deep Graph Structure Learning for Robust Representations: A Survey</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
