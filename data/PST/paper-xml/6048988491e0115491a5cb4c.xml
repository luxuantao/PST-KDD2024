<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pretrained Transformers As Universal Computation Engines</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-03-09">9 Mar 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kevin</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">U</forename><forename type="middle">C</forename><surname>Berkeley</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
							<email>pabbeel@cs.berkeley.edu</email>
						</author>
						<author>
							<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
							<email>adityagrover@fb.com</email>
						</author>
						<author>
							<persName><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
							<email>imordatch@google.com</email>
						</author>
						<title level="a" type="main">Pretrained Transformers As Universal Computation Engines</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-03-09">9 Mar 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2103.05247v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We investigate the capability of a transformer pretrained on natural language to generalize to other modalities with minimal finetuning -in particular, without finetuning of the self-attention and feedforward layers of the residual blocks. We consider such a model, which we call a Frozen Pretrained Transformer (FPT), and study finetuning it on a variety of sequence classification tasks spanning numerical computation, vision, and protein fold prediction. In contrast to prior works which investigate finetuning on the same modality as the pretraining dataset, we show that pretraining on natural language improves performance and compute efficiency on non-language downstream tasks. In particular, we find that such pretraining enables FPT to generalize in zero-shot to these modalities, matching the performance of a transformer fully trained on these tasks 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Performance on Multimodal Sequence Benchmarks</p><p>Frozen Pretrained Transformer Full Transformer Full LSTM</p><p>Figure <ref type="figure">1</ref>: A frozen language-pretrained transformer (FPT) -without finetuning the self-attention and feedforward layers -can match the performance of a transformer fully trained on a downstream modality from scratch. We show results on diverse classification tasks (see Section 2.1): numerical computation (Bit Memory/XOR, ListOps), image classification (MNIST, CIFAR-10), and protein fold prediction (Homology). We also show results for a fully trained LSTM to provide a baseline. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experimental Details 1 Introduction</head><p>The transformer architecture <ref type="bibr" target="#b59">(Vaswani et al., 2017)</ref> has shown broad successes in deep learning, serving as the backbone of large models for tasks such as modeling natural language <ref type="bibr" target="#b8">(Brown et al., 2020)</ref>, images <ref type="bibr" target="#b17">(Dosovitskiy et al., 2020)</ref>, proteins <ref type="bibr" target="#b30">(Jumper et al., 2021)</ref>, behaviors <ref type="bibr" target="#b0">(Abramson et al., 2020)</ref>, or multimodal tasks comprising of both images and text <ref type="bibr" target="#b35">(Lu et al., 2019;</ref><ref type="bibr" target="#b45">Radford et al., 2021)</ref>.</p><p>Inspired by these successes, we seek to explore the generalization capabilities of a transformer in transferring from one modality to another.</p><p>Classical approaches to sequence processing used recurrent neural network (RNN) approaches <ref type="bibr" target="#b52">(Rumelhart et al., 1985;</ref><ref type="bibr" target="#b25">Hochreiter &amp; Schmidhuber, 1997)</ref>. In contrast, transformers utilize selfattention layers to extract features across tokens of a sequence, such as words <ref type="bibr" target="#b59">(Vaswani et al., 2017)</ref> or image patches <ref type="bibr" target="#b17">(Dosovitskiy et al., 2020)</ref>. Furthermore, it has become common practice to train large models on unsupervised or weakly supervised objectives before finetuning or evaluating zeroshot generalization on a downstream task. However, the downstream tasks that have been studied are generally restricted to the same modality as the original training set: for example, train GPT <ref type="bibr" target="#b43">(Radford et al., 2018)</ref> on a large language corpus, and finetune on a small task-specific dataset. Our goal in this work is to investigate finetuning on modalities distinct from the training modality.</p><p>We hypothesize that transformers, namely the self-attention layers, can be pretrained on a datarich modality (i.e. where data is plentiful, such as a natural language corpus) and identify feature representations that are useful for arbitrary data sequences, enabling effective downstream transfer to different modalities without expensive finetuning of the self-attention layers. In particular, we seek to investigate what pretrained language models (LMs) are capable of in terms of generalizing to other modalities with sequential structure, including numerical computation, image classification, and protein fold prediction.</p><p>In Figure <ref type="figure">1</ref>, we take a transformer model pretrained on natural language data, GPT-2 <ref type="bibr" target="#b44">(Radford et al., 2019)</ref>, and finetune only the linear input and output layers, as well as the positional embeddings and layer norm parameters. We call this model a Frozen Pretrained Transformer (FPT). On a range of tasks across a variety of modalities, FPT displays comparable performance to training the entire transformer or LSTM models, despite finetuning only .1% of the total number of parameters of the transformer model and none of the self-attention parameters. Additionally, we find FPT models also converge faster during training. Our results suggest that the self-attention layers learned by a language model may have properties amenable to efficient universal computation. Through a series of experiments, we seek to investigate why language pretraining can transfer to other modalities by examining pretraining regimes, architecture choice, attention maps, generalization abilities, model size, and importance of different sets of parameters in a transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Tasks</head><p>We evaluate on a diverse set of classification tasks representative of different modalities. In particular, we are interested in if language models are inherently capable of universal computation, by which we mean the ability to learn representations for predictive learning across diverse modalities.</p><p>Bit memory. Similar to the task proposed by <ref type="bibr" target="#b37">Miconi et al. (2018)</ref>, we consider a bit memory task where the model is shown 5 bitstrings each of length 1000. Afterwards, the model is shown a masked version of one of the bitstrings, where each bit is masked with probability 0.5, and the model is tasked with producing the original bitstring. The bitstrings are broken up into sequences of length 50, so that the models are fed 120 tokens of dimension 50.</p><p>Bit XOR. Similar to the bit memory task, the model is shown 2 bitstrings of length 5, where the model must predict the element-wise XOR of the two bitstrings. The bitstrings are shown 1 bit at a time, so the models are fed 10 tokens of dimension 1.</p><p>ListOps. Taken from <ref type="bibr" target="#b57">Tay et al. (2020)</ref>, the model is shown a sequence of list operations (ex. [ MAX 4 3 [ MIN 2 3 ] 1 0 ]) and tasked with predicting the resulting output digit (ex. 4). This task evaluates the ability of a model to parse mathematical expressions and evaluate over a long context. The model is shown 1 token at a time, so the models are fed 512 tokens of dimension 15.  MNIST. We use the standard MNIST benchmark, where the model must classify a handwritten digit from a 32 ? 32 black-and-white image. The tokens given to the model are 4 ? 4 image patches, so the models are fed 64 tokens of dimension 16. CIFAR-10. We use the standard CIFAR-10 benchmark <ref type="bibr" target="#b33">(Krizhevsky et al., 2009)</ref>, where the tokens given to the model are 4 ? 4 image patches, so the models are fed 64 tokens of dimension 16.</p><p>CIFAR-10 LRA. This is a modified version of the above task taken from the Long Range Arena benchmark where the images are converted to grayscale and flattened with a token length of 1 <ref type="bibr" target="#b57">(Tay et al., 2020)</ref>. As a result, the input sequence consists of 1024 tokens of dimension 1. This task is much more challenging than vanilla CIFAR-10 classification above as the models must learn patterns over a significantly longer sequence length and have minimal spatial inductive bias.</p><p>Remote homology detection. In this task, we are interested in predicting the fold for a protein, represented as an amino acid sequence. We use the datasets provided by TAPE <ref type="bibr" target="#b49">(Rao et al., 2019;</ref><ref type="bibr" target="#b19">Fox et al., 2013;</ref><ref type="bibr" target="#b26">Hou et al., 2018)</ref>, where the train/test split is generated by holding out certain evolutionary groups. Note that we do not pretrain on Pfam <ref type="bibr" target="#b18">(El-Gebali et al., 2019)</ref>, which is common in other works. There are 20 common and 5 uncommon amino acids (25 different types of inputs), and there are 1195 possible labels to predict. We only consider sequences of length less than 1024 for simplicity. The models are thus fed up to 1024 tokens of dimension 25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Architecture</head><p>The architecture we use is summarized in Figure <ref type="figure" target="#fig_2">2</ref>. Denote the embedding size/hidden dimension of the transformer as n dim , the number of layers as n layers , (note n dim = 768 and n layers = 12 for the base size models), the input dimension as d in , the output dimension (number of classes) as d out , and the maximum length of the sequence as l. We consider finetuning the following parameters of a pretrained GPT-2 model <ref type="bibr" target="#b44">(Radford et al., 2019</ref>):</p><p>? Output layer: it is crucial to finetune the output layer since we are transferring to a completely new task -we use the simplest possible instantiation of an output network, being a single linear layer applied to the last output token output by the transformer, in order to highlight that almost all the computation is being performed by the frozen transformer. The output layer has n dim ? d out parameters for the weight matrix. For example, for the base models on CIFAR-10, this comes out to 768 ? 10 = 7680 parameters.  <ref type="bibr" target="#b51">(Rebuffi et al., 2017;</ref><ref type="bibr" target="#b27">Houlsby et al., 2019)</ref>, we also finetune the affine layer norm parameters (scale and bias), which adapt to the statistics of the downstream task in a new domain. In GPT-2, layer norm is applied twice per block, so these are a total of 4 ? n dim ? n layers parameters. For the base models on CIFAR-10, these come out to 4 ? 768 ? 12 = 36684 parameters. ? Positional embeddings: though we find these are surprisingly universal between modalities (see Section 3.10), we generally see a small benefit to finetuning the positional embeddings which have a cheap parameter cost of l ? n dim . For the base models on CIFAR-10, these come out to 64 ? 768 = 49512 parameters.</p><p>Given the cheap linear scaling of these parameters, the parameter counts of large transformer models are dominated by the quadratic (in n dim and l) self-attention and feedforward layers. For the base CIFAR-10 model with 124M parameters, these come out to approximately 0.086% of the network. Due to this scaling, this number decreases with larger model sizes, down to 0.029% of the GPT-2 XL model. We further ablate the importance of each parameter in Section 3.10. For more details and a description of the architecture, see Appendix A.</p><p>Note that, crucially, all communication between tokens in the model are frozen. The data in each datapoint is chunked into discrete tokens (bits, image patches, amino acids, etc.), and can only reference each other via the frozen attention connections, which are not trained; additionally, neither the output nor the input layers are connected to multiple tokens. Our key investigation is to analyze the computation that is already inherent in the language model, and hence we do a minimal amount of computation that is learned on the downstream modality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Empirical Evaluations</head><p>In this section, we review the results demonstrating transfer from language to other modalities, and seek to better understand why this occurs and what enables this transfer. We also show benefits of language pretraining for compute efficiency, ablate the importance of training each parameter, and run miscellaneous experiments for related questions. All model sizes are the base model size (12 layers), unless stated otherwise. See Appendix B for more details on experiments.</p><p>3.1 Can pretrained language models transfer to different modalities?</p><p>We investigate if the self-attention and feedforward layers -the main body -of a pretrained transformer can be applied to a classification problem in a different modality without finetuning. To do this, we apply our base method as described above, where the input embedding layer, output readout layer, and layer norm parameters are finetuned.</p><p>Our results are shown in Figure <ref type="figure">1</ref> and also summarized below in Table <ref type="table" target="#tab_2">1</ref>. Note that some numbers are reported without a decimal point if we report a number from a different work (see below).</p><p>We find that across all seven tasks considered, FPT achieves comparable, if not marginally better performance than fully training a transformer. We believe these results support the idea that these models are learning representations and performing computation that is agnostic to the modality. We also note that both transformer variants significantly outperform LSTMs on some tasks, particularly ListOps and CIFAR-10 LRA, which have long sequence lengths of 512 and 1024, respectively.</p><p>On the two bit tasks (Memory and XOR), the models achieve 100% performance, i.e. they are able to recover the exact algorithm. Although our tables show results for n = 5, we actually find FPT can still recover the exact algorithm on sequence lengths greater than n = 256 (the elementwise XOR of two bitstrings each of length 256), hinting that FPT has a fairly large memory.  <ref type="figure">1</ref>).</p><p>We highlight a few important points for contextualizing these results. In particular, we find that it can be difficult to fully train a 12-layer transformer on some of these (relatively small) datasets, as training can either diverge/overfit or be unstable. For CIFAR-10, we report the full transformer results for a 3-layer model, and for CIFAR-10 LRA we report the number given for the 3-layer model from <ref type="bibr" target="#b57">Tay et al. (2020)</ref>. From an engineering perspective, this makes the full transformers harder to tune since we must choose model sizes that are stable and avoid overfitting -see Section 3.6 for more analysis. In contrast, we find it is easy to improve the performance of FPT by increasing model size (see Section 3.7); the number shown for FPT here is for the 36-layer large model. The baselines for remote homology are reported from <ref type="bibr" target="#b49">Rao et al. (2019)</ref>.</p><p>Furthermore, unlike some other works utilizing transformers for vision, we do not use convolutional input layers for a clean and fair analysis; only a linear layer is learned, which makes the vision tasks difficult. Note that we also do not use 2D positional embeddings (or other domain-specific techniques), hence providing very weak inductive prior to the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">What is the importance of the pretraining modality?</head><p>We compare the performance of FPT to other pretraining methods for the base model sizes:</p><p>? Random initialization (Random): initialization of the frozen transformer parameters randomly using the default initialization choices for GPT-2, i.e. without pretraining.</p><p>? Bit memory pretraining (Bit): pretraining on the Bit Memory task and then freezing the parameters before transferring. This allows the transformer to gain supervision working with arbitrary bit strings and performing memory/denoising on independent inputs.</p><p>? Image pretraining (ViT): using a pretrained Vision Transformer <ref type="bibr" target="#b17">(Dosovitskiy et al., 2020)</ref> pretrained on ImageNet-21k <ref type="bibr" target="#b14">(Deng et al., 2009)</ref>. Note that the architecture is a bit different, notably not using the autoregressive masking of GPT-2, since ViT is only pretrained on classification tasks.</p><p>These experiments highlight the significance of language supervision -as opposed to simply the transformer architecture -and compare to other methods of supervision. Our results are shown in Table <ref type="table" target="#tab_3">2</ref>. Although the random transformers can achieve surprisingly strong accuracies, there is a considerable gap to using natural language pretraining, such as in MNIST, where random transformers achieve similar performance to a linear classifier on top of raw features (92%). Thus we believe that while the transformer architecture might be naturally conducive to these evaluations, the attention mechanisms used to transfer may be nontrivial and not fully specified by the architecture. Pretraining on bit memory improves performance compared to the random models, but still lags behind training on natural language data. Furthermore, measured by gradient steps, all models converge faster than the randomly initialized transformers (more details in Section 3.4), indicating that all modes of pretraining improve upon random initialization even without considering accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Additionally, while freezing a vision transformer yields better improvements on CIFAR-10, pretraining on images is not uniformly better; e.g., ViT is worse on protein classification. One hypothesis is that protein sequences are structured like language, in terms of discrete units of information with a "grammar", so transfer from language to proteins may be more natural.</p><p>3.3 How important is the transformer architecture compared to LSTM architecture?</p><p>In Section 3.2 we found that the transformer architecture can already be fairly effective in this regime, even with only random parameters. In this section, we consider using a random LSTM architecture instead of the transformer, allowing us to consider the raw effect of the architecture.  Note that unlike in Figure <ref type="figure">1</ref>, the LSTM here is frozen. Frozen LSTMs perform very poorly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Our results are shown in Table <ref type="table" target="#tab_5">3</ref>. We find that the self-attention architecture already serves as an effective inductive bias for universal computation, improving significantly over the recurrent LSTM model and comprising most of the improvement in test accuracy from random LSTM to FPT. Note that despite a relatively small improvement in test accuracy, language pretraining still offers significant computation gains compared to random transformers (Section 3.4).</p><p>3.4 Does language pretraining improve compute efficiency over random initialization?</p><p>We investigate compute efficiency by considering the number of gradient steps to converge for FPT vs random transformer models, shown in Table <ref type="table">4</ref>. We generally find FPT converges faster, which indicates we can utilize language pretraining to gain compute benefits for non-language tasks. While random transformer models achieve decent test accuracies, in particular when compared to random LSTMs, there is still a considerable gap in the compute efficiency compared to using pretraining. Note that bit memory pretraining introduced in Section 3.2 generally falls between the two models, and notably is 6? slower than FPT on Bit XOR, which is significantly better than random.</p><formula xml:id="formula_0">Model Memory XOR ListOps MNIST C10 C10 LRA Homology FPT 1 ? 10 4 5 ? 10 2 2 ? 10 3 5 ? 10 3 4 ? 10 5 3 ? 10 5 1 ? 10 5 Random 4 ? 10 4 2 ? 10 4 6 ? 10 3 2 ? 10 4 4 ? 10 5 6 ? 10 5 1 ? 10 5 Speedup 4? 40? 3? 4? 1? 2? 1?</formula><p>Table <ref type="table">4</ref>: Approximate number of gradient steps until convergence for pretrained (FPT) vs randomly initialized (Random) models. Note that we use the same batch size and learning rate for both models.</p><p>3.5 Do the frozen attention layers attend to modality-specific tokens?</p><p>We investigate if FPT attends to semantically meaningful patterns in the data. We show the results in Figures <ref type="figure">3</ref> and<ref type="figure">4</ref> for the bit tasks. The attention map shows the attention weights from the first layer. Note GPT-2 is autoregressive, so the upper right corner of the attention mask is zeroed out. We did not find easily interpretable patterns on the other tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Does freezing the transformer prevent overfitting or underfitting?</head><p>Our general findings are that -in contrast to their fully trained counterparts -FPT models underfit the data, which lends them to further improvements by increasing model capacity (see Section 3.7). For example, consider CIFAR-10 LRA, which is maximally difficult due to lack of inductive prior over the sequence (each pixel is fed in as an arbitrary token only ordered by a raster scan) and relatively small dataset (50k images). In Table <ref type="table" target="#tab_6">5</ref>, we show the train/test gap between training FPT vs a 3-layer transformer from <ref type="bibr" target="#b57">Tay et al. (2020)</ref>, which we find to give stronger results than our experiments. In particular, they are much better than training a 12-layer transformer, which works poorly. Our results indicate that FPT is generally providing generalizable task representations without causing overfitting, whereas transformers can overfit arbitrarily poorly in low-data regimes (such as for Linformer, which overfit the most out of the architectures tested by <ref type="bibr" target="#b57">Tay et al. (2020)</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Does performance scale with model size?</head><p>We evaluate the efficacy of adding more parameters to these models on CIFAR-10. Most of the additional parameters are in the transformer layers and are trained during the natural language pretraining phase. Our results for pretrained and random models are in Table <ref type="table" target="#tab_7">6</ref>. Unlike fully training a transformer, which exhibits more overfitting and divergence during training with larger models, increasing model size stably increases the capacity of the models. This result indicates our observations and results are likely to scale as we move towards larger models and higher-data regimes. 3.8 Can performance be attributed simply to better statistics for initialization?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>In this section, we ablate taking the layer-wise mean and standard deviation from the pretrained model and using it to initialize a random transformer, in order to ablate if a better initialization scheme via an "oracle" standard deviation can recover the performance of FPT. Note that the GPT-2 initialization scheme initializes parameters as Gaussian; traditionally, the standard deviation is 0.02 by default. For clarity, we show the standard deviation by layer for the weights and biases of the attention and feedforward layers in Figure <ref type="figure" target="#fig_5">5</ref> for the pretrained models.  We show the results using this initialization scheme in Table <ref type="table" target="#tab_8">7</ref> (note that all of the weights, biases, layer norm, and positional embeddings are initialized -both mean and variance -in this fashion). This yields better results on most tasks, but does poorly on CIFAR-10. As a result, we believe the benefits of language pretrained cannot be recovered with a simple better initialization scheme. 3.9 Does finetuning the self-attention and feedforward layers further improve performance?</p><p>In this section, we investigate additionally finetuning the self-attention and feedforward layers, which were previously frozen. We simply add them to the list of parameters finetuned, without changing the optimization or learning rate scheme, although this is likely suboptimal. Our results are shown in Table <ref type="table" target="#tab_9">8</ref>. Note that +Both is fully finetuning the 12-layer transformer (in other sections, we use full transformer to denote fully finetuning a transformer from scratch where the depth was tuned, whereas here the depth is fixed). We find that finetuning the feedforward layers can improve performance, which is similar to techniques used in prior work <ref type="bibr" target="#b27">(Houlsby et al., 2019)</ref>, but finetuning the attention layers can lead to divergence. More sophisticated schemes finetuning only a subset of layers/parameters could further improve performance, as early experiments indicate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.10">Which parameters of the model are important to finetune?</head><p>We run ablations for only finetuning select parameters. Note for all experiments (including the previous ones), we initialize the input layers as Gaussian if embeddings are used, or use an orthogonal initialization for linear layers; in particular, we find orthogonal initialization to be very important when input parameters are not finetuned. Our results are shown on Page 13. We generally find the layer norm parameters to be most important.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.11">How well do the trends hold across other transformer models?</head><p>We also investigate how other transformer architectures perform when swapped out with GPT-2: BERT <ref type="bibr" target="#b15">(Devlin et al., 2018)</ref>, T5 <ref type="bibr" target="#b46">(Raffel et al., 2019)</ref>, and Longformer <ref type="bibr" target="#b6">(Beltagy et al., 2020)</ref>. For T5, we only use the encoder, and not the decoder. Our results are in Table <ref type="table" target="#tab_10">9</ref>. We find results to roughly hold across architectures, although T5 tends to be slightly worse than the other models. 3.12 Can we train a transformer by only finetuning the output layer?</p><p>We consider using FPT solely for naive feature extraction for linear regression, where we fix a randomly initialized input layer and freeze all parts of the model except for the output. Note that this resembles resevoir computing/echo state networks (see Section 4.5 for discussion). The model evaluates on every example in the training set once, caches the features, and then we train a linear output layer. This enables subsequent epochs after the first to run extremely quickly, but does not easily handle dropout/data augmentations, and scales well in terms of number of epochs, but not in dataset size. Our results are shown in Table <ref type="table" target="#tab_11">10</ref>. Although we find speedups extremely significant and they obtain nontrivial performance, performance significantly degrades and the models also exhibit overfitting (likely due to lack of regularization; unlike the training of FPT, dropout is not applied). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Transformers in multimodal settings</head><p>Transformers <ref type="bibr" target="#b59">(Vaswani et al., 2017)</ref> were first used successfully for natural language processing <ref type="bibr" target="#b43">(Radford et al., 2018;</ref><ref type="bibr" target="#b15">Devlin et al., 2018;</ref><ref type="bibr" target="#b44">Radford et al., 2019;</ref><ref type="bibr" target="#b8">Brown et al., 2020)</ref>. In recent years, they have also been shown to be effective architectures for other modalities. One particular modality of interest is computer vision <ref type="bibr">(Chen et al., 2020a;</ref><ref type="bibr" target="#b58">Touvron et al., 2020)</ref>; in particular, <ref type="bibr" target="#b17">Dosovitskiy et al. (2020)</ref> showed that transformers can outperform CNNs in the high-data regime on standard object recognition benchmarks such as ImageNet and CIFAR. Furthermore, transformers have also been used for prediction tasks over protein sequences <ref type="bibr" target="#b30">(Jumper et al., 2021;</ref><ref type="bibr">Rao et al., 2021)</ref>, reinforcement learning <ref type="bibr" target="#b40">(Parisotto et al., 2020)</ref>, and imitation learning <ref type="bibr" target="#b0">(Abramson et al., 2020)</ref>.</p><p>Work specifically tackling multimodal tasks include <ref type="bibr" target="#b31">Kaiser et al. (2017)</ref>, who showed a single model could learn a variety of multimodal tasks with an attention architecture. Recent work has utilized transformers for multimodal predictive tasks, such as images and text in ViLBERT <ref type="bibr" target="#b35">(Lu et al., 2019)</ref> and CLIP <ref type="bibr" target="#b45">(Radford et al., 2021)</ref>; these approaches generally use two distinct transformers to embed images and text. <ref type="bibr" target="#b36">Lu et al. (2020)</ref> applies ViLBERT to train a single model for a variety of combined vision and language tasks. Recent work from OpenAI <ref type="bibr" target="#b20">(Goh et al., 2021)</ref> finds that some neurons learned by CLIP are activated by a particular semantic concept, regardless of if the concept is presented in language or picture form. Our work is most similar to DALL-E <ref type="bibr" target="#b47">(Ramesh et al., 2021)</ref>, which uses a single transformer to embed both the image and text modalities, which we consider to be generating a "universal latent space" that projects any type of input into a single latent space. Such a latent space would be useful for a model that could learn from many sources of supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Transformers in transfer settings</head><p>There are also many works looking at transformers specifically in the context of in-modality transfer, such as ViT for vision <ref type="bibr" target="#b17">(Dosovitskiy et al., 2020)</ref>, T5 for language <ref type="bibr" target="#b46">(Raffel et al., 2019)</ref>, and UDSM-Prot for protein sequences <ref type="bibr" target="#b55">(Strodthoff et al., 2020)</ref>. CLIP <ref type="bibr" target="#b45">(Radford et al., 2021)</ref> showed that training on text in addition to images could allow for zero-shot classification via providing downstream labels as text. <ref type="bibr" target="#b24">Hernandez et al. (2021)</ref> do a thorough investigation of transfer with language pretraining, notably showing transfer from English to Python, which they consider to be reasonably distanced from English; many works have also looked at transferring from one langauge to another <ref type="bibr" target="#b2">(Artetxe et al., 2019;</ref><ref type="bibr" target="#b42">Ponti et al., 2019)</ref>. Similar to our work, <ref type="bibr" target="#b39">Papadimitriou &amp; Jurafsky (2020)</ref> investigate transfer for LSTMs between modalities including code, different languages, and music, finding that pretraining on "non-linguistic data with latent structure" can transfer to language, finding grammatical structure in a modality to be important, although we generally investigate the other direction and explore more distanced modalities. <ref type="bibr" target="#b34">Li et al. (2020)</ref> pretrain on a referential communication game where an emergent learned language is used to transfer to NLP tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Pretraining and finetuning of transformer models</head><p>A common trend in deep learning models is to first train a large model on an unsupervised objective on a large dataset <ref type="bibr" target="#b13">(Dai &amp; Le, 2015;</ref><ref type="bibr" target="#b43">Radford et al., 2018)</ref> and then finetune on a small downstream dataset (e.g., by freezing the model and only finetuing the output layer). A common method used to finetune transformers are adapter networks <ref type="bibr" target="#b51">(Rebuffi et al., 2017;</ref><ref type="bibr" target="#b27">Houlsby et al., 2019)</ref>, which add a fully connected residual block for each unique downstream task and also finetune the layer norm parameters. For simplicity, we do not add the full adapter block but only train the layer norm parameters, reducing the number of parameters we consider. These techniques used are similar to prior approaches such as FiLM <ref type="bibr" target="#b41">(Perez et al., 2018)</ref> and self-modulation <ref type="bibr" target="#b10">(Chen et al., 2018)</ref>. A recent direction of research has explored learning prompt templates for large models <ref type="bibr" target="#b54">(Shin et al., 2020)</ref> that simply require forward passes over the transformer. Unlike these works, we consider finetuning on one modality (language) and finetuning on others, whereas prior work investigates finetuning on the same modality as the pretraining task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Self-attention layers as optimization steps</head><p>The nature of computation performed by self-attention layers has also been explored by other related works. <ref type="bibr" target="#b5">Bai et al. (2019)</ref> show that a single transformer self-attention block can be trained to perform an optimization step towards finding a stationary point, representing the solution to the task. <ref type="bibr" target="#b48">Ramsauer et al. (2020)</ref> show that the self-attention layer is a gradient step in a Hopfield network with a learning rate of 1, hinting that transformers are capable of storing and retrieving a large amount of patterns with an implicit energy function. An interesting discussion from <ref type="bibr" target="#b21">Goyal &amp; Bengio (2020)</ref> points out a connection in viewing the key-value queries used in attention as similar to function signatures in computer programming: the key maps the input to a type (e.g., float) and the value maps the input to its value (e.g., 3.14), and if the type matches the function signature, the function can be applied to the value -this may be particularly relevant when we consider using a single self-attention layer applied to different modalities, as the modality may be embedded in the type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Global workspace theory</head><p>A common technique for evaluating the embeddings learned by an unsupervised model is to train a linear layer on top of the embeddings for a downstream task <ref type="bibr" target="#b16">(Donahue et al., 2016;</ref><ref type="bibr" target="#b38">Oord et al., 2018;</ref><ref type="bibr">Chen et al., 2020b)</ref>, which is reasonable when you finetune on the same modality as the pretrained one. However, when finetuning on a different modality, as in our setting, we have to reframe this notion of generalizable embedding quality -instead of only finetuning the output layer, we also want to finetune the input layer, and instead evaluate the ability of the frozen intermediate model to perform generalizable computation. This is reminiscent of Global Workspace Theory <ref type="bibr" target="#b4">(Baars, 1993)</ref>, which revolves around the notion that there is a "blackboard" that different parts of the brain send data to; we might consider the frozen language model as being a blackboard in this setting. Language might also be a natural choice of model for this blackboard, as there are hypotheses that language may serve as a good multipurpose high-level representation for cognitive behavior and conscious planning <ref type="bibr" target="#b1">(Andreas et al., 2017;</ref><ref type="bibr" target="#b21">Goyal &amp; Bengio, 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Reservoir computing</head><p>Similarly to the FPT setup and Global Workspace Theory, in reservoir computing <ref type="bibr" target="#b56">(Tanaka et al., 2019)</ref> and echo state networks <ref type="bibr" target="#b28">(Jaeger, 2001;</ref><ref type="bibr" target="#b29">Jaeger &amp; Haas, 2004)</ref>, a random recurrent network is frozen and only the output readout layer is trained. These models are very fast to train, using a similar setup as in Section 3.12, because the activations of the recurrent network can be cached and it is unnecessary to backpropagate over time. Somewhat differently to the FPT architecture, echo state networks are recurrent and thus feed back into themselves, which allows the outputs of the random frozen network to modulate future inputs. Unlike echo state networks, we also notably finetune the input and positional embeddings, which allow the inputs to the frozen network to adapt to a particular modality/for a query to the frozen network to be learned. Echo state networks are also similar to the perspective of self-attention applying a data-dependent filter to the inputs, as opposed to 1D convolutions, which are fixed filters regardless of the input modality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed transferring a pretrained transformer language model for downstream tasks in nonlanguage modalities. Through extensive empirical evaluation, we showed that these models could achieve performance competitive with transformers fully trained on the downstream task without having to finetune the self-attention and feedforward layers, relying solely on frozen parameters from the language model to perform the bulk of the computation.</p><p>We believe this work can serve as the foundation for future work investigating transfer between modalities. In future, we are interested in investigating the use of other data-rich modalities (e.g., vision) or a hybrid of multiple domains being used to provide the necessary substrate for pretraining a universal computational engine. It would also be interesting to explore frozen pretrained models for tasks beyond predictive modeling, such as reinforcement learning <ref type="bibr" target="#b0">(Abramson et al., 2020)</ref>.</p><p>For high stakes applications in the real-world, there are potential concerns with transfer of harmful biases from one modality to one another using pretrained transformer models trained on vast quantities of unlabeled, uncurated datasets <ref type="bibr" target="#b53">(Sheng et al., 2019;</ref><ref type="bibr" target="#b7">Bender et al., 2021)</ref>. Mitigating these biases is an active area of research <ref type="bibr" target="#b22">(Grover et al., 2019;</ref><ref type="bibr" target="#b12">Choi et al., 2020)</ref>. Conversely, there are also potential upsides with FPT models being able to better exploit representative datasets from one or more modalities, which merit future investigation as well.</p><p>Parameter ablations for pretrained models </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Background on Transformers</head><p>In this section, we give a description of the transformer architecture used in our experiments, namely the GPT-2 architecture <ref type="bibr" target="#b44">(Radford et al., 2019)</ref>.</p><p>A.1 Self-Attention</p><p>The main subcomponent of the transformer architecture is the self-attention layer, which takes in l input tokens and outputs l output tokens, both of dimension n dim . Each input token x i is mapped by linear transformations Q, K, and V -denoting query, key, and values, respectively -into q i , k i , and v i . Both q i and k i have dimension d k , and v i has dimension d v . To generate the output token y i , dot products are calculated between query q i and keys k j , and fed into a softmax operation to generate weights w i ? [0, 1] (in practice, a scaling temperature factor of 1</p><formula xml:id="formula_1">? d k</formula><p>is used to reduce the sharpness of the softmax). Then, the weights are used to generate y i as a weighted sum of all the values, i.e.:</p><formula xml:id="formula_2">y i = l j=1 exp(q i k j ) l k=1 exp(q i k k ) v j</formula><p>(1)</p><p>This is extended to multi-head attention over n heads heads by doing the above procedure n heads times, and then concatenating. To recover the original dimension the concatenated vector (of dimension d v n heads ) is multiplied by a projection matrix W proj ? R dvn heads ?n dim .</p><p>GPT-2 applies a causal mask to its inputs, i.e. the output token i is only allowed to attend to the input tokens j ? i, which changes the upper bounds of the sums in Equation 1 to i instead of l. This allows for unsupervised pretraining methods like language modeling (see Appendix A.4).</p><p>A residual connection is used to connect the inputs with the outputs of the attention layer. Then, in the rest of the transformer block, a two-layer MLP is used, conventionally projecting the dimension upwards to 4 ? n dim for the inner dimension and using the GELU activation function <ref type="bibr" target="#b23">(Hendrycks &amp; Gimpel, 2016)</ref>. Another residual connection is used to connect the outputs of the MLP with the previous outputs of the attention layer.</p><p>This forms the basis of the transformer block. As it preserves the dimension n dim , multiple blocks can be learned and stacked on top of each other n layers times, before feeding the final hidden states to the output layer. In our work, we only use the output of the last hidden state for classification, although in principle other methods are reasonable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Positional Embeddings</head><p>As the self-attention blocks are permutation-invariant, in order to capture positional information about sequences, positional embeddings are learned. For each position i ? (1, . . . , max len), a vector p i is learned. At the front of the transformer, before feeding in the inputs x i into the selfattention blocks, the positional embeddings are added to the input embeddings as x i := x i + p i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Layer Norm</head><p>Layer norm <ref type="bibr" target="#b3">(Ba et al., 2016)</ref> is frequently used in recurrent and transformer architectures as a means of normalizing the activations. In particular, for the activations of training example x of dimension n dim , it normalizes by the mean and variance over the features: ?i = x imean({x j } n dim j=1 ) std({x j } n dim j=1 )</p><p>(2)</p><p>Then, affine scale and shift parameters each of dimension n dim -? and ?, respectively -are learned to generate the outputs y.</p><formula xml:id="formula_3">y i = ? i ?i + ? i (3)</formula><p>Layer norm is applied twice per self-attention block: once before the attention layer and once before the MLP. As a result, a total of 4 ? n layers ? n dim layer norm parameters are learned.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Empirical Evaluations 3.1 Can pretrained language models transfer to different modalities? . . . . . . . . . . 3.2 What is the importance of the pretraining modality? . . . . . . . . . . . . . . . . . 3.3 How important is the transformer architecture compared to LSTM architecture? . . 3.4 Does language pretraining improve compute efficiency over random initialization? 3.5 Do the frozen attention layers attend to modality-specific tokens? . . . . . . . . . . 3.6 Does freezing the transformer prevent overfitting or underfitting? . . . . . . . . . . 3.7 Does performance scale with model size? . . . . . . . . . . . . . . . . . . . . . . 3.8 Can performance be attributed simply to better statistics for initialization? . . . . . 3.9 Does finetuning the self-attention and feedforward layers further improve performance? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.10 Which parameters of the model are important to finetune? . . . . . . . . . . . . . . 3.11 How well do the trends hold across other transformer models? . . . . . . . . . . . 3.12 Can we train a transformer by only finetuning the output layer? . . . . . . . . . . . 4 Related Work and Discussion 4.1 Transformers in multimodal settings . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Transformers in transfer settings . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3 Pretraining and finetuning of transformer models . . . . . . . . . . . . . . . . . . 4.4 Self-attention layers as optimization steps . . . . . . . . . . . . . . . . . . . . . . 4.5 Global workspace theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.6 Reservoir computing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . -Attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Positional Embeddings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Layer Norm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.4 Pretraining Objective . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.5 Model Sizes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Frozen Pretrained Transformer (FPT). The self-attention &amp; feedforward layers are frozen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: On Bit XOR, the model must produce the element-wise XOR of two bitstrings presented sequentially (inputs 0-4 are the first bitstring, inputs 5-9 are the second). Each token is one bit. FPT learns to attend positionally to the two bits that are XOR'ed by the output token.</figDesc><graphic url="image-1.png" coords="7,211.47,421.03,96.91,96.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Standard deviation of the parameters by layer for the pretrained GPT-2 model versus default initialization hyperparameters (0.02 for weights and 0 for biases).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Test accuracy of FPT vs fully training transformer on downstream task vs fully training LSTM on downstream task (results are transcribed from Figure</figDesc><table><row><cell cols="8">Model Bit Memory XOR ListOps MNIST CIFAR-10 C10 LRA Homology</cell></row><row><cell>FPT</cell><cell>100%</cell><cell>100%</cell><cell>38.4%</cell><cell>98.0%</cell><cell>72.1%</cell><cell>38.6%</cell><cell>12.7%</cell></row><row><cell>Full</cell><cell>100%</cell><cell>100%</cell><cell>38%</cell><cell>99.1%</cell><cell>70.3%</cell><cell>42%</cell><cell>9%</cell></row><row><cell>LSTM</cell><cell>60.9%</cell><cell>50.1%</cell><cell>17.1%</cell><cell>99.5%</cell><cell>73.6%</cell><cell>11.7%</cell><cell>12%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">Bit Memory XOR ListOps MNIST</cell><cell>C10</cell><cell cols="2">C10 LRA Homology</cell></row><row><cell>FPT</cell><cell>100%</cell><cell>100%</cell><cell>38.4%</cell><cell>98.0%</cell><cell>68.2%</cell><cell>38.6%</cell><cell>12.7%</cell></row><row><cell>Random</cell><cell>75.8%</cell><cell>100%</cell><cell>34.3%</cell><cell>91.7%</cell><cell>61.7%</cell><cell>36.1%</cell><cell>9.3%</cell></row><row><cell>Bit</cell><cell>100%</cell><cell>100%</cell><cell>35.4%</cell><cell>97.8%</cell><cell>62.6%</cell><cell>36.7%</cell><cell>7.8%</cell></row><row><cell>ViT</cell><cell>100%</cell><cell>100%</cell><cell>37.4%</cell><cell>97.8%</cell><cell>72.5%</cell><cell>43.0%</cell><cell>7.5%</cell></row></table><note><p>Test accuracy of language-pretrained (FPT) vs randomly initialized (Random) vs Bit Memory pretraining (Bit) vs pretrained Vision Transformer (ViT) models. The transformer is frozen.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Bit Memory XOR ListOps MNIST CIFAR-10 C10 LRA Homology</figDesc><table><row><cell>Trans.</cell><cell>75.8%</cell><cell>100%</cell><cell>34.3%</cell><cell>91.7%</cell><cell>61.7%</cell><cell>36.1%</cell><cell>9.3%</cell></row><row><cell>LSTM</cell><cell>50.9%</cell><cell>50.0%</cell><cell>16.8%</cell><cell>70.9%</cell><cell>34.4%</cell><cell>10.4%</cell><cell>6.6%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Test accuracy of randomly initialized transformers vs randomly initialized LSTM models.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Train vs test accuracies on CIFAR-10 LRA task.</figDesc><table><row><cell></cell><cell cols="3"># Layers Test Accuracy Train Accuracy</cell></row><row><cell>FPT (GPT-2)</cell><cell>12</cell><cell>38.6%</cell><cell>38.5%</cell></row><row><cell>Vanilla Transformer</cell><cell>3</cell><cell>42%</cell><cell>70%</cell></row><row><cell>Linformer</cell><cell>3</cell><cell>39%</cell><cell>97%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Test accuracy of larger frozen transformer models on CIFAR-10.</figDesc><table><row><cell cols="4">Size # Layers Total Params Trained Params</cell><cell>FPT</cell><cell>Random</cell></row><row><cell>Small (Base)</cell><cell>12</cell><cell>117M</cell><cell>106K</cell><cell>68.2%</cell><cell>61.7%</cell></row><row><cell>Medium</cell><cell>24</cell><cell>345M</cell><cell>190K</cell><cell>69.8%</cell><cell>64.0%</cell></row><row><cell>Large</cell><cell>36</cell><cell>774M</cell><cell>300K</cell><cell>72.1%</cell><cell>65.7%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Test accuracy when initializing parameters with pretrained weights (i.e., FPT) vs randomly initializing parameters according to the mean and variance of the pretrained transformer (Statistics Only) vs random initialization with default parameters (Default).</figDesc><table><row><cell cols="5">Initialization Memory XOR ListOps MNIST</cell><cell>C10</cell><cell cols="2">C10 LRA Homology</cell></row><row><cell>Pretrained</cell><cell>100%</cell><cell>100%</cell><cell>38.4%</cell><cell>98.0%</cell><cell>68.2%</cell><cell>38.6%</cell><cell>12.7%</cell></row><row><cell>Statistics Only</cell><cell>100%</cell><cell>100%</cell><cell>37.4%</cell><cell>97.2%</cell><cell>56.5%</cell><cell>33.1%</cell><cell>11.0%</cell></row><row><cell>Default</cell><cell>75.8%</cell><cell>100%</cell><cell>34.3%</cell><cell>91.7%</cell><cell>61.7%</cell><cell>36.1%</cell><cell>9.3%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Additionally finetuning either the feedforward layers, attention layers, or both. We do not use a per-layer learning scheme/etc.</figDesc><table><row><cell></cell><cell cols="4">Memory XOR ListOps MNIST</cell><cell>C10</cell><cell cols="2">C10 LRA Homology</cell></row><row><cell>FPT</cell><cell>100%</cell><cell>100%</cell><cell>38.4%</cell><cell>98.0%</cell><cell>68.2%</cell><cell>38.6%</cell><cell>12.7%</cell></row><row><cell>+ Feedforward</cell><cell>100%</cell><cell>100%</cell><cell>36.0%</cell><cell>98.3%</cell><cell>76.6%</cell><cell>38.2%</cell><cell>13.1%</cell></row><row><cell>+ Attention</cell><cell>100%</cell><cell>100%</cell><cell>36.8%</cell><cell cols="2">89.0%  ? 47.7%  ?</cell><cell>23.0%</cell><cell>10.9%</cell></row><row><cell>+ Both</cell><cell>100%</cell><cell>100%</cell><cell>35.8%</cell><cell>93.1%  ?</cell><cell>32.9%</cell><cell>21.0%</cell><cell>10.5%</cell></row></table><note><p>? training diverged, number reported before divergence.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Test accuracy for frozen pretrained transformer variants (base model sizes).</figDesc><table><row><cell>Task</cell><cell cols="2">GPT-2 (FPT Default) BERT</cell><cell>T5</cell><cell>Longformer</cell></row><row><cell>ListOps</cell><cell>38.4%</cell><cell cols="2">38.3% 15.4%</cell><cell>17.0%</cell></row><row><cell>CIFAR-10</cell><cell>68.2%</cell><cell cols="2">68.8% 64.7%</cell><cell>66.8%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Training only the output layer as a linear regression problem. Speedup refers to wall clock time per epoch (after the first). Larger models have larger speedups.</figDesc><table><row><cell></cell><cell>Speedup</cell><cell>Output Only</cell><cell>FPT</cell><cell>Full Transformer</cell></row><row><cell>ListOps</cell><cell>500 -2000?</cell><cell>32.8%</cell><cell>38.4%</cell><cell>38%</cell></row><row><cell cols="2">CIFAR-10 LRA 500 -2000?</cell><cell>24.7%</cell><cell>38.6%</cell><cell>42%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>Ablation by only finetuning individual types of parameters for pretrained frozen transformers. We bold the most important parameter (measured by highest test accuracy) for each task.</figDesc><table><row><cell>Task</cell><cell cols="5">output only output + input output + positions output + layernorm</cell></row><row><cell>Bit Memory</cell><cell>76%</cell><cell>98%</cell><cell></cell><cell>93%</cell><cell>94%</cell></row><row><cell>Bit XOR</cell><cell>56%</cell><cell>72%</cell><cell></cell><cell>84%</cell><cell>98%</cell></row><row><cell>ListOps</cell><cell>15%</cell><cell>17%</cell><cell></cell><cell>35%</cell><cell>36%</cell></row><row><cell>MNIST</cell><cell>23%</cell><cell>85%</cell><cell></cell><cell>93%</cell><cell>96%</cell></row><row><cell>CIFAR-10</cell><cell>25%</cell><cell>53%</cell><cell></cell><cell>38%</cell><cell>54%</cell></row><row><cell>CIFAR-10 LRA</cell><cell>17%</cell><cell>22%</cell><cell></cell><cell>30%</cell><cell>39%</cell></row><row><cell>Homology</cell><cell>2%</cell><cell>8%</cell><cell></cell><cell>8%</cell><cell>9%</cell></row><row><cell></cell><cell>Task</cell><cell cols="4">output only + layernorm + input + positions</cell></row><row><cell cols="2">Bit Memory</cell><cell>76%</cell><cell>94%</cell><cell>100%</cell><cell>100%</cell></row><row><cell cols="2">Bit XOR</cell><cell>56%</cell><cell>98%</cell><cell>98%</cell><cell>100%</cell></row><row><cell cols="2">ListOps</cell><cell>15%</cell><cell>36%</cell><cell>36%</cell><cell>38%</cell></row><row><cell cols="2">MNIST</cell><cell>23%</cell><cell>96%</cell><cell>98%</cell><cell>98%</cell></row><row><cell cols="2">CIFAR-10</cell><cell>25%</cell><cell>54%</cell><cell>60%</cell><cell>68%</cell></row><row><cell cols="2">CIFAR-10 LRA</cell><cell>17%</cell><cell>39%</cell><cell>39%</cell><cell>39%</cell></row><row><cell cols="2">Homology</cell><cell>2%</cell><cell>9%</cell><cell>10%</cell><cell>13%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 :</head><label>12</label><figDesc>Ablation by successively adding certain parameters to the list of finetuned parameters for pretrained frozen transformers.</figDesc><table><row><cell cols="3">Parameter ablations for random models</cell><cell></cell><cell></cell></row><row><cell>Task</cell><cell cols="4">output only output + input output + positions output + layernorm</cell></row><row><cell>Bit Memory</cell><cell>75%</cell><cell>75%</cell><cell>75%</cell><cell>75%</cell></row><row><cell>Bit XOR</cell><cell>50%</cell><cell>51%</cell><cell>59%</cell><cell>100%</cell></row><row><cell>ListOps</cell><cell>17%</cell><cell>17%</cell><cell>18%</cell><cell>35%</cell></row><row><cell>MNIST</cell><cell>25%</cell><cell>28%</cell><cell>34%</cell><cell>83%</cell></row><row><cell>CIFAR-10</cell><cell>20%</cell><cell>24%</cell><cell>21%</cell><cell>46%</cell></row><row><cell>CIFAR-10 LRA</cell><cell>11%</cell><cell>16%</cell><cell>12%</cell><cell>34%</cell></row><row><cell>Homology</cell><cell>2%</cell><cell>2%</cell><cell>6%</cell><cell>9%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 13 :</head><label>13</label><figDesc>Finetuning individual types of parameters for random frozen transformers.</figDesc><table><row><cell>Task</cell><cell cols="4">output only + layernorm + input + positions</cell></row><row><cell>Bit Memory</cell><cell>75%</cell><cell>75%</cell><cell>75%</cell><cell>76%</cell></row><row><cell>Bit XOR</cell><cell>50%</cell><cell>100%</cell><cell>100%</cell><cell>100%</cell></row><row><cell>ListOps</cell><cell>17%</cell><cell>35%</cell><cell>36%</cell><cell>37%</cell></row><row><cell>MNIST</cell><cell>25%</cell><cell>83%</cell><cell>92%</cell><cell>92%</cell></row><row><cell>CIFAR-10</cell><cell>20%</cell><cell>46%</cell><cell>56%</cell><cell>62%</cell></row><row><cell>CIFAR-10 LRA</cell><cell>11%</cell><cell>34%</cell><cell>36%</cell><cell>36%</cell></row><row><cell>Homology</cell><cell>2%</cell><cell>9%</cell><cell>9%</cell><cell>9%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 14 :</head><label>14</label><figDesc>Ablation by successively adding certain parameters to the list of finetuned parameters for random frozen transformers.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We would like to thank <rs type="person">Luke Metz</rs>, <rs type="person">Kimin Lee</rs>, <rs type="person">Fangchen Liu</rs>, <rs type="person">Roshan Rao</rs>, <rs type="person">Aravind Srinivas</rs>, <rs type="person">Nikita Kitaev</rs>, <rs type="person">Daniel Freeman</rs>, <rs type="person">Marc'Aurelio Ranzato</rs>, <rs type="person">Jacob Andreas</rs>, and <rs type="person">Ashish Vaswani</rs> for valuable feedback and discussions.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Pretraining Objective</head><p>GPT-2 is pretrained on an retrogressive language modeling objective optimizing for parameters which maximize the log-likelihood of the data: max ? E[log p ? (x)]. GPT-2 models sequences autoregressively, factorizing the probability distribution p(x) = p(x 1 , . . . , x l ) via chain rule as:</p><p>For the language domain, this objective can be interpreted as "given the previous i -1 words of a sentence, predict the next word".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Model Sizes</head><p>The model sizes from Section 3.7 are as follows:</p><p>Model </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experimental Details</head><p>We use implementations of and pretrained models from the Huggingface Transformers library <ref type="bibr" target="#b60">(Wolf et al., 2020)</ref>. We train the models using the Adam (Kingma &amp; Ba, 2014) optimizer with a learning rate of 10 -3 (no learning rate scheduling). For the remote homology task only, we use a learning rate of 10 -4 as we found it to give better performance than 10 -3 . We generally use the largest batch size that fits on an RTX 2080 Ti graphics card, somewhere between 2 and 16, without gradient accumulation. Models are trained to convergence and evaluated on a heldout test set.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Josh</forename><surname>Abramson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Brussee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Carnevale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><surname>Cassin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Dudzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petko</forename><surname>Georgiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelia</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.05672</idno>
		<title level="m">Imitating interactive intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00482</idno>
		<title level="m">Learning with latent language</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">On the cross-lingual transferability of monolingual representations</title>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.11856</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A cognitive theory of consciousness</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName><surname>Baars</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01377</idno>
		<title level="m">Deep equilibrium models</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><surname>Longformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<title level="m">The long-document transformer</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the dangers of stochastic parrots: Can language models be too big</title>
		<author>
			<persName><forename type="first">Emily</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelina</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shmargaret</forename><surname>Shmitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the 2020 Conference on Fairness, Accountability, and Transparency<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1691" to="1703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">On self modulation for generative adversarial networks</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.01365</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fair generative modeling via weak supervision</title>
		<author>
			<persName><forename type="first">Kristy</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trisha</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1887" to="1898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Semi-supervised sequence learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.01432</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09782</idno>
		<title level="m">Adversarial feature learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The Pfam protein families database in 2019</title>
		<author>
			<persName><forename type="first">Sara</forename><surname>El-Gebali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaina</forename><surname>Mistry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Bateman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><forename type="middle">R</forename><surname>Eddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aur?lien</forename><surname>Luciani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">C</forename><surname>Potter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matloob</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorna</forename><forename type="middle">J</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><forename type="middle">A</forename><surname>Salazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alfredo</forename><surname>Smart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L L</forename><surname>Erik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Layla</forename><surname>Sonnhammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisanna</forename><surname>Hirsh</surname></persName>
		</author>
		<author>
			<persName><surname>Paladin</surname></persName>
		</author>
		<author>
			<persName><surname>Damiano Piovesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C E</forename><surname>Silvio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">D</forename><surname>Tosatto</surname></persName>
		</author>
		<author>
			<persName><surname>Finn</surname></persName>
		</author>
		<idno type="DOI">10.1093/nar/gky995</idno>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<idno type="ISSN">0305-1048</idno>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scope: Structural classification of proteins-extended, integrating scop and astral data and classification of new structures</title>
		<author>
			<persName><forename type="first">Naomi K</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">E</forename><surname>Brenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John-Marc</forename><surname>Chandonia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="304" to="D309" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Multimodal neurons in artificial neural networks</title>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniela</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shan</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><forename type="middle">Jay</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Cammarata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Inductive biases for deep learning of higher-level cognition</title>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.15091</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Bias correction of learned generative models using likelihood-free importance weighting</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alekh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.09531</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Danny</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.01293</idno>
		<title level="m">Scaling laws for transfer</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deepsf: deep convolutional neural network for mapping protein sequences to folds</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Badri</forename><surname>Adhikari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1295" to="1303" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Parameter-efficient transfer learning for nlp</title>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Giurgiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruna</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>De Laroussilhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Attariyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2790" to="2799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The &quot;echo state&quot; approach to analysing and training recurrent neural networks-with an erratum note</title>
		<author>
			<persName><forename type="first">Herbert</forename><surname>Jaeger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>German National Research Center for Information Technology GMD Technical Report</publisher>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="page">13</biblScope>
			<pubPlace>Bonn, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless communication</title>
		<author>
			<persName><forename type="first">Herbert</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harald</forename><surname>Haas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">304</biblScope>
			<biblScope unit="issue">5667</biblScope>
			<biblScope unit="page" from="78" to="80" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">John</forename><surname>Jumper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><surname>Tunyasuvunakool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustin</forename><surname>??dek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Bridgland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A A</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardino</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislav</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishub</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stig</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Reiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michalina</forename><surname>Steinegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Pacholska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><surname>Kavukcuoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Pushmeet Kohli, and Demis Hassabis. High accuracy protein structure prediction using deep learning</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05137</idno>
		<title level="m">Llion Jones, and Jakob Uszkoreit. One model to learn them all</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Emergent communication pretraining for few-shot machine translation</title>
		<author>
			<persName><forename type="first">Yaoyiran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Edoardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Ponti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Vuli?</surname></persName>
		</author>
		<author>
			<persName><surname>Korhonen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.00890</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Vilbert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.02265</idno>
		<title level="m">Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">12-in-1: Multi-task vision and language representation learning</title>
		<author>
			<persName><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedanuj</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10437" to="10446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Differentiable plasticity: training plastic neural networks with backpropagation</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Miconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3559" to="3568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Pretraining on non-linguistic structure as a tool for analyzing learning bias in language models</title>
		<author>
			<persName><forename type="first">Isabel</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.14601</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Stabilizing transformers for reinforcement learning</title>
		<author>
			<persName><forename type="first">Emilio</forename><surname>Parisotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhant</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Lopez Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seb</forename><surname>Noury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7487" to="7498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harm</forename><forename type="middle">De</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Towards zero-shot language modeling</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Edoardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Ponti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Vuli?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roi</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2893" to="2903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Image, 2:T2</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Pavolv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedant</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gertchen</forename><surname>Krueger</surname></persName>
		</author>
		<title level="m">Sandhini Agarwal, and Ilya Sutskever. Dall?e: Creating images from text</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Sch?fl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Lehner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Seidl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Widrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Holzleitner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milena</forename><surname>Pavlovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geir</forename><surname>Kjetil Sandve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Greiff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.02217</idno>
		<title level="m">Hopfield networks is all you need</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Evaluating protein transfer learning with tape</title>
		<author>
			<persName><forename type="first">Roshan</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Canny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><forename type="middle">S</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Roshan</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Verkuil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">F</forename><surname>Canny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rives</surname></persName>
		</author>
		<idno type="DOI">10.1101/2021.02.12.430858</idno>
	</analytic>
	<monogr>
		<title level="j">Msa transformer. bioRxiv</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Learning multiple visual domains with residual adapters</title>
		<author>
			<persName><forename type="first">Hakan</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08045</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Learning internal representations by error propagation</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>David E Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">1985</date>
		</imprint>
		<respStmt>
			<orgName>California Univ San Diego La Jolla Inst for Cognitive Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Emily</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Premkumar</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01326</idno>
		<title level="m">The woman worked as a babysitter: On biases in language generation</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Autoprompt: Eliciting knowledge from language models with automatically generated prompts</title>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.15980</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Udsmprot: universal deep sequence models for protein classification</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Strodthoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Wenzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2401" to="2409" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Recent advances in physical reservoir computing: A review</title>
		<author>
			<persName><forename type="first">Gouhei</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toshiyuki</forename><surname>Yamane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Benoit H?roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryosho</forename><surname>Nakane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoki</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seiji</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hidetoshi</forename><surname>Numata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daiju</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akira</forename><surname>Hirose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="100" to="123" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.04006</idno>
		<title level="m">Long range arena: A benchmark for efficient transformers</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-10">October 2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
